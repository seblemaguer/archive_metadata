{
  "title": "Interspeech 2020",
  "location": "Shanghai, China",
  "startDate": "25/10/2020",
  "endDate": "29/10/2020",
  "URL": "http://www.interspeech2020.org/",
  "chair": "General Chair: Helen Meng, General Co-Chairs: Bo Xu and Thomas Zheng",
  "conf": "Interspeech",
  "year": "2020",
  "name": "interspeech_2020",
  "series": "Interspeech",
  "SIG": "",
  "title1": "Interspeech 2020",
  "date": "25-29 October 2020",
  "papers": {
    "pierrehumbert20_interspeech": {
      "authors": [
        [
          "Janet B.",
          "Pierrehumbert"
        ]
      ],
      "title": "The cognitive status of simple and complex models",
      "original": "keynote1",
      "page_count": 0,
      "order": 1,
      "p1": "0",
      "pn": "",
      "abstract": [
        "Human languages are extraordinarily rich systems. They have extremely large lexical inventories, and the elements in these inventories can be combined to generate a potentially unbounded set of distinct messages. Regularities at many different levels of representation \u2014 from the phonetic level through the syntax and semantics \u2014 support people's ability to process mappings between the physical reality of speech, and the objects, events, and relationships that speech refers to. However, human languages also simplify reality. The phonological system establishes equivalence classes amongst articulatory-acoustic events that have considerable variation at the parametric level. The semantic system similarly establishes equivalence classes amongst real-world phenomena having considerable variation. The tension between simplicity and complexity is a recurring theme of research on language modelling. In this talk, I will present three case studies in which a pioneering simple model omitted important complexities that were either included in later models, or that remain as challenges to this day.  The first is the acoustic theory of speech production, as developed by Gunnar Fant, the inaugural Medal recipient in 1989. By approximating the vocal tract as a half-open tube, it showed that the first three formants of vowels (which are the most important for the perception of vowel quality) can be computed as a linear systems problem. The second is the autosegmental-metrical theory of intonation, to which I contributed early in my career. It made the simplifying assumption that the correct model of phonological representation will support the limited set of observed non-local patterns, while excluding non-local patterns that do not naturally occur. The third case concerns how word-formation patterns are generalised in forming new words, whether though inflectional morphology (as in \u201cone wug; two wugs\u201d) or derivational morphology (as in \u201cnickname, unnicknameable\u201d). Several early models of word-formation assume that the morphemes are conceptual categories, sharing formal properties of other categories in the cognitive system. For all three case studies, I will suggest that \u2014 contrary to what one might imagine \u2014 the simple models enjoyed good success precisely because they were cognitively realistic. The most successful early models effectively incorporated ways in which the cognitive system simplifies reality. These simplifications are key to the learnability and adaptability of human languages.  The simplified core of the system provides the scaffolding for more complex or irregular aspects of language. In progressing from simple models to fully complex models, we should make sure we continue to profit from insights into how humans learn, encode, remember, and produce speech patterns.\n",
        "Bio: Janet B. Pierrehumbert is the Professor of Language Modelling in the Department of Engineering Science at the University of Oxford. She received her BA in Linguistics and Mathematics at Harvard in 1975, and her Ph.D in Linguistics from MIT in 1980.  Much of her Ph.D dissertation research on English prosody and intonation was carried out at AT&T Bell Laboratories, where she was also a Member of Technical Staff from 1982 to 1989. At AT&T Bell Labs, she collaborated with 2015 ISCA Medalist Mary Beckman on a theory of tone structure in Japanese, and with 2011 ISCA Medalist Julia Hirschberg on a theory of intonational meaning. After she moved to Northwestern University in1989, her research program used a wide variety of experimental and computational methods to explore how lexical systems emerge in speech communities. She showed that the mental representations of words are at once abstract and phonetically detailed, and that social factors interact with cognitive factors as lexical patterns are learned, remembered, and generalized. Pierrehumbert joined the faculty at the University of Oxford in 2015 as a member of the interdisciplinary Oxford e-Research Centre; she is also an adjunct faculty member at New Zealand Institute of Language, Brain, and Behaviour.  Her current research uses machine-learning methods to model the dynamics of on-line language. She is a founding member of the Association for Laboratory Phonology, and a Fellow of the Linguistic Society of America, the Cognitive Science Society, and the American Academy of Arts and Sciences. She was elected to the National Academy of Sciences in 2019.  "
      ]
    },
    "shinncunningham20_interspeech": {
      "authors": [
        [
          "Barbara",
          "Shinn-Cunningham"
        ]
      ],
      "title": "Brain networks enabling speech perception in everyday settings",
      "original": "keynote2",
      "page_count": 0,
      "order": 303,
      "p1": "0",
      "pn": "",
      "abstract": [
        "While cocktail parties aren't as common as they once were, we all can recall the feeling. You are at a loud party, in a boring conversation. Though you nod politely at all the right moments, your brain is busy listening to the juicy gossip in the interchange behind you. How is it that your brain enables this feat of volitionally directing attention, determining what sound energy is from what sound source, letting through sounds that seem important while filtering out the rest? How is it that unexpected sounds, like the sudden crash  of a shattering window, interrupt volitional attention? This talk will explain what we know about control of both spatial and non-spatial processing of sound, based on neuroimaging and behavioral studies, and discuss ways this knowledge can be utilized in developing new assistive listening devices.\n",
        "Bio: Barbara Shinn-Cunningham is an electrical engineer turned neuroscientist who uses behavioral, neuroimaging, and computational methods to understand auditory processing and perception. Her interests span from sensory coding in the cochlea to influences of brain networks on auditory processing in cortex (and everything in between). She is the Cowan Professor of Auditory Neuroscience in and Inaugural Director of the Neuroscience Institute at Carnegie Mellon University, a position she took up after over two decades on the faculty of Boston University. In her copious spare time, she competes in saber fencing and plays the oboe/English horn. She received the 2019 Helmholtz-Rayleigh Interdisciplinary Silver Medal and the 2013 Mentorship Award, both from the Acoustical Society of America (ASA). She is a Fellow of the ASA and of the American Institute for Medical and Biological Engineers, a lifetime National Associate of the National Research Council, and a recipient of fellowships from the Alfred P Sloan Foundation, the Whitaker Foundation, and the Vannevar Bush Fellows program.\n"
      ]
    },
    "lee20_interspeech": {
      "authors": [
        [
          "Lin-shan",
          "Lee"
        ]
      ],
      "title": "Doing Something we Never could with Spoken Language Technologies-from early days to the era of deep learning",
      "original": "keynote3",
      "page_count": 0,
      "order": 394,
      "p1": "0",
      "pn": "",
      "abstract": [
        "Some research effort tries to do something better, while some tries to do something we never could. Good examples for the former include having aircrafts fly faster, and having images look more beautiful; while good examples for the latter include developing the Internet to connect everyone over the world, and selecting information out of everything over the Internet with Google; to name a few. The former is always very good, while the latter is usually challenging. This talk is about the latter.A major problem for the latter is those we could never do before was very often very far from realization. This is actually normal for most research work, which could be enjoyed by users only after being realized by industry when the correct time arrived. The only difference is here we may need to wait for longer until the right time comes and the right industry appears. Also, the right industry eventually appeared at the right time may use new generations of technologies very different from the earlier solutions found in research. In this talk I'll present my personal experiences of doing something we never could with spoken language technologies, from early days to the era of deep learning, including how I considered, what I did and found, and what lessons we can learn today, ranging over various areas of spoken language technologies.\n",
        "Bio: Lin-shan Lee has been teaching in Electrical Engineering and Computer Science at National Taiwan University since 1979. He invented, published and demonstrated the earliest but very complete set of fundamental technologies and systems for Chinese spoken language technologies including TTS (1984-89), natural language grammar and parser (1986-91) and LVCSR (1987-97), considering the structural features of Chinese language (monosyllable per character, limited number of distinct monosyllables, tones, etc.) and the extremely limited resources. He then focused his work on speech information retrieval, proposing a whole set of approaches making retrieval performance less dependent on ASR accuracy, and improving retrieval efficiency by better user-content interaction. This part of work applies equally to all different languages, and was described as the stepping stones towards \"a spoken version of Google\" when Nature selected him in 2018 as one of the 10 \"Science Stars of East Asia\" in a special issue on scientific research in East Asia."
      ]
    },
    "mevawalla20_interspeech": {
      "authors": [
        [
          "Shehzad",
          "Mevawalla"
        ]
      ],
      "title": "Successes, Challenges and Opportunities for Speech Technology in Conversational Agents",
      "original": "keynote4",
      "page_count": 0,
      "order": 721,
      "p1": "0",
      "pn": "",
      "abstract": [
        "From the early days of modern ASR research in the 1990s, one of the driving visions of the field has been a computer-based assistant that could accomplish tasks for the user, simply by being spoken to. Today, we are close to achieving that vision, with a whole array of speech-enabled AI agents eager to help users. Amazon\u2019s Alexa pioneered the AI assistant concept for smart speaker devices enabled by far-field ASR. It currently supports billions of customer interactions per week, on over 100 million devices across multiple languages. This keynote will give an overview of the interplay between underlying speech technologies, including wakeword detection, endpointing, speaker identification, and speech recognition that enable Alexa. We highlight the complexities of combining these technologies into a seamless and robust speech-enabled user experience under large production load and real-time constraints. Interesting algorithmic and engineering challenges arise from  choices between deployment in the cloud versus on edge devices, and from constraints on latency and memory versus trade-offs in accuracy. Adapting recognition systems to trending topics, changing domain knowledge bases, and to the customer\u2019s personal catalogs adds additional complexity, as does the need to support adaptive conversational behavior (such as normal versus whispered speech). We also dive into the unique data aspects of large-scale deployments like Alexa, where a continuous stream of unlabeled data enables successful applications of weakly supervised learning. Finally, we highlight problems for the speech research community that remain to be solved before the promise of a fully natural, conversational assistant is fully realized.\n",
        "Bio: Shehzad Mevawalla is a Director in Amazon and responsible for automatic speech recognition, speaker recognition and paralinguistics in Alexa world-wide. Recognition from far-field speech input is a key enabling technology for Alexa, and Shehzad and his team work to advance the state of the art in this area for both cloud and edge device. A thirteen-year veteran at Amazon, he has held a variety of senior technical roles, which include supply chain optimization, marketplace trust and safety, and business intelligence, prior to his position with Alexa. Before joining Amazon in 2007, Shehzad was Director of Software at HNC, a company that specialized in financial AI, where he worked on products that used neural networks to detect fraud. Shehzad holds a Master\u2019s degree in Computer Engineering and a Bachelor\u2019s degree in Computer Science, both from the University of Southern California."
      ]
    },
    "li20_interspeech": {
      "authors": [
        [
          "Jinyu",
          "Li"
        ],
        [
          "Yu",
          "Wu"
        ],
        [
          "Yashesh",
          "Gaur"
        ],
        [
          "Chengyi",
          "Wang"
        ],
        [
          "Rui",
          "Zhao"
        ],
        [
          "Shujie",
          "Liu"
        ]
      ],
      "title": "On the Comparison of Popular End-to-End Models for Large Scale Speech Recognition",
      "original": "2846",
      "page_count": 5,
      "order": 2,
      "p1": "1",
      "pn": "5",
      "abstract": [
        "Recently, there has been a strong push to transition from hybrid models\nto end-to-end (E2E) models for automatic speech recognition. Currently,\nthere are three promising E2E methods: recurrent neural network transducer\n(RNN-T), RNN attention-based encoder-decoder (AED), and Transformer-AED.\nIn this study, we conduct an empirical comparison of RNN-T, RNN-AED,\nand Transformer-AED models, in both non-streaming and streaming modes.\nWe use 65 thousand hours of Microsoft anonymized training data to train\nthese models. As E2E models are more data hungry, it is better to compare\ntheir effectiveness with large amount of training data. To the best\nof our knowledge, no such comprehensive study has been conducted yet.\nWe show that although AED models are stronger than RNN-T in the non-streaming\nmode, RNN-T is very competitive in streaming mode if its encoder can\nbe properly initialized. Among all three E2E models, transformer-AED\nachieved the best accuracy in both streaming and non-streaming mode.\nWe show that both streaming RNN-T and transformer-AED models can obtain\nbetter accuracy than a highly-optimized hybrid model.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2846",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "gao20_interspeech": {
      "authors": [
        [
          "Zhifu",
          "Gao"
        ],
        [
          "Shiliang",
          "Zhang"
        ],
        [
          "Ming",
          "Lei"
        ],
        [
          "Ian",
          "McLoughlin"
        ]
      ],
      "title": "SAN-M: Memory Equipped Self-Attention for End-to-End Speech Recognition",
      "original": "2471",
      "page_count": 5,
      "order": 3,
      "p1": "6",
      "pn": "10",
      "abstract": [
        "End-to-end speech recognition has become popular in recent years, since\nit can integrate the acoustic, pronunciation and language models into\na single neural network. Among end-to-end approaches, attention-based\nmethods have emerged as being superior. For example,  Transformer,\nwhich adopts an encoder-decoder architecture. The key improvement introduced\nby Transformer is the utilization of self-attention instead of recurrent\nmechanisms, enabling both encoder and decoder to capture long-range\ndependencies with lower computational complexity. In this work, we\npropose boosting the self-attention ability with a DFSMN memory block,\nforming the proposed memory equipped self-attention (SAN-M) mechanism.\nTheoretical and empirical comparisons have been made to demonstrate\nthe relevancy and complementarity between self-attention and the DFSMN\nmemory block. Furthermore, the proposed SAN-M provides an efficient\nmechanism to integrate these two modules. We have evaluated our approach\non the public AISHELL-1 benchmark and an industrial-level 20,000-hour\nMandarin speech recognition task. On both tasks, SAN-M systems achieved\nmuch better performance than the self-attention based  Transformer\nbaseline system. Specially, it can achieve a CER of 6.46% on the AISHELL-1\ntask even without using any external LM, comfortably outperforming\nother state-of-the-art systems.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2471",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "jain20_interspeech": {
      "authors": [
        [
          "Mahaveer",
          "Jain"
        ],
        [
          "Gil",
          "Keren"
        ],
        [
          "Jay",
          "Mahadeokar"
        ],
        [
          "Geoffrey",
          "Zweig"
        ],
        [
          "Florian",
          "Metze"
        ],
        [
          "Yatharth",
          "Saraf"
        ]
      ],
      "title": "Contextual RNN-T for Open Domain ASR",
      "original": "2986",
      "page_count": 5,
      "order": 4,
      "p1": "11",
      "pn": "15",
      "abstract": [
        "End-to-end (E2E) systems for automatic speech recognition (ASR), such\nas RNN Transducer (RNN-T) and Listen-Attend-Spell (LAS) blend the individual\ncomponents of a traditional hybrid ASR system &#8212; acoustic model,\nlanguage model, pronunciation model &#8212; into a single neural network.\nWhile this has some nice advantages, it limits the system to be trained\nusing only paired audio and text. Because of this, E2E models tend\nto have difficulties with correctly recognizing rare words that are\nnot frequently seen during training, such as entity names. In this\npaper, we propose modifications to the RNN-T model that allow the model\nto utilize additional metadata text with the objective of improving\nperformance on these named entity words. We evaluate our approach on\nan in-house dataset sampled from de-identified public social media\nvideos, which represent an open domain ASR task. By using an attention\nmodel to leverage the contextual metadata that accompanies a video,\nwe observe a relative improvement of about 16% in Word Error Rate on\nNamed Entities (WER-NE) for videos with related metadata.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2986",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "pan20_interspeech": {
      "authors": [
        [
          "Jing",
          "Pan"
        ],
        [
          "Joshua",
          "Shapiro"
        ],
        [
          "Jeremy",
          "Wohlwend"
        ],
        [
          "Kyu J.",
          "Han"
        ],
        [
          "Tao",
          "Lei"
        ],
        [
          "Tao",
          "Ma"
        ]
      ],
      "title": "ASAPP-ASR: Multistream CNN and Self-Attentive SRU for SOTA Speech Recognition",
      "original": "2947",
      "page_count": 5,
      "order": 5,
      "p1": "16",
      "pn": "20",
      "abstract": [
        "In this paper we present state-of-the-art (SOTA) performance on the\nLibriSpeech corpus with two novel neural network architectures, a \nmultistream CNN for acoustic modeling and a  self-attentive simple\nrecurrent unit (SRU) for language modeling. In the hybrid ASR framework,\nthe multistream CNN acoustic model processes an input of speech frames\nin multiple parallel pipelines where each stream has a unique dilation\nrate for diversity. Trained with the SpecAugment data augmentation\nmethod, it achieves relative word error rate (WER) improvements of\n4% on test-clean and 14% on test-other. We further improve the performance\nvia N-best rescoring using a 24-layer self-attentive SRU language model,\nachieving WERs of 1.75% on test-clean and 4.46% on test-other.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2947",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "kadetotad20_interspeech": {
      "authors": [
        [
          "Deepak",
          "Kadetotad"
        ],
        [
          "Jian",
          "Meng"
        ],
        [
          "Visar",
          "Berisha"
        ],
        [
          "Chaitali",
          "Chakrabarti"
        ],
        [
          "Jae-sun",
          "Seo"
        ]
      ],
      "title": "Compressing LSTM Networks with Hierarchical Coarse-Grain Sparsity",
      "original": "1270",
      "page_count": 5,
      "order": 6,
      "p1": "21",
      "pn": "25",
      "abstract": [
        "The long short-term memory (LSTM) network is one of the most widely\nused recurrent neural networks (RNNs) for automatic speech recognition\n(ASR), but is parametrized by millions of parameters. This makes it\nprohibitive for memory-constrained hardware accelerators as the storage\ndemand causes higher dependence on off-chip memory, which bottlenecks\nlatency and power. In this paper, we propose a new LSTM training technique\nbased on hierarchical coarse-grain sparsity (HCGS), which enforces\nhierarchical structured sparsity by randomly dropping static block-wise\nconnections between layers. HCGS maintains the same hierarchical structured\nsparsity throughout training and inference; this reduces weight storage\nfor both training and inference hardware systems. We also jointly optimize\nin-training quantization with HCGS on 2-/3-layer LSTM networks for\nthe TIMIT and TED-LIUM corpora. With 16&#215; structured compression\nand 6-bit weight precision, we achieved a phoneme error rate (PER)\nof 16.9% for TIMIT and a word error rate (WER) of 18.9% for TED-LIUM,\nshowing the best trade-off between error rate and LSTM memory compression\ncompared to prior works.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1270",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "lohrenz20_interspeech": {
      "authors": [
        [
          "Timo",
          "Lohrenz"
        ],
        [
          "Tim",
          "Fingscheidt"
        ]
      ],
      "title": "BLSTM-Driven Stream Fusion for Automatic Speech Recognition: Novel Methods and a Multi-Size Window Fusion Example",
      "original": "2560",
      "page_count": 5,
      "order": 7,
      "p1": "26",
      "pn": "30",
      "abstract": [
        "Optimal fusion of streams for ASR is a nontrivial problem. Recently,\nso-called posterior-in-posterior-out (PIPO-)BLSTMs have been proposed\nthat serve as state sequence enhancers and have highly attractive training\nproperties. In this work, we adopt the PIPO-BLSTMs and employ them\nin the context of stream fusion for ASR. Our contributions are the\nfollowing: First, we show the positive effect of a PIPO-BLSTM as state\nsequence enhancer for various stream fusion approaches. Second, we\nconfirm the advantageous context-free (CF) training property of the\nPIPO-BLSTM for all investigated fusion approaches. Third, we show with\na fusion example of two streams, stemming from different short-time\nFourier transform window lengths, that all investigated fusion approaches\ntake profit. Finally, the turbo fusion approach turns out to be best,\nemploying a CF-type PIPO-BLSTM with a novel iterative augmentation\nin training.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2560",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "pham20_interspeech": {
      "authors": [
        [
          "Ngoc-Quan",
          "Pham"
        ],
        [
          "Thanh-Le",
          "Ha"
        ],
        [
          "Tuan-Nam",
          "Nguyen"
        ],
        [
          "Thai-Son",
          "Nguyen"
        ],
        [
          "Elizabeth",
          "Salesky"
        ],
        [
          "Sebastian",
          "St\u00fcker"
        ],
        [
          "Jan",
          "Niehues"
        ],
        [
          "Alex",
          "Waibel"
        ]
      ],
      "title": "Relative Positional Encoding for Speech Recognition and Direct Translation",
      "original": "2526",
      "page_count": 5,
      "order": 8,
      "p1": "31",
      "pn": "35",
      "abstract": [
        "Transformer models are powerful sequence-to-sequence architectures\nthat are capable of directly mapping speech inputs to transcriptions\nor translations. However, the mechanism for modeling positions in this\nmodel was tailored for text modeling, and thus is less ideal for acoustic\ninputs. In this work, we adapt the relative position encoding scheme\nto the Speech Transformer, where the key addition is relative distance\nbetween input states in the self-attention network. As a result, the\nnetwork can better adapt to the variable distributions present in speech\ndata. Our experiments show that our resulting model achieves the best\nrecognition result on the Switchboard benchmark in the non-augmentation\ncondition, and the best published result in the MuST-C speech translation\nbenchmark. We also show that this model is able to better utilize synthetic\ndata than the Transformer, and adapts better to variable sentence segmentation\nquality for speech translation.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2526",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "kanda20_interspeech": {
      "authors": [
        [
          "Naoyuki",
          "Kanda"
        ],
        [
          "Yashesh",
          "Gaur"
        ],
        [
          "Xiaofei",
          "Wang"
        ],
        [
          "Zhong",
          "Meng"
        ],
        [
          "Zhuo",
          "Chen"
        ],
        [
          "Tianyan",
          "Zhou"
        ],
        [
          "Takuya",
          "Yoshioka"
        ]
      ],
      "title": "Joint Speaker Counting, Speech Recognition, and Speaker Identification for Overlapped Speech of any Number of Speakers",
      "original": "1085",
      "page_count": 5,
      "order": 9,
      "p1": "36",
      "pn": "40",
      "abstract": [
        "We propose an end-to-end speaker-attributed automatic speech recognition\nmodel that unifies speaker counting, speech recognition, and speaker\nidentification on monaural overlapped speech. Our model is built on\nserialized output training (SOT) with attention-based encoder-decoder,\na recently proposed method for recognizing overlapped speech comprising\nan arbitrary number of speakers. We extend SOT by introducing a speaker\ninventory as an auxiliary input to produce speaker labels as well as\nmulti-speaker transcriptions. All model parameters are optimized by\nspeaker-attributed maximum mutual information criterion, which represents\na joint probability for overlapped speech recognition and speaker identification.\nExperiments on LibriSpeech corpus show that our proposed method achieves\nsignificantly better speaker-attributed word error rate than the baseline\nthat separately performs overlapped speech recognition and speaker\nidentification.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1085",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "fukuda20_interspeech": {
      "authors": [
        [
          "Takashi",
          "Fukuda"
        ],
        [
          "Samuel",
          "Thomas"
        ]
      ],
      "title": "Implicit Transfer of Privileged Acoustic Information in a Generalized Knowledge Distillation Framework",
      "original": "1575",
      "page_count": 5,
      "order": 10,
      "p1": "41",
      "pn": "45",
      "abstract": [
        "This paper proposes a novel generalized knowledge distillation framework,\nwith an implicit transfer of privileged information. In our proposed\nframework, teacher networks are trained with two input branches on\npairs of time-synchronous lossless and lossy acoustic features. While\none branch of the teacher network processes a privileged view of the\ndata using lossless features, the second branch models a student view,\nby processing lossy features corresponding to the same data. During\nthe training step, weights of this teacher network are updated using\na composite two-part cross entropy loss. The first part of this loss\nis computed between the predicted output labels of the lossless data\nand the actual ground truth. The second part of the loss is computed\nbetween the predicted output labels of the lossy data and lossless\ndata. In the next step of generating soft labels, only the student\nview branch of the teacher is used with lossy data. The benefit of\nthis proposed technique is shown on speech signals with long-term time-frequency\nbandwidth loss due to recording devices and network conditions. Compared\nto conventional generalized knowledge distillation with privileged\ninformation, the proposed method has a relative improvement of 9.5%\non both lossless and lossy test sets.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1575",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "park20_interspeech": {
      "authors": [
        [
          "Jinhwan",
          "Park"
        ],
        [
          "Wonyong",
          "Sung"
        ]
      ],
      "title": "Effect of Adding Positional Information on Convolutional Neural Networks for End-to-End Speech Recognition",
      "original": "3163",
      "page_count": 5,
      "order": 11,
      "p1": "46",
      "pn": "50",
      "abstract": [
        "Attention-based models with convolutional encoders enable faster training\nand inference than recurrent neural network-based ones. However, convolutional\nmodels often require a very large receptive field to achieve high recognition\naccuracy, which not only increases the parameter size but also the\ncomputational cost and run-time memory footprint. A convolutional encoder\nwith a short receptive field length can suffer from looping or skipping\nproblems when the input utterance contains the same words as nearby\nsentences. We believe that this is due to the insufficient receptive\nfield length, and try to remedy this problem by adding positional information\nto the convolution-based encoder. It is shown that the word error rate\n(WER) of a convolutional encoder with a short receptive field size\ncan be reduced significantly by augmenting it with positional information.\nVisualization results are presented to demonstrate the effectiveness\nof adding positional information. The proposed method improves the\naccuracy of attention models with a convolutional encoder and achieves\na WER of 10.60% on TED-LIUMv2 for an end-to-end speech recognition\ntask.\n"
      ],
      "doi": "10.21437/Interspeech.2020-3163",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "li20b_interspeech": {
      "authors": [
        [
          "Guanjun",
          "Li"
        ],
        [
          "Shan",
          "Liang"
        ],
        [
          "Shuai",
          "Nie"
        ],
        [
          "Wenju",
          "Liu"
        ],
        [
          "Zhanlei",
          "Yang"
        ],
        [
          "Longshuai",
          "Xiao"
        ]
      ],
      "title": "Deep Neural Network-Based Generalized Sidelobe Canceller for Robust Multi-Channel Speech Recognition",
      "original": "1101",
      "page_count": 5,
      "order": 12,
      "p1": "51",
      "pn": "55",
      "abstract": [
        "The elastic spatial filter (ESF) proposed in recent years is a popular\nmulti-channel speech enhancement front end based on deep neural network\n(DNN). It is suitable for real-time processing and has shown promising\nautomatic speech recognition (ASR) results. However, the ESF only utilizes\nthe knowledge of fixed beamforming, resulting in limited noise reduction\ncapabilities. In this paper, we propose a DNN-based generalized sidelobe\ncanceller (GSC) that can automatically track the target speaker&#8217;s\ndirection in real time and use the blocking technique to generate reference\nnoise signals to further reduce noise from the fixed beam pointing\nto the target direction. The coefficients in the proposed GSC are fully\nlearnable and an ASR criterion is used to optimize the entire network.\nThe 4-channel experiments show that the proposed GSC achieves a relative\nword error rate improvement of 27.0% compared to the raw observation,\n20.6% compared to the oracle direction-based traditional GSC, 10.5%\ncompared to the ESF and 7.9% compared to the oracle mask-based generalized\neigenvalue (GEV) beamformer.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1101",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "xu20_interspeech": {
      "authors": [
        [
          "Yong",
          "Xu"
        ],
        [
          "Meng",
          "Yu"
        ],
        [
          "Shi-Xiong",
          "Zhang"
        ],
        [
          "Lianwu",
          "Chen"
        ],
        [
          "Chao",
          "Weng"
        ],
        [
          "Jianming",
          "Liu"
        ],
        [
          "Dong",
          "Yu"
        ]
      ],
      "title": "Neural Spatio-Temporal Beamformer for Target Speech Separation",
      "original": "1458",
      "page_count": 5,
      "order": 13,
      "p1": "56",
      "pn": "60",
      "abstract": [
        "Purely neural network (NN) based speech separation and enhancement\nmethods, although can achieve good objective scores, inevitably cause\nnonlinear speech distortions that are harmful for the automatic speech\nrecognition (ASR). On the other hand, the minimum variance distortionless\nresponse (MVDR) beamformer with NN-predicted masks, although can significantly\nreduce speech distortions, has limited noise reduction capability.\nIn this paper, we propose a multi-tap MVDR beamformer with complex-valued\nmasks for speech separation and enhancement. Compared to the state-of-the-art\nNN-mask based MVDR beamformer, the multi-tap MVDR beamformer exploits\nthe inter-frame correlation in addition to the inter-microphone correlation\nthat is already utilized in prior arts. Further improvements include\nthe replacement of the real-valued masks with the complex-valued masks\nand the joint training of the complex-mask NN. The evaluation on our\nmulti-modal multi-channel target speech separation and enhancement\nplatform demonstrates that our proposed multi-tap MVDR beamformer improves\nboth the ASR accuracy and the perceptual speech quality against prior\narts.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1458",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "li20c_interspeech": {
      "authors": [
        [
          "Li",
          "Li"
        ],
        [
          "Kazuhito",
          "Koishida"
        ],
        [
          "Shoji",
          "Makino"
        ]
      ],
      "title": "Online Directional Speech Enhancement Using Geometrically Constrained Independent Vector Analysis",
      "original": "1484",
      "page_count": 5,
      "order": 14,
      "p1": "61",
      "pn": "65",
      "abstract": [
        "This paper proposes an online dual-microphone system for directional\nspeech enhancement, which employs geometrically constrained independent\nvector analysis (IVA) based on the auxiliary function approach and\nvectorwise coordinate descent. Its offline version has recently been\nproposed and shown to outperform the conventional auxiliary function\napproach-based IVA (AuxIVA) thanks to the properly designed spatial\nconstraints. We extend the offline algorithm to online by incorporating\nthe autoregressive approximation of an auxiliary variable. Experimental\nevaluations revealed that the proposed online algorithm could work\nin real-time and achieved superior speech enhancement performance to\nonline AuxIVA in both situations where a fixed target was interfered\nby a spatially stationary or dynamic interference.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1484",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "yu20_interspeech": {
      "authors": [
        [
          "Meng",
          "Yu"
        ],
        [
          "Xuan",
          "Ji"
        ],
        [
          "Bo",
          "Wu"
        ],
        [
          "Dan",
          "Su"
        ],
        [
          "Dong",
          "Yu"
        ]
      ],
      "title": "End-to-End Multi-Look Keyword Spotting",
      "original": "1521",
      "page_count": 5,
      "order": 15,
      "p1": "66",
      "pn": "70",
      "abstract": [
        "The performance of keyword spotting (KWS), measured in false alarms\nand false rejects, degrades significantly under the far field and noisy\nconditions. In this paper, we propose a multi-look neural network modeling\nfor speech enhancement which simultaneously steers to listen to multiple\nsampled look directions. The multi-look enhancement is then jointly\ntrained with KWS to form an end-to-end KWS model which integrates the\nenhanced signals from multiple look directions and leverages an attention\nmechanism to dynamically tune the model&#8217;s attention to the reliable\nsources. We demonstrate, on our large noisy and far-field evaluation\nsets, that the proposed approach significantly improves the KWS performance\nagainst the baseline KWS system and a recent beamformer based multi-beam\nKWS system.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1521",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "huang20_interspeech": {
      "authors": [
        [
          "Weilong",
          "Huang"
        ],
        [
          "Jinwei",
          "Feng"
        ]
      ],
      "title": "Differential Beamforming for Uniform Circular Array with Directional Microphones",
      "original": "1571",
      "page_count": 5,
      "order": 16,
      "p1": "71",
      "pn": "75",
      "abstract": [
        "Use of omni-directional microphones is commonly assumed in the differential\nbeamforming with uniform circular arrays. The conventional differential\nbeamforming with omni-directional elements tends to suffer in low white-noise-gain\n(WNG) at the low frequencies and decrease of directivity factor (DF)\nat high frequencies. WNG measures the robustness of beamformer and\nDF evaluates the array performance in the presence of reverberation.\nThe major contributions of this paper are as follows: First, we extends\nthe existing work by presenting a new approach with the use of the\ndirectional microphone elements, and show clearly the connection between\nthe conventional beamforming and the proposed beamforming. Second,\na comparative study is made to show that the proposed approach brings\nabout the noticeable improvement in WNG at the low frequencies and\nsome improvement in DF at the high frequencies by exploiting an additional\ndegree of freedom in the differential beamforming design. In addition,\nthe beampattern appears more frequency-invariant than that of the conventional\nmethod. Third, we study how the proposed beamformer performs as the\nnumber of microphone elements and the radius of the array vary.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1571",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "qi20_interspeech": {
      "authors": [
        [
          "Jun",
          "Qi"
        ],
        [
          "Hu",
          "Hu"
        ],
        [
          "Yannan",
          "Wang"
        ],
        [
          "Chao-Han Huck",
          "Yang"
        ],
        [
          "Sabato Marco",
          "Siniscalchi"
        ],
        [
          "Chin-Hui",
          "Lee"
        ]
      ],
      "title": "Exploring Deep Hybrid Tensor-to-Vector Network Architectures for Regression Based Speech Enhancement",
      "original": "1900",
      "page_count": 5,
      "order": 17,
      "p1": "76",
      "pn": "80",
      "abstract": [
        "This paper investigates different trade-offs between the number of\nmodel parameters and enhanced speech qualities by employing several\ndeep tensor-to-vector regression models for speech enhancement. We\nfind that a hybrid architecture, namely CNN-TT, is capable of maintaining\na good quality performance with a reduced model parameter size. CNN-TT\nis composed of several convolutional layers at the bottom for feature\nextraction to improve speech quality and a tensor-train (TT) output\nlayer on the top to reduce model parameters. We first derive a new\nupper bound on the generalization power of the convolutional neural\nnetwork (CNN) based vector-to-vector regression models. Then, we provide\nexperimental evidence on the Edinburgh noisy speech corpus to demonstrate\nthat, in single-channel speech enhancement, CNN outperforms DNN at\nthe expense of a small increment of model sizes. Besides, CNN-TT slightly\noutperforms the CNN counterpart by utilizing only 32% of the CNN model\nparameters. Besides, further performance improvement can be attained\nif the number of CNN-TT parameters is increased to 44% of the CNN model\nsize. Finally, our experiments of multi-channel speech enhancement\non a simulated noisy WSJ0 corpus demonstrate that our proposed hybrid\nCNN-TT architecture achieves better results than both DNN and CNN models\nin terms of better-enhanced speech qualities and smaller parameter\nsizes.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1900",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "wu20_interspeech": {
      "authors": [
        [
          "Jian",
          "Wu"
        ],
        [
          "Zhuo",
          "Chen"
        ],
        [
          "Jinyu",
          "Li"
        ],
        [
          "Takuya",
          "Yoshioka"
        ],
        [
          "Zhili",
          "Tan"
        ],
        [
          "Edward",
          "Lin"
        ],
        [
          "Yi",
          "Luo"
        ],
        [
          "Lei",
          "Xie"
        ]
      ],
      "title": "An End-to-End Architecture of Online Multi-Channel Speech Separation",
      "original": "1981",
      "page_count": 5,
      "order": 18,
      "p1": "81",
      "pn": "85",
      "abstract": [
        "Multi-speaker speech recognition has been one of the key challenges\nin conversation transcription as it breaks the single active speaker\nassumption employed by most state-of-the-art speech recognition systems.\nSpeech separation is considered as a remedy to this problem. Previously,\nwe introduced a system, called  unmixing, fixed-beamformer and  extraction\n(UFE), that was shown to be effective in addressing the speech overlap\nproblem in conversation transcription. With UFE, an input mixed signal\nis processed by fixed beamformers, followed by a neural network post\nfiltering. Although promising results were obtained, the system contains\nmultiple individually developed modules, leading potentially sub-optimum\nperformance. In this work, we introduce an end-to-end modeling version\nof UFE. To enable gradient propagation all the way, an attentional\nselection module is proposed, where an attentional weight is learnt\nfor each beamformer and spatial feature sampled over space. Experimental\nresults show that the proposed system achieves comparable performance\nin an offline evaluation with the original separate processing-based\npipeline, while producing remarkable improvements in an online evaluation.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1981",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "nakagome20_interspeech": {
      "authors": [
        [
          "Yu",
          "Nakagome"
        ],
        [
          "Masahito",
          "Togami"
        ],
        [
          "Tetsuji",
          "Ogawa"
        ],
        [
          "Tetsunori",
          "Kobayashi"
        ]
      ],
      "title": "Mentoring-Reverse Mentoring for Unsupervised Multi-Channel Speech Source Separation",
      "original": "2082",
      "page_count": 5,
      "order": 19,
      "p1": "86",
      "pn": "90",
      "abstract": [
        " Mentoring-reverse mentoring, which is a novel knowledge transfer framework\nfor unsupervised learning, is introduced in multi-channel speech source\nseparation. This framework aims to improve two different systems, which\nare referred to as a  senior and a  junior system, by mentoring each\nother. The senior system, which is composed of a neural separator and\na statistical blind source separation (BSS) model, generates a pseudo-target\nsignal. The junior system, which is composed of a neural separator\nand a post-filter, was constructed using teacher-student learning with\nthe pseudo-target signal generated from the senior system i.e, imitating\nthe output from the senior system (mentoring step). Then, the senior\nsystem can be improved by propagating the shared neural separator of\nthe grown-up junior system to the senior system (reverse mentoring\nstep). Since the improved neural separator can give better initial\nparameters for the statistical BSS model, the senior system can yield\nmore accurate pseudo-target signals, leading to iterative improvement\nof the pseudo-target signal generator and the neural separator. Experimental\ncomparisons conducted under the condition where mixture-clean parallel\ndata are not available demonstrated that the proposed mentoring-reverse\nmentoring framework yielded improvements in speech source separation\nover the existing unsupervised source separation methods.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2082",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "nakatani20_interspeech": {
      "authors": [
        [
          "Tomohiro",
          "Nakatani"
        ],
        [
          "Rintaro",
          "Ikeshita"
        ],
        [
          "Keisuke",
          "Kinoshita"
        ],
        [
          "Hiroshi",
          "Sawada"
        ],
        [
          "Shoko",
          "Araki"
        ]
      ],
      "title": "Computationally Efficient and Versatile Framework for Joint Optimization of Blind Speech Separation and Dereverberation",
      "original": "2138",
      "page_count": 5,
      "order": 20,
      "p1": "91",
      "pn": "95",
      "abstract": [
        "This paper proposes new blind signal processing techniques for optimizing\na multi-input multi-output (MIMO) convolutional beamformer (CBF) in\na computationally efficient way to simultaneously perform dereverberation\nand source separation. For effective CBF optimization, a conventional\ntechnique factorizes it into a multiple-target weighted prediction\nerror (WPE) based dereverberation filter and a separation matrix. However,\nthis technique requires the calculation of a huge spatio-temporal covariance\nmatrix that reflects the statistics of all the sources, which makes\nthe computational cost very high. For computationally efficient optimization,\nthis paper introduces two techniques: one that decomposes the huge\ncovariance matrix into ones for individual sources, and another that\ndecomposes the CBF into sub-filters for estimating individual sources.\nBoth techniques effectively and substantively reduce the size of the\ncovariance matrices that must calculated, and allow us to greatly reduce\nthe computational cost without loss of optimality.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2138",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "tu20_interspeech": {
      "authors": [
        [
          "Yan-Hui",
          "Tu"
        ],
        [
          "Jun",
          "Du"
        ],
        [
          "Lei",
          "Sun"
        ],
        [
          "Feng",
          "Ma"
        ],
        [
          "Jia",
          "Pan"
        ],
        [
          "Chin-Hui",
          "Lee"
        ]
      ],
      "title": "A Space-and-Speaker-Aware Iterative Mask Estimation Approach to Multi-Channel Speech Recognition in the CHiME-6 Challenge",
      "original": "2150",
      "page_count": 5,
      "order": 21,
      "p1": "96",
      "pn": "100",
      "abstract": [
        "We propose a space-and-speaker-aware iterative mask estimation (SSA-IME)\napproach to improving complex angular central Gaussian distributions\n(cACGMM) based beamforming in an iterative manner by leveraging upon\nthe complementary information obtained from SSA-based regression. First,\na mask calculated by beamformed speech features is proposed to enhance\nthe estimation accuracy of the ideal ratio mask from noisy speech.\nSecond, the outputs of cACGMM-beamformed speech with given time annotation\nas initial values are used to extract the log-power spectral and inter-phase\ndifference features of different speakers serving as inputs to estimate\nthe regression-based SSA model. Finally, in decoding, the mask estimated\nby the SSA model is also used to iteratively refine cACGMM-based masks,\nyielding enhanced multi-array speech. Tested on the recent CHiME-6\nChallenge Track 1 tasks, the proposed SSA-IME framework significantly\nand consistently outperforms state-of-the-art approaches, and achieves\nthe lowest word error rates for both Track 1 speech recognition tasks.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2150",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "youssef20_interspeech": {
      "authors": [
        [
          "Hmamouche",
          "Youssef"
        ],
        [
          "Pr\u00e9vot",
          "Laurent"
        ],
        [
          "Ochs",
          "Magalie"
        ],
        [
          "Chaminade",
          "Thierry"
        ]
      ],
      "title": "Identifying Causal Relationships Between Behavior and Local Brain Activity During Natural Conversation",
      "original": "2074",
      "page_count": 5,
      "order": 22,
      "p1": "101",
      "pn": "105",
      "abstract": [
        "Characterizing precisely neurophysiological activity involved in natural\nconversations remains a major challenge. We explore in this paper the\nrelationship between multimodal conversational behavior and brain activity\nduring natural conversations. This is challenging due to Functional\nMagnetic Resonance Imaging (fMRI) time resolution and to the diversity\nof the recorded multimodal signals. We use a unique corpus including\nlocalized brain activity and behavior recorded during a fMRI experiment\nwhen several participants had natural conversations alternatively with\na human and a conversational robot. The corpus includes fMRI responses\nas well as conversational signals that consist of synchronized raw\naudio and their transcripts, video and eye-tracking recordings. The\nproposed approach includes a first step to extract discrete neurophysiological\ntime-series from functionally well defined brain areas, as well as\nbehavioral time-series describing specific behaviors. Then, machine\nlearning models are applied to predict neurophysiological time-series\nbased on the extracted behavioral features. The results show promising\nprediction scores, and specific causal relationships are found between\nbehaviors and the activity in functional brain areas for both conditions,\ni.e., human-human and human-robot conversations.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2074",
      "author_area_id": "1",
      "author_area_label": "Speech Perception, Production and Acquisition"
    },
    "zhou20_interspeech": {
      "authors": [
        [
          "Di",
          "Zhou"
        ],
        [
          "Gaoyan",
          "Zhang"
        ],
        [
          "Jianwu",
          "Dang"
        ],
        [
          "Shuang",
          "Wu"
        ],
        [
          "Zhuo",
          "Zhang"
        ]
      ],
      "title": "Neural Entrainment to Natural Speech Envelope Based on Subject Aligned EEG Signals",
      "original": "1558",
      "page_count": 5,
      "order": 23,
      "p1": "106",
      "pn": "110",
      "abstract": [
        "Reconstruction of speech envelope from neural signal is a general way\nto study neural entrainment, which helps to understand the neural mechanism\nunderlying speech processing. Previous neural entrainment studies were\nmainly based on single-trial neural activities, and the reconstruction\naccuracy of speech envelope is not high enough, probably due to the\ninterferences from diverse noises such as breath and heartbeat. Considering\nthat such noises independently emerge in the consistent neural processing\nof the subjects responding to the same speech stimulus, we proposed\na method to align and average electroencephalograph (EEG) signals of\nthe subjects for the same stimuli to reduce the noises of neural signals.\nPearson correlation of constructed speech envelops with the original\nones showed a great improvement comparing to the single-trial based\nmethod. Our study improved the correlation coefficient in delta band\nfrom around 0.25 to 0.5, where 0.25 was obtained in previous leading\nstudies based on single-trial. The speech tracking phenomenon not only\noccurred in the commonly reported delta and theta band, but also occurred\nin the gamma band of EEG. Moreover, the reconstruction accuracy for\nregular speech was higher than that for the time-reversed speech, suggesting\nthat neural entrainment to natural speech envelope reflects speech\nsemantics.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1558",
      "author_area_id": "1",
      "author_area_label": "Speech Perception, Production and Acquisition"
    },
    "lian20_interspeech": {
      "authors": [
        [
          "Chongyuan",
          "Lian"
        ],
        [
          "Tianqi",
          "Wang"
        ],
        [
          "Mingxiao",
          "Gu"
        ],
        [
          "Manwa L.",
          "Ng"
        ],
        [
          "Feiqi",
          "Zhu"
        ],
        [
          "Lan",
          "Wang"
        ],
        [
          "Nan",
          "Yan"
        ]
      ],
      "title": "Does Lexical Retrieval Deteriorate in Patients with Mild Cognitive Impairment? Analysis of Brain Functional Network Will Tell",
      "original": "2490",
      "page_count": 5,
      "order": 24,
      "p1": "111",
      "pn": "115",
      "abstract": [
        "Alterations in speech and language are typical signs of mild cognitive\nimpairment (MCI), considered to be the prodromal stage of Alzheimer&#8217;s\ndisease (AD). Yet, very few studies have pointed out at what stage\ntheir speech production is disrupted. To bridge this knowledge gap,\nthe present study focused on lexical retrieval, a specific process\nduring speech production, and investigated how it is affected in cognitively\nimpairment patients with the state-of-the-art analysis of brain functional\nnetwork. 17 patients with MCI and 20 age-matched controls were invited\nto complete a primed picture naming task, of which the prime was either\nsemantically related or unrelated to the target. Using electroencephalography\n(EEG) signals collected during task performance, even-related potentials\n(ERPs) were analyzed, together with the construction of the brain functional\nnetwork. Results showed that whereas MCI patients did not exhibit significant\ndifferences in reaction time and ERP responses, their brain functional\nnetwork did alter associated with a significant main effect in accuracy.\nThe observation of increased cluster coefficients and characteristic\npath length indicated deteriorations in global information processing,\nwhich provided evidence that deficits in lexical retrieval might have\noccurred even at the preclinical stage of AD.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2490",
      "author_area_id": "1",
      "author_area_label": "Speech Perception, Production and Acquisition"
    },
    "fu20_interspeech": {
      "authors": [
        [
          "Zhen",
          "Fu"
        ],
        [
          "Jing",
          "Chen"
        ]
      ],
      "title": "Congruent Audiovisual Speech Enhances Cortical Envelope Tracking During Auditory Selective Attention",
      "original": "1957",
      "page_count": 5,
      "order": 25,
      "p1": "116",
      "pn": "120",
      "abstract": [
        "Listeners usually have the ability to selectively attend to the target\nspeech while ignoring competing sounds. The mechanism that top-down\nattention modulates the cortical envelope tracking to speech was proposed\nto account for this ability. Additional visual input, such as lipreading\nwas considered beneficial for speech perception, especially in noise.\nHowever, the effect of audiovisual (AV) congruency on the dynamic properties\nof cortical envelope tracking activities was not discussed explicitly.\nAnd the involvement of cortical regions processing AV speech was unclear.\nTo solve these issues, electroencephalography (EEG) was recorded while\nparticipants attending to one talker from a mixture for several AV\nconditions (audio-only, congruent and incongruent). Approaches of temporal\nresponse functions (TRFs) and inter-trial phase coherence (ITPC) analysis\nwere utilized to index the cortical envelope tracking for each condition.\nComparing with the audio-only condition, both indices were enhanced\nonly for the congruent AV condition, and the enhancement was prominent\nover both the auditory and visual cortex. In addition, timings of different\ncortical regions involved in cortical envelope tracking activities\nwere subject to stimulus modality. The present work provides new insight\ninto the neural mechanisms of auditory selective attention when visual\ninput is available.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1957",
      "author_area_id": "1",
      "author_area_label": "Speech Perception, Production and Acquisition"
    },
    "wang20_interspeech": {
      "authors": [
        [
          "Lei",
          "Wang"
        ],
        [
          "Ed X.",
          "Wu"
        ],
        [
          "Fei",
          "Chen"
        ]
      ],
      "title": "Contribution of RMS-Level-Based Speech Segments to Target Speech Decoding Under Noisy Conditions",
      "original": "1652",
      "page_count": 4,
      "order": 26,
      "p1": "121",
      "pn": "124",
      "abstract": [
        "Human listeners can recognize target speech streams in complex auditory\nscenes. The cortical activities can robustly track the amplitude fluctuations\nof target speech with auditory attentional modulation under a range\nof signal-to-masker ratios (SMRs). The root-mean-square (RMS) level\nof the speech signal is a crucial acoustic cue for target speech perception.\nHowever, in most studies, the neural-tracking activities were analyzed\nwith the intact speech temporal envelopes, ignoring the characteristic\ndecoding features in different RMS-level-specific speech segments.\nThis study aimed to explore the contributions of high- and middle-RMS-level\nsegments to target speech decoding in noisy conditions based on electroencephalogram\n(EEG) signals. The target stimulus was mixed with a competing speaker\nat five SMRs (i.e., 6, 3, 0, -3, and -6 dB), and then the temporal\nresponse function (TRF) was used to analyze the relationship between\nneural responses and high/middle-RMS-level segments. Experimental results\nshowed that target and ignored speech streams had significantly different\nTRF responses under conditions with the high- or middle-RMS-level segments.\nBesides, the high- and middle-RMS-level segments elicited different\nTRF responses in morphological distributions. These results suggested\nthat distinct models could be used in different RMS-level-specific\nspeech segments to better decode target speech with corresponding EEG\nsignals.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1652",
      "author_area_id": "1",
      "author_area_label": "Speech Perception, Production and Acquisition"
    },
    "zhao20_interspeech": {
      "authors": [
        [
          "Bin",
          "Zhao"
        ],
        [
          "Jianwu",
          "Dang"
        ],
        [
          "Gaoyan",
          "Zhang"
        ],
        [
          "Masashi",
          "Unoki"
        ]
      ],
      "title": "Cortical Oscillatory Hierarchy for Natural Sentence Processing",
      "original": "1633",
      "page_count": 5,
      "order": 27,
      "p1": "125",
      "pn": "129",
      "abstract": [
        "Human speech processing, either for listening or oral reading, requires\ndynamic cortical activities that are not only driven by sensory stimuli\nexternally but also influenced by semantic knowledge and speech planning\ngoals internally. Each of these functions has been known to accompany\nspecific rhythmic oscillations and be localized in distributed networks.\nThe question is how the brain organizes these spatially and spectrally\ndistinct functional networks in such a temporal precision that endows\nus with incredible speech abilities. For clarification, this study\nconducted an oral reading task with natural sentences and collected\nsimultaneously the involved brain waves, eye movements, and speech\nsignals with high-density EEG and eye movement equipment. By examining\nthe regional oscillatory spectral perturbation and modeling the frequency-specific\ninterregional connections, our results revealed a hierarchical oscillatory\nmechanism, in which gamma oscillation entrains with the fine-structured\nsensory input while beta oscillation modulated the sensory output.\nAlpha oscillation mediated between sensory perception and cognitive\nfunction via selective suppression. Theta oscillation synchronized\nlocal networks for large-scale coordination. Differing from a single\nfunction-frequency-correspondence, the coexistence of multi-frequency\noscillations was found to be critical for local regions to communicate\nremotely and diversely in a larger network.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1633",
      "author_area_id": "1",
      "author_area_label": "Speech Perception, Production and Acquisition"
    },
    "bosch20_interspeech": {
      "authors": [
        [
          "Louis ten",
          "Bosch"
        ],
        [
          "Kimberley",
          "Mulder"
        ],
        [
          "Lou",
          "Boves"
        ]
      ],
      "title": "Comparing EEG Analyses with Different Epoch Alignments in an Auditory Lexical Decision Experiment",
      "original": "2450",
      "page_count": 5,
      "order": 28,
      "p1": "130",
      "pn": "134",
      "abstract": [
        "In processing behavioral data from auditory lexical decision, reaction\ntimes (RT) can be defined relative to stimulus onset or relative to\nstimulus offset. Using stimulus onset as the reference invokes models\nthat assumes that relevant processing starts immediately, while stimulus\noffset invokes models that assume that relevant processing can only\nstart when the acoustic input is complete. It is suggested that EEG\nrecordings can be used to tear apart putative processes. EEG analysis\nrequires some kind of time-locking of epochs, so that averaging of\nmultiple signals does not mix up effects of different processes. However,\nin many lexical decision experiments the duration of the speech stimuli\nvaries substantially. Consequently, processes tied to stimulus offset\nare not appropriately aligned and might get lost in the averaging process.\nIn this paper we investigate whether the time course of putative processes\nsuch as phonetic encoding, lexical access and decision making can be\nderived from ERPs and from instantaneous power representations in several\nfrequency bands when epochs are time-locked at stimulus onset or stimulus\noffset. In addition, we investigate whether time-locking at the moment\nwhen the response is given can shed light on the decision process per\ns&#233;.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2450",
      "author_area_id": "1",
      "author_area_label": "Speech Perception, Production and Acquisition"
    },
    "talkar20_interspeech": {
      "authors": [
        [
          "Tanya",
          "Talkar"
        ],
        [
          "Sophia",
          "Yuditskaya"
        ],
        [
          "James R.",
          "Williamson"
        ],
        [
          "Adam C.",
          "Lammert"
        ],
        [
          "Hrishikesh",
          "Rao"
        ],
        [
          "Daniel",
          "Hannon"
        ],
        [
          "Anne",
          "O\u2019Brien"
        ],
        [
          "Gloria",
          "Vergara-Diaz"
        ],
        [
          "Richard",
          "DeLaura"
        ],
        [
          "Douglas",
          "Sturim"
        ],
        [
          "Gregory",
          "Ciccarelli"
        ],
        [
          "Ross",
          "Zafonte"
        ],
        [
          "Jeffrey",
          "Palmer"
        ],
        [
          "Paolo",
          "Bonato"
        ],
        [
          "Thomas F.",
          "Quatieri"
        ]
      ],
      "title": "Detection of Subclinical Mild Traumatic Brain Injury (mTBI) Through Speech and Gait",
      "original": "2651",
      "page_count": 5,
      "order": 29,
      "p1": "135",
      "pn": "139",
      "abstract": [
        "Between 15% to 40% of mild traumatic brain injury (mTBI) patients experience\nincomplete recoveries or provide subjective reports of decreased motor\nabilities, despite a clinically-determined complete recovery. This\ndemonstrates a need for objective measures capable of detecting subclinical\nresidual mTBI, particularly in return-to-duty decisions for warfighters\nand return-to-play decisions for athletes. In this paper, we utilize\nfeatures from recordings of directed speech and gait tasks completed\nby ten healthy controls and eleven subjects with lingering subclinical\nimpairments from an mTBI. We hypothesize that decreased coordination\nand precision during fine motor movements governing speech production\n(articulation, phonation, and respiration), as well as during gross\nmotor movements governing gait, can be effective indicators of subclinical\nmTBI. Decreases in coordination are measured from correlations of vocal\nacoustic feature time series and torso acceleration time series. We\napply eigenspectra derived from these correlations to machine learning\nmodels to discriminate between the two subject groups. The fusion of\ncorrelation features derived from acoustic and gait time series achieve\nan AUC of 0.98. This highlights the potential of using the combination\nof vocal acoustic features from speech tasks and torso acceleration\nduring a simple gait task as a rapid screening tool for subclinical\nmTBI.<SUP>1</SUP>\n"
      ],
      "doi": "10.21437/Interspeech.2020-2651",
      "author_area_id": "1",
      "author_area_label": "Speech Perception, Production and Acquisition"
    },
    "shor20_interspeech": {
      "authors": [
        [
          "Joel",
          "Shor"
        ],
        [
          "Aren",
          "Jansen"
        ],
        [
          "Ronnie",
          "Maor"
        ],
        [
          "Oran",
          "Lang"
        ],
        [
          "Omry",
          "Tuval"
        ],
        [
          "F\u00e9lix de Chaumont",
          "Quitry"
        ],
        [
          "Marco",
          "Tagliasacchi"
        ],
        [
          "Ira",
          "Shavitt"
        ],
        [
          "Dotan",
          "Emanuel"
        ],
        [
          "Yinnon",
          "Haviv"
        ]
      ],
      "title": "Towards Learning a Universal Non-Semantic Representation of Speech",
      "original": "1242",
      "page_count": 5,
      "order": 30,
      "p1": "140",
      "pn": "144",
      "abstract": [
        "The ultimate goal of transfer learning is to reduce labeled data requirements\nby exploiting a pre-existing embedding model trained for different\ndatasets or tasks. The visual and language communities have established\nbenchmarks to compare embeddings, but the speech community has yet\nto do so. This paper proposes a benchmark for comparing speech representations\non non-semantic tasks, and proposes a representation based on an unsupervised\ntriplet-loss objective. The proposed representation outperforms other\nrepresentations on the benchmark, and even exceeds state-of-the-art\nperformance on a number of transfer learning tasks. The embedding is\ntrained on a publicly available dataset, and it is tested on a variety\nof low-resource downstream tasks, including personalization tasks and\nmedical domain. The benchmark<SUP>4</SUP>, models<SUP>5</SUP>, and\nevaluation code<SUP>6</SUP> are publicly released.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1242",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "rajan20_interspeech": {
      "authors": [
        [
          "Rajeev",
          "Rajan"
        ],
        [
          "Aiswarya Vinod",
          "Kumar"
        ],
        [
          "Ben P.",
          "Babu"
        ]
      ],
      "title": "Poetic Meter Classification Using i-Vector-MTF Fusion",
      "original": "1794",
      "page_count": 5,
      "order": 31,
      "p1": "145",
      "pn": "149",
      "abstract": [
        "In this paper, a deep neural network (DNN)-based poetic meter classification\nscheme is proposed using a fusion of musical texture features (MTF)\nand i-vectors. The experiment is performed in two phases. Initially,\nthe mel-frequency cepstral coefficient (MFCC) features are fused with\nMTF and classification is done using DNN. MTF include timbral, rhythmic,\nand melodic features. Later, in the second phase, the MTF is fused\nwith i-vectors and classification is performed. The performance is\nevaluated using a newly created poetic corpus in Malayalam, one of\nthe prominent languages in India. While the MFCC-MTF/DNN system reports\nan overall accuracy of 80.83%, the i-vector/MTF fusion reports an overall\naccuracy of 86.66%. The performance is also compared with a baseline\nsupport vector machine (SVM)-based classifier. The results show that\nthe architectural choice of i-vector fusion with MTF on DNN has merit\nin recognizing meters from recited poems.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1794",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "dai20_interspeech": {
      "authors": [
        [
          "Wang",
          "Dai"
        ],
        [
          "Jinsong",
          "Zhang"
        ],
        [
          "Yingming",
          "Gao"
        ],
        [
          "Wei",
          "Wei"
        ],
        [
          "Dengfeng",
          "Ke"
        ],
        [
          "Binghuai",
          "Lin"
        ],
        [
          "Yanlu",
          "Xie"
        ]
      ],
      "title": "Formant Tracking Using Dilated Convolutional Networks Through Dense Connection with Gating Mechanism",
      "original": "1804",
      "page_count": 5,
      "order": 32,
      "p1": "150",
      "pn": "154",
      "abstract": [
        "Formant tracking is one of the most fundamental problems in speech\nprocessing. Traditionally, formants are estimated using signal processing\nmethods. Recent studies showed that generic convolutional architectures\ncan outperform recurrent networks on temporal tasks such as speech\nsynthesis and machine translation. In this paper, we explored the use\nof Temporal Convolutional Network (TCN) for formant tracking. In addition\nto the conventional implementation, we modified the architecture from\nthree aspects. First, we turned off the &#8220;causal&#8221; mode of\ndilated convolution, making the dilated convolution see the future\nspeech frames. Second, each hidden layer reused the output information\nfrom  all the previous layers through dense connection. Third, we also\nadopted a gating mechanism to alleviate the problem of gradient disappearance\nby selectively forgetting unimportant information. The model was validated\non the open access formant database VTR. The experiment showed that\nour proposed model was easy to converge and achieved an overall mean\nabsolute percent error (MAPE) of 8.2% on speech-labeled frames, compared\nto three competitive baselines of 9.4% (LSTM), 9.1% (Bi-LSTM) and 8.9%\n(TCN).\n"
      ],
      "doi": "10.21437/Interspeech.2020-1804",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "hu20_interspeech": {
      "authors": [
        [
          "Na",
          "Hu"
        ],
        [
          "Berit",
          "Janssen"
        ],
        [
          "Judith",
          "Hanssen"
        ],
        [
          "Carlos",
          "Gussenhoven"
        ],
        [
          "Aoju",
          "Chen"
        ]
      ],
      "title": "Automatic Analysis of Speech Prosody in Dutch",
      "original": "2142",
      "page_count": 5,
      "order": 33,
      "p1": "155",
      "pn": "159",
      "abstract": [
        "In this paper we present a publicly available tool for automatic analysis\nof speech prosody (AASP) in Dutch. Incorporating the state-of-the-art\nanalytical frameworks, AASP enables users to analyze prosody at two\nlevels from different theoretical perspectives. Holistically, by means\nof the Functional Principal Component Analysis (FPCA) it generates\nmathematical functions that capture changes in the shape of a pitch\ncontour. The tool outputs the weights of principal components in a\ntable for users to process in further statistical analysis. Structurally,\nAASP analyzes prosody in terms of prosodic events within the auto-segmental\nmetrical framework, hypothesizing prosodic labels in accordance with\nTranscription of Dutch Intonation (ToDI) with accuracy comparable to\nsimilar tools for other languages. Published as a Docker container,\nthe tool can be set up on various operating systems in only two steps.\nMoreover, the tool is accessed through a graphic user interface, making\nit accessible to users with limited programming skills.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2142",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "gresse20_interspeech": {
      "authors": [
        [
          "Adrien",
          "Gresse"
        ],
        [
          "Mathias",
          "Quillot"
        ],
        [
          "Richard",
          "Dufour"
        ],
        [
          "Jean-Fran\u00e7ois",
          "Bonastre"
        ]
      ],
      "title": "Learning Voice Representation Using Knowledge Distillation for Automatic Voice Casting",
      "original": "2236",
      "page_count": 5,
      "order": 34,
      "p1": "160",
      "pn": "164",
      "abstract": [
        "The search for professional voice-actors for audiovisual productions\nis a sensitive task, performed by the artistic directors (ADs). The\nADs have a strong appetite for new talents/voices but cannot perform\nlarge scale auditions. Automatic tools able to suggest the most suited\nvoices are of a great interest for audiovisual industry. In previous\nworks, we showed the existence of acoustic information allowing to\nmimic the AD&#8217;s choices. However, the only available information\nis the ADs&#8217; choices from the already dubbed multimedia productions.\nIn this paper, we propose a representation-learning based strategy\nto build a character/role representation, called p-vector. In addition,\nthe large variability between audiovisual productions makes it difficult\nto have homogeneous training datasets. We overcome this difficulty\nby using knowledge distillation methods to take advantage of external\ndatasets. Experiments are conducted on video-game voice excerpts. Results\nshow a significant improvement using the p-vector, compared to the\nspeaker-based x-vector representation.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2236",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "yegnanarayana20_interspeech": {
      "authors": [
        [
          "B.",
          "Yegnanarayana"
        ],
        [
          "Anand",
          "Joseph"
        ],
        [
          "Vishala",
          "Pannala"
        ]
      ],
      "title": "Enhancing Formant Information in Spectrographic Display of Speech",
      "original": "2653",
      "page_count": 5,
      "order": 35,
      "p1": "165",
      "pn": "169",
      "abstract": [
        "Formants are resonances of the time varying vocal tract system, and\ntheir characteristics are reflected in the response of the system for\na sequence of impulse-like excitation sequence originated at the glottis.\nThis paper presents a method to enhance the formants information in\nthe display of spectrogram of the speech signal, especially for high\npitched voices. It is well known that in the narrowband spectrogram,\nthe presence of pitch harmonics masks the formant information, whereas\nin the wideband spectrogram, the formant regions are smeared. Using\nsingle frequency filtering (SFF) analysis, we show that the wideband\nequivalent SFF spectrogram can be modified to enhance the formant information\nin the display by improving the frequency resolution. For this, we\nobtain two SFF spectrograms by using single frequency filtering of\nthe speech signal at two closely spaced roots on the real axis in the\nz-plane. The ratio or difference of the two SFF spectrograms is processed\nto enhance the formant information in the spectrographic display. This\nwill help in tracking rapidly changing formants and in resolving closely\nspaced formants. The effect is more pronounced in the case of high-pitched\nvoices, like female and children speech.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2653",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "gump20_interspeech": {
      "authors": [
        [
          "Michael",
          "Gump"
        ],
        [
          "Wei-Ning",
          "Hsu"
        ],
        [
          "James",
          "Glass"
        ]
      ],
      "title": "Unsupervised Methods for Evaluating Speech Representations",
      "original": "2990",
      "page_count": 5,
      "order": 36,
      "p1": "170",
      "pn": "174",
      "abstract": [
        "Disentanglement is a desired property in representation learning and\na significant body of research has tried to show that it is a useful\nrepresentational prior. Evaluating disentanglement is challenging,\nparticularly for real world data like speech, where ground truth generative\nfactors are typically not available. Previous work on disentangled\nrepresentation learning in speech has used categorical supervision\nlike phoneme or speaker identity in order to disentangle grouped feature\nspaces. However, this work differs from the typical dimension-wise\nview of disentanglement in other domains. This paper proposes to use\nlow-level acoustic features to provide the structure required to evaluate\ndimension-wise disentanglement. By choosing well-studied acoustic features,\ngrounded and descriptive evaluation is made possible for unsupervised\nrepresentation learning. This work produces a toolkit for evaluating\ndisentanglement in unsupervised representations of speech and evaluates\nits efficacy on previous research.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2990",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "tran20_interspeech": {
      "authors": [
        [
          "Dung N.",
          "Tran"
        ],
        [
          "Uros",
          "Batricevic"
        ],
        [
          "Kazuhito",
          "Koishida"
        ]
      ],
      "title": "Robust Pitch Regression with Voiced/Unvoiced Classification in Nonstationary Noise Environments",
      "original": "3019",
      "page_count": 5,
      "order": 37,
      "p1": "175",
      "pn": "179",
      "abstract": [
        "Accurate voiced/unvoiced information is crucial in estimating the pitch\nof a target speech signal in severe nonstationary noise environments.\nNevertheless, state-of-the-art pitch estimators based on deep neural\nnetworks (DNN) lack a dedicated mechanism for robustly detecting voiced\nand unvoiced segments in the target speech in noisy conditions. In\nthis work, we proposed an end-to-end deep learning-based pitch estimation\nframework which jointly detects voiced/unvoiced segments and predicts\npitch values for the voiced regions of the ground-truth speech. We\nempirically showed that our proposed framework significantly more robust\nthan state-of-the-art DNN based pitch detectors in nonstationary noise\nsettings. Our results suggest that joint training of voiced/unvoiced\ndetection and voiced pitch prediction can significantly improve pitch\nestimation performance.\n"
      ],
      "doi": "10.21437/Interspeech.2020-3019",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "setlur20_interspeech": {
      "authors": [
        [
          "Amrith",
          "Setlur"
        ],
        [
          "Barnab\u00e1s",
          "P\u00f3czos"
        ],
        [
          "Alan W.",
          "Black"
        ]
      ],
      "title": "Nonlinear ISA with Auxiliary Variables for Learning Speech Representations",
      "original": "3050",
      "page_count": 5,
      "order": 38,
      "p1": "180",
      "pn": "184",
      "abstract": [
        "This paper extends recent work on nonlinear Independent Component Analysis\n( ica) by introducing a theoretical framework for nonlinear Independent\nSubspace Analysis ( isa) in the presence of auxiliary variables. Observed\nhigh dimensional acoustic features like log Mel spectrograms can be\nconsidered as surface level manifestations of nonlinear transformations\nover individual multivariate sources of information like speaker characteristics,\nphonological content etc. Under assumptions of energy based models\nwe use the theory of nonlinear  isa to propose an algorithm that learns\nunsupervised speech representations whose subspaces are independent\nand potentially highly correlated with the original non-stationary\nmultivariate sources. We show how nonlinear  ica with auxiliary variables\ncan be extended to a generic identifiable model for subspaces as well\nwhile also providing sufficient conditions for the identifiability\nof these high dimensional subspaces. Our proposed methodology is generic\nand can be integrated with standard unsupervised approaches to learn\nspeech representations with subspaces that can theoretically capture\nindependent higher order speech signals. We evaluate the gains of our\nalgorithm when integrated with the Autoregressive Predictive Coding\n( apc) model by showing empirical results on the speaker verification\nand phoneme recognition tasks.\n"
      ],
      "doi": "10.21437/Interspeech.2020-3050",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "takeuchi20_interspeech": {
      "authors": [
        [
          "Hirotoshi",
          "Takeuchi"
        ],
        [
          "Kunio",
          "Kashino"
        ],
        [
          "Yasunori",
          "Ohishi"
        ],
        [
          "Hiroshi",
          "Saruwatari"
        ]
      ],
      "title": "Harmonic Lowering for Accelerating Harmonic Convolution for Audio Signals",
      "original": "3185",
      "page_count": 5,
      "order": 39,
      "p1": "185",
      "pn": "189",
      "abstract": [
        "Convolutional neural networks have been successfully applied to a variety\nof audio signal processing tasks including sound source separation,\nspeech recognition and acoustic scene understanding. Since many pitched\nsounds have a harmonic structure, an operation, called harmonic convolution,\nhas been proposed to take advantages of the structure appearing in\nthe audio signals. However, the computational cost involved is higher\nthan that of normal convolution. This paper proposes a faster calculation\nmethod of harmonic convolution called Harmonic Lowering. The method\nunrolls the input data to a redundant layout so that the normal convolution\noperation can be applied. The analysis of the runtimes and the number\nof multiplication operations show that the proposed method accelerates\nthe harmonic convolution 2 to 7 times faster than the conventional\nmethod under realistic parameter settings, while no approximation is\nintroduced.\n"
      ],
      "doi": "10.21437/Interspeech.2020-3185",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "ai20_interspeech": {
      "authors": [
        [
          "Yang",
          "Ai"
        ],
        [
          "Zhen-Hua",
          "Ling"
        ]
      ],
      "title": "Knowledge-and-Data-Driven Amplitude Spectrum Prediction for Hierarchical Neural Vocoders",
      "original": "1046",
      "page_count": 5,
      "order": 40,
      "p1": "190",
      "pn": "194",
      "abstract": [
        "In our previous work, we have proposed a neural vocoder called HiNet\nwhich recovers speech waveforms by predicting amplitude and phase spectra\nhierarchically from input acoustic features. In HiNet, the amplitude\nspectrum predictor (ASP) predicts log amplitude spectra (LAS) from\ninput acoustic features. This paper proposes a novel knowledge-and-data-driven\nASP (KDD-ASP) to improve the conventional one. First, acoustic features\n(i.e., F0 and mel-cepstra) pass through a knowledge-driven LAS recovery\nmodule to obtain approximate LAS (ALAS). This module is designed based\non the combination of STFT and source-filter theory, in which the source\npart and the filter part are designed based on input F0 and mel-cepstra,\nrespectively. Then, the recovered ALAS are processed by a data-driven\nLAS refinement module which consists of multiple trainable convolutional\nlayers to get the final LAS. Experimental results show that the HiNet\nvocoder using KDD-ASP can achieve higher quality of synthetic speech\nthan that using conventional ASP and the WaveRNN vocoder on a text-to-speech\n(TTS) task.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1046",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "tian20_interspeech": {
      "authors": [
        [
          "Qiao",
          "Tian"
        ],
        [
          "Zewang",
          "Zhang"
        ],
        [
          "Heng",
          "Lu"
        ],
        [
          "Ling-Hui",
          "Chen"
        ],
        [
          "Shan",
          "Liu"
        ]
      ],
      "title": "FeatherWave: An Efficient High-Fidelity Neural Vocoder with Multi-Band Linear Prediction",
      "original": "1156",
      "page_count": 5,
      "order": 41,
      "p1": "195",
      "pn": "199",
      "abstract": [
        "In this paper, we propose the FeatherWave, yet another variant of WaveRNN\nvocoder combining the multi-band signal processing and the linear predictive\ncoding. The LPCNet, a recently proposed neural vocoder which utilized\nthe linear predictive characteristic of speech signal in the WaveRNN\narchitecture, can generate high quality speech with a speed faster\nthan real-time on a single CPU core. However, LPCNet is still not efficient\nenough for online speech generation tasks. To address this issue, we\nadopt the multi-band linear predictive coding for WaveRNN vocoder.\nThe multi-band method enables the model to generate several speech\nsamples in parallel at one step. Therefore, it can significantly improve\nthe efficiency of speech synthesis. The proposed model with 4 sub-bands\nneeds less than 1.6 GFLOPS for speech generation. In our experiments,\nit can generate 24 kHz high-fidelity audio 9&#215; faster than real-time\non a single CPU, which is much faster than the LPCNet vocoder. Furthermore,\nour subjective listening test shows that the FeatherWave can generate\nspeech with better quality than LPCNet.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1156",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "yang20_interspeech": {
      "authors": [
        [
          "Jinhyeok",
          "Yang"
        ],
        [
          "Junmo",
          "Lee"
        ],
        [
          "Youngik",
          "Kim"
        ],
        [
          "Hoon-Young",
          "Cho"
        ],
        [
          "Injung",
          "Kim"
        ]
      ],
      "title": "VocGAN: A High-Fidelity Real-Time Vocoder with a Hierarchically-Nested Adversarial Network",
      "original": "1238",
      "page_count": 5,
      "order": 42,
      "p1": "200",
      "pn": "204",
      "abstract": [
        "We present a novel high-fidelity real-time neural vocoder called VocGAN.\nA recently developed GAN-based vocoder, MelGAN, produces speech waveforms\nin real-time. However, it often produces a waveform that is insufficient\nin quality or inconsistent with acoustic characteristics of the input\nmel spectrogram. VocGAN is nearly as fast as MelGAN, but it significantly\nimproves the quality and consistency of the output waveform. VocGAN\napplies a multi-scale waveform generator and a hierarchically-nested\ndiscriminator to learn multiple levels of acoustic properties in a\nbalanced way. It also applies the joint conditional and unconditional\nobjective, which has shown successful results in high-resolution image\nsynthesis. In experiments, VocGAN synthesizes speech waveforms 416.7&#215;\nfaster on a GTX 1080Ti GPU and 3.24&#215; faster on a CPU than real-time.\nCompared with MelGAN, it also exhibits significantly improved quality\nin multiple evaluation metrics including mean opinion score (MOS) with\nminimal additional overhead. Additionally, compared with Parallel WaveGAN,\nanother recently developed high-fidelity vocoder, VocGAN is 6.98&#215;\nfaster on a CPU and exhibits higher MOS.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1238",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "kanagawa20_interspeech": {
      "authors": [
        [
          "Hiroki",
          "Kanagawa"
        ],
        [
          "Yusuke",
          "Ijima"
        ]
      ],
      "title": "Lightweight LPCNet-Based Neural Vocoder with Tensor Decomposition",
      "original": "1642",
      "page_count": 5,
      "order": 43,
      "p1": "205",
      "pn": "209",
      "abstract": [
        "This paper proposes a lightweight neural vocoder based on LPCNet. The\nrecently proposed LPCNet exploits linear predictive coding to represent\nvocal tract characteristics, and can rapidly synthesize high-quality\nwaveforms with fewer parameters than WaveRNN. For even greater speeds,\nit is necessary to reduce the time-heavy two GRUs and the DualFC. Although\nthe original work only pruned the first GRU weight, there is room for\nimprovements in the other GRU and DualFC. Accordingly, we use tensor\ndecomposition to reduce these remaining parameters by more than 80%.\nFor the proposed method we demonstrate that 1) it is 1.26 times faster\non a CPU, and 2) it matched naturalness of the original LPCNet for\nacoustic features extracted from natural speech and for those predicted\nby TTS.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1642",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "hsu20_interspeech": {
      "authors": [
        [
          "Po-chun",
          "Hsu"
        ],
        [
          "Hung-yi",
          "Lee"
        ]
      ],
      "title": "WG-WaveNet: Real-Time High-Fidelity Speech Synthesis Without GPU",
      "original": "1736",
      "page_count": 5,
      "order": 44,
      "p1": "210",
      "pn": "214",
      "abstract": [
        "In this paper, we propose WG-WaveNet, a fast, lightweight, and high-quality\nwaveform generation model. WG-WaveNet is composed of a compact flow-based\nmodel and a post-filter. The two components are jointly trained by\nmaximizing the likelihood of the training data and optimizing loss\nfunctions on the frequency domains. As we design a flow-based model\nthat is heavily compressed, the proposed model requires much less computational\nresources compared to other waveform generation models during both\ntraining and inference time; even though the model is highly compressed,\nthe post-filter maintains the quality of generated waveform. Our PyTorch\nimplementation can be trained using less than 8 GB GPU memory and generates\naudio samples at a rate of more than 960 kHz on an NVIDIA 1080Ti GPU.\nFurthermore, even if synthesizing on a CPU, we show that the proposed\nmethod is capable of generating 44.1 kHz speech waveform 1.2 times\nfaster than real-time. Experiments also show that the quality of generated\naudio is comparable to those of other methods. Audio samples are publicly\navailable online.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1736",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "stephenson20_interspeech": {
      "authors": [
        [
          "Brooke",
          "Stephenson"
        ],
        [
          "Laurent",
          "Besacier"
        ],
        [
          "Laurent",
          "Girin"
        ],
        [
          "Thomas",
          "Hueber"
        ]
      ],
      "title": "What the Future Brings: Investigating the Impact of Lookahead for Incremental Neural TTS",
      "original": "2103",
      "page_count": 5,
      "order": 45,
      "p1": "215",
      "pn": "219",
      "abstract": [
        "In incremental text to speech synthesis (iTTS), the synthesizer produces\nan audio output before it has access to the entire input sentence.\nIn this paper, we study the behavior of a neural sequence-to-sequence\nTTS system when used in an incremental mode, i.e. when generating speech\noutput for token n, the system has access to  n+k tokens from the text\nsequence. We first analyze the impact of this incremental policy on\nthe evolution of the encoder representations of token n for different\nvalues of k (the lookahead parameter). The results show that, on average,\ntokens travel 88% of the way to their full context representation with\na one-word lookahead and 94% after 2 words. We then investigate which\ntext features are the most influential on the evolution towards the\nfinal representation using a random forest analysis. The results show\nthat the most salient factors are related to token length. We finally\nevaluate the effects of lookahead k at the decoder level, using a MUSHRA\nlistening test. This test shows results that contrast with the above\nhigh figures: speech synthesis quality obtained with 2 word-lookahead\nis significantly lower than the one obtained with the full sentence.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2103",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "popov20_interspeech": {
      "authors": [
        [
          "Vadim",
          "Popov"
        ],
        [
          "Stanislav",
          "Kamenev"
        ],
        [
          "Mikhail",
          "Kudinov"
        ],
        [
          "Sergey",
          "Repyevsky"
        ],
        [
          "Tasnima",
          "Sadekova"
        ],
        [
          "Vitalii",
          "Bushaev"
        ],
        [
          "Vladimir",
          "Kryzhanovskiy"
        ],
        [
          "Denis",
          "Parkhomenko"
        ]
      ],
      "title": "Fast and Lightweight On-Device TTS with Tacotron2 and LPCNet",
      "original": "2169",
      "page_count": 5,
      "order": 46,
      "p1": "220",
      "pn": "224",
      "abstract": [
        "We present a fast and lightweight on-device text-to-speech system based\non state-of-art methods of feature and speech generation i.e. Tacotron2\nand LPCNet. We show that modification of the basic pipeline combined\nwith hardware-specific optimizations and extensive usage of parallelization\nenables running TTS service even on low-end devices with faster than\nrealtime waveform generation. Moreover, the system preserves high quality\nof speech without noticeable degradation of Mean Opinion Score compared\nto the non-optimized baseline. While the system is mostly oriented\non low-to-mid range hardware we believe that it can also be used in\nany CPU-based environment.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2169",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "song20_interspeech": {
      "authors": [
        [
          "Wei",
          "Song"
        ],
        [
          "Guanghui",
          "Xu"
        ],
        [
          "Zhengchen",
          "Zhang"
        ],
        [
          "Chao",
          "Zhang"
        ],
        [
          "Xiaodong",
          "He"
        ],
        [
          "Bowen",
          "Zhou"
        ]
      ],
      "title": "Efficient WaveGlow: An Improved WaveGlow Vocoder with Enhanced Speed",
      "original": "2172",
      "page_count": 5,
      "order": 47,
      "p1": "225",
      "pn": "229",
      "abstract": [
        "Neural vocoder, such as WaveGlow, has become an important component\nin recent high-quality text-to-speech (TTS) systems. In this paper,\nwe propose Efficient WaveGlow (EWG), a flow-based generative model\nserving as an efficient neural vocoder. Similar to WaveGlow, EWG has\na normalizing flow backbone where each flow step consists of an affine\ncoupling layer and an invertible 1&#215;1 convolution. To reduce the\nnumber of model parameters and enhance the speed without sacrificing\nthe quality of the synthesized speech, EWG improves WaveGlow in three\naspects. First, the WaveNet-style transform network in WaveGlow is\nreplaced with an FFTNet-style dilated convolution network. Next, to\nreduce the computation cost, group convolution is applied to both audio\nand local condition features. At last, the local condition is shared\namong the transform network layers in each coupling layer. As a result,\nEWG can reduce the number of floating-point operations (FLOPs) required\nto generate one-second audio and the number of model parameters both\nby more than 12 times. Experimental results show that EWG can reduce\nreal-world inference time cost by more than twice, without any obvious\nreduction in the speech quality.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2172",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "maguer20_interspeech": {
      "authors": [
        [
          "S\u00e9bastien Le",
          "Maguer"
        ],
        [
          "Naomi",
          "Harte"
        ]
      ],
      "title": "Can Auditory Nerve Models Tell us What&#8217;s Different About WaveNet Vocoded Speech?",
      "original": "2596",
      "page_count": 5,
      "order": 48,
      "p1": "230",
      "pn": "234",
      "abstract": [
        "Nowadays, synthetic speech is almost indistinguishable from human speech.\nThe remarkable quality is mainly due to the displacing of signal processing\nbased vocoders in favour of neural vocoders and, in particular, the\nWaveNet architecture. At the same time, speech synthesis evaluation\nis still facing difficulties in adjusting to these improvements. These\ndifficulties are even more prevalent in the case of objective evaluation\nmethodologies which do not correlate well with human perception. Yet,\nan often forgotten use of objective evaluation is to uncover prominent\ndifferences between speech signals. Such differences are crucial to\ndecipher the improvement introduced by the use of WaveNet. Therefore,\nabandoning objective evaluation could be a serious mistake. In this\npaper, we analyze vocoded synthetic speech re-rendered using WaveNet,\ncomparing it to standard vocoded speech. To do so, we objectively compare\nspectrograms and neurograms, the latter being the output of AN models.\nThe spectrograms allow us to look at the speech production side, and\nthe neurograms relate to the speech perception path. While we were\nnot yet able to pinpoint how WaveNet and WORLD differ, our results\nsuggest that the Mean-Rate (MR) neurograms in particular warrant further\ninvestigation.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2596",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "paul20_interspeech": {
      "authors": [
        [
          "Dipjyoti",
          "Paul"
        ],
        [
          "Yannis",
          "Pantazis"
        ],
        [
          "Yannis",
          "Stylianou"
        ]
      ],
      "title": "Speaker Conditional WaveRNN: Towards Universal Neural Vocoder for Unseen Speaker and Recording Conditions",
      "original": "2786",
      "page_count": 5,
      "order": 49,
      "p1": "235",
      "pn": "239",
      "abstract": [
        "Recent advancements in deep learning led to human-level performance\nin single-speaker speech synthesis. However, there are still limitations\nin terms of speech quality when generalizing those systems into multiple-speaker\nmodels especially for unseen speakers and unseen recording qualities.\nFor instance, conventional neural vocoders are adjusted to the training\nspeaker and have poor generalization capabilities to unseen speakers.\nIn this work, we propose a variant of WaveRNN, referred to as speaker\nconditional WaveRNN (SC-WaveRNN). We target towards the development\nof an efficient universal vocoder even for unseen speakers and recording\nconditions. In contrast to standard WaveRNN, SC-WaveRNN exploits additional\ninformation given in the form of speaker embeddings. Using publicly-available\ndata for training, SC-WaveRNN achieves significantly better performance\nover baseline WaveRNN on both subjective and objective metrics. In\nMOS, SC-WaveRNN achieves an improvement of about 23% for seen speaker\nand seen recording condition and up to 95% for unseen speaker and unseen\ncondition. Finally, we extend our work by implementing a multi-speaker\ntext-to-speech (TTS) synthesis similar to zero-shot speaker adaptation.\nIn terms of performance, our system has been preferred over the baseline\nTTS system by 60% over 15.5% and by 60.9% over 32.6%, for seen and\nunseen speakers, respectively.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2786",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "liu20_interspeech": {
      "authors": [
        [
          "Zhijun",
          "Liu"
        ],
        [
          "Kuan",
          "Chen"
        ],
        [
          "Kai",
          "Yu"
        ]
      ],
      "title": "Neural Homomorphic Vocoder",
      "original": "3188",
      "page_count": 5,
      "order": 50,
      "p1": "240",
      "pn": "244",
      "abstract": [
        "In this paper, we propose the neural homomorphic vocoder (NHV), a source-filter\nmodel based neural vocoder framework. NHV synthesizes speech by filtering\nimpulse trains and noise with linear time-varying (LTV) filters. A\nneural network controls the LTV filters by estimating complex cepstrums\nof time-varying impulse responses given acoustic features. The proposed\nframework can be trained with a combination of multi-resolution STFT\nloss and adversarial loss functions. Due to the use of DSP-based synthesis\nmethods, NHV is highly efficient, fully controllable and interpretable.\nA vocoder was built under the framework to synthesize speech given\nlog-Mel spectrograms and fundamental frequencies. While the model cost\nonly 15 kFLOPs per sample, the synthesis quality remained comparable\nto baseline neural vocoders in both copy-synthesis and text-to-speech.\n"
      ],
      "doi": "10.21437/Interspeech.2020-3188",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "gretter20_interspeech": {
      "authors": [
        [
          "Roberto",
          "Gretter"
        ],
        [
          "Marco",
          "Matassoni"
        ],
        [
          "Daniele",
          "Falavigna"
        ],
        [
          "Keelan",
          "Evanini"
        ],
        [
          "Chee Wee",
          "Leong"
        ]
      ],
      "title": "Overview of the Interspeech TLT2020 Shared Task on ASR for Non-Native Children&#8217;s Speech",
      "original": "2133",
      "page_count": 5,
      "order": 51,
      "p1": "245",
      "pn": "249",
      "abstract": [
        "We present an overview of the ASR challenge for non-native children&#8217;s\nspeech organized for a special session at Interspeech 2020. The data\nfor the challenge was obtained in the context of a spoken language\nproficiency assessment administered at Italian schools for students\nbetween the ages of 9 and 16 who were studying English and German as\na foreign language. The corpus distributed for the challenge was a\nsubset of the English recordings. Participating teams competed either\nin a closed track, in which they could use only the training data released\nby the organizers of the challenge, or in an open track, in which they\nwere allowed to use additional training data. The closed track received\n9 entries and the open track received 7 entries, with the best scoring\nsystems achieving substantial improvements over a state-of-the-art\nbaseline system. This paper describes the corpus of non-native children&#8217;s\nspeech that was used for the challenge, analyzes the results, and discusses\nsome points that should be considered for subsequent challenges in\nthis domain in the future.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2133",
      "author_area_id": "13",
      "author_area_label": "Special Sessions"
    },
    "lo20_interspeech": {
      "authors": [
        [
          "Tien-Hong",
          "Lo"
        ],
        [
          "Fu-An",
          "Chao"
        ],
        [
          "Shi-Yan",
          "Weng"
        ],
        [
          "Berlin",
          "Chen"
        ]
      ],
      "title": "The NTNU System at the Interspeech 2020 Non-Native Children&#8217;s Speech ASR Challenge",
      "original": "1990",
      "page_count": 5,
      "order": 52,
      "p1": "250",
      "pn": "254",
      "abstract": [
        "This paper describes the NTNU ASR system participating in the Interspeech\n2020 Non-Native Children&#8217;s Speech ASR Challenge supported by\nthe SIG-CHILD group of ISCA. This ASR shared task is made much more\nchallenging due to the coexisting diversity of non-native and children\nspeaking characteristics. In the setting of closed-track evaluation,\nall participants were restricted to develop their systems merely based\non the speech and text corpora provided by the organizer. To work around\nthis under-resourced issue, we built our ASR system on top of CNN-TDNNF-based\nacoustic models, meanwhile harnessing the synergistic power of various\ndata augmentation strategies, including both utterance- and word-level\nspeed perturbation and spectrogram augmentation, alongside a simple\nyet effective data-cleansing approach. All variants of our ASR system\nemployed an RNN-based language model to rescore the first-pass recognition\nhypotheses, which was trained solely on the text dataset released by\nthe organizer. Our system with the best configuration came out in second\nplace, resulting in a word error rate (WER) of 17.59%, while those\nof the top-performing, second runner-up and official baseline systems\nare 15.67%, 18.71%, 35.09%, respectively.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1990",
      "author_area_id": "13",
      "author_area_label": "Special Sessions"
    },
    "knill20_interspeech": {
      "authors": [
        [
          "Kate M.",
          "Knill"
        ],
        [
          "Linlin",
          "Wang"
        ],
        [
          "Yu",
          "Wang"
        ],
        [
          "Xixin",
          "Wu"
        ],
        [
          "Mark J.F.",
          "Gales"
        ]
      ],
      "title": "Non-Native Children&#8217;s Automatic Speech Recognition: The INTERSPEECH 2020 Shared Task ALTA Systems",
      "original": "2154",
      "page_count": 5,
      "order": 53,
      "p1": "255",
      "pn": "259",
      "abstract": [
        "Automatic spoken language assessment (SLA) is a challenging problem\ndue to the large variations in learner speech combined with limited\nresources. These issues are even more problematic when considering\nchildren learning a language, with higher levels of acoustic and lexical\nvariability, and of code-switching compared to adult data. This paper\ndescribes the ALTA system for the INTERSPEECH 2020 Shared Task on Automatic\nSpeech Recognition for Non-Native Children&#8217;s Speech. The data\nfor this task consists of examination recordings of Italian school\nchildren aged 9&#8211;16, ranging in ability from minimal, to basic,\nto limited but effective command of spoken English. A variety of systems\nwere developed using the limited training data available, 49 hours.\nState-of-the-art acoustic models and language models were evaluated,\nincluding a diversity of lexical representations, handling code-switching\nand learner pronunciation errors, and grade specific models. The best\nsingle system achieved a word error rate (WER) of 16.9% on the evaluation\ndata. By combining multiple diverse systems, including both grade independent\nand grade specific models, the error rate was reduced to 15.7%. This\ncombined system was the best performing submission for both the closed\nand open tasks.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2154",
      "author_area_id": "13",
      "author_area_label": "Special Sessions"
    },
    "kathania20_interspeech": {
      "authors": [
        [
          "Hemant",
          "Kathania"
        ],
        [
          "Mittul",
          "Singh"
        ],
        [
          "Tam\u00e1s",
          "Gr\u00f3sz"
        ],
        [
          "Mikko",
          "Kurimo"
        ]
      ],
      "title": "Data Augmentation Using Prosody and False Starts to Recognize Non-Native Children&#8217;s Speech",
      "original": "2199",
      "page_count": 5,
      "order": 54,
      "p1": "260",
      "pn": "264",
      "abstract": [
        "This paper describes AaltoASR&#8217;s speech recognition system for\nthe INTERSPEECH 2020 shared task on Automatic Speech Recognition (ASR)\nfor non-native children&#8217;s speech. The task is to recognize non-native\nspeech from children of various age groups given a limited amount of\nspeech. Moreover, the speech being spontaneous has false starts transcribed\nas partial words, which in the test transcriptions leads to unseen\npartial words. To cope with these two challenges, we investigate a\ndata augmentation-based approach. Firstly, we apply the prosody-based\ndata augmentation to supplement the audio data. Secondly, we simulate\nfalse starts by introducing partial-word noise in the language modeling\ncorpora creating new words. Acoustic models trained on prosody-based\naugmented data outperform the models using the baseline recipe or the\nSpecAugment-based augmentation. The partial-word noise also helps to\nimprove the baseline language model. Our ASR system, a combination\nof these schemes, is placed third in the evaluation period and achieves\nthe word error rate of 18.71%. Post-evaluation period, we observe that\nincreasing the amounts of prosody-based augmented data leads to better\nperformance. Furthermore, removing low-confidence-score words from\nhypotheses can lead to further gains. These two improvements lower\nthe ASR error rate to 17.99%.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2199",
      "author_area_id": "13",
      "author_area_label": "Special Sessions"
    },
    "shahin20_interspeech": {
      "authors": [
        [
          "Mostafa",
          "Shahin"
        ],
        [
          "Ren\u00e9e",
          "Lu"
        ],
        [
          "Julien",
          "Epps"
        ],
        [
          "Beena",
          "Ahmed"
        ]
      ],
      "title": "UNSW System Description for the Shared Task on Automatic Speech Recognition for Non-Native Children&#8217;s Speech",
      "original": "3111",
      "page_count": 4,
      "order": 55,
      "p1": "265",
      "pn": "268",
      "abstract": [
        "In this paper we describe our children&#8217;s Automatic Speech Recognition\n(ASR) system for the first shared task on ASR for English non-native\nchildren&#8217;s speech. The acoustic model comprises 6 Convolutional\nNeural Network (CNN) layers and 12 Factored Time-Delay Neural Network\n(TDNN-F) layers, trained by data from 5 different children&#8217;s\nspeech corpora. Speed perturbation, Room Impulse Response (RIR), babble\nnoise and non-speech noise data augmentation methods were utilized\nto enhance the model robustness. Three Language Models (LMs) were employed:\nan in-domain LM trained on written data and speech transcriptions of\nnon-native children, a LM trained on non-native written data and transcription\nof both native and non-native children&#8217;s speech and a TEDLIUM\nLM trained on adult TED talks transcriptions. Lattices produced from\nthe different ASR systems were combined and decoded using the Minimum\nBayes-Risk (MBR) decoding algorithm to get the final output. Our system\nachieved a final Word Error Rate (WER) of 17.55% and 16.59% for both\ndeveloping and testing sets respectively and ranked second among the\n10 teams participating in the task.\n"
      ],
      "doi": "10.21437/Interspeech.2020-3111",
      "author_area_id": "13",
      "author_area_label": "Special Sessions"
    },
    "horiguchi20_interspeech": {
      "authors": [
        [
          "Shota",
          "Horiguchi"
        ],
        [
          "Yusuke",
          "Fujita"
        ],
        [
          "Shinji",
          "Watanabe"
        ],
        [
          "Yawen",
          "Xue"
        ],
        [
          "Kenji",
          "Nagamatsu"
        ]
      ],
      "title": "End-to-End Speaker Diarization for an Unknown Number of Speakers with Encoder-Decoder Based Attractors",
      "original": "1022",
      "page_count": 5,
      "order": 56,
      "p1": "269",
      "pn": "273",
      "abstract": [
        "End-to-end speaker diarization for an unknown number of speakers is\naddressed in this paper. Recently proposed end-to-end speaker diarization\noutperformed conventional clustering-based speaker diarization, but\nit has one drawback: it is less flexible in terms of the number of\nspeakers. This paper proposes a method for encoder-decoder based attractor\ncalculation (EDA), which first generates a flexible number of attractors\nfrom a speech embedding sequence. Then, the generated multiple attractors\nare multiplied by the speech embedding sequence to produce the same\nnumber of speaker activities. The speech embedding sequence is extracted\nusing the conventional self-attentive end-to-end neural speaker diarization\n(SA-EEND) network. In a two-speaker condition, our method achieved\na 2.69% diarization error rate (DER) on simulated mixtures and a 8.07%\nDER on the two-speaker subset of CALLHOME, while vanilla SA-EEND attained\n4.56% and 9.54%, respectively. In unknown numbers of speakers conditions,\nour method attained a 15.29% DER on CALLHOME, while the x-vector-based\nclustering method achieved a 19.43% DER.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1022",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "medennikov20_interspeech": {
      "authors": [
        [
          "Ivan",
          "Medennikov"
        ],
        [
          "Maxim",
          "Korenevsky"
        ],
        [
          "Tatiana",
          "Prisyach"
        ],
        [
          "Yuri",
          "Khokhlov"
        ],
        [
          "Mariya",
          "Korenevskaya"
        ],
        [
          "Ivan",
          "Sorokin"
        ],
        [
          "Tatiana",
          "Timofeeva"
        ],
        [
          "Anton",
          "Mitrofanov"
        ],
        [
          "Andrei",
          "Andrusenko"
        ],
        [
          "Ivan",
          "Podluzhny"
        ],
        [
          "Aleksandr",
          "Laptev"
        ],
        [
          "Aleksei",
          "Romanenko"
        ]
      ],
      "title": "Target-Speaker Voice Activity Detection: A Novel Approach for Multi-Speaker Diarization in a Dinner Party Scenario",
      "original": "1602",
      "page_count": 5,
      "order": 57,
      "p1": "274",
      "pn": "278",
      "abstract": [
        "Speaker diarization for real-life scenarios is an extremely challenging\nproblem. Widely used clustering-based diarization approaches perform\nrather poorly in such conditions, mainly due to the limited ability\nto handle overlapping speech. We propose a novel Target-Speaker Voice\nActivity Detection (TS-VAD) approach, which directly predicts an activity\nof each speaker on each time frame. TS-VAD model takes conventional\nspeech features (e.g., MFCC) along with i-vectors for each speaker\nas inputs. A set of binary classification output layers produces activities\nof each speaker. I-vectors can be estimated iteratively, starting with\na strong clustering-based diarization.<br style=\"mso-data-placement:same-cell;\"\n/><br style=\"mso-data-placement:same-cell;\" />\n  We also extend the\nTS-VAD approach to the multi-microphone case using a simple attention\nmechanism on top of hidden representations extracted from the single-channel\nTS-VAD model. Moreover, post-processing strategies for the predicted\nspeaker activity probabilities are investigated. Experiments on the\nCHiME-6 unsegmented data show that TS-VAD achieves state-of-the-art\nresults outperforming the baseline x-vector-based system by more than\n30% Diarization Error Rate (DER) abs.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1602",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "aronowitz20_interspeech": {
      "authors": [
        [
          "Hagai",
          "Aronowitz"
        ],
        [
          "Weizhong",
          "Zhu"
        ],
        [
          "Masayuki",
          "Suzuki"
        ],
        [
          "Gakuto",
          "Kurata"
        ],
        [
          "Ron",
          "Hoory"
        ]
      ],
      "title": "New Advances in Speaker Diarization",
      "original": "1879",
      "page_count": 5,
      "order": 58,
      "p1": "279",
      "pn": "283",
      "abstract": [
        "Recently, speaker diarization based on speaker embeddings has shown\nexcellent results in many works. In this paper we propose several enhancements\nthroughout the diarization pipeline. This work addresses two clustering\nframeworks: agglomerative hierarchical clustering (AHC) and spectral\nclustering (SC).<br style=\"mso-data-placement:same-cell;\" /><br style=\"mso-data-placement:same-cell;\"\n/>\n  First, we use multiple speaker embeddings. We show that fusion\nof x-vectors and d-vectors boosts accuracy significantly. Second, we\ntrain neural networks to leverage both acoustic and duration information\nfor scoring similarity of segments or clusters. Third, we introduce\na novel method to guide the AHC clustering mechanism using a neural\nnetwork. Fourth, we handle short duration segments in SC by deemphasizing\ntheir effect on setting the number of speakers.<br style=\"mso-data-placement:same-cell;\"\n/><br style=\"mso-data-placement:same-cell;\" />\n  Finally, we propose\na novel method for estimating the number of clusters in the SC framework.\nThe method takes each eigenvalue and analyzes the projections of the\nSC similarity matrix on the corresponding eigenvector.<br style=\"mso-data-placement:same-cell;\"\n/><br style=\"mso-data-placement:same-cell;\" />\n  We evaluated our system\non NIST SRE 2000 CALLHOME and, using cross-validation, we achieved\nan error rate of 5.1%, going beyond state-of-the-art speaker diarization.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1879",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "lin20_interspeech": {
      "authors": [
        [
          "Qingjian",
          "Lin"
        ],
        [
          "Yu",
          "Hou"
        ],
        [
          "Ming",
          "Li"
        ]
      ],
      "title": "Self-Attentive Similarity Measurement Strategies in Speaker Diarization",
      "original": "1908",
      "page_count": 5,
      "order": 59,
      "p1": "284",
      "pn": "288",
      "abstract": [
        "Speaker diarization can be described as the process of extracting sequential\nspeaker embeddings from an audio stream and clustering them according\nto speaker identities. Nowadays, deep neural network based approaches\nlike x-vector have been widely adopted for speaker embedding extraction.\nHowever, in the clustering back-end, probabilistic linear discriminant\nanalysis (PLDA) is still the dominant algorithm for similarity measurement.\nPLDA works in a pair-wise and independent manner, which may ignore\nthe positional correlation of adjacent speaker embeddings. To address\nthis issue, our previous work proposed the long short-term memory (LSTM)\nbased scoring model, followed by the spectral clustering algorithm.\nIn this paper, we further propose two enhanced methods based on the\nself-attention mechanism, which no longer focuses on the local correlation\nbut searches for similar speaker embeddings in the whole sequence.\nThe first approach achieves state-of-the-art performance on the DIHARD\nII Eval Set (18.44% DER after resegmentation), while the second one\noperates with higher efficiency.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1908",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "wang20b_interspeech": {
      "authors": [
        [
          "Jixuan",
          "Wang"
        ],
        [
          "Xiong",
          "Xiao"
        ],
        [
          "Jian",
          "Wu"
        ],
        [
          "Ranjani",
          "Ramamurthy"
        ],
        [
          "Frank",
          "Rudzicz"
        ],
        [
          "Michael",
          "Brudno"
        ]
      ],
      "title": "Speaker Attribution with Voice Profiles by Graph-Based Semi-Supervised Learning",
      "original": "1950",
      "page_count": 5,
      "order": 60,
      "p1": "289",
      "pn": "293",
      "abstract": [
        "Speaker attribution is required in many real-world applications, such\nas meeting transcription, where speaker identity is assigned to each\nutterance according to speaker voice profiles. In this paper, we propose\nto solve the speaker attribution problem by using graph-based semi-supervised\nlearning methods. A graph of speech segments is built for each session,\non which segments from voice profiles are represented by labeled nodes\nwhile segments from test utterances are unlabeled nodes. The weight\nof edges between nodes is evaluated by the similarities between the\npretrained speaker embeddings of speech segments. Speaker attribution\nthen becomes a semi-supervised learning problem on graphs, on which\ntwo graph-based methods are applied: label propagation (LP) and graph\nneural networks (GNNs). The proposed approaches are able to utilize\nthe structural information of the graph to improve speaker attribution\nperformance. Experimental results on real meeting data show that the\ngraph based approaches reduce speaker attribution error by up to 68%\ncompared to a baseline speaker identification approach that processes\neach utterance independently.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1950",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "singh20_interspeech": {
      "authors": [
        [
          "Prachi",
          "Singh"
        ],
        [
          "Sriram",
          "Ganapathy"
        ]
      ],
      "title": "Deep Self-Supervised Hierarchical Clustering for Speaker Diarization",
      "original": "2297",
      "page_count": 5,
      "order": 61,
      "p1": "294",
      "pn": "298",
      "abstract": [
        "The state-of-the-art speaker diarization systems use agglomerative\nhierarchical clustering (AHC) which performs the clustering of previously\nlearned neural embeddings. While the clustering approach attempts to\nidentify speaker clusters, the AHC algorithm does not involve any further\nlearning. In this paper, we propose a novel algorithm for hierarchical\nclustering which combines the speaker clustering along with a representation\nlearning framework. The proposed approach is based on principles of\nself-supervised learning where the self-supervision is derived from\nthe clustering algorithm. The representation learning network is trained\nwith a regularized triplet loss using the clustering solution at the\ncurrent step while the clustering algorithm uses the deep embeddings\nfrom the representation learning step. By combining the self-supervision\nbased representation learning along with the clustering algorithm,\nwe show that the proposed algorithm improves significantly (29% relative\nimprovement) over the AHC algorithm with cosine similarity for a speaker\ndiarization task on CALLHOME dataset. In addition, the proposed approach\nalso improves over the state-of-the-art system with PLDA affinity matrix\nwith 10% relative improvement in DER.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2297",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "chung20_interspeech": {
      "authors": [
        [
          "Joon Son",
          "Chung"
        ],
        [
          "Jaesung",
          "Huh"
        ],
        [
          "Arsha",
          "Nagrani"
        ],
        [
          "Triantafyllos",
          "Afouras"
        ],
        [
          "Andrew",
          "Zisserman"
        ]
      ],
      "title": "Spot the Conversation: Speaker Diarisation in the Wild",
      "original": "2337",
      "page_count": 5,
      "order": 62,
      "p1": "299",
      "pn": "303",
      "abstract": [
        "The goal of this paper is speaker diarisation of videos collected &#8216;in\nthe wild&#8217;.<br style=\"mso-data-placement:same-cell;\" /><br style=\"mso-data-placement:same-cell;\"\n/>\n  We make three key contributions. First, we propose an automatic\naudio-visual diarisation method for YouTube videos. Our method consists\nof active speaker detection using audio-visual methods and speaker\nverification using self-enrolled speaker models. Second, we integrate\nour method into a semi-automatic dataset creation pipeline which significantly\nreduces the number of hours required to annotate videos with diarisation\nlabels. Finally, we use this pipeline to create a large-scale diarisation\ndataset called  VoxConverse, collected from &#8216;in the wild&#8217;\nvideos, which we will release publicly to the research community. Our\ndataset consists of overlapping speech, a large and diverse speaker\npool, and challenging background conditions.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2337",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "zhang20_interspeech": {
      "authors": [
        [
          "Wangyou",
          "Zhang"
        ],
        [
          "Yanmin",
          "Qian"
        ]
      ],
      "title": "Learning Contextual Language Embeddings for Monaural Multi-Talker Speech Recognition",
      "original": "2015",
      "page_count": 5,
      "order": 63,
      "p1": "304",
      "pn": "308",
      "abstract": [
        "End-to-end multi-speaker speech recognition has been a popular topic\nin recent years, as more and more researches focus on speech processing\nin more realistic scenarios. Inspired by the hearing mechanism of human\nbeings, which enables us to concentrate on the interested speaker from\nthe multi-speaker mixed speech by utilizing both audio and context\nknowledge, this paper explores the contextual information to improve\nthe multi-talker speech recognition. In the proposed architecture,\nthe novel embedding learning model is designed to accurately extract\nthe contextual embedding from the multi-talker mixed speech directly.\nThen two advanced training strategies are further proposed to improve\nthe new model. Experimental results show that our proposed method achieves\na very large improvement on multi-speaker speech recognition, with\n&#126;25% relative WER reduction against the baseline end-to-end multi-talker\nASR model.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2015",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "du20_interspeech": {
      "authors": [
        [
          "Zhihao",
          "Du"
        ],
        [
          "Jiqing",
          "Han"
        ],
        [
          "Xueliang",
          "Zhang"
        ]
      ],
      "title": "Double Adversarial Network Based Monaural Speech Enhancement for Robust Speech Recognition",
      "original": "1504",
      "page_count": 5,
      "order": 64,
      "p1": "309",
      "pn": "313",
      "abstract": [
        "To improve the noise robustness of automatic speech recognition (ASR),\nthe generative adversarial network (GAN) based enhancement methods\nare employed as the front-end processing, which comprise a single adversarial\nprocess of an enhancement model and a discriminator. In this single\nadversarial process, the discriminator is encouraged to find differences\nbetween the enhanced and clean speeches, but the distribution of clean\nspeeches is ignored. In this paper, we propose a double adversarial\nnetwork (DAN) by adding another adversarial generation process (AGP),\nwhich forces the discriminator not only to find the differences but\nalso to model the distribution. Furthermore, a functional mean square\nerror (f-MSE) is proposed to utilize the representations learned by\nthe discriminator. Experimental results reveal that AGP and f-MSE are\ncrucial for the enhancement performance on ASR task, which are missed\nin previous GAN-based methods. Specifically, our DAN achieves 13.00%\nrelative word error rate improvements over the noisy speeches on the\ntest set of CHiME-2, which outperforms several recent GAN-based enhancement\nmethods significantly.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1504",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "bruguier20_interspeech": {
      "authors": [
        [
          "Antoine",
          "Bruguier"
        ],
        [
          "Ananya",
          "Misra"
        ],
        [
          "Arun",
          "Narayanan"
        ],
        [
          "Rohit",
          "Prabhavalkar"
        ]
      ],
      "title": "Anti-Aliasing Regularization in Stacking Layers",
      "original": "1497",
      "page_count": 5,
      "order": 65,
      "p1": "314",
      "pn": "318",
      "abstract": [
        "Shift-invariance is a desirable property of many machine learning models.\nIt means that delaying the input of a model in time should only result\nin delaying its prediction in time. A model that is shift-invariant,\nalso eliminates undesirable side effects like frequency aliasing. When\nbuilding sequence models, not only should the shift-invariance property\nbe preserved when sampling input features, it must also be respected\ninside the model itself. Here, we study the impact of the commonly\nused stacking layer in LSTM-based ASR models and show that aliasing\nis likely to occur. Experimentally, by adding merely 7 parameters to\nan existing speech recognition model that has 120 million parameters,\nwe are able to reduce the impact of aliasing. This acts as a regularizer\nthat discards frequencies the model shouldn&#8217;t be relying on for\npredictions. Our results show that under conditions unseen at training,\nwe are able to reduce the relative word error rate by up to 5%.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1497",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "andrusenko20_interspeech": {
      "authors": [
        [
          "Andrei",
          "Andrusenko"
        ],
        [
          "Aleksandr",
          "Laptev"
        ],
        [
          "Ivan",
          "Medennikov"
        ]
      ],
      "title": "Towards a Competitive End-to-End Speech Recognition for CHiME-6 Dinner Party Transcription",
      "original": "1074",
      "page_count": 5,
      "order": 66,
      "p1": "319",
      "pn": "323",
      "abstract": [
        "While end-to-end ASR systems have proven competitive with the conventional\nhybrid approach, they are prone to accuracy degradation when it comes\nto noisy and low-resource conditions. In this paper, we argue that,\neven in such difficult cases, some end-to-end approaches show performance\nclose to the hybrid baseline. To demonstrate this, we use the CHiME-6\nChallenge data as an example of challenging environments and noisy\nconditions of everyday speech. We experimentally compare and analyze\nCTC-Attention versus RNN-Transducer approaches along with RNN versus\nTransformer architectures. We also provide a comparison of acoustic\nfeatures and speech enhancements. Besides, we evaluate the effectiveness\nof neural network language models for hypothesis re-scoring in low-resource\nconditions. Our best end-to-end model based on RNN-Transducer, together\nwith improved beam search, reaches quality by only 3.8% WER abs. worse\nthan the LF-MMI TDNN-F CHiME-6 Challenge baseline. With the Guided\nSource Separation based training data augmentation, this approach outperforms\nthe hybrid baseline system by 2.7% WER abs. and the end-to-end system\nbest known before by 25.7% WER abs.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1074",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "zhang20b_interspeech": {
      "authors": [
        [
          "Wangyou",
          "Zhang"
        ],
        [
          "Aswin Shanmugam",
          "Subramanian"
        ],
        [
          "Xuankai",
          "Chang"
        ],
        [
          "Shinji",
          "Watanabe"
        ],
        [
          "Yanmin",
          "Qian"
        ]
      ],
      "title": "End-to-End Far-Field Speech Recognition with Unified Dereverberation and Beamforming",
      "original": "2432",
      "page_count": 5,
      "order": 67,
      "p1": "324",
      "pn": "328",
      "abstract": [
        "Despite successful applications of end-to-end approaches in multi-channel\nspeech recognition, the performance still degrades severely when the\nspeech is corrupted by reverberation. In this paper, we integrate the\ndereverberation module into the end-to-end multi-channel speech recognition\nsystem and explore two different frontend architectures. First, a multi-source\nmask-based weighted prediction error (WPE) module is incorporated in\nthe frontend for dereverberation. Second, another novel frontend architecture\nis proposed, which extends the weighted power minimization distortionless\nresponse (WPD) convolutional beamformer to perform simultaneous separation\nand dereverberation. We derive a new formulation from the original\nWPD, which can handle multi-source input, and replace eigenvalue decomposition\nwith the matrix inverse operation to make the back-propagation algorithm\nmore stable. The above two architectures are optimized in a fully end-to-end\nmanner, only using the speech recognition criterion. Experiments on\nboth spatialized wsj1-2mix corpus and REVERB show that our proposed\nmodel outperformed the conventional methods in reverberant scenarios.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2432"
    },
    "qiu20_interspeech": {
      "authors": [
        [
          "Xinchi",
          "Qiu"
        ],
        [
          "Titouan",
          "Parcollet"
        ],
        [
          "Mirco",
          "Ravanelli"
        ],
        [
          "Nicholas D.",
          "Lane"
        ],
        [
          "Mohamed",
          "Morchid"
        ]
      ],
      "title": "Quaternion Neural Networks for Multi-Channel Distant Speech Recognition",
      "original": "1682",
      "page_count": 5,
      "order": 68,
      "p1": "329",
      "pn": "333",
      "abstract": [
        "Despite the significant progress in automatic speech recognition (ASR),\ndistant ASR remains challenging due to noise and reverberation. A common\napproach to mitigate this issue consists of equipping the recording\ndevices with multiple microphones that capture the acoustic scene from\ndifferent perspectives. These multi-channel audio recordings contain\nspecific internal relations between each signal. In this paper, we\npropose to capture these inter- and intra- structural dependencies\nwith quaternion neural networks, which can jointly process multiple\nsignals as whole quaternion entities. The quaternion algebra replaces\nthe standard dot product with the Hamilton one, thus offering a simple\nand elegant way to model dependencies between elements. The quaternion\nlayers are then coupled with a recurrent neural network, which can\nlearn long-term dependencies in the time domain. We show that a quaternion\nlong-short term memory neural network (QLSTM), trained on the concatenated\nmulti-channel speech signals, outperforms equivalent real-valued LSTM\non two different tasks of multi-channel distant speech recognition.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1682",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "chen20_interspeech": {
      "authors": [
        [
          "Hangting",
          "Chen"
        ],
        [
          "Pengyuan",
          "Zhang"
        ],
        [
          "Qian",
          "Shi"
        ],
        [
          "Zuozhen",
          "Liu"
        ]
      ],
      "title": "Improved Guided Source Separation Integrated with a Strong Back-End for the CHiME-6 Dinner Party Scenario",
      "original": "1606",
      "page_count": 5,
      "order": 69,
      "p1": "334",
      "pn": "338",
      "abstract": [
        "The CHiME-6 dataset presents a difficult task with extreme speech overlap,\nsevere noise and a natural speaking style. The gap of the word error\nrate (WER) is distinct between the audios recorded by the distant microphone\narrays and the individual headset microphones. The official baseline\nexhibits a WER gap of approximately 10% even though the guided source\nseparation (GSS) has achieved considerable WER reduction. In the paper,\nwe make an effort to integrate an improved GSS with a strong automatic\nspeech recognition (ASR) back-end, which bridges the WER gap and achieves\nsubstantial ASR performance improvement. Specifically, the proposed\nGSS is initialized by masks from data-driven deep-learning models,\nutilizes the spectral information and conducts a selection of the input\nchannels. Meanwhile, we propose a data augmentation technique via random\nchannel selection and deep convolutional neural network-based multi-channel\nacoustic models for back-end modeling. In the experiments, our framework\nlargely reduced the WER to 34.78%/36.85% on the CHiME-6 development/evaluation\nset. Moreover, a narrower gap of 0.89%/4.67% was observed between the\ndistant and headset audios. This framework is also the foundation of\nthe IOA&#8217;s submission to the CHiME-6 competition, which is ranked\namong the top systems.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1606",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "wang20c_interspeech": {
      "authors": [
        [
          "Dongmei",
          "Wang"
        ],
        [
          "Zhuo",
          "Chen"
        ],
        [
          "Takuya",
          "Yoshioka"
        ]
      ],
      "title": "Neural Speech Separation Using Spatially Distributed Microphones",
      "original": "1089",
      "page_count": 5,
      "order": 70,
      "p1": "339",
      "pn": "343",
      "abstract": [
        "This paper proposes a neural network based speech separation method\nusing spatially distributed microphones. Unlike with traditional microphone\narray settings, neither the number of microphones nor their spatial\narrangement is known in advance, which hinders the use of conventional\nmulti-channel speech separation neural networks based on fixed size\ninput. To overcome this, a novel network architecture is proposed that\ninterleaves inter-channel processing layers and temporal processing\nlayers. The inter-channel processing layers apply a self-attention\nmechanism along the channel dimension to exploit the information obtained\nwith a varying number of microphones. The temporal processing layers\nare based on a bidirectional long short term memory (BLSTM) model and\napplied to each channel independently. The proposed network leverages\ninformation across time and space by stacking these two kinds of layers\nalternately. Our network estimates time-frequency (TF) masks for each\nspeaker, which are then used to generate enhanced speech signals either\nwith TF masking or beamforming. Speech recognition experimental results\nshow that the proposed method significantly outperforms baseline multi-channel\nspeech separation systems.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1089",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "horiguchi20b_interspeech": {
      "authors": [
        [
          "Shota",
          "Horiguchi"
        ],
        [
          "Yusuke",
          "Fujita"
        ],
        [
          "Kenji",
          "Nagamatsu"
        ]
      ],
      "title": "Utterance-Wise Meeting Transcription System Using Asynchronous Distributed Microphones",
      "original": "1050",
      "page_count": 5,
      "order": 71,
      "p1": "344",
      "pn": "348",
      "abstract": [
        "A novel framework for meeting transcription using asynchronous microphones\nis proposed in this paper. It consists of audio synchronization, speaker\ndiarization, utterance-wise speech enhancement using guided source\nseparation, automatic speech recognition, and duplication reduction.\nDoing speaker diarization before speech enhancement enables the system\nto deal with overlapped speech without considering sampling frequency\nmismatch between microphones. Evaluation on our real meeting datasets\nshowed that our framework achieved a character error rate (CER) of\n28.7% by using 11 distributed microphones, while a monaural microphone\nplaced on the center of the table had a CER of 38.2%. We also showed\nthat our framework achieved CER of 21.8%, which is only 2.1 percentage\npoints higher than the CER in headset microphone-based transcription.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1050",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "deadman20_interspeech": {
      "authors": [
        [
          "Jack",
          "Deadman"
        ],
        [
          "Jon",
          "Barker"
        ]
      ],
      "title": "Simulating Realistically-Spatialised Simultaneous Speech Using Video-Driven Speaker Detection and the CHiME-5 Dataset",
      "original": "2807",
      "page_count": 5,
      "order": 72,
      "p1": "349",
      "pn": "353",
      "abstract": [
        "Simulated data plays a crucial role in the development and evaluation\nof novel distant microphone ASR techniques. However, the commonly used\nsimulated datasets adopt uninformed and potentially unrealistic speaker\nlocation distributions. We wish to generate more realistic simulations\ndriven by recorded human behaviour. By using devices with a paired\nmicrophone array and camera, we analyse unscripted dinner party scenarios\n(CHiME-5) to estimate the distribution of speaker separation in a realistic\nsetting. We deploy face-detection, and pose-detection techniques on\n114 cameras to automatically locate speakers in 20 dinner party sessions.\nOur analysis found that on average, the separation between speakers\nwas only 17 degrees. We use this analysis to create datasets with realistic\ndistributions and compare it with commonly used datasets of simulated\nsignals. By changing the position of speakers, we show that the word\nerror rate can increase by over 73.5% relative when using a strong\nspeech enhancement and ASR system.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2807",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "botelho20_interspeech": {
      "authors": [
        [
          "Catarina",
          "Botelho"
        ],
        [
          "Lorenz",
          "Diener"
        ],
        [
          "Dennis",
          "K\u00fcster"
        ],
        [
          "Kevin",
          "Scheck"
        ],
        [
          "Shahin",
          "Amiriparian"
        ],
        [
          "Bj\u00f6rn W.",
          "Schuller"
        ],
        [
          "Tanja",
          "Schultz"
        ],
        [
          "Alberto",
          "Abad"
        ],
        [
          "Isabel",
          "Trancoso"
        ]
      ],
      "title": "Toward Silent Paralinguistics: Speech-to-EMG &#8212; Retrieving Articulatory Muscle Activity from Speech",
      "original": "2926",
      "page_count": 5,
      "order": 73,
      "p1": "354",
      "pn": "358",
      "abstract": [
        "Electromyographic (EMG) signals recorded during speech production encode\ninformation on articulatory muscle activity and also on the facial\nexpression of emotion, thus representing a speech-related biosignal\nwith strong potential for paralinguistic applications. In this work,\nwe estimate the electrical activity of the muscles responsible for\nspeech articulation directly from the speech signal. To this end, we\nfirst perform a neural conversion of speech features into electromyographic\ntime domain features, and then attempt to retrieve the original EMG\nsignal from the time domain features. We propose a feed forward neural\nnetwork to address the first step of the problem (speech features to\nEMG features) and a neural network composed of a convolutional block\nand a bidirectional long short-term memory block to address the second\nproblem (true EMG features to EMG signal). We observe that four out\nof the five originally proposed time domain features can be estimated\nreasonably well from the speech signal. Further, the five time domain\nfeatures are able to predict the original speech-related EMG signal\nwith a concordance correlation coefficient of 0.663. We further compare\nour results with the ones achieved on the inverse problem of generating\nacoustic speech features from EMG features.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2926",
      "author_area_id": "3",
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "zhang20c_interspeech": {
      "authors": [
        [
          "Jiaxuan",
          "Zhang"
        ],
        [
          "Sarah Ita",
          "Levitan"
        ],
        [
          "Julia",
          "Hirschberg"
        ]
      ],
      "title": "Multimodal Deception Detection Using Automatically Extracted Acoustic, Visual, and Lexical Features",
      "original": "2320",
      "page_count": 5,
      "order": 74,
      "p1": "359",
      "pn": "363",
      "abstract": [
        "Deception detection in conversational dialogue has attracted much attention\nin recent years. Yet existing methods for this rely heavily on human-labeled\nannotations that are costly and potentially inaccurate. In this work,\nwe present an automated system that utilizes multimodal features for\nconversational deception detection, without the use of human annotations.\nWe study the predictive power of different modalities and combine them\nfor better performance. We use openSMILE to extract acoustic features\nafter applying noise reduction techniques to the original audio. Facial\nlandmark features are extracted from the visual modality. We experiment\nwith training facial expression detectors and applying Fisher Vectors\nto encode sequences of facial landmarks with varying length. Linguistic\nfeatures are extracted from automatic transcriptions of the data. We\nexamine the performance of these methods on the Box of Lies dataset\nof deception game videos, achieving 73% accuracy using features from\nall modalities. This result is significantly better than previous results\non this corpus which relied on manual annotations, and also better\nthan human performance.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2320",
      "author_area_id": "3",
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "pan20b_interspeech": {
      "authors": [
        [
          "Zexu",
          "Pan"
        ],
        [
          "Zhaojie",
          "Luo"
        ],
        [
          "Jichen",
          "Yang"
        ],
        [
          "Haizhou",
          "Li"
        ]
      ],
      "title": "Multi-Modal Attention for Speech Emotion Recognition",
      "original": "1653",
      "page_count": 5,
      "order": 75,
      "p1": "364",
      "pn": "368",
      "abstract": [
        "Emotion represents an essential aspect of human speech that is manifested\nin speech prosody. Speech, visual, and textual cues are complementary\nin human communication. In this paper, we study a hybrid fusion method,\nreferred to as multi-modal attention network (MMAN) to makes use of\nvisual and textual cues in speech emotion recognition. We propose a\nnovel multi-modal attention mechanism, cLSTM-MMA, which facilitates\nthe attention across three modalities and selectively fuse the information.\ncLSTM-MMA is fused with other uni-modal sub-networks in the late fusion.\nThe experiments show that speech emotion recognition benefits significantly\nfrom visual and textual cues, and the proposed cLSTM-MMA alone is as\ncompetitive as other fusion methods in terms of accuracy, but with\na much more compact network structure. The proposed hybrid network\nMMAN achieves state-of-the-art performance on IEMOCAP database for\nemotion recognition.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1653",
      "author_area_id": "3",
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "shen20_interspeech": {
      "authors": [
        [
          "Guang",
          "Shen"
        ],
        [
          "Riwei",
          "Lai"
        ],
        [
          "Rui",
          "Chen"
        ],
        [
          "Yu",
          "Zhang"
        ],
        [
          "Kejia",
          "Zhang"
        ],
        [
          "Qilong",
          "Han"
        ],
        [
          "Hongtao",
          "Song"
        ]
      ],
      "title": "WISE: Word-Level Interaction-Based Multimodal Fusion for Speech Emotion Recognition",
      "original": "3131",
      "page_count": 5,
      "order": 76,
      "p1": "369",
      "pn": "373",
      "abstract": [
        "While having numerous real-world applications, speech emotion recognition\nis still a technically challenging problem. How to effectively leverage\nthe inherent multiple modalities in speech data (e.g., audio and text)\nis key to accurate classification. Existing studies normally choose\nto fuse multimodal features at the utterance level and largely neglect\nthe dynamic interplay of features from different modalities at a fine-granular\nlevel over time. In this paper, we explicitly model dynamic interactions\nbetween audio and text at the word level via interaction units between\ntwo long short-term memory networks representing audio and text. We\nalso devise a hierarchical representation of audio information from\nthe frame, phoneme and word levels, which largely improves the expressiveness\nof resulting audio features. We finally propose WISE, a novel word-level\ninteraction-based multimodal fusion framework for speech emotion recognition,\nto accommodate the aforementioned components. We evaluate WISE on the\npublic benchmark IEMOCAP corpus and demonstrate that it outperforms\nstate-of-the-art methods.\n"
      ],
      "doi": "10.21437/Interspeech.2020-3131",
      "author_area_id": "3",
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "chen20b_interspeech": {
      "authors": [
        [
          "Ming",
          "Chen"
        ],
        [
          "Xudong",
          "Zhao"
        ]
      ],
      "title": "A Multi-Scale Fusion Framework for Bimodal Speech Emotion Recognition",
      "original": "3156",
      "page_count": 5,
      "order": 77,
      "p1": "374",
      "pn": "378",
      "abstract": [
        "Speech emotion recognition (SER) is a challenging task that requires\nto learn suitable features for achieving good performance. The development\nof deep learning techniques makes it possible to automatically extract\nfeatures rather than construct hand-crafted features. In this paper,\na multi-scale fusion framework named STSER is proposed for bimodal\nSER by using speech and text information. A smodel, which takes advantage\nof convolutional neural network (CNN), bi-directional long short-term\nmemory (Bi-LSTM) and the attention mechanism, is proposed to learn\nspeech representation from the log-mel spectrogram extracted from speech\ndata. Specifically, the CNN layers are utilized to learn local correlations.\nThen the Bi-LSTM layer is applied to learn long-term dependencies and\ncontextual information. Finally, the multi-head self-attention layer\nmakes the model focus on the features that are most related to the\nemotions. A tmodel using a pre-trained ALBERT model is applied for\nlearning text representation from text data. Finally, a multi-scale\nfusion strategy, including feature fusion and ensemble learning, is\napplied to improve the overall performance. Experiments conducted on\nthe public emotion dataset IEMOCAP have shown that the proposed STSER\ncan achieve comparable recognition accuracy with fewer feature inputs.\n"
      ],
      "doi": "10.21437/Interspeech.2020-3156",
      "author_area_id": "3",
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "liu20b_interspeech": {
      "authors": [
        [
          "Pengfei",
          "Liu"
        ],
        [
          "Kun",
          "Li"
        ],
        [
          "Helen",
          "Meng"
        ]
      ],
      "title": "Group Gated Fusion on Attention-Based Bidirectional Alignment for Multimodal Emotion Recognition",
      "original": "2067",
      "page_count": 5,
      "order": 78,
      "p1": "379",
      "pn": "383",
      "abstract": [
        "Emotion recognition is a challenging and actively-studied research\narea that plays a critical role in emotion-aware human-computer interaction\nsystems. In a multimodal setting, temporal alignment between different\nmodalities has not been well investigated yet. This paper presents\na new model named as Gated Bidirectional Alignment Network (GBAN),\nwhich consists of an attention-based bidirectional alignment network\nover LSTM hidden states to explicitly capture the alignment relationship\nbetween speech and text, and a novel group gated fusion (GGF) layer\nto integrate the representations of different modalities. We empirically\nshow that the attention-aligned representations outperform the last-hidden-states\nof LSTM significantly, and the proposed GBAN model outperforms existing\nstate-of-the-art multimodal approaches on the IEMOCAP dataset.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2067",
      "author_area_id": "3",
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "khare20_interspeech": {
      "authors": [
        [
          "Aparna",
          "Khare"
        ],
        [
          "Srinivas",
          "Parthasarathy"
        ],
        [
          "Shiva",
          "Sundaram"
        ]
      ],
      "title": "Multi-Modal Embeddings Using Multi-Task Learning for Emotion Recognition",
      "original": "1827",
      "page_count": 5,
      "order": 79,
      "p1": "384",
      "pn": "388",
      "abstract": [
        "General embeddings like word2vec, GloVe and ELMo have shown a lot of\nsuccess in natural language tasks. The embeddings are typically extracted\nfrom models that are built on general tasks such as skip-gram models\nand natural language generation. In this paper, we extend the work\nfrom natural language understanding to multi-modal architectures that\nuse audio, visual and textual information for machine learning tasks.\nThe embeddings in our network are extracted using the encoder of a\ntransformer model trained using multi-task training. We use person\nidentification and automatic speech recognition as the tasks in our\nembedding generation framework. We tune and evaluate the embeddings\non the downstream task of emotion recognition and demonstrate that\non the CMU-MOSEI dataset, the embeddings can be used to improve over\nprevious state of the art results.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1827",
      "author_area_id": "3",
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "li20d_interspeech": {
      "authors": [
        [
          "Jeng-Lin",
          "Li"
        ],
        [
          "Chi-Chun",
          "Lee"
        ]
      ],
      "title": "Using Speaker-Aligned Graph Memory Block in Multimodally Attentive Emotion Recognition Network",
      "original": "1688",
      "page_count": 5,
      "order": 80,
      "p1": "389",
      "pn": "393",
      "abstract": [
        "Integrating multimodal emotion sensing modules in realizing human-centered\ntechnologies is rapidly growing. Despite recent advancement of deep\narchitectures in improving recognition performances, inability to handle\nindividual differences in the expressive cues creates a major hurdle\nfor real world applications. In this work, we propose a Speaker-aligned\nGraph Memory Network (SaGMN) that leverages the use of speaker embedding\nlearned from a large speaker verification network to characterize such\nan individualized personal difference across speakers. Specifically,\nthe learning of the gated memory block is jointly optimized with a\nspeaker graph encoder which aligns similar vocal characteristics samples\ntogether while effectively enlarge the discrimination across emotion\nclasses. We evaluate our multimodal emotion recognition network on\nthe CMU-MOSEI database and achieve a state-of-art accuracy of 65.1%\nUAR and 74.7% F1 score. Further visualization experiments demonstrate\nthe effect of speaker space alignment with the use of graph memory\nblocks.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1688",
      "author_area_id": "3",
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "lian20b_interspeech": {
      "authors": [
        [
          "Zheng",
          "Lian"
        ],
        [
          "Jianhua",
          "Tao"
        ],
        [
          "Bin",
          "Liu"
        ],
        [
          "Jian",
          "Huang"
        ],
        [
          "Zhanlei",
          "Yang"
        ],
        [
          "Rongjun",
          "Li"
        ]
      ],
      "title": "Context-Dependent Domain Adversarial Neural Network for Multimodal Emotion Recognition",
      "original": "1705",
      "page_count": 5,
      "order": 81,
      "p1": "394",
      "pn": "398",
      "abstract": [
        "Emotion recognition remains a complex task due to speaker variations\nand low-resource training samples. To address these difficulties, we\nfocus on the domain adversarial neural networks (DANN) for emotion\nrecognition. The primary task is to predict emotion labels. The secondary\ntask is to learn a common representation where speaker identities can\nnot be distinguished. By using this approach, we bring the representations\nof different speakers closer. Meanwhile, through using the unlabeled\ndata in the training process, we alleviate the impact of low-resource\ntraining samples. In the meantime, prior work found that contextual\ninformation and multimodal features are important for emotion recognition.\nHowever, previous DANN based approaches ignore these information, thus\nlimiting their performance. In this paper, we propose the context-dependent\ndomain adversarial neural network for multimodal emotion recognition.\nTo verify the effectiveness of our proposed method, we conduct experiments\non the benchmark dataset IEMOCAP. Experimental results demonstrate\nthat the proposed method shows an absolute improvement of 3.48% over\nstate-of-the-art strategies.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1705",
      "author_area_id": "3",
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "yang20b_interspeech": {
      "authors": [
        [
          "Bo",
          "Yang"
        ],
        [
          "Xianlong",
          "Tan"
        ],
        [
          "Zhengmao",
          "Chen"
        ],
        [
          "Bing",
          "Wang"
        ],
        [
          "Min",
          "Ruan"
        ],
        [
          "Dan",
          "Li"
        ],
        [
          "Zhongping",
          "Yang"
        ],
        [
          "Xiping",
          "Wu"
        ],
        [
          "Yi",
          "Lin"
        ]
      ],
      "title": "ATCSpeech: A Multilingual Pilot-Controller Speech Corpus from Real Air Traffic Control Environment",
      "original": "1020",
      "page_count": 5,
      "order": 82,
      "p1": "399",
      "pn": "403",
      "abstract": [
        "Automatic Speech Recognition (ASR) technique has been greatly developed\nin recent years, which expedites many applications in other fields.\nFor the ASR research, speech corpus is always an essential foundation,\nespecially for the vertical industry, such as Air Traffic Control (ATC).\nThere are some speech corpora for common applications, public or paid.\nHowever, for the ATC domain, it is difficult to collect raw speeches\nfrom real systems due to safety issues. More importantly, annotating\nthe transcription is a more laborious work for the supervised learning\nASR task, which hugely restricts the prospect of ASR application. In\nthis paper, a multilingual speech corpus (ATCSpeech) from real ATC\nsystems, including accented Mandarin Chinese and English speeches,\nis built and released to encourage the non-commercial ASR research\nin the ATC domain. The corpus is detailly introduced from the perspective\nof data amount, speaker gender and role, speech quality and other attributions.\nIn addition, the performance of baseline ASR models is also reported.\nA community edition for our speech database can be applied and used\nunder a special contract. To our best knowledge, this is the first\nwork that aims at building a real and multilingual ASR corpus for the\nATC related research. \n"
      ],
      "doi": "10.21437/Interspeech.2020-1020",
      "author_area_id": "12",
      "author_area_label": "Spoken Language Processing: Translation, Information Retrieval, Summarization, Resources and Evaluation"
    },
    "gutkin20_interspeech": {
      "authors": [
        [
          "Alexander",
          "Gutkin"
        ],
        [
          "I\u015f\u0131n",
          "Demir\u015fahin"
        ],
        [
          "Oddur",
          "Kjartansson"
        ],
        [
          "Clara",
          "Rivera"
        ],
        [
          "K\u00f3\u0323l\u00e1",
          "T\u00fab\u00f2\u0323s\u00fan"
        ]
      ],
      "title": "Developing an Open-Source Corpus of Yoruba Speech",
      "original": "1096",
      "page_count": 5,
      "order": 83,
      "p1": "404",
      "pn": "408",
      "abstract": [
        "This paper introduces an open-source speech dataset for Yoruba &#8212;\none of the largest low-resource West African languages spoken by at\nleast 22 million people. Yoruba is one of the official languages of\nNigeria, Benin and Togo, and is spoken in other neighboring African\ncountries and beyond. The corpus consists of over four hours of 48\nkHz recordings from 36 male and female volunteers and the corresponding\ntranscriptions that include disfluency annotation. The transcriptions\nhave full diacritization, which is vital for pronunciation and lexical\ndisambiguation. The annotated speech dataset described in this paper\nis primarily intended for use in text-to-speech systems, serve as adaptation\ndata in automatic speech recognition and speech-to-speech translation,\nand provide insights in West African corpus linguistics. We demonstrate\nthe use of this corpus in a simple statistical parametric speech synthesis\n(SPSS) scenario evaluating it against the related languages from the\nCMU Wilderness dataset and the Yoruba Lagos-NWU corpus.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1096",
      "author_area_id": "12",
      "author_area_label": "Spoken Language Processing: Translation, Information Retrieval, Summarization, Resources and Evaluation"
    },
    "ha20_interspeech": {
      "authors": [
        [
          "Jung-Woo",
          "Ha"
        ],
        [
          "Kihyun",
          "Nam"
        ],
        [
          "Jingu",
          "Kang"
        ],
        [
          "Sang-Woo",
          "Lee"
        ],
        [
          "Sohee",
          "Yang"
        ],
        [
          "Hyunhoon",
          "Jung"
        ],
        [
          "Hyeji",
          "Kim"
        ],
        [
          "Eunmi",
          "Kim"
        ],
        [
          "Soojin",
          "Kim"
        ],
        [
          "Hyun Ah",
          "Kim"
        ],
        [
          "Kyoungtae",
          "Doh"
        ],
        [
          "Chan Kyu",
          "Lee"
        ],
        [
          "Nako",
          "Sung"
        ],
        [
          "Sunghun",
          "Kim"
        ]
      ],
      "title": "ClovaCall: Korean Goal-Oriented Dialog Speech Corpus for Automatic Speech Recognition of Contact Centers",
      "original": "1136",
      "page_count": 5,
      "order": 84,
      "p1": "409",
      "pn": "413",
      "abstract": [
        "Automatic speech recognition (ASR) via call is essential for various\napplications, including AI for contact center (AICC) services. Despite\nthe advancement of ASR, however, most publicly available call-based\nspeech corpora such as Switchboard are old-fashioned. Also, most existing\ncall corpora are in English and mainly focus on open domain dialog\nor general scenarios such as audiobooks. Here we introduce a new large-scale\nKorean call-based speech corpus under a goal-oriented dialog scenario\nfrom more than 11,000 people, i.e., ClovaCall corpus. ClovaCall includes\napproximately 60,000 pairs of a short sentence and its corresponding\nspoken utterance in a restaurant reservation domain. We validate the\neffectiveness of our dataset with intensive experiments using two standard\nASR models. Furthermore, we release our ClovaCall dataset and baseline\nsource codes to be available via <KBD>https://github.com/ClovaAI/ClovaCall</KBD>\n"
      ],
      "doi": "10.21437/Interspeech.2020-1136",
      "author_area_id": "12",
      "author_area_label": "Spoken Language Processing: Translation, Information Retrieval, Summarization, Resources and Evaluation"
    },
    "wang20d_interspeech": {
      "authors": [
        [
          "Yanhong",
          "Wang"
        ],
        [
          "Huan",
          "Luan"
        ],
        [
          "Jiahong",
          "Yuan"
        ],
        [
          "Bin",
          "Wang"
        ],
        [
          "Hui",
          "Lin"
        ]
      ],
      "title": "LAIX Corpus of Chinese Learner English: Towards a Benchmark for L2 English ASR",
      "original": "1677",
      "page_count": 5,
      "order": 85,
      "p1": "414",
      "pn": "418",
      "abstract": [
        "This paper introduces a corpus of Chinese Learner English containing\n82 hours of L2 English speech by Chinese learners from all major dialect\nregions, collected through mobile apps developed by LAIX Inc. The LAIX\ncorpus was created to serve as a benchmark dataset for evaluating Automatic\nSpeech Recognition (ASR) performance on L2 English, the first of this\nkind as far as we know. The paper describes our effort to build the\ncorpus, including corpus design, data selection and transcription.\nMultiple rounds of quality check were conducted in the transcription\nprocess. Transcription errors were analyzed in terms of error types,\nrounds of reviewing, and learners&#8217; proficiency levels. Word error\nrates of state-of-the-art ASR systems on the benchmark corpus were\nalso reported.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1677",
      "author_area_id": "12",
      "author_area_label": "Spoken Language Processing: Translation, Information Retrieval, Summarization, Resources and Evaluation"
    },
    "ramanarayanan20_interspeech": {
      "authors": [
        [
          "Vikram",
          "Ramanarayanan"
        ]
      ],
      "title": "Design and Development of a Human-Machine Dialog Corpus for the Automated Assessment of Conversational English Proficiency",
      "original": "1988",
      "page_count": 5,
      "order": 86,
      "p1": "419",
      "pn": "423",
      "abstract": [
        "This paper presents a carefully designed corpus of scored spoken conversations\nbetween English language learners and a dialog system to facilitate\nresearch and development of both human and machine scoring of dialog\ninteractions. We collected speech, demographic and user experience\ndata from non-native speakers of English who interacted with a virtual\nboss as part of a workplace pragmatics skill building application.\nExpert raters then scored the dialogs on a custom rubric encompassing\n12 aspects of conversational proficiency as well as an overall holistic\nperformance score. We analyze key corpus statistics and discuss the\nadvantages of such a corpus for both human and machine scoring.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1988",
      "author_area_id": "12",
      "author_area_label": "Spoken Language Processing: Translation, Information Retrieval, Summarization, Resources and Evaluation"
    },
    "ng20_interspeech": {
      "authors": [
        [
          "Si-Ioi",
          "Ng"
        ],
        [
          "Cymie Wing-Yee",
          "Ng"
        ],
        [
          "Jiarui",
          "Wang"
        ],
        [
          "Tan",
          "Lee"
        ],
        [
          "Kathy Yuet-Sheung",
          "Lee"
        ],
        [
          "Michael Chi-Fai",
          "Tong"
        ]
      ],
      "title": "CUCHILD: A Large-Scale Cantonese Corpus of Child Speech for Phonology and Articulation Assessment",
      "original": "2148",
      "page_count": 5,
      "order": 87,
      "p1": "424",
      "pn": "428",
      "abstract": [
        "This paper describes the design and development of CUCHILD, a large-scale\nCantonese corpus of child speech. The corpus contains spoken words\ncollected from 1,986 child speakers aged from 3 to 6 years old. The\nspeech materials include 130 words of 1 to 4 syllables in length. The\nspeakers cover both typically developing (TD) children and children\nwith speech disorder. The intended use of the corpus is to support\nscientific and clinical research, as well as technology development\nrelated to child speech assessment. The design of the corpus, including\nselection of words, participants recruitment, data acquisition process,\nand data pre-processing are described in detail. The results of acoustical\nanalysis are presented to illustrate the properties of child speech.\nPotential applications of the corpus in automatic speech recognition,\nphonological error detection and speaker diarization are also discussed.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2148",
      "author_area_id": "12",
      "author_area_label": "Spoken Language Processing: Translation, Information Retrieval, Summarization, Resources and Evaluation"
    },
    "leino20_interspeech": {
      "authors": [
        [
          "Katri",
          "Leino"
        ],
        [
          "Juho",
          "Leinonen"
        ],
        [
          "Mittul",
          "Singh"
        ],
        [
          "Sami",
          "Virpioja"
        ],
        [
          "Mikko",
          "Kurimo"
        ]
      ],
      "title": "FinChat: Corpus and Evaluation Setup for Finnish Chat Conversations on Everyday Topics",
      "original": "2511",
      "page_count": 5,
      "order": 88,
      "p1": "429",
      "pn": "433",
      "abstract": [
        "Creating open-domain chatbots requires large amounts of conversational\ndata and related benchmark tasks to evaluate them. Standardized evaluation\ntasks are crucial for creating automatic evaluation metrics for model\ndevelopment; otherwise, comparing the models would require resource-expensive\nhuman evaluation. While chatbot challenges have recently managed to\nprovide a plethora of such resources for English, resources in other\nlanguages are not yet available. In this work, we provide a starting\npoint for Finnish open-domain chatbot research. We describe our collection\nefforts to create the Finnish chat conversation corpus FinChat, which\nis made available publicly. FinChat includes unscripted conversations\non seven topics from people of different ages. Using this corpus, we\nalso construct a retrieval-based evaluation task for Finnish chatbot\ndevelopment. We observe that off-the-shelf chatbot models trained on\nconversational corpora do not perform better than chance at choosing\nthe right answer based on automatic metrics, while humans can do the\nsame task almost perfectly. Similarly, in a human evaluation, responses\nto questions from the evaluation set generated by the chatbots are\npredominantly marked as incoherent. Thus, FinChat provides a challenging\nevaluation set, meant to encourage chatbot development in Finnish.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2511",
      "author_area_id": "12",
      "author_area_label": "Spoken Language Processing: Translation, Information Retrieval, Summarization, Resources and Evaluation"
    },
    "segbroeck20_interspeech": {
      "authors": [
        [
          "Maarten Van",
          "Segbroeck"
        ],
        [
          "Ahmed",
          "Zaid"
        ],
        [
          "Ksenia",
          "Kutsenko"
        ],
        [
          "Cirenia",
          "Huerta"
        ],
        [
          "Tinh",
          "Nguyen"
        ],
        [
          "Xuewen",
          "Luo"
        ],
        [
          "Bj\u00f6rn",
          "Hoffmeister"
        ],
        [
          "Jan",
          "Trmal"
        ],
        [
          "Maurizio",
          "Omologo"
        ],
        [
          "Roland",
          "Maas"
        ]
      ],
      "title": "DiPCo &#8212; Dinner Party Corpus",
      "original": "2800",
      "page_count": 3,
      "order": 89,
      "p1": "434",
      "pn": "436",
      "abstract": [
        "We present a speech data corpus that simulates a &#8220;dinner party&#8221;\nscenario taking place in an everyday home environment. The corpus was\ncreated by recording multiple groups of four Amazon employee volunteers\nhaving a natural conversation in English around a dining table. The\nparticipants were recorded by a single-channel close-talk microphone\nand by five far-field 7-microphone array devices positioned at different\nlocations in the recording room. The dataset contains the audio recordings\nand human labeled transcripts of a total of 10 sessions with a duration\nbetween 15 and 45 minutes. The corpus was created to advance in the\nfield of noise robust and distant speech processing and is intended\nto serve as a public research and benchmarking data set.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2800",
      "author_area_id": "12",
      "author_area_label": "Spoken Language Processing: Translation, Information Retrieval, Summarization, Resources and Evaluation"
    },
    "wang20e_interspeech": {
      "authors": [
        [
          "Bo",
          "Wang"
        ],
        [
          "Yue",
          "Wu"
        ],
        [
          "Niall",
          "Taylor"
        ],
        [
          "Terry",
          "Lyons"
        ],
        [
          "Maria",
          "Liakata"
        ],
        [
          "Alejo J.",
          "Nevado-Holgado"
        ],
        [
          "Kate E.A.",
          "Saunders"
        ]
      ],
      "title": "Learning to Detect Bipolar Disorder and Borderline Personality Disorder with Language and Speech in Non-Clinical Interviews",
      "original": "3040",
      "page_count": 5,
      "order": 90,
      "p1": "437",
      "pn": "441",
      "abstract": [
        "Bipolar disorder (BD) and borderline personality disorder (BPD) are\nboth chronic psychiatric disorders. However, their overlapping symptoms\nand common comorbidity make it challenging for the clinicians to distinguish\nthe two conditions on the basis of a clinical interview. In this work,\nwe first present a new multi-modal dataset containing interviews involving\nindividuals with BD or BPD being interviewed about a non-clinical topic\n. We investigate the automatic detection of the two conditions, and\ndemonstrate a good linear classifier that can be learnt using a down-selected\nset of features from the different aspects of the interviews and a\nnovel approach of summarising these features. Finally, we find that\ndifferent sets of features characterise BD and BPD, thus providing\ninsights into the difference between the automatic screening of the\ntwo conditions.\n"
      ],
      "doi": "10.21437/Interspeech.2020-3040",
      "author_area_id": "12",
      "author_area_label": "Spoken Language Processing: Translation, Information Retrieval, Summarization, Resources and Evaluation"
    },
    "kirkedal20_interspeech": {
      "authors": [
        [
          "Andreas",
          "Kirkedal"
        ],
        [
          "Marija",
          "Stepanovi\u0107"
        ],
        [
          "Barbara",
          "Plank"
        ]
      ],
      "title": " FT Speech: Danish Parliament Speech Corpus",
      "original": "3164",
      "page_count": 5,
      "order": 91,
      "p1": "442",
      "pn": "446",
      "abstract": [
        "This paper introduces  FT Speech, a new speech corpus created from\nthe recorded meetings of the Danish Parliament, otherwise known as\nthe  Folketing (FT). The corpus contains over 1,800 hours of transcribed\nspeech by a total of 434 speakers. It is significantly larger in duration,\nvocabulary, and amount of spontaneous speech than the existing public\nspeech corpora for Danish, which are largely limited to read-aloud\nand dictation data. We outline design considerations, including the\npreprocessing methods and the alignment procedure. To evaluate the\nquality of the corpus, we train automatic speech recognition systems\n(ASR) on the new resource and compare them to the systems trained on\nthe Danish part of Spr&#229;kbanken, the largest public ASR corpus\nfor Danish to date. Our baseline results show that we achieve a 14.01\nWER on the new corpus. A combination of  FT Speech with in-domain language\ndata provides comparable results to models trained specifically on\nSpr&#229;kbanken, showing that  FT Speech transfers well to this data\nset. Interestingly, our results demonstrate that the opposite is not\nthe case. This shows that  FT Speech provides a valuable resource for\npromoting research on Danish ASR with more spontaneous speech.\n"
      ],
      "doi": "10.21437/Interspeech.2020-3164",
      "author_area_id": "12",
      "author_area_label": "Spoken Language Processing: Translation, Information Retrieval, Summarization, Resources and Evaluation"
    },
    "duroselle20_interspeech": {
      "authors": [
        [
          "Rapha\u00ebl",
          "Duroselle"
        ],
        [
          "Denis",
          "Jouvet"
        ],
        [
          "Irina",
          "Illina"
        ]
      ],
      "title": "Metric Learning Loss Functions to Reduce Domain Mismatch in the x-Vector Space for Language Recognition",
      "original": "1708",
      "page_count": 5,
      "order": 92,
      "p1": "447",
      "pn": "451",
      "abstract": [
        "State-of-the-art language recognition systems are based on discriminative\nembeddings called  x-vectors. Channel and gender distortions produce\nmismatch in such  x-vector space where embeddings corresponding to\nthe same language are not grouped in an unique cluster. To control\nthis mismatch, we propose to train the  x-vector DNN with metric learning\nobjective functions. Combining a classification loss with the metric\nlearning n-pair loss allows to improve the language recognition performance.\nSuch a system achieves a robustness comparable to a system trained\nwith a domain adaptation loss function but without using the domain\ninformation. We also analyze the mismatch due to channel and gender,\nin comparison to language proximity, in the  x-vector space. This is\nachieved using the Maximum Mean Discrepancy divergence measure between\ngroups of  x-vectors. Our analysis shows that using the metric learning\nloss function reduces gender and channel mismatch in the  x-vector\nspace, even for languages only observed on one channel in the train\nset.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1708",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "li20e_interspeech": {
      "authors": [
        [
          "Zheng",
          "Li"
        ],
        [
          "Miao",
          "Zhao"
        ],
        [
          "Jing",
          "Li"
        ],
        [
          "Yiming",
          "Zhi"
        ],
        [
          "Lin",
          "Li"
        ],
        [
          "Qingyang",
          "Hong"
        ]
      ],
      "title": "The XMUSPEECH System for the AP19-OLR Challenge",
      "original": "1923",
      "page_count": 5,
      "order": 93,
      "p1": "452",
      "pn": "456",
      "abstract": [
        "In this paper, we present our XMUSPEECH system for the oriental language\nrecognition (OLR) challenge, AP19-OLR. The challenge this year contained\nthree tasks: (1) short-utterance LID, (2) cross-channel LID, and (3)\nzero-resource LID. We leveraged the system pipeline from three aspects,\nincluding front-end training, back-end processing, and fusion strategy.\nWe implemented many encoder networks for Tasks 1 and 3, such as extended\nx-vector, multi-task learning x-vector with phonetic information, and\nour previously presented multi-feature integration structure. Furthermore,\nour previously proposed length expansion method was used in the test\nset for Task 1. I-vector systems based on different acoustic features\nwere built for the cross-channel task. For all of three tasks, the\nsame back-end procedure was used for the sake of stability but with\ndifferent settings for three tasks. Finally, the greedy fusion strategy\nhelped to choose the subsystems to compose the final fusion systems\n(submitted systems).  Cavg values of 0.0263, 0.2813, and 0.1697 from\nthe development set for Task 1, 2, and 3 were obtained from our submitted\nsystems, and we achieved rank  3rd,  3rd, and  1st in the three tasks\nin this challenge, respectively.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1923"
    },
    "li20f_interspeech": {
      "authors": [
        [
          "Zheng",
          "Li"
        ],
        [
          "Miao",
          "Zhao"
        ],
        [
          "Jing",
          "Li"
        ],
        [
          "Lin",
          "Li"
        ],
        [
          "Qingyang",
          "Hong"
        ]
      ],
      "title": "On the Usage of Multi-Feature Integration for Speaker Verification and Language Identification",
      "original": "1960",
      "page_count": 5,
      "order": 94,
      "p1": "457",
      "pn": "461",
      "abstract": [
        "In this paper, we study the technology of multiple acoustic feature\nintegration for the applications of Automatic Speaker Verification\n(ASV) and Language Identification (LID). In contrast to score level\nfusion, a common method for integrating subsystems built upon various\nacoustic features, we explore a new integration strategy, which integrates\nmultiple acoustic features based on the x-vector framework. The frame\nlevel, statistics pooling level, segment level, and embedding level\nintegrations are investigated in this study. Our results indicate that\nframe level integration of multiple acoustic features achieves the\nbest performance in both speaker and language recognition tasks, and\nthe multi-feature integration strategy can be generalized in both classification\ntasks. Furthermore, we introduce a time-restricted attention mechanism\ninto the frame level integration structure to further improve the performance\nof multi-feature integration. The experiments are conducted on VoxCeleb\n1 for ASV and AP-OLR-17 for LID, and we achieve 28% and 19% relative\nimprovement in terms of Equal Error Rate (EER) in ASV and LID tasks,\nrespectively.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1960",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "chowdhury20_interspeech": {
      "authors": [
        [
          "Shammur A.",
          "Chowdhury"
        ],
        [
          "Ahmed",
          "Ali"
        ],
        [
          "Suwon",
          "Shon"
        ],
        [
          "James",
          "Glass"
        ]
      ],
      "title": "What Does an End-to-End Dialect Identification Model Learn About Non-Dialectal Information?",
      "original": "2235",
      "page_count": 5,
      "order": 95,
      "p1": "462",
      "pn": "466",
      "abstract": [
        "An end-to-end dialect identification system generates the likelihood\nof each dialect, given a speech utterance. The performance relies on\nits capabilities to discriminate the acoustic properties between the\ndifferent dialects, even though the input signal contains non-dialectal\ninformation such as speaker and channel. In this work, we study how\nnon-dialectal information are encoded inside the end-to-end dialect\nidentification model. We design several proxy tasks to understand the\nmodel&#8217;s ability to represent speech input for differentiating\nnon-dialectal information &#8212; such as (a) gender and voice identity\nof speakers, (b) languages, (c) channel (recording and transmission)\nquality &#8212; and compare with dialectal information (i.e., predicting\ngeographic region of the dialects). By analyzing non-dialectal representations\nfrom layers of an end-to-end Arabic dialect identification (ADI) model,\nwe observe that the model retains gender and channel information throughout\nthe network while learning a speaker-invariant representation. Our\nfindings also suggest that the CNN layers of the end-to-end model mirror\nfeature extractors capturing voice-specific information, while the\nfully-connected layers encode more dialectal information.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2235",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "lindgren20_interspeech": {
      "authors": [
        [
          "Matias",
          "Lindgren"
        ],
        [
          "Tommi",
          "Jauhiainen"
        ],
        [
          "Mikko",
          "Kurimo"
        ]
      ],
      "title": "Releasing a Toolkit and Comparing the Performance of Language Embeddings Across Various Spoken Language Identification Datasets",
      "original": "2706",
      "page_count": 5,
      "order": 96,
      "p1": "467",
      "pn": "471",
      "abstract": [
        "In this paper, we propose a software toolkit for easier end-to-end\ntraining of deep learning based spoken language identification models\nacross several speech datasets. We apply our toolkit to implement three\nbaseline models, one speaker recognition model, and three x-vector\narchitecture variations, which are trained on three datasets previously\nused in spoken language identification experiments. All models are\ntrained separately on each dataset (closed task) and on a combination\nof all datasets (open task), after which we compare if the open task\ntraining yields better language embeddings. We begin by training all\nmodels end-to-end as discriminative classifiers of spectral features,\nlabeled by language. Then, we extract language embedding vectors from\nthe trained end-to-end models, train separate Gaussian Naive Bayes\nclassifiers on the vectors, and compare which model provides best language\nembeddings for the backend classifier. Our experiments show that the\nopen task condition leads to improved language identification performance\non only one of the datasets. In addition, we discovered that increasing\nx-vector model robustness with random frequency channel dropout significantly\nreduces its end-to-end classification performance on the test set,\nwhile not affecting back-end classification performance of its embeddings.\nFinally, we note that two baseline models consistently outperformed\nall other models.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2706",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "alvarez20_interspeech": {
      "authors": [
        [
          "Aitor Arronte",
          "Alvarez"
        ],
        [
          "Elsayed Sabry Abdelaal",
          "Issa"
        ]
      ],
      "title": "Learning Intonation Pattern Embeddings for Arabic Dialect Identification",
      "original": "2906",
      "page_count": 5,
      "order": 97,
      "p1": "472",
      "pn": "476",
      "abstract": [
        "This article presents a full end-to-end pipeline for Arabic Dialect\nIdentification (ADI) using intonation patterns and acoustic representations.\nRecent approaches to language and dialect identification use linguistic-aware\ndeep architectures that are able to capture phonetic differences amongst\nlanguages and dialects. Specifically, in ADI tasks, different combinations\nof linguistic features and acoustic representations have been successful\nwith deep learning models. The approach presented in this article uses\nintonation patterns and hybrid residual and bidirectional LSTM networks\nto learn acoustic embeddings with no additional linguistic information.\nResults of the experiments show that intonation patterns for Arabic\ndialects provide sufficient information to achieve state-of-the-art\nresults on the VarDial 17 ADI dataset, outperforming single-feature\nsystems. The pipeline presented is robust to data sparsity, in contrast\nto other deep learning approaches that require large quantities of\ndata. We conjecture on the importance of sufficient information as\na criterion for optimality in a deep learning ADI task, and more generally,\nits application to acoustic modeling problems. Small intonation patterns,\nwhen sufficient in an information-theoretic sense, allow deep learning\narchitectures to learn more accurate speech representations.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2906",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "abdullah20_interspeech": {
      "authors": [
        [
          "Badr M.",
          "Abdullah"
        ],
        [
          "Tania",
          "Avgustinova"
        ],
        [
          "Bernd",
          "M\u00f6bius"
        ],
        [
          "Dietrich",
          "Klakow"
        ]
      ],
      "title": "Cross-Domain Adaptation of Spoken Language Identification for Related Languages: The Curious Case of Slavic Languages",
      "original": "2930",
      "page_count": 5,
      "order": 98,
      "p1": "477",
      "pn": "481",
      "abstract": [
        "State-of-the-art spoken language identification (LID) systems, which\nare based on end-to-end deep neural networks, have shown remarkable\nsuccess not only in discriminating between distant languages but also\nbetween closely-related languages or even different spoken varieties\nof the same language. However, it is still unclear to what extent neural\nLID models generalize to speech samples with different acoustic conditions\ndue to domain shift. In this paper, we present a set of experiments\nto investigate the impact of domain mismatch on the performance of\nneural LID systems for a subset of six Slavic languages across two\ndomains (read speech and radio broadcast) and examine two low-level\nsignal descriptors (spectral and cepstral features) for this task.\nOur experiments show that (1) out-of-domain speech samples severely\nhinder the performance of neural LID models, and (2) while both spectral\nand cepstral features show comparable performance within-domain, spectral\nfeatures show more robustness under domain mismatch. Moreover, we apply\nunsupervised domain adaptation to minimize the discrepancy between\nthe two domains in our study. We achieve relative accuracy improvements\nthat range from 9% to 77% depending on the diversity of acoustic conditions\nin the source domain.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2930",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "tits20_interspeech": {
      "authors": [
        [
          "No\u00e9",
          "Tits"
        ],
        [
          "Kevin El",
          "Haddad"
        ],
        [
          "Thierry",
          "Dutoit"
        ]
      ],
      "title": "ICE-Talk: An Interface for a Controllable Expressive Talking Machine",
      "original": "4001",
      "page_count": 2,
      "order": 99,
      "p1": "482",
      "pn": "483",
      "abstract": [
        "ICE-Talk is an open source<SUP>1</SUP> web-based GUI that allows the\nuse of a TTS system with controllable parameters via a text field and\na clickable 2D plot. It enables the study of latent spaces for controllable\nTTS. Moreover it is implemented as a module that can be used as part\nof a Human-Agent interaction.\n"
      ]
    },
    "hu20b_interspeech": {
      "authors": [
        [
          "Mathieu",
          "Hu"
        ],
        [
          "Laurent",
          "Pierron"
        ],
        [
          "Emmanuel",
          "Vincent"
        ],
        [
          "Denis",
          "Jouvet"
        ]
      ],
      "title": "Kaldi-Web: An Installation-Free, On-Device Speech Recognition System",
      "original": "4003",
      "page_count": 2,
      "order": 100,
      "p1": "484",
      "pn": "485",
      "abstract": [
        "Speech provides an intuitive interface to communicate with machines.\nToday, developers willing to implement such an interface must either\nrely on third-party proprietary software or become experts in speech\nrecognition. Conversely, researchers in speech recognition wishing\nto demonstrate their results need to be familiar with technologies\nthat are not relevant to their research (e.g., graphical user interface\nlibraries). In this demo, we introduce Kaldi-web<SUP>1</SUP>: an open-source,\ncross-platform tool which bridges this gap by providing a user interface\nbuilt around the online decoder of the Kaldi toolkit. Additionally,\nbecause we compile Kaldi to Web Assembly, speech recognition is performed\ndirectly in web browsers. This addresses privacy issues as no data\nis transmitted to the network for speech recognition.\n"
      ]
    },
    "kelly20_interspeech": {
      "authors": [
        [
          "Amelia C.",
          "Kelly"
        ],
        [
          "Eleni",
          "Karamichali"
        ],
        [
          "Armin",
          "Saeb"
        ],
        [
          "Karel",
          "Vesel\u00fd"
        ],
        [
          "Nicholas",
          "Parslow"
        ],
        [
          "Agape",
          "Deng"
        ],
        [
          "Arnaud",
          "Letondor"
        ],
        [
          "Robert",
          "O\u2019Regan"
        ],
        [
          "Qiru",
          "Zhou"
        ]
      ],
      "title": "Soapbox Labs Verification Platform for Child Speech",
      "original": "4006",
      "page_count": 2,
      "order": 101,
      "p1": "486",
      "pn": "487",
      "abstract": [
        "SoapBox Labs&#8217; child speech verification platform is a service\ndesigned specifically for identifying keywords and phrases in children&#8217;s\nspeech. Given an audio file containing children&#8217;s speech and\none or more target keywords or phrases, the system will return the\nconfidence score of recognition for the word(s) or phrase(s) within\nthe the audio file. The confidence scores are provided at utterance\nlevel, word level and phoneme level. The service is available online\nthrough an cloud API service, or offline on Android and iOS. The platform\nis accurate for child speech from children as young as 3, and is robust\nto noisy environments. In this demonstration we show how to access\nthe online API and give some examples of common use cases in literacy\nand language learning, gaming and robotics.\n"
      ]
    },
    "kelly20b_interspeech": {
      "authors": [
        [
          "Amelia C.",
          "Kelly"
        ],
        [
          "Eleni",
          "Karamichali"
        ],
        [
          "Armin",
          "Saeb"
        ],
        [
          "Karel",
          "Vesel\u00fd"
        ],
        [
          "Nicholas",
          "Parslow"
        ],
        [
          "Gloria Montoya",
          "Gomez"
        ],
        [
          "Agape",
          "Deng"
        ],
        [
          "Arnaud",
          "Letondor"
        ],
        [
          "Niall",
          "Mullally"
        ],
        [
          "Adrian",
          "Hempel"
        ],
        [
          "Robert",
          "O\u2019Regan"
        ],
        [
          "Qiru",
          "Zhou"
        ]
      ],
      "title": "SoapBox Labs Fluency Assessment Platform for Child Speech",
      "original": "4007",
      "page_count": 2,
      "order": 102,
      "p1": "488",
      "pn": "489",
      "abstract": [
        "The SoapBox Labs Fluency API service allows the automatic assessment\nof a child&#8217;s reading fluency. The system uses automatic speech\nrecognition (ASR) to transcribe the child&#8217;s speech as they read\na passage. The ASR output is then compared to the text of the reading\npassage, and the fluency algorithm returns information about the accuracy\nof the child&#8217;s reading attempt. In this show and tell paper we\ndescribe how the fluency cloud API is accessed and demonstrate how\nthe fluency demo system processes an audio file, as shown in the accompanying\nvideo.\n"
      ]
    },
    "kulebi20_interspeech": {
      "authors": [
        [
          "Baybars",
          "K\u00fclebi"
        ],
        [
          "Alp",
          "\u00d6ktem"
        ],
        [
          "Alex",
          "Peir\u00f3-Lilja"
        ],
        [
          "Santiago",
          "Pascual"
        ],
        [
          "Mireia",
          "Farr\u00fas"
        ]
      ],
      "title": "CATOTRON &#8212; A Neural Text-to-Speech System in Catalan",
      "original": "4009",
      "page_count": 2,
      "order": 103,
      "p1": "490",
      "pn": "491",
      "abstract": [
        "We present Catotron, a neural network-based open-source speech synthesis\nsystem in Catalan. Catotron consists of a sequence-to-sequence model\ntrained with two small open-source datasets based on semi-spontaneous\nand read speech. We demonstrate how a neural TTS can be built for languages\nwith limited resources using found-data optimization and cross-lingual\ntransfer learning. We make the datasets, initial models and source\ncode publicly available for both commercial and research purposes.\n"
      ]
    },
    "ramanarayanan20b_interspeech": {
      "authors": [
        [
          "Vikram",
          "Ramanarayanan"
        ],
        [
          "Oliver",
          "Roesler"
        ],
        [
          "Michael",
          "Neumann"
        ],
        [
          "David",
          "Pautler"
        ],
        [
          "Doug",
          "Habberstad"
        ],
        [
          "Andrew",
          "Cornish"
        ],
        [
          "Hardik",
          "Kothare"
        ],
        [
          "Vignesh",
          "Murali"
        ],
        [
          "Jackson",
          "Liscombe"
        ],
        [
          "Dirk",
          "Schnelle-Walka"
        ],
        [
          "Patrick",
          "Lange"
        ],
        [
          "David",
          "Suendermann-Oeft"
        ]
      ],
      "title": "Toward Remote Patient Monitoring of Speech, Video, Cognitive and Respiratory Biomarkers Using Multimodal Dialog Technology",
      "original": "4013",
      "page_count": 2,
      "order": 104,
      "p1": "492",
      "pn": "493",
      "abstract": [
        "We demonstrate a multimodal conversational platform for remote patient\ndiagnosis and monitoring. The platform engages patients in an interactive\ndialog session and automatically computes metrics relevant to speech\nacoustics and articulation, oro-motor and oro-facial movement, cognitive\nfunction and respiratory function. The dialog session includes a selection\nof exercises that have been widely used in both speech language pathology\nresearch as well as clinical practice &#8212; an oral motor exam, sustained\nphonation, diadochokinesis, read speech, spontaneous speech, spirometry,\npicture description, emotion elicitation and other cognitive tasks.\nFinally, the system automatically computes speech, video, cognitive\nand respiratory biomarkers that have been shown to be useful in capturing\nvarious aspects of speech motor function and neurological health and\nvisualizes them in a user-friendly dashboard.\n"
      ]
    },
    "lin20b_interspeech": {
      "authors": [
        [
          "Baihan",
          "Lin"
        ],
        [
          "Xinxin",
          "Zhang"
        ]
      ],
      "title": "VoiceID on the Fly: A Speaker Recognition System that Learns from Scratch",
      "original": "4014",
      "page_count": 2,
      "order": 105,
      "p1": "494",
      "pn": "495",
      "abstract": [
        "We proposed a novel AI framework to conduct real-time multi-speaker\nrecognition without any prior registration or pretraining by learning\nthe speaker identification on the fly. We considered the practical\nproblem of online learning with episodically revealed rewards and introduced\na solution based on semi-supervised and self-supervised learning methods\nin a web-based application at <KBD>https://www.baihan.nyc/viz/VoiceID/</KBD>\n"
      ]
    },
    "ren20_interspeech": {
      "authors": [
        [
          "Zhao",
          "Ren"
        ],
        [
          "Jing",
          "Han"
        ],
        [
          "Nicholas",
          "Cummins"
        ],
        [
          "Bj\u00f6rn W.",
          "Schuller"
        ]
      ],
      "title": "Enhancing Transferability of Black-Box Adversarial Attacks via Lifelong Learning for Speech Emotion Recognition Models",
      "original": "1869",
      "page_count": 5,
      "order": 106,
      "p1": "496",
      "pn": "500",
      "abstract": [
        "Well-designed adversarial examples can easily fool deep speech emotion\nrecognition models into misclassifications. The transferability of\nadversarial attacks is a crucial evaluation indicator when generating\nadversarial examples to fool a new target model or multiple models.\nHerein, we propose a method to improve the transferability of black-box\nadversarial attacks using lifelong learning. First, black-box adversarial\nexamples are generated by an atrous Convolutional Neural Network (CNN)\nmodel. This initial model is trained to attack a CNN target model.\nThen, we adapt the trained atrous CNN attacker to a new CNN target\nmodel using lifelong learning. We use this paradigm, as it enables\nmulti-task sequential learning, which saves more memory space than\nconventional multi-task learning. We verify this property on an emotional\nspeech database, by demonstrating that the updated atrous CNN model\ncan attack all target models which have been learnt, and can better\nattack a new target model than an attack model trained on one target\nmodel only.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1869",
      "author_area_id": "3",
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "feng20_interspeech": {
      "authors": [
        [
          "Han",
          "Feng"
        ],
        [
          "Sei",
          "Ueno"
        ],
        [
          "Tatsuya",
          "Kawahara"
        ]
      ],
      "title": "End-to-End Speech Emotion Recognition Combined with Acoustic-to-Word ASR Model",
      "original": "1180",
      "page_count": 5,
      "order": 107,
      "p1": "501",
      "pn": "505",
      "abstract": [
        "In this paper, we propose speech emotion recognition (SER) combined\nwith an acoustic-to-word automatic speech recognition (ASR) model.\nWhile acoustic prosodic features are primarily used for SER, textual\nfeatures are also useful but are error-prone, especially in emotional\nspeech. To solve this problem, we integrate ASR model and SER model\nin an end-to-end manner. This is done by using an acoustic-to-word\nmodel. Specifically, we utilize the states of the decoder in the ASR\nmodel with the acoustic features and input them into the SER model.\nOn top of a recurrent network to learn features from this input, we\nadopt a self-attention mechanism to focus on important feature frames.\nFinally, we finetune the ASR model on the new dataset using a multi-task\nlearning method to jointly optimize ASR with the SER task. Our model\nhas achieved a 68.63% weighted accuracy (WA) and 69.67% unweighted\naccuracy (UA) on the IEMOCAP database, which is state-of-the-art performance.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1180",
      "author_area_id": "3",
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "su20_interspeech": {
      "authors": [
        [
          "Bo-Hao",
          "Su"
        ],
        [
          "Chun-Min",
          "Chang"
        ],
        [
          "Yun-Shao",
          "Lin"
        ],
        [
          "Chi-Chun",
          "Lee"
        ]
      ],
      "title": "Improving Speech Emotion Recognition Using Graph Attentive Bi-Directional Gated Recurrent Unit Network",
      "original": "1733",
      "page_count": 5,
      "order": 108,
      "p1": "506",
      "pn": "510",
      "abstract": [
        "The manner that human encodes emotion information within an utterance\nis often complex and could result in a diverse salient acoustic profile\nthat is conditioned on emotion types. In this work, we propose a framework\nin imposing a graph attention mechanism on gated recurrent unit network\n(GA-GRU) to improve utterance-based speech emotion recognition (SER).\nOur proposed GA-GRU combines both long-range time-series based modeling\nof speech and further integrates complex saliency using a graph structure.\nWe evaluate our proposed GA-GRU on the IEMOCAP and the MSP-IMPROV database\nand achieve a 63.8% UAR and 57.47% UAR in a four class emotion recognition\ntask. The GA-GRU obtains consistently better performances as compared\nto recent state-of-art in per-utterance emotion classification model,\nand we further observe that different emotion categories would require\ndistinct flexible structures in modeling emotion information in the\nacoustic data that is beyond conventional  left-to-right or vice versa.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1733",
      "author_area_id": "3",
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "mallolragolta20_interspeech": {
      "authors": [
        [
          "Adria",
          "Mallol-Ragolta"
        ],
        [
          "Nicholas",
          "Cummins"
        ],
        [
          "Bj\u00f6rn W.",
          "Schuller"
        ]
      ],
      "title": "An Investigation of Cross-Cultural Semi-Supervised Learning for Continuous Affect Recognition",
      "original": "2641",
      "page_count": 5,
      "order": 109,
      "p1": "511",
      "pn": "515",
      "abstract": [
        "One of the keys for supervised learning techniques to succeed resides\nin the access to vast amounts of labelled training data. The process\nof data collection, however, is expensive, time-consuming, and application\ndependent. In the current digital era, data can be collected continuously.\nThis continuity renders data annotation into an endless task, which\npotentially, in problems such as emotion recognition, requires annotators\nwith different cultural backgrounds. Herein, we study the impact of\nutilising data from different cultures in a semi-supervised learning\napproach to label training material for the automatic recognition of\narousal and valence. Specifically, we compare the performance of culture-specific\naffect recognition models trained with manual or cross-cultural automatic\nannotations. The experiments performed in this work use the dataset\nreleased for the Cross-cultural Emotion Sub-challenge of the Audio/Visual\nEmotion Challenge (AVEC) 2019. The results obtained convey that the\ncultures used for training impact on the system performance. Furthermore,\nin most of the scenarios assessed, affect recognition models trained\nwith hybrid solutions, combining manual and automatic annotations,\nsurpass the baseline model, which was exclusively trained with manual\nannotations.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2641",
      "author_area_id": "3",
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "sridhar20_interspeech": {
      "authors": [
        [
          "Kusha",
          "Sridhar"
        ],
        [
          "Carlos",
          "Busso"
        ]
      ],
      "title": "Ensemble of Students Taught by Probabilistic Teachers to Improve Speech Emotion Recognition",
      "original": "2694",
      "page_count": 5,
      "order": 110,
      "p1": "516",
      "pn": "520",
      "abstract": [
        "Reliable and generalizable  speech emotion recognition (SER) systems\nhave wide applications in various fields including healthcare, customer\nservice, and security and defense. Towards this goal, this study presents\na novel  teacher-student (T-S) framework for SER, relying on an ensemble\nof probabilistic predictions of teacher embeddings to train an ensemble\nof students. We use uncertainty modeling with  Monte-Carlo (MC) dropout\nto create a distribution for the embeddings of an intermediate dense\nlayer of the teacher. The embeddings guiding the student models are\nderived by sampling from this distribution. The final prediction combines\nthe results obtained by the student ensemble. The proposed model not\nonly increases the prediction performance over the teacher model, but\nalso generates more consistent predictions. As a T-S formulation, the\napproach allows the use of unlabeled data to improve the performance\nof the students in a semi-supervised manner. An ablation analysis shows\nthe importance of the MC-based ensemble and the use of unlabeled data.\nThe results show relative improvements in  concordance correlation\ncoefficient (CCC) up to 4.25% for arousal, 2.67% for valence and 4.98%\nfor dominance from their baseline results. The results also show that\nthe student ensemble decreases the uncertainty in the predictions,\nleading to more consistent results.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2694",
      "author_area_id": "3",
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "latif20_interspeech": {
      "authors": [
        [
          "Siddique",
          "Latif"
        ],
        [
          "Muhammad",
          "Asim"
        ],
        [
          "Rajib",
          "Rana"
        ],
        [
          "Sara",
          "Khalifa"
        ],
        [
          "Raja",
          "Jurdak"
        ],
        [
          "Bj\u00f6rn W.",
          "Schuller"
        ]
      ],
      "title": "Augmenting Generative Adversarial Networks for Speech Emotion Recognition",
      "original": "3194",
      "page_count": 5,
      "order": 111,
      "p1": "521",
      "pn": "525",
      "abstract": [
        "Generative adversarial networks (GANs) have shown potential in learning\nemotional attributes and generating new data samples. However, their\nperformance is usually hindered by the unavailability of larger speech\nemotion recognition (SER) data. In this work, we propose a framework\nthat utilises the mixup data augmentation scheme to augment the GAN\nin feature learning and generation. To show the effectiveness of the\nproposed framework, we present results for SER on (i) synthetic feature\nvectors, (ii) augmentation of the training data with synthetic features,\n(iii) encoded features in compressed representation. Our results show\nthat the proposed framework can effectively learn compressed emotional\nrepresentations as well as it can generate synthetic samples that help\nimprove performance in within-corpus and cross-corpus evaluation.\n"
      ],
      "doi": "10.21437/Interspeech.2020-3194",
      "author_area_id": "3",
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "dissanayake20_interspeech": {
      "authors": [
        [
          "Vipula",
          "Dissanayake"
        ],
        [
          "Haimo",
          "Zhang"
        ],
        [
          "Mark",
          "Billinghurst"
        ],
        [
          "Suranga",
          "Nanayakkara"
        ]
      ],
      "title": "Speech Emotion Recognition &#8216;in the Wild&#8217; Using an Autoencoder",
      "original": "1356",
      "page_count": 5,
      "order": 112,
      "p1": "526",
      "pn": "530",
      "abstract": [
        "Speech Emotion Recognition (SER) has been a challenging task on which\nresearchers have been working for decades. Recently, Deep Learning\n(DL) based approaches have been shown to perform well in SER tasks;\nhowever, it has been noticed that their superior performance is limited\nto the distribution of the data used to train the model. In this paper,\nwe present an analysis of using autoencoders to improve the generalisability\nof DL based SER solutions. We train a sparse autoencoder using a large\nspeech corpus extracted from social media. Later, the trained encoder\npart of the autoencoder is reused as the input to a long short-term\nmemory (LSTM) network, and the encoder-LSTM modal is re-trained on\nan aggregation of five commonly used speech emotion corpora. Our evaluation\nuses an unseen corpus in the training &amp; validation stages to simulate\n&#8216;in the wild&#8217; condition and analyse the generalisability\nof our solution. A performance comparison is carried out between the\nencoder based model and a model trained without an encoder. Our results\nshow that the autoencoder based model improves the unweighted accuracy\nof the unseen corpus by 8%, indicating autoencoder based pre-training\ncan improve the generalisability of DL based SER solutions.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1356",
      "author_area_id": "3",
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "mao20_interspeech": {
      "authors": [
        [
          "Shuiyang",
          "Mao"
        ],
        [
          "P.C.",
          "Ching"
        ],
        [
          "Tan",
          "Lee"
        ]
      ],
      "title": "Emotion Profile Refinery for Speech Emotion Classification",
      "original": "1771",
      "page_count": 5,
      "order": 113,
      "p1": "531",
      "pn": "535",
      "abstract": [
        "Human emotions are inherently ambiguous and impure. When designing\nsystems to anticipate human emotions based on speech, the lack of emotional\npurity must be considered. However, most of the current methods for\nspeech emotion classification rest on the consensus, e. g., one single\nhard label for an utterance. This labeling principle imposes challenges\nfor system performance considering emotional impurity. In this paper,\nwe recommend the use of emotional profiles (EPs), which provides a\ntime series of segment-level soft labels to capture the subtle blends\nof emotional cues present across a specific speech utterance. We further\npropose the emotion profile refinery (EPR), an iterative procedure\nto update EPs. The EPR method produces soft, dynamically-generated,\nmultiple probabilistic class labels during successive stages of refinement,\nwhich results in significant improvements in the model accuracy. Experiments\non three well-known emotion corpora show noticeable gain using the\nproposed method.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1771",
      "author_area_id": "3",
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "yeh20_interspeech": {
      "authors": [
        [
          "Sung-Lin",
          "Yeh"
        ],
        [
          "Yun-Shao",
          "Lin"
        ],
        [
          "Chi-Chun",
          "Lee"
        ]
      ],
      "title": "Speech Representation Learning for Emotion Recognition Using End-to-End ASR with Factorized Adaptation",
      "original": "2524",
      "page_count": 5,
      "order": 114,
      "p1": "536",
      "pn": "540",
      "abstract": [
        "Developing robust speech emotion recognition (SER) systems is challenging\ndue to small-scale of existing emotional speech datasets. However,\nprevious works have mostly relied on handcrafted acoustic features\nto build SER models that are difficult to handle a wide range of acoustic\nvariations. One way to alleviate this problem is by using speech representations\nlearned from deep end-to-end models trained on large-scale speech database.\nSpecifically, in this paper, we leverage an end-to-end ASR to extract\nASR-based representations for speech emotion recognition. We further\ndevise a factorized domain adaptation approach on the pre-trained ASR\nmodel to improve both the speech recognition rate and the emotion recognition\naccuracy on the target emotion corpus, and we also provide an analysis\nin the effectiveness of representations extracted from different ASR\nlayers. Our experiments demonstrate the importance of ASR adaptation\nand layer depth for emotion recognition.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2524",
      "author_area_id": "3",
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "kumar20_interspeech": {
      "authors": [
        [
          "Kshitiz",
          "Kumar"
        ],
        [
          "Emilian",
          "Stoimenov"
        ],
        [
          "Hosam",
          "Khalil"
        ],
        [
          "Jian",
          "Wu"
        ]
      ],
      "title": "Fast and Slow Acoustic Model",
      "original": "2887",
      "page_count": 5,
      "order": 115,
      "p1": "541",
      "pn": "545",
      "abstract": [
        "In this work we layout a Fast &amp; Slow (F&amp;S) acoustic model (AM)\nin an encoder-decoder architecture for streaming automatic speech recognition\n(ASR). The Slow model represents our baseline ASR model; it&#8217;s\nsignificantly larger than Fast model and provides stronger accuracy.\nThe Fast model is generally developed for related speech applications.\nIt has weaker ASR accuracy but is faster to evaluate and consequently\nleads to better user-perceived latency. We propose a joint F&amp;S\nmodel that encodes output state information from Fast model, feeds\nthat to Slow model to improve overall model accuracy from F&amp;S AM.\nWe demonstrate scenarios where individual Fast and Slow models are\nalready available to build the joint F&amp;S model. We apply our work\non a large vocabulary ASR task. Compared to Slow AM, our Fast AM is\n3&#8211;4&#215; smaller and 11.5% relatively weaker in ASR accuracy.\nThe proposed F&amp;S AM achieves 4.7% relative gain over the Slow AM.\nWe also report a progression of techniques and improve the relative\ngain to 8.1% by encoding additional Fast AM outputs. Our proposed framework\nhas generic attributes &#8212; we demonstrate a specific extension\nby encoding two Slow models to achieve 12.2% relative gain.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2887",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "moriya20_interspeech": {
      "authors": [
        [
          "Takafumi",
          "Moriya"
        ],
        [
          "Tsubasa",
          "Ochiai"
        ],
        [
          "Shigeki",
          "Karita"
        ],
        [
          "Hiroshi",
          "Sato"
        ],
        [
          "Tomohiro",
          "Tanaka"
        ],
        [
          "Takanori",
          "Ashihara"
        ],
        [
          "Ryo",
          "Masumura"
        ],
        [
          "Yusuke",
          "Shinohara"
        ],
        [
          "Marc",
          "Delcroix"
        ]
      ],
      "title": "Self-Distillation for Improving CTC-Transformer-Based ASR Systems",
      "original": "1223",
      "page_count": 5,
      "order": 116,
      "p1": "546",
      "pn": "550",
      "abstract": [
        "We present a novel training approach for encoder-decoder-based sequence-to-sequence\n(S2S) models. S2S models have been used successfully by the automatic\nspeech recognition (ASR) community. The important key factor of S2S\nis the attention mechanism as it captures the relationships between\ninput and output sequences. The attention weights inform which time\nframes should be attended to for predicting the output labels. In previous\nwork, we proposed distilling S2S knowledge into connectionist temporal\nclassification (CTC) based models by using the attention characteristics\nto create pseudo-targets for an auxiliary cross-entropy loss term.\nThis approach can significantly improve CTC models. However, it remained\nunclear whether our proposal could be used to improve S2S models. In\nthis paper, we extend our previous work to create a strong S2S model,\ni.e. Transformer with CTC (CTC-Transformer). We utilize Transformer\noutputs and the source attention weights for making pseudo-targets\nthat contain both the posterior and the timing information of each\nTransformer output. These pseudo-targets are used to train the shared\nencoder of the CTC-Transformer through the use of direct feedback from\nthe Transformer-decoder and thus obtain more informative representations.\nExperiments on public and private datasets to perform various tasks\ndemonstrate that our proposal is also effective for enhancing S2S model\ntraining. In particular, on a Japanese ASR task, our best system outperforms\nthe previous state-of-the-art alternative.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1223",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "tuske20_interspeech": {
      "authors": [
        [
          "Zolt\u00e1n",
          "T\u00fcske"
        ],
        [
          "George",
          "Saon"
        ],
        [
          "Kartik",
          "Audhkhasi"
        ],
        [
          "Brian",
          "Kingsbury"
        ]
      ],
      "title": "Single Headed Attention Based Sequence-to-Sequence Model for State-of-the-Art Results on Switchboard",
      "original": "1488",
      "page_count": 5,
      "order": 117,
      "p1": "551",
      "pn": "555",
      "abstract": [
        "It is generally believed that direct sequence-to-sequence (seq2seq)\nspeech recognition models are competitive with hybrid models only when\na large amount of data, at least a thousand hours, is available for\ntraining. In this paper, we show that state-of-the-art recognition\nperformance can be achieved on the Switchboard-300 database using a\nsingle headed attention, LSTM based model. Using a cross-utterance\nlanguage model, our single-pass speaker independent system reaches\n6.4% and 12.5% word error rate (WER) on the Switchboard and CallHome\nsubsets of Hub5&#8217;00, without a pronunciation lexicon. While careful\nregularization and data augmentation are crucial in achieving this\nlevel of performance, experiments on Switchboard-2000 show that nothing\nis more useful than more data. Overall, the combination of various\nregularizations and a simple but fairly large model results in a new\nstate of the art, 4.8% and 8.3% WER on the Switchboard and CallHome\nsets, using SWB-2000 without any external data resources.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1488",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "chen20c_interspeech": {
      "authors": [
        [
          "Zhehuai",
          "Chen"
        ],
        [
          "Andrew",
          "Rosenberg"
        ],
        [
          "Yu",
          "Zhang"
        ],
        [
          "Gary",
          "Wang"
        ],
        [
          "Bhuvana",
          "Ramabhadran"
        ],
        [
          "Pedro J.",
          "Moreno"
        ]
      ],
      "title": "Improving Speech Recognition Using GAN-Based Speech Synthesis and Contrastive Unspoken Text Selection",
      "original": "1475",
      "page_count": 5,
      "order": 118,
      "p1": "556",
      "pn": "560",
      "abstract": [
        "Text-to-Speech synthesis (TTS) based data augmentation is a relatively\nnew mechanism for utilizing text-only data to improve automatic speech\nrecognition (ASR) training without parameter or inference architecture\nchanges. However, efforts to train speech recognition systems on synthesized\nutterances suffer from limited acoustic diversity of TTS outputs. Additionally,\nthe text-only corpus is always much larger than the transcribed speech\ncorpus by several orders of magnitude, which makes speech synthesis\nof all the text data impractical. In this work, we propose to combine\ngenerative adversarial network (GAN) and multi-style training (MTR)\nto increase acoustic diversity in the synthesized data. We also present\na contrastive language model-based data selection technique to improve\nthe efficiency of learning from unspoken text. We demonstrate that\nour proposed method allows ASR models to learn from synthesis of large-scale\nunspoken text sources and achieves a 35% relative WER reduction on\na voice-search task.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1475",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "shao20_interspeech": {
      "authors": [
        [
          "Yiwen",
          "Shao"
        ],
        [
          "Yiming",
          "Wang"
        ],
        [
          "Daniel",
          "Povey"
        ],
        [
          "Sanjeev",
          "Khudanpur"
        ]
      ],
      "title": " PyChain: A Fully Parallelized PyTorch Implementation of LF-MMI for End-to-End ASR",
      "original": "3053",
      "page_count": 5,
      "order": 119,
      "p1": "561",
      "pn": "565",
      "abstract": [
        "We present  PyChain, a fully parallelized PyTorch implementation of\nend-to-end lattice-free maximum mutual information (LF-MMI) training\nfor the so-called  chain models in the Kaldi automatic speech recognition\n(ASR) toolkit. Unlike other PyTorch and Kaldi based ASR toolkits, \nPyChain is designed to be as flexible and light-weight as possible\nso that it can be easily plugged into new ASR projects, or other existing\nPyTorch-based ASR tools, as exemplified respectively by a new project\n PyChain-example, and  Espresso, an existing end-to-end ASR toolkit.\n PyChain&#8217;s efficiency and flexibility is demonstrated through\nsuch novel features as full GPU training on numerator/denominator graphs,\nand support for unequal length sequences. Experiments on the WSJ dataset\nshow that with simple neural networks and commonly used machine learning\ntechniques,  PyChain can achieve competitive results that are comparable\nto Kaldi and better than other end-to-end ASR systems.\n"
      ],
      "doi": "10.21437/Interspeech.2020-3053",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "an20_interspeech": {
      "authors": [
        [
          "Keyu",
          "An"
        ],
        [
          "Hongyu",
          "Xiang"
        ],
        [
          "Zhijian",
          "Ou"
        ]
      ],
      "title": "CAT: A CTC-CRF Based ASR Toolkit Bridging the Hybrid and the End-to-End Approaches Towards Data Efficiency and Low Latency",
      "original": "2732",
      "page_count": 5,
      "order": 120,
      "p1": "566",
      "pn": "570",
      "abstract": [
        "In this paper, we present a new open source toolkit for speech recognition,\nnamed CAT (CTC-CRF based ASR Toolkit). CAT inherits the data-efficiency\nof the hybrid approach and the simplicity of the E2E approach, providing\na full-fledged implementation of CTC-CRFs and complete training and\ntesting scripts for a number of English and Chinese benchmarks. Experiments\nshow CAT obtains state-of-the-art results, which are comparable to\nthe fine-tuned hybrid models in Kaldi but with a much simpler training\npipeline. Compared to existing non-modularized E2E models, CAT performs\nbetter on limited-scale datasets, demonstrating its data efficiency.\nFurthermore, we propose a new method called contextualized soft forgetting,\nwhich enables CAT to do streaming ASR without accuracy degradation.\nWe hope CAT, especially the CTC-CRF based framework and software, will\nbe of broad interest to the community, and can be further explored\nand improved.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2732",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "inaguma20_interspeech": {
      "authors": [
        [
          "Hirofumi",
          "Inaguma"
        ],
        [
          "Masato",
          "Mimura"
        ],
        [
          "Tatsuya",
          "Kawahara"
        ]
      ],
      "title": "CTC-Synchronous Training for Monotonic Attention Model",
      "original": "1069",
      "page_count": 5,
      "order": 121,
      "p1": "571",
      "pn": "575",
      "abstract": [
        "Monotonic chunkwise attention (MoChA) has been studied for the online\nstreaming automatic speech recognition (ASR) based on a sequence-to-sequence\nframework. In contrast to connectionist temporal classification (CTC),\nbackward probabilities cannot be leveraged in the alignment marginalization\nprocess during training due to left-to-right dependency in the decoder.\nThis results in the error propagation of alignments to subsequent token\ngeneration. To address this problem, we propose CTC-synchronous training\n(CTC-ST), in which MoChA uses CTC alignments to learn optimal monotonic\nalignments. Reference CTC alignments are extracted from a CTC branch\nsharing the same encoder with the decoder. The entire model is jointly\noptimized so that the expected boundaries from MoChA are synchronized\nwith the alignments. Experimental evaluations of the TEDLIUM release-2\nand Librispeech corpora show that the proposed method significantly\nimproves recognition, especially for long utterances. We also show\nthat CTC-ST can bring out the full potential of SpecAugment for MoChA.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1069",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "houston20_interspeech": {
      "authors": [
        [
          "Brady",
          "Houston"
        ],
        [
          "Katrin",
          "Kirchhoff"
        ]
      ],
      "title": "Continual Learning for Multi-Dialect Acoustic Models",
      "original": "1797",
      "page_count": 5,
      "order": 122,
      "p1": "576",
      "pn": "580",
      "abstract": [
        "Using data from multiple dialects has shown promise in improving neural\nnetwork acoustic models. While such training can improve the performance\nof an acoustic model on a single dialect, it can also produce a model\ncapable of good performance on multiple dialects. However, training\nan acoustic model on pooled data from multiple dialects takes a significant\namount of time and computing resources, and it needs to be retrained\nevery time a new dialect is added to the model. In contrast, sequential\ntransfer learning (fine-tuning) does not require retraining using all\ndata, but may result in catastrophic forgetting of previously-seen\ndialects. Using data from four english dialects, we demonstrate that\nby using loss functions that mitigate catastrophic forgetting, sequential\ntransfer learning can be used to train multi-dialect acoustic models\nthat narrow the WER gap between the best (combined training) and worst\n(fine-tuning) case by up to 65%. Continual learning shows great promise\nin minimizing training time while approaching the performance of models\nthat require much more training time.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1797",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "song20b_interspeech": {
      "authors": [
        [
          "Xingchen",
          "Song"
        ],
        [
          "Zhiyong",
          "Wu"
        ],
        [
          "Yiheng",
          "Huang"
        ],
        [
          "Dan",
          "Su"
        ],
        [
          "Helen",
          "Meng"
        ]
      ],
      "title": "SpecSwap: A Simple Data Augmentation Method for End-to-End Speech Recognition",
      "original": "2275",
      "page_count": 5,
      "order": 123,
      "p1": "581",
      "pn": "585",
      "abstract": [
        "Recently, End-to-End (E2E) models have achieved state-of-the-art performance\nfor automatic speech recognition (ASR). Within these large and deep\nmodels, overfitting remains an important problem that heavily influences\nthe model performance. One solution to deal with the overfitting problem\nis to increase the quantity and variety of the training data with the\nhelp of data augmentation. In this paper, we present SpecSwap, a simple\ndata augmentation scheme for automatic speech recognition that acts\ndirectly on the spectrogram of input utterances. The augmentation policy\nconsists of swapping blocks of frequency channels and swapping blocks\nof time steps. We apply SpecSwap on Transformer-based networks for\nend-to-end speech recognition task. Our experiments on Aishell-1 show\nstate-of-the-art performance for E2E models that are trained solely\non the speech training data. Further, by increasing the depth of model,\nthe Transformers trained with augmentations can outperform certain\nhybrid systems, even without the aid of a language model.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2275",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "stan20_interspeech": {
      "authors": [
        [
          "Adriana",
          "Stan"
        ]
      ],
      "title": "RECOApy: Data Recording, Pre-Processing and Phonetic Transcription for End-to-End Speech-Based Applications",
      "original": "1184",
      "page_count": 5,
      "order": 124,
      "p1": "586",
      "pn": "590",
      "abstract": [
        "Deep learning enables the development of efficient end-to-end speech\nprocessing applications while bypassing the need for expert linguistic\nand signal processing features. Yet, recent studies show that good\nquality speech resources and phonetic transcription of the training\ndata can enhance the results of these applications. In this paper,\nthe RECOApy tool is introduced. RECOApy streamlines the steps of data\nrecording and pre-processing required in end-to-end speech-based applications.\nThe tool implements an easy-to-use interface for prompted speech recording,\nspectrogram and waveform analysis, utterance-level normalisation and\nsilence trimming, as well grapheme-to-phoneme conversion of the prompts\nin eight languages: Czech, English, French, German, Italian, Polish,\nRomanian and Spanish.<br style=\"mso-data-placement:same-cell;\" /><br\nstyle=\"mso-data-placement:same-cell;\" />\n  The grapheme-to-phoneme\n(G2P) converters are deep neural network (DNN) based architectures\ntrained on lexicons extracted from the Wiktionary online collaborative\nresource. With the different degree of orthographic transparency, as\nwell as the varying amount of phonetic entries across the languages,\nthe DNN&#8217;s hyperparameters are optimised with an evolution strategy.\nThe phoneme and word error rates of the resulting G2P converters are\npresented and discussed. The tool, the processed phonetic lexicons\nand trained G2P models are made freely available.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1184",
      "author_area_id": "12",
      "author_area_label": "Spoken Language Processing: Translation, Information Retrieval, Summarization, Resources and Evaluation"
    },
    "shangguan20_interspeech": {
      "authors": [
        [
          "Yuan",
          "Shangguan"
        ],
        [
          "Kate",
          "Knister"
        ],
        [
          "Yanzhang",
          "He"
        ],
        [
          "Ian",
          "McGraw"
        ],
        [
          "Fran\u00e7oise",
          "Beaufays"
        ]
      ],
      "title": "Analyzing the Quality and Stability of a Streaming End-to-End On-Device Speech Recognizer",
      "original": "1194",
      "page_count": 5,
      "order": 125,
      "p1": "591",
      "pn": "595",
      "abstract": [
        "The demand for fast and accurate incremental speech recognition increases\nas the applications of automatic speech recognition (ASR) proliferate.\nIncremental speech recognizers output chunks of partially recognized\nwords while the user is still talking. Partial results can be revised\nbefore the ASR finalizes its hypothesis, causing instability issues.\nWe analyze the quality and stability of on-device streaming end-to-end\n(E2E) ASR models. We first introduce a novel set of metrics that quantify\nthe instability at word and segment levels. We study the impact of\nseveral model training techniques that improve E2E model qualities\nbut degrade model stability. We categorize the causes of instability\nand explore various solutions to mitigate them in a streaming E2E ASR\nsystem.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1194",
      "author_area_id": "12",
      "author_area_label": "Spoken Language Processing: Translation, Information Retrieval, Summarization, Resources and Evaluation"
    },
    "liu20c_interspeech": {
      "authors": [
        [
          "Zhe",
          "Liu"
        ],
        [
          "Fuchun",
          "Peng"
        ]
      ],
      "title": "Statistical Testing on ASR Performance via Blockwise Bootstrap",
      "original": "1338",
      "page_count": 5,
      "order": 126,
      "p1": "596",
      "pn": "600",
      "abstract": [
        "A common question being raised in automatic speech recognition (ASR)\nevaluations is how reliable is an observed word error rate (WER) improvement\ncomparing two ASR systems, where statistical hypothesis testing and\nconfidence interval (CI) can be utilized to tell whether this improvement\nis real or only due to random chance. The bootstrap resampling method\nhas been popular for such significance analysis which is intuitive\nand easy to use. However, this method fails in dealing with dependent\ndata, which is prevalent in speech world &#8212; for example, ASR performance\non utterances from the same speaker could be correlated. In this paper\nwe present blockwise bootstrap approach &#8212; by dividing evaluation\nutterances into nonoverlapping blocks, this method resamples these\nblocks instead of original data. We show that the resulting variance\nestimator of absolute WER difference between two ASR systems is consistent\nunder mild conditions. We also demonstrate the validity of blockwise\nbootstrap method on both synthetic and real-world speech data.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1338",
      "author_area_id": "12",
      "author_area_label": "Spoken Language Processing: Translation, Information Retrieval, Summarization, Resources and Evaluation"
    },
    "ramakrishna20_interspeech": {
      "authors": [
        [
          "Anil",
          "Ramakrishna"
        ],
        [
          "Shrikanth",
          "Narayanan"
        ]
      ],
      "title": "Sentence Level Estimation of Psycholinguistic Norms Using Joint Multidimensional Annotations",
      "original": "1841",
      "page_count": 5,
      "order": 127,
      "p1": "601",
      "pn": "605",
      "abstract": [
        "Psycholinguistic normatives represent various affective and mental\nconstructs using numeric scores and are used in a variety of applications\nin natural language processing. They are commonly used at the sentence\nlevel, the scores of which are estimated by extrapolating word level\nscores using simple aggregation strategies, which may not always be\noptimal. In this work, we present a novel approach to estimate the\npsycholinguistic norms at sentence level. We apply a multidimensional\nannotation fusion model on annotations at the word level to estimate\na parameter which captures relationships between different norms. We\nthen use this parameter at sentence level to estimate the norms. We\nevaluate our approach by predicting sentence level scores for various\nnormative dimensions and compare with standard word aggregation schemes.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1841",
      "author_area_id": "12",
      "author_area_label": "Spoken Language Processing: Translation, Information Retrieval, Summarization, Resources and Evaluation"
    },
    "fan20_interspeech": {
      "authors": [
        [
          "Kai",
          "Fan"
        ],
        [
          "Bo",
          "Li"
        ],
        [
          "Jiayi",
          "Wang"
        ],
        [
          "Shiliang",
          "Zhang"
        ],
        [
          "Boxing",
          "Chen"
        ],
        [
          "Niyu",
          "Ge"
        ],
        [
          "Zhijie",
          "Yan"
        ]
      ],
      "title": "Neural Zero-Inflated Quality Estimation Model for Automatic Speech Recognition System",
      "original": "1881",
      "page_count": 5,
      "order": 128,
      "p1": "606",
      "pn": "610",
      "abstract": [
        "The performances of automatic speech recognition (ASR) systems are\nusually evaluated by the metric word error rate (WER) when the manually\ntranscribed data are provided, which are, however, expensively available\nin the real scenario. In addition, the empirical distribution of WER\nfor most ASR systems usually tends to put a significant mass near zero,\nmaking it difficult to simulate with a single continuous distribution.\nIn order to address the two issues of ASR quality estimation (QE),\nwe propose a novel neural zero-inflated model to predict the WER of\nthe ASR result without transcripts. We design a neural zero-inflated\nbeta regression on top of a bidirectional transformer language model\nconditional on speech features (speech-BERT). We adopt the pre-training\nstrategy of token level masked language modeling for speech-BERT as\nwell, and further fine-tune with our zero-inflated layer for the mixture\nof discrete and continuous outputs. The experimental results show that\nour approach achieves better performance on WER prediction compared\nwith strong baselines.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1881",
      "author_area_id": "12",
      "author_area_label": "Spoken Language Processing: Translation, Information Retrieval, Summarization, Resources and Evaluation"
    },
    "woodward20_interspeech": {
      "authors": [
        [
          "Alejandro",
          "Woodward"
        ],
        [
          "Clara",
          "Bonn\u00edn"
        ],
        [
          "Issey",
          "Masuda"
        ],
        [
          "David",
          "Varas"
        ],
        [
          "Elisenda",
          "Bou-Balust"
        ],
        [
          "Juan Carlos",
          "Riveiro"
        ]
      ],
      "title": "Confidence Measures in Encoder-Decoder Models for Speech Recognition",
      "original": "2215",
      "page_count": 5,
      "order": 129,
      "p1": "611",
      "pn": "615",
      "abstract": [
        "Recent improvements in Automatic Speech Recognition (ASR) systems have\nenabled the growth of myriad applications such as voice assistants,\nintent detection, keyword extraction and sentiment analysis. These\napplications, which are now widely used in the industry, are very sensitive\nto the errors generated by ASR systems. This could be overcome by having\na reliable confidence measurement associated to the predicted output.\nThis work presents a novel method which uses internal neural features\nof a frozen ASR model to train an independent neural network to predict\na softmax temperature value. This value is computed in each decoder\ntime step and multiplied by the logits in order to redistribute the\noutput probabilities. The resulting softmax values corresponding to\npredicted tokens constitute a more reliable confidence measure. Moreover,\nthis work also studies the effect of teacher forcing on the training\nof the proposed temperature prediction module. The output confidence\nestimation shows an improvement of -25.78% in EER and +7.59% in AUC-ROC\nwith respect to the unaltered softmax values of the predicted tokens,\nevaluated on a proprietary dataset consisting on News and Entertainment\nvideos.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2215",
      "author_area_id": "12",
      "author_area_label": "Spoken Language Processing: Translation, Information Retrieval, Summarization, Resources and Evaluation"
    },
    "ali20_interspeech": {
      "authors": [
        [
          "Ahmed",
          "Ali"
        ],
        [
          "Steve",
          "Renals"
        ]
      ],
      "title": "Word Error Rate Estimation Without ASR Output: e-WER2",
      "original": "2357",
      "page_count": 5,
      "order": 130,
      "p1": "616",
      "pn": "620",
      "abstract": [
        "Measuring the performance of automatic speech recognition (ASR) systems\nrequires manually transcribed data in order to compute the word error\nrate (WER), which is often time-consuming and expensive. In this paper,\nwe continue our effort in estimating WER using acoustic, lexical and\nphonotactic features. Our novel approach to estimate the WER uses a\nmultistream end-to-end architecture. We report results for systems\nusing internal speech decoder features (glass-box), systems without\nspeech decoder features (black-box), and for systems without having\naccess to the ASR system (no-box). The no-box system learns joint acoustic-lexical\nrepresentation from phoneme recognition results along with MFCC acoustic\nfeatures to estimate WER. Considering WER per sentence, our no-box\nsystem achieves 0.56 Pearson correlation with the reference evaluation\nand 0.24 root mean square error (RMSE) across 1,400 sentences. The\nestimated overall WER by e-WER2 is 30.9% for a three hours test set,\nwhile the WER computed using the reference transcriptions was 28.5%.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2357",
      "author_area_id": "12",
      "author_area_label": "Spoken Language Processing: Translation, Information Retrieval, Summarization, Resources and Evaluation"
    },
    "ludusan20_interspeech": {
      "authors": [
        [
          "Bogdan",
          "Ludusan"
        ],
        [
          "Petra",
          "Wagner"
        ]
      ],
      "title": "An Evaluation of Manual and Semi-Automatic Laughter Annotation",
      "original": "2521",
      "page_count": 5,
      "order": 131,
      "p1": "621",
      "pn": "625",
      "abstract": [
        "With laughter research seeing a development in recent years, there\nis also an increased need in materials having laughter annotations.\nWe examine in this study how one can leverage existing spontaneous\nspeech resources to this goal. We first analyze the process of manual\nlaughter annotation in corpora, by establishing two important parameters\nof the process: the amount of time required and its inter-rater reliability.\nNext, we propose a novel semi-automatic tool for laughter annotation,\nbased on a signal-based representation of speech rhythm. We test both\nannotation approaches on the same recordings, containing German dyadic\nspontaneous interactions, and employing a larger pool of annotators\nthan previously done. We then compare and discuss the obtained results\nbased on the two aforementioned parameters, highlighting the benefits\nand costs associated to each approach.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2521",
      "author_area_id": "12",
      "author_area_label": "Spoken Language Processing: Translation, Information Retrieval, Summarization, Resources and Evaluation"
    },
    "martin20_interspeech": {
      "authors": [
        [
          "Joshua L.",
          "Martin"
        ],
        [
          "Kevin",
          "Tang"
        ]
      ],
      "title": "Understanding Racial Disparities in Automatic Speech Recognition: The Case of Habitual &#8220;be&#8221;",
      "original": "2893",
      "page_count": 5,
      "order": 132,
      "p1": "626",
      "pn": "630",
      "abstract": [
        "Recent research has highlighted that state-of-the-art automatic speech\nrecognition (ASR) systems exhibit a bias against African American speakers.\nIn this research, we investigate the underlying causes of this racially\nbased disparity in performance, focusing on a unique morpho-syntactic\nfeature of African American English (AAE), namely habitual &#8220;be&#8221;,\nan invariant form of &#8220;be&#8221; that encodes the habitual aspect.\nBy looking at over 100 hours of spoken AAE, we evaluated two ASR systems\n&#8212; DeepSpeech and Google Cloud Speech &#8212; to examine how well\nhabitual &#8220;be&#8221; and its surrounding contexts are inferred.\nWhile controlling for local language and acoustic factors such as the\namount of context, noise, and speech rate, we found that habitual &#8220;be&#8221;\nand its surrounding words were more error prone than non-habitual &#8220;be&#8221;\nand its surrounding words. These findings hold both when the utterance\ncontaining &#8220;be&#8221; is processed in isolation and in conjunction\nwith surrounding utterances within speaker turn. Our research highlights\nthe need for equitable ASR systems to take into account dialectal differences\nbeyond acoustic modeling.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2893",
      "author_area_id": "12",
      "author_area_label": "Spoken Language Processing: Translation, Information Retrieval, Summarization, Resources and Evaluation"
    },
    "zellou20_interspeech": {
      "authors": [
        [
          "Georgia",
          "Zellou"
        ],
        [
          "Rebecca",
          "Scarborough"
        ],
        [
          "Renee",
          "Kemp"
        ]
      ],
      "title": "Secondary Phonetic Cues in the Production of the Nasal Short-a System in California English",
      "original": "1322",
      "page_count": 5,
      "order": 133,
      "p1": "631",
      "pn": "635",
      "abstract": [
        "A production study explored the acoustic characteristics of /&#230;/\nin CVC and CVN words spoken by California speakers who raise /&#230;/\nin pre-nasal contexts. Results reveal that the phonetic realization\nof the /&#230;/-/&#949;/ contrast in these contexts is multidimensional.\nRaised pre-nasal /&#230;/ is close in formant space to /&#949;/, particularly\nover the second half of the vowel. Yet, systematic differences in the\nrealization of the secondary acoustic features of duration, formant\nmovement, and degree of coarticulatory vowel nasalization keep these\nvowels phonetically distinct. These findings have implications for\nsystems of vowel contrast and the use of secondary phonetic properties\nto maintain lexical distinctions.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1322",
      "author_area_id": "2",
      "author_area_label": "Phonetics, Phonology, and Prosody"
    },
    "lorin20_interspeech": {
      "authors": [
        [
          "Louis-Marie",
          "Lorin"
        ],
        [
          "Lorenzo",
          "Maselli"
        ],
        [
          "L\u00e9o",
          "Varnet"
        ],
        [
          "Maria",
          "Giavazzi"
        ]
      ],
      "title": "Acoustic Properties of Strident Fricatives at the Edges: Implications for Consonant Discrimination",
      "original": "2913",
      "page_count": 5,
      "order": 134,
      "p1": "636",
      "pn": "640",
      "abstract": [
        "Languages tend to license segmental contrasts where they are maximally\nperceptible, i.e. where more perceptual cues to the contrast are available.\nFor strident fricatives, the most salient cues to the presence of voicing\nare low-frequency energy concentrations and fricative duration, as\nvoiced fricatives are systematically shorter than voiceless ones. Cross-linguistically,\nthe voicing contrast is more frequently realized word-initially than\nword-finally, as for obstruents. We investigate the phonetic underpinnings\nof this asymmetric behavior at the word edges, focusing on the availability\nof durational cues to the contrast in the two positions. To assess\nsegmental duration, listeners rely on temporal markers, i.e. jumps\nin acoustic energy which demarcate segmental boundaries, thereby facilitating\nduration discrimination. We conducted an acoustic analysis of word-initial\nand word-final strident fricatives in American English. We found that\ntemporal markers are sharper at the left edge of word-initial fricatives\nthan at the right edge of word-final fricatives, in terms of absolute\nvalue of the intensity slope, in the high-frequency region. These findings\nallow us to make predictions about the availability of durational cues\nto the voicing contrast in the two positions.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2913",
      "author_area_id": "2",
      "author_area_label": "Phonetics, Phonology, and Prosody"
    },
    "luo20_interspeech": {
      "authors": [
        [
          "Mingqiong",
          "Luo"
        ]
      ],
      "title": "Processes and Consequences of Co-Articulation  in Mandarin V<SUB>1</SUB>N.(C<SUB>2</SUB>)V<SUB>2</SUB> Context: Phonology and Phonetics",
      "original": "1041",
      "page_count": 5,
      "order": 135,
      "p1": "641",
      "pn": "645",
      "abstract": [
        "It is well known that in Mandarin Chinese (MC) nasal rhymes, non-high\nvowels /a/ and /e/ undergo Vowel Nasalization and Backness Feature\nSpecification processes to harmonize with the nasal coda in both manner\nand place of articulation. Specifically, the vowel is specified with\nthe [+front] feature when followed by the /n/ coda and the [+back]\nfeature when followed by /&#x014B;/. On the other hand, phonetic experiments\nin recent researches have shown that in MC disyllabic words, the nasal\ncoda tends to undergo place assimilation in the V<SUB>1</SUB>N.C<SUB>2</SUB>V<SUB>2</SUB>\ncontext and complete deletion in the V<SUB>1</SUB>N.V<SUB>2</SUB> context.<br\nstyle=\"mso-data-placement:same-cell;\" /><br style=\"mso-data-placement:same-cell;\"\n/>\n  These processes raises two questions: firstly, will V<SUB>1</SUB>\nin V<SUB>1</SUB>N.C<SUB>2</SUB>V<SUB>2</SUB> contexts also change in\nits backness feature to harmonize with the assimilated nasal coda?\nSecondly, will the duration of V<SUB>1</SUB>N reduce significantly\nafter nasal coda deletion in the V<SUB>1</SUB>N.(G)V context?<br style=\"mso-data-placement:same-cell;\"\n/><br style=\"mso-data-placement:same-cell;\" />\n  A production experiment\nand a perception experiment were designed to answer these two questions.\nResults show that the vowel backness feature of V<SUB>1</SUB> is not\nre-specified despite the appropriate environment, and the duration\nof V<SUB>1</SUB>N is not reduced after nasal deletion. The phonological\nconsequences of these findings will be discussed.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1041"
    },
    "yue20_interspeech": {
      "authors": [
        [
          "Yang",
          "Yue"
        ],
        [
          "Fang",
          "Hu"
        ]
      ],
      "title": "Voicing Distinction of Obstruents in the Hangzhou Wu Chinese Dialect",
      "original": "1259",
      "page_count": 5,
      "order": 136,
      "p1": "646",
      "pn": "650",
      "abstract": [
        "This paper gives an acoustic phonetic description of the obstruents\nin the Hangzhou Wu Chinese dialect. Based on the data from 8 speakers\n(4 male and 4 female), obstruents were examined in terms of VOT, silent\nclosure duration, segment duration, and spectral properties such as\nH1-H2, H1-F1 and H1-F3. Results suggest that VOT cannot differentiate\nthe voiced obstruents from their voiceless counterparts, but the silent\nclosure duration can. There is no voiced aspiration. And breathiness\nwas detected on the vowel following the voiced category of obstruents.\nAn acoustic consequence is that there is no segment for the voiced\nglottal fricative [&#614;], since it was realized as the breathiness\non the following vowel. But interestingly, it is observed that syllables\nwith [&#614;] are longer than their onset-less counterparts.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1259",
      "author_area_id": "2",
      "author_area_label": "Phonetics, Phonology, and Prosody"
    },
    "wang20f_interspeech": {
      "authors": [
        [
          "Lei",
          "Wang"
        ]
      ],
      "title": "The Phonology and Phonetics of Kaifeng Mandarin Vowels",
      "original": "2375",
      "page_count": 5,
      "order": 137,
      "p1": "651",
      "pn": "655",
      "abstract": [
        "In this present study, we re-analyze the vowel system in Kaifeng Mandarin,\nadopting a phoneme-based approach. Our analysis deviates from the previous\nsyllable-based analyses in a number of ways. First, we treat apical\nvowels [&#x27f; &#x285;] as syllabic approximants and analyze them\nas allophones of the retroflex approximant /&#x27B;/. Second, the vowel\ninventory is of three sets, monophthongs, diphthongs and retroflex\nvowels. The classification of monophthongs and diphthongs is based\non the phonological distribution of the coda nasal. That is, monophthongs\ncan be followed by a nasal coda, while diphthongs cannot. This argument\nhas introduced two new opening diphthongs /e&#949; &#x264;&#652;/ in\nthe inventory, which have traditionally been described as monophthongs.\nOur phonological characterization of the vowels in Kaifeng Mandarin\nis further backed up by acoustic data. It is argued that the present\nstudy has gone some way towards enhancing our understanding of Mandarin\nsegmental phonology in general.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2375",
      "author_area_id": "2",
      "author_area_label": "Phonetics, Phonology, and Prosody"
    },
    "zellers20_interspeech": {
      "authors": [
        [
          "Margaret",
          "Zellers"
        ],
        [
          "Barbara",
          "Schuppler"
        ]
      ],
      "title": "Microprosodic Variability in Plosives in German and Austrian German",
      "original": "2353",
      "page_count": 5,
      "order": 138,
      "p1": "656",
      "pn": "660",
      "abstract": [
        "Fundamental frequency (F0) contours may show slight, microprosodic\nvariations in the vicinity of plosive segments, which may have distinctive\npatterns relative to the place of articulation and voicing. Similarly,\nplosive bursts have distinctive characteristics associated with these\narticulatory features. The current study investigates the degree to\nwhich such microprosodic variations arise in two varieties of German,\nand how the two varieties differ. We find that microprosodic effects\nindeed arise in F0 as well as burst intensity and Center of Gravity,\nbut that the extent of the variability is different in the two varieties\nunder investigation, with northern German tending towards more variability\nin the microprosody of plosives than Austrian German. Coarticulatory\neffects on the burst with the following segment also arise, but also\nhave different features in the two varieties. This evidence is consistent\nwith the possibility that the fortis-lenis contrast is not equally\nstable in Austrian German and northern German.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2353",
      "author_area_id": "2",
      "author_area_label": "Phonetics, Phonology, and Prosody"
    },
    "huang20b_interspeech": {
      "authors": [
        [
          "Jing",
          "Huang"
        ],
        [
          "Feng-fan",
          "Hsieh"
        ],
        [
          "Yueh-chin",
          "Chang"
        ]
      ],
      "title": " Er-Suffixation in Southwestern Mandarin: An EMA and Ultrasound Study",
      "original": "2453",
      "page_count": 5,
      "order": 139,
      "p1": "661",
      "pn": "665",
      "abstract": [
        "This paper is an articulatory study of the  er-suffixation (a.k.a.\n erhua) in Southwestern Mandarin (SWM), using co-registered EMA and\nultrasound. Data from two female speakers in their twenties were analyzed\nand discussed. Our recording materials contain unsuffixed stems,  er-suffixed\nforms and the rhotic schwa [&#x25A;], a phonemic vowel in its own right.\nResults suggest that the  er-suffixation in SWM involves suffixing\na rhotic schwa [&#x25A;] to the stem, unlike its counterpart in Beijing\nand Northeastern Mandarin [5]. Specifically, an entire rime will be\nreplaced with the  er-suffix if the nucleus vowel is non-high; only\nhigh vocoids will be preserved after the  er-suffixation. The &#8220;rhoticity&#8221;\nis primarily realized as a  bunched tongue shape configuration (i.e.\na domed tongue body), while the Tongue Tip gesture plays a more limited\nrole in SWM. A phonological analysis is accordingly proposed for the\n er-suffixation in SWM.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2453",
      "author_area_id": "2",
      "author_area_label": "Phonetics, Phonology, and Prosody"
    },
    "li20g_interspeech": {
      "authors": [
        [
          "Yinghao",
          "Li"
        ],
        [
          "Jinghua",
          "Zhang"
        ]
      ],
      "title": "Electroglottographic-Phonetic Study on Korean Phonation Induced by Tripartite Plosives in Yanbian Korean",
      "original": "2350",
      "page_count": 5,
      "order": 140,
      "p1": "666",
      "pn": "670",
      "abstract": [
        "This paper examined the phonatory features induced by the tripartite\nplosives in Yanbian Korean, broadly considered as Hamkyungbukdo Korean\ndialect. Electroglottographic (EGG) and acoustic analysis was applied\nfor five elderly Korean speakers. The results show that fortis-induced\nphonation is characterized with more constricted glottis, slower spectral\ntilt, and higher sub-harmonic-harmonic ratio. Lenis-induced phonation\nis shown to be breathier with smaller Contact Quotient and faster spectral\ntilt. Most articulatory and acoustic measures for the aspirated are\nshown to be patterned with the lenis; However, sporadic difference\nbetween the two indicates that the lenis induces more breathier phonation.\nThe diplophonia phonation is argued to be a salient feature for the\nfortis-head syllables in Yanbian Korean. The vocal fold medial compression\nand adductive tension mechanisms are tentatively argued to be responsible\nfor the production of the fortis. At last, gender difference is shown\nto be salient in the fortis-induced phonation.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2350",
      "author_area_id": "2",
      "author_area_label": "Phonetics, Phonology, and Prosody"
    },
    "wilkins20_interspeech": {
      "authors": [
        [
          "Nicholas",
          "Wilkins"
        ],
        [
          "Max Cordes",
          "Galbraith"
        ],
        [
          "Ifeoma",
          "Nwogu"
        ]
      ],
      "title": "Modeling Global Body Configurations in American Sign Language",
      "original": "2873",
      "page_count": 5,
      "order": 141,
      "p1": "671",
      "pn": "675",
      "abstract": [
        "In this paper we consider the problem of computationally representing\nAmerican Sign Language (ASL) phonetics. We specifically present a computational\nmodel inspired by the sequential phonological ASL representation, known\nas the Movement-Hold (MH) Model. Our computational model is capable\nof not only capturing ASL phonetics, but also has generative abilities.\nWe present a Probabilistic Graphical Model (PGM) which explicitly models\nholds and implicitly models movement in the MH model. For evaluation,\nwe introduce a novel data corpus, ASLing, and compare our PGM to other\nmodels (GMM, LDA, and VAE) and show its superior performance. Finally,\nwe demonstrate our model&#8217;s interpretability by computing various\nphonetic properties of ASL through the inspection of our learned model.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2873",
      "author_area_id": "2",
      "author_area_label": "Phonetics, Phonology, and Prosody"
    },
    "li20h_interspeech": {
      "authors": [
        [
          "Hang",
          "Li"
        ],
        [
          "Siyuan",
          "Chen"
        ],
        [
          "Julien",
          "Epps"
        ]
      ],
      "title": "Augmenting Turn-Taking Prediction with Wearable Eye Activity During Conversation",
      "original": "3204",
      "page_count": 5,
      "order": 142,
      "p1": "676",
      "pn": "680",
      "abstract": [
        "In a variety of conversation contexts, accurately predicting the time\npoint at which a conversational participant is about to speak can help\nimprove computer-mediated human-human communications. Although it is\nnot difficult for a human to perceive turn-taking intent in conversations,\nit has been a challenging task for computers to date. In this study,\nwe employed eye activity acquired from low-cost wearable hardware during\nnatural conversation and studied how pupil diameter, blink and gaze\ndirection could assist speech in voice activity and turn-taking prediction.\nExperiments on a new 2-hour corpus of natural conversational speech\nbetween six pairs of speakers wearing near-field eye video glasses\nrevealed that the F1 score for predicting the voicing activity up to\n1s ahead of the current instant can be above 80%, for speech and non-speech\ndetection with fused eye and speech features. Further, extracting features\nsynchronously from both interlocutors provides a relative reduction\nin error rate of 8.5% compared with a system based on just a single\nspeaker. The performance of four turn-taking states based on the predicted\nvoice activity also achieved F1 scores significantly higher than chance\nlevel. These findings suggest that wearable eye activity can play a\nrole in future speech communication systems.\n"
      ],
      "doi": "10.21437/Interspeech.2020-3204",
      "author_area_id": "10",
      "author_area_label": "Speech Recognition: Technologies and Systems for New Applications"
    },
    "lu20_interspeech": {
      "authors": [
        [
          "Weiyi",
          "Lu"
        ],
        [
          "Yi",
          "Xu"
        ],
        [
          "Peng",
          "Yang"
        ],
        [
          "Belinda",
          "Zeng"
        ]
      ],
      "title": "CAM: Uninteresting Speech Detector",
      "original": "1192",
      "page_count": 5,
      "order": 143,
      "p1": "681",
      "pn": "685",
      "abstract": [
        "Voice assistants such as Siri, Alexa, etc. usually adopt a pipeline\nto process users&#8217; utterances, which generally include transcribing\nthe audio into text, understanding the text, and finally responding\nback to users. One potential issue is that some utterances could be\ndevoid of any interesting speech, and are thus not worth being processed\nthrough the entire pipeline. Examples of uninteresting utterances include\nthose that have too much noise, are devoid of intelligible speech,\netc. It is therefore desirable to have a model to filter out such useless\nutterances before they are ingested for downstream processing, thus\nsaving system resources. Towards this end, we propose the Combination\nof Audio and Metadata (CAM) detector to identify utterances that contain\nonly uninteresting speech. Our experimental results show that the CAM\ndetector considerably outperforms using either an audio model or a\nmetadata model alone, which demonstrates the effectiveness of the proposed\nsystem.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1192",
      "author_area_id": "10",
      "author_area_label": "Speech Recognition: Technologies and Systems for New Applications"
    },
    "caseiro20_interspeech": {
      "authors": [
        [
          "Diamantino",
          "Caseiro"
        ],
        [
          "Pat",
          "Rondon"
        ],
        [
          "Quoc-Nam Le",
          "The"
        ],
        [
          "Petar",
          "Aleksic"
        ]
      ],
      "title": "Mixed Case Contextual ASR Using Capitalization Masks",
      "original": "2367",
      "page_count": 5,
      "order": 144,
      "p1": "686",
      "pn": "690",
      "abstract": [
        "End-to-end (E2E) mixed-case automatic speech recognition (ASR) systems\nthat directly predict words in the written domain are attractive due\nto being simple to build, not requiring explicit capitalization models,\nallowing streaming capitalization without additional effort beyond\nthat required for streaming ASR, and their small size. However, the\nfact that these systems produce various versions of the same word with\ndifferent capitalizations, and even different word segmentations for\ndifferent case variants when wordpieces (WP) are predicted, leads to\nmultiple problems with contextual ASR. In particular, the size of and\ntime to build contextual models grows considerably with the number\nof variants per word. In this paper, we propose separating orthographic\nrecognition from capitalization, so that the ASR system first predicts\na word, then predicts its capitalization in the form of a capitalization\nmask. We show that the use of capitalization masks achieves the same\nlow error rate as traditional mixed-case ASR, while reducing the size\nand compilation time of contextual models. Furthermore, we observe\nsignificant improvements in capitalization quality.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2367",
      "author_area_id": "10",
      "author_area_label": "Speech Recognition: Technologies and Systems for New Applications"
    },
    "mao20b_interspeech": {
      "authors": [
        [
          "Huanru Henry",
          "Mao"
        ],
        [
          "Shuyang",
          "Li"
        ],
        [
          "Julian",
          "McAuley"
        ],
        [
          "Garrison W.",
          "Cottrell"
        ]
      ],
      "title": "Speech Recognition and Multi-Speaker Diarization of Long Conversations",
      "original": "3039",
      "page_count": 5,
      "order": 145,
      "p1": "691",
      "pn": "695",
      "abstract": [
        "Speech recognition (ASR) and speaker diarization (SD) models have traditionally\nbeen trained separately to produce rich conversation transcripts with\nspeaker labels. Recent advances [1] have shown that joint ASR and SD\nmodels can learn to leverage audio-lexical inter-dependencies to improve\nword diarization performance. We introduce a new benchmark of hour-long\npodcasts collected from the weekly  This American Life radio program\nto better compare these approaches when applied to extended multi-speaker\nconversations. We find that training separate ASR and SD models perform\nbetter when utterance boundaries are known but otherwise joint models\ncan perform better. To handle long conversations with unknown utterance\nboundaries, we introduce a striding attention decoding algorithm and\ndata augmentation techniques which, combined with model pre-training,\nimproves ASR and SD.\n"
      ],
      "doi": "10.21437/Interspeech.2020-3039",
      "author_area_id": "10",
      "author_area_label": "Speech Recognition: Technologies and Systems for New Applications"
    },
    "geng20_interspeech": {
      "authors": [
        [
          "Mengzhe",
          "Geng"
        ],
        [
          "Xurong",
          "Xie"
        ],
        [
          "Shansong",
          "Liu"
        ],
        [
          "Jianwei",
          "Yu"
        ],
        [
          "Shoukang",
          "Hu"
        ],
        [
          "Xunying",
          "Liu"
        ],
        [
          "Helen",
          "Meng"
        ]
      ],
      "title": "Investigation of Data Augmentation Techniques for Disordered Speech Recognition",
      "original": "1161",
      "page_count": 5,
      "order": 146,
      "p1": "696",
      "pn": "700",
      "abstract": [
        "Disordered speech recognition is a highly challenging task. The underlying\nneuro-motor conditions of people with speech disorders, often compounded\nwith co-occurring physical disabilities, lead to the difficulty in\ncollecting large quantities of speech required for system development.\nThis paper investigates a set of data augmentation techniques for disordered\nspeech recognition, including vocal tract length perturbation (VTLP),\ntempo perturbation and speed perturbation. Both normal and disordered\nspeech were exploited in the augmentation process. Variability among\nimpaired speakers in both the original and augmented data was modeled\nusing learning hidden unit contributions (LHUC) based speaker adaptive\ntraining. The final speaker adapted system constructed using the UASpeech\ncorpus and the best augmentation approach based on speed perturbation\nproduced up to 2.92% absolute (9.3% relative) word error rate (WER)\nreduction over the baseline system without data augmentation, and gave\nan overall WER of 26.37% on the test set containing 16 dysarthric speakers.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1161",
      "author_area_id": "10",
      "author_area_label": "Speech Recognition: Technologies and Systems for New Applications"
    },
    "wei20_interspeech": {
      "authors": [
        [
          "Wenqi",
          "Wei"
        ],
        [
          "Jianzong",
          "Wang"
        ],
        [
          "Jiteng",
          "Ma"
        ],
        [
          "Ning",
          "Cheng"
        ],
        [
          "Jing",
          "Xiao"
        ]
      ],
      "title": "A Real-Time Robot-Based Auxiliary System for Risk Evaluation of COVID-19 Infection",
      "original": "2105",
      "page_count": 5,
      "order": 147,
      "p1": "701",
      "pn": "705",
      "abstract": [
        "In this paper, we propose a real-time robot-based auxiliary system\nfor risk evaluation of COVID-19 infection. It combines real-time speech\nrecognition, temperature measurement, keyword detection, cough detection\nand other functions in order to convert live audio into actionable\nstructured data to achieve the COVID-19 infection risk assessment function.\nIn order to better evaluate the COVID-19 infection, we propose an end-to-end\nmethod for cough detection and classification for our proposed system.\nIt is based on real conversation data from human-robot, which processes\nspeech signals to detect cough and classifies it if detected. The structure\nof our model are maintained concise to be implemented for real-time\napplications. And we further embed this entire auxiliary diagnostic\nsystem in the robot and it is placed in the communities, hospitals\nand supermarkets to support COVID-19 testing. The system can be further\nleveraged within a business rules engine, thus serving as a foundation\nfor real-time supervision and assistance applications. Our model utilizes\na pretrained, robust training environment that allows for efficient\ncreation and customization of customer-specific health states.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2105",
      "author_area_id": "10",
      "author_area_label": "Speech Recognition: Technologies and Systems for New Applications"
    },
    "barbera20_interspeech": {
      "authors": [
        [
          "David S.",
          "Barbera"
        ],
        [
          "Mark",
          "Huckvale"
        ],
        [
          "Victoria",
          "Fleming"
        ],
        [
          "Emily",
          "Upton"
        ],
        [
          "Henry",
          "Coley-Fisher"
        ],
        [
          "Ian",
          "Shaw"
        ],
        [
          "William",
          "Latham"
        ],
        [
          "Alexander P.",
          "Leff"
        ],
        [
          "Jenny",
          "Crinion"
        ]
      ],
      "title": "An Utterance Verification System for Word Naming Therapy in Aphasia",
      "original": "2265",
      "page_count": 5,
      "order": 148,
      "p1": "706",
      "pn": "710",
      "abstract": [
        "Anomia (word finding difficulties) is the hallmark of aphasia an acquired\nlanguage disorder, most commonly caused by stroke. Assessment of speech\nperformance using picture naming tasks is therefore a key method for\nidentification of the disorder and monitoring patient&#8217;s response\nto treatment interventions. Currently, this assessment is conducted\nmanually by speech and language therapists (SLT). Surprisingly, despite\nadvancements in ASR and artificial intelligence with technologies like\ndeep learning, research on developing automated systems for this task\nhas been scarce. Here we present an utterance verification system incorporating\na deep learning element that classifies &#8216;correct&#8217;/&#8216;incorrect&#8217;\nnaming attempts from aphasic stroke patients. When tested on 8 native\nBritish-English speaking aphasics the system&#8217;s performance accuracy\nranged between 83.6% to 93.6%, with a 10 fold cross validation mean\nof 89.5%. This performance was not only significantly better than one\nof the leading commercially available ASRs (Google speech-to-text service)\nbut also comparable in some instances with two independent SLT ratings\nfor the same dataset.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2265",
      "author_area_id": "10",
      "author_area_label": "Speech Recognition: Technologies and Systems for New Applications"
    },
    "liu20d_interspeech": {
      "authors": [
        [
          "Shansong",
          "Liu"
        ],
        [
          "Xurong",
          "Xie"
        ],
        [
          "Jianwei",
          "Yu"
        ],
        [
          "Shoukang",
          "Hu"
        ],
        [
          "Mengzhe",
          "Geng"
        ],
        [
          "Rongfeng",
          "Su"
        ],
        [
          "Shi-Xiong",
          "Zhang"
        ],
        [
          "Xunying",
          "Liu"
        ],
        [
          "Helen",
          "Meng"
        ]
      ],
      "title": "Exploiting Cross-Domain Visual Feature Generation for Disordered Speech Recognition",
      "original": "2282",
      "page_count": 5,
      "order": 149,
      "p1": "711",
      "pn": "715",
      "abstract": [
        "Audio-visual speech recognition (AVSR) technologies have been successfully\napplied to a wide range of tasks. When developing AVSR systems for\ndisordered speech characterized by severe degradation of voice quality\nand large mismatch against normal, it is difficult to record large\namounts of high quality audio-visual data. In order to address this\nissue, a cross-domain visual feature generation approach is proposed\nin this paper. Audio-visual inversion DNN system constructed using\nwidely available out-of-domain audio-visual data was used to generate\nvisual features for disordered speakers for whom video data is either\nvery limited or unavailable. Experiments conducted on the UASpeech\ncorpus suggest that the proposed cross-domain visual feature generation\nbased AVSR system consistently outperformed the baseline ASR system\nand AVSR system using original visual features. An overall word error\nrate reduction of 3.6% absolute (14% relative) was obtained over the\npreviously published best system on the 8 UASpeech dysarthric speakers\nwith audio-visual data of the same task.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2282",
      "author_area_id": "10",
      "author_area_label": "Speech Recognition: Technologies and Systems for New Applications"
    },
    "lin20c_interspeech": {
      "authors": [
        [
          "Binghuai",
          "Lin"
        ],
        [
          "Liyuan",
          "Wang"
        ]
      ],
      "title": "Joint Prediction of Punctuation and Disfluency in Speech Transcripts",
      "original": "1277",
      "page_count": 5,
      "order": 150,
      "p1": "716",
      "pn": "720",
      "abstract": [
        "Spoken language transcripts generated from Automatic speech recognition\n(ASR) often contain a large portion of disfluency and lack punctuation\nsymbols. Punctuation restoration and disfluency removal of the transcripts\ncan facilitate downstream tasks such as machine translation, information\nextraction and syntactic analysis [1]. Various studies have shown the\ninfluence between these two tasks and thus performed modeling based\non a multi-task learning (MTL) framework [2, 3], which learns general\nrepresentations in the shared layers and separate representations in\nthe task-specific layers. However, task dependencies are normally ignored\nin the task-specific layers. To model the dependencies of tasks, we\npropose an attention-based structure in the task-specific layers of\nthe MTL framework incorporating the pretrained BERT (a state-of-art\nNLP-related model) [4]. Experimental results based on English IWSLT\ndataset and the Switchboard dataset show the proposed architecture\noutperforms the separate modeling methods as well as the traditional\nMTL methods.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1277",
      "author_area_id": "10",
      "author_area_label": "Speech Recognition: Technologies and Systems for New Applications"
    },
    "yi20_interspeech": {
      "authors": [
        [
          "Jiangyan",
          "Yi"
        ],
        [
          "Jianhua",
          "Tao"
        ],
        [
          "Zhengkun",
          "Tian"
        ],
        [
          "Ye",
          "Bai"
        ],
        [
          "Cunhang",
          "Fan"
        ]
      ],
      "title": "Focal Loss for Punctuation Prediction",
      "original": "1638",
      "page_count": 5,
      "order": 151,
      "p1": "721",
      "pn": "725",
      "abstract": [
        "Many approaches have been proposed to predict punctuation marks. Previous\nresults demonstrate that these methods are effective. However, there\nstill exists class imbalance problem during training. Most of the classes\nin the training set for punctuation prediction are non-punctuation\nmarks. This will affect the performance of punctuation prediction tasks.\nTherefore, this paper uses a focal loss to alleviate this issue. The\nfocal loss can down-weight easy examples and focus training on a sparse\nset of hard examples. Experiments are conducted on IWSLT2011 datasets.\nThe results show that the punctuation predicting models trained with\na focal loss obtain performance improvement over that trained with\na cross entropy loss by up to 2.7% absolute overall F<SUB>1</SUB>-score\non test set. The proposed model also outperforms previous state-of-the-art\nmodels.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1638",
      "author_area_id": "10",
      "author_area_label": "Speech Recognition: Technologies and Systems for New Applications"
    },
    "chen20d_interspeech": {
      "authors": [
        [
          "Zhuxin",
          "Chen"
        ],
        [
          "Yue",
          "Lin"
        ]
      ],
      "title": "Improving X-Vector and PLDA for Text-Dependent Speaker Verification",
      "original": "1188",
      "page_count": 5,
      "order": 152,
      "p1": "726",
      "pn": "730",
      "abstract": [
        "Recently, the pipeline consisting of an x-vector speaker embedding\nfront-end and a Probabilistic Linear Discriminant Analysis (PLDA) back-end\nhas achieved state-of-the-art results in text-independent speaker verification.\nIn this paper, we further improve the performance of x-vector and PLDA\nbased system for text-dependent speaker verification by exploring the\nchoice of layer to produce embedding and modifying the back-end training\nstrategies. In particular, we probe that x-vector based embeddings,\nspecifically the standard deviation statistics in the pooling layer,\ncontain the information related to both speaker characteristics and\nspoken content. Accordingly, we modify the back-end training labels\nby utilizing both of the speaker-id and phrase-id. A correlation-alignment-based\nPLDA adaptation is also adopted to make use of the text-independent\nlabeled data during back-end training. Experimental results on the\nSDSVC 2020 dataset show that our proposed methods achieve significant\nperformance improvement compared with the x-vector and HMM based i-vector\nbaselines.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1188",
      "author_area_id": "13",
      "author_area_label": "Special Sessions"
    },
    "zeinali20_interspeech": {
      "authors": [
        [
          "Hossein",
          "Zeinali"
        ],
        [
          "Kong Aik",
          "Lee"
        ],
        [
          "Jahangir",
          "Alam"
        ],
        [
          "Luk\u00e1\u0161",
          "Burget"
        ]
      ],
      "title": "SdSV Challenge 2020: Large-Scale Evaluation of Short-Duration Speaker Verification",
      "original": "1485",
      "page_count": 5,
      "order": 153,
      "p1": "731",
      "pn": "735",
      "abstract": [
        "Modern approaches to speaker verification represent speech utterances\nas fixed-length embeddings. With these approaches, we implicitly assume\nthat speaker characteristics are independent of the spoken content.\nSuch an assumption generally holds when sufficiently long utterances\nare given. In this context, speaker embeddings, like i-vector and x-vector,\nhave shown to be extremely effective. For speech utterances of short\nduration (in the order of a few seconds), speaker embeddings have shown\nsignificant dependency on the phonetic content. In this regard, the\n SdSV Challenge 2020 was organized with a broad focus on systematic\nbenchmark and analysis on varying degrees of phonetic variability on\nshort-duration speaker verification (SdSV). In addition to text-dependent\nand text-independent tasks, the challenge features an unusual and difficult\ntask of cross-lingual speaker verification (English vs. Persian). This\npaper describes the dataset and tasks, the evaluation rules and protocols,\nthe performance metric, baseline systems, and challenge results. We\nalso present insights gained from the evaluation and future research\ndirections.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1485",
      "author_area_id": "13",
      "author_area_label": "Special Sessions"
    },
    "jiang20_interspeech": {
      "authors": [
        [
          "Tao",
          "Jiang"
        ],
        [
          "Miao",
          "Zhao"
        ],
        [
          "Lin",
          "Li"
        ],
        [
          "Qingyang",
          "Hong"
        ]
      ],
      "title": "The XMUSPEECH System for Short-Duration Speaker Verification Challenge 2020",
      "original": "1704",
      "page_count": 5,
      "order": 154,
      "p1": "736",
      "pn": "740",
      "abstract": [
        "In this paper, we present our XMUSPEECH system for Task 1 in the Short-duration\nSpeaker Verification (SdSV) Challenge. In this challenge, Task 1 is\na Text-Dependent (TD) mode where speaker verification systems are required\nto automatically determine whether a test segment with specific phrase\nbelongs to the target speaker. We leveraged the system pipeline from\nthree aspects, including the data processing, front-end training and\nback-end processing. In addition, we have explored some training strategies\nsuch as spectrogram augmentation and transfer learning. The experimental\nresults show that the attempts we had done are effective and our best\nsingle system, a transferred model with spectrogram augmentation and\nattentive statistic pooling, significantly outperforms the official\nbaseline on both progress subset and evaluation subset. Finally, a\nfusion of seven subsystems are chosen as our primary system which yielded\n0.0856 and 0.0862 in term of minDCF, for the progress subset and evaluation\nsubset respectively.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1704",
      "author_area_id": "13",
      "author_area_label": "Special Sessions"
    },
    "mun20_interspeech": {
      "authors": [
        [
          "Sung Hwan",
          "Mun"
        ],
        [
          "Woo Hyun",
          "Kang"
        ],
        [
          "Min Hyun",
          "Han"
        ],
        [
          "Nam Soo",
          "Kim"
        ]
      ],
      "title": "Robust Text-Dependent Speaker Verification via Character-Level Information Preservation for the SdSV Challenge 2020",
      "original": "2183",
      "page_count": 5,
      "order": 155,
      "p1": "741",
      "pn": "745",
      "abstract": [
        "This paper describes our submission to Task 1 of the Short-duration\nSpeaker Verification (SdSV) challenge 2020. Task 1 is a text-dependent\nspeaker verification task, where both the speaker and phrase are required\nto be verified. The submitted systems were composed of TDNN-based and\nResNet-based front-end architectures, in which the frame-level features\nwere aggregated with various pooling methods (e.g., statistical, self-attentive,\nghostVLAD pooling). Although the conventional pooling methods provide\nembeddings with a sufficient amount of speaker-dependent information,\nour experiments show that these embeddings often lack phrase-dependent\ninformation. To mitigate this problem, we propose a new pooling and\nscore compensation methods that leverage a CTC-based automatic speech\nrecognition (ASR) model for taking the lexical content into account.\nBoth methods showed improvement over the conventional techniques, and\nthe best performance was achieved by fusing all the experimented systems,\nwhich showed 0.0785% MinDCF and 2.23% EER on the challenge&#8217;s\nevaluation subset.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2183",
      "author_area_id": "13",
      "author_area_label": "Special Sessions"
    },
    "alumae20_interspeech": {
      "authors": [
        [
          "Tanel",
          "Alum\u00e4e"
        ],
        [
          "J\u00f6rgen",
          "Valk"
        ]
      ],
      "title": "The TalTech Systems for the Short-Duration Speaker Verification Challenge 2020",
      "original": "2233",
      "page_count": 5,
      "order": 156,
      "p1": "746",
      "pn": "750",
      "abstract": [
        "This paper presents the Tallinn University of Technology systems submitted\nto the Short-duration Speaker Verification Challenge 2020. The challenge\nconsists of two tasks, focusing on text-dependent and text-independent\nspeaker verification with some cross-lingual aspects. We used speaker\nembedding models that consist of squeeze-and-attention based residual\nlayers, multi-head attention and either cross-entropy-based or additive\nangular margin based objective function. In order to encourage the\nmodel to produce language-independent embeddings, we trained the models\nin a multi-task manner, using dataset specific output layers. In the\ntext-dependent task we employed a phrase classifier to reject trials\nwith non-matching phrases. In the text-independent task we used a language\nclassifier to boost the scores of trials where the language of the\ntest and enrollment utterances does not match. Our final primary metric\nscore was 0.075 in Task 1 (ranked as 6th) and 0.118 in Task 2 (rank\n8).\n"
      ],
      "doi": "10.21437/Interspeech.2020-2233",
      "author_area_id": "13",
      "author_area_label": "Special Sessions"
    },
    "shen20b_interspeech": {
      "authors": [
        [
          "Peng",
          "Shen"
        ],
        [
          "Xugang",
          "Lu"
        ],
        [
          "Hisashi",
          "Kawai"
        ]
      ],
      "title": "Investigation of NICT Submission for Short-Duration Speaker Verification Challenge 2020",
      "original": "2351",
      "page_count": 5,
      "order": 157,
      "p1": "751",
      "pn": "755",
      "abstract": [
        "In this paper, we describe the NICT speaker verification system for\nthe text-independent task of the short-duration speaker verification\n(SdSV) challenge 2020. We firstly present the details of the training\ndata and feature preparation. Then, x-vector-based front-ends by considering\ndifferent network configurations, back-ends of probabilistic linear\ndiscriminant analysis (PLDA), simplified PLDA, cosine similarity, and\nneural network-based PLDA are investigated and explored. Finally, we\napply a greedy fusion and calibration approach to select and combine\nthe subsystems. To improve the performance of the speaker verification\nsystem on short-duration evaluation data, we introduce our investigations\non how to reduce the duration mismatch between training and test datasets.\nExperimental results showed that our primary fusion yielded minDCF\nof 0.074 and EER of 1.50 on the evaluation subset, which was the 2nd\nbest result in the text-independent speaker verification task.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2351",
      "author_area_id": "13",
      "author_area_label": "Special Sessions"
    },
    "thienpondt20_interspeech": {
      "authors": [
        [
          "Jenthe",
          "Thienpondt"
        ],
        [
          "Brecht",
          "Desplanques"
        ],
        [
          "Kris",
          "Demuynck"
        ]
      ],
      "title": "Cross-Lingual Speaker Verification with Domain-Balanced Hard Prototype Mining and Language-Dependent Score Normalization",
      "original": "2662",
      "page_count": 5,
      "order": 158,
      "p1": "756",
      "pn": "760",
      "abstract": [
        "In this paper we describe the top-scoring IDLab submission for the\ntext-independent task of the Short-duration Speaker Verification (SdSV)\nChallenge 2020. The main difficulty of the challenge exists in the\nlarge degree of varying phonetic overlap between the potentially cross-lingual\ntrials, along with the limited availability of in-domain DeepMine Farsi\ntraining data. We introduce domain-balanced hard prototype mining to\nfinetune the state-of-the-art ECAPA-TDNN x-vector based speaker embedding\nextractor. The sample mining technique efficiently exploits speaker\ndistances between the speaker prototypes of the popular AAM-softmax\nloss function to construct challenging training batches that are balanced\non the domain-level. To enhance the scoring of cross-lingual trials,\nwe propose a language-dependent s-norm score normalization. The imposter\ncohort only contains data from the Farsi target-domain which simulates\nthe enrollment data always being Farsi. In case a Gaussian-Backend\nlanguage model detects the test speaker embedding to contain English,\na cross-language compensation offset determined on the AAM-softmax\nspeaker prototypes is subtracted from the maximum expected imposter\nmean score. A fusion of five systems with minor topological tweaks\nresulted in a final MinDCF and EER of 0.065 and 1.45% respectively\non the SdSVC evaluation set.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2662",
      "author_area_id": "13",
      "author_area_label": "Special Sessions"
    },
    "lozanodiez20_interspeech": {
      "authors": [
        [
          "Alicia",
          "Lozano-Diez"
        ],
        [
          "Anna",
          "Silnova"
        ],
        [
          "Bhargav",
          "Pulugundla"
        ],
        [
          "Johan",
          "Rohdin"
        ],
        [
          "Karel",
          "Vesel\u00fd"
        ],
        [
          "Luk\u00e1\u0161",
          "Burget"
        ],
        [
          "Old\u0159ich",
          "Plchot"
        ],
        [
          "Ond\u0159ej",
          "Glembek"
        ],
        [
          "Ondvrej",
          "Novotn\u00fd"
        ],
        [
          "Pavel",
          "Mat\u011bjka"
        ]
      ],
      "title": "BUT Text-Dependent Speaker Verification System for SdSV Challenge 2020",
      "original": "2882",
      "page_count": 5,
      "order": 159,
      "p1": "761",
      "pn": "765",
      "abstract": [
        "In this paper, we present the winning BUT submission for the text-dependent\ntask of the SdSV challenge 2020. Given the large amount of training\ndata available in this challenge, we explore successful techniques\nfrom text-independent systems in the text-dependent scenario. In particular,\nwe trained x-vector extractors on both in-domain and out-of-domain\ndatasets and combine them with i-vectors trained on concatenated MFCCs\nand bottleneck features, which have proven effective for the text-dependent\nscenario. Moreover, we proposed the use of phrase-dependent PLDA backend\nfor scoring and its combination with a simple phrase recognizer, which\nbrings up to 63% relative improvement on our development set with respect\nto using standard PLDA. Finally, we combine our different i-vector\nand x-vector based systems using a simple linear logistic regression\nscore level fusion, which provides 28% relative improvement on the\nevaluation set with respect to our best single system.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2882",
      "author_area_id": "13",
      "author_area_label": "Special Sessions"
    },
    "ravi20_interspeech": {
      "authors": [
        [
          "Vijay",
          "Ravi"
        ],
        [
          "Ruchao",
          "Fan"
        ],
        [
          "Amber",
          "Afshan"
        ],
        [
          "Huanhua",
          "Lu"
        ],
        [
          "Abeer",
          "Alwan"
        ]
      ],
      "title": "Exploring the Use of an Unsupervised Autoregressive Model as a Shared Encoder for Text-Dependent Speaker Verification",
      "original": "2957",
      "page_count": 5,
      "order": 160,
      "p1": "766",
      "pn": "770",
      "abstract": [
        "In this paper, we propose a novel way of addressing text-dependent\nautomatic speaker verification (TD-ASV) by using a shared-encoder with\ntask-specific decoders. An autoregressive predictive coding (APC) encoder\nis pre-trained in an unsupervised manner using both out-of-domain (LibriSpeech,\nVoxCeleb) and in-domain (DeepMine) unlabeled datasets to learn generic,\nhigh-level feature representation that encapsulates speaker and phonetic\ncontent. Two task-specific decoders were trained using labeled datasets\nto classify speakers (SID) and phrases (PID). Speaker embeddings extracted\nfrom the SID decoder were scored using a PLDA. SID and PID systems\nwere fused at the score level. There is a 51.9% relative improvement\nin minDCF for our system compared to the fully supervised x-vector\nbaseline on the cross-lingual DeepMine dataset. However, the i-vector/HMM\nmethod outperformed the proposed APC encoder-decoder system. A fusion\nof the x-vector/PLDA baseline and the SID/PLDA scores prior to PID\nfusion further improved performance by 15% indicating complementarity\nof the proposed approach to the x-vector system. We show that the proposed\napproach can leverage from large, unlabeled, data-rich domains, and\nlearn speech patterns independent of downstream tasks. Such a system\ncan provide competitive performance in domain-mismatched scenarios\nwhere test data is from data-scarce domains.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2957",
      "author_area_id": "13",
      "author_area_label": "Special Sessions"
    },
    "zhang20d_interspeech": {
      "authors": [
        [
          "Jing-Xuan",
          "Zhang"
        ],
        [
          "Zhen-Hua",
          "Ling"
        ],
        [
          "Li-Rong",
          "Dai"
        ]
      ],
      "title": "Recognition-Synthesis Based Non-Parallel Voice Conversion with Adversarial Learning",
      "original": "0036",
      "page_count": 5,
      "order": 161,
      "p1": "771",
      "pn": "775",
      "abstract": [
        "This paper presents an adversarial learning method for recognition-synthesis\nbased non-parallel voice conversion. A recognizer is used to transform\nacoustic features into linguistic representations while a synthesizer\nrecovers output features from the recognizer outputs together with\nthe speaker identity. By separating the speaker characteristics from\nthe linguistic representations, voice conversion can be achieved by\nreplacing the speaker identity with the target one. In our proposed\nmethod, a speaker adversarial loss is adopted in order to obtain speaker-independent\nlinguistic representations using the recognizer. Furthermore, discriminators\nare introduced and a generative adversarial network (GAN) loss is used\nto prevent the predicted features from being over-smoothed. For training\nmodel parameters, a strategy of pre-training on a multi-speaker dataset\nand then fine-tuning on the source-target speaker pair is designed.\nOur method achieved higher similarity than the baseline model that\nobtained the best performance in Voice Conversion Challenge 2018.\n"
      ],
      "doi": "10.21437/Interspeech.2020-36",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "ding20_interspeech": {
      "authors": [
        [
          "Shaojin",
          "Ding"
        ],
        [
          "Guanlong",
          "Zhao"
        ],
        [
          "Ricardo",
          "Gutierrez-Osuna"
        ]
      ],
      "title": "Improving the Speaker Identity of Non-Parallel Many-to-Many Voice Conversion with Adversarial Speaker Recognition",
      "original": "1033",
      "page_count": 5,
      "order": 162,
      "p1": "776",
      "pn": "780",
      "abstract": [
        "Phonetic Posteriorgrams (PPGs) have received much attention for non-parallel\nmany-to-many Voice Conversion (VC), and have been shown to achieve\nstate-of-the-art performance. These methods implicitly assume that\nPPGs are speaker-independent and contain only linguistic information\nin an utterance. In practice, however, PPGs carry speaker individuality\ncues, such as accent, intonation, and speaking rate. As a result, these\ncues can leak into the voice conversion, making it sound similar to\nthe source speaker. To address this issue, we propose an adversarial\nlearning approach that can remove speaker-dependent information in\nVC models based on a PPG2speech synthesizer. During training, the encoder\noutput of a PPG2speech synthesizer is fed to a classifier trained to\nidentify the corresponding speaker, while the encoder is trained to\nfool the classifier. As a result, a more speaker-independent representation\nis learned. The proposed method is advantageous as it does not require\npre-training the speaker classifier, and the adversarial speaker classifier\nis jointly trained with the PPG2speech synthesizer end-to-end. We conduct\nobjective and subjective experiments on the CSTR VCTK Corpus under\nstandard and one-shot VC conditions. Results show that the proposed\nmethod significantly improves the speaker identity of VC syntheses\nwhen compared with a baseline system trained without adversarial learning.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1033",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "li20i_interspeech": {
      "authors": [
        [
          "Yanping",
          "Li"
        ],
        [
          "Dongxiang",
          "Xu"
        ],
        [
          "Yan",
          "Zhang"
        ],
        [
          "Yang",
          "Wang"
        ],
        [
          "Binbin",
          "Chen"
        ]
      ],
      "title": "Non-Parallel Many-to-Many Voice Conversion with PSR-StarGAN",
      "original": "1310",
      "page_count": 5,
      "order": 163,
      "p1": "781",
      "pn": "785",
      "abstract": [
        "Voice Conversion (VC) aims at modifying source speaker&#8217;s speech\nto sound like that of target speaker while preserving linguistic information\nof given speech. StarGAN-VC was recently proposed, which utilizes a\nvariant of Generative Adversarial Networks (GAN) to perform non-parallel\nmany-to-many VC. However, the quality of generated speech is not satisfactory\nenough. An improved method named &#8220;PSR-StarGAN-VC&#8221; is proposed\nin this paper by incorporating three improvements. Firstly, perceptual\nloss functions are introduced to optimize the generator in StarGAN-VC\naiming to learn high-level spectral features. Secondly, considering\nthat Switchable Normalization (SN) could learn different operations\nin different normalization layers of model, it is introduced to replace\nBatch Normalization (BN) in StarGAN-VC. Lastly, Residual Network (ResNet)\nis applied to establish the mapping of different layers between the\nencoder and decoder of generator aiming to retain more semantic features\nwhen converting speech, and to reduce the difficulty of training. Experiment\nresults on the VCC 2018 datasets demonstrate superiority of the proposed\nmethod in terms of naturalness and speaker similarity.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1310",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "polyak20_interspeech": {
      "authors": [
        [
          "Adam",
          "Polyak"
        ],
        [
          "Lior",
          "Wolf"
        ],
        [
          "Yaniv",
          "Taigman"
        ]
      ],
      "title": "TTS Skins: Speaker Conversion via ASR",
      "original": "1416",
      "page_count": 5,
      "order": 164,
      "p1": "786",
      "pn": "790",
      "abstract": [
        "We present a fully convolutional wav-to-wav network for converting\nbetween speakers&#8217; voices, without relying on text. Our network\nis based on an encoder-decoder architecture, where the encoder is pre-trained\nfor the task of Automatic Speech Recognition, and a multi-speaker waveform\ndecoder is trained to reconstruct the original signal in an autoregressive\nmanner. We train the network on narrated audiobooks, and demonstrate\nmulti-voice TTS in those voices, by converting the voice of a TTS robot.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1416",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "zhang20e_interspeech": {
      "authors": [
        [
          "Zining",
          "Zhang"
        ],
        [
          "Bingsheng",
          "He"
        ],
        [
          "Zhenjie",
          "Zhang"
        ]
      ],
      "title": "GAZEV: GAN-Based Zero-Shot Voice Conversion Over Non-Parallel Speech Corpus",
      "original": "1710",
      "page_count": 5,
      "order": 165,
      "p1": "791",
      "pn": "795",
      "abstract": [
        "Non-parallel many-to-many voice conversion is recently attracting huge\nresearch efforts in the speech processing community. A voice conversion\nsystem transforms an utterance of a source speaker to another utterance\nof a target speaker by keeping the content in the original utterance\nand replacing by the vocal features from the target speaker. Existing\nsolutions, e.g., StarGAN-VC2, present promising results,  only when\nspeech corpus of the engaged speakers is available during model training.\nAUTOVC is able to perform voice conversion on unseen speakers, but\nit needs an external pretrained speaker verification model. In this\npaper, we present our new GAN-based zero-shot voice conversion solution,\ncalled GAZEV, which targets to support unseen speakers on both source\nand target utterances. Our key technical contribution is the adoption\nof speaker embedding loss on top of the GAN framework, as well as adaptive\ninstance normalization strategy, in order to address the limitations\nof speaker identity transfer in existing solutions. Our empirical evaluations\ndemonstrate significant performance improvement on output speech quality,\nand comparable speaker similarity to AUTOVC.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1710",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "wang20g_interspeech": {
      "authors": [
        [
          "Tao",
          "Wang"
        ],
        [
          "Jianhua",
          "Tao"
        ],
        [
          "Ruibo",
          "Fu"
        ],
        [
          "Jiangyan",
          "Yi"
        ],
        [
          "Zhengqi",
          "Wen"
        ],
        [
          "Rongxiu",
          "Zhong"
        ]
      ],
      "title": "Spoken Content and Voice Factorization for Few-Shot Speaker Adaptation",
      "original": "1745",
      "page_count": 5,
      "order": 166,
      "p1": "796",
      "pn": "800",
      "abstract": [
        "The low similarity and naturalness of synthesized speech remain a challenging\nproblem for speaker adaptation with few resources. Since the acoustic\nmodel is too complex to interpret, overfitting will occur when training\nwith few data. To prevent the model from overfitting, this paper proposes\na novel speaker adaptation framework that decomposes the parameter\nspace of the end-to-end acoustic model into two parts, with the one\non predicting spoken content and the other on modeling speaker&#8217;s\nvoice. The spoken content is represented by phone posteriorgram (PPG)\nwhich is speaker independent. By adapting the two sub-modules separately,\nthe overfitting can be alleviated effectively. Moreover, we propose\ntwo different adaptation strategies based on whether the data has text\nannotation. In this way, speaker adaptation can also be performed without\ntext annotations. Experimental results confirm the adaptability of\nour proposed method of factorizating spoken content and voice. Listening\ntests demonstrate that our proposed method can achieve better performance\nwith just 10 sentences than speaker adaptation conducted on Tacotron\nin terms of naturalness and speaker similarity.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1745",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "polyak20b_interspeech": {
      "authors": [
        [
          "Adam",
          "Polyak"
        ],
        [
          "Lior",
          "Wolf"
        ],
        [
          "Yossi",
          "Adi"
        ],
        [
          "Yaniv",
          "Taigman"
        ]
      ],
      "title": "Unsupervised Cross-Domain Singing Voice Conversion",
      "original": "1862",
      "page_count": 5,
      "order": 167,
      "p1": "801",
      "pn": "805",
      "abstract": [
        "We present a wav-to-wav generative model for the task of singing voice\nconversion from any identity. Our method utilizes both an acoustic\nmodel, trained for the task of automatic speech recognition, together\nwith melody extracted features to drive a waveform-based generator.\nThe proposed generative architecture is invariant to the speaker&#8217;s\nidentity and can be trained to generate target singers from unlabeled\ntraining data, using either speech or singing sources. The model is\noptimized in an end-to-end fashion without any manual supervision,\nsuch as lyrics, musical notes or parallel samples. The proposed approach\nis fully-convolutional and can generate audio in realtime. Experiments\nshow that our method significantly outperforms the baseline methods\nwhile generating convincingly better audio samples than alternative\nattempts.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1862",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "ishihara20_interspeech": {
      "authors": [
        [
          "Tatsuma",
          "Ishihara"
        ],
        [
          "Daisuke",
          "Saito"
        ]
      ],
      "title": "Attention-Based Speaker Embeddings for One-Shot Voice Conversion",
      "original": "2512",
      "page_count": 5,
      "order": 168,
      "p1": "806",
      "pn": "810",
      "abstract": [
        "This paper proposes a novel approach to embed speaker information to\nfeature vectors at frame level using an attention mechanism, and its\napplication to one-shot voice conversion. A one-shot voice conversion\nsystem is a type of voice conversion system where only one utterance\nfrom a target speaker is available for conversion. In many one-shot\nvoice conversion systems, a speaker encoder mechanism compresses an\nutterance of the target speaker into a fixed-size vector for propagating\nspeaker information. However, the obtained representation has lost\ntemporal information related to speaker identities and it could degrade\nconversion quality. To alleviate this problem, we propose a novel way\nto embed speaker information using an attention mechanism. Instead\nof compressing into a fixed-size vector, our proposed speaker encoder\noutputs a sequence of speaker embedding vectors. The obtained sequence\nis selectively combined with input frames of a source speaker by an\nattention mechanism. Finally the obtained time varying speaker information\nis utilized for a decoder to generate the converted features. Objective\nevaluation showed that our method reduced the averaged mel-cepstrum\ndistortion to 5.23 dB from 5.34 dB compared with the baseline system.\nThe subjective preference test showed that our proposed system outperformed\nthe baseline one.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2512",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "cong20_interspeech": {
      "authors": [
        [
          "Jian",
          "Cong"
        ],
        [
          "Shan",
          "Yang"
        ],
        [
          "Lei",
          "Xie"
        ],
        [
          "Guoqiao",
          "Yu"
        ],
        [
          "Guanglu",
          "Wan"
        ]
      ],
      "title": "Data Efficient Voice Cloning from Noisy Samples with Domain Adversarial Training",
      "original": "2530",
      "page_count": 5,
      "order": 169,
      "p1": "811",
      "pn": "815",
      "abstract": [
        "Data efficient voice cloning aims at synthesizing target speaker&#8217;s\nvoice with only a few enrollment samples at hand. To this end, speaker\nadaptation and speaker encoding are two typical methods based on base\nmodel trained from multiple speakers. The former uses a small set of\ntarget speaker data to transfer the multi-speaker model to target speaker&#8217;s\nvoice through direct model update, while in the latter, only a few\nseconds of target speaker&#8217;s audio directly goes through an extra\nspeaker encoding model along with the multi-speaker model to synthesize\ntarget speaker&#8217;s voice without model update. Nevertheless, the\ntwo methods need clean target speaker data. However, the samples provided\nby user may inevitably contain acoustic noise in real applications.\nIt&#8217;s still challenging to generating target voice with noisy\ndata. In this paper, we study the data efficient voice cloning problem\nfrom noisy samples under the sequence-to-sequence based TTS paradigm.\nSpecifically, we introduce domain adversarial training (DAT) to speaker\nadaptation and speaker encoding, which aims to disentangle noise from\nspeech-noise mixture. Experiments show that for both speaker adaptation\nand encoding, the proposed approaches can consistently synthesize clean\nspeech from noisy speaker samples, apparently outperforming the method\nadopting state-of-the-art speech enhancement module.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2530",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "hong20_interspeech": {
      "authors": [
        [
          "Sixin",
          "Hong"
        ],
        [
          "Yuexian",
          "Zou"
        ],
        [
          "Wenwu",
          "Wang"
        ]
      ],
      "title": "Gated Multi-Head Attention Pooling for Weakly Labelled Audio Tagging",
      "original": "1197",
      "page_count": 5,
      "order": 170,
      "p1": "816",
      "pn": "820",
      "abstract": [
        "Multiple instance learning (MIL) has recently been used for weakly\nlabelled audio tagging, where the spectrogram of an audio signal is\ndivided into segments to form instances in a bag, and then the low-dimensional\nfeatures of these segments are pooled for tagging. The choice of a\npooling scheme is the key to exploiting the weakly labelled data. However,\nthe traditional pooling schemes are usually fixed and unable to distinguish\nthe contributions, making it difficult to adapt to the characteristics\nof the sound events. In this paper, a novel pooling algorithm is proposed\nfor MIL, named gated multi-head attention pooling (GMAP), which is\nable to attend to the information of events from different heads at\ndifferent positions. Each head allows the model to learn information\nfrom different representation subspaces. Furthermore, in order to avoid\nthe redundancy of multi-head information, a gating mechanism is used\nto fuse individual head features. The proposed GMAP increases the modeling\npower of the single-head attention with no computational overhead.\nExperiments are carried out on Audioset, which is a large-scale weakly\nlabelled dataset, and show superior results to the non-adaptive pooling\nand the vanilla attention pooling schemes.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1197",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "wang20h_interspeech": {
      "authors": [
        [
          "Helin",
          "Wang"
        ],
        [
          "Yuexian",
          "Zou"
        ],
        [
          "Dading",
          "Chong"
        ],
        [
          "Wenwu",
          "Wang"
        ]
      ],
      "title": "Environmental Sound Classification with Parallel Temporal-Spectral Attention",
      "original": "1219",
      "page_count": 5,
      "order": 171,
      "p1": "821",
      "pn": "825",
      "abstract": [
        "Convolutional neural networks (CNN) are one of the best-performing\nneural network architectures for environmental sound classification\n(ESC). Recently, temporal attention mechanisms have been used in CNN\nto capture the useful information from the relevant time frames for\naudio classification, especially for weakly labelled data where the\nonset and offset times of the sound events are not applied. In these\nmethods, however, the inherent spectral characteristics and variations\nare not explicitly exploited when obtaining the deep features. In this\npaper, we propose a novel parallel temporal-spectral attention mechanism\nfor CNN to learn discriminative sound representations, which enhances\nthe temporal and spectral features by capturing the importance of different\ntime frames and frequency bands. Parallel branches are constructed\nto allow temporal attention and spectral attention to be applied respectively\nin order to mitigate interference from the segments without the presence\nof sound events. The experiments on three environmental sound classification\n(ESC) datasets and two acoustic scene classification (ASC) datasets\nshow that our method improves the classification performance and also\nexhibits robustness to noise.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1219",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "wang20i_interspeech": {
      "authors": [
        [
          "Luyu",
          "Wang"
        ],
        [
          "Kazuya",
          "Kawakami"
        ],
        [
          "Aaron van den",
          "Oord"
        ]
      ],
      "title": "Contrastive Predictive Coding of Audio with an Adversary",
      "original": "1891",
      "page_count": 5,
      "order": 172,
      "p1": "826",
      "pn": "830",
      "abstract": [
        "With the vast amount of audio data available, powerful sound representations\ncan be learned with self-supervised methods even in the absence of\nexplicit annotations. In this work we investigate learning general\naudio representations directly from raw signals using the Contrastive\nPredictive Coding objective. We further extend it by leveraging ideas\nfrom adversarial machine learning to produce additive perturbations\nthat effectively makes the learning harder, such that the predictive\ntasks will not be distracted by trivial details. We also look at the\neffects of different design choices for the objective, including the\nnonlinear similarity measure and the way the negatives are drawn. Combining\nthese contributions our models are able to considerably outperform\nprevious spectrogram-based unsupervised methods. On AudioSet we observe\na relative improvement of 14% in mean average precision over the state\nof the art with half the size of the training data.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1891",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "pankajakshan20_interspeech": {
      "authors": [
        [
          "Arjun",
          "Pankajakshan"
        ],
        [
          "Helen L.",
          "Bear"
        ],
        [
          "Vinod",
          "Subramanian"
        ],
        [
          "Emmanouil",
          "Benetos"
        ]
      ],
      "title": "Memory Controlled Sequential Self Attention for Sound Recognition",
      "original": "1953",
      "page_count": 5,
      "order": 173,
      "p1": "831",
      "pn": "835",
      "abstract": [
        "In this paper we investigate the importance of the extent of memory\nin sequential self attention for sound recognition. We propose to use\na memory controlled sequential self attention mechanism on top of a\nconvolutional recurrent neural network (CRNN) model for polyphonic\nsound event detection (SED). Experiments on the URBAN-SED dataset demonstrate\nthe impact of the extent of memory on sound recognition performance\nwith the self attention induced SED model. We extend the proposed idea\nwith a multi-head self attention mechanism where each attention head\nprocesses the audio embedding with explicit attention width values.\nThe proposed use of memory controlled sequential self attention offers\na way to induce relations among frames of sound event tokens. We show\nthat our memory controlled self attention model achieves an event based\nF-score of 33.92% on the URBAN-SED dataset, outperforming the F-score\nof 20.10% reported by the model without self attention.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1953",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "kim20_interspeech": {
      "authors": [
        [
          "Donghyeon",
          "Kim"
        ],
        [
          "Jaihyun",
          "Park"
        ],
        [
          "David K.",
          "Han"
        ],
        [
          "Hanseok",
          "Ko"
        ]
      ],
      "title": "Dual Stage Learning Based Dynamic Time-Frequency Mask Generation for Audio Event Classification",
      "original": "2152",
      "page_count": 5,
      "order": 174,
      "p1": "836",
      "pn": "840",
      "abstract": [
        "Audio based event recognition becomes quite challenging in real world\nnoisy environments. To alleviate the noise issue, time-frequency mask\nbased feature enhancement methods have been proposed. While these methods\nwith fixed filter settings have been shown to be effective in familiar\nnoise backgrounds, they become brittle when exposed to unexpected noise.\nTo address the unknown noise problem, we develop an approach based\non dynamic filter generation learning. In particular, we propose a\ndual stage dynamic filter generator networks that can be trained to\ngenerate a time-frequency mask specifically created for each input\naudio. Two alternative approaches of training the mask generator network\nare developed for feature enhancements in high noise environments.\nOur proposed method shows improved performance and robustness in both\nclean and unseen noise environments.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2152",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "zheng20_interspeech": {
      "authors": [
        [
          "Xu",
          "Zheng"
        ],
        [
          "Yan",
          "Song"
        ],
        [
          "Jie",
          "Yan"
        ],
        [
          "Li-Rong",
          "Dai"
        ],
        [
          "Ian",
          "McLoughlin"
        ],
        [
          "Lin",
          "Liu"
        ]
      ],
      "title": "An Effective Perturbation Based Semi-Supervised Learning Method for Sound Event Detection",
      "original": "2329",
      "page_count": 5,
      "order": 175,
      "p1": "841",
      "pn": "845",
      "abstract": [
        "Mean teacher based methods are increasingly achieving state-of-the-art\nperformance for large-scale weakly labeled and unlabeled sound event\ndetection (SED) tasks in recent DCASE challenges. By penalizing inconsistent\npredictions under different perturbations, mean teacher methods can\nexploit large-scale unlabeled data in a self-ensembling manner. In\nthis paper, an effective perturbation based semi-supervised learning\n(SSL) method is proposed based on the mean teacher method. Specifically,\na new independent component (IC) module is proposed to introduce perturbations\nfor different convolutional layers, designed as a combination of batch\nnormalization and dropblock operations. The proposed IC module can\nreduce correlation between neurons to improve performance. A global\nstatistics pooling based attention module is further proposed to explicitly\nmodel inter-dependencies between the time-frequency domain and channels,\nusing statistics information (e.g. mean, standard deviation, max) along\ndifferent dimensions. This can provide an effective attention mechanism\nto adaptively re-calibrate the output feature map. Experimental results\non Task 4 of the DCASE2018 challenge demonstrate the superiority of\nthe proposed method, achieving about 39.8% F1-score, outperforming\nthe previous winning system&#8217;s 32.4% by a significant margin.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2329",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "kao20_interspeech": {
      "authors": [
        [
          "Chieh-Chi",
          "Kao"
        ],
        [
          "Bowen",
          "Shi"
        ],
        [
          "Ming",
          "Sun"
        ],
        [
          "Chao",
          "Wang"
        ]
      ],
      "title": "A Joint Framework for Audio Tagging and Weakly Supervised Acoustic Event Detection Using DenseNet with Global Average Pooling",
      "original": "2791",
      "page_count": 5,
      "order": 176,
      "p1": "846",
      "pn": "850",
      "abstract": [
        "This paper proposes a network architecture mainly designed for audio\ntagging, which can also be used for weakly supervised acoustic event\ndetection (AED). The proposed network consists of a modified DenseNet\nas the feature extractor, and a global average pooling (GAP) layer\nto predict frame-level labels at inference time. This architecture\nis inspired by the work proposed by Zhou et al., a well-known framework\nusing GAP to localize visual objects given image-level labels. While\nmost of the previous works on weakly supervised AED used recurrent\nlayers with attention-based mechanism to localize acoustic events,\nthe proposed network directly localizes events using the feature map\nextracted by DenseNet without any recurrent layers. In the audio tagging\ntask of DCASE 2017, our method significantly outperforms the state-of-the-art\nmethod in F1 score by 5.3% on the dev set, and 6.0% on the eval set\nin terms of absolute values. For weakly supervised AED task in DCASE\n2018, our model outperforms the state-of-the-art method in event-based\nF1 by 8.1% on the dev set, and 0.5% on the eval set in terms of absolute\nvalues, by using data augmentation and tri-training to leverage unlabeled\ndata.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2791",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "chang20_interspeech": {
      "authors": [
        [
          "Chun-Chieh",
          "Chang"
        ],
        [
          "Chieh-Chi",
          "Kao"
        ],
        [
          "Ming",
          "Sun"
        ],
        [
          "Chao",
          "Wang"
        ]
      ],
      "title": "Intra-Utterance Similarity Preserving Knowledge Distillation for Audio Tagging",
      "original": "2835",
      "page_count": 5,
      "order": 177,
      "p1": "851",
      "pn": "855",
      "abstract": [
        "Knowledge Distillation (KD) is a popular area of research for reducing\nthe size of large models while still maintaining good performance.\nThe outputs of larger teacher models are used to guide the training\nof smaller student models. Given the repetitive nature of acoustic\nevents, we propose to leverage this information to regulate the KD\ntraining for Audio Tagging. This novel KD method, Intra-Utterance Similarity\nPreserving KD (IUSP), shows promising results for the audio tagging\ntask. It is motivated by the previously published KD method: Similarity\nPreserving KD (SP). However, instead of preserving the pairwise similarities\nbetween inputs within a mini-batch, our method preserves the pairwise\nsimilarities between the frames of a single input utterance. Our proposed\nKD method, IUSP, shows consistent improvements over SP across student\nmodels of different sizes on the DCASE 2019 Task 5 dataset for audio\ntagging. There is a 27.1% to 122.4% percent increase in improvement\nof micro AUPRC over the baseline relative to SPs improvement of over\nthe baseline.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2835",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "park20b_interspeech": {
      "authors": [
        [
          "Inyoung",
          "Park"
        ],
        [
          "Hong Kook",
          "Kim"
        ]
      ],
      "title": "Two-Stage Polyphonic Sound Event Detection Based on Faster R-CNN-LSTM with Multi-Token Connectionist Temporal Classification",
      "original": "3097",
      "page_count": 5,
      "order": 178,
      "p1": "856",
      "pn": "860",
      "abstract": [
        "We propose a two-stage sound event detection (SED) model to deal with\nsound events overlapping in time-frequency. In the first stage which\nconsists of a faster R-CNN and an attention-LSTM, each log-mel spectrogram\nsegment is divided into one or more proposed regions (PRs) according\nto the coordinates of a region proposal network. To efficiently train\npolyphonic sound, we take only one PR for each sound event from a bounding\nbox regressor associated with the attention-LSTM. In the second stage,\nthe original input image and the difference image between adjacent\nsegments are separately pooled according to the coordinate of each\nPR predicted in the first stage. Then, two feature maps using CNNs\nare concatenated and processed further by LSTM. Finally, CTC-based\nn-best SED is conducted using the softmax output from the CNN-LSTM,\nwhere CTC has two tokens for each event so that the start and ending\ntime frames are accurately detected. Experiments on SED using DCASE\n2019 Task 3 show that the proposed two-stage model with multi-token\nCTC achieves an F1-score of 97.5%, while the first stage alone and\nthe two-stage model with a conventional CTC yield F1-scores of 91.9%\nand 95.6%, respectively.\n"
      ],
      "doi": "10.21437/Interspeech.2020-3097",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "jindal20_interspeech": {
      "authors": [
        [
          "Amit",
          "Jindal"
        ],
        [
          "Narayanan Elavathur",
          "Ranganatha"
        ],
        [
          "Aniket",
          "Didolkar"
        ],
        [
          "Arijit Ghosh",
          "Chowdhury"
        ],
        [
          "Di",
          "Jin"
        ],
        [
          "Ramit",
          "Sawhney"
        ],
        [
          "Rajiv Ratn",
          "Shah"
        ]
      ],
      "title": "SpeechMix &#8212; Augmenting Deep Sound Recognition Using Hidden Space Interpolations",
      "original": "3147",
      "page_count": 5,
      "order": 179,
      "p1": "861",
      "pn": "865",
      "abstract": [
        "This paper presents SpeechMix, a regularization and data augmentation\ntechnique for deep sound recognition. Our strategy is to create virtual\ntraining samples by interpolating speech samples in hidden space. SpeechMix\nhas the potential to generate an infinite number of new augmented speech\nsamples since the combination of speech samples is continuous. Thus,\nit allows downstream models to avoid overfitting drastically. Unlike\nother mixing strategies that only work on the input space, we apply\nour method on the intermediate layers to capture a broader representation\nof the feature space. Through an extensive quantitative evaluation,\nwe demonstrate the effectiveness of SpeechMix in comparison to standard\nlearning regimes and previously applied mixing strategies. Furthermore,\nwe highlight how different hidden layers contribute to the improvements\nin classification using an ablation study.\n"
      ],
      "doi": "10.21437/Interspeech.2020-3147",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "radfar20_interspeech": {
      "authors": [
        [
          "Martin",
          "Radfar"
        ],
        [
          "Athanasios",
          "Mouchtaris"
        ],
        [
          "Siegfried",
          "Kunzmann"
        ]
      ],
      "title": "End-to-End Neural Transformer Based Spoken Language Understanding",
      "original": "1963",
      "page_count": 5,
      "order": 180,
      "p1": "866",
      "pn": "870",
      "abstract": [
        "Spoken language understanding (SLU) refers to the process of inferring\nthe semantic information from audio signals. While the neural transformers\nconsistently deliver the best performance among the state-of-the-art\nneural architectures in field of natural language processing (NLP),\ntheir merits in a closely related field, i.e., spoken language understanding\n(SLU) have not been investigated. In this paper, we introduce an end-to-end\nneural transformer-based SLU model that can predict the variable-length\ndomain, intent, and slots vectors embedded in an audio signal with\nno intermediate token prediction architecture. This new architecture\nleverages the self-attention mechanism by which the audio signal is\ntransformed to various sub-subspaces allowing to extract the semantic\ncontext implied by an utterance. Our end-to-end transformer SLU predicts\nthe domains, intents and slots in the Fluent Speech Commands dataset\nwith accuracy equal to 98.1%, 99.6%, and 99.6%, respectively and outperforms\nthe SLU models that leverage a combination of recurrent and convolutional\nneural networks by 1.4% while the size of our model is 25% smaller\nthan that of these architectures. Additionally, due to independent\nsub-space projections in the self-attention layer, the model is highly\nparallelizable which makes it a good candidate for on-device SLU.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1963",
      "author_area_id": "11",
      "author_area_label": "Spoken dialog systems and conversational analysis"
    },
    "liu20e_interspeech": {
      "authors": [
        [
          "Chen",
          "Liu"
        ],
        [
          "Su",
          "Zhu"
        ],
        [
          "Zijian",
          "Zhao"
        ],
        [
          "Ruisheng",
          "Cao"
        ],
        [
          "Lu",
          "Chen"
        ],
        [
          "Kai",
          "Yu"
        ]
      ],
      "title": "Jointly Encoding Word Confusion Network and Dialogue Context with BERT for Spoken Language Understanding",
      "original": "1632",
      "page_count": 5,
      "order": 181,
      "p1": "871",
      "pn": "875",
      "abstract": [
        "Spoken Language Understanding (SLU) converts hypotheses from automatic\nspeech recognizer (ASR) into structured semantic representations. ASR\nrecognition errors can severely degenerate the performance of the subsequent\nSLU module. To address this issue, word confusion networks (WCNs) have\nbeen used as the input for SLU, which contain richer information than\n1-best or n-best hypotheses list. To further eliminate ambiguity, the\nlast system act of dialogue context is also utilized as additional\ninput. In this paper, a novel BERT based SLU model (WCN-BERT SLU) is\nproposed to encode WCNs and the dialogue context jointly. It can integrate\nboth structural information and ASR posterior probabilities of WCNs\nin the BERT architecture. Experiments on DSTC2, a benchmark of SLU,\nshow that the proposed method is effective and can outperform previous\nstate-of-the-art models significantly.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1632",
      "author_area_id": "11",
      "author_area_label": "Spoken dialog systems and conversational analysis"
    },
    "rao20_interspeech": {
      "authors": [
        [
          "Milind",
          "Rao"
        ],
        [
          "Anirudh",
          "Raju"
        ],
        [
          "Pranav",
          "Dheram"
        ],
        [
          "Bach",
          "Bui"
        ],
        [
          "Ariya",
          "Rastrow"
        ]
      ],
      "title": "Speech to Semantics: Improve ASR and NLU Jointly via All-Neural Interfaces",
      "original": "2976",
      "page_count": 5,
      "order": 182,
      "p1": "876",
      "pn": "880",
      "abstract": [
        "We consider the problem of spoken language understanding (SLU) of extracting\nnatural language intents and associated slot arguments or named entities\nfrom speech that is primarily directed at voice assistants. Such a\nsystem subsumes both automatic speech recognition (ASR) as well as\nnatural language understanding (NLU). An end-to-end joint SLU model\ncan be built to a required specification opening up the opportunity\nto deploy on hardware constrained scenarios like devices enabling voice\nassistants to work offline, in a privacy preserving manner, whilst\nalso reducing server costs.<br style=\"mso-data-placement:same-cell;\"\n/><br style=\"mso-data-placement:same-cell;\" />\n  We first present models\nthat extract utterance intent directly from speech without intermediate\ntext output. We then present a compositional model, which generates\nthe transcript using the Listen Attend Spell ASR system and then extracts\ninterpretation using a neural NLU model. Finally, we contrast these\nmethods to a jointly trained end-to-end joint SLU model, consisting\nof ASR and NLU subsystems which are connected by a neural network based\ninterface instead of text, that produces transcripts as well as NLU\ninterpretation. We show that the jointly trained model shows improvements\nto ASR incorporating semantic information from NLU and also improves\nNLU by exposing it to ASR confusion encoded in the hidden layer.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2976",
      "author_area_id": "11",
      "author_area_label": "Spoken dialog systems and conversational analysis"
    },
    "denisov20_interspeech": {
      "authors": [
        [
          "Pavel",
          "Denisov"
        ],
        [
          "Ngoc Thang",
          "Vu"
        ]
      ],
      "title": "Pretrained Semantic Speech Embeddings for End-to-End Spoken Language Understanding via Cross-Modal Teacher-Student Learning",
      "original": "2456",
      "page_count": 5,
      "order": 183,
      "p1": "881",
      "pn": "885",
      "abstract": [
        "Spoken language understanding is typically based on pipeline architectures\nincluding speech recognition and natural language understanding steps.\nThese components are optimized independently to allow usage of available\ndata, but the overall system suffers from error propagation. In this\npaper, we propose a novel training method that enables pretrained contextual\nembeddings to process acoustic features. In particular, we extend it\nwith an encoder of pretrained speech recognition systems in order to\nconstruct end-to-end spoken language understanding systems. Our proposed\nmethod is based on the teacher-student framework across speech and\ntext modalities that aligns the acoustic and the semantic latent spaces.\nExperimental results in three benchmarks show that our system reaches\nthe performance comparable to the pipeline architecture without using\nany training data and outperforms it after fine-tuning with ten examples\nper class on two out of three benchmarks.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2456",
      "author_area_id": "11",
      "author_area_label": "Spoken dialog systems and conversational analysis"
    },
    "chetupalli20_interspeech": {
      "authors": [
        [
          "Srikanth Raj",
          "Chetupalli"
        ],
        [
          "Sriram",
          "Ganapathy"
        ]
      ],
      "title": "Context Dependent RNNLM for Automatic Transcription of Conversations",
      "original": "1813",
      "page_count": 5,
      "order": 184,
      "p1": "886",
      "pn": "890",
      "abstract": [
        "Conversational speech, while being unstructured at an utterance level,\ntypically has a macro topic which provides larger context spanning\nmultiple utterances. The current language models in speech recognition\nsystems using recurrent neural networks (RNNLM) rely mainly on the\nlocal context and exclude the larger context. In order to model the\nlong term dependencies of words across multiple sentences, we propose\na novel architecture where the words from prior utterances are converted\nto an embedding. The relevance of these embeddings for the prediction\nof next word in the current sentence is found using a gating network.\nThe relevance weighted context embedding vector is combined in the\nlanguage model to improve the next word prediction, and the entire\nmodel including the context embedding and the relevance weighting layers\nis jointly learned for a conversational language modeling task. Experiments\nare performed on two conversational datasets &#8212; AMI corpus and\nthe Switchboard corpus. In these tasks, we illustrate that the proposed\napproach yields significant improvements in language model perplexity\nover the RNNLM baseline. In addition, the use of proposed conversational\nLM for ASR rescoring results in absolute WER reduction of 1.2% on Switchboard\ndataset and 1.0% on AMI dataset over the RNNLM based ASR baseline.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1813",
      "author_area_id": "11",
      "author_area_label": "Spoken dialog systems and conversational analysis"
    },
    "tian20b_interspeech": {
      "authors": [
        [
          "Yusheng",
          "Tian"
        ],
        [
          "Philip John",
          "Gorinski"
        ]
      ],
      "title": "Improving End-to-End Speech-to-Intent Classification with Reptile",
      "original": "1160",
      "page_count": 5,
      "order": 185,
      "p1": "891",
      "pn": "895",
      "abstract": [
        "End-to-end spoken language understanding (SLU) systems have many advantages\nover conventional pipeline systems, but collecting in-domain speech\ndata to train an end-to-end system is costly and time consuming. One\nquestion arises from this: how to train an end-to-end SLU with limited\namounts of data? Many researchers have explored approaches that make\nuse of other related data resources, typically by pre-training parts\nof the model on high-resource speech recognition. In this paper, we\nsuggest improving the generalization performance of SLU models with\na non-standard learning algorithm, Reptile. Though Reptile was originally\nproposed for model-agnostic meta learning, we argue that it can also\nbe used to directly learn a target task and result in better generalization\nthan conventional gradient descent. In this work, we employ Reptile\nto the task of end-to-end spoken intent classification. Experiments\non four datasets of different languages and domains show improvement\nof intent prediction accuracy, both when Reptile is used alone and\nused in addition to pre-training.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1160",
      "author_area_id": "11",
      "author_area_label": "Spoken dialog systems and conversational analysis"
    },
    "cho20_interspeech": {
      "authors": [
        [
          "Won Ik",
          "Cho"
        ],
        [
          "Donghyun",
          "Kwak"
        ],
        [
          "Ji Won",
          "Yoon"
        ],
        [
          "Nam Soo",
          "Kim"
        ]
      ],
      "title": "Speech to Text Adaptation: Towards an Efficient Cross-Modal Distillation",
      "original": "1246",
      "page_count": 5,
      "order": 186,
      "p1": "896",
      "pn": "900",
      "abstract": [
        "Speech is one of the most effective means of communication and is full\nof information that helps the transmission of utterer&#8217;s thoughts.\nHowever, mainly due to the cumbersome processing of acoustic features,\nphoneme or word posterior probability has frequently been discarded\nin understanding the natural language. Thus, some recent spoken language\nunderstanding (SLU) modules have utilized end-to-end structures that\npreserve the uncertainty information. This further reduces the propagation\nof speech recognition error and guarantees computational efficiency.\nWe claim that in this process, the speech comprehension can benefit\nfrom the inference of massive pre-trained language models (LMs). We\ntransfer the knowledge from a concrete Transformer-based text LM to\nan SLU module which can face a data shortage, based on recent cross-modal\ndistillation methodologies. We demonstrate the validity of our proposal\nupon the performance on Fluent Speech Command, an English SLU benchmark.\nThereby, we experimentally verify our hypothesis that the knowledge\ncould be shared from the top layer of the LM to a fully speech-based\nmodule, in which the abstracted speech is expected to meet the semantic\nrepresentation.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1246",
      "author_area_id": "11",
      "author_area_label": "Spoken dialog systems and conversational analysis"
    },
    "ruan20_interspeech": {
      "authors": [
        [
          "Weitong",
          "Ruan"
        ],
        [
          "Yaroslav",
          "Nechaev"
        ],
        [
          "Luoxin",
          "Chen"
        ],
        [
          "Chengwei",
          "Su"
        ],
        [
          "Imre",
          "Kiss"
        ]
      ],
      "title": "Towards an ASR Error Robust Spoken Language Understanding System",
      "original": "2844",
      "page_count": 5,
      "order": 187,
      "p1": "901",
      "pn": "905",
      "abstract": [
        "A modern Spoken Language Understanding (SLU) system usually contains\ntwo sub-systems, Automatic Speech Recognition (ASR) and Natural Language\nUnderstanding (NLU), where ASR transforms voice signal to text form\nand NLU provides intent classification and slot filling from the text.\nIn practice, such decoupled ASR/NLU design facilitates fast model iteration\nfor both components. However, this makes downstream NLU susceptible\nto errors from the upstream ASR, causing significant performance degradation.\nTherefore, dealing with such errors is a major opportunity to improve\noverall SLU model performance. In this work, we first propose a general\nevaluation criterion that requires an ASR error robust model to perform\nwell on both transcription and ASR hypothesis. Then robustness training\ntechniques for both classification task and NER task are introduced.\nExperimental results on two datasets show that our proposed approaches\nimprove model robustness to ASR errors for both tasks.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2844",
      "author_area_id": "11",
      "author_area_label": "Spoken dialog systems and conversational analysis"
    },
    "kuo20_interspeech": {
      "authors": [
        [
          "Hong-Kwang J.",
          "Kuo"
        ],
        [
          "Zolt\u00e1n",
          "T\u00fcske"
        ],
        [
          "Samuel",
          "Thomas"
        ],
        [
          "Yinghui",
          "Huang"
        ],
        [
          "Kartik",
          "Audhkhasi"
        ],
        [
          "Brian",
          "Kingsbury"
        ],
        [
          "Gakuto",
          "Kurata"
        ],
        [
          "Zvi",
          "Kons"
        ],
        [
          "Ron",
          "Hoory"
        ],
        [
          "Luis",
          "Lastras"
        ]
      ],
      "title": "End-to-End Spoken Language Understanding Without Full Transcripts",
      "original": "2924",
      "page_count": 5,
      "order": 188,
      "p1": "906",
      "pn": "910",
      "abstract": [
        "An essential component of spoken language understanding (SLU) is slot\nfilling: representing the meaning of a spoken utterance using semantic\nentity labels. In this paper, we develop end-to-end (E2E) spoken language\nunderstanding systems that directly convert speech input to semantic\nentities and investigate if these E2E SLU models can be trained solely\non semantic entity annotations without word-for-word transcripts. Training\nsuch models is very useful as they can drastically reduce the cost\nof data collection. We created two types of such speech-to-entities\nmodels, a CTC model and an attention-based encoder-decoder model, by\nadapting models trained originally for speech recognition. Given that\nour experiments involve speech input, these systems need to recognize\nboth the entity label and words representing the entity value correctly.\nFor our speech-to-entities experiments on the ATIS corpus, both the\nCTC and attention models showed impressive ability to skip non-entity\nwords: there was little degradation when trained on just entities versus\nfull transcripts. We also explored the scenario where the entities\nare in an order not necessarily related to spoken order in the utterance.\nWith its ability to do re-ordering, the attention model did remarkably\nwell, achieving only about 2% degradation in speech-to-bag-of-entities\nF1 score.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2924",
      "author_area_id": "11",
      "author_area_label": "Spoken dialog systems and conversational analysis"
    },
    "gopalakrishnan20_interspeech": {
      "authors": [
        [
          "Karthik",
          "Gopalakrishnan"
        ],
        [
          "Behnam",
          "Hedayatnia"
        ],
        [
          "Longshaokan",
          "Wang"
        ],
        [
          "Yang",
          "Liu"
        ],
        [
          "Dilek",
          "Hakkani-T\u00fcr"
        ]
      ],
      "title": "Are Neural Open-Domain Dialog Systems Robust to Speech Recognition Errors in the Dialog History? An Empirical Study",
      "original": "1508",
      "page_count": 5,
      "order": 189,
      "p1": "911",
      "pn": "915",
      "abstract": [
        "Large end-to-end neural open-domain chatbots are becoming increasingly\npopular. However, research on building such chatbots has typically\nassumed that the user input is written in nature and it is not clear\nwhether these chatbots would seamlessly integrate with automatic speech\nrecognition (ASR) models to serve the speech modality. We aim to bring\nattention to this important question by empirically studying the effects\nof various types of synthetic and actual ASR hypotheses in the dialog\nhistory on TransferTransfo, a state-of-the-art Generative Pre-trained\nTransformer (GPT) based neural open-domain dialog system from the NeurIPS\nConvAI2 challenge. We observe that TransferTransfo trained on written\ndata is very sensitive to such hypotheses introduced to the dialog\nhistory during inference time. As a baseline mitigation strategy, we\nintroduce synthetic ASR hypotheses to the dialog history during training\nand observe marginal improvements, demonstrating the need for further\nresearch into techniques to make end-to-end open-domain chatbots fully\nspeech-robust. To the best of our knowledge, this is the first study\nto evaluate the effects of synthetic and actual ASR hypotheses on a\nstate-of-the-art neural open-domain dialog system and we hope it promotes\nspeech-robustness as an evaluation criterion in open-domain dialog.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1508",
      "author_area_id": "11",
      "author_area_label": "Spoken dialog systems and conversational analysis"
    },
    "ding20b_interspeech": {
      "authors": [
        [
          "Shaojin",
          "Ding"
        ],
        [
          "Tianlong",
          "Chen"
        ],
        [
          "Xinyu",
          "Gong"
        ],
        [
          "Weiwei",
          "Zha"
        ],
        [
          "Zhangyang",
          "Wang"
        ]
      ],
      "title": "AutoSpeech: Neural Architecture Search for Speaker Recognition",
      "original": "1258",
      "page_count": 5,
      "order": 190,
      "p1": "916",
      "pn": "920",
      "abstract": [
        "Speaker recognition systems based on Convolutional Neural Networks\n(CNNs) are often built with off-the-shelf backbones such as VGG-Net\nor ResNet. However, these backbones were originally proposed for image\nclassification, and therefore may not be naturally fit for speaker\nrecognition. Due to the prohibitive complexity of manually exploring\nthe design space, we propose the first neural architecture search approach\nfor the speaker recognition tasks, named as AutoSpeech. Our algorithm\nfirst identifies the optimal operation combination in a neural cell\nand then derives a CNN model by stacking the neural cell for multiple\ntimes. The final speaker recognition model can be obtained by training\nthe derived CNN model through the standard scheme. To evaluate the\nproposed approach, we conduct experiments on both speaker identification\nand speaker verification tasks using the VoxCeleb1 dataset. Results\ndemonstrate that the derived CNN architectures from the proposed approach\nsignificantly outperform current speaker recognition systems based\non VGG-M, ResNet-18, and ResNet-34 backbones, while enjoying lower\nmodel complexity.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1258",
      "author_area_id": "13",
      "author_area_label": "Special Sessions"
    },
    "yu20b_interspeech": {
      "authors": [
        [
          "Ya-Qi",
          "Yu"
        ],
        [
          "Wu-Jun",
          "Li"
        ]
      ],
      "title": "Densely Connected Time Delay Neural Network for Speaker Verification",
      "original": "1275",
      "page_count": 5,
      "order": 191,
      "p1": "921",
      "pn": "925",
      "abstract": [
        "Time delay neural network (TDNN) has been widely used in speaker verification\ntasks. Recently, two TDNN-based models, including extended TDNN (E-TDNN)\nand factorized TDNN (F-TDNN), are proposed to improve the accuracy\nof vanilla TDNN. But E-TDNN and F-TDNN increase the number of parameters\ndue to deeper networks, compared with vanilla TDNN. In this paper,\nwe propose a novel TDNN-based model, called densely connected TDNN\n(D-TDNN), by adopting bottleneck layers and dense connectivity. D-TDNN\nhas fewer parameters than existing TDNN-based models. Furthermore,\nwe propose an improved variant of D-TDNN, called D-TDNN-SS, to employ\nmultiple TDNN branches with short-term and long-term contexts. D-TDNN-SS\ncan integrate the information from multiple TDNN branches with a newly\ndesigned channel-wise selection mechanism called statistics-and- selection\n(SS). Experiments on VoxCeleb datasets show that both D-TDNN and D-TDNN-SS\ncan outperform existing models to achieve state-of-the-art accuracy\nwith fewer parameters, and D-TDNN-SS can achieve better accuracy than\nD-TDNN.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1275",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "zheng20b_interspeech": {
      "authors": [
        [
          "Siqi",
          "Zheng"
        ],
        [
          "Yun",
          "Lei"
        ],
        [
          "Hongbin",
          "Suo"
        ]
      ],
      "title": "Phonetically-Aware Coupled Network For Short Duration Text-Independent Speaker Verification",
      "original": "1306",
      "page_count": 5,
      "order": 192,
      "p1": "926",
      "pn": "930",
      "abstract": [
        "In this paper we propose an end-to-end phonetically-aware coupled network\nfor short duration speaker verification tasks. Phonetic information\nis shown to be beneficial for identifying short utterances. A coupled\nnetwork structure is proposed to exploit phonetic information. The\ncoupled convolutional layers allow the network to provide frame-level\nsupervision based on phonetic representations of the corresponding\nframes. The end-to-end training scheme using triplet loss function\nprovides direct comparison of speech contents between two utterances\nand hence enabling phonetic-based normalization. Our systems are compared\nagainst the current mainstream speaker verification systems on both\nNIST SRE and VoxCeleb evaluation datasets. Relative reductions of up\nto 34% in equal error rate are reported.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1306",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "jung20_interspeech": {
      "authors": [
        [
          "Myunghun",
          "Jung"
        ],
        [
          "Youngmoon",
          "Jung"
        ],
        [
          "Jahyun",
          "Goo"
        ],
        [
          "Hoirin",
          "Kim"
        ]
      ],
      "title": "Multi-Task Network for Noise-Robust Keyword Spotting and Speaker Verification Using CTC-Based Soft VAD and Global Query Attention",
      "original": "1420",
      "page_count": 5,
      "order": 193,
      "p1": "931",
      "pn": "935",
      "abstract": [
        "Keyword spotting (KWS) and speaker verification (SV) have been studied\nindependently although it is known that acoustic and speaker domains\nare complementary. In this paper, we propose a multi-task network that\nperforms KWS and SV simultaneously to fully utilize the interrelated\ndomain information. The multi-task network tightly combines sub-networks\naiming at performance improvement in challenging conditions such as\nnoisy environments, open-vocabulary KWS, and short-duration SV, by\nintroducing novel techniques of connectionist temporal classification\n(CTC)-based soft voice activity detection (VAD) and global query attention.\nFrame-level acoustic and speaker information is integrated with phonetically\noriginated weights so that forms a word-level global representation.\nThen it is used for the aggregation of feature vectors to generate\ndiscriminative embeddings. Our proposed approach shows 4.06% and 26.71%\nrelative improvements in equal error rate (EER) compared to the baselines\nfor both tasks. We also present a visualization example and results\nof ablation experiments.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1420",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "wu20b_interspeech": {
      "authors": [
        [
          "Yanfeng",
          "Wu"
        ],
        [
          "Chenkai",
          "Guo"
        ],
        [
          "Hongcan",
          "Gao"
        ],
        [
          "Xiaolei",
          "Hou"
        ],
        [
          "Jing",
          "Xu"
        ]
      ],
      "title": "Vector-Based Attentive Pooling for Text-Independent Speaker Verification",
      "original": "1422",
      "page_count": 5,
      "order": 194,
      "p1": "936",
      "pn": "940",
      "abstract": [
        "The pooling mechanism plays an important role in deep neural network\nbased systems for text-independent speaker verification, which aggregates\nthe variable-length frame-level vector sequence across all frames into\na fixed-dimensional utterance-level representation. Previous attentive\npooling methods employ scalar attention weights for each frame-level\nvector, resulting in insufficient collection of discriminative information.\nTo address this issue, this paper proposes a vector-based attentive\npooling method, which adopts vectorial attention instead of scalar\nattention. The vectorial attention can extract fine-grained features\nfor discriminating different speakers. Besides, the vector-based attentive\npooling is extended in a multi-head way for better speaker embeddings\nfrom multiple aspects. The proposed pooling method is evaluated with\nthe x-vector baseline system. Experiments are conducted on two public\ndatasets, VoxCeleb and Speaker in the Wild (SITW). The results show\nthat the vector-based attentive pooling method achieves superior performance\ncompared with statistics pooling and three state-of-the-art attentive\npooling methods, with the best equal error rate (EER) of 2.734 and\n3.062 in SITW as well as the best EER of 2.466 in VoxCeleb.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1422",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "safari20_interspeech": {
      "authors": [
        [
          "Pooyan",
          "Safari"
        ],
        [
          "Miquel",
          "India"
        ],
        [
          "Javier",
          "Hernando"
        ]
      ],
      "title": "Self-Attention Encoding and Pooling for Speaker Recognition",
      "original": "1446",
      "page_count": 5,
      "order": 195,
      "p1": "941",
      "pn": "945",
      "abstract": [
        "The computing power of mobile devices limits the end-user applications\nin terms of storage size, processing, memory and energy consumption.\nThese limitations motivate researchers for the design of more efficient\ndeep models. On the other hand, self-attention networks based on  Transformer\narchitecture have attracted remarkable interests due to their high\nparallelization capabilities and strong performance on a variety of\nNatural Language Processing (NLP) applications. Inspired by the  Transformer,\nwe propose a tandem Self-Attention Encoding and Pooling (SAEP) mechanism\nto obtain a discriminative speaker embedding given non-fixed length\nspeech utterances. SAEP is a stack of identical blocks solely relied\non self-attention and position-wise feed-forward networks to create\nvector representation of speakers. This approach encodes short-term\nspeaker spectral features into speaker embeddings to be used in text-independent\nspeaker verification. We have evaluated this approach on both  VoxCeleb1\n&amp; 2 datasets. The proposed architecture is able to outperform the\nbaseline x-vector, and shows competitive performance to some other\nbenchmarks based on convolutions, with a significant reduction in model\nsize. It employs 94%, 95%, and 73% less parameters compared to ResNet-34,\nResNet-50, and x-vector, respectively. This indicates that the proposed\nfully attention based architecture is more efficient in extracting\ntime-invariant features from speaker utterances.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1446",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "zhang20f_interspeech": {
      "authors": [
        [
          "Ruiteng",
          "Zhang"
        ],
        [
          "Jianguo",
          "Wei"
        ],
        [
          "Wenhuan",
          "Lu"
        ],
        [
          "Longbiao",
          "Wang"
        ],
        [
          "Meng",
          "Liu"
        ],
        [
          "Lin",
          "Zhang"
        ],
        [
          "Jiayu",
          "Jin"
        ],
        [
          "Junhai",
          "Xu"
        ]
      ],
      "title": "ARET: Aggregated Residual Extended Time-Delay Neural Networks for Speaker Verification",
      "original": "1626",
      "page_count": 5,
      "order": 196,
      "p1": "946",
      "pn": "950",
      "abstract": [
        "The time-delay neural network (TDNN) is widely used in speaker verification\nto extract long-term temporal features of speakers. Although common\nTDNN approaches well capture time-sequential information, they lack\nthe delicate transformations needed for deep representation. To solve\nthis problem, we propose two TDNN architectures. RET integrates shortcut\nconnections into conventional time-delay blocks, and ARET adopts a\nsplit-transform-merge strategy to extract more discriminative representation.\nExperiments on VoxCeleb datasets without augmentation indicate that\nARET realizes satisfactory performance on the VoxCeleb1 test set, VoxCeleb1-E,\nand VoxCeleb1-H, with 1.389%, 1.520%, and 2.614% equal error rate (EER),\nrespectively. Compared to state-of-the-art results on these test sets,\nRET achieves a 23%&#126;43% relative reduction in EER, and ARET reaches\n32%&#126;45%.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1626",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "zhang20g_interspeech": {
      "authors": [
        [
          "Hanyi",
          "Zhang"
        ],
        [
          "Longbiao",
          "Wang"
        ],
        [
          "Yunchun",
          "Zhang"
        ],
        [
          "Meng",
          "Liu"
        ],
        [
          "Kong Aik",
          "Lee"
        ],
        [
          "Jianguo",
          "Wei"
        ]
      ],
      "title": "Adversarial Separation Network for Speaker Recognition",
      "original": "1966",
      "page_count": 5,
      "order": 197,
      "p1": "951",
      "pn": "955",
      "abstract": [
        "Deep neural networks (DNN) have achieved great success in speaker recognition\nsystems. However, it is observed that DNN based systems are easily\ndeceived by adversarial examples leading to wrong predictions. Adversarial\nexamples, which are generated by adding purposeful perturbations on\nnatural examples, pose a serious security threat. In this study, we\npropose the adversarial separation network ( AS-Net) to protect the\nspeaker recognition system against adversarial attacks. Our proposed\n AS-Net is featured by its ability to separate adversarial perturbation\nfrom the test speech to restore the natural clean speech. As a standalone\ncomponent, each input speech is pre-processed by  AS-Net first. Furthermore,\nwe incorporate the compression structure and the speaker quality loss\nto enhance the capacity of the  AS-Net. Experimental results on the\nVCTK dataset demonstrated that the  AS-Net effectively enhanced the\nrobustness of speaker recognition systems against adversarial examples.\nIt also significantly outperformed other state-of-the-art adversarial-detection\nmechanisms, including adversarial perturbation elimination network\n(APE-GAN), feature squeezing, and adversarial training.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1966",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "li20j_interspeech": {
      "authors": [
        [
          "Jingyu",
          "Li"
        ],
        [
          "Tan",
          "Lee"
        ]
      ],
      "title": "Text-Independent Speaker Verification with Dual Attention Network",
      "original": "2031",
      "page_count": 5,
      "order": 198,
      "p1": "956",
      "pn": "960",
      "abstract": [
        "This paper presents a novel design of attention model for text-independent\nspeaker verification. The model takes a pair of input utterances and\ngenerates an utterance-level embedding to represent speaker-specific\ncharacteristics in each utterance. The input utterances are expected\nto have highly similar embeddings if they are from the same speaker.\nThe proposed attention model consists of a self-attention module and\na mutual attention module, which jointly contributes to the generation\nof the utterance-level embedding. The self-attention weights are computed\nfrom the utterance itself while the mutual-attention weights are computed\nwith the involvement of the other utterance in the input pairs. As\na result, each utterance is represented by a self-attention weighted\nembedding and a mutual-attention weighted embedding. The similarity\nbetween the embeddings is measured by a cosine distance score and a\nbinary classifier output score. The whole model, named Dual Attention\nNetwork, is trained end-to-end on Voxceleb database. The evaluation\nresults on Voxceleb 1 test set show that the Dual Attention Network\nsignificantly outperforms the baseline systems. The best result yields\nan equal error rate of 1.6%.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2031",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "qu20_interspeech": {
      "authors": [
        [
          "Xiaoyang",
          "Qu"
        ],
        [
          "Jianzong",
          "Wang"
        ],
        [
          "Jing",
          "Xiao"
        ]
      ],
      "title": "Evolutionary Algorithm Enhanced Neural Architecture Search for Text-Independent Speaker Verification",
      "original": "3057",
      "page_count": 5,
      "order": 199,
      "p1": "961",
      "pn": "965",
      "abstract": [
        "State-of-the-art speaker verification models are based on deep learning\ntechniques, which heavily depend on the hand-designed neural architectures\nfrom experts or engineers. We borrow the idea of  neural architecture\nsearch (NAS) for the  text-independent speaker verification task. As\nNAS can learn deep network structures automatically, we introduce the\nNAS conception into the well-known x-vector network. Furthermore, this\npaper proposes an evolutionary algorithm enhanced neural architecture\nsearch method called Auto-Vector to automatically discover promising\nnetworks for the speaker verification task. The experimental results\ndemonstrate our NAS-based model outperforms state-of-the-art speaker\nverification models.\n"
      ],
      "doi": "10.21437/Interspeech.2020-3057",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "weng20_interspeech": {
      "authors": [
        [
          "Chao",
          "Weng"
        ],
        [
          "Chengzhu",
          "Yu"
        ],
        [
          "Jia",
          "Cui"
        ],
        [
          "Chunlei",
          "Zhang"
        ],
        [
          "Dong",
          "Yu"
        ]
      ],
      "title": "Minimum Bayes Risk Training of RNN-Transducer for End-to-End Speech Recognition",
      "original": "1221",
      "page_count": 5,
      "order": 200,
      "p1": "966",
      "pn": "970",
      "abstract": [
        "In this work, we propose minimum Bayes risk (MBR) training of RNN-Transducer\n(RNN-T) for end-to-end speech recognition. Specifically, initialized\nwith a RNN-T trained model, MBR training is conducted via minimizing\nthe expected edit distance between the reference label sequence and\non-the-fly generated N-best hypothesis. We also introduce a heuristic\nto incorporate an external neural network language model (NNLM) in\nRNN-T beam search decoding and explore MBR training with the external\nNNLM. Experimental results demonstrate an MBR trained model outperforms\na RNN-T trained model substantially and further improvements can be\nachieved if trained with an external NNLM. Our best MBR trained system\nachieves absolute character error rate (CER) reductions of 1.2% and\n0.5% on read and spontaneous Mandarin speech respectively over a strong\nconvolution and transformer based RNN-T baseline trained on &#126;21,000\nhours of speech.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1221",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "wang20j_interspeech": {
      "authors": [
        [
          "Chengyi",
          "Wang"
        ],
        [
          "Yu",
          "Wu"
        ],
        [
          "Yujiao",
          "Du"
        ],
        [
          "Jinyu",
          "Li"
        ],
        [
          "Shujie",
          "Liu"
        ],
        [
          "Liang",
          "Lu"
        ],
        [
          "Shuo",
          "Ren"
        ],
        [
          "Guoli",
          "Ye"
        ],
        [
          "Sheng",
          "Zhao"
        ],
        [
          "Ming",
          "Zhou"
        ]
      ],
      "title": "Semantic Mask for Transformer Based End-to-End Speech Recognition",
      "original": "1778",
      "page_count": 5,
      "order": 201,
      "p1": "971",
      "pn": "975",
      "abstract": [
        "Attention-based encoder-decoder model has achieved impressive results\nfor both automatic speech recognition (ASR) and text-to-speech (TTS)\ntasks. This approach takes advantage of the memorization capacity of\nneural networks to learn the mapping from the input sequence to the\noutput sequence from scratch, without the assumption of prior knowledge\nsuch as the alignments. However, this model is prone to overfitting,\nespecially when the amount of training data is limited. Inspired by\nSpecAugment and BERT, in this paper, we propose a semantic mask based\nregularization for training such kind of end-to-end (E2E) model. The\nidea is to mask the input features corresponding to a particular output\ntoken, e.g., a word or a word-piece, in order to encourage the model\nto fill the token based on the contextual information. While this approach\nis applicable to the encoder-decoder framework with any type of neural\nnetwork architecture, we study the transformer-based model for ASR\nin this work. We perform experiments on Librispeech 960h and TedLium2\ndata sets, and achieve the state-of-the-art performance on the test\nset in the scope of E2E models.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1778",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "zhang20h_interspeech": {
      "authors": [
        [
          "Frank",
          "Zhang"
        ],
        [
          "Yongqiang",
          "Wang"
        ],
        [
          "Xiaohui",
          "Zhang"
        ],
        [
          "Chunxi",
          "Liu"
        ],
        [
          "Yatharth",
          "Saraf"
        ],
        [
          "Geoffrey",
          "Zweig"
        ]
      ],
      "title": "Faster, Simpler and More Accurate Hybrid ASR Systems Using Wordpieces",
      "original": "1995",
      "page_count": 5,
      "order": 202,
      "p1": "976",
      "pn": "980",
      "abstract": [
        "In this work, we first show that on the widely used LibriSpeech benchmark,\nour transformer-based context-dependent connectionist temporal classification\n(CTC) system produces state-of-the-art results. We then show that using\nwordpieces as modeling units combined with CTC training, we can greatly\nsimplify the engineering pipeline compared to conventional frame-based\ncross-entropy training by excluding all the GMM bootstrapping, decision\ntree building and force alignment steps, while still achieving very\ncompetitive word-error-rate. Additionally, using wordpieces as modeling\nunits can significantly improve runtime efficiency since we can use\nlarger stride without losing accuracy. We further confirm these findings\non two internal  VideoASR datasets: German, which is similar to English\nas a fusional language, and Turkish, which is an agglutinative language.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1995",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "dimitriadis20_interspeech": {
      "authors": [
        [
          "Dimitrios",
          "Dimitriadis"
        ],
        [
          "Kenichi",
          "Kumatani"
        ],
        [
          "Robert",
          "Gmyr"
        ],
        [
          "Yashesh",
          "Gaur"
        ],
        [
          "Sefik Emre",
          "Eskimez"
        ]
      ],
      "title": "A Federated Approach in Training Acoustic Models",
      "original": "1791",
      "page_count": 5,
      "order": 203,
      "p1": "981",
      "pn": "985",
      "abstract": [
        "In this paper, a novel platform for Acoustic Model training based on\nFederated Learning (FL) is described. This is the first attempt to\nintroduce Federated Learning techniques in Speech Recognition (SR)\ntasks. Besides the novelty of the task, the paper describes an easily\ngeneralizable FL platform and presents the design decisions used for\nthis task. Amongst the novel algorithms introduced is a hierarchical\noptimization scheme employing pairs of optimizers and an algorithm\nfor gradient selection, leading to improvements in training time and\nSR performance. The gradient selection algorithm is based on weighting\nthe gradients during the aggregation step. It effectively acts as a\nregularization process right before the gradient propagation. This\nprocess may address one of the FL challenges, i.e. training on vastly\nheterogeneous data. The experimental validation of the proposed system\nis based on the LibriSpeech task, presenting a speed-up of &#215;1.5\nand 6% WERR. The proposed Federated Learning system appears to outperform\nthe golden standard of distributed training in both convergence speed\nand overall model performance. Further improvements have been experienced\nin internal tasks.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1791",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "sheikh20_interspeech": {
      "authors": [
        [
          "Imran",
          "Sheikh"
        ],
        [
          "Emmanuel",
          "Vincent"
        ],
        [
          "Irina",
          "Illina"
        ]
      ],
      "title": "On Semi-Supervised LF-MMI Training of Acoustic Models with Limited Data",
      "original": "2242",
      "page_count": 5,
      "order": 204,
      "p1": "986",
      "pn": "990",
      "abstract": [
        "This work investigates semi-supervised training of acoustic models\n(AM) with the lattice-free maximum mutual information (LF-MMI) objective\nin practically relevant scenarios with a limited amount of labeled\nin-domain data. An error detection driven semi-supervised AM training\napproach is proposed, in which an error detector controls the hypothesized\ntranscriptions or lattices used as LF-MMI training targets on additional\nunlabeled data. Under this approach, our first method uses a single\nerror-tagged hypothesis whereas our second method uses a modified supervision\nlattice. These methods are evaluated and compared with existing semi-supervised\nAM training methods in three different matched or mismatched, limited\ndata setups. Word error recovery rates of 28 to 89% are reported.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2242",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "gao20b_interspeech": {
      "authors": [
        [
          "Yixin",
          "Gao"
        ],
        [
          "Noah D.",
          "Stein"
        ],
        [
          "Chieh-Chi",
          "Kao"
        ],
        [
          "Yunliang",
          "Cai"
        ],
        [
          "Ming",
          "Sun"
        ],
        [
          "Tao",
          "Zhang"
        ],
        [
          "Shiv Naga Prasad",
          "Vitaladevuni"
        ]
      ],
      "title": "On Front-End Gain Invariant Modeling for Wake Word Spotting",
      "original": "1992",
      "page_count": 5,
      "order": 205,
      "p1": "991",
      "pn": "995",
      "abstract": [
        "Wake word (WW) spotting is challenging in far-field due to the complexities\nand variations in acoustic conditions and the environmental interference\nin signal transmission. A suite of carefully designed and optimized\naudio front-end (AFE) algorithms help mitigate these challenges and\nprovide better quality audio signals to the downstream modules such\nas WW spotter. Since the WW model is trained with the AFE-processed\naudio data, its performance is sensitive to AFE variations, such as\ngain changes. In addition, when deploying to new devices, the WW performance\nis not guaranteed because the AFE is unknown to the WW model. To address\nthese issues, we propose a novel approach to use a new feature called\n&#916;LFBE to decouple the AFE gain variations from the WW model. We\nmodified the neural network architectures to accommodate the delta\ncomputation, with the feature extraction module unchanged. We evaluate\nour WW models using data collected from real household settings and\nshowed the models with the &#916;LFBE is robust to AFE gain changes.\nSpecifically, when AFE gain changes up to &#177;12dB, the baseline\nCNN model lost up to relative 19.0% in false alarm rate or 34.3% in\nfalse reject rate, while the model with &#916;LFBE demonstrates no\nperformance loss.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1992",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "ding20c_interspeech": {
      "authors": [
        [
          "Fenglin",
          "Ding"
        ],
        [
          "Wu",
          "Guo"
        ],
        [
          "Bin",
          "Gu"
        ],
        [
          "Zhen-Hua",
          "Ling"
        ],
        [
          "Jun",
          "Du"
        ]
      ],
      "title": "Unsupervised Regularization-Based Adaptive Training for Speech Recognition",
      "original": "1689",
      "page_count": 5,
      "order": 206,
      "p1": "996",
      "pn": "1000",
      "abstract": [
        "In this paper, we propose two novel regularization-based speaker adaptive\ntraining approaches for connectionist temporal classification (CTC)\nbased speech recognition. The first method is center loss (CL) regularization,\nwhich is used to penalize the distances between the embeddings of different\nspeakers and the only center. The second method is speaker variance\nloss (SVL) regularization in which we directly minimize the speaker\ninterclass variance during model training. Both methods achieve the\npurpose of training an adaptive model on the fly by adding regularization\nterms to the training loss function. Our experiment on the AISHELL-1\nMandarin recognition task shows that both methods are effective at\nadapting the CTC model without requiring any specific fine-tuning or\nadditional complexity, achieving character error rate improvements\nof up to 8.1% and 8.6% over the speaker independent (SI) model, respectively.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1689",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "loweimi20_interspeech": {
      "authors": [
        [
          "Erfan",
          "Loweimi"
        ],
        [
          "Peter",
          "Bell"
        ],
        [
          "Steve",
          "Renals"
        ]
      ],
      "title": "On the Robustness and Training Dynamics of Raw Waveform Models",
      "original": "0017",
      "page_count": 5,
      "order": 207,
      "p1": "1001",
      "pn": "1005",
      "abstract": [
        "We investigate the robustness and training dynamics of raw waveform\nacoustic models for automatic speech recognition (ASR). It is known\nthat the first layer of such models learn a set of filters, performing\na form of time-frequency analysis. This layer is liable to be under-trained\nowing to gradient vanishing, which can negatively affect the network\nperformance. Through a set of experiments on TIMIT, Aurora-4 and WSJ\ndatasets, we investigate the training dynamics of the first layer by\nmeasuring the evolution of its average frequency response over different\nepochs. We demonstrate that the network efficiently learns an optimal\nset of filters with a high spectral resolution and the dynamics of\nthe first layer highly correlates with the dynamics of the cross entropy\n(CE) loss and word error rate (WER). In addition, we study the robustness\nof raw waveform models in both matched and mismatched conditions. The\naccuracy of these models is found to be comparable to, or better than,\ntheir MFCC-based counterparts in matched conditions and notably improved\nby using a better alignment. The role of raw waveform normalisation\nwas also examined and up to 4.3% absolute WER reduction in mismatched\nconditions was achieved.\n"
      ],
      "doi": "10.21437/Interspeech.2020-17",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "xu20b_interspeech": {
      "authors": [
        [
          "Qiantong",
          "Xu"
        ],
        [
          "Tatiana",
          "Likhomanenko"
        ],
        [
          "Jacob",
          "Kahn"
        ],
        [
          "Awni",
          "Hannun"
        ],
        [
          "Gabriel",
          "Synnaeve"
        ],
        [
          "Ronan",
          "Collobert"
        ]
      ],
      "title": "Iterative Pseudo-Labeling for Speech Recognition",
      "original": "1800",
      "page_count": 5,
      "order": 208,
      "p1": "1006",
      "pn": "1010",
      "abstract": [
        "Pseudo-labeling has recently shown promise in end-to-end automatic\nspeech recognition (ASR). We study Iterative Pseudo-Labeling (IPL),\na semi-supervised algorithm which efficiently performs multiple iterations\nof pseudo-labeling on unlabeled data as the acoustic model evolves.\nIn particular, IPL fine tunes an existing model at each iteration using\nboth labeled data and a subset of unlabeled data. We study the main\ncomponents of IPL: decoding with a language model and data augmentation.\nWe then demonstrate the effectiveness of IPL by achieving state-of-the-art\nword-error rate on the  LibriSpeech test sets in both standard and\nlow-resource setting. We also study the effect of language models trained\non different corpora to show IPL can effectively utilize additional\ntext. Finally, we release a new large in-domain text corpus which does\nnot overlap with the  LibriSpeech training transcriptions to foster\nresearch in low-resource, semi-supervised ASR.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1800",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "kawamura20_interspeech": {
      "authors": [
        [
          "Naoko",
          "Kawamura"
        ],
        [
          "Tatsuya",
          "Kitamura"
        ],
        [
          "Kenta",
          "Hamada"
        ]
      ],
      "title": "Smart Tube: A Biofeedback System for Vocal Training and Therapy Through Tube Phonation",
      "original": "4002",
      "page_count": 2,
      "order": 209,
      "p1": "1011",
      "pn": "1012",
      "abstract": [
        "Tube phonation, or straw phonation, is a frequently used vocal training\ntechnique to improve the efficiency of the vocal mechanism by repeatedly\nproducing a speech sound into a tube or straw. Use of the straw results\nin a semi-occluded vocal tract in order to maximize the interaction\nbetween the vocal fold vibration and the vocal tract. This method requires\na voice trainer or therapist to raise the trainee or patient&#8217;s\nawareness of the vibrations around his or her mouth, guiding him/her\nto maximize the vibrations, which results in efficient phonation. A\nmajor problem with this process is that the trainer cannot monitor\nthe trainee/patient&#8217;s vibratory state in a quantitative manner.\nThis study proposes the use of Smart Tube, a straw with an attached\nacceleration sensor and LED strip that can measure vibrations and provide\ncorresponding feedback through LED lights in real-time. The biofeedback\nsystem was implemented using a microcontroller board, Arduino Uno,\nto minimize cost. Possible system function enhancements include Bluetooth\ncompatibility with personal computers and/or smartphones. Smart Tube\ncan facilitate improved phonation for trainees/patients by providing\nquantitative visual feedback.\n"
      ]
    },
    "choi20_interspeech": {
      "authors": [
        [
          "Seong",
          "Choi"
        ],
        [
          "Seunghoon",
          "Jeong"
        ],
        [
          "Jeewoo",
          "Yoon"
        ],
        [
          "Migyeong",
          "Yang"
        ],
        [
          "Minsam",
          "Ko"
        ],
        [
          "Eunil",
          "Park"
        ],
        [
          "Jinyoung",
          "Han"
        ],
        [
          "Munyoung",
          "Lee"
        ],
        [
          "Seonghee",
          "Lee"
        ]
      ],
      "title": "VCTUBE : A Library for Automatic Speech Data Annotation",
      "original": "4004",
      "page_count": 2,
      "order": 210,
      "p1": "1013",
      "pn": "1014",
      "abstract": [
        "We introduce an open-source Python library, VCTUBE, which can automatically\ngenerate &#60;audio, text&#62; pair of speech data from a given Youtube\nURL. We believe VCTUBE is useful for collecting, processing, and annotating\nspeech data easily toward developing speech synthesis systems.\n"
      ]
    },
    "xie20_interspeech": {
      "authors": [
        [
          "Yanlu",
          "Xie"
        ],
        [
          "Xiaoli",
          "Feng"
        ],
        [
          "Boxue",
          "Li"
        ],
        [
          "Jinsong",
          "Zhang"
        ],
        [
          "Yujia",
          "Jin"
        ]
      ],
      "title": "A Mandarin L2 Learning APP with Mispronunciation Detection and Feedback",
      "original": "4005",
      "page_count": 2,
      "order": 211,
      "p1": "1015",
      "pn": "1016",
      "abstract": [
        "In this paper, an APP with Mispronunciation Detection and Feedback\nfor Mandarin L2 Learners is shown. The APP could detect the mispronunciation\nin the words and highlight it with red at the phone level. Also, the\nscore will be shown to evaluate the overall pronunciation. When touching\nthe highlight, the pronunciation of the learner&#8217;s and the standard&#8217;s\nis played. Then the flash animation that describes the movement of\nthe tongue, mouth, and other articulators will be shown to the learner.\nThe learner could repeat the process to improve and excise the pronunciation.\nThe App called &#8216;SAIT H&#224;ny&#x1D4;&#8217; can be downloaded\nat App Store.\n"
      ]
    },
    "udayakumar20_interspeech": {
      "authors": [
        [
          "Tejas",
          "Udayakumar"
        ],
        [
          "Kinnera",
          "Saranu"
        ],
        [
          "Mayuresh Sanjay",
          "Oak"
        ],
        [
          "Ajit Ashok",
          "Saunshikar"
        ],
        [
          "Sandip Shriram",
          "Bapat"
        ]
      ],
      "title": "Rapid Enhancement of NLP Systems by Acquisition of Data in Correlated Domains",
      "original": "4008",
      "page_count": 2,
      "order": 212,
      "p1": "1017",
      "pn": "1018",
      "abstract": [
        "In a generation where industries are going through a paradigm shift\nbecause of the rampant growth of deep learning, structured data plays\na crucial role in the automation of various tasks. Textual structured\ndata is one such kind which is extensively used in systems like chat\nbots and automatic speech recognition. Unfortunately, a majority of\nthese textual data available is unstructured in the form of user reviews\nand feedback, social media posts etc. Automating the task of categorizing\nor clustering these data into meaningful domains will reduce the time\nand effort needed in building sophisticated human-interactive systems.\nIn this paper, we present a web tool that builds a domain specific\ndata based on a search phrase from a database of highly unstructured\nuser utterances. We also show the usage of Elasticsearch database with\ncustom indexes for full correlated text-search. This tool uses the\nopen sourced Glove model combined with cosine similarity and performs\na graph based search to provide semantically and syntactically meaningful\ncorpora. In the end, we discuss its applications with respect to natural\nlanguage processing.\n"
      ]
    },
    "shi20_interspeech": {
      "authors": [
        [
          "Ke",
          "Shi"
        ],
        [
          "Kye Min",
          "Tan"
        ],
        [
          "Richeng",
          "Duan"
        ],
        [
          "Siti Umairah Md.",
          "Salleh"
        ],
        [
          "Nur Farah Ain",
          "Suhaimi"
        ],
        [
          "Rajan",
          "Vellu"
        ],
        [
          "Ngoc Thuy Huong Helen",
          "Thai"
        ],
        [
          "Nancy F.",
          "Chen"
        ]
      ],
      "title": "Computer-Assisted Language Learning System: Automatic Speech Evaluation for Children Learning Malay and Tamil",
      "original": "4010",
      "page_count": 2,
      "order": 213,
      "p1": "1019",
      "pn": "1020",
      "abstract": [
        "We present a computer-assisted language learning system that automatically\nevaluates the pronunciation and fluency of spoken Malay and Tamil.\nOur system consists of a server and a user-facing Android application,\nwhere the server is responsible for speech-to-text alignment as well\nas pronunciation and fluency scoring. We describe our system architecture\nand discuss the technical challenges associated with low resource languages.\nTo the best of our knowledge, this work is the first pronunciation\nand fluency scoring system for Malay and Tamil.\n"
      ]
    },
    "saeki20_interspeech": {
      "authors": [
        [
          "Takaaki",
          "Saeki"
        ],
        [
          "Yuki",
          "Saito"
        ],
        [
          "Shinnosuke",
          "Takamichi"
        ],
        [
          "Hiroshi",
          "Saruwatari"
        ]
      ],
      "title": "Real-Time, Full-Band, Online DNN-Based Voice Conversion System Using a Single CPU",
      "original": "4011",
      "page_count": 2,
      "order": 214,
      "p1": "1021",
      "pn": "1022",
      "abstract": [
        "We present a real-time, full-band, online voice conversion (VC) system\nthat uses a single CPU. For practical applications, VC must be high\nquality and able to perform real-time, online conversion with fewer\ncomputational resources. Our system achieves this by combining non-linear\nconversion with a deep neural network and short-tap, sub-band filtering.\nWe evaluate our system and demonstrate that it 1) achieves the estimated\ncomplexity around 2.5 GFLOPS and measures real-time factor (RTF) around\n0.5 with a single CPU and 2) can attain converted speech with a 3.4\n/ 5.0 mean opinion score (MOS) of naturalness.\n"
      ]
    },
    "feng20b_interspeech": {
      "authors": [
        [
          "Xiaoli",
          "Feng"
        ],
        [
          "Yanlu",
          "Xie"
        ],
        [
          "Yayue",
          "Deng"
        ],
        [
          "Boxue",
          "Li"
        ]
      ],
      "title": "A Dynamic 3D Pronunciation Teaching Model Based on Pronunciation Attributes and Anatomy",
      "original": "4012",
      "page_count": 2,
      "order": 215,
      "p1": "1023",
      "pn": "1024",
      "abstract": [
        "In this paper, a dynamic three dimensional (3D) head model is introduced\nwhich is built based on knowledge of (the human) anatomy and the theory\nof distinctive features. The model is used to help Chinese learners\nunderstand the exact location and method of the phoneme articulation\nintuitively. You can access the phonetic learning system, choose the\ntarget sound you want to learn and then watch the 3D dynamic animations\nof the phonemes. You can look at the lips, tongue, soft palate, uvula,\nand other dynamic vocal organs as well as teeth, gums, hard jaw, and\nother passive vocal organs from different angles. In this process,\nyou can make the skin and some of the muscles semi-transparent, or\nzoom in or out the model to see the dynamic changes of articulators\nclearly. By looking at the 3D model, learners can find the exact location\nof each sound and imitate the pronunciation actions.\n"
      ]
    },
    "kimura20_interspeech": {
      "authors": [
        [
          "Naoki",
          "Kimura"
        ],
        [
          "Zixiong",
          "Su"
        ],
        [
          "Takaaki",
          "Saeki"
        ]
      ],
      "title": "End-to-End Deep Learning Speech Recognition Model for Silent Speech Challenge",
      "original": "4015",
      "page_count": 2,
      "order": 216,
      "p1": "1025",
      "pn": "1026",
      "abstract": [
        "This work is the first attempt to apply an end-to-end, deep neural\nnetwork-based automatic speech recognition (ASR) pipeline to the Silent\nSpeech Challenge dataset (SSC), which contains synchronized ultrasound\nimages and lip images captured when a single speaker read the TIMIT\ncorpus without uttering audible sounds. In silent speech research using\nSSC dataset, established methods in ASR have been utilized with some\nmodifications to use it in visual speech recognition. In this work,\nwe tested the SOTA method of ASR on the SSC dataset using the End-to-End\nSpeech Processing Toolkit, ESPnet. The experimental results show that\nthis end-to-end method achieved a character error rate (CER) of 10.1%\nand a WER of 20.5% by incorporating SpecAugment, demonstrating the\npossibility to further improve the performance with additional data\ncollection.\n"
      ]
    },
    "li20k_interspeech": {
      "authors": [
        [
          "Jialu",
          "Li"
        ],
        [
          "Mark",
          "Hasegawa-Johnson"
        ]
      ],
      "title": "Autosegmental Neural Nets: Should Phones and Tones be Synchronous or Asynchronous?",
      "original": "1834",
      "page_count": 5,
      "order": 217,
      "p1": "1027",
      "pn": "1031",
      "abstract": [
        "Phones, the segmental units of the International Phonetic Alphabet\n(IPA), are used for lexical distinctions in most human languages; Tones,\nthe suprasegmental units of the IPA, are used in perhaps 70%. Many\nprevious studies have explored cross-lingual adaptation of automatic\nspeech recognition (ASR) phone models, but few have explored the multilingual\nand cross-lingual transfer of synchronization between phones and tones.\nIn this paper, we test four Connectionist Temporal Classification (CTC)-based\nacoustic models, differing in the degree of synchrony they impose between\nphones and tones. Models are trained and tested multilingually in three\nlanguages, then adapted and tested cross-lingually in a fourth. Both\nsynchronous and asynchronous models are effective in both multilingual\nand cross-lingual settings. Synchronous models achieve lower error\nrate in the joint phone+tone tier, but asynchronous training results\nin lower tone error rate.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1834",
      "author_area_id": "9",
      "author_area_label": "Speech Recognition: Architecture, Search, and Linguistic Components"
    },
    "tachbelie20_interspeech": {
      "authors": [
        [
          "Martha Yifiru",
          "Tachbelie"
        ],
        [
          "Solomon Teferra",
          "Abate"
        ],
        [
          "Tanja",
          "Schultz"
        ]
      ],
      "title": "Development of Multilingual ASR Using GlobalPhone for Less-Resourced Languages: The Case of Ethiopian Languages",
      "original": "2827",
      "page_count": 5,
      "order": 218,
      "p1": "1032",
      "pn": "1036",
      "abstract": [
        "In this paper, we present the cross-lingual and multilingual experiments\nwe have conducted using existing resources of other languages for the\ndevelopment of speech recognition system for less-resourced languages.\nIn our experiments, we used the Globalphone corpus as source and considered\nfour Ethiopian languages namely Amharic, Oromo, Tigrigna and Wolaytta\nas targets. We have developed multilingual (ML) Automatic Speech Recognition\n(ASR) systems and decoded speech of the four Ethiopian languages. A\nmultilingual acoustic model (AM) trained with speech data of 22 Globalphone\nlanguages but the target languages, achieved a Word Error Rate (WER)\nof 15.79%. Moreover, including training speech of one closely related\nlanguage (in terms of phonetic overlap) in ML AM training resulted\nin a relative WER reduction of 51.41%. Although adaptation of ML systems\ndid not give significant WER reduction over the monolingual ones, it\nenables us to rapidly adapt existing ML ASR systems to new languages.\nIn sum, our experiments demonstrated that ASR systems can be developed\nrapidly with a pronunciation dictionary (PD) of low out of vocabulary\n(OOV) rate and a strong language model (LM).\n"
      ],
      "doi": "10.21437/Interspeech.2020-2827",
      "author_area_id": "9",
      "author_area_label": "Speech Recognition: Architecture, Search, and Linguistic Components"
    },
    "hou20_interspeech": {
      "authors": [
        [
          "Wenxin",
          "Hou"
        ],
        [
          "Yue",
          "Dong"
        ],
        [
          "Bairong",
          "Zhuang"
        ],
        [
          "Longfei",
          "Yang"
        ],
        [
          "Jiatong",
          "Shi"
        ],
        [
          "Takahiro",
          "Shinozaki"
        ]
      ],
      "title": "Large-Scale End-to-End Multilingual Speech Recognition and Language Identification with Multi-Task Learning",
      "original": "2164",
      "page_count": 5,
      "order": 219,
      "p1": "1037",
      "pn": "1041",
      "abstract": [
        "In this paper, we report a large-scale end-to-end language-independent\nmultilingual model for joint automatic speech recognition (ASR) and\nlanguage identification (LID). This model adopts hybrid CTC/attention\narchitecture and achieves word error rate (WER) of 52.8 and LID accuracy\nof 93.5 on 42 languages with around 5000 hours of training data. We\nalso compare the effects of using subword-level or character-level\nvocabulary for large-scale multilingual tasks. Furthermore, we transfer\nthe pre-trained model to 14 low-resource languages. Results show that\nthe pre-trained model achieves significantly better results than non-pretrained\nbaselines on both language-specific and multilingual low-resource ASR\ntasks in terms of WER, which is reduced by 28.1% and 11.4% respectively.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2164",
      "author_area_id": "9",
      "author_area_label": "Speech Recognition: Architecture, Search, and Linguistic Components"
    },
    "zhou20b_interspeech": {
      "authors": [
        [
          "Xinyuan",
          "Zhou"
        ],
        [
          "Emre",
          "Y\u0131lmaz"
        ],
        [
          "Yanhua",
          "Long"
        ],
        [
          "Yijie",
          "Li"
        ],
        [
          "Haizhou",
          "Li"
        ]
      ],
      "title": "Multi-Encoder-Decoder Transformer for Code-Switching Speech Recognition",
      "original": "2488",
      "page_count": 5,
      "order": 220,
      "p1": "1042",
      "pn": "1046",
      "abstract": [
        "Code-switching (CS) occurs when a speaker alternates words of two or\nmore languages within a single sentence or across sentences. Automatic\nspeech recognition (ASR) of CS speech has to deal with two or more\nlanguages at the same time. In this study, we propose a Transformer-based\narchitecture with two symmetric language-specific encoders to capture\nthe individual language attributes, that improve the acoustic representation\nof each language. These representations are combined using a language-specific\nmulti-head attention mechanism in the decoder module. Each encoder\nand its corresponding attention module in the decoder are pre-trained\nusing a large monolingual corpus aiming to alleviate the impact of\nlimited CS training data. We call such a network a multi-encoder-decoder\n(MED) architecture. Experiments on the SEAME corpus show that the proposed\nMED architecture achieves 10.2% and 10.8% relative error rate reduction\non the CS evaluation sets with Mandarin and English as the matrix language\nrespectively.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2488",
      "author_area_id": "9",
      "author_area_label": "Speech Recognition: Architecture, Search, and Linguistic Components"
    },
    "abate20_interspeech": {
      "authors": [
        [
          "Solomon Teferra",
          "Abate"
        ],
        [
          "Martha Yifiru",
          "Tachbelie"
        ],
        [
          "Tanja",
          "Schultz"
        ]
      ],
      "title": "Multilingual Acoustic and Language Modeling for Ethio-Semitic Languages",
      "original": "2856",
      "page_count": 5,
      "order": 221,
      "p1": "1047",
      "pn": "1051",
      "abstract": [
        "Development of Multilingual Automatic Speech Recognition (ASR) systems\nenables to share existing speech and text corpora among languages.\nWe have conducted experiments on the development of multilingual Acoustic\nModels (AM) and Language Models (LM) for Tigrigna. Using Amharic Deep\nNeural Network (DNN) AM, Tigrigna pronunciation dictionary and trigram\nLM, we achieved a Word Error Rate (WER) of 30.9% for Tigrigna. Adding\ntraining speech from the target language (Tigrigna) to the whole training\nspeech of the donor language (Amharic) continuously reduces WER with\nthe amount of added data. We have also developed different (including\nrecurrent neural networks based) multilingual LMs and achieved a relative\nWER reduction of 3.56% compared to the use of monolingual trigram LMs.\nConsidering scarcity of computational resources to decode with very\nlarge vocabularies, we have also experimented on the use of morphemes\nas pronunciation and language modeling units. We have achieved character\nerror rate (CER) of 7.9% which is relatively lower by 38.3% to 1.3%\nthan the CER of the word-based models of smaller vocabularies than\n162k. Our results show the possibility of developing ASR system for\nan Ethio-Semitic language using an existing speech and text corpora\nof another language in the family.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2856",
      "author_area_id": "9",
      "author_area_label": "Speech Recognition: Architecture, Search, and Linguistic Components"
    },
    "hu20c_interspeech": {
      "authors": [
        [
          "Yushi",
          "Hu"
        ],
        [
          "Shane",
          "Settle"
        ],
        [
          "Karen",
          "Livescu"
        ]
      ],
      "title": "Multilingual Jointly Trained Acoustic and Written Word Embeddings",
      "original": "2828",
      "page_count": 5,
      "order": 222,
      "p1": "1052",
      "pn": "1056",
      "abstract": [
        "Acoustic word embeddings (AWEs) are vector representations of spoken\nword segments. AWEs can be learned jointly with embeddings of character\nsequences, to generate phonetically meaningful embeddings of written\nwords, or acoustically grounded word embeddings (AGWEs). Such embeddings\nhave been used to improve speech retrieval, recognition, and spoken\nterm discovery. In this work, we extend this idea to multiple low-resource\nlanguages. We jointly train an AWE model and an AGWE model, using phonetically\ntranscribed data from multiple languages. The pre-trained models can\nthen be used for unseen zero-resource languages, or fine-tuned on data\nfrom low-resource languages. We also investigate distinctive features,\nas an alternative to phone labels, to better share cross-lingual information.\nWe test our models on word discrimination tasks for twelve languages.\nWhen trained on eleven languages and tested on the remaining unseen\nlanguage, our model outperforms traditional unsupervised approaches\nlike dynamic time warping. After fine-tuning the pre-trained models\non one hour or even ten minutes of data from a new language, performance\nis typically much better than training on only the target-language\ndata. We also find that phonetic supervision improves performance over\ncharacter sequences, and that distinctive feature supervision is helpful\nin handling unseen phones in the target language.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2828",
      "author_area_id": "9",
      "author_area_label": "Speech Recognition: Architecture, Search, and Linguistic Components"
    },
    "li20l_interspeech": {
      "authors": [
        [
          "Chia-Yu",
          "Li"
        ],
        [
          "Ngoc Thang",
          "Vu"
        ]
      ],
      "title": "Improving Code-Switching Language Modeling with Artificially Generated Texts Using Cycle-Consistent Adversarial Networks",
      "original": "2177",
      "page_count": 5,
      "order": 223,
      "p1": "1057",
      "pn": "1061",
      "abstract": [
        "This paper presents our latest effort on improving Code-switching language\nmodels that suffer from data scarcity. We investigate methods to augment\nCode-switching training text data by artificially generating them.\nConcretely, we propose a cycle-consistent adversarial networks based\nframework to transfer monolingual text into Code-switching text, considering\nCode-switching as a speaking style. Our experimental results on the\nSEAME corpus show that utilizing artificially generated Code-switching\ntext data improves consistently the language model as well as the automatic\nspeech recognition performance.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2177",
      "author_area_id": "9",
      "author_area_label": "Speech Recognition: Architecture, Search, and Linguistic Components"
    },
    "hu20d_interspeech": {
      "authors": [
        [
          "Xinhui",
          "Hu"
        ],
        [
          "Qi",
          "Zhang"
        ],
        [
          "Lei",
          "Yang"
        ],
        [
          "Binbin",
          "Gu"
        ],
        [
          "Xinkang",
          "Xu"
        ]
      ],
      "title": "Data Augmentation for Code-Switch Language Modeling by Fusing Multiple Text Generation Methods",
      "original": "2219",
      "page_count": 5,
      "order": 224,
      "p1": "1062",
      "pn": "1066",
      "abstract": [
        "To deal with the problem of data scarce in training language model\n(LM) for code-switching (CS) speech recognition, we proposed an approach\nto obtain augmentation texts from three different viewpoints. The first\none is to enhance monolingual LM by selecting corresponding sentences\nfor existing conversational corpora; The second one is based on replacements\nusing syntactic constraint for a monolingual Chinese corpus, with the\nhelps of an aligned word list obtained from a pseudo-parallel corpus,\nand part-of-speech (POS) of words; The third one is to use text generation\nbased on a pointer-generator network with copy mechanism, using a real\nCS text data for training. All sentences from these approaches show\nimprovement for CS LMs, and they are finally fused into an LM for CS\nASR tasks.<br style=\"mso-data-placement:same-cell;\" /><br style=\"mso-data-placement:same-cell;\"\n/>\n  Evaluations on LMs built by the above augmented data were conducted\non two Mandarin-English CS speech sets DTANG, and SEAME. The perplexities\nwere greatly reduced with all kinds of augmented texts, and speech\nrecognition performances were steadily improved. The mixed word error\nrate (MER) of DTANG and SEAME evaluation dataset got relative reduction\nby 9.10% and 29.73%, respectively.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2219",
      "author_area_id": "9",
      "author_area_label": "Speech Recognition: Architecture, Search, and Linguistic Components"
    },
    "li20m_interspeech": {
      "authors": [
        [
          "Xinxing",
          "Li"
        ],
        [
          "Edward",
          "Lin"
        ]
      ],
      "title": "A 43 Language Multilingual Punctuation Prediction Neural Network Model",
      "original": "2052",
      "page_count": 5,
      "order": 225,
      "p1": "1067",
      "pn": "1071",
      "abstract": [
        "Punctuation prediction is a critical component for speech recognition\nreadability and speech translation segmentation. When considering multiple\nlanguage support, traditional monolingual neural network models used\nfor punctuation prediction can be costly to manage and may not produce\nthe best accuracy. In this paper, we investigate multilingual Long\nShort-Term Memory (LSTM) modeling using Byte Pair Encoding (BPE) for\npunctuation prediction to support 43 languages<SUP>1</SUP> across 69\ncountries. Our findings show a single multilingual BPE-based model\ncan achieve similar or even better performance than separate monolingual\nword-based models by benefiting from shared information across different\nlanguages. On an in-domain news text test set, the multilingual model\nachieves on average 80.2%  F1-score while on out-of-domain speech recognition\ntext, it achieves 73.5%  F1-score. We also show that the shared information\ncan help in fine-tuning for low-resource languages as well.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2052",
      "author_area_id": "9",
      "author_area_label": "Speech Recognition: Architecture, Search, and Linguistic Components"
    },
    "wang20k_interspeech": {
      "authors": [
        [
          "Jisung",
          "Wang"
        ],
        [
          "Jihwan",
          "Kim"
        ],
        [
          "Sangki",
          "Kim"
        ],
        [
          "Yeha",
          "Lee"
        ]
      ],
      "title": "Exploring Lexicon-Free Modeling Units for End-to-End Korean and Korean-English Code-Switching Speech Recognition",
      "original": "2440",
      "page_count": 4,
      "order": 226,
      "p1": "1072",
      "pn": "1075",
      "abstract": [
        "Automatic speech recognition (ASR) tasks are usually solved using lexicon-based\nhybrid systems or character-based acoustic models to automatically\ntranslate speech data into written text. While hybrid systems require\na manually designed lexicon, end-to-end models can process character-based\nspeech data. This resolves the need to define a lexicon for non-English\nlanguages for which a standard lexicon may be absent. Korean is relatively\nphonetic and has a unique writing system, and it is thus worth investigating\nuseful modeling units for end-to-end Korean ASR. Our work is the first\nto compare the performance of deep neural networks (DNNs), designed\nas a combination of connectionist temporal classification and attention-based\nencoder-decoder, on various lexicon-free Korean models. Experiments\non the Zeroth-Korean dataset and medical records, which consist of\nKorean-only and Korean-English code-switching corpora respectively,\nshow how DNNs based on syllables and sub-words significantly outperform\nJamo-based models on Korean ASR tasks. Our successful application of\nusing lexicon-free modeling units on non-English ASR tasks provides\ncompelling evidence that lexicon-free approaches can alleviate the\nheavy code-switching involved in non-English medical transcriptions.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2440",
      "author_area_id": "9",
      "author_area_label": "Speech Recognition: Architecture, Search, and Linguistic Components"
    },
    "platen20_interspeech": {
      "authors": [
        [
          "Patrick von",
          "Platen"
        ],
        [
          "Fei",
          "Tao"
        ],
        [
          "Gokhan",
          "Tur"
        ]
      ],
      "title": "Multi-Task Siamese Neural Network for Improving Replay Attack Detection",
      "original": "0086",
      "page_count": 5,
      "order": 227,
      "p1": "1076",
      "pn": "1080",
      "abstract": [
        "Automatic speaker verification systems are vulnerable to audio replay\nattacks which bypass security by replaying recordings of authorized\nspeakers. Replay attack detection (RA) systems built upon Residual\nNeural Networks (ResNet)s have yielded astonishing results on the public\nbenchmark ASVspoof 2019 Physical Access challenge. With most teams\nusing fine-tuned feature extraction pipelines and model architectures,\nthe generalizability of such systems remains questionable though. In\nthis work, we analyse the effect of discriminative feature learning\nin a multi-task learning (MTL) setting can have on the generalizability\nand discriminability of RA detection systems. We use a popular ResNet\narchitecture optimized by the cross-entropy criterion as our baseline\nand compare it to the same architecture optimized by MTL using Siamese\nNeural Networks (SNN). It can be shown that 26.8% relative improvement\non Equal Error Rate (EER) is obtained by leveraging SNN.We further\nenhance the model&#8217;s architecture and demonstrate that SNN with\nadditional reconstruction loss yield another significant improvement\nof relative 13.8% EER.\n"
      ],
      "doi": "10.21437/Interspeech.2020-86",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "akimoto20_interspeech": {
      "authors": [
        [
          "Kosuke",
          "Akimoto"
        ],
        [
          "Seng Pei",
          "Liew"
        ],
        [
          "Sakiko",
          "Mishima"
        ],
        [
          "Ryo",
          "Mizushima"
        ],
        [
          "Kong Aik",
          "Lee"
        ]
      ],
      "title": "POCO: A Voice Spoofing and Liveness Detection Corpus Based on Pop Noise",
      "original": "1243",
      "page_count": 5,
      "order": 228,
      "p1": "1081",
      "pn": "1085",
      "abstract": [
        "We present a new database of voice recordings with the goal of promoting\nresearch on protection of automatic speaker verification systems from\nvoice spoofing, such as replay attacks. Specifically, we focus on the\nliveness feature of live speech, i.e., pop noise, and the corresponding\nvoice recordings without this feature, for the purpose of combating\nspoofing via liveness detection. Our database includes simultaneous\nrecordings using a microphone array, as well as recordings at various\ndistances and positions. To the best of our knowledge, this is the\nfirst publicly available database that has been particularly designed\nto study the liveness features of voice recordings under various conditions.<SUP>1</SUP>\n"
      ],
      "doi": "10.21437/Interspeech.2020-1243",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "wang20l_interspeech": {
      "authors": [
        [
          "Hongji",
          "Wang"
        ],
        [
          "Heinrich",
          "Dinkel"
        ],
        [
          "Shuai",
          "Wang"
        ],
        [
          "Yanmin",
          "Qian"
        ],
        [
          "Kai",
          "Yu"
        ]
      ],
      "title": "Dual-Adversarial Domain Adaptation for Generalized Replay Attack Detection",
      "original": "1255",
      "page_count": 5,
      "order": 229,
      "p1": "1086",
      "pn": "1090",
      "abstract": [
        "Despite tremendous progress in speaker verification recently, replay\nspoofing attacks are still a major threat to these systems. Focusing\non dataset-specific scenarios, anti-spoofing systems have achieved\npromising in-domain performance at the cost of poor generalization\ntowards unseen out-of-domain datasets. This is treated as a domain\nmismatch problem with a domain adversarial training (DAT) framework,\nwhich has previously been applied to enhance generalization. However,\nsince only one domain discriminator is adopted, DAT suffers from the\nfalse alignment of cross-domain spoofed and genuine pairs, thus failing\nto acquire a strong spoofing-discriminative capability. In this work,\nwe propose the dual-adversarial domain adaptation (DADA) framework\nto enable fine-grained alignment of spoofed and genuine data separately\nby using two domain discriminators, which effectively alleviates the\nabove problem and further improves spoofing detection performance.\nExperiments on the ASVspoof 2017 V.2 dataset and the physical access\nportion of BTAS 2016 dataset demonstrate that the proposed DADA framework\nsignificantly outperforms the baseline model and DAT framework in cross-domain\nevaluation scenarios. It is shown that the newly proposed DADA architecture\nis more robust and effective for generalized replay attack detection.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1255",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "shim20_interspeech": {
      "authors": [
        [
          "Hye-jin",
          "Shim"
        ],
        [
          "Hee-Soo",
          "Heo"
        ],
        [
          "Jee-weon",
          "Jung"
        ],
        [
          "Ha-Jin",
          "Yu"
        ]
      ],
      "title": "Self-Supervised Pre-Training with Acoustic Configurations for Replay Spoofing Detection",
      "original": "1345",
      "page_count": 5,
      "order": 230,
      "p1": "1091",
      "pn": "1095",
      "abstract": [
        "Constructing a dataset for replay spoofing detection requires a physical\nprocess of playing an utterance and re-recording it, presenting a challenge\nto the collection of large-scale datasets. In this study, we propose\na self-supervised framework for pre-training acoustic configurations\nusing datasets published for other tasks, such as speaker verification.\nHere, acoustic configurations refer to the environmental factors generated\nduring the process of voice recording but not the voice itself, including\nmicrophone types, place and ambient noise levels. Specifically, we\nselect pairs of segments from utterances and train deep neural networks\nto determine whether the acoustic configurations of the two segments\nare identical. We validate the effectiveness of the proposed method\nbased on the ASVspoof 2019 physical access dataset utilizing two well-performing\nsystems. The experimental results demonstrate that the proposed method\noutperforms the baseline approach by 30%.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1345",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "g20_interspeech": {
      "authors": [
        [
          "Abhijith",
          "G."
        ],
        [
          "Adharsh",
          "S."
        ],
        [
          "Akshay P.",
          "L."
        ],
        [
          "Rajeev",
          "Rajan"
        ]
      ],
      "title": "Competency Evaluation in Voice Mimicking Using Acoustic Cues",
      "original": "1790",
      "page_count": 5,
      "order": 231,
      "p1": "1096",
      "pn": "1100",
      "abstract": [
        "The fusion of i-vector with prosodic features is used to identify the\nmost competent voice imitator through a deep neural network framework\n(DNN) in this paper. This experiment is conducted by analyzing the\nspectral and prosodic characteristics during voice imitation. Spectral\nfeatures include mel-frequency cepstral features (MFCC) and modified\ngroup delay features (MODGDF). Prosodic features, computed by the Legendre\npolynomial approximation, are used as complementary information to\nthe i-vector model. Proposed system evaluates the competence of artists\nin voice mimicking and ranks them according to the scores from a classifier\nbased on mean opinion score (MOS). If the artist with the highest MOS\nis identified as rank-1 by the proposed system, a hit occurs. DNN-based\nclassifier makes the decision based on the probability value on the\nnodes at the output layer. The performance is evaluated using top X-hit\ncriteria on a mimicry dataset. Top-2 hit rate of 81.81% is obtained\nfor fusion experiment. The experiments demonstrate the potential of\ni-vector framework and its fusion in competency evaluation of voice\nmimicking.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1790",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "wu20c_interspeech": {
      "authors": [
        [
          "Zhenzong",
          "Wu"
        ],
        [
          "Rohan Kumar",
          "Das"
        ],
        [
          "Jichen",
          "Yang"
        ],
        [
          "Haizhou",
          "Li"
        ]
      ],
      "title": "Light Convolutional Neural Network with Feature Genuinization for Detection of Synthetic Speech Attacks",
      "original": "1810",
      "page_count": 5,
      "order": 232,
      "p1": "1101",
      "pn": "1105",
      "abstract": [
        "Modern text-to-speech (TTS) and voice conversion (VC) systems produce\nnatural sounding speech that questions the security of automatic speaker\nverification (ASV). This makes detection of such synthetic speech very\nimportant to safeguard ASV systems from unauthorized access. Most of\nthe existing spoofing countermeasures perform well when the nature\nof the attacks is made known to the system during training. However,\ntheir performance degrades in face of unseen nature of attacks. In\ncomparison to the synthetic speech created by a wide range of TTS and\nVC methods, genuine speech has a more consistent distribution. We believe\nthat the difference between the distribution of synthetic and genuine\nspeech is an important discriminative feature between the two classes.\nIn this regard, we propose a novel method referred to as feature genuinization\nthat learns a transformer with convolutional neural network (CNN) using\nthe characteristics of only genuine speech. We then use this genuinization\ntransformer with a light CNN classifier. The ASVspoof 2019 logical\naccess corpus is used to evaluate the proposed method. The studies\nshow that the proposed feature genuinization based LCNN system outperforms\nother state-of-the-art spoofing countermeasures, depicting its effectiveness\nfor detection of synthetic speech attacks.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1810",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "tak20_interspeech": {
      "authors": [
        [
          "Hemlata",
          "Tak"
        ],
        [
          "Jose",
          "Patino"
        ],
        [
          "Andreas",
          "Nautsch"
        ],
        [
          "Nicholas",
          "Evans"
        ],
        [
          "Massimiliano",
          "Todisco"
        ]
      ],
      "title": "Spoofing Attack Detection Using the Non-Linear Fusion of Sub-Band Classifiers",
      "original": "1844",
      "page_count": 5,
      "order": 233,
      "p1": "1106",
      "pn": "1110",
      "abstract": [
        "The threat of spoofing can pose a risk to the reliability of automatic\nspeaker verification. Results from the biannual ASVspoof evaluations\nshow that effective countermeasures demand front-ends designed specifically\nfor the detection of spoofing artefacts. Given the diversity in spoofing\nattacks, ensemble methods are particularly effective. The work in this\npaper shows that a bank of very simple classifiers, each with a front-end\ntuned to the detection of different spoofing attacks and combined at\nthe score level through non-linear fusion, can deliver superior performance\nthan more sophisticated ensemble solutions that rely upon complex neural\nnetwork architectures. Our comparatively simple approach outperforms\nall but 2 of the 48 systems submitted to the logical access condition\nof the most recent ASVspoof 2019 challenge.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1844",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "parasu20_interspeech": {
      "authors": [
        [
          "Prasanth",
          "Parasu"
        ],
        [
          "Julien",
          "Epps"
        ],
        [
          "Kaavya",
          "Sriskandaraja"
        ],
        [
          "Gajan",
          "Suthokumar"
        ]
      ],
      "title": "Investigating Light-ResNet Architecture for Spoofing Detection Under Mismatched Conditions",
      "original": "2039",
      "page_count": 5,
      "order": 234,
      "p1": "1111",
      "pn": "1115",
      "abstract": [
        "Current approaches to Voice Presentation Attack (VPA) detection have\nlargely focused on spoofing detection within a single database and/or\nattack type. However, for practical Presentation Attack Detection (PAD)\nsystems to be adopted by industry, they must be able to generalise\nto detect diverse and previously unseen VPAs. Inspired by successful\naspects of deep learning systems for image classification such as the\nintroduction of residual mappings through shortcut connections, this\npaper proposes a novel Light-ResNet architecture that provides good\ngeneralisation across databases and attack types. The introduction\nof skip connections within residual modules enables the training of\ndeeper spoofing classifiers that can leverage more useful discriminative\ninformation learned in the hidden layers, while still generalising\nwell under mismatched conditions. Utilising the wide variety of databases\navailable for VPA research, this paper also proposes a set of generalisation\nevaluations which a practical PAD system should be able to meet: generalising\nwithin a database, generalising across databases within attack type\nand generalising across all spoofing classes. Evaluations on the ASVspoof\n2015, BTAS 2016 (replay) and ASVspoof 2017 V2.0 databases show that\nthe proposed Light-ResNet architecture is able to generalise across\nthese diverse tasks consistently, outperforming CQCC-GMM and Attentive\nFiltering Network (AFN) baselines.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2039",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "lei20_interspeech": {
      "authors": [
        [
          "Zhenchun",
          "Lei"
        ],
        [
          "Yingen",
          "Yang"
        ],
        [
          "Changhong",
          "Liu"
        ],
        [
          "Jihua",
          "Ye"
        ]
      ],
      "title": "Siamese Convolutional Neural Network Using Gaussian Probability Feature for Spoofing Speech Detection",
      "original": "2723",
      "page_count": 5,
      "order": 235,
      "p1": "1116",
      "pn": "1120",
      "abstract": [
        "The security and reliability of automatic speaker verification systems\ncan be threatened by different types of spoofing attacks using speech\nsynthetic, voice conversion, or replay. The 2-class Gaussian Mixture\nModel classifier for genuine and spoofed speech is usually used as\nthe baseline in the ASVspoof challenge, which is designed to develop\nthe generalized countermeasures with potential to detect varying and\nunforeseen spoofing attacks. In the scoring phase, the GMM accumulates\nthe scores on all frames in a test speech independently, and does not\nconsider the relationship between adjacent frames. We propose the 1-D\nConvolutional Neural Network whose input is the log-probabilities of\nthe speech frames on the GMM components. The new model considers not\nonly the score distribution of GMM components, but also the local relationship\nof frames. And the pooling is used to extract the speech global character.\nThe Siamese CNN is also proposed, which is based on two GMMs trained\non genuine and spoofed speech respectively. Experiments on the ASVspoof\n2019 challenge logical and physical access scenarios show that the\nproposed model can improve performance greatly compared with the baseline\nsystems.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2723",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "schroter20_interspeech": {
      "authors": [
        [
          "H.",
          "Schr\u00f6ter"
        ],
        [
          "T.",
          "Rosenkranz"
        ],
        [
          "A.N.",
          "Escalante-B."
        ],
        [
          "P.",
          "Zobel"
        ],
        [
          "Andreas",
          "Maier"
        ]
      ],
      "title": "Lightweight Online Noise Reduction on Embedded Devices Using Hierarchical Recurrent Neural Networks",
      "original": "1131",
      "page_count": 5,
      "order": 236,
      "p1": "1121",
      "pn": "1125",
      "abstract": [
        "Deep-learning based noise reduction algorithms have proven their success\nespecially for non-stationary noises, which makes it desirable to also\nuse them for embedded devices like hearing aids (HAs). This, however,\nis currently not possible with state-of-the-art methods. They either\nrequire a lot of parameters and computational power and thus are only\nfeasible using modern CPUs. Or they are not suitable for online processing,\nwhich requires constraints like low-latency by the filter bank and\nthe algorithm itself.<br style=\"mso-data-placement:same-cell;\" /><br\nstyle=\"mso-data-placement:same-cell;\" />\n  In this work, we propose\na mask-based noise reduction approach. Using hierarchical recurrent\nneural networks, we are able to drastically reduce the number of neurons\nper layer while including temporal context via hierarchical connections.\nThis allows us to optimize our model towards a minimum number of parameters\nand floating-point operations (FLOPs), while preserving noise reduction\nquality compared to previous work. Our smallest network contains only\n5k parameters, which makes this algorithm applicable on embedded devices.\nWe evaluate our model on a mixture of EUROM and a real-world noise\ndatabase and report objective metrics on unseen noise.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1131",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "tagliasacchi20_interspeech": {
      "authors": [
        [
          "Marco",
          "Tagliasacchi"
        ],
        [
          "Yunpeng",
          "Li"
        ],
        [
          "Karolis",
          "Misiunas"
        ],
        [
          "Dominik",
          "Roblek"
        ]
      ],
      "title": "SEANet: A Multi-Modal Speech Enhancement Network",
      "original": "1563",
      "page_count": 5,
      "order": 237,
      "p1": "1126",
      "pn": "1130",
      "abstract": [
        "We explore the possibility of leveraging accelerometer data to perform\nspeech enhancement in very noisy conditions. Although it is possible\nto only partially reconstruct user&#8217;s speech from the accelerometer,\nthe latter provides a strong conditioning signal that is not influenced\nfrom noise sources in the environment. Based on this observation, we\nfeed a multi-modal input to SEANet (Sound EnhAncement Network), a wave-to-wave\nfully convolutional model, which adopts a combination of feature losses\nand adversarial losses to reconstruct an enhanced version of user&#8217;s\nspeech. We trained our model with data collected by sensors mounted\non an earbud and synthetically corrupted by adding different kinds\nof noise sources to the audio signal. Our experimental results demonstrate\nthat it is possible to achieve very high quality results, even in the\ncase of interfering speech at the same level of loudness. A sample\nof the output produced by our model is available at <KBD>https://google-research.github.io/seanet/multimodal/speech</KBD>\n"
      ],
      "doi": "10.21437/Interspeech.2020-1563",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "chuang20_interspeech": {
      "authors": [
        [
          "Shang-Yi",
          "Chuang"
        ],
        [
          "Yu",
          "Tsao"
        ],
        [
          "Chen-Chou",
          "Lo"
        ],
        [
          "Hsin-Min",
          "Wang"
        ]
      ],
      "title": "Lite Audio-Visual Speech Enhancement",
      "original": "1617",
      "page_count": 5,
      "order": 238,
      "p1": "1131",
      "pn": "1135",
      "abstract": [
        "Previous studies have confirmed the effectiveness of incorporating\nvisual information into speech enhancement (SE) systems. Despite improved\ndenoising performance, two problems may be encountered when implementing\nan audio-visual SE (AVSE) system: (1) additional processing costs are\nincurred to incorporate visual input and (2) the use of face or lip\nimages may cause privacy problems. In this study, we propose a Lite\nAVSE (LAVSE) system to address these problems. The system includes\ntwo visual data compression techniques and removes the visual feature\nextraction network from the training model, yielding better online\ncomputation efficiency. Our experimental results indicate that the\nproposed LAVSE system can provide notably better performance than an\naudio-only SE system with a similar number of model parameters. In\naddition, the experimental results confirm the effectiveness of the\ntwo techniques for visual data compression.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1617",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "bergler20_interspeech": {
      "authors": [
        [
          "Christian",
          "Bergler"
        ],
        [
          "Manuel",
          "Schmitt"
        ],
        [
          "Andreas",
          "Maier"
        ],
        [
          "Simeon",
          "Smeele"
        ],
        [
          "Volker",
          "Barth"
        ],
        [
          "Elmar",
          "N\u00f6th"
        ]
      ],
      "title": "ORCA-CLEAN: A Deep Denoising Toolkit for Killer Whale Communication",
      "original": "1316",
      "page_count": 5,
      "order": 239,
      "p1": "1136",
      "pn": "1140",
      "abstract": [
        "In bioacoustics, passive acoustic monitoring of animals living in the\nwild, both on land and underwater, leads to large data archives characterized\nby a strong imbalance between recorded animal sounds and ambient noises.\nBioacoustic datasets suffer extremely from such large noise-variety,\ncaused by a multitude of external influences and changing environmental\nconditions over years. This leads to significant deficiencies/problems\nconcerning the analysis and interpretation of animal vocalizations\nby biologists and machine-learning algorithms. To counteract such huge\nnoise diversity, it is essential to develop a denoising procedure enabling\nautomated, efficient, and robust data enhancement. However, a fundamental\nproblem is the lack of clean/denoised ground-truth samples. The current\nwork is the first presenting a fully-automated deep denoising approach\nfor bioacoustics, not requiring any clean ground-truth, together with\none of the largest data archives recorded on killer whales ( Orcinus\nOrca) &#8212; the Orchive. Therefore, an approach, originally developed\nfor image restoration, known as Noise2Noise (N2N), was transferred\nto the field of bioacoustics, and extended by using automatic machine-generated\nbinary masks as additional network attention mechanism. Besides a significant\ncross-domain signal enhancement, our previous results regarding supervised\norca/noise segmentation and orca call type identification were outperformed\nby applying ORCA-CLEAN as additional data preprocessing/enhancement\nstep.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1316",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "zhang20i_interspeech": {
      "authors": [
        [
          "Hao",
          "Zhang"
        ],
        [
          "DeLiang",
          "Wang"
        ]
      ],
      "title": "A Deep Learning Approach to Active Noise Control",
      "original": "1768",
      "page_count": 5,
      "order": 240,
      "p1": "1141",
      "pn": "1145",
      "abstract": [
        "We formulate active noise control (ANC) as a supervised learning problem\nand propose a deep learning approach, called deep ANC, to address the\nnonlinear ANC problem. A convolutional recurrent network (CRN) is trained\nto estimate the real and imaginary spectrograms of the canceling signal\nfrom the reference signal so that the corresponding anti-noise can\neliminate or attenuate the primary noise in the ANC system. Large-scale\nmulti-condition training is employed to achieve good generalization\nand robustness against a variety of noises. The deep ANC method can\nbe trained to achieve active noise cancellation no matter whether the\nreference signal is noise or noisy speech. In addition, a delay-compensated\nstrategy is introduced to address the potential latency problem of\nANC systems. Experimental results show that the proposed method is\neffective for wide-band noise reduction and generalizes well to untrained\nnoises. Moreover, the proposed method can be trained to achieve ANC\nwithin a quiet zone.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1768",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "dinh20_interspeech": {
      "authors": [
        [
          "Tuan",
          "Dinh"
        ],
        [
          "Alexander",
          "Kain"
        ],
        [
          "Kris",
          "Tjaden"
        ]
      ],
      "title": "Improving Speech Intelligibility Through Speaker Dependent and Independent Spectral Style Conversion",
      "original": "0054",
      "page_count": 5,
      "order": 241,
      "p1": "1146",
      "pn": "1150",
      "abstract": [
        "Increasing speech intelligibility for hearing-impaired listeners and\nnormal-hearing listeners in noisy environments remains a challenging\nproblem. Spectral style conversion from habitual to clear speech is\na promising approach to address the problem. Motivated by the success\nof generative adversarial networks (GANs) in various applications of\nimage and speech processing, we explore the potential of conditional\nGANs (cGANs) to learn the mapping from habitual speech to clear speech.\nWe evaluated the performance of cGANs in three tasks: 1) speaker-dependent\none-to-one mappings, 2) speaker-independent many-to-one mappings, and\n3) speaker-independent many-to-many mappings. In the first task, cGANs\noutperformed a traditional deep neural network mapping in terms of\naverage keyword recall accuracy and the number of speakers with improved\nintelligibility. In the second task, we significantly improved intelligibility\nof one of three speakers, without any source speaker training data.\nIn the third and most challenging task, we improved keyword recall\naccuracy for two of three speakers, but without statistical significance.\n"
      ],
      "doi": "10.21437/Interspeech.2020-54",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "pedersen20_interspeech": {
      "authors": [
        [
          "Mathias B.",
          "Pedersen"
        ],
        [
          "Morten",
          "Kolb\u00e6k"
        ],
        [
          "Asger H.",
          "Andersen"
        ],
        [
          "S\u00f8ren H.",
          "Jensen"
        ],
        [
          "Jesper",
          "Jensen"
        ]
      ],
      "title": "End-to-End Speech Intelligibility Prediction Using Time-Domain Fully Convolutional Neural Networks",
      "original": "1740",
      "page_count": 5,
      "order": 242,
      "p1": "1151",
      "pn": "1155",
      "abstract": [
        "Data-driven speech intelligibility prediction has been slow to take\noff. Datasets of measured speech intelligibility are scarce, and so\ncurrent models are relatively small and rely on hand-picked features.\nClassical predictors based on psychoacoustic models and heuristics\nare still the state-of-the-art. This work proposes a U-Net inspired\nfully convolutional neural network architecture, NSIP, trained and\ntested on ten datasets to predict intelligibility of time-domain speech.\nThe architecture is compared to a frequency domain data-driven predictor\nand to the classical state-of-the-art predictors STOI, ESTOI, HASPI\nand SIIB. The performance of NSIP is found to be superior for datasets\nseen in the training phase. On unseen datasets NSIP reaches performance\ncomparable to classical predictors.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1740",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "arai20_interspeech": {
      "authors": [
        [
          "Kenichi",
          "Arai"
        ],
        [
          "Shoko",
          "Araki"
        ],
        [
          "Atsunori",
          "Ogawa"
        ],
        [
          "Keisuke",
          "Kinoshita"
        ],
        [
          "Tomohiro",
          "Nakatani"
        ],
        [
          "Toshio",
          "Irino"
        ]
      ],
      "title": "Predicting Intelligibility of Enhanced Speech Using Posteriors Derived from DNN-Based ASR System",
      "original": "1591",
      "page_count": 5,
      "order": 243,
      "p1": "1156",
      "pn": "1160",
      "abstract": [
        "The measurement of speech intelligibility (SI) still mainly relies\non time-consuming and expensive subjective experiments because no versatile\nobjective measure can predict SI. One promising candidate of an SI\nprediction method is an approach with a deep neural network (DNN)-based\nautomatic speech recognition (ASR) system, due to its recent great\nadvance. In this paper, we propose and evaluate SI prediction methods\nbased on the posteriors of DNN-based ASR systems. Posteriors, which\nare the probabilities of phones given acoustic features, are derived\nusing forced alignments between clean speech and a phone sequence.\nWe evaluated some variations of the posteriors to improve the prediction\nperformance. As a result of our experiments, a prediction method using\na squared cumulative posterior probability achieved better accuracy\nthan the conventional SI predictors based on well-established objective\nmeasures (STOI and eSTOI).\n"
      ],
      "doi": "10.21437/Interspeech.2020-1591",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "abavisani20_interspeech": {
      "authors": [
        [
          "Ali",
          "Abavisani"
        ],
        [
          "Mark",
          "Hasegawa-Johnson"
        ]
      ],
      "title": "Automatic Estimation of Intelligibility Measure for Consonants in Speech",
      "original": "2121",
      "page_count": 5,
      "order": 244,
      "p1": "1161",
      "pn": "1165",
      "abstract": [
        "In this article, we provide a model to estimate a real-valued measure\nof the intelligibility of individual speech segments. We trained regression\nmodels based on Convolutional Neural Networks (CNN) for stop consonants\n/p,t,k,b,d,&#609;/ associated with vowel /&#593;/, to estimate the\ncorresponding Signal to Noise Ratio (SNR) at which the Consonant-Vowel\n(CV) sound becomes intelligible for Normal Hearing (NH) ears. The intelligibility\nmeasure for each sound is called SNR<SUB>90</SUB>, and is defined to\nbe the SNR level at which human participants are able to recognize\nthe consonant at least 90% correctly, on average, as determined in\nprior experiments with NH subjects. Performance of the CNN is compared\nto a baseline prediction based on automatic speech recognition (ASR),\nspecifically, a constant offset subtracted from the SNR at which the\nASR becomes capable of correctly labeling the consonant. Compared to\nbaseline, our models were able to accurately estimate the SNR<SUB>90</SUB>\nintelligibility measure with less than 2 [dB<SUP>2</SUP>] Mean Squared\nError (MSE) on average, while the baseline ASR-defined measure computes\nSNR<SUB>90</SUB> with a variance of 5.2 to 26.6 [dB<SUP>2</SUP>], depending\non the consonant.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2121"
    },
    "trinh20_interspeech": {
      "authors": [
        [
          "Viet Anh",
          "Trinh"
        ],
        [
          "Michael I.",
          "Mandel"
        ]
      ],
      "title": "Large Scale Evaluation of Importance Maps in Automatic Speech Recognition",
      "original": "2883",
      "page_count": 5,
      "order": 245,
      "p1": "1166",
      "pn": "1170",
      "abstract": [
        "This paper proposes a metric that we call the structured saliency benchmark\n(SSBM) to evaluate importance maps computed for automatic speech recognizers\non individual utterances. These maps indicate time-frequency points\nof the utterance that are most important for correct recognition of\na target word. Our evaluation technique is not only suitable for standard\nclassification tasks, but is also appropriate for structured prediction\ntasks like sequence-to-sequence models. Additionally, we use this approach\nto perform a comparison of the importance maps created by our previously\nintroduced technique using &#8220;bubble noise&#8221; to identify important\npoints through correlation with a baseline approach based on smoothed\nspeech energy and forced alignment. Our results show that the bubble\nanalysis approach is better at identifying important speech regions\nthan this baseline on 100 sentences from the AMI corpus.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2883",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "li20n_interspeech": {
      "authors": [
        [
          "Jixiang",
          "Li"
        ],
        [
          "Chuming",
          "Liang"
        ],
        [
          "Bo",
          "Zhang"
        ],
        [
          "Zhao",
          "Wang"
        ],
        [
          "Fei",
          "Xiang"
        ],
        [
          "Xiangxiang",
          "Chu"
        ]
      ],
      "title": "Neural Architecture Search on Acoustic Scene Classification",
      "original": "0057",
      "page_count": 5,
      "order": 246,
      "p1": "1171",
      "pn": "1175",
      "abstract": [
        "Convolutional neural networks are widely adopted in Acoustic Scene\nClassification (ASC) tasks, but they generally carry a heavy computational\nburden. In this work, we propose a high-performance yet lightweight\nbaseline network inspired by MobileNetV2, which replaces square convolutional\nkernels with unidirectional ones to extract features alternately in\ntemporal and frequency dimensions. Furthermore, we explore a dynamic\narchitecture space built on the basis of the proposed baseline with\nthe recent Neural Architecture Search (NAS) paradigm, which first train\na supernet that incorporates all candidate architectures and then apply\na well-known evolutionary algorithm NSGA-II to discover more efficient\nnetworks with higher accuracy and lower computational cost from the\nsupernet. Experimental results demonstrate that our searched network\nis competent in ASC tasks, which achieves 90.3% F1-score on the DCASE2018\ntask 5 evaluation set, marking a new state-of-the-art performance while\nsaving 25% of FLOPs compared to our baseline network.\n"
      ],
      "doi": "10.21437/Interspeech.2020-57",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "jung20b_interspeech": {
      "authors": [
        [
          "Jee-weon",
          "Jung"
        ],
        [
          "Hye-jin",
          "Shim"
        ],
        [
          "Ju-ho",
          "Kim"
        ],
        [
          "Seung-bin",
          "Kim"
        ],
        [
          "Ha-Jin",
          "Yu"
        ]
      ],
      "title": "Acoustic Scene Classification Using Audio Tagging",
      "original": "0992",
      "page_count": 5,
      "order": 247,
      "p1": "1176",
      "pn": "1180",
      "abstract": [
        "Acoustic scene classification systems using deep neural networks classify\ngiven recordings into pre-defined classes. In this study, we propose\na novel scheme for acoustic scene classification which adopts an audio\ntagging system inspired by the human perception mechanism. When humans\nidentify an acoustic scene, the existence of different sound events\nprovides discriminative information which affects the judgement. The\nproposed framework mimics this mechanism using various approaches.\nFirstly, we employ three methods to concatenate tag vectors extracted\nusing an audio tagging system with an intermediate hidden layer of\nan acoustic scene classification system. We also explore the multi-head\nattention on the feature map of an acoustic scene classification system\nusing tag vectors. Experiments conducted on the detection and classification\nof acoustic scenes and events 2019 task 1-a dataset demonstrate the\neffectiveness of the proposed scheme. Concatenation and multi-head\nattention show a classification accuracy of 75.66% and 76.58%, respectively,\ncompared to 73.63% accuracy of the baseline. The system with the proposed\ntwo approaches combined demonstrates an accuracy of 76.75%.\n"
      ],
      "doi": "10.21437/Interspeech.2020-992",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "zhang20j_interspeech": {
      "authors": [
        [
          "Liwen",
          "Zhang"
        ],
        [
          "Jiqing",
          "Han"
        ],
        [
          "Ziqiang",
          "Shi"
        ]
      ],
      "title": "ATReSN-Net: Capturing Attentive Temporal Relations in Semantic Neighborhood for Acoustic Scene Classification",
      "original": "1151",
      "page_count": 5,
      "order": 248,
      "p1": "1181",
      "pn": "1185",
      "abstract": [
        "Convolutional Neural Networks (CNNs) have been widely investigated\non Acoustic Scene Classification (ASC). Where the convolutional operation\ncan extract useful semantic contents from a local receptive field in\nthe input spectrogram within certain Manhattan distance, i.e., the\nkernel size. Although stacking multiple convolution layers can increase\nthe range of the receptive field, without explicitly considering the\ntemporal relations of different receptive fields, the increased range\nis limited around the kernel. In this paper, we propose a 3D CNN for\nASC, named ATReSN-Net, which can capture temporal relations of different\nreceptive fields from arbitrary time-frequency locations by mapping\nthe semantic features obtained from the residual block into a semantic\nspace. The ATReSN module has two primary components: first, a k-NN-based\ngrouper for gathering a semantic neighborhood for each feature point\nin the feature maps. Second, an attentive pooling-based temporal relations\naggregator for generating the temporal relations embedding of each\nfeature point and its neighborhood. Experiments showed that our ATReSN-Net\noutperforms most of the state-of-the-art CNN models. We shared our\ncode at ATReSN-Net.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1151",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "sharma20_interspeech": {
      "authors": [
        [
          "Jivitesh",
          "Sharma"
        ],
        [
          "Ole-Christoffer",
          "Granmo"
        ],
        [
          "Morten",
          "Goodwin"
        ]
      ],
      "title": "Environment Sound Classification Using Multiple Feature Channels and Attention Based Deep Convolutional Neural Network",
      "original": "1303",
      "page_count": 5,
      "order": 249,
      "p1": "1186",
      "pn": "1190",
      "abstract": [
        "In this paper, we propose a model for the Environment Sound Classification\nTask (ESC) that consists of multiple feature channels given as input\nto a Deep Convolutional Neural Network (CNN) with Attention mechanism.\nThe novelty of the paper lies in using multiple feature channels consisting\nof Mel-Frequency Cepstral Coefficients (MFCC), Gammatone Frequency\nCepstral Coefficients (GFCC), the Constant Q-transform (CQT) and Chromagram.\nAnd, we employ a deeper CNN (DCNN) compared to previous models, consisting\nof spatially separable convolutions working on time and feature domain\nseparately. Alongside, we use attention modules that perform channel\nand spatial attention together. We use the mix-up data augmentation\ntechnique to further boost performance. Our model is able to achieve\nstate-of-the-art performance on three benchmark environment sound classification\ndatasets, i.e. the UrbanSound8K (97.52%), ESC-10 (94.75%) and ESC-50\n(87.45%).\n"
      ],
      "doi": "10.21437/Interspeech.2020-1303",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "wang20m_interspeech": {
      "authors": [
        [
          "Weimin",
          "Wang"
        ],
        [
          "Weiran",
          "Wang"
        ],
        [
          "Ming",
          "Sun"
        ],
        [
          "Chao",
          "Wang"
        ]
      ],
      "title": "Acoustic Scene Analysis with Multi-Head Attention Networks",
      "original": "1342",
      "page_count": 5,
      "order": 250,
      "p1": "1191",
      "pn": "1195",
      "abstract": [
        "Acoustic Scene Classification (ASC) is a challenging task, as a single\nscene may involve multiple events that contain complex sound patterns.\nFor example, a cooking scene may contain several sound sources including\nsilverware clinking, chopping, frying, etc. What complicates ASC more\nis that classes of different activities could have overlapping sounds\npatterns (e.g. both cooking and dishwashing could have silverware clinking\nsound). In this paper, we propose a multi-head attention network to\nmodel the complex temporal input structures for ASC. The proposed network\ntakes the audio&#8217;s time-frequency representation as input, and\nit leverages standard VGG plus LSTM layers to extract high-level feature\nrepresentation. Further more, it applies multiple attention heads to\nsummarize various patterns of sound events into fixed dimensional representation,\nfor the purpose of final scene classification. The whole network is\ntrained in an end-to-end fashion with backpropagation. Experimental\nresults confirm that our model discovers meaningful sound patterns\nthrough the attention mechanism, without using explicit supervision\nin the alignment. We evaluated our proposed model using DCASE 2018\nTask 5 dataset, and achieved competitive performance on par with previous\nwinner&#8217;s results.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1342",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "hu20e_interspeech": {
      "authors": [
        [
          "Hu",
          "Hu"
        ],
        [
          "Sabato Marco",
          "Siniscalchi"
        ],
        [
          "Yannan",
          "Wang"
        ],
        [
          "Chin-Hui",
          "Lee"
        ]
      ],
      "title": "Relational Teacher Student Learning with Neural Label Embedding for Device Adaptation in Acoustic Scene Classification",
      "original": "2038",
      "page_count": 5,
      "order": 251,
      "p1": "1196",
      "pn": "1200",
      "abstract": [
        "In this paper, we propose a domain adaptation framework to address\nthe device mismatch issue in acoustic scene classification leveraging\nupon neural label embedding (NLE) and relational teacher student learning\n(RTSL). Taking into account the structural relationships between acoustic\nscene classes, our proposed framework captures such relationships which\nare intrinsically device-independent. In the training stage, transferable\nknowledge is condensed in NLE from the source domain. Next in the adaptation\nstage, a novel RTSL strategy is adopted to learn adapted target models\nwithout using paired source-target data often required in conventional\nteacher student learning. The proposed framework is evaluated on the\nDCASE 2018 Task1b data set. Experimental results based on AlexNet-L\ndeep classification models confirm the effectiveness of our proposed\napproach for mismatch situations. NLE-alone adaptation compares favourably\nwith the conventional device adaptation and teacher student based adaptation\ntechniques. NLE with RTSL further improves the classification accuracy.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2038",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "hu20f_interspeech": {
      "authors": [
        [
          "Hu",
          "Hu"
        ],
        [
          "Sabato Marco",
          "Siniscalchi"
        ],
        [
          "Yannan",
          "Wang"
        ],
        [
          "Xue",
          "Bai"
        ],
        [
          "Jun",
          "Du"
        ],
        [
          "Chin-Hui",
          "Lee"
        ]
      ],
      "title": "An Acoustic Segment Model Based Segment Unit Selection Approach to Acoustic Scene Classification with Partial Utterances",
      "original": "2044",
      "page_count": 5,
      "order": 252,
      "p1": "1201",
      "pn": "1205",
      "abstract": [
        "In this paper, we propose a sub-utterance unit selection framework\nto remove acoustic segments in audio recordings that carry little information\nfor acoustic scene classification (ASC). Our approach is built upon\na universal set of acoustic segment units covering the overall acoustic\nscene space. First, those units are modeled with acoustic segment models\n(ASMs) used to tokenize acoustic scene utterances into sequences of\nacoustic segment units. Next, paralleling the idea of stop words in\ninformation retrieval, stop ASMs are automatically detected. Finally,\nacoustic segments associated with the stop ASMs are blocked, because\nof their low indexing power in retrieval of most acoustic scenes. In\ncontrast to building scene models with whole utterances, the ASM-removed\nsub-utterances, i.e., acoustic utterances without stop acoustic segments,\nare then used as inputs to the AlexNet-L back-end for final classification.\nOn the DCASE 2018 dataset, scene classification accuracy increases\nfrom 68%, with whole utterances, to 72.1%, with segment selection.\nThis represents a competitive accuracy without any data augmentation,\nand/or ensemble strategy. Moreover, our approach compares favourably\nto AlexNet-L with attention.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2044",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "devalraju20_interspeech": {
      "authors": [
        [
          "Dhanunjaya Varma",
          "Devalraju"
        ],
        [
          "Muralikrishna",
          "H."
        ],
        [
          "Padmanabhan",
          "Rajan"
        ],
        [
          "Dileep Aroor",
          "Dinesh"
        ]
      ],
      "title": "Attention-Driven Projections for Soundscape Classification",
      "original": "2476",
      "page_count": 5,
      "order": 253,
      "p1": "1206",
      "pn": "1210",
      "abstract": [
        "Acoustic soundscapes can be made up of background sound events and\nforeground sound events. Many times, either the background (or the\nforeground) may provide useful cues in discriminating one soundscape\nfrom another. A part of the background or a part of the foreground\ncan be suppressed by using subspace projections. These projections\ncan be learnt by utilising the framework of robust principal component\nanalysis. In this work, audio signals are represented as embeddings\nfrom a convolutional neural network, and meta-embeddings are derived\nusing an attention mechanism. This representation enables the use of\nclass-specific projections for effective suppression, leading to good\ndiscrimination. Our experimental evaluation demonstrates the effectiveness\nof the method on standard datasets for acoustic scene classification.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2476",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "tzirakis20_interspeech": {
      "authors": [
        [
          "Panagiotis",
          "Tzirakis"
        ],
        [
          "Alexander",
          "Shiarella"
        ],
        [
          "Robert",
          "Ewers"
        ],
        [
          "Bj\u00f6rn W.",
          "Schuller"
        ]
      ],
      "title": "Computer Audition for Continuous Rainforest Occupancy Monitoring: The Case of Bornean Gibbons&#8217; Call Detection",
      "original": "2655",
      "page_count": 5,
      "order": 254,
      "p1": "1211",
      "pn": "1215",
      "abstract": [
        "Auditory data is used by ecologists for a variety of purposes, including\nidentifying species ranges, estimating population sizes, and studying\nbehaviour. Autonomous recording units (ARUs) enable auditory data collection\nover a wider area, and can provide improved consistency over traditional\nsampling methods. The result is an abundance of audio data &#8212;\nmuch more than can be analysed by scientists with the appropriate taxonomic\nskills. In this paper, we address the divide between academic machine\nlearning research on animal vocalisation classifiers, and their application\nto conservation efforts. As a unique case study, we build a Bornean\ngibbon call detection system by first manually annotating existing\ndata, and then comparing audio analysis tool kits including end-to-end\nand bag-of-audio-word modelling. Finally, we propose a deep architecture\nthat outperforms the other approaches with respect to unweighted average\nrecall. The code is available at: <KBD>https://github.com/glam-imperial/Bornean-Gibbons-Call-Detection</KBD>\n"
      ],
      "doi": "10.21437/Interspeech.2020-2655",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "kwiatkowska20_interspeech": {
      "authors": [
        [
          "Zuzanna",
          "Kwiatkowska"
        ],
        [
          "Beniamin",
          "Kalinowski"
        ],
        [
          "Micha\u0142",
          "Ko\u015bmider"
        ],
        [
          "Krzysztof",
          "Rykaczewski"
        ]
      ],
      "title": "Deep Learning Based Open Set Acoustic Scene Classification",
      "original": "3092",
      "page_count": 5,
      "order": 255,
      "p1": "1216",
      "pn": "1220",
      "abstract": [
        "In this work, we compare the performance of three selected techniques\nin open set acoustic scenes classification (ASC). We test thresholding\nof the softmax output of a deep network classifier, which is the most\npopular technique nowadays employed in ASC. Further we compare the\nresults with the Openmax classifier which is derived from the computer\nvision field. As the third model, we use the Adapted Class-Conditioned\nAutoencoder (Adapted C2AE) which is our variation of another computer\nvision related technique called C2AE. Adapted C2AE encompasses a more\nfair comparison of the given experiments and simplifies the original\ninference procedure, making it more applicable in the real-life scenarios.\nWe also analyse two training scenarios: without additional knowledge\nof unknown classes and another where a limited subset of examples from\nthe unknown classes is available. We find that the C2AE based method\noutperforms the thresholding and Openmax, obtaining 85.5% Area Under\nthe Receiver Operating Characteristic curve (AUROC) and 66% of open\nset accuracy on data used in Detection and Classification of Acoustic\nScenes and Events Challenge 2019 Task 1C.\n"
      ],
      "doi": "10.21437/Interspeech.2020-3092",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "angelini20_interspeech": {
      "authors": [
        [
          "Orazio",
          "Angelini"
        ],
        [
          "Alexis",
          "Moinet"
        ],
        [
          "Kayoko",
          "Yanagisawa"
        ],
        [
          "Thomas",
          "Drugman"
        ]
      ],
      "title": "Singing Synthesis: With a Little Help from my Attention",
      "original": "1399",
      "page_count": 5,
      "order": 256,
      "p1": "1221",
      "pn": "1225",
      "abstract": [
        "We present UTACO, a singing synthesis model based on an attention-based\nsequence-to-sequence mechanism and a vocoder based on dilated causal\nconvolutions. These two classes of models have significantly affected\nthe field of text-to-speech, but have never been thoroughly applied\nto the task of singing synthesis. UTACO demonstrates that attention\ncan be successfully applied to the singing synthesis field and improves\nnaturalness over the state of the art. The system requires considerably\nless explicit modelling of voice features such as F0 patterns, vibratos,\nand note and phoneme durations, than previous models in the literature.\nDespite this, it shows a strong improvement in naturalness with respect\nto previous neural singing synthesis models. The model does not require\nany durations or pitch patterns as inputs, and learns to insert vibrato\nautonomously according to the musical context. However, we observe\nthat, by completely dispensing with any explicit duration modelling\nit becomes harder to obtain the fine control of timing needed to exactly\nmatch the tempo of a song.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1399",
      "author_area_id": "13",
      "author_area_label": "Special Sessions"
    },
    "wu20d_interspeech": {
      "authors": [
        [
          "Yusong",
          "Wu"
        ],
        [
          "Shengchen",
          "Li"
        ],
        [
          "Chengzhu",
          "Yu"
        ],
        [
          "Heng",
          "Lu"
        ],
        [
          "Chao",
          "Weng"
        ],
        [
          "Liqiang",
          "Zhang"
        ],
        [
          "Dong",
          "Yu"
        ]
      ],
      "title": "Peking Opera Synthesis via Duration Informed Attention Network",
      "original": "1724",
      "page_count": 5,
      "order": 257,
      "p1": "1226",
      "pn": "1230",
      "abstract": [
        "Peking Opera has been the most dominant form of Chinese performing\nart since around 200 years ago. A Peking Opera singer usually exhibits\na very strong personal style via introducing improvisation and expressiveness\non stage which leads the actual rhythm and pitch contour to deviate\nsignificantly from the original music score. This inconsistency poses\na great challenge in Peking Opera singing voice synthesis from a music\nscore. In this work, we propose to deal with this issue and synthesize\nexpressive Peking Opera singing from the music score based on the Duration\nInformed Attention Network (DurIAN) framework. To tackle the rhythm\nmismatch, Lagrange multiplier is used to find the optimal output phoneme\nduration sequence with the constraint of the given note duration from\nmusic score. As for the pitch contour mismatch, instead of directly\ninferring from music score, we adopt a pseudo music score generated\nfrom the real singing and feed it as input during training. The experiments\ndemonstrate that with the proposed system we can synthesize Peking\nOpera singing voice with high-quality timbre, pitch and expressiveness.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1724",
      "author_area_id": "13",
      "author_area_label": "Special Sessions"
    },
    "zhang20k_interspeech": {
      "authors": [
        [
          "Liqiang",
          "Zhang"
        ],
        [
          "Chengzhu",
          "Yu"
        ],
        [
          "Heng",
          "Lu"
        ],
        [
          "Chao",
          "Weng"
        ],
        [
          "Chunlei",
          "Zhang"
        ],
        [
          "Yusong",
          "Wu"
        ],
        [
          "Xiang",
          "Xie"
        ],
        [
          "Zijin",
          "Li"
        ],
        [
          "Dong",
          "Yu"
        ]
      ],
      "title": "DurIAN-SC: Duration Informed Attention Network Based Singing Voice Conversion System",
      "original": "1789",
      "page_count": 5,
      "order": 258,
      "p1": "1231",
      "pn": "1235",
      "abstract": [
        "Singing voice conversion is converting the timbre in the source singing\nto the target speaker&#8217;s voice while keeping singing content the\nsame. However, singing data for target speaker is much more difficult\nto collect compared with normal speech data. In this paper, we introduce\na singing voice conversion algorithm that is capable of generating\nhigh quality target speaker&#8217;s singing using only his/her normal\nspeech data. First, we manage to integrate the training and conversion\nprocess of speech and singing into one framework by unifying the features\nused in standard speech synthesis system and singing synthesis system.\nIn this way, normal speech data can also contribute to singing voice\nconversion training, making the singing voice conversion system more\nrobust especially when the singing database is small. Moreover, in\norder to achieve one-shot singing voice conversion, a speaker embedding\nmodule is developed using both speech and singing data, which provides\ntarget speaker identify information during conversion. Experiments\nindicate proposed sing conversion system can convert source singing\nto target speaker&#8217;s high-quality singing with only 20 seconds\nof target speaker&#8217;s enrollment speech data.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1789",
      "author_area_id": "13",
      "author_area_label": "Special Sessions"
    },
    "hou20b_interspeech": {
      "authors": [
        [
          "Yuanbo",
          "Hou"
        ],
        [
          "Frank K.",
          "Soong"
        ],
        [
          "Jian",
          "Luan"
        ],
        [
          "Shengchen",
          "Li"
        ]
      ],
      "title": "Transfer Learning for Improving Singing-Voice Detection in Polyphonic Instrumental Music",
      "original": "1806",
      "page_count": 5,
      "order": 259,
      "p1": "1236",
      "pn": "1240",
      "abstract": [
        "Detecting singing-voice in polyphonic instrumental music is critical\nto music information retrieval. To train a robust vocal detector, a\nlarge dataset marked with  vocal or  non-vocal label at frame-level\nis essential. However, frame-level labeling is time-consuming and labor\nexpensive, resulting there is little well-labeled dataset available\nfor singing-voice detection (S-VD). Hence, we propose a data augmentation\nmethod for S-VD by transfer learning. In this study, clean speech clips\nwith voice activity endpoints and separate instrumental music clips\nare artificially added together to simulate polyphonic vocals to train\na  vocal /non-vocal detector. Due to the different articulation and\nphonation between speaking and singing, the vocal detector trained\nwith the artificial dataset does not match well with the polyphonic\nmusic which is singing vocals together with the instrumental accompaniments.\nTo reduce this mismatch, transfer learning is used to transfer the\nknowledge learned from the artificial speech-plus-music training set\nto a small but matched polyphonic dataset, i.e., singing vocals with\naccompaniments. By transferring the related knowledge to make up for\nthe lack of well-labeled training data in S-VD, the proposed data augmentation\nmethod by transfer learning can improve S-VD performance with an  F-score\nimprovement from 89.5% to 93.2%.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1806",
      "author_area_id": "13",
      "author_area_label": "Special Sessions"
    },
    "liu20f_interspeech": {
      "authors": [
        [
          "Haohe",
          "Liu"
        ],
        [
          "Lei",
          "Xie"
        ],
        [
          "Jian",
          "Wu"
        ],
        [
          "Geng",
          "Yang"
        ]
      ],
      "title": "Channel-Wise Subband Input for Better Voice and Accompaniment Separation on High Resolution Music",
      "original": "2555",
      "page_count": 5,
      "order": 260,
      "p1": "1241",
      "pn": "1245",
      "abstract": [
        "This paper presents a new input format, channel-wise subband input\n(CWS), for convolutional neural networks (CNN) based music source separation\n(MSS) models in the frequency domain. We aim to address the major issues\nin CNN-based high-resolution MSS model: high computational cost and\nweight sharing between distinctly different bands. Specifically, in\nthis paper, we decompose the input mixture spectra into several bands\nand concatenate them channel-wise as the model input. The proposed\napproach enables effective weight sharing in each subband and introduces\nmore flexibility between channels. For comparison purposes, we perform\nvoice and accompaniment separation (VAS) on models with different scales,\narchitectures, and CWS settings. Experiments show that the CWS input\nis beneficial in many aspects. We evaluate our method on  musdb18hq\ntest set, focusing on SDR, SIR and SAR metrics. Among all our experiments,\nCWS enables models to obtain 6.9% performance gain on the average metrics.\nWith even a smaller number of parameters, less training data, and shorter\ntraining time, ourMDenseNet with 8-bands CWS input still surpasses\nthe original MMDenseNet with a large margin. Moreover, CWS also reduces\ncomputational cost and training time to a large extent.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2555",
      "author_area_id": "13",
      "author_area_label": "Special Sessions"
    },
    "sadhu20_interspeech": {
      "authors": [
        [
          "Samik",
          "Sadhu"
        ],
        [
          "Hynek",
          "Hermansky"
        ]
      ],
      "title": "Continual Learning in Automatic Speech Recognition",
      "original": "2962",
      "page_count": 5,
      "order": 261,
      "p1": "1246",
      "pn": "1250",
      "abstract": [
        "We emulate continual learning observed in real life, where new training\ndata, which represent new application domain, are used for gradual\nimprovement of an Automatic Speech Recognizer (ASR) trained on old\ndomains. The data on which the original classifier was trained is no\nlonger required and we observe no loss of performance on the original\ndomain. Further, on previously unseen domain, our technique appears\nto yield slight advantage over offline multi-condition training. The\nproposed learning technique is consistent with our previously studied\n ad hoc stream attention based multi-stream ASR.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2962",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "wan20_interspeech": {
      "authors": [
        [
          "Genshun",
          "Wan"
        ],
        [
          "Jia",
          "Pan"
        ],
        [
          "Qingran",
          "Wang"
        ],
        [
          "Jianqing",
          "Gao"
        ],
        [
          "Zhongfu",
          "Ye"
        ]
      ],
      "title": "Speaker Adaptive Training for Speech Recognition Based on Attention-Over-Attention Mechanism",
      "original": "1727",
      "page_count": 5,
      "order": 262,
      "p1": "1251",
      "pn": "1255",
      "abstract": [
        "In our previous work, we introduced a speaker adaptive training method\nbased on frame-level attention mechanism for speech recognition, which\nhas been proved an effective way to do speaker adaptive training. In\nthis paper, we present an improved method by introducing the attention-over-attention\nmechanism. This attention module is used to further measure the contribution\nof each frame to the speaker embeddings in an utterance, and then generate\nan utterance-level speaker embedding to perform speaker adaptive training.\nCompared with the frame-level ones, the generated utterance-level speaker\nembeddings are more representative and stable. Experiments on both\nthe Switchboard and AISHELL-2 tasks show that our method can achieve\na relative word error rate reduction of approximately 8.0% compared\nwith the speaker independent model, and over 6.0% compared with the\ntraditional utterance-level d-vector-based speaker adaptive training\nmethod.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1727",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "huang20c_interspeech": {
      "authors": [
        [
          "Yan",
          "Huang"
        ],
        [
          "Jinyu",
          "Li"
        ],
        [
          "Lei",
          "He"
        ],
        [
          "Wenning",
          "Wei"
        ],
        [
          "William",
          "Gale"
        ],
        [
          "Yifan",
          "Gong"
        ]
      ],
      "title": "Rapid RNN-T Adaptation Using Personalized Speech Synthesis and Neural Language Generator",
      "original": "1290",
      "page_count": 5,
      "order": 263,
      "p1": "1256",
      "pn": "1260",
      "abstract": [
        "Rapid unsupervised speaker adaptation in an E2E system posits us new\nchallenges due to its end-to-end unified structure in addition to its\nintrinsic difficulty of data sparsity and imperfect label [1]. Previously\nwe proposed utilizing the content relevant personalized speech synthesis\nfor rapid speaker adaptation and achieved significant performance breakthrough\nin a hybrid system [2]. In this paper, we answer the following two\nquestions: First, how to effectively perform rapid speaker adaptation\nin an RNN-T. Second, whether our previously proposed approach is still\nbeneficial for the RNN-T and what are the modification and distinct\nobservations. We apply the proposed methodology to a speaker adaptation\ntask in a state-of-art presentation transcription RNN-T system. In\nthe 1 min setup, it yields 11.58% or 7.95% relative word error rate\n(WER) reduction for the sup/unsup adaptation, comparing to the negligible\ngain when adapting with 1 min source speech. In the 10 min setup, it\nyields 15.71% or 8.00% relative WER reduction, doubling the gain of\nthe source speech adaptation. We further apply various data filtering\ntechniques and significantly bridge the gap between sup/unsup adaptation.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1290",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "zhao20b_interspeech": {
      "authors": [
        [
          "Yingzhu",
          "Zhao"
        ],
        [
          "Chongjia",
          "Ni"
        ],
        [
          "Cheung-Chi",
          "Leung"
        ],
        [
          "Shafiq",
          "Joty"
        ],
        [
          "Eng Siong",
          "Chng"
        ],
        [
          "Bin",
          "Ma"
        ]
      ],
      "title": "Speech Transformer with Speaker Aware Persistent Memory",
      "original": "1281",
      "page_count": 5,
      "order": 264,
      "p1": "1261",
      "pn": "1265",
      "abstract": [
        "End-to-end models have been introduced into automatic speech recognition\n(ASR) successfully and achieved superior performance compared with\nconventional hybrid systems, especially with the newly proposed transformer\nmodel. However, speaker mismatch between training and test data remains\na problem, and speaker adaptation for transformer model can be further\nimproved. In this paper, we propose to conduct speaker aware training\nfor ASR in transformer model. Specifically, we propose to embed speaker\nknowledge through a persistent memory model into speech transformer\nencoder at utterance level. The speaker information is represented\nby a number of static speaker i-vectors, which is concatenated to speech\nutterance at each encoder self-attention layer. Persistent memory is\nthus formed by carrying speaker information through the depth of encoder.\nThe speaker knowledge is captured from self-attention between speech\nand persistent memory vector in encoder. Experiment results on LibriSpeech,\nSwitchboard and AISHELL-1 ASR task show that our proposed model brings\nrelative 4.7%&#8211;12.5% word error rate (WER) reductions, and achieves\nsuperior results compared with other models with the same objective.\nFurthermore, our model brings relative 2.1%&#8211;8.3% WER reductions\ncompared with the first persistent memory model used in ASR.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1281",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "ding20d_interspeech": {
      "authors": [
        [
          "Fenglin",
          "Ding"
        ],
        [
          "Wu",
          "Guo"
        ],
        [
          "Bin",
          "Gu"
        ],
        [
          "Zhen-Hua",
          "Ling"
        ],
        [
          "Jun",
          "Du"
        ]
      ],
      "title": "Adaptive Speaker Normalization for CTC-Based Speech Recognition",
      "original": "1390",
      "page_count": 5,
      "order": 265,
      "p1": "1266",
      "pn": "1270",
      "abstract": [
        "In this paper, we propose a new speaker normalization technique for\nacoustic model adaptation in connectionist temporal classification\n(CTC)-based automatic speech recognition. In the proposed method, for\nthe inputs of a hidden layer, the mean and variance of each activation\nare first estimated at the speaker level. Then, we normalize each speaker\nrepresentation independently by making them follow a standard normal\ndistribution. Furthermore, we propose using an auxiliary network to\ndynamically generate the scaling and shifting parameters of speaker\nnormalization, and an attention mechanism is introduced to improve\nperformance. The experiments are conducted on the public Chinese dataset\nAISHELL-1. Our proposed methods present high effectiveness in adapting\nthe CTC model, achieving up to 17.5% character error rate improvement\nover the speaker-independent (SI) model.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1390",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "mathur20_interspeech": {
      "authors": [
        [
          "Akhil",
          "Mathur"
        ],
        [
          "Nadia",
          "Berthouze"
        ],
        [
          "Nicholas D.",
          "Lane"
        ]
      ],
      "title": "Unsupervised Domain Adaptation Under Label Space Mismatch for Speech Classification",
      "original": "1861",
      "page_count": 5,
      "order": 266,
      "p1": "1271",
      "pn": "1275",
      "abstract": [
        "Unsupervised domain adaptation using adversarial learning has shown\npromise in adapting speech models from a labeled source domain to an\nunlabeled target domain. However, prior works make a strong assumption\nthat the label spaces of source and target domains are identical, which\ncan be easily violated in real-world conditions. We present AMLS, an\nend-to-end architecture that performs  Adaptation under Mismatched\nLabel Spaces using two weighting schemes to separate shared and private\nclasses in each domain. An evaluation on three speech adaptation tasks,\nnamely gender, microphone, and emotion adaptation, shows that AMLS\nprovides significant accuracy gains over baselines used in speech and\nvision adaptation tasks. Our contribution paves the way for applying\nUDA to speech models in unconstrained settings with no assumptions\non the source and target label spaces.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1861",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "winata20_interspeech": {
      "authors": [
        [
          "Genta Indra",
          "Winata"
        ],
        [
          "Samuel",
          "Cahyawijaya"
        ],
        [
          "Zihan",
          "Liu"
        ],
        [
          "Zhaojiang",
          "Lin"
        ],
        [
          "Andrea",
          "Madotto"
        ],
        [
          "Peng",
          "Xu"
        ],
        [
          "Pascale",
          "Fung"
        ]
      ],
      "title": "Learning Fast Adaptation on Cross-Accented Speech Recognition",
      "original": "0045",
      "page_count": 5,
      "order": 267,
      "p1": "1276",
      "pn": "1280",
      "abstract": [
        "Local dialects influence people to pronounce words of the same language\ndifferently from each other. The great variability and complex characteristics\nof accents create a major challenge for training a robust and accent-agnostic\nautomatic speech recognition (ASR) system. In this paper, we introduce\na cross-accented English speech recognition task as a benchmark for\nmeasuring the ability of the model to adapt to unseen accents using\nthe existing CommonVoice corpus. We also propose an accent-agnostic\napproach that extends the model-agnostic meta-learning (MAML) algorithm\nfor fast adaptation to unseen accents. Our approach significantly outperforms\njoint training in both zero-shot, few-shot, and all-shot in the mixed-region\nand cross-region settings in terms of word error rate.\n"
      ],
      "doi": "10.21437/Interspeech.2020-45",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "khandelwal20_interspeech": {
      "authors": [
        [
          "Kartik",
          "Khandelwal"
        ],
        [
          "Preethi",
          "Jyothi"
        ],
        [
          "Abhijeet",
          "Awasthi"
        ],
        [
          "Sunita",
          "Sarawagi"
        ]
      ],
      "title": "Black-Box Adaptation of ASR for Accented Speech",
      "original": "3162",
      "page_count": 5,
      "order": 268,
      "p1": "1281",
      "pn": "1285",
      "abstract": [
        "We introduce the problem of adapting a black-box, cloud-based ASR system\nto speech from a target accent. While leading online ASR services obtain\nimpressive performance on mainstream accents, they perform poorly on\nsub-populations &#8212; we observed that the word error rate (WER)\nachieved by Google&#8217;s ASR API on Indian accents is almost twice\nthe WER on US accents. Existing adaptation methods either require access\nto model parameters or overlay an error correcting module on output\ntranscripts. We highlight the need for correlating outputs with the\noriginal speech to fix accent errors. Accordingly, we propose a novel\ncoupling of an open-source accent-tuned local model with the black-box\nservice where the output from the service guides frame-level inference\nin the local model. Our fine-grained merging algorithm is better at\nfixing accent errors than existing word-level combination strategies.\nExperiments on Indian and Australian accents with three leading ASR\nmodels as service, show that we achieve upto 28% relative reduction\nin WER over both the local and service models.\n"
      ],
      "doi": "10.21437/Interspeech.2020-3162",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "turan20_interspeech": {
      "authors": [
        [
          "M.A. Tu\u011ftekin",
          "Turan"
        ],
        [
          "Emmanuel",
          "Vincent"
        ],
        [
          "Denis",
          "Jouvet"
        ]
      ],
      "title": "Achieving Multi-Accent ASR via Unsupervised Acoustic Model Adaptation",
      "original": "2742",
      "page_count": 5,
      "order": 269,
      "p1": "1286",
      "pn": "1290",
      "abstract": [
        "Current automatic speech recognition (ASR) systems trained on native\nspeech often perform poorly when applied to non-native or accented\nspeech. In this work, we propose to compute x-vector-like accent embeddings\nand use them as auxiliary inputs to an acoustic model trained on native\ndata only in order to improve the recognition of multi-accent data\ncomprising native, non-native, and accented speech. In addition, we\nleverage untranscribed accented training data by means of semi-supervised\nlearning. Our experiments show that acoustic models trained with the\nproposed accent embeddings outperform those trained with conventional\ni-vector or x-vector speaker embeddings, and achieve a 15% relative\nword error rate (WER) reduction on non-native and accented speech w.r.t.\nacoustic models trained with regular spectral features only. Semi-supervised\ntraining using just 1 hour of untranscribed speech per accent yields\nan additional 15% relative WER reduction w.r.t. models trained on native\ndata only.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2742",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "takeda20_interspeech": {
      "authors": [
        [
          "Ryu",
          "Takeda"
        ],
        [
          "Kazunori",
          "Komatani"
        ]
      ],
      "title": "Frame-Wise Online Unsupervised Adaptation of DNN-HMM Acoustic Model from Perspective of Robust Adaptive Filtering",
      "original": "1301",
      "page_count": 5,
      "order": 270,
      "p1": "1291",
      "pn": "1295",
      "abstract": [
        "We present a new frame-wise  online unsupervised adaptation method\nfor an acoustic model based on a deep neural network (DNN). This is\nin contrast to many existing methods that assume  offline and supervised\nprocessing. We use a likelihood cost function conditioned by past observations,\nwhich mathematically integrate the unsupervised adaptation and decoding\nprocess for automatic speech recognition (ASR). The issue is that the\nparameter update of the DNN should be less affected by outliers (model\nmismatch) and ASR recognition errors. Inspired by the robust adaptive\nfilter techniques, we propose 1) parameter update control to remove\nthe influence of the outliers and 2) regularization using L2-norm of\nDNN&#8217;s posterior probabilities of specific phonemes that are prone\nto recognition errors. Experiments showed that the phoneme recognition\naccuracies were improved by a maximum of 6.3 points, with an average\nerror reduction rate of 10%, for various speakers.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1301",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "wu20e_interspeech": {
      "authors": [
        [
          "Jie",
          "Wu"
        ],
        [
          "Jian",
          "Luan"
        ]
      ],
      "title": "Adversarially Trained Multi-Singer Sequence-to-Sequence Singing Synthesizer",
      "original": "1109",
      "page_count": 5,
      "order": 271,
      "p1": "1296",
      "pn": "1300",
      "abstract": [
        "This paper presents a high quality singing synthesizer that is able\nto model a voice with limited available recordings. Based on the sequence-to-sequence\nsinging model, we design a multi-singer framework to leverage all the\nexisting singing data of different singers. To attenuate the issue\nof musical score unbalance among singers, we incorporate an adversarial\ntask of singer classification to make encoder output less singer dependent.\nFurthermore, we apply multiple random window discriminators (MRWDs)\non the generated acoustic features to make the network be a GAN. Both\nobjective and subjective evaluations indicate that the proposed synthesizer\ncan generate higher quality singing voice than baseline (4.12 vs 3.53\nin MOS). Especially, the articulation of high-pitched vowels is significantly\nenhanced.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1109",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "lu20b_interspeech": {
      "authors": [
        [
          "JinHong",
          "Lu"
        ],
        [
          "Hiroshi",
          "Shimodaira"
        ]
      ],
      "title": "Prediction of Head Motion from Speech Waveforms with a Canonical-Correlation-Constrained Autoencoder",
      "original": "1218",
      "page_count": 5,
      "order": 272,
      "p1": "1301",
      "pn": "1305",
      "abstract": [
        "This study investigates the direct use of speech waveforms to predict\nhead motion for speech-driven head-motion synthesis, whereas the use\nof spectral features such as MFCC as basic input features together\nwith additional features such as energy and F0 is common in the literature.\nWe show that, rather than combining different features that originate\nfrom waveforms, it is more effective to use waveforms directly predicting\ncorresponding head motion. The challenge with the waveform-based approach\nis that waveforms contain a large amount of information irrelevant\nto predict head motion, which hinders the training of neural networks.\nTo overcome the problem, we propose a canonical-correlation-constrained\nautoencoder (CCCAE), where hidden layers are trained to not only minimise\nthe error but also maximise the canonical correlation with head motion.\nCompared with an MFCC-based system, the proposed system shows comparable\nperformance in objective evaluation, and better performance in subject\nevaluation.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1218",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "lu20c_interspeech": {
      "authors": [
        [
          "Peiling",
          "Lu"
        ],
        [
          "Jie",
          "Wu"
        ],
        [
          "Jian",
          "Luan"
        ],
        [
          "Xu",
          "Tan"
        ],
        [
          "Li",
          "Zhou"
        ]
      ],
      "title": "XiaoiceSing: A High-Quality and Integrated Singing Voice Synthesis System",
      "original": "1410",
      "page_count": 5,
      "order": 273,
      "p1": "1306",
      "pn": "1310",
      "abstract": [
        "This paper presents XiaoiceSing, a high-quality singing voice synthesis\nsystem which employs an integrated network for spectrum, F0 and duration\nmodeling. We follow the main architecture of FastSpeech while proposing\nsome singing-specific design: 1) Besides phoneme ID and position encoding,\nfeatures from musical score (e.g. note pitch and length) are also added.\n2) To attenuate off-key issues, we add a residual connection in F0\nprediction. 3) In addition to the duration loss of each phoneme, the\nduration of all the phonemes in a musical note is accumulated to calculate\nthe syllable duration loss for rhythm enhancement. Experiment results\nshow that XiaoiceSing outperforms the baseline system of convolutional\nneural networks by 1.44 MOS on sound quality, 1.18 on pronunciation\naccuracy and 1.38 on naturalness respectively. In two A/B tests, the\nproposed F0 and duration modeling methods achieve 97.3% and 84.3% preference\nrate over baseline respectively, which demonstrates the overwhelming\nadvantages of XiaoiceSing.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1410",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "yadav20_interspeech": {
      "authors": [
        [
          "Ravindra",
          "Yadav"
        ],
        [
          "Ashish",
          "Sardana"
        ],
        [
          "Vinay P.",
          "Namboodiri"
        ],
        [
          "Rajesh M.",
          "Hegde"
        ]
      ],
      "title": "Stochastic Talking Face Generation Using Latent Distribution Matching",
      "original": "1823",
      "page_count": 5,
      "order": 274,
      "p1": "1311",
      "pn": "1315",
      "abstract": [
        "The ability to envisage the visual of a talking face based just on\nhearing a voice is a unique human capability. There have been a number\nof works that have solved for this ability recently. We differ from\nthese approaches by enabling a variety of talking face generations\nbased on single audio input. Indeed, just having the ability to generate\na single talking face would make a system almost robotic in nature.\nIn contrast, our unsupervised stochastic audio-to-video generation\nmodel allows for diverse generations from a single audio input. Particularly,\nwe present an unsupervised stochastic audio-to-video generation model\nthat can capture multiple modes of the video distribution. We ensure\nthat all the diverse generations are plausible. We do so through a\nprincipled multi-modal variational autoencoder framework. We demonstrate\nits efficacy on the challenging LRWand GRID datasets and demonstrate\nperformance better than the baseline, while having the ability to generate\nmultiple diverse lip synchronized videos.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1823",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "wu20f_interspeech": {
      "authors": [
        [
          "Da-Yi",
          "Wu"
        ],
        [
          "Yi-Hsuan",
          "Yang"
        ]
      ],
      "title": "Speech-to-Singing Conversion Based on Boundary Equilibrium GAN",
      "original": "1984",
      "page_count": 5,
      "order": 275,
      "p1": "1316",
      "pn": "1320",
      "abstract": [
        "This paper investigates the use of generative adversarial network (GAN)-based\nmodels for converting a speech signal into a singing one, without reference\nto the phoneme sequence underlying the speech. This is achieved by\nviewing speech-to-singing conversion as a style transfer problem. Specifically,\ngiven a speech input, and the F0 contour of the target singing output,\nthe proposed model generates the spectrogram of a singing signal with\na progressive-growing encoder/decoder architecture. Moreover, the model\nuses a boundary equilibrium GAN loss term such that it can learn from\nboth paired and unpaired data. The spectrogram is finally converted\ninto wave with a separate GAN-based vocoder. Our quantitative and qualitative\nanalysis show that the proposed model generates singing voices with\nmuch higher naturalness than an existing non adversarially-trained\nbaseline.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1984",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "goto20_interspeech": {
      "authors": [
        [
          "Shunsuke",
          "Goto"
        ],
        [
          "Kotaro",
          "Onishi"
        ],
        [
          "Yuki",
          "Saito"
        ],
        [
          "Kentaro",
          "Tachibana"
        ],
        [
          "Koichiro",
          "Mori"
        ]
      ],
      "title": "Face2Speech: Towards Multi-Speaker Text-to-Speech Synthesis Using an Embedding Vector Predicted from a Face Image",
      "original": "2136",
      "page_count": 5,
      "order": 276,
      "p1": "1321",
      "pn": "1325",
      "abstract": [
        "We are quite able to imagine voice characteristics of a speaker from\nhis/her appearance, especially a face. In this paper, we propose Face2Speech,\nwhich generates speech with its characteristics predicted from a face\nimage. This framework consists of three separately trained modules:\na speech encoder, a multi-speaker text-to-speech (TTS), and a face\nencoder. The speech encoder outputs an embedding vector which is distinguishable\nfrom other speakers. The multi-speaker TTS synthesizes speech by using\nthe embedding vector, and then the face encoder outputs the embedding\nvector of a speaker from the speaker&#8217;s face image. Experimental\nresults of matching and naturalness tests demonstrate that synthetic\nspeech generated with the face-derived embedding vector is comparable\nto one with the speech-derived embedding vector.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2136",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "wang20n_interspeech": {
      "authors": [
        [
          "Wentao",
          "Wang"
        ],
        [
          "Yan",
          "Wang"
        ],
        [
          "Jianqing",
          "Sun"
        ],
        [
          "Qingsong",
          "Liu"
        ],
        [
          "Jiaen",
          "Liang"
        ],
        [
          "Teng",
          "Li"
        ]
      ],
      "title": "Speech Driven Talking Head Generation via Attentional Landmarks Based Representation",
      "original": "2304",
      "page_count": 5,
      "order": 277,
      "p1": "1326",
      "pn": "1330",
      "abstract": [
        "Previous talking head generation methods mostly focus on frontal face\nsynthesis while neglecting natural person head motion. In this paper,\na generative adversarial network (GAN) based method is proposed to\ngenerate talking head video with not only high quality facial appearance,\naccurate lip movement, but also natural head motion. To this aim, the\nfacial landmarks are detected and used to represent lip motion and\nhead pose, and the conversions from speech to these middle level representations\nare learned separately through Convolutional Neural Networks (CNN)\nwith wingloss. The Gated Recurrent Unit (GRU) is adopted to regularize\nthe sequential transition. The representations for different factors\nof talking head are jointly feeded to a Generative Adversarial Network\n(GAN) based model with an attentional mechanism to synthesize the talking\nvideo. Extensive experiments on the benchmark dataset as well as our\nown collected dataset validate that the propose method can yield talking\nvideos with natural head motions, and the performance is superior to\nstate-of-the-art talking face generation methods.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2304",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "schadler20_interspeech": {
      "authors": [
        [
          "Marc Ren\u00e9",
          "Sch\u00e4dler"
        ]
      ],
      "title": "Optimization and Evaluation of an Intelligibility-Improving Signal Processing Approach (IISPA) for the Hurricane Challenge 2.0 with FADE",
      "original": "0093",
      "page_count": 5,
      "order": 278,
      "p1": "1331",
      "pn": "1335",
      "abstract": [
        "This contributions describes the &#8220;IISPA&#8221; submission to\nthe Hurricane Challenge 2.0. The challenge organizers called for submissions\nof speech signals processed with the aim to improve their intelligibility\nin adverse listening conditions. They evaluated the submissions with\nmatrix sentence tests in an international listening experiment. An\nintelligibility-improving signal processing approach (IISPA) inspired\nfrom research on speech perception of listeners with impaired hearing\nwas designed. Its parameters were optimized with an objective intelligibility\nmodel, the simulation framework for auditory discrimination experiments\n(FADE). In FADE, a re-purposed automatic speech recognition (ASR) system\nis employed as a models for human speech recognition performance. The\nmodel predicted an improvement in speech recognition threshold (SRT)\nof approximately 5.0 dB due to the optimized IISPA. The processed speech\nsignals were evaluated in the Hurricane Challenge 2.0. The measured\nimprovements were language-dependent: up to 4.8 dB for the Spanish\ntest, up to 3.8 dB for the German test, and up to 2.1 dB for the English\ntest. The results show on the one hand the potential of using an ASR-based\nspeech recognition model to optimize an intelligibility-improving signal\nprocessing scheme, and on the other hand the need for thorough listening\nexperiments.\n"
      ],
      "doi": "10.21437/Interspeech.2020-93",
      "author_area_id": "13",
      "author_area_label": "Special Sessions"
    },
    "li20o_interspeech": {
      "authors": [
        [
          "Haoyu",
          "Li"
        ],
        [
          "Szu-Wei",
          "Fu"
        ],
        [
          "Yu",
          "Tsao"
        ],
        [
          "Junichi",
          "Yamagishi"
        ]
      ],
      "title": "iMetricGAN: Intelligibility Enhancement for Speech-in-Noise Using Generative Adversarial Network-Based Metric Learning",
      "original": "1016",
      "page_count": 5,
      "order": 279,
      "p1": "1336",
      "pn": "1340",
      "abstract": [
        "The intelligibility of natural speech is seriously degraded when exposed\nto adverse noisy environments. In this work, we propose a deep learning-based\nspeech modification method to compensate for the intelligibility loss,\nwith the constraint that the root mean square (RMS) level and duration\nof the speech signal are maintained before and after modifications.\nSpecifically, we utilize an iMetricGAN approach to optimize the speech\nintelligibility metrics with generative adversarial networks (GANs).\nExperimental results show that the proposed iMetricGAN outperforms\nconventional state-of-the-art algorithms in terms of objective measures,\ni.e., speech intelligibility in bits (SIIB) and extended short-time\nobjective intelligibility (ESTOI), under a Cafeteria noise condition.\nIn addition, formal listening tests reveal significant intelligibility\ngains when both noise and reverberation exist.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1016",
      "author_area_id": "13",
      "author_area_label": "Special Sessions"
    },
    "rennies20_interspeech": {
      "authors": [
        [
          "Jan",
          "Rennies"
        ],
        [
          "Henning",
          "Schepker"
        ],
        [
          "Cassia",
          "Valentini-Botinhao"
        ],
        [
          "Martin",
          "Cooke"
        ]
      ],
      "title": "Intelligibility-Enhancing Speech Modifications &#8212; The Hurricane Challenge 2.0",
      "original": "1641",
      "page_count": 5,
      "order": 280,
      "p1": "1341",
      "pn": "1345",
      "abstract": [
        "Understanding speech played back in noisy and reverberant conditions\nremains a challenging task. This paper describes the Hurricane Challenge\n2.0, the second large-scale evaluation of algorithms aiming to solve\nthe near-end listening enhancement problem. The challenge consisted\nof modifying German, English, and Spanish speech, which was then evaluated\nby a total of 187 listeners at three sites. Nine algorithms participated\nin the challenge. Results indicate a large variability in performance\nbetween the algorithms, and that some entries achieved large speech\nintelligibility benefits. The largest observed benefits corresponded\nto intensity changes of about 7 dB, which exceeded the results obtained\nin the previous challenge despite more complex listening conditions.\nA priori information about the acoustic conditions did not provide\na general advantage.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1641",
      "author_area_id": "13",
      "author_area_label": "Special Sessions"
    },
    "simantiraki20_interspeech": {
      "authors": [
        [
          "Olympia",
          "Simantiraki"
        ],
        [
          "Martin",
          "Cooke"
        ]
      ],
      "title": "Exploring Listeners&#8217; Speech Rate Preferences",
      "original": "1832",
      "page_count": 5,
      "order": 281,
      "p1": "1346",
      "pn": "1350",
      "abstract": [
        "Fast speech may reduce intelligibility, but there is little agreement\nas to whether listeners benefit from slower speech in noisy conditions.\nThe current study explored the relationship between speech rate and\nmasker properties using a listening preference technique in which participants\nwere able to control speech rate in real time. Spanish listeners adjusted\nspeech rate while listening to word sequences in quiet, in stationary\nnoise at signal-to-noise ratios of 0, +6 and +12 dB, and in modulated\nnoise for 5 envelope modulation rates. Following selection of a preferred\nrate, participants went on to identify words presented at that rate.\nListeners favoured faster speech in quiet, chose increasingly slower\nrates in increasing levels of stationary noise, and showed a preference\nfor speech rates that led to a contrast with masker envelope modulation\nrates. Participants showed distinct preferences even when intelligibility\nwas near ceiling levels. These outcomes suggest that individuals attempt\nto compensate for the decrement in cognitive resources availability\nin more adverse conditions by reducing speech rate and are able to\nexploit differences in modulation properties of the target speech and\nmasker. The listening preference approach provides insights into factors\nsuch as listening effort that are not measured in intelligibility-based\nmetrics.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1832",
      "author_area_id": "13",
      "author_area_label": "Special Sessions"
    },
    "bederna20_interspeech": {
      "authors": [
        [
          "Felicitas",
          "Bederna"
        ],
        [
          "Henning",
          "Schepker"
        ],
        [
          "Christian",
          "Rollwage"
        ],
        [
          "Simon",
          "Doclo"
        ],
        [
          "Arne",
          "Pusch"
        ],
        [
          "J\u00f6rg",
          "Bitzer"
        ],
        [
          "Jan",
          "Rennies"
        ]
      ],
      "title": "Adaptive Compressive Onset-Enhancement for Improved Speech Intelligibility in Noise and Reverberation",
      "original": "2640",
      "page_count": 5,
      "order": 282,
      "p1": "1351",
      "pn": "1355",
      "abstract": [
        "Near-end listening enhancement (NELE) algorithms aim to pre-process\nspeech prior to playback via loudspeakers so as to maintain high speech\nintelligibility even when listening conditions are not optimal, e.g.,\ndue to noise or reverberation. Often NELE algorithms are designed for\nscenarios considering either only the detrimental effect of noise or\nonly reverberation, but not both disturbances. In many typical applications\nscenarios, however, both factors are present. In this paper, we evaluate\na new combination of a noise-dependent and a reverberation-dependent\nalgorithm implemented in a common framework. Specifically, we use instrumental\nmeasures as well as subjective ratings of listening effort for acoustic\nscenarios with different reverberation times and realistic signal-to-noise\nratios. The results show that the noise-dependent algorithm also performs\nwell in reverberation, and that the combination of both algorithms\ncan yield slightly better performance than the individual algorithms\nalone. This benefit appears to depend strongly on the specific acoustic\ncondition, indicating that further work is required to optimize the\nadaptive algorithm behavior.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2640",
      "author_area_id": "13",
      "author_area_label": "Special Sessions"
    },
    "chermaz20_interspeech": {
      "authors": [
        [
          "Carol",
          "Chermaz"
        ],
        [
          "Simon",
          "King"
        ]
      ],
      "title": "A Sound Engineering Approach to Near End Listening Enhancement",
      "original": "2748",
      "page_count": 5,
      "order": 283,
      "p1": "1356",
      "pn": "1360",
      "abstract": [
        "We present the beta version of ASE (the Automatic Sound Engineer),\na NELE (Near End Listening Enhancement) algorithm based on audio engineering\nknowledge. Generations of sound engineers have improved the intelligibility\nof speech against competing sounds and reverberation, while maintaining\nhigh sound quality and artistic integrity (e.g., audio track mixing\nin music and movies). We try to grasp the essential aspects of this\nexpert knowledge and apply it to the more mundane context of speech\nplayback in realistic noise. The algorithm described here was entered\ninto the Hurricane Challenge 2.0, an evaluation of NELE algorithms.\nResults from those listening tests across three languages show the\npotential of our approach, which achieved improvements of over 7 dB\nEIC (Equivalent Intensity Change), corresponding to an absolute increase\nof 58% WAR (Word Accuracy Rate).\n"
      ],
      "doi": "10.21437/Interspeech.2020-2748",
      "author_area_id": "13",
      "author_area_label": "Special Sessions"
    },
    "paul20b_interspeech": {
      "authors": [
        [
          "Dipjyoti",
          "Paul"
        ],
        [
          "Muhammed P.V.",
          "Shifas"
        ],
        [
          "Yannis",
          "Pantazis"
        ],
        [
          "Yannis",
          "Stylianou"
        ]
      ],
      "title": "Enhancing Speech Intelligibility in Text-To-Speech Synthesis Using Speaking Style Conversion",
      "original": "2793",
      "page_count": 5,
      "order": 284,
      "p1": "1361",
      "pn": "1365",
      "abstract": [
        "The increased adoption of digital assistants makes text-to-speech (TTS)\nsynthesis systems an indispensable feature of modern mobile devices.\nIt is hence desirable to build a system capable of generating highly\nintelligible speech in the presence of noise. Past studies have investigated\nstyle conversion in TTS synthesis, yet degraded synthesized quality\noften leads to worse intelligibility. To overcome such limitations,\nwe proposed a novel transfer learning approach using Tacotron and WaveRNN\nbased TTS synthesis. The proposed speech system exploits two modification\nstrategies: (a) Lombard speaking style data and (b) Spectral Shaping\nand Dynamic Range Compression (SSDRC) which has been shown to provide\nhigh intelligibility gains by redistributing the signal energy on the\ntime-frequency domain. We refer to this extension as Lombard-SSDRC\nTTS system. Intelligibility enhancement as quantified by the Intelligibility\nin Bits (SIIB<SUP>Gauss</SUP>) measure shows that the proposed Lombard-SSDRC\nTTS system shows significant relative improvement between 110% and\n130% in speech-shaped noise (SSN), and 47% to 140% in competing-speaker\nnoise (CSN) against the state-of-the-art TTS approach. Additional subjective\nevaluation shows that Lombard-SSDRC TTS successfully increases the\nspeech intelligibility with relative improvement of 455% for SSN and\n104% for CSN in median keyword correction rate compared to the baseline\nTTS method.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2793",
      "author_area_id": "13",
      "author_area_label": "Special Sessions"
    },
    "arai20b_interspeech": {
      "authors": [
        [
          "Takayuki",
          "Arai"
        ]
      ],
      "title": "Two Different Mechanisms of Movable Mandible for Vocal-Tract Model with Flexible Tongue",
      "original": "1159",
      "page_count": 5,
      "order": 285,
      "p1": "1366",
      "pn": "1370",
      "abstract": [
        "In 2017 and 2018, two types of vocal-tract models with physical materials\nwere developed that resemble anatomical models and can physically produce\nhuman-like speech sounds. The 2017 model is a static-model, and its\nvocal-tract configuration is set to produce the vowel /a/. The 2018\nmodel is a dynamic-model, and portions of the articulators including\nthe top surface of the tongue are made of a gel-type material. This\nallows a user to manipulate the shape of the tongue and articulate\ndifferent vowels and a certain set of consonants. However, the mandible\nof the model is fixed, making it difficult to manipulate different\nsounds with different jaw openings, such as high vs. low vowels. Therefore,\nin 2019, two types were developed by adding an additional mandible\nmechanism to the 2018 model. For the first type, the mandible was designed\nto move between the open and closed positions by creating an arc-shape\nrail. For the second type, the mandible moves the same trajectory with\nan additional support. As a result, various speech sounds with a flexible-tongue\nand moveable mandible can be easily produced. These models are more\nrealistic than the anatomical models proposed in 2017 and 2018 in terms\nof articulatory movements.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1159",
      "author_area_id": "1",
      "author_area_label": "Speech Perception, Production and Acquisition"
    },
    "fang20_interspeech": {
      "authors": [
        [
          "Qiang",
          "Fang"
        ]
      ],
      "title": "Improving the Performance of Acoustic-to-Articulatory Inversion by Removing the Training Loss of Noncritical Portions of Articulatory Channels Dynamically",
      "original": "1187",
      "page_count": 5,
      "order": 286,
      "p1": "1371",
      "pn": "1375",
      "abstract": [
        "For decades, average Root Mean Square Error (RMSE) over all the articulatory\nchannels is one of the most prevalent cost functions for training statistical\nmodels for the task of acoustic-to-articulatory inversion (AAI). One\nof the underlying assumptions is that the samples of all the articulatory\nchannels used for training are balanced and play the same role in AAI.\nHowever, this is not true from speech production point view. In this\nstudy, at each time instant, each articulatory channel is classified\nto be critical or noncritical according to their roles in the formation\nof constrictions along the vocal tract when producing speech sound.\nIt is found that the training set is dominated by the samples of noncritical\narticulatory channels. To deal with the unbalanced dataset problem,\nseveral Bi-LSTM networks are trained by removing the of noncritical\nportions of each articulatory channels if the training errors are less\nthan some dynamic threshold. The results indicate that the average\nRMSE over all the articulatory channels, the average RMSE over the\ncritical articulators, and the average RMSE over the noncritical articulators\ncan be reduced significantly by the proposed method.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1187"
    },
    "illa20_interspeech": {
      "authors": [
        [
          "Aravind",
          "Illa"
        ],
        [
          "Prasanta Kumar",
          "Ghosh"
        ]
      ],
      "title": "Speaker Conditioned Acoustic-to-Articulatory Inversion Using x-Vectors",
      "original": "1222",
      "page_count": 5,
      "order": 287,
      "p1": "1376",
      "pn": "1380",
      "abstract": [
        "Speech production involves the movement of various articulators, including\ntongue, jaw, and lips. Estimating the movement of the articulators\nfrom the acoustics of speech is known as acoustic-to-articulatory inversion\n(AAI). Recently, it has been shown that instead of training AAI in\na speaker specific manner, pooling the acoustic-articulatory data from\nmultiple speakers is beneficial. Further, additional conditioning with\nspeaker specific information by one-hot encoding at the input of AAI\nalong with acoustic features benefits the AAI performance in a closed-set\nspeaker train and test condition. In this work, we carry out an experimental\nstudy on the benefit of using x-vectors for providing speaker specific\ninformation to condition AAI. Experiments with 30 speakers have shown\nthat the AAI performance benefits from the use of x-vectors in a closed\nset seen speaker condition. Further, x-vectors also generalizes well\nfor unseen speaker evaluation.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1222",
      "author_area_id": "1",
      "author_area_label": "Speech Perception, Production and Acquisition"
    },
    "liu20g_interspeech": {
      "authors": [
        [
          "Zirui",
          "Liu"
        ],
        [
          "Yi",
          "Xu"
        ],
        [
          "Feng-fan",
          "Hsieh"
        ]
      ],
      "title": "Coarticulation as Synchronised Sequential Target Approximation: An EMA Study",
      "original": "1432",
      "page_count": 5,
      "order": 288,
      "p1": "1381",
      "pn": "1385",
      "abstract": [
        "In this study we tested the hypothesis that consonant and vowel articulations\nstart at the same time at syllable onset [1]. Articulatory data was\ncollected for Mandarin Chinese using Electromagnetic Articulography\n(EMA), which tracks flesh-point movements in time and space. Unlike\nthe traditional velocity threshold method [2], we used a triplet method\nbased on the minimal pair paradigm [3] that detects divergence points\nbetween contrastive pairs of C or V respectively, before comparing\ntheir relative timing. Results show that articulatory onsets of consonant\nand vowel in CV syllables do not differ significantly from each other,\nwhich is consistent with the CV synchrony hypothesis. At the same time,\nthe results also show some evidence that articulators that are shared\nby both C and V are engaged in sequential articulation, i.e., approaching\nthe V target after approaching the C target.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1432",
      "author_area_id": "1",
      "author_area_label": "Speech Perception, Production and Acquisition"
    },
    "santos20_interspeech": {
      "authors": [
        [
          "J\u00f4natas",
          "Santos"
        ],
        [
          "Jugurta",
          "Montalv\u00e3o"
        ],
        [
          "Israel",
          "Santos"
        ]
      ],
      "title": "Improved Model for Vocal Folds with a Polyp with Potential Application",
      "original": "3049",
      "page_count": 5,
      "order": 289,
      "p1": "1386",
      "pn": "1390",
      "abstract": [
        "A new model for vocal folds with a polyp is proposed, based on a mass-spring-damper\nsystem and body-cover structure. The model was used to synthesize a\nwide variety of sustained vowels samples, with and without vocal polyps.\nAnalytical conjectures regarding the effect of a polyp on synthesized\nvoice signals corresponding to sustained vowels were performed. These\nconjectures are then used to estimate intrinsic dimension and differential\nentropy. These parameters were used to implement a naive classifier\nwith the samples of the public  Saarbruecken Voice Database, as a proof\nof concept. The results obtained suggests that the model presented\nin this paper might be a useful tool for tuning actual polyp detectors.\n"
      ],
      "doi": "10.21437/Interspeech.2020-3049",
      "author_area_id": "1",
      "author_area_label": "Speech Perception, Production and Acquisition"
    },
    "zhang20l_interspeech": {
      "authors": [
        [
          "Lin",
          "Zhang"
        ],
        [
          "Kiyoshi",
          "Honda"
        ],
        [
          "Jianguo",
          "Wei"
        ],
        [
          "Seiji",
          "Adachi"
        ]
      ],
      "title": "Regional Resonance of the Lower Vocal Tract and its Contribution to Speaker Characteristics",
      "original": "2024",
      "page_count": 5,
      "order": 290,
      "p1": "1391",
      "pn": "1395",
      "abstract": [
        "This study attempts to describe a plausible causal mechanism of generating\nindividual vocal characteristics in higher spectra. The lower vocal\ntract has been suggested to be such a causal region, but a question\nremains as to how this region modulates vowels&#8217; higher spectra.\nBased on existing data, this study predicts that resonance of the lower\nvocal tract modulates higher vowel spectra into a peak-dip-peak pattern.\nA preliminary acoustic simulation was made to confirm that complexity\nof lower vocal-tract cavities generates such a pattern with the second\npeak. This spectral modulation pattern was further examined to see\nto what extent it contributes to generating static speaker characteristics.\nTo do so, a statistical analysis of male and female F-ratio curves\nwas conducted based on a speech database. In the result, three frequency\nregions for the peak-dip-peak patterns correspond to three regions\nin the gender-specific F-ratio curves. Thus, this study suggests that,\nwhile the first peak may be the major determinant by the human ears,\nthe whole frequency pattern facilitates speaker recognition by machines.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2024",
      "author_area_id": "1",
      "author_area_label": "Speech Perception, Production and Acquisition"
    },
    "mannem20_interspeech": {
      "authors": [
        [
          "Renuka",
          "Mannem"
        ],
        [
          "Navaneetha",
          "Gaddam"
        ],
        [
          "Prasanta Kumar",
          "Ghosh"
        ]
      ],
      "title": "Air-Tissue Boundary Segmentation in Real Time Magnetic Resonance Imaging Video Using 3-D Convolutional Neural Network",
      "original": "2241",
      "page_count": 5,
      "order": 291,
      "p1": "1396",
      "pn": "1400",
      "abstract": [
        "The real-time Magnetic Resonance Imaging (rtMRI) is often used for\nspeech production research as it captures the complete view of the\nvocal tract during speech. Air-tissue boundaries (ATBs) are the contours\nthat trace the transition between high-intensity tissue region and\nlow-intensity airway cavity region in an rtMRI video. The ATBs are\nused in several speech related applications. However, the ATB segmentation\nis a challenging task as the rtMRI frames have low resolution and low\nsignal-to-noise ratio. Several works have been proposed in the past\nfor ATB segmentation. Among these, the supervised algorithms have been\nshown to perform well compared to the unsupervised algorithms. However,\nthe supervised algorithms have limited generalizability towards subjects\nnot involved in training. In this work, we propose a 3-dimensional\nconvolutional neural network (3D-CNN) which utilizes both spatial and\ntemporal information from the rtMRI video for accurate ATB segmentation.\nThe 3D-CNN model captures the vocal tract dynamics in an rtMRI video\nindependent of the morphology of the subject leading to an accurate\nATB segmentation for unseen subjects. In a leave-one-subject-out experimental\nsetup, it is observed that the proposed approach provides &#126;32%\nrelative improvement in the performance compared to the best (SegNet\nbased) baseline approach.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2241",
      "author_area_id": "1",
      "author_area_label": "Speech Perception, Production and Acquisition"
    },
    "purohit20_interspeech": {
      "authors": [
        [
          "Tilak",
          "Purohit"
        ],
        [
          "Prasanta Kumar",
          "Ghosh"
        ]
      ],
      "title": "An Investigation of the Virtual Lip Trajectories During the Production of Bilabial Stops and Nasal at Different Speaking Rates",
      "original": "2709",
      "page_count": 5,
      "order": 292,
      "p1": "1401",
      "pn": "1405",
      "abstract": [
        "We propose a technique to estimate virtual upper lip (VUL) and virtual\nlower lip (VLL) trajectories during production of bilabial stop consonants\n(/p/, /b/) and nasal (/m/). A VUL (VLL) is a hypothetical trajectory\nbelow (above) the measured UL (LL) trajectory which could have been\nachieved by UL (LL) if UL and LL were not in contact with each other\nduring bilabial stops and nasal. Maximum deviation of UL from VUL and\nits location as well as the range of VUL are used as features, denoted\nby VUL MD, VUL MDL, and VUL R, respectively. Similarly, VLL MD, VLL\nMDL, and VLL R are also computed. Analyses of these six features are\ncarried out for /p/, /b/, and /m/ at slow, normal and fast rates based\non electromagnetic articulograph (EMA) recordings of VCV stimuli spoken\nby ten subjects. While no significant differences were observed among\n/p/, /b/, and /m/ in every rate, all six features except VLL MD were\nfound to drop significantly from slow to fast rates. These six features\nwere also found to perform better in an automatic classification task\nbetween slow vs fast rates compared to five baseline features computed\nfrom UL and LL comprising their ranges, velocities and minimum distance\nfrom each other.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2709",
      "author_area_id": "1",
      "author_area_label": "Speech Perception, Production and Acquisition"
    },
    "ge20_interspeech": {
      "authors": [
        [
          "Meng",
          "Ge"
        ],
        [
          "Chenglin",
          "Xu"
        ],
        [
          "Longbiao",
          "Wang"
        ],
        [
          "Eng Siong",
          "Chng"
        ],
        [
          "Jianwu",
          "Dang"
        ],
        [
          "Haizhou",
          "Li"
        ]
      ],
      "title": "SpEx+: A Complete Time Domain Speaker Extraction Network",
      "original": "1397",
      "page_count": 5,
      "order": 293,
      "p1": "1406",
      "pn": "1410",
      "abstract": [
        "Speaker extraction aims to extract the target speech signal from a\nmulti-talker environment given a target speaker&#8217;s reference speech.\nWe recently proposed a time-domain solution, SpEx, that avoids the\nphase estimation in frequency-domain approaches. Unfortunately, SpEx\nis not fully a time-domain solution since it performs time-domain speech\nencoding for speaker extraction, while taking frequency-domain speaker\nembedding as the reference. The size of the analysis window for time-domain\nand the size for frequency-domain input are also different. Such mismatch\nhas an adverse effect on the system performance. To eliminate such\nmismatch, we propose a complete time-domain speaker extraction solution,\nthat is called SpEx+. Specifically, we tie the weights of two identical\nspeech encoder networks, one for the encoder-extractor-decoder pipeline,\nanother as part of the speaker encoder. Experiments show that the SpEx+\nachieves 0.8dB and 2.1dB SDR improvement over the state-of-the-art\nSpEx baseline, under different and same gender conditions on WSJ0-2mix-extr\ndatabase respectively.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1397",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "li20p_interspeech": {
      "authors": [
        [
          "Tingle",
          "Li"
        ],
        [
          "Qingjian",
          "Lin"
        ],
        [
          "Yuanyuan",
          "Bao"
        ],
        [
          "Ming",
          "Li"
        ]
      ],
      "title": "Atss-Net: Target Speaker Separation via Attention-Based Neural Network",
      "original": "1436",
      "page_count": 5,
      "order": 294,
      "p1": "1411",
      "pn": "1415",
      "abstract": [
        "Recently, Convolutional Neural Network (CNN) and Long short-term memory\n(LSTM) based models have been introduced to deep learning-based target\nspeaker separation. In this paper, we propose an Attention-based neural\nnetwork (Atss-Net) in the spectrogram domain for the task. It allows\nthe network to compute the correlation between each feature parallelly,\nand using shallower layers to extract more features, compared with\nthe CNN-LSTM architecture. Experimental results show that our Atss-Net\nyields better performance than the VoiceFilter, although it only contains\nhalf of the parameters. Furthermore, our proposed model also demonstrates\npromising performance in speech enhancement.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1436",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "qu20b_interspeech": {
      "authors": [
        [
          "Leyuan",
          "Qu"
        ],
        [
          "Cornelius",
          "Weber"
        ],
        [
          "Stefan",
          "Wermter"
        ]
      ],
      "title": "Multimodal Target Speech Separation with Voice and Face References",
      "original": "1697",
      "page_count": 5,
      "order": 295,
      "p1": "1416",
      "pn": "1420",
      "abstract": [
        "Target speech separation refers to isolating target speech from a multi-speaker\nmixture signal by conditioning on auxiliary information about the target\nspeaker. Different from the mainstream audio-visual approaches which\nusually require simultaneous visual streams as additional input, e.g.\nthe corresponding lip movement sequences, in our approach we propose\nthe novel use of a single face profile of the target speaker to separate\nexpected clean speech. We exploit the fact that the image of a face\ncontains information about the person&#8217;s speech sound. Compared\nto using a simultaneous visual sequence, a face image is easier to\nobtain by pre-enrollment or on websites, which enables the system to\ngeneralize to devices without cameras. To this end, we incorporate\nface embeddings extracted from a pre-trained model for face recognition\ninto the speech separation, which guide the system in predicting a\ntarget speaker mask in the time-frequency domain. The experimental\nresults show that a pre-enrolled face image is able to benefit separating\nexpected speech signals. Additionally, face information is complementary\nto voice reference and we show that further improvement can be achieved\nwhen combining both face and voice embeddings<SUP>1</SUP>.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1697",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "zhang20m_interspeech": {
      "authors": [
        [
          "Zining",
          "Zhang"
        ],
        [
          "Bingsheng",
          "He"
        ],
        [
          "Zhenjie",
          "Zhang"
        ]
      ],
      "title": "X-TaSNet: Robust and Accurate Time-Domain Speaker Extraction Network",
      "original": "1706",
      "page_count": 5,
      "order": 296,
      "p1": "1421",
      "pn": "1425",
      "abstract": [
        "Extracting the speech of a target speaker from mixed audios, based\non a reference speech from the target speaker, is a challenging yet\npowerful technology in speech processing. Recent studies of speaker-independent\nspeech separation, such as TasNet, have shown promising results by\napplying deep neural networks over the time-domain waveform. Such separation\nneural network does not directly generate reliable and accurate output\nwhen target speakers are specified, because of the necessary prior\non the number of speakers and the lack of robustness when dealing with\naudios with absent speakers. In this paper, we break these limitations\nby introducing a new speaker-aware speech masking method, called X-TaSNet.\nOur proposal adopts new strategies, including a distortion-based loss\nand corresponding alternating training scheme, to better address the\nrobustness issue. X-TaSNet significantly enhances the extracted speech\nquality, doubling SDRi and SI-SNRi of the output speech audio over\nstate-of-the-art voice filtering approach. X-TaSNet also improves the\nreliability of the results by improving the accuracy of speaker identity\nin the output audio to 95.4%, such that it returns silent audios in\nmost cases when the target speaker is absent. These results demonstrate\nX-TaSNet moves one solid step towards more practical applications of\nspeaker extraction technology.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1706",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "li20q_interspeech": {
      "authors": [
        [
          "Chenda",
          "Li"
        ],
        [
          "Yanmin",
          "Qian"
        ]
      ],
      "title": "Listen, Watch and Understand at the Cocktail Party: Audio-Visual-Contextual Speech Separation",
      "original": "2028",
      "page_count": 5,
      "order": 297,
      "p1": "1426",
      "pn": "1430",
      "abstract": [
        "Solving the cocktail party problem with the multi-modal approach has\nbecome popular in recent years. Humans can focus on the speech that\nthey are interested in for the multi-talker mixed speech, by hearing\nthe mixed speech, watching the speaker, and understanding the context\nwhat the speaker is talking about. In this paper, we try to solve the\nspeaker-independent speech separation problem with all three audio-visual-contextual\nmodalities at the first time, and those are hearing speech, watching\nspeaker and understanding contextual language. Compared to the previous\nmethods applying pure audio modal or audio-visual modals, a specific\nmodel is further designed to extract contextual language information\nfor all target speakers directly from the speech mixture. Then these\nextracted contextual knowledge are further incorporated into the multi-modal\nbased speech separation architecture with an appropriate attention\nmechanism. The experiments show that a significant performance improvement\ncan be observed with the newly proposed audio-visual-contextual speech\nseparation.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2028",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "hao20_interspeech": {
      "authors": [
        [
          "Yunzhe",
          "Hao"
        ],
        [
          "Jiaming",
          "Xu"
        ],
        [
          "Jing",
          "Shi"
        ],
        [
          "Peng",
          "Zhang"
        ],
        [
          "Lei",
          "Qin"
        ],
        [
          "Bo",
          "Xu"
        ]
      ],
      "title": "A Unified Framework for Low-Latency Speaker Extraction in Cocktail Party Environments",
      "original": "2085",
      "page_count": 5,
      "order": 298,
      "p1": "1431",
      "pn": "1435",
      "abstract": [
        "Speech recognition technology in single-talker scenes has matured in\nrecent years. However, in noisy environments, especially in multi-talker\nscenes, speech recognition performance is significantly reduced. Towards\ncocktail party problem, we propose a unified time-domain target speaker\nextraction framework. In this framework, we obtain a voiceprint from\na clean speech of the target speaker and then extract the speech of\nthe same speaker in a mixed speech based on the previously obtained\nvoiceprint. This framework uses voiceprint information to avoid permutation\nproblems. In addition, a time-domain model can avoid the phase reconstruction\nproblem of traditional time-frequency domain models. Our framework\nis suitable for scenes where people are relatively fixed and their\nvoiceprints are easily registered, such as in a car, home, meeting\nroom, or other such scenes. The proposed global model based on the\ndual-path recurrent neural network (DPRNN) block achieved state-of-the-art\nunder speaker extraction tasks on the WSJ0-2mix dataset. We also built\ncorresponding low-latency models. Results showed comparable model performance\nand a much shorter upper limit latency than time-frequency domain models.\nWe found that performance of the low-latency model gradually decreased\nas latency decreased, which is important when deploying models in actual\napplication scenarios.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2085"
    },
    "zhao20c_interspeech": {
      "authors": [
        [
          "Jianshu",
          "Zhao"
        ],
        [
          "Shengzhou",
          "Gao"
        ],
        [
          "Takahiro",
          "Shinozaki"
        ]
      ],
      "title": "Time-Domain Target-Speaker Speech Separation with Waveform-Based Speaker Embedding",
      "original": "2108",
      "page_count": 5,
      "order": 299,
      "p1": "1436",
      "pn": "1440",
      "abstract": [
        "Target-speaker speech separation, due to its essence in industrial\napplications, has been heavily researched for long by many. The key\nmetric for qualifying a good separation algorithm still lies on the\nseparation performance, i.e., the quality of the separated voice. In\nthis paper, we presented a novel high-performance time-domain waveform\nbased target-speaker speech separation architecture (WaveFilter) for\nthis task. Unlike most previous researches which adopted Time-Frequency\nbased approaches, WaveFilter does the job by applying Convolutional\nNeural Network (CNN) based feature extractors directly on the raw Time-domain\naudio data, for both the speech separation network and the auxiliary\ntarget-speaker feature extraction network. We achieved a 10.46 Signal\nto Noise Ratio (SNR) improvement on the WSJ0 2-mix dataset and a 10.44\nSNR improvement on the Librispeech dataset as our final results, which\nis much higher than the existing approaches. Our method also achieved\nan 4.9 SNR improvement on the WSJ0 3-mix data. This proves the feasibility\nof WaveFilter on separating the target-speaker&#8217;s voice from multi-speaker\nvoice mixtures without knowing the exact number of speakers in advance,\nwhich in turn proves the readiness of our method for real-world applications.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2108",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "ochiai20_interspeech": {
      "authors": [
        [
          "Tsubasa",
          "Ochiai"
        ],
        [
          "Marc",
          "Delcroix"
        ],
        [
          "Yuma",
          "Koizumi"
        ],
        [
          "Hiroaki",
          "Ito"
        ],
        [
          "Keisuke",
          "Kinoshita"
        ],
        [
          "Shoko",
          "Araki"
        ]
      ],
      "title": "Listen to What You Want: Neural Network-Based Universal Sound Selector",
      "original": "2210",
      "page_count": 5,
      "order": 300,
      "p1": "1441",
      "pn": "1445",
      "abstract": [
        "Being able to control the acoustic events (AEs) to which we want to\nlisten would allow the development of more controllable hearable devices.\nThis paper addresses the AE sound selection (or removal) problems,\nthat we define as the extraction (or suppression) of all the sounds\nthat belong to one or multiple desired AE classes. Although this problem\ncould be addressed with a combination of source separation followed\nby AE classification, this is a sub-optimal way of solving the problem.\nMoreover, source separation usually requires knowing the maximum number\nof sources, which may not be practical when dealing with AEs. In this\npaper, we propose instead a universal sound selection neural network\nthat enables to directly select AE sounds from a mixture given user-specified\ntarget AE classes. The proposed framework can be explicitly optimized\nto simultaneously select sounds from multiple desired AE classes, independently\nof the number of sources in the mixture. We experimentally show that\nthe proposed method achieves promising AE sound selection performance\nand could be generalized to mixtures with a number of sources that\nare unseen during training.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2210",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "yasuda20_interspeech": {
      "authors": [
        [
          "Masahiro",
          "Yasuda"
        ],
        [
          "Yasunori",
          "Ohishi"
        ],
        [
          "Yuma",
          "Koizumi"
        ],
        [
          "Noboru",
          "Harada"
        ]
      ],
      "title": "Crossmodal Sound Retrieval Based on Specific Target Co-Occurrence Denoted with Weak Labels",
      "original": "2445",
      "page_count": 5,
      "order": 301,
      "p1": "1446",
      "pn": "1450",
      "abstract": [
        "Recent advancements in representation learning enable cross-modal retrieval\nby modeling an audio-visual co-occurrence in a single aspect, such\nas physical and linguistic. Unfortunately, in real-world media data,\nsince co-occurrences in various aspects are complexly mixed, it is\ndifficult to distinguish a specific target co-occurrence from many\nother non-target co-occurrences, resulting in failure in crossmodal\nretrieval. To overcome this problem, we propose a triplet-loss-based\nrepresentation learning method that incorporates an awareness mechanism.\nWe adopt weakly-supervised event detection, which provides a constraint\nin representation learning so that our method can &#8220;be aware&#8221;\nof a specific target audio-visual co-occurrence and discriminate it\nfrom other non-target co-occurrences. We evaluated the performance\nof our method by applying it to a sound effect retrieval task using\nrecorded TV broadcast data. In the task, a sound effect appropriate\nfor a given video input should be retrieved. We then conducted objective\nand subjective evaluations, the results indicating that the proposed\nmethod produces significantly better associations of sound and visual\neffects than baselines with no awareness mechanism.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2445",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "xu20c_interspeech": {
      "authors": [
        [
          "Jiahao",
          "Xu"
        ],
        [
          "Kun",
          "Hu"
        ],
        [
          "Chang",
          "Xu"
        ],
        [
          "Duc Chung",
          "Tran"
        ],
        [
          "Zhiyong",
          "Wang"
        ]
      ],
      "title": "Speaker-Aware Monaural Speech Separation",
      "original": "2483",
      "page_count": 5,
      "order": 302,
      "p1": "1451",
      "pn": "1455",
      "abstract": [
        "Predicting and applying Time-Frequency (T-F) masks on mixture signals\nhave been successfully utilized for speech separation. However, existing\nstudies have not well utilized the identity context of a speaker for\nthe inference of masks. In this paper, we propose a novel speaker-aware\nmonaural speech separation model. We firstly devise an encoder to disentangle\nspeaker identity information with the supervision from the auxiliary\nspeaker verification task. Then, we develop a spectrogram masking network\nto predict speaker masks, which would be applied to the mixture signal\nfor the reconstruction of source signals. Experimental results on two\nWSJ0 mixed datasets demonstrate that our proposed model outperforms\nexisting models in different separation scenarios.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2483",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "wang20o_interspeech": {
      "authors": [
        [
          "Liming",
          "Wang"
        ],
        [
          "Mark",
          "Hasegawa-Johnson"
        ]
      ],
      "title": "A DNN-HMM-DNN Hybrid Model for Discovering Word-Like Units from Spoken Captions and Image Regions",
      "original": "1148",
      "page_count": 5,
      "order": 304,
      "p1": "1456",
      "pn": "1460",
      "abstract": [
        "Discovering word-like units without textual transcriptions is an important\nstep in low-resource speech technology. In this work, we demonstrate\na model inspired by statistical machine translation and hidden Markov\nmodel/deep neural network (HMM-DNN) hybrid systems. Our learning algorithm\nis capable of discovering the visual and acoustic correlates of K distinct\nwords in an unknown language by simultaneously learning the mapping\nfrom image regions to concepts (the first DNN), the mapping from acoustic\nfeature vectors to phones (the second DNN), and the optimum alignment\nbetween the two (the HMM). In the simulated low-resource setting using\nMSCOCO and SpeechCOCO datasets, our model achieves 62.4% alignment\naccuracy and outperforms the audio-only segmental embedded GMM approach\non standard word discovery evaluation metrics.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1148",
      "author_area_id": "12",
      "author_area_label": "Spoken Language Processing: Translation, Information Retrieval, Summarization, Resources and Evaluation"
    },
    "elbayad20_interspeech": {
      "authors": [
        [
          "Maha",
          "Elbayad"
        ],
        [
          "Laurent",
          "Besacier"
        ],
        [
          "Jakob",
          "Verbeek"
        ]
      ],
      "title": "Efficient Wait-k Models for Simultaneous Machine Translation",
      "original": "1241",
      "page_count": 5,
      "order": 305,
      "p1": "1461",
      "pn": "1465",
      "abstract": [
        "Simultaneous machine translation consists in starting output generation\nbefore the entire input sequence is available. Wait-k decoders offer\na simple but efficient approach for this problem. They first read k\nsource tokens, after which they alternate between producing a target\ntoken and reading another source token. We investigate the behavior\nof wait-k decoding in low resource settings for spoken corpora using\nIWSLT datasets. We improve training of these models using unidirectional\nencoders, and training across multiple values of k. Experiments with\nTransformer and 2D-convolutional architectures show that our wait-k\nmodels generalize well across a wide range of latency levels. We also\nshow that the 2D-convolution architecture is competitive with Transformers\nfor simultaneous translation of spoken language.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1241",
      "author_area_id": "12",
      "author_area_label": "Spoken Language Processing: Translation, Information Retrieval, Summarization, Resources and Evaluation"
    },
    "nguyen20_interspeech": {
      "authors": [
        [
          "Ha",
          "Nguyen"
        ],
        [
          "Fethi",
          "Bougares"
        ],
        [
          "N.",
          "Tomashenko"
        ],
        [
          "Yannick",
          "Est\u00e8ve"
        ],
        [
          "Laurent",
          "Besacier"
        ]
      ],
      "title": "Investigating Self-Supervised Pre-Training for End-to-End Speech Translation",
      "original": "1835",
      "page_count": 5,
      "order": 306,
      "p1": "1466",
      "pn": "1470",
      "abstract": [
        "Self-supervised learning from raw speech has been proven beneficial\nto improve automatic speech recognition (ASR). We investigate here\nits impact on end-to-end automatic speech translation (AST) performance.\nWe use a contrastive predictive coding (CPC) model pre-trained from\nunlabeled speech as a feature extractor for a downstream AST task.\nWe show that self-supervised pre-training is particularly efficient\nin low resource settings and that fine-tuning CPC models on the AST\ntraining data further improves performance. Even in higher resource\nsettings, ensembling AST models trained with filter-bank and CPC representations\nleads to near state-of-the-art models without using any ASR pre-training.\nThis might be particularly beneficial when one needs to develop a system\nthat translates from speech in a language with poorly standardized\northography or even from speech in an unwritten language.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1835",
      "author_area_id": "12",
      "author_area_label": "Spoken Language Processing: Translation, Information Retrieval, Summarization, Resources and Evaluation"
    },
    "gaido20_interspeech": {
      "authors": [
        [
          "Marco",
          "Gaido"
        ],
        [
          "Mattia A. Di",
          "Gangi"
        ],
        [
          "Matteo",
          "Negri"
        ],
        [
          "Mauro",
          "Cettolo"
        ],
        [
          "Marco",
          "Turchi"
        ]
      ],
      "title": "Contextualized Translation of Automatically Segmented Speech",
      "original": "2860",
      "page_count": 5,
      "order": 307,
      "p1": "1471",
      "pn": "1475",
      "abstract": [
        "Direct speech-to-text translation (ST) models are usually trained on\ncorpora segmented at sentence level, but at inference time they are\ncommonly fed with audio split by a voice activity detector (VAD). Since\nVAD segmentation is not syntax-informed, the resulting segments do\nnot necessarily correspond to well-formed sentences uttered by the\nspeaker but, most likely, to fragments of one or more sentences. This\nsegmentation mismatch degrades considerably the quality of ST models&#8217;\noutput. So far, researchers have focused on improving audio segmentation\ntowards producing sentence-like splits. In this paper, instead, we\naddress the issue in the model, making it more robust to a different,\npotentially sub-optimal segmentation. To this aim, we train our models\non randomly segmented data and compare two approaches: fine-tuning\nand adding the previous segment as context. We show that our context-aware\nsolution is more robust to VAD-segmented input, outperforming a strong\nbase model and the fine-tuning on different VAD segmentations of an\nEnglish-German test set by up to 4.25 BLEU points.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2860",
      "author_area_id": "12",
      "author_area_label": "Spoken Language Processing: Translation, Information Retrieval, Summarization, Resources and Evaluation"
    },
    "pino20_interspeech": {
      "authors": [
        [
          "Juan",
          "Pino"
        ],
        [
          "Qiantong",
          "Xu"
        ],
        [
          "Xutai",
          "Ma"
        ],
        [
          "Mohammad Javad",
          "Dousti"
        ],
        [
          "Yun",
          "Tang"
        ]
      ],
      "title": "Self-Training for End-to-End Speech Translation",
      "original": "2938",
      "page_count": 5,
      "order": 308,
      "p1": "1476",
      "pn": "1480",
      "abstract": [
        "One of the main challenges for end-to-end speech translation is data\nscarcity. We leverage pseudo-labels generated from unlabeled audio\nby a cascade and an end-to-end speech translation model. This provides\n8.3 and 5.7 BLEU gains over a strong semi-supervised baseline on the\nMuST-C English-French and English-German datasets, reaching state-of-the\nart performance. The effect of the quality of the pseudo-labels is\ninvestigated. Our approach is shown to be more effective than simply\npre-training the encoder on the speech recognition task. Finally, we\ndemonstrate the effectiveness of self-training by directly generating\npseudo-labels with an end-to-end model instead of a cascade model.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2938",
      "author_area_id": "12",
      "author_area_label": "Spoken Language Processing: Translation, Information Retrieval, Summarization, Resources and Evaluation"
    },
    "federico20_interspeech": {
      "authors": [
        [
          "Marcello",
          "Federico"
        ],
        [
          "Yogesh",
          "Virkar"
        ],
        [
          "Robert",
          "Enyedi"
        ],
        [
          "Roberto",
          "Barra-Chicote"
        ]
      ],
      "title": "Evaluating and Optimizing Prosodic Alignment for Automatic Dubbing",
      "original": "2983",
      "page_count": 5,
      "order": 309,
      "p1": "1481",
      "pn": "1485",
      "abstract": [
        "Automatic dubbing aims at replacing all speech contained in a video\nwith speech in a different language, so that the result sounds and\nlooks as natural as the original. Hence, in addition to conveying the\nsame content of an original utterance (which is the typical objective\nof speech translation), dubbed speech should ideally also match its\nduration, the lip movements and gestures in the video, timbre, emotion\nand prosody of the speaker, and finally background noise and reverberation\nof the environment. In this paper, after describing our dubbing architecture,\nwe focus on recent progress on the prosodic alignment component, which\naims at synchronizing the translated transcript with the original utterances.\nWe present empirical results for English-to-Italian dubbing on a publicly\navailable collection of TED Talks. Our new prosodic alignment model,\nwhich allows for small relaxations in synchronicity, shows to significantly\nimprove both prosodic alignment accuracy and overall subjective dubbing\nquality of previous work.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2983",
      "author_area_id": "12",
      "author_area_label": "Spoken Language Processing: Translation, Information Retrieval, Summarization, Resources and Evaluation"
    },
    "ohishi20_interspeech": {
      "authors": [
        [
          "Yasunori",
          "Ohishi"
        ],
        [
          "Akisato",
          "Kimura"
        ],
        [
          "Takahito",
          "Kawanishi"
        ],
        [
          "Kunio",
          "Kashino"
        ],
        [
          "David",
          "Harwath"
        ],
        [
          "James",
          "Glass"
        ]
      ],
      "title": "Pair Expansion for Learning Multilingual Semantic Embeddings Using Disjoint Visually-Grounded Speech Audio Datasets",
      "original": "3078",
      "page_count": 5,
      "order": 310,
      "p1": "1486",
      "pn": "1490",
      "abstract": [
        "We propose a data expansion method for learning a multilingual semantic\nembedding model using disjoint datasets containing images and their\nmultilingual audio captions. Here, disjoint means that there are no\nshared images among the multiple language datasets, in contrast to\nexisting works on multilingual semantic embedding based on visually-grounded\nspeech audio, where it has been assumed that each image is associated\nwith spoken captions of multiple languages. Although learning on disjoint\ndatasets is more challenging, we consider it crucial in practical situations.\nOur main idea is to refer to another paired data when evaluating a\nloss value regarding an anchor image. We call this scheme &#8220;pair\nexpansion&#8221;. The motivation behind this idea is to utilize even\ndisjoint pairs by finding similarities, or commonalities, that may\nexist in different images. Specifically, we examine two approaches\nfor calculating similarities: one using image embedding vectors and\nthe other using object recognition results. Our experiments show that\nexpanded pairs improve crossmodal and cross-lingual retrieval accuracy\ncompared with non-expanded cases. They also show that similarities\nmeasured by the image embedding vectors yield better accuracy than\nthose based on object recognition results.\n"
      ],
      "doi": "10.21437/Interspeech.2020-3078",
      "author_area_id": "12",
      "author_area_label": "Spoken Language Processing: Translation, Information Retrieval, Summarization, Resources and Evaluation"
    },
    "wu20g_interspeech": {
      "authors": [
        [
          "Anne",
          "Wu"
        ],
        [
          "Changhan",
          "Wang"
        ],
        [
          "Juan",
          "Pino"
        ],
        [
          "Jiatao",
          "Gu"
        ]
      ],
      "title": "Self-Supervised Representations Improve End-to-End Speech Translation",
      "original": "3094",
      "page_count": 5,
      "order": 311,
      "p1": "1491",
      "pn": "1495",
      "abstract": [
        "End-to-end speech-to-text translation can provide a simpler and smaller\nsystem but is facing the challenge of data scarcity. Pre-training methods\ncan leverage unlabeled data and have been shown to be effective on\ndata-scarce settings. In this work, we explore whether self-supervised\npre-trained speech representations can benefit the speech translation\ntask in both high- and low-resource settings, whether they can transfer\nwell to other languages, and whether they can be effectively combined\nwith other common methods that help improve low-resource end-to-end\nspeech translation such as using a pre-trained high-resource speech\nrecognition system. We demonstrate that self-supervised pre-trained\nfeatures can consistently improve the translation performance, and\ncross-lingual transfer allows to extend to a variety of languages without\nor with little tuning.\n"
      ],
      "doi": "10.21437/Interspeech.2020-3094",
      "author_area_id": "12",
      "author_area_label": "Spoken Language Processing: Translation, Information Retrieval, Summarization, Resources and Evaluation"
    },
    "jung20c_interspeech": {
      "authors": [
        [
          "Jee-weon",
          "Jung"
        ],
        [
          "Seung-bin",
          "Kim"
        ],
        [
          "Hye-jin",
          "Shim"
        ],
        [
          "Ju-ho",
          "Kim"
        ],
        [
          "Ha-Jin",
          "Yu"
        ]
      ],
      "title": "Improved RawNet with Feature Map Scaling for Text-Independent Speaker Verification Using Raw Waveforms",
      "original": "1011",
      "page_count": 5,
      "order": 312,
      "p1": "1496",
      "pn": "1500",
      "abstract": [
        "Recent advances in deep learning have facilitated the design of speaker\nverification systems that directly input raw waveforms. For example,\nRawNet [1] extracts speaker embeddings from raw waveforms, which simplifies\nthe process pipeline and demonstrates competitive performance. In this\nstudy, we improve RawNet by scaling feature maps using various methods.\nThe proposed mechanism utilizes a scale vector that adopts a sigmoid\nnon-linear function. It refers to a vector with dimensionality equal\nto the number of filters in a given feature map. Using a scale vector,\nwe propose to scale the feature map multiplicatively, additively, or\nboth. In addition, we investigate replacing the first convolution layer\nwith the sinc-convolution layer of SincNet. Experiments performed on\nthe VoxCeleb1 evaluation dataset demonstrate the effectiveness of the\nproposed methods, and the best performing system reduces the equal\nerror rate by half compared to the original RawNet. Expanded evaluation\nresults obtained using the VoxCeleb1-E and VoxCeleb-H protocols marginally\noutperform existing state-of-the-art systems.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1011",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "jung20d_interspeech": {
      "authors": [
        [
          "Youngmoon",
          "Jung"
        ],
        [
          "Seong Min",
          "Kye"
        ],
        [
          "Yeunju",
          "Choi"
        ],
        [
          "Myunghun",
          "Jung"
        ],
        [
          "Hoirin",
          "Kim"
        ]
      ],
      "title": "Improving Multi-Scale Aggregation Using Feature Pyramid Module for Robust Speaker Verification of Variable-Duration Utterances",
      "original": "1025",
      "page_count": 5,
      "order": 313,
      "p1": "1501",
      "pn": "1505",
      "abstract": [
        "Currently, the most widely used approach for speaker verification is\nthe deep speaker embedding learning. In this approach, we obtain a\nspeaker embedding vector by pooling single-scale features that are\nextracted from the last layer of a speaker feature extractor. Multi-scale\naggregation (MSA), which utilizes multi-scale features from different\nlayers of the feature extractor, has recently been introduced and shows\nsuperior performance for variable-duration utterances. To increase\nthe robustness dealing with utterances of arbitrary duration, this\npaper improves the MSA by using a feature pyramid module. The module\nenhances speaker-discriminative information of features from multiple\nlayers via a top-down pathway and lateral connections. We extract speaker\nembeddings using the enhanced features that contain rich speaker information\nwith different time scales. Experiments on the VoxCeleb dataset show\nthat the proposed module improves previous MSA methods with a smaller\nnumber of parameters. It also achieves better performance than state-of-the-art\napproaches for both short and long utterances.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1025",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "gu20_interspeech": {
      "authors": [
        [
          "Bin",
          "Gu"
        ],
        [
          "Wu",
          "Guo"
        ],
        [
          "Fenglin",
          "Ding"
        ],
        [
          "Zhen-Hua",
          "Ling"
        ],
        [
          "Jun",
          "Du"
        ]
      ],
      "title": "An Adaptive X-Vector Model for Text-Independent Speaker Verification",
      "original": "1071",
      "page_count": 5,
      "order": 314,
      "p1": "1506",
      "pn": "1510",
      "abstract": [
        "In this paper, adaptive mechanisms are applied in deep neural network\n(DNN) training for x-vector-based text-independent speaker verification.\nFirst, adaptive convolutional neural networks (ACNNs) are employed\nin frame-level embedding layers, where the parameters of the convolution\nfilters are adjusted based on the input features. Compared with conventional\nCNNs, ACNNs have more flexibility in capturing speaker information.\nMoreover, we replace conventional batch normalization (BN) with adaptive\nbatch normalization (ABN). By dynamically generating the scaling and\nshifting parameters in BN, ABN adapts models to the acoustic variability\narising from various factors such as channel and environmental noises.\nFinally, we incorporate these two methods to further improve performance.\nExperiments are carried out on the speaker in the wild (SITW) and VOiCES\ndatabases. The results demonstrate that the proposed methods significantly\noutperform the original x-vector approach. \n"
      ],
      "doi": "10.21437/Interspeech.2020-1071",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "prieto20_interspeech": {
      "authors": [
        [
          "Santi",
          "Prieto"
        ],
        [
          "Alfonso",
          "Ortega"
        ],
        [
          "Iv\u00e1n",
          "L\u00f3pez-Espejo"
        ],
        [
          "Eduardo",
          "Lleida"
        ]
      ],
      "title": "Shouted Speech Compensation for Speaker Verification Robust to Vocal Effort Conditions",
      "original": "1402",
      "page_count": 5,
      "order": 315,
      "p1": "1511",
      "pn": "1515",
      "abstract": [
        "The performance of speaker verification systems degrades when vocal\neffort conditions between enrollment and test (e.g., shouted vs. normal\nspeech) are different. This is a potential situation in non-cooperative\nspeaker verification tasks. In this paper, we present a study on different\nmethods for linear compensation of embeddings making use of Gaussian\nmixture models to cluster shouted and normal speech domains. These\ncompensation techniques are borrowed from the area of robustness for\nautomatic speech recognition and, in this work, we apply them to compensate\nthe mismatch between shouted and normal conditions in speaker verification.\nBefore compensation, shouted condition is automatically detected by\nmeans of logistic regression. The process is computationally light\nand it is performed in the back-end of an x-vector system. Experimental\nresults show that applying the proposed approach in the presence of\nvocal effort mismatch yields up to 13.8% equal error rate relative\nimprovement with respect to a system that applies neither shouted speech\ndetection nor compensation.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1402",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "nicolson20_interspeech": {
      "authors": [
        [
          "Aaron",
          "Nicolson"
        ],
        [
          "Kuldip K.",
          "Paliwal"
        ]
      ],
      "title": "Sum-Product Networks for Robust Automatic Speaker Identification",
      "original": "1501",
      "page_count": 5,
      "order": 316,
      "p1": "1516",
      "pn": "1520",
      "abstract": [
        "We introduce sum-product networks (SPNs) for robust speech processing\nthrough a simple robust automatic speaker identification (ASI) task.<SUP>*</SUP>\nSPNs are deep probabilistic graphical models capable of answering multiple\nprobabilistic queries. We show that SPNs are able to remain robust\nby using the marginal probability density function (PDF) of the spectral\nfeatures that reliably represent speech. Though current SPN toolkits\nand learning algorithms are in their infancy, we aim to show that SPNs\nhave the potential to become a useful tool for robust speech processing\nin the future. SPN speaker models are evaluated here on real-world\nnon-stationary and coloured noise sources at multiple signal-to-noise\nratio (SNR) levels. In terms of ASI accuracy, we find that SPN speaker\nmodels are more robust than two recent convolutional neural network\n(CNN)-based ASI systems. Additionally, SPN speaker models consist of\nsignificantly fewer parameters than their CNN-based counterparts. The\nresults indicate that SPN speaker models could be a robust, parameter-efficient\nalternative for ASI. Additionally, this work demonstrates that SPNs\nhave potential in related tasks, such as robust automatic speech recognition\n(ASR) and automatic speaker verification (ASV).\n"
      ],
      "doi": "10.21437/Interspeech.2020-1501",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "kim20b_interspeech": {
      "authors": [
        [
          "Seung-bin",
          "Kim"
        ],
        [
          "Jee-weon",
          "Jung"
        ],
        [
          "Hye-jin",
          "Shim"
        ],
        [
          "Ju-ho",
          "Kim"
        ],
        [
          "Ha-Jin",
          "Yu"
        ]
      ],
      "title": "Segment Aggregation for Short Utterances Speaker Verification Using Raw Waveforms",
      "original": "1564",
      "page_count": 5,
      "order": 317,
      "p1": "1521",
      "pn": "1525",
      "abstract": [
        "Most studies on speaker verification systems focus on long-duration\nutterances, which are composed of sufficient phonetic information.\nHowever, the performances of these systems are known to degrade when\nshort-duration utterances are inputted due to the lack of phonetic\ninformation as compared to the long utterances. In this paper, we propose\na method that compensates for the performance degradation of speaker\nverification for short utterances, referred to as &#8220; segment aggregation&#8221;.\nThe proposed method adopts an ensemble-based design to improve the\nstability and accuracy of speaker verification systems. The proposed\nmethod segments an input utterance into several short utterances and\nthen aggregates the segment embeddings extracted from the segmented\ninputs to compose a speaker embedding. Then, this method simultaneously\ntrains the segment embeddings and the aggregated speaker embedding.\nIn addition, we also modified the teacher-student learning method for\nthe proposed method. Experimental results on different input duration\nusing the VoxCeleb1 test set demonstrate that the proposed technique\nimproves speaker verification performance by about 45.37% relatively\ncompared to the baseline system with 1-second test utterance condition.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1564",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "rozenberg20_interspeech": {
      "authors": [
        [
          "Shai",
          "Rozenberg"
        ],
        [
          "Hagai",
          "Aronowitz"
        ],
        [
          "Ron",
          "Hoory"
        ]
      ],
      "title": "Siamese X-Vector Reconstruction for Domain Adapted Speaker Recognition",
      "original": "1742",
      "page_count": 4,
      "order": 318,
      "p1": "1526",
      "pn": "1529",
      "abstract": [
        "With the rise of voice-activated applications, the need for speaker\nrecognition is rapidly increasing. The x-vector, an embedding approach\nbased on a deep neural network (DNN), is considered the state-of-the-art\nwhen proper end-to-end training is not feasible. However, the accuracy\nsignificantly decreases when recording conditions (noise, sample rate,\netc.) are mismatched, either between the x-vector training data and\nthe target data or between enrollment and test data. We introduce the\nSiamese x-vector Reconstruction (SVR) for domain adaptation. We reconstruct\nthe embedding of a higher quality signal from a lower quality counterpart\nusing a lean auxiliary Siamese DNN. We evaluate our method on several\nmismatch scenarios and demonstrate significant improvement over the\nbaseline.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1742",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "shi20b_interspeech": {
      "authors": [
        [
          "Yanpei",
          "Shi"
        ],
        [
          "Qiang",
          "Huang"
        ],
        [
          "Thomas",
          "Hain"
        ]
      ],
      "title": "Speaker Re-Identification with Speaker Dependent Speech Enhancement",
      "original": "1772",
      "page_count": 5,
      "order": 319,
      "p1": "1530",
      "pn": "1534",
      "abstract": [
        "While the use of deep neural networks has significantly boosted speaker\nrecognition performance, it is still challenging to separate speakers\nin poor acoustic environments. Here speech enhancement methods have\ntraditionally allowed improved performance. The recent works have shown\nthat adapting speech enhancement can lead to further gains. This paper\nintroduces a novel approach that cascades speech enhancement and speaker\nrecognition. In the first step, a speaker embedding vector is generated,\nwhich is used in the second step to enhance the speech quality and\nre-identify the speakers. Models are trained in an integrated framework\nwith joint optimisation. The proposed approach is evaluated using the\nVoxceleb1 dataset, which aims to assess speaker recognition in real\nworld situations. In addition three types of noise at different signal-noise-ratios\nwere added for this work. The obtained results show that the proposed\napproach using speaker dependent speech enhancement can yield better\nspeaker recognition and speech enhancement performances than two baselines\nin various noise conditions.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1772",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "lavrentyeva20_interspeech": {
      "authors": [
        [
          "Galina",
          "Lavrentyeva"
        ],
        [
          "Marina",
          "Volkova"
        ],
        [
          "Anastasia",
          "Avdeeva"
        ],
        [
          "Sergey",
          "Novoselov"
        ],
        [
          "Artem",
          "Gorlanov"
        ],
        [
          "Tseren",
          "Andzhukaev"
        ],
        [
          "Artem",
          "Ivanov"
        ],
        [
          "Alexander",
          "Kozlov"
        ]
      ],
      "title": "Blind Speech Signal Quality Estimation for Speaker Verification Systems",
      "original": "1826",
      "page_count": 5,
      "order": 320,
      "p1": "1535",
      "pn": "1539",
      "abstract": [
        "The problem of system performance degradation in mismatched acoustic\nconditions has been widely acknowledged in the community and is common\nfor different fields. The present state-of-the-art deep speaker embedding\nmodels are domain-sensitive. The main idea of the current research\nis to develop a single method for automatic signal quality estimation,\nwhich allows to evaluate short-term signal characteristics.<br style=\"mso-data-placement:same-cell;\"\n/><br style=\"mso-data-placement:same-cell;\" />\n  This paper presents\na neural network based approach for blind speech signal quality estimation\nin terms of signal-to-noise ratio (SNR) and reverberation time (RT60),\nwhich is able to classify the type of underlying additive noise. Additionally,\ncurrent research revealed the need for an accurate voice activity detector\nthat performs well in both clean and noisy unseen environments. Therefore\na novel neural network VAD based on U-net architecture is presented.The\nproposed algorithms allow to perform the analysis of NIST, SITW, Voices\ndatasets commonly used for objective comparison of speaker verification\nsystems from the new point of view and consider effective calibration\nsteps to improve speaker recognition quality on them.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1826",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "li20r_interspeech": {
      "authors": [
        [
          "Xu",
          "Li"
        ],
        [
          "Na",
          "Li"
        ],
        [
          "Jinghua",
          "Zhong"
        ],
        [
          "Xixin",
          "Wu"
        ],
        [
          "Xunying",
          "Liu"
        ],
        [
          "Dan",
          "Su"
        ],
        [
          "Dong",
          "Yu"
        ],
        [
          "Helen",
          "Meng"
        ]
      ],
      "title": "Investigating Robustness of Adversarial Samples Detection for Automatic Speaker Verification",
      "original": "2441",
      "page_count": 5,
      "order": 321,
      "p1": "1540",
      "pn": "1544",
      "abstract": [
        "Recently adversarial attacks on automatic speaker verification (ASV)\nsystems attracted widespread attention as they pose severe threats\nto ASV systems. However, methods to defend against such attacks are\nlimited. Existing approaches mainly focus on retraining ASV systems\nwith adversarial data augmentation. Also, countermeasure robustness\nagainst different attack settings are insufficiently investigated.\nOrthogonal to prior approaches, this work proposes to defend ASV systems\nagainst adversarial attacks with a separate detection network, rather\nthan augmenting adversarial data into ASV training. A VGG-like binary\nclassification detector is introduced and demonstrated to be effective\non detecting adversarial samples. To investigate detector robustness\nin a realistic defense scenario where unseen attack settings may exist,\nwe analyze various kinds of unseen attack settings&#8217; impact and\nobserve that the detector is robust (6.27% EER<SUB>det</SUB> degradation\nin the worst case) against unseen substitute ASV systems, but it has\nweak robustness (50.37% EER<SUB>det</SUB> degradation in the worst\ncase) against unseen perturbation methods. The weak robustness against\nunseen perturbation methods shows a direction for developing stronger\ncountermeasures.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2441",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "pal20_interspeech": {
      "authors": [
        [
          "Vaishali",
          "Pal"
        ],
        [
          "Fabien",
          "Guillot"
        ],
        [
          "Manish",
          "Shrivastava"
        ],
        [
          "Jean-Michel",
          "Renders"
        ],
        [
          "Laurent",
          "Besacier"
        ]
      ],
      "title": "Modeling ASR Ambiguity for Neural Dialogue State Tracking",
      "original": "1783",
      "page_count": 5,
      "order": 322,
      "p1": "1545",
      "pn": "1549",
      "abstract": [
        "Spoken dialogue systems typically use one or several (top-N) ASR sequence(s)\nfor inferring the semantic meaning and tracking the state of the dialogue.\nHowever, ASR graphs, such as confusion networks (confnets), provide\na compact representation of a richer hypothesis space than a top-N\nASR list. In this paper, we study the benefits of using confusion networks\nwith a neural dialogue state tracker (DST). We encode the 2-dimensional\nconfnet into a 1-dimensional sequence of embeddings using a confusion\nnetwork encoder which can be used with any DST system. Our confnet\nencoder is plugged into the &#8216;Global-locally Self-Attentive Dialogue\nState Tacker&#8217; (GLAD) model for DST and obtains significant improvements\nin both accuracy and inference time compared to using top-N ASR hypotheses.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1783",
      "author_area_id": "11",
      "author_area_label": "Spoken dialog systems and conversational analysis"
    },
    "wang20p_interspeech": {
      "authors": [
        [
          "Haoyu",
          "Wang"
        ],
        [
          "Shuyan",
          "Dong"
        ],
        [
          "Yue",
          "Liu"
        ],
        [
          "James",
          "Logan"
        ],
        [
          "Ashish Kumar",
          "Agrawal"
        ],
        [
          "Yang",
          "Liu"
        ]
      ],
      "title": "ASR Error Correction with Augmented Transformer for Entity Retrieval",
      "original": "1753",
      "page_count": 5,
      "order": 323,
      "p1": "1550",
      "pn": "1554",
      "abstract": [
        "Domain-agnostic Automatic Speech Recognition (ASR) systems suffer from\nthe issue of mistranscribing domain-specific words, which leads to\nfailures in downstream tasks. In this paper, we present a post-editing\nASR error correction method using the Transformer model for entity\nmention correction and retrieval. Specifically, we propose a novel\naugmented variant of the Transformer model that encodes both the word\nand phoneme sequence of an entity, and attends to phoneme information\nin addition to word-level information during decoding to correct mistranscribed\nnamed entities. We evaluate our method on both the ASR error correction\ntask and the downstream retrieval task. Our method achieves 48.08%\nentity error rate (EER) reduction in ASR error correction task and\n26.74% mean reciprocal rank (MRR) improvement for the retrieval task.\nIn addition, our augmented Transformer model significantly outperforms\nthe vanilla Transformer model with 17.89% EER reduction and 1.98% MRR\nincrease, demonstrating the effectiveness of incorporating phoneme\ninformation in the correction model.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1753",
      "author_area_id": "11",
      "author_area_label": "Spoken dialog systems and conversational analysis"
    },
    "jia20_interspeech": {
      "authors": [
        [
          "Xueli",
          "Jia"
        ],
        [
          "Jianzong",
          "Wang"
        ],
        [
          "Zhiyong",
          "Zhang"
        ],
        [
          "Ning",
          "Cheng"
        ],
        [
          "Jing",
          "Xiao"
        ]
      ],
      "title": "Large-Scale Transfer Learning for Low-Resource Spoken Language Understanding",
      "original": "0059",
      "page_count": 5,
      "order": 324,
      "p1": "1555",
      "pn": "1559",
      "abstract": [
        "End-to-end Spoken Language Understanding (SLU) models are made increasingly\nlarge and complex to achieve the state-of-the-art accuracy. However,\nthe increased complexity of a model can also introduce high risk of\nover-fitting, which is a major challenge in SLU tasks due to the limitation\nof available data. In this paper, we propose an attention-based SLU\nmodel together with three encoder enhancement strategies to overcome\ndata sparsity challenge. The first strategy focuses on the transfer-learning\napproach to improve feature extraction capability of the encoder. It\nis implemented by pre-training the encoder component with a quantity\nof Automatic Speech Recognition annotated data relying on the standard\nTransformer architecture and then fine-tuning the SLU model with a\nsmall amount of target labelled data. The second strategy adopts multi-task\nlearning strategy, the SLU model integrates the speech recognition\nmodel by sharing the same underlying encoder, such that improving robustness\nand generalization ability. The third strategy, learning from Component\nFusion (CF) idea, involves a Bidirectional Encoder Representation from\nTransformer (BERT) model and aims to boost the capability of the decoder\nwith an auxiliary network. It hence reduces the risk of over-fitting\nand augments the ability of the underlying encoder, indirectly. Experiments\non the FluentAI dataset show that cross-language transfer learning\nand multi-task strategies have been improved by up to 4.52% and 3.89%\nrespectively, compared to the baseline.\n"
      ],
      "doi": "10.21437/Interspeech.2020-59",
      "author_area_id": "11",
      "author_area_label": "Spoken dialog systems and conversational analysis"
    },
    "gaspers20_interspeech": {
      "authors": [
        [
          "Judith",
          "Gaspers"
        ],
        [
          "Quynh",
          "Do"
        ],
        [
          "Fabian",
          "Triefenbach"
        ]
      ],
      "title": "Data Balancing for Boosting Performance of Low-Frequency Classes in Spoken Language Understanding",
      "original": "1676",
      "page_count": 5,
      "order": 325,
      "p1": "1560",
      "pn": "1564",
      "abstract": [
        "Despite the fact that data imbalance is becoming more and more common\nin real-world Spoken Language Understanding (SLU) applications, it\nhas not been studied extensively in the literature. To the best of\nour knowledge, this paper presents the first systematic study on handling\ndata imbalance for SLU. In particular, we discuss the application of\nexisting data balancing techniques for SLU and propose a multi-task\nSLU model for intent classification and slot filling. Aiming to avoid\nover-fitting, in our model methods for data balancing are leveraged\nindirectly via an auxiliary task which makes use of a class-balanced\nbatch generator and (possibly) synthetic data. Our results on a real-world\ndataset indicate that i) our proposed model can boost performance on\nlow frequency intents significantly while avoiding a potential performance\ndecrease on the head intents, ii) synthetic data are beneficial for\nbootstrapping new intents when realistic data are not available, but\niii) once a certain amount of realistic data becomes available, using\nsynthetic data in the auxiliary task only yields better performance\nthan adding them to the primary task training data, and iv) in a joint\ntraining scenario, balancing the intent distribution individually improves\nnot only intent classification but also slot filling performance.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1676",
      "author_area_id": "11",
      "author_area_label": "Spoken dialog systems and conversational analysis"
    },
    "wang20q_interspeech": {
      "authors": [
        [
          "Yu",
          "Wang"
        ],
        [
          "Yilin",
          "Shen"
        ],
        [
          "Hongxia",
          "Jin"
        ]
      ],
      "title": "An Interactive Adversarial Reward Learning-Based Spoken Language Understanding System",
      "original": "2967",
      "page_count": 5,
      "order": 326,
      "p1": "1565",
      "pn": "1569",
      "abstract": [
        "Most of the existing spoken language understanding systems can perform\nonly semantic frame parsing based on a single-round user query. They\ncannot take users&#8217; feedback to update/add/remove slot values\nthrough multiround interactions with users. In this paper, we introduce\na novel interactive adversarial reward learning-based spoken language\nunderstanding system that can leverage the multiround users&#8217;\nfeedback to update slot values. We perform two experiments on the benchmark\nATIS dataset and demonstrate that the new system can improve parsing\nperformance by at least 2.5% in terms of F1, with only one round of\nfeedback. The improvement becomes even larger when the number of feedback\nrounds increases. Furthermore, we also compare the new system with\nstate-of-the-art dialogue state tracking systems and demonstrate that\nthe new interactive system can perform better on multiround spoken\nlanguage understanding tasks in terms of slot- and sentence-level accuracy.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2967",
      "author_area_id": "11",
      "author_area_label": "Spoken dialog systems and conversational analysis"
    },
    "cao20_interspeech": {
      "authors": [
        [
          "Jin",
          "Cao"
        ],
        [
          "Jun",
          "Wang"
        ],
        [
          "Wael",
          "Hamza"
        ],
        [
          "Kelly",
          "Vanee"
        ],
        [
          "Shang-Wen",
          "Li"
        ]
      ],
      "title": "Style Attuned Pre-Training and Parameter Efficient Fine-Tuning for Spoken Language Understanding",
      "original": "2907",
      "page_count": 5,
      "order": 327,
      "p1": "1570",
      "pn": "1574",
      "abstract": [
        "Neural models have yielded state-of-the-art results in deciphering\nspoken language understanding (SLU) problems; however, these models\nrequire a significant amount of domain-specific labeled examples for\ntraining, which is prohibitively expensive. While pre-trained language\nmodels like BERT have been shown to capture a massive amount of knowledge\nby learning from unlabeled corpora and solve SLU using fewer labeled\nexamples for adaption, the encoding of knowledge is implicit and agnostic\nto downstream tasks. Such encoding results in model inefficiencies\nin parameter usage: an entirely new model is required for every domain.\nTo address these challenges, we introduce a novel SLU framework, comprising\na conversational language modeling (CLM) pre-training task and a light\nencoder architecture. The CLM pre-training enables networks to capture\nthe representation of the language in conversation style with the presence\nof ASR errors. The light encoder architecture separates the shared\npre-trained networks from the mappings of generally encoded knowledge\nto specific domains of SLU, allowing for the domain adaptation to be\nperformed solely at the light encoder and thus increasing efficiency.\nWith the framework, we match the performance of state-of-the-art SLU\nresults on Alexa internal datasets and on two public ones (ATIS, SNIPS),\nadding only 4.4% parameters per task.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2907",
      "author_area_id": "11",
      "author_area_label": "Spoken dialog systems and conversational analysis"
    },
    "orihashi20_interspeech": {
      "authors": [
        [
          "Shota",
          "Orihashi"
        ],
        [
          "Mana",
          "Ihori"
        ],
        [
          "Tomohiro",
          "Tanaka"
        ],
        [
          "Ryo",
          "Masumura"
        ]
      ],
      "title": "Unsupervised Domain Adaptation for Dialogue Sequence Labeling Based on Hierarchical Adversarial Training",
      "original": "2010",
      "page_count": 5,
      "order": 328,
      "p1": "1575",
      "pn": "1579",
      "abstract": [
        "This paper presents a novel unsupervised domain adaptation method for\ndialogue sequence labeling. Dialogue sequence labeling is a supervised\nlearning task that estimates labels for each utterance in the given\ndialogue document, and is useful for many applications such as topic\nsegmentation and dialogue act estimation. Accurate labeling often requires\na large amount of labeled training data, but it is difficult to collect\nsuch data every time we need to support a new domain, such as contact\ncenters in a new business field. In order to solve this difficulty,\nwe propose an unsupervised domain adaptation method for dialogue sequence\nlabeling. Our key idea is to construct dialogue sequence labeling using\nlabeled source domain data and unlabeled target domain data so as to\nremove domain dependencies at utterance-level and dialogue-level contexts.\nThe proposed method adopts hierarchical adversarial training; two domain\nadversarial networks, an utterance-level context independent network\nand a dialogue-level context dependent network, are introduced for\nimproving domain invariance in the dialogue sequence labeling. Experiments\non Japanese simulated contact center dialogue datasets demonstrate\nthe effectiveness of the proposed method.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2010",
      "author_area_id": "11",
      "author_area_label": "Spoken dialog systems and conversational analysis"
    },
    "sar20_interspeech": {
      "authors": [
        [
          "Leda",
          "Sar\u0131"
        ],
        [
          "Mark",
          "Hasegawa-Johnson"
        ]
      ],
      "title": "Deep F-Measure Maximization for End-to-End Speech Understanding",
      "original": "1949",
      "page_count": 5,
      "order": 329,
      "p1": "1580",
      "pn": "1584",
      "abstract": [
        "Spoken language understanding (SLU) datasets, like many other machine\nlearning datasets, usually suffer from the label imbalance problem.\nLabel imbalance usually causes the learned model to replicate similar\nbiases at the output which raises the issue of unfairness to the minority\nclasses in the dataset. In this work, we approach the fairness problem\nby maximizing the F-measure instead of accuracy in neural network model\ntraining. We propose a differentiable approximation to the F-measure\nand train the network with this objective using standard back-propagation.\nWe perform experiments on two standard fairness datasets, Adult, and\nCommunities and Crime, and also on speech-to-intent detection on the\nATIS dataset and speech-to-image concept classification on the Speech-COCO\ndataset. In all four of these tasks, F-measure maximization results\nin improved micro-F1 scores, with absolute improvements of up to 8%\nabsolute, as compared to models trained with the cross-entropy loss\nfunction. In the two multi-class SLU tasks, the proposed approach significantly\nimproves class coverage, i.e., the number of classes with positive\nrecall.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1949",
      "author_area_id": "11",
      "author_area_label": "Spoken dialog systems and conversational analysis"
    },
    "whang20_interspeech": {
      "authors": [
        [
          "Taesun",
          "Whang"
        ],
        [
          "Dongyub",
          "Lee"
        ],
        [
          "Chanhee",
          "Lee"
        ],
        [
          "Kisu",
          "Yang"
        ],
        [
          "Dongsuk",
          "Oh"
        ],
        [
          "Heuiseok",
          "Lim"
        ]
      ],
      "title": "An Effective Domain Adaptive Post-Training Method for BERT in Response Selection",
      "original": "2153",
      "page_count": 5,
      "order": 330,
      "p1": "1585",
      "pn": "1589",
      "abstract": [
        "We focus on multi-turn response selection in a retrieval-based dialog\nsystem. In this paper, we utilize the powerful pre-trained language\nmodel Bi-directional Encoder Representations from Transformer (BERT)\nfor a multi-turn dialog system and propose a highly effective post-training\nmethod on domain-specific corpus. Although BERT is easily adopted to\nvarious NLP tasks and outperforms previous baselines of each task,\nit still has limitations if a task corpus is too focused on a certain\ndomain. Post-training on domain-specific corpus (e.g., Ubuntu Corpus)\nhelps the model to train contextualized representations and words that\ndo not appear in general corpus (e.g., English Wikipedia). Experimental\nresults show that our approach achieves new state-of-the-art on two\nresponse selection benchmarks (i.e., Ubuntu Corpus V1, Advising Corpus)\nperformance improvement by 5.9% and 6% on R<SUB>10</SUB>@1.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2153",
      "author_area_id": "11",
      "author_area_label": "Spoken dialog systems and conversational analysis"
    },
    "caubriere20_interspeech": {
      "authors": [
        [
          "Antoine",
          "Caubri\u00e8re"
        ],
        [
          "Yannick",
          "Est\u00e8ve"
        ],
        [
          "Antoine",
          "Laurent"
        ],
        [
          "Emmanuel",
          "Morin"
        ]
      ],
      "title": "Confidence Measure for Speech-to-Concept End-to-End Spoken Language Understanding",
      "original": "2298",
      "page_count": 5,
      "order": 331,
      "p1": "1590",
      "pn": "1594",
      "abstract": [
        "Recent studies have led to the introduction of Speech-to-Concept End-to-End\n(E2E) neural architectures for Spoken Language Understanding (SLU)\nthat reach state of the art performance. In this work, we propose a\nway to compute confidence measures on semantic concepts recognized\nby a Speech-to-Text E2E SLU system. We investigate the use of the hidden\nrepresentations of our CTC-based SLU system to train an external simple\nclassifier. We experiment two kinds of external simple classifiers\nto analyze subsequences of hidden representations involved in recognized\nsemantic concepts. The first external classifier is based on a MLP\nwhile the second one is based on a bLSTM neural network. We compare\nthem to a baseline confidence measure computed directly from the softmax\noutputs of the E2E system. On the French challenging MEDIA corpus,\nwhen the confidence measure is used to reject, experiments show that\nusing an external BLSTM significantly outperforms the other approaches\nin terms of precision/recall. To evaluate the additional information\nprovided by this confidence measure, we compute the value of Normalised\nCross-Entropy (NCE). Reaching a value equal to 0.288, we show that\nour best proposed confidence measure brings relevant information about\nthe reliability of a recognized concept.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2298",
      "author_area_id": "11",
      "author_area_label": "Spoken dialog systems and conversational analysis"
    },
    "mcguire20_interspeech": {
      "authors": [
        [
          "Grant L.",
          "McGuire"
        ],
        [
          "Molly",
          "Babel"
        ]
      ],
      "title": "Attention to Indexical Information Improves Voice Recall",
      "original": "3042",
      "page_count": 5,
      "order": 332,
      "p1": "1595",
      "pn": "1599",
      "abstract": [
        "In an exposure phase, two groups of listeners were exposed to a set\nof 10 voices. These groups differed in terms of the task assigned during\nexposure: one group was asked to make a decision about the regional\naffiliation of the voices (Indexical Condition), while the other group\northographically transcribed the words presented (Lexical Condition).\nBoth groups were given an identical test phase where they were presented\nwith 20 voices (10 old, 10 new) and asked to make old/new decisions\non the voices. While both groups of listeners performed at above chance\naccuracy levels in recognizing voices at test as old/new, listeners\nin the Indexical Condition performed more accurately. These results\nsuggest that the nature of attention during exposure has consequences\nfor subsequent performance, suggesting encoding differences as a result\nof task demands.\n"
      ],
      "doi": "10.21437/Interspeech.2020-3042",
      "author_area_id": "1",
      "author_area_label": "Speech Perception, Production and Acquisition"
    },
    "ngoc20_interspeech": {
      "authors": [
        [
          "Ana\u00efs Tran",
          "Ngoc"
        ],
        [
          "Julien",
          "Meyer"
        ],
        [
          "Fanny",
          "Meunier"
        ]
      ],
      "title": "Categorization of Whistled Consonants by French Speakers",
      "original": "2683",
      "page_count": 5,
      "order": 333,
      "p1": "1600",
      "pn": "1604",
      "abstract": [
        "Whistled speech is a form of modified speech where some frequencies\nof vowels and consonants are augmented and transposed to whistling,\nmodifying the timbre and the construction of each phoneme. These transformations\ncause only some elements of the signal to be intelligible for naive\nlisteners, which, according to previous studies, includes vowel recognition.\nHere, we analyze naive listeners&#8217; capacities for whistled consonant\ncategorization for four consonants: /p/, /k/, /t/ and /s/ by presenting\nthe findings of two behavioral experiments. Though both experiments\nmeasure whistled consonant categorization, we used modified frequencies\n&#8212; lowered with a phase vocoder &#8212; of the whistled stimuli\nin the second experiment to better identify the relative nature of\npitch cues employed in this process. Results show that participants\nobtained approximately 50% of correct responses (when chance is at\n25%). These findings show specific consonant preferences for &#8220;s&#8221;\nand &#8220;t&#8221; over &#8220;k&#8221; and &#8220;p&#8221;, specifically\nwhen stimuli is unmodified. Previous research on whistled consonants\nsystems has often opposed &#8220;s&#8221; and &#8220;t&#8221; to &#8220;k&#8221;\nand &#8220;p&#8221;, due to their strong pitch modulations. The preference\nfor these two consonants underlines the importance of these cues in\nphoneme processing.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2683",
      "author_area_id": "1",
      "author_area_label": "Speech Perception, Production and Acquisition"
    },
    "ngoc20b_interspeech": {
      "authors": [
        [
          "Ana\u00efs Tran",
          "Ngoc"
        ],
        [
          "Julien",
          "Meyer"
        ],
        [
          "Fanny",
          "Meunier"
        ]
      ],
      "title": "Whistled Vowel Identification by French Listeners",
      "original": "2697",
      "page_count": 5,
      "order": 334,
      "p1": "1605",
      "pn": "1609",
      "abstract": [
        "In this paper, we analyzed whistled vowel categorization by native\nFrench listeners. Whistled speech, a natural, yet modified register\nof speech, is used here as a tool to investigate perceptual processes\nin languages. We focused on four whistled vowels: /i, e, a, o/. After\na detailed description of the vowels, we built and ran a behavioral\nexperiment in which we asked native French speakers to categorize whistled\nvowel stimuli in which we introduced intra- and inter- production variations.\nIn addition, half of the participants performed the experiment in person\n(at the laboratory) while the other half participated online, allowing\nus to evaluate the impact of the testing set up. Our results confirm\nthat the categorization rate of whistled vowels is above chance. They\nreveal significant differences in performance for different vowels\nand suggest an influence of certain acoustic parameters from the whistlers&#8217;\nvowel range on categorization. Moreover, no effect or interaction was\nfound for testing location and circumstances in our data set. This\nstudy confirms that whistled stimuli are a useful tool for studying\nhow listeners process modified speech and which parameters impact sound\ncategorization.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2697",
      "author_area_id": "1",
      "author_area_label": "Speech Perception, Production and Acquisition"
    },
    "cordero20_interspeech": {
      "authors": [
        [
          "Maria del Mar",
          "Cordero"
        ],
        [
          "Fanny",
          "Meunier"
        ],
        [
          "Nicolas",
          "Grimault"
        ],
        [
          "St\u00e9phane",
          "Pota"
        ],
        [
          "Elsa",
          "Spinelli"
        ]
      ],
      "title": "F0 Slope and Mean: Cues to Speech Segmentation in French",
      "original": "2509",
      "page_count": 5,
      "order": 335,
      "p1": "1610",
      "pn": "1614",
      "abstract": [
        "This paper evaluates the use of intonational cues during word segmentation\nin French. Specifically, we aim to examine how the characteristics\nof the fundamental frequency (F0) that can be observed at the beginning\nof words influence their processing. Native speakers of French were\npresented with phonemically identical sequences, such as /selami/ (\nc&#8217;est l&#8217;amie/la mie &#8220;it&#8217;s the friend/the crumb&#8221;).\nTo test which properties of the F0 affect the perceived segmentation,\nwe manipulated the F0 slope and/or the mean value of the first vowel\n/a/ in consonant-initial items (e.g.,  la mie). To assess differences\nin off-line vs online processing, we used a two-alternative, forced-choice\ntask in Experiment 1 and a lexical decision task in Experiment 2. A\nprevious study showed that vowel-initial segmentation was enhanced\nwhen the F0 mean value increased. However, the present study shows\nthat modifying the F0 slope while keeping the F0 mean value constant\nalso influences speech segmentation in both off-line and online tasks.\nThis suggests that listeners use the F0 slope as a cue at the beginning\nof content words.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2509"
    },
    "michelas20_interspeech": {
      "authors": [
        [
          "Amandine",
          "Michelas"
        ],
        [
          "Sophie",
          "Dufour"
        ]
      ],
      "title": "Does French Listeners&#8217; Ability to Use Accentual Information at the Word Level Depend on the Ear of Presentation?",
      "original": "1263",
      "page_count": 5,
      "order": 336,
      "p1": "1615",
      "pn": "1619",
      "abstract": [
        "In two long-term repetition priming experiments, we investigated how\naccentual information is processed and represented in the French listeners&#8217;\nmind. Repeated prime and target words either matched (/b&#227;&#x2C8;do/\n- / b&#227;&#x2C8;do/ &#8216;headband&#8217;) or mismatched in their\naccentual patterns (/b&#227;do/ - /b&#227;&#x2C8;do/). In experiment\n1, the target words were presented in the left ear only, and attenuation\nin the repetition priming effect was observed when the primes and the\ntargets mismatched in their accentual pattern. The differential priming\neffect between match and mismatch primes was no longer observed in\nExperiment 2 when the targets were presented in the right ear only.\nTogether, these results showed that accentual variation at the word\nlevel in French is treated as related-talker variation, and only influences\nword recognition under specific circumstances, in particular, when\nwe push word processing in the right hemisphere.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1263",
      "author_area_id": "1",
      "author_area_label": "Speech Perception, Production and Acquisition"
    },
    "liu20h_interspeech": {
      "authors": [
        [
          "Wen",
          "Liu"
        ]
      ],
      "title": "A Perceptual Study of the Five Level Tones in Hmu (Xinzhai Variety)",
      "original": "0056",
      "page_count": 4,
      "order": 337,
      "p1": "1620",
      "pn": "1623",
      "abstract": [
        "Previous studies have shown that the perception is categorical when\ntones have different contours, whereas continuous when tones have the\nsame contour. In this study, a perceptual experiment of the five level\ntones in Hmu (Xinzhai variety) was conducted to further examine this\nconclusion. Results show that in the identification test, continua\nbetween different level tones have different boundary width, which\nhas a negative correlation with the pitch interval of two level tones.\nIn the discrimination test, though there is no peak in discrimination\ncurve, the discrimination accuracy reveals an important phenomenon\nthat the accuracy is approximately 50% between two neighboring level\ntones, but higher when the level tones have a larger pitch interval.\nBesides, the boundary width is highly correlated with the discrimination\naccuracy (e.g., the narrower the boundary width, the higher the discrimination\naccuracy). These results reveal the basic characteristic of continuous\nperception, especially for level tones. Finally, the results also demonstrate\nthat the category in categorical perception is not equal to phonological\ncategory.\n"
      ],
      "doi": "10.21437/Interspeech.2020-56",
      "author_area_id": "1",
      "author_area_label": "Speech Perception, Production and Acquisition"
    },
    "zeng20_interspeech": {
      "authors": [
        [
          "Zhen",
          "Zeng"
        ],
        [
          "Karen",
          "Mattock"
        ],
        [
          "Liquan",
          "Liu"
        ],
        [
          "Varghese",
          "Peter"
        ],
        [
          "Alba",
          "Tuninetti"
        ],
        [
          "Feng-Ming",
          "Tsao"
        ]
      ],
      "title": "Mandarin and English Adults&#8217; Cue-Weighting of Lexical Stress",
      "original": "2612",
      "page_count": 5,
      "order": 338,
      "p1": "1624",
      "pn": "1628",
      "abstract": [
        "Listeners segment speech based on the rhythm of their native language(s)\n(e.g., stress- vs. syllable-timed, tone vs. non-tone) [1,2]. In English,\nthe perception of speech rhythm relies on analyzing auditory cues pertinent\nto lexical stress, including pitch, duration and intensity [3]. Focusing\non cross-linguistic impact on English lexical stress cue processing,\nthe present study aims to explore English stress cue-weighting by Mandarin-speaking\nadults (with English adults as control), using an MMN multi-feature\nparadigm.<br style=\"mso-data-placement:same-cell;\" /><br style=\"mso-data-placement:same-cell;\"\n/>\n  Preliminary ERP data revealed cross-linguistic perceptual differences\nto pitch and duration cues, but not to intensity cues in the bisyllabic\nnon-word /dede/. Specifically, while English adults were similarly\nsensitive to pitch change at the initial and final syllable of the\nnon-word, they were more sensitive to the duration change at the initial\nsyllable. Comparatively, Mandarin adults were similarly sensitive to\nduration change at each position, but more sensitive to pitch at the\nfinal syllable. Lastly, both the Mandarin group and the English group\nwere more sensitive to the intensity sound change at the second syllable.\nPossible explanations for these findings are discussed.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2612",
      "author_area_id": "1",
      "author_area_label": "Speech Perception, Production and Acquisition"
    },
    "feng20c_interspeech": {
      "authors": [
        [
          "Yan",
          "Feng"
        ],
        [
          "Gang",
          "Peng"
        ],
        [
          "William Shi-Yuan",
          "Wang"
        ]
      ],
      "title": "Age-Related Differences of Tone Perception in Mandarin-Speaking Seniors",
      "original": "2194",
      "page_count": 5,
      "order": 339,
      "p1": "1629",
      "pn": "1633",
      "abstract": [
        "This study examined age-related differences in categorical perception\nof Mandarin lexical tones through comparing identification and discrimination\nperformance among young adults, seniors aged 60&#8211;65 years, and\nolder seniors aged 75&#8211;80 years. Results showed a significantly\nwider boundary and smaller peakedness in older seniors. There was also\na positive correlation between the hearing level at 125 Hz and boundary\nwidth, and a negative correlation between hearing level (125 Hz) and\npeakedness in older seniors, indicating that the decline of tone perception\nin this population might be associated with degradation of hearing\nsensitivity. However, there was no significant difference between young\nadults and seniors aged 60&#8211;65 years, which might reveal that\nyounger seniors could maintain normal ability to perceive tones categorically.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2194",
      "author_area_id": "1",
      "author_area_label": "Speech Perception, Production and Acquisition"
    },
    "zellou20b_interspeech": {
      "authors": [
        [
          "Georgia",
          "Zellou"
        ],
        [
          "Michelle",
          "Cohn"
        ]
      ],
      "title": "Social and Functional Pressures in Vocal Alignment: Differences for Human and Voice-AI Interlocutors",
      "original": "1335",
      "page_count": 5,
      "order": 340,
      "p1": "1634",
      "pn": "1638",
      "abstract": [
        "Increasingly, people are having conversational interactions with voice-AI\nsystems, such as Amazon&#8217;s Alexa. Do the same social and functional\npressures that mediate alignment toward human interlocutors also predict\nalign patterns toward voice-AI? We designed an interactive dialogue\ntask to investigate this question. Each trial consisted of scripted,\ninteractive turns between a participant and a model talker (pre-recorded\nfrom either a natural production or voice-AI): First, participants\nproduced target words in a carrier phrase. Then, a model talker responded\nwith an utterance containing the target word. The interlocutor responses\nvaried by 1) communicative affect (social) and 2) correctness (functional).\nFinally, participants repeated the carrier phrase. Degree of phonetic\nalignment was assessed acoustically between the target word in the\nmodel&#8217;s response and participants&#8217; response. Results indicate\nthat social and functional factors distinctly mediate alignment toward\nAI and humans. Findings are discussed with reference to theories of\nalignment and human-computer interaction.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1335",
      "author_area_id": "1",
      "author_area_label": "Speech Perception, Production and Acquisition"
    },
    "kavaki20_interspeech": {
      "authors": [
        [
          "Hassan Salami",
          "Kavaki"
        ],
        [
          "Michael I.",
          "Mandel"
        ]
      ],
      "title": "Identifying Important Time-Frequency Locations in Continuous Speech Utterances",
      "original": "2637",
      "page_count": 5,
      "order": 341,
      "p1": "1639",
      "pn": "1643",
      "abstract": [
        "Human listeners use specific cues to recognize speech and recent experiments\nhave shown that certain time-frequency regions of individual utterances\nare more important to their correct identification than others. A model\nthat could identify such cues or regions from clean speech would facilitate\nspeech recognition and speech enhancement by focusing on those important\nregions. Thus, in this paper we present a model that can predict the\nregions of individual utterances that are important to an automatic\nspeech recognition (ASR) &#8220;listener&#8221; by learning to add\nas much noise as possible to these utterances while still permitting\nthe ASR to correctly identify them. This work utilizes a continuous\nspeech recognizer to recognize multi-word utterances and builds upon\nour previous work that performed the same process for an isolated word\nrecognizer. Our experimental results indicate that our model can apply\nnoise to obscure 90.5% of the spectrogram while leaving recognition\nperformance nearly unchanged.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2637",
      "author_area_id": "1",
      "author_area_label": "Speech Perception, Production and Acquisition"
    },
    "loweimi20b_interspeech": {
      "authors": [
        [
          "Erfan",
          "Loweimi"
        ],
        [
          "Peter",
          "Bell"
        ],
        [
          "Steve",
          "Renals"
        ]
      ],
      "title": "Raw Sign and Magnitude Spectra for Multi-Head Acoustic Modelling",
      "original": "0018",
      "page_count": 5,
      "order": 342,
      "p1": "1644",
      "pn": "1648",
      "abstract": [
        "In this paper we investigate the usefulness of the sign spectrum and\nits combination with the raw magnitude spectrum in acoustic modelling\nfor automatic speech recognition (ASR). The sign spectrum is a sequence\nof &#177;1s, capturing one bit of the phase spectrum. It encodes information\noverlooked by the magnitude spectrum enabling unique signal characterisation\nand reconstruction. In particular, we demonstrate it carries information\nrelated to the temporal structure of the signal as well as the speech&#8217;s\nsource component. Furthermore, we investigate the usefulness of combining\nit with the raw magnitude spectrum via multi-head CNNs at different\nfusion levels for ASR. While information-wise these two streams of\ninformation are together equivalent to the raw waveform signal the\noverall performance is noticeably higher than raw waveform and classic\nfeatures such as MFCC and filterbank. This has been observed and verified\nin TIMIT, NTIMT, Aurora-4 and WSJ tasks and up to 14.5% relative WER\nreduction has been achieved.\n"
      ],
      "doi": "10.21437/Interspeech.2020-18",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "agrawal20_interspeech": {
      "authors": [
        [
          "Purvi",
          "Agrawal"
        ],
        [
          "Sriram",
          "Ganapathy"
        ]
      ],
      "title": "Robust Raw Waveform Speech Recognition Using Relevance Weighted Representations",
      "original": "2301",
      "page_count": 5,
      "order": 343,
      "p1": "1649",
      "pn": "1653",
      "abstract": [
        "Speech recognition in noisy and channel distorted scenarios is often\nchallenging as the current acoustic modeling schemes are not adaptive\nto the changes in the signal distribution in the presence of noise.\nIn this work, we develop a novel acoustic modeling framework for noise\nrobust speech recognition based on relevance weighting mechanism. The\nrelevance weighting is achieved using a sub-network approach that performs\nfeature selection. A relevance sub-network is applied on the output\nof first layer of a convolutional network model operating on raw speech\nsignals while a second relevance sub-network is applied on the second\nconvolutional layer output. The relevance weights for the first layer\ncorrespond to an acoustic filterbank selection while the relevance\nweights in the second layer perform modulation filter selection. The\nmodel is trained for a speech recognition task on noisy and reverberant\nspeech. The speech recognition experiments on multiple datasets (Aurora-4,\nCHiME-3, VOiCES) reveal that the incorporation of relevance weighting\nin the neural network architecture improves the speech recognition\nword error rates significantly (average relative improvements of 10%\nover the baseline systems).\n"
      ],
      "doi": "10.21437/Interspeech.2020-2301",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "oglic20_interspeech": {
      "authors": [
        [
          "Dino",
          "Oglic"
        ],
        [
          "Zoran",
          "Cvetkovic"
        ],
        [
          "Peter",
          "Bell"
        ],
        [
          "Steve",
          "Renals"
        ]
      ],
      "title": "A Deep 2D Convolutional Network for Waveform-Based Speech Recognition",
      "original": "1870",
      "page_count": 5,
      "order": 344,
      "p1": "1654",
      "pn": "1658",
      "abstract": [
        "Due to limited computational resources, acoustic models of early automatic\nspeech recognition ( asr) systems were built in low-dimensional feature\nspaces that incur considerable information loss at the outset of the\nprocess. Several comparative studies of automatic and human speech\nrecognition suggest that this information loss can adversely affect\nthe robustness of  asr systems. To mitigate that and allow for learning\nof robust models, we propose a deep 2 d convolutional network in the\nwaveform domain. The first layer of the network decomposes waveforms\ninto frequency sub-bands, thereby representing them in a structured\nhigh-dimensional space. This is achieved by means of a parametric convolutional\nblock defined via cosine modulations of compactly supported windows.\nThe next layer embeds the waveform in an even higher-dimensional space\nof high-resolution spectro-temporal patterns, implemented via a 2 d\nconvolutional block. This is followed by a gradual compression phase\nthat selects most relevant spectro-temporal patterns using wide-pass\n2 d filtering. Our results show that the approach significantly outperforms\nalternative waveform-based models on both noisy and spontaneous conversational\nspeech (24% and 11% relative error reduction, respectively). Moreover,\nthis study provides empirical evidence that learning directly from\nthe waveform domain could be more effective than learning using hand-crafted\nfeatures.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1870",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "kurzinger20_interspeech": {
      "authors": [
        [
          "Ludwig",
          "K\u00fcrzinger"
        ],
        [
          "Nicolas",
          "Lindae"
        ],
        [
          "Palle",
          "Klewitz"
        ],
        [
          "Gerhard",
          "Rigoll"
        ]
      ],
      "title": "Lightweight End-to-End Speech Recognition from Raw Audio Data Using Sinc-Convolutions",
      "original": "1392",
      "page_count": 5,
      "order": 345,
      "p1": "1659",
      "pn": "1663",
      "abstract": [
        "Many end-to-end Automatic Speech Recognition (ASR) systems still rely\non pre-processed frequency-domain features that are handcrafted to\nemulate the human hearing. Our work is motivated by recent advances\nin integrated learnable feature extraction. For this, we propose Lightweight\nSinc-Convolutions (LSC) that integrate Sinc-convolutions with depthwise\nconvolutions as a low-parameter machine-learnable feature extraction\nfor end-to-end ASR systems.<br style=\"mso-data-placement:same-cell;\"\n/><br style=\"mso-data-placement:same-cell;\" />\n  We integrated LSC\ninto the hybrid CTC/attention architecture for evaluation. The resulting\nend-to-end model shows smooth convergence behaviour that is further\nimproved by applying SpecAugment in the time domain. We also discuss\nfilter-level improvements, such as using log-compression as activation\nfunction. Our model achieves a word error rate of 10.7% on the TEDlium\nv2 test dataset, surpassing the corresponding architecture with log-mel\nfilterbank features by an absolute 1.9%, but only has 21% of its model\nsize.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1392",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "ghahramani20_interspeech": {
      "authors": [
        [
          "Pegah",
          "Ghahramani"
        ],
        [
          "Hossein",
          "Hadian"
        ],
        [
          "Daniel",
          "Povey"
        ],
        [
          "Hynek",
          "Hermansky"
        ],
        [
          "Sanjeev",
          "Khudanpur"
        ]
      ],
      "title": "An Alternative to MFCCs for ASR",
      "original": "2690",
      "page_count": 4,
      "order": 346,
      "p1": "1664",
      "pn": "1667",
      "abstract": [
        "The Mel scale is the most commonly used frequency warping function\nto extract features for automatic speech recognition (ASR) and is known\nto be quite effective. However, it is not specifically designed for\nASR acoustic models based on deep neural networks (DNN). In this study,\nwe introduce a frequency warping function which is a modified version\nof Mel scale. This warping function is parameterized using 2 parameters\nand we use it to propose a new set of features called modified Mel-frequency\ncepstral coefficients (MFCC), which use cosine-shaped filters. The\nbandwidths are computed using a new function. By evaluating the proposed\nfeatures on a variety of ASR data sets, we see consistent improvements\nover regular MFCCs and (log) Mel filter bank energies.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2690",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "dutta20_interspeech": {
      "authors": [
        [
          "Anirban",
          "Dutta"
        ],
        [
          "G.",
          "Ashishkumar"
        ],
        [
          "Ch.V. Rama",
          "Rao"
        ]
      ],
      "title": "Phase Based Spectro-Temporal Features for Building a Robust ASR System",
      "original": "2258",
      "page_count": 5,
      "order": 347,
      "p1": "1668",
      "pn": "1672",
      "abstract": [
        "Spectro-temporal feature extraction has shown its robustness in the\nfield of speech recognition. However, these features are derived from\nmagnitude spectrum of the complex Fourier Transform (FT). In this work,\nwe investigate to see if phase information can substitute magnitude\nbased spectro-temporal features. We compared with different state of\nart phase spectrum and evaluated its performance. The experiments are\ncarried out in different noisy environments. We found Modified Group\nDelay (MODGD) spectrum to closely resemble the structure of power spectrum.\nA relative performance difference of 0.03% on average is observed for\nthe MODGD spectro-temporal features compared to the magnitude based\nfeatures. The analysis showed that phase can indeed carry equivalent\nor complementary information to magnitude based spectro-temporal features.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2258",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "joy20_interspeech": {
      "authors": [
        [
          "Neethu M.",
          "Joy"
        ],
        [
          "Dino",
          "Oglic"
        ],
        [
          "Zoran",
          "Cvetkovic"
        ],
        [
          "Peter",
          "Bell"
        ],
        [
          "Steve",
          "Renals"
        ]
      ],
      "title": "Deep Scattering Power Spectrum Features for Robust Speech Recognition",
      "original": "2656",
      "page_count": 5,
      "order": 348,
      "p1": "1673",
      "pn": "1677",
      "abstract": [
        "Deep scattering spectrum consists of a cascade of wavelet transforms\nand modulus non-linearity. It generates features of different orders,\nwith the first order coefficients approximately equal to the Mel-frequency\ncepstrum, and higher order coefficients recovering information lost\nat lower levels. We investigate the effect of including the information\nrecovered by higher order coefficients on the robustness of speech\nrecognition. To that end, we also propose a modification to the original\nscattering transform tailored for noisy speech. In particular, instead\nof the modulus non-linearity we opt to work with power coefficients\nand, therefore, use the squared modulus non-linearity. We quantify\nthe robustness of scattering features using the word error rates of\nacoustic models trained on clean speech and evaluated using sets of\nutterances corrupted with different noise types. Our empirical results\nshow that the second order scattering power spectrum coefficients capture\ninvariants relevant for noise robustness and that this additional information\nimproves generalization to unseen noise conditions (almost 20% relative\nerror reduction on  aurora 4). This finding can have important consequences\non speech recognition systems that typically discard the second order\ninformation and keep only the first order features (known for emulating\n mfcc and  fbank values) when representing speech.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2656",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "parcollet20_interspeech": {
      "authors": [
        [
          "Titouan",
          "Parcollet"
        ],
        [
          "Xinchi",
          "Qiu"
        ],
        [
          "Nicholas D.",
          "Lane"
        ]
      ],
      "title": "FusionRNN: Shared Neural Parameters for Multi-Channel Distant Speech Recognition",
      "original": "2102",
      "page_count": 5,
      "order": 349,
      "p1": "1678",
      "pn": "1682",
      "abstract": [
        "Distant speech recognition remains a challenging application for modern\ndeep learning based Automatic Speech Recognition (ASR) systems, due\nto complex recording conditions involving noise and reverberation.\nMultiple microphones are commonly combined with well-known speech processing\ntechniques to enhance the original signals and thus enhance the speech\nrecognizer performance. These multi-channel follow similar input distributions\nwith respect to the global speech information but also contain an important\npart of noise. Consequently, the input representation robustness is\nkey to obtaining reasonable recognition rates. In this work, we propose\na Fusion Layer (FL) based on shared neural parameters. We use it to\nproduce an expressive embedding of multiple microphone signals, that\ncan easily be combined with any existing ASR pipeline. The proposed\nmodel called FusionRNN showed promising results on a multi-channel\ndistant speech recognition task, and consistently outperformed baseline\nmodels while maintaining an equal training time.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2102",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "kumar20b_interspeech": {
      "authors": [
        [
          "Kshitiz",
          "Kumar"
        ],
        [
          "Bo",
          "Ren"
        ],
        [
          "Yifan",
          "Gong"
        ],
        [
          "Jian",
          "Wu"
        ]
      ],
      "title": "Bandpass Noise Generation and Augmentation for Unified ASR",
      "original": "2904",
      "page_count": 5,
      "order": 350,
      "p1": "1683",
      "pn": "1687",
      "abstract": [
        "Data Simulation is a crucial technique for robust automatic speech\nrecognition (ASR) systems. We develop this work in the scope of data\naugmentation and improve robustness by generating new bandpass noise\nresources from an existing noise corpus. We design numerous bandpass\nfilters with varying center frequencies and filter bandwidths, and\nobtain corresponding bandpass noise samples. We augment our baseline\ndata simulation with bandpass noises to ingest additional robustness\nand generalization to generic and unknown acoustic scenarios. This\nwork targets ASR robustness to individual subband noises, and improves\nrobustness to unseen real-world noise that can be approximated as a\nfactorial combination of subband noises. We demonstrate our work for\na large scale unified ASR task. We obtained 7% word error rate relative\nreduction (WERR) across unseen acoustic conditions and 11% WERR for\nkids speech. We also demonstrate generalization to new ASR applications.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2904",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "purushothaman20_interspeech": {
      "authors": [
        [
          "Anurenjan",
          "Purushothaman"
        ],
        [
          "Anirudh",
          "Sreeram"
        ],
        [
          "Rohit",
          "Kumar"
        ],
        [
          "Sriram",
          "Ganapathy"
        ]
      ],
      "title": "Deep Learning Based Dereverberation of Temporal Envelopes for Robust Speech Recognition",
      "original": "2283",
      "page_count": 5,
      "order": 351,
      "p1": "1688",
      "pn": "1692",
      "abstract": [
        "Automatic speech recognition in reverberant conditions is a challenging\ntask as the long-term envelopes of the reverberant speech are temporally\nsmeared. In this paper, we propose a neural model for enhancement of\nsub-band temporal envelopes for dereverberation of speech. The temporal\nenvelopes are derived using the autoregressive modeling framework of\nfrequency domain linear prediction (FDLP). The neural enhancement model\nproposed in this paper performs an envelop gain based enhancement of\ntemporal envelopes and it consists of a series of convolutional and\nrecurrent neural network layers. The enhanced sub-band envelopes are\nused to generate features for automatic speech recognition (ASR). The\nASR experiments are performed on the REVERB challenge dataset as well\nas the CHiME-3 dataset. In these experiments, the proposed neural enhancement\napproach provides significant improvements over a baseline ASR system\nwith beamformed audio (average relative improvements of 21% on the\ndevelopment set and about 11% on the evaluation set in word error rates\nfor REVERB challenge dataset).\n"
      ],
      "doi": "10.21437/Interspeech.2020-2283",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "tomashenko20_interspeech": {
      "authors": [
        [
          "N.",
          "Tomashenko"
        ],
        [
          "Brij Mohan Lal",
          "Srivastava"
        ],
        [
          "Xin",
          "Wang"
        ],
        [
          "Emmanuel",
          "Vincent"
        ],
        [
          "Andreas",
          "Nautsch"
        ],
        [
          "Junichi",
          "Yamagishi"
        ],
        [
          "Nicholas",
          "Evans"
        ],
        [
          "Jose",
          "Patino"
        ],
        [
          "Jean-Fran\u00e7ois",
          "Bonastre"
        ],
        [
          "Paul-Gauthier",
          "No\u00e9"
        ],
        [
          "Massimiliano",
          "Todisco"
        ]
      ],
      "title": "Introducing the VoicePrivacy Initiative",
      "original": "1333",
      "page_count": 5,
      "order": 352,
      "p1": "1693",
      "pn": "1697",
      "abstract": [
        "The VoicePrivacy initiative aims to promote the development of privacy\npreservation tools for speech technology by gathering a new community\nto define the tasks of interest and the evaluation methodology, and\nbenchmarking solutions through a series of challenges. In this paper,\nwe formulate the voice anonymization task selected for the VoicePrivacy\n2020 Challenge and describe the datasets used for system development\nand evaluation. We also present the attack models and the associated\nobjective and subjective evaluation metrics. We introduce two anonymization\nbaselines and report objective evaluation results.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1333",
      "author_area_id": "13",
      "author_area_label": "Special Sessions"
    },
    "nautsch20_interspeech": {
      "authors": [
        [
          "Andreas",
          "Nautsch"
        ],
        [
          "Jose",
          "Patino"
        ],
        [
          "N.",
          "Tomashenko"
        ],
        [
          "Junichi",
          "Yamagishi"
        ],
        [
          "Paul-Gauthier",
          "No\u00e9"
        ],
        [
          "Jean-Fran\u00e7ois",
          "Bonastre"
        ],
        [
          "Massimiliano",
          "Todisco"
        ],
        [
          "Nicholas",
          "Evans"
        ]
      ],
      "title": "The Privacy ZEBRA: Zero Evidence Biometric Recognition Assessment",
      "original": "1815",
      "page_count": 5,
      "order": 353,
      "p1": "1698",
      "pn": "1702",
      "abstract": [
        "Mounting privacy legislation calls for the preservation of privacy\nin speech technology, though solutions are gravely lacking. While evaluation\ncampaigns are long-proven tools to drive progress, the need to consider\na privacy  adversary implies that traditional approaches to evaluation\nmust be adapted to the assessment of privacy and privacy preservation\nsolutions. This paper presents the first step in this direction:  metrics.<br\nstyle=\"mso-data-placement:same-cell;\" /><br style=\"mso-data-placement:same-cell;\"\n/>\n  We introduce the zero evidence biometric recognition assessment\n(ZEBRA) framework and propose two new privacy metrics. They measure\nthe  average level of privacy preservation afforded by a given safeguard\nfor a population and the  worst-case privacy disclosure for an individual.\nThe paper demonstrates their application to privacy preservation assessment\nwithin the scope of the VoicePrivacy challenge. While the ZEBRA framework\nis designed with speech applications in mind, it is a candidate for\nincorporation into biometric information protection standards and is\nreadily extendable to the study of privacy in applications even beyond\nspeech and biometrics.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1815",
      "author_area_id": "13",
      "author_area_label": "Special Sessions"
    },
    "mawalim20_interspeech": {
      "authors": [
        [
          "Candy Olivia",
          "Mawalim"
        ],
        [
          "Kasorn",
          "Galajit"
        ],
        [
          "Jessada",
          "Karnjana"
        ],
        [
          "Masashi",
          "Unoki"
        ]
      ],
      "title": "X-Vector Singular Value Modification and Statistical-Based Decomposition with Ensemble Regression Modeling for Speaker Anonymization System",
      "original": "1887",
      "page_count": 5,
      "order": 354,
      "p1": "1703",
      "pn": "1707",
      "abstract": [
        "Anonymizing speaker individuality is crucial for ensuring voice privacy\nprotection. In this paper, we propose a speaker individuality anonymization\nsystem that uses singular value modification and statistical-based\ndecomposition on an x-vector with ensemble regression modeling. An\nanonymization system requires speaker-to-speaker correspondence (each\nspeaker corresponds to a pseudo-speaker), which may be possible by\nmodifying significant x-vector elements. The significant elements were\ndetermined by singular value decomposition and variant analysis. Subsequently,\nthe anonymization process was performed by an ensemble regression model\ntrained using x-vector pools with clustering-based pseudo-targets.\nThe results demonstrated that our proposed anonymization system effectively\nimproves objective verifiability, especially in anonymized trials and\nanonymized enrollments setting, by preserving similar intelligibility\nscores with the baseline system introduced in the VoicePrivacy 2020\nChallenge.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1887",
      "author_area_id": "13",
      "author_area_label": "Special Sessions"
    },
    "maouche20_interspeech": {
      "authors": [
        [
          "Mohamed",
          "Maouche"
        ],
        [
          "Brij Mohan Lal",
          "Srivastava"
        ],
        [
          "Nathalie",
          "Vauquier"
        ],
        [
          "Aur\u00e9lien",
          "Bellet"
        ],
        [
          "Marc",
          "Tommasi"
        ],
        [
          "Emmanuel",
          "Vincent"
        ]
      ],
      "title": "A Comparative Study of Speech Anonymization Metrics",
      "original": "2248",
      "page_count": 5,
      "order": 355,
      "p1": "1708",
      "pn": "1712",
      "abstract": [
        "Speech anonymization techniques have recently been proposed for preserving\nspeakers&#8217; privacy. They aim at concealing speakers&#8217; identities\nwhile preserving the spoken content. In this study, we compare three\nmetrics proposed in the literature to assess the level of privacy achieved.\nWe exhibit through simulation the differences and blindspots of some\nmetrics. In addition, we conduct experiments on real data and state-of-the-art\nanonymization techniques to study how they behave in a practical scenario.\nWe show that the application-independent log-likelihood-ratio cost\nfunction C<SUP>min</SUP><SUB>llr</SUB> provides a more robust evaluation\nof privacy than the equal error rate (EER), and that detection-based\nmetrics provide different information from linkability metrics. Interestingly,\nthe results on real data indicate that current anonymization design\nchoices do not induce a regime where the differences between those\nmetrics become apparent.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2248",
      "author_area_id": "13",
      "author_area_label": "Special Sessions"
    },
    "srivastava20_interspeech": {
      "authors": [
        [
          "Brij Mohan Lal",
          "Srivastava"
        ],
        [
          "N.",
          "Tomashenko"
        ],
        [
          "Xin",
          "Wang"
        ],
        [
          "Emmanuel",
          "Vincent"
        ],
        [
          "Junichi",
          "Yamagishi"
        ],
        [
          "Mohamed",
          "Maouche"
        ],
        [
          "Aur\u00e9lien",
          "Bellet"
        ],
        [
          "Marc",
          "Tommasi"
        ]
      ],
      "title": "Design Choices for X-Vector Based Speaker Anonymization",
      "original": "2692",
      "page_count": 5,
      "order": 356,
      "p1": "1713",
      "pn": "1717",
      "abstract": [
        "The recently proposed x-vector based anonymization scheme converts\nany input voice into that of a random  pseudo-speaker. In this paper,\nwe present a flexible pseudo-speaker selection technique as a baseline\nfor the first VoicePrivacy Challenge. We explore several design choices\nfor the distance metric between speakers, the region of x-vector space\nwhere the pseudo-speaker is picked, and gender selection. To assess\nthe strength of anonymization achieved, we consider attackers using\nan x-vector based speaker verification system who may use original\nor anonymized speech for enrollment, depending on their knowledge of\nthe anonymization scheme. The Equal Error Rate (EER) achieved by the\nattackers and the decoding Word Error Rate (WER) over anonymized data\nare reported as the measures of privacy and utility. Experiments are\nperformed using datasets derived from LibriSpeech to find the optimal\ncombination of design choices in terms of privacy and utility.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2692",
      "author_area_id": "13",
      "author_area_label": "Special Sessions"
    },
    "noe20_interspeech": {
      "authors": [
        [
          "Paul-Gauthier",
          "No\u00e9"
        ],
        [
          "Jean-Fran\u00e7ois",
          "Bonastre"
        ],
        [
          "Driss",
          "Matrouf"
        ],
        [
          "N.",
          "Tomashenko"
        ],
        [
          "Andreas",
          "Nautsch"
        ],
        [
          "Nicholas",
          "Evans"
        ]
      ],
      "title": "Speech Pseudonymisation Assessment Using Voice Similarity Matrices",
      "original": "2720",
      "page_count": 5,
      "order": 357,
      "p1": "1718",
      "pn": "1722",
      "abstract": [
        "The proliferation of speech technologies and rising privacy legislation\ncalls for the development of privacy preservation solutions for speech\napplications. These are essential since speech signals convey a wealth\nof rich, personal and potentially sensitive information. Anonymisation,\nthe focus of the recent VoicePrivacy initiative, is one strategy to\nprotect speaker identity information. Pseudonymisation solutions aim\nnot only to mask the speaker identity and preserve the linguistic content,\nquality and naturalness, as is the goal of anonymisation, but also\nto preserve voice distinctiveness. Existing metrics for the assessment\nof anonymisation are ill-suited and those for the assessment of pseudonymisation\nare completely lacking. Based upon voice similarity matrices, this\npaper proposes the first intuitive visualisation of pseudonymisation\nperformance for speech signals and two novel metrics for objective\nassessment. They reflect the two, key pseudonymisation requirements\nof de-identification and voice distinctiveness.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2720",
      "author_area_id": "13",
      "author_area_label": "Special Sessions"
    },
    "park20c_interspeech": {
      "authors": [
        [
          "Kyubyong",
          "Park"
        ],
        [
          "Seanie",
          "Lee"
        ]
      ],
      "title": "g2pM: A Neural Grapheme-to-Phoneme Conversion Package for Mandarin Chinese Based on a New Open Benchmark Dataset",
      "original": "1094",
      "page_count": 5,
      "order": 358,
      "p1": "1723",
      "pn": "1727",
      "abstract": [
        "Conversion of Chinese graphemes to phonemes (G2P) is an essential component\nin Mandarin Chinese Text-To-Speech (TTS) systems. One of the biggest\nchallenges in Chinese G2P conversion is how to disambiguate the pronunciation\nof polyphones &#8212; characters having multiple pronunciations. Although\nmany academic efforts have been made to address it, there has been\nno open dataset that can serve as a standard benchmark for a fair comparison\nto date. In addition, most of the reported systems are hard to employ\nfor researchers or practitioners who want to convert Chinese text into\npinyin at their convenience. Motivated by these, in this work, we introduce\na new benchmark dataset that consists of 99,000+ sentences for Chinese\npolyphone disambiguation. We train a simple Bi-LSTM model on it and\nfind that it outperforms other pre-existing G2P systems and slightly\nunderperforms pre-trained Chinese BERT. Finally, we package our project\nand share it on PyPi.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1094",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "zhang20n_interspeech": {
      "authors": [
        [
          "Haiteng",
          "Zhang"
        ],
        [
          "Huashan",
          "Pan"
        ],
        [
          "Xiulin",
          "Li"
        ]
      ],
      "title": "A Mask-Based Model for Mandarin Chinese Polyphone Disambiguation",
      "original": "1142",
      "page_count": 5,
      "order": 359,
      "p1": "1728",
      "pn": "1732",
      "abstract": [
        "Polyphone disambiguation serves as an essential part of Mandarin text-to-speech\n(TTS) system. However, conventional system modelling the entire Pinyin\nset causes the case that prediction belongs to the unrelated polyphonic\ncharacter instead of the current input one, which has negative impacts\non TTS performance. To address this issue, we introduce a mask-based\nmodel for polyphone disambiguation. The model takes a mask vector extracted\nfrom the context as an extra input. In our model, the mask vector not\nonly acts as a weighting factor in Weighted-softmax to prevent the\ncase of mis-prediction but also eliminates the contribution of non-candidate\nset to the overall loss. Moreover, to mitigate the uneven distribution\nof pronunciation, we introduce a new loss called Modified Focal Loss.\nThe experimental result shows the effectiveness of the proposed mask-based\nmodel. We also empirically studied the impact of Weighted-softmax and\nModified Focal Loss. It was found that Weighted-softmax can effectively\nprevent the model from predicting outside the candidate set. Besides,\nModified Focal Loss can reduce the adverse impacts of the uneven distribution\nof pronunciation.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1142",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "cohn20_interspeech": {
      "authors": [
        [
          "Michelle",
          "Cohn"
        ],
        [
          "Georgia",
          "Zellou"
        ]
      ],
      "title": "Perception of Concatenative vs. Neural Text-To-Speech (TTS): Differences in Intelligibility in Noise and Language Attitudes",
      "original": "1336",
      "page_count": 5,
      "order": 360,
      "p1": "1733",
      "pn": "1737",
      "abstract": [
        "This study tests speech-in-noise perception and social ratings of speech\nproduced by different text-to-speech (TTS) synthesis methods. We used\nidentical speaker training datasets for a set of 4 voices (using AWS\nPolly TTS), generated using neural and concatenative TTS. In Experiment\n1, listeners identified target words in semantically predictable and\nunpredictable sentences in concatenative and neural TTS at two noise\nlevels (-3 dB, -6 dB SNR). Correct word identification was lower for\nneural TTS than for concatenative TTS, in the lower SNR, and for semantically\nunpredictable sentences. In Experiment 2, listeners rated the voices\non 4 social attributes. Neural TTS was rated as more human-like, natural,\nlikeable, and familiar than concatenative TTS. Furthermore, how natural\nlisteners rated the neural TTS voice was positively related to their\nspeech-in-noise accuracy. Together, these findings show that the TTS\nmethod influences both intelligibility and social judgments of speech\n&#8212; and that these patterns are linked. Overall, this work contributes\nto our understanding of the nexus of speech technology and human speech\nperception.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1336",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "taylor20_interspeech": {
      "authors": [
        [
          "Jason",
          "Taylor"
        ],
        [
          "Korin",
          "Richmond"
        ]
      ],
      "title": "Enhancing Sequence-to-Sequence Text-to-Speech with Morphology",
      "original": "1547",
      "page_count": 5,
      "order": 361,
      "p1": "1738",
      "pn": "1742",
      "abstract": [
        "Neural sequence-to-sequence (S2S) modelling encodes a single, unified\nrepresentation for each input sequence. When used for text-to-speech\nsynthesis (TTS), such representations must embed ambiguities between\nEnglish spelling and pronunciation. For example, in  pothole and  there\nthe character sequence  th sounds different. This can be problematic\nwhen predicting pronunciation directly from letters. We posit pronunciation\nbecomes easier to predict when letters are grouped into sub-word units\nlike morphemes (e.g. a boundary lies between  t and  h in  pothole\nbut not  there). Moreover, morphological boundaries can reduce the\ntotal number of, and increase the counts of, seen unit subsequences.\nAccordingly, we test here the effect of augmenting input sequences\nof letters with morphological boundaries. We find morphological boundaries\nsubstantially lower the Word and Phone Error Rates (WER and PER) for\na Bi-LSTM performing G2P on one hand, and also increase the naturalness\nscores of Tacotrons performing TTS in a MUSHRA listening test on the\nother. The improvements to TTS quality are such that grapheme input\naugmented with morphological boundaries outperforms phone input without\nboundaries. Since morphological segmentation may be predicted with\nhigh accuracy, we highlight this simple pre-processing step has important\npotential for S2S modelling in TTS.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1547",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "choi20b_interspeech": {
      "authors": [
        [
          "Yeunju",
          "Choi"
        ],
        [
          "Youngmoon",
          "Jung"
        ],
        [
          "Hoirin",
          "Kim"
        ]
      ],
      "title": "Deep MOS Predictor for Synthetic Speech Using Cluster-Based Modeling",
      "original": "2111",
      "page_count": 5,
      "order": 362,
      "p1": "1743",
      "pn": "1747",
      "abstract": [
        "While deep learning has made impressive progress in speech synthesis\nand voice conversion, the assessment of the synthesized speech is still\ncarried out by human participants. Several recent papers have proposed\ndeep-learning-based assessment models and shown the potential to automate\nthe speech quality assessment. To improve the previously proposed assessment\nmodel, MOSNet, we propose three models using cluster-based modeling\nmethods: using a global quality token (GQT) layer, using an Encoding\nLayer, and using both of them. We perform experiments using the evaluation\nresults of the Voice Conversion Challenge 2018 to predict the mean\nopinion score of synthesized speech and similarity score between synthesized\nspeech and reference speech. The results show that the GQT layer helps\nto predict human assessment better by automatically learning the useful\nquality tokens for the task and that the Encoding Layer helps to utilize\nframe-level scores more precisely.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2111",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "mittag20_interspeech": {
      "authors": [
        [
          "Gabriel",
          "Mittag"
        ],
        [
          "Sebastian",
          "M\u00f6ller"
        ]
      ],
      "title": "Deep Learning Based Assessment of Synthetic Speech Naturalness",
      "original": "2382",
      "page_count": 5,
      "order": 363,
      "p1": "1748",
      "pn": "1752",
      "abstract": [
        "In this paper, we present a new objective prediction model for synthetic\nspeech naturalness. It can be used to evaluate Text-To-Speech or Voice\nConversion systems and works language independently. The model is trained\nend-to-end and based on a CNN-LSTM network that previously showed to\ngive good results for speech quality estimation. We trained and tested\nthe model on 16 different datasets, such as from the Blizzard Challenge\nand the Voice Conversion Challenge. Further, we show that the reliability\nof deep learning-based naturalness prediction can be improved by transfer\nlearning from speech quality prediction models that are trained on\nobjective POLQA scores. The proposed model is made publicly available\nand can, for example, be used to evaluate different TTS system configurations.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2382",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "zhang20o_interspeech": {
      "authors": [
        [
          "Jiawen",
          "Zhang"
        ],
        [
          "Yuanyuan",
          "Zhao"
        ],
        [
          "Jiaqi",
          "Zhu"
        ],
        [
          "Jinba",
          "Xiao"
        ]
      ],
      "title": "Distant Supervision for Polyphone Disambiguation in Mandarin Chinese",
      "original": "2427",
      "page_count": 5,
      "order": 364,
      "p1": "1753",
      "pn": "1757",
      "abstract": [
        "Grapheme-to-phoneme (G2P) conversion plays an important role in building\na Mandarin Chinese text-to-speech (TTS) system, where the polyphone\ndisambiguation is an indispensable task. However, most of the previous\npolyphone disambiguation models are trained on manually annotated datasets,\nwhich are suffering from data scarcity, narrow coverage, and unbalanced\ndata distribution. In this paper, we propose a framework that can predict\nthe pronunciations of Chinese characters, and the core model is trained\nin a distantly supervised way. Specifically, we utilize the alignment\nprocedure used for acoustic models to produce abundant character-phoneme\nsequence pairs, which are employed to train a Seq2Seq model with attention\nmechanism. We also make use of a language model that is trained on\nphoneme sequences to alleviate the impact of noises in the auto-generated\ndataset. Experimental results demonstrate that even without additional\nsyntactic features and pre-trained embeddings, our approach achieves\ncompetitive prediction results, and especially improves the predictive\naccuracy for unbalanced polyphonic characters. In addition, compared\nwith the manually annotated training datasets, the auto-generated one\nis more diversified and makes the results more consistent with the\npronunciation habits of most people.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2427",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "gallegos20_interspeech": {
      "authors": [
        [
          "Pilar Oplustil",
          "Gallegos"
        ],
        [
          "Jennifer",
          "Williams"
        ],
        [
          "Joanna",
          "Rownicka"
        ],
        [
          "Simon",
          "King"
        ]
      ],
      "title": "An Unsupervised Method to Select a Speaker Subset from Large Multi-Speaker Speech Synthesis Datasets",
      "original": "2567",
      "page_count": 5,
      "order": 365,
      "p1": "1758",
      "pn": "1762",
      "abstract": [
        "Large multi-speaker datasets for TTS typically contain diverse speakers,\nrecording conditions, styles and quality of data. Although one might\ngenerally presume that more data is better, in this paper we show that\na model trained on a carefully-chosen subset of speakers from LibriTTS\nprovides significantly better quality synthetic speech than a model\ntrained on a larger set. We propose an unsupervised methodology to\nfind this subset by clustering per-speaker acoustic representations.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2567",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "das20_interspeech": {
      "authors": [
        [
          "Anurag",
          "Das"
        ],
        [
          "Guanlong",
          "Zhao"
        ],
        [
          "John",
          "Levis"
        ],
        [
          "Evgeny",
          "Chukharev-Hudilainen"
        ],
        [
          "Ricardo",
          "Gutierrez-Osuna"
        ]
      ],
      "title": "Understanding the Effect of Voice Quality and Accent on Talker Similarity",
      "original": "2910",
      "page_count": 5,
      "order": 366,
      "p1": "1763",
      "pn": "1767",
      "abstract": [
        "This paper presents a methodology to study the role of non-native accents\non talker recognition by humans. The methodology combines a state-of-the-art\naccent-conversion system to resynthesize the voice of a speaker with\na different accent of her/his own, and a protocol for perceptual listening\ntests to measure the relative contribution of accent and voice quality\non speaker similarity. Using a corpus of non-native and native speakers,\nwe generated accent conversions in two different directions: non-native\nspeakers with native accents, and native speakers with non-native accents.\nThen, we asked listeners to rate the similarity between 50 pairs of\nreal or synthesized speakers. Using a linear mixed effects model, we\nfind that (for our corpus) the effect of voice quality is five times\nas large as that of non-native accent, and that the effect goes away\nwhen speakers share the same (native) accent. We discuss the potential\nsignificance of this work in earwitness identification and sociophonetics.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2910",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "zhou20c_interspeech": {
      "authors": [
        [
          "Wei",
          "Zhou"
        ],
        [
          "Ralf",
          "Schl\u00fcter"
        ],
        [
          "Hermann",
          "Ney"
        ]
      ],
      "title": "Robust Beam Search for Encoder-Decoder Attention Based Speech Recognition Without Length Bias",
      "original": "1958",
      "page_count": 5,
      "order": 367,
      "p1": "1768",
      "pn": "1772",
      "abstract": [
        "As one popular modeling approach for end-to-end speech recognition,\nattention-based encoder-decoder models are known to suffer the length\nbias and corresponding beam problem. Different approaches have been\napplied in simple beam search to ease the problem, most of which are\nheuristic-based and require considerable tuning. We show that heuristics\nare not proper modeling refinement, which results in severe performance\ndegradation with largely increased beam sizes. We propose a novel beam\nsearch derived from reinterpreting the sequence posterior with an explicit\nlength modeling. By applying the reinterpreted probability together\nwith beam pruning, the obtained final probability leads to a robust\nmodel modification, which allows reliable comparison among output sequences\nof different lengths. Experimental verification on the LibriSpeech\ncorpus shows that the proposed approach solves the length bias problem\nwithout heuristics or additional tuning effort. It provides robust\ndecision making and consistently good performance under both small\nand very large beam sizes. Compared with the best results of the heuristic\nbaseline, the proposed approach achieves the same WER on the &#8216;clean&#8217;\nsets and 4% relative improvement on the &#8216;other&#8217; sets. We\nalso show that it is more efficient with the additional derived early\nstopping criterion.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1958",
      "author_area_id": "9",
      "author_area_label": "Speech Recognition: Architecture, Search, and Linguistic Components"
    },
    "chen20e_interspeech": {
      "authors": [
        [
          "Xi",
          "Chen"
        ],
        [
          "Songyang",
          "Zhang"
        ],
        [
          "Dandan",
          "Song"
        ],
        [
          "Peng",
          "Ouyang"
        ],
        [
          "Shouyi",
          "Yin"
        ]
      ],
      "title": "Transformer with Bidirectional Decoder for Speech Recognition",
      "original": "2677",
      "page_count": 5,
      "order": 368,
      "p1": "1773",
      "pn": "1777",
      "abstract": [
        "Attention-based models have made tremendous progress on end-to-end\nautomatic speech recognition (ASR) recently. However, the conventional\ntransformer-based approaches usually generate the sequence results\ntoken by token from left to right, leaving the right-to-left contexts\nunexploited. In this work, we introduce a bidirectional speech transformer\nto utilize the different directional contexts simultaneously. Specifically,\nthe outputs of our proposed transformer include a left-to-right target,\nand a right-to-left target. In inference stage, we use the introduced\nbidirectional beam search method, which can not only generate left-to-right\ncandidates but also generate right-to-left candidates, and determine\nthe best hypothesis by the score.<br style=\"mso-data-placement:same-cell;\"\n/><br style=\"mso-data-placement:same-cell;\" />\n  To demonstrate our\nproposed speech transformer with a bidirectional decoder (STBD), we\nconduct extensive experiments on the AISHELL-1 dataset. The results\nof experiments show that STBD achieves a 3.6% relative CER reduction\n(CERR) over the unidirectional speech transformer baseline. Besides,\nthe strongest model in this paper called STBD-Big can achieve 6.64%\nCER on the test set, without language model rescoring and any extra\ndata augmentation strategies.<SUP>1</SUP>\n"
      ],
      "doi": "10.21437/Interspeech.2020-2677",
      "author_area_id": "9",
      "author_area_label": "Speech Recognition: Architecture, Search, and Linguistic Components"
    },
    "wang20r_interspeech": {
      "authors": [
        [
          "Weiran",
          "Wang"
        ],
        [
          "Guangsen",
          "Wang"
        ],
        [
          "Aadyot",
          "Bhatnagar"
        ],
        [
          "Yingbo",
          "Zhou"
        ],
        [
          "Caiming",
          "Xiong"
        ],
        [
          "Richard",
          "Socher"
        ]
      ],
      "title": "An Investigation of Phone-Based Subword Units for End-to-End Speech Recognition",
      "original": "1873",
      "page_count": 5,
      "order": 369,
      "p1": "1778",
      "pn": "1782",
      "abstract": [
        "Phones and their context-dependent variants have been the standard\nmodeling units for conventional speech recognition systems, while characters\nand subwords have demonstrated their effectiveness for end-to-end recognition\nsystems. We investigate the use of phone-based subwords, in particular,\nbyte pair encoder (BPE), as modeling units for end-to-end speech recognition.\nIn addition, we also developed multi-level language model-based decoding\nalgorithms based on a pronunciation dictionary. Besides the use of\nthe lexicon, which is easily available, our system avoids the need\nof additional expert knowledge or processing steps from conventional\nsystems. Experimental results show that phone-based BPEs tend to yield\nmore accurate recognition systems than the character-based counterpart.\nIn addition, further improvement can be obtained with a novel one-pass\njoint beam search decoder, which efficiently combines phone- and character-based\nBPE systems. For Switchboard, our phone-based BPE system achieves 6.8%/14.4%\nword error rate (WER) on the Switchboard/CallHome portion of the test\nset while joint decoding achieves 6.3%/13.3% WER. On Fisher + Switchboard,\njoint decoding leads to 4.9%/9.5% WER, setting new milestones for telephony\nspeech recognition.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1873",
      "author_area_id": "9",
      "author_area_label": "Speech Recognition: Architecture, Search, and Linguistic Components"
    },
    "wong20_interspeech": {
      "authors": [
        [
          "Jeremy H.M.",
          "Wong"
        ],
        [
          "Yashesh",
          "Gaur"
        ],
        [
          "Rui",
          "Zhao"
        ],
        [
          "Liang",
          "Lu"
        ],
        [
          "Eric",
          "Sun"
        ],
        [
          "Jinyu",
          "Li"
        ],
        [
          "Yifan",
          "Gong"
        ]
      ],
      "title": "Combination of End-to-End and Hybrid Models for Speech Recognition",
      "original": "2141",
      "page_count": 5,
      "order": 370,
      "p1": "1783",
      "pn": "1787",
      "abstract": [
        "Recent studies suggest that it may now be possible to construct end-to-end\nNeural Network (NN) models that perform on-par with, or even outperform,\nhybrid models in speech recognition. These models differ in their designs,\nand as such, may exhibit diverse and complementary error patterns.\nA combination between the predictions of these models may therefore\nyield significant gains. This paper studies the feasibility of performing\nhypothesis-level combination between hybrid and end-to-end NN models.\nThe end-to-end NN models often exhibit a bias in their posteriors toward\nshort hypotheses, and this may adversely affect Minimum Bayes&#8217;\nRisk (MBR) combination methods. MBR training and length normalisation\ncan be used to reduce this bias. Models are trained on Microsoft&#8217;s\n75 thousand hours of anonymised data and evaluated on test sets with\n1.8 million words. The results show that significant gains can be obtained\nby combining the hypotheses of hybrid and end-to-end NN models together.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2141",
      "author_area_id": "9",
      "author_area_label": "Speech Recognition: Architecture, Search, and Linguistic Components"
    },
    "kim20c_interspeech": {
      "authors": [
        [
          "Jihwan",
          "Kim"
        ],
        [
          "Jisung",
          "Wang"
        ],
        [
          "Sangki",
          "Kim"
        ],
        [
          "Yeha",
          "Lee"
        ]
      ],
      "title": "Evolved Speech-Transformer: Applying Neural Architecture Search to End-to-End Automatic Speech Recognition",
      "original": "1233",
      "page_count": 5,
      "order": 371,
      "p1": "1788",
      "pn": "1792",
      "abstract": [
        "Neural architecture search (NAS) has been successfully applied to finding\nefficient, high-performance deep neural network architectures in a\ntask-adaptive manner without extensive human intervention. This is\nachieved by choosing genetic, reinforcement learning, or gradient -based\nalgorithms as automative alternatives of manual architecture design.\nHowever, a naive application of existing NAS algorithms to different\ntasks may result in architectures which perform sub-par to those manually\ndesigned. In this work, we show that NAS can provide efficient architectures\nthat outperform manually designed attention-based architectures on\nspeech recognition tasks, after which we named Evolved Speech-Transformer\n(EST). With a combination of carefully designed search space and Progressive\ndynamic hurdles, a genetic algorithm based, our algorithm finds a memory-efficient\narchitecture which outperforms vanilla Transformer with reduced training\ntime.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1233"
    },
    "garg20_interspeech": {
      "authors": [
        [
          "Abhinav",
          "Garg"
        ],
        [
          "Ashutosh",
          "Gupta"
        ],
        [
          "Dhananjaya",
          "Gowda"
        ],
        [
          "Shatrughan",
          "Singh"
        ],
        [
          "Chanwoo",
          "Kim"
        ]
      ],
      "title": "Hierarchical Multi-Stage Word-to-Grapheme Named Entity Corrector for Automatic Speech Recognition",
      "original": "3174",
      "page_count": 5,
      "order": 372,
      "p1": "1793",
      "pn": "1797",
      "abstract": [
        "In this paper, we propose a hierarchical multi-stage word-to-grapheme\nNamed Entity Correction (NEC) algorithm. Conventional NEC algorithms\nuse a single-stage grapheme or phoneme level edit distance to search\nand replace Named Entities (NEs) misrecognized by a speech recognizer.\nHowever, longer named entities like song titles cannot be easily handled\nby such a single stage correction. We propose a three-stage NEC, starting\nwith a word-level matching, followed by a phonetic double metaphone\nbased matching, and a final grapheme level candidate selection. We\nalso propose a novel NE Rejection mechanism which is important to ensure\nthat the NEC does not replace correctly recognized NEs with unintended\nbut similar named entities. We evaluate our solution on two different\ntest sets from the  call and  music domains, for both server as well\nas on-device speech recognition configurations. For the on-device model,\nour NEC outperforms an n-gram fusion when employed standalone. Our\nNEC reduces the word error rate by 14% and 63% relatively for  music\nand  call, respectively, when used after an n-gram based biasing language\nmodel. The average latency of our NEC is under 3 ms per input sentence\nwhile using only &#126;1 MB for an input NE list of 20,000 entries.\n"
      ],
      "doi": "10.21437/Interspeech.2020-3174",
      "author_area_id": "9",
      "author_area_label": "Speech Recognition: Architecture, Search, and Linguistic Components"
    },
    "beck20_interspeech": {
      "authors": [
        [
          "Eugen",
          "Beck"
        ],
        [
          "Ralf",
          "Schl\u00fcter"
        ],
        [
          "Hermann",
          "Ney"
        ]
      ],
      "title": "LVCSR with Transformer Language Models",
      "original": "1164",
      "page_count": 5,
      "order": 373,
      "p1": "1798",
      "pn": "1802",
      "abstract": [
        "Neural network language models (LMs) based on self-attention have recently\noutperformed the previous state of the art, LSTM LMs. Transformer LMs\ntoday are often used as a postprocessing step in lattice or n-best\nlist rescoring. In this work the main focus is on using them in one-pass\nrecognition. We show that by a simple reduction of redundant computations\nin batched self-attention we can obtain a 15% reduction in overall\nRTF on a well-tuned system. We also show that through proper initialization\nthe layer normalization inside the residual blocks can be removed,\nyielding a further increase in forwarding speed. This is done under\nthe constraint of staying close to state-of-the-art in terms of word-error\nrate (5.4% on LibriSpeech test-other) and achieving a real-time factor\nof around 1. Last but not least we also present an approach to speed\nup classic push-forward rescoring by mixing it with n-best list rescoring\nto better utilize the inherent parallelizability of Transformer language\nmodels, cutting the time needed for rescoring in half.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1164",
      "author_area_id": "9",
      "author_area_label": "Speech Recognition: Architecture, Search, and Linguistic Components"
    },
    "chen20f_interspeech": {
      "authors": [
        [
          "Yi-Chen",
          "Chen"
        ],
        [
          "Jui-Yang",
          "Hsu"
        ],
        [
          "Cheng-Kuang",
          "Lee"
        ],
        [
          "Hung-yi",
          "Lee"
        ]
      ],
      "title": "DARTS-ASR: Differentiable Architecture Search for Multilingual Speech Recognition and Adaptation",
      "original": "1315",
      "page_count": 5,
      "order": 374,
      "p1": "1803",
      "pn": "1807",
      "abstract": [
        "In previous works, only parameter weights of ASR models are optimized\nunder fixed-topology architecture. However, the design of successful\nmodel architecture has always relied on human experience and intuition.\nBesides, many hyperparameters related to model architecture need to\nbe manually tuned. Therefore in this paper, we propose an ASR approach\nwith efficient gradient-based architecture search, DARTS-ASR. In order\nto examine the generalizability of DARTS-ASR, we apply our approach\nnot only on many languages to perform monolingual ASR, but also on\na multilingual ASR setting. Following previous works, we conducted\nexperiments on a multilingual dataset, IARPA BABEL. The experiment\nresults show that our approach outperformed the baseline fixed-topology\narchitecture by 10.2% and 10.0% relative reduction on character error\nrates under monolingual and multilingual ASR settings respectively.\nFurthermore, we perform some analysis on the searched architectures\nby DARTS-ASR.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1315",
      "author_area_id": "9",
      "author_area_label": "Speech Recognition: Architecture, Search, and Linguistic Components"
    },
    "stappen20_interspeech": {
      "authors": [
        [
          "Lukas",
          "Stappen"
        ],
        [
          "Georgios",
          "Rizos"
        ],
        [
          "Madina",
          "Hasan"
        ],
        [
          "Thomas",
          "Hain"
        ],
        [
          "Bj\u00f6rn W.",
          "Schuller"
        ]
      ],
      "title": "Uncertainty-Aware Machine Support for Paper Reviewing on the Interspeech 2019 Submission Corpus",
      "original": "2862",
      "page_count": 5,
      "order": 375,
      "p1": "1808",
      "pn": "1812",
      "abstract": [
        "The evaluation of scientific submissions through peer review is both\nthe most fundamental component of the publication process, as well\nas the most frequently criticised and questioned. Academic journals\nand conferences request reviews from multiple reviewers per submission,\nwhich an editor, or area chair aggregates into the final acceptance\ndecision. Reviewers are often in disagreement due to varying levels\nof domain expertise, confidence, levels of motivation, as well as due\nto the heavy workload and the different interpretations by the reviewers\nof the score scale. Herein, we explore the possibility of a computational\ndecision support tool for the editor, based on Natural Language Processing,\nthat offers an additional aggregated recommendation. We provide a comparative\nstudy of state-of-the-art text modelling methods on the newly crafted,\nlargest review dataset of its kind based on Interspeech 2019, and we\nare the first to explore uncertainty-aware methods (soft labels, quantile\nregression) to address the subjectivity inherent in this problem.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2862",
      "author_area_id": "3",
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "cohn20b_interspeech": {
      "authors": [
        [
          "Michelle",
          "Cohn"
        ],
        [
          "Melina",
          "Sarian"
        ],
        [
          "Kristin",
          "Predeck"
        ],
        [
          "Georgia",
          "Zellou"
        ]
      ],
      "title": "Individual Variation in Language Attitudes Toward Voice-AI: The Role of Listeners&#8217; Autistic-Like Traits",
      "original": "1339",
      "page_count": 5,
      "order": 376,
      "p1": "1813",
      "pn": "1817",
      "abstract": [
        "More and more, humans are engaging with voice-activated artificially\nintelligent (voice-AI) systems that have names (e.g., Alexa), apparent\ngenders, and even emotional expression; they are in many ways a growing\n&#8216;social&#8217;[ presence. But to what extent do people display\nsociolinguistic attitudes, developed from human-human interaction,\ntoward these disembodied text-to-speech (TTS) voices? And how might\nthey vary based on the cognitive traits of the individual user? The\ncurrent study addresses these questions, testing native English speakers&#8217;\njudgments for 6 traits (intelligent, likeable, attractive, professional,\nhuman-like, and age) for a naturally-produced female human voice and\nthe US-English default Amazon Alexa voice. Following exposure to the\nvoices, participants completed these ratings for each speaker, as well\nas the Autism Quotient (AQ) survey, to assess individual differences\nin cognitive processing style. Results show differences in individuals&#8217;\nratings of the likeability and human-likeness of the human and AI talkers\nbased on AQ score. Results suggest that humans transfer social assessment\nof human voices to voice-AI, but that the way they do so is mediated\nby their own cognitive characteristics.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1339",
      "author_area_id": "3",
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "cohn20c_interspeech": {
      "authors": [
        [
          "Michelle",
          "Cohn"
        ],
        [
          "Eran",
          "Raveh"
        ],
        [
          "Kristin",
          "Predeck"
        ],
        [
          "Iona",
          "Gessinger"
        ],
        [
          "Bernd",
          "M\u00f6bius"
        ],
        [
          "Georgia",
          "Zellou"
        ]
      ],
      "title": "Differences in Gradient Emotion Perception: Human vs. Alexa Voices",
      "original": "1938",
      "page_count": 5,
      "order": 377,
      "p1": "1818",
      "pn": "1822",
      "abstract": [
        "The present study compares how individuals perceive gradient acoustic\nrealizations of emotion produced by a human voice versus an Amazon\nAlexa text-to-speech (TTS) voice. We manipulated semantically neutral\nsentences spoken by both talkers with identical emotional synthesis\nmethods, using three levels of increasing &#8216;happiness&#8217; (0%,\n33%, 66% &#8216;happier&#8217;). On each trial, listeners (native speakers\nof American English, n=99) rated a given sentence on two scales to\nassess dimensions of emotion: valence (negative-positive) and arousal\n(calm-excited). Participants also rated the Alexa voice on several\nparameters to assess anthropomorphism (e.g., naturalness, human-likeness,\netc.). Results showed that the emotion manipulations led to increases\nin perceived positive valence and excitement. Yet, the effect differed\nby interlocutor: increasing &#8216;happiness&#8217; manipulations led\nto larger changes for the human voice than the Alexa voice. Additionally,\nwe observed individual differences in perceived valence/arousal based\non participants&#8217; anthropomorphism scores. Overall, this line\nof research can speak to theories of computer personification and elucidate\nour changing relationship with voice-AI technology.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1938",
      "author_area_id": "3",
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "martinezlucas20_interspeech": {
      "authors": [
        [
          "Luz",
          "Martinez-Lucas"
        ],
        [
          "Mohammed",
          "Abdelwahab"
        ],
        [
          "Carlos",
          "Busso"
        ]
      ],
      "title": "The MSP-Conversation Corpus",
      "original": "2444",
      "page_count": 5,
      "order": 378,
      "p1": "1823",
      "pn": "1827",
      "abstract": [
        "Human-computer interactions can be very effective, especially if computers\ncan automatically recognize the emotional state of the user. A key\nbarrier for effective speech emotion recognition systems is the lack\nof large corpora annotated with emotional labels that reflect the temporal\ncomplexity of expressive behaviors, especially during multiparty interactions.\nThis paper introduces the MSP-Conversation corpus, which contains interactions\nannotated with time-continuous emotional traces for arousal (calm to\nactive), valence (negative to positive), and dominance (weak to strong).\nTime-continuous annotations offer the flexibility to explore emotional\ndisplays at different temporal resolutions while leveraging contextual\ninformation. This is an ongoing effort, where the corpus currently\ncontains more than 15 hours of speech annotated by at least five annotators.\nThe data is sourced from the MSP-Podcast corpus, which contains speech\ndata from online audio-sharing websites annotated with sentence-level\nemotional scores. This data collection scheme is an easy, affordable,\nand scalable approach to obtain natural data with diverse emotional\ncontent from multiple speakers. This study describes the key features\nof the corpus. It also compares the time-continuous evaluations from\nthe MSP-Conversation corpus with the sentence-level annotations of\nthe MSP-Podcast corpus for the speech segments that overlap between\nthe two corpora.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2444",
      "author_area_id": "3",
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "tao20_interspeech": {
      "authors": [
        [
          "Fuxiang",
          "Tao"
        ],
        [
          "Anna",
          "Esposito"
        ],
        [
          "Alessandro",
          "Vinciarelli"
        ]
      ],
      "title": "Spotting the Traces of Depression in Read Speech: An Approach Based on Computational Paralinguistics and Social Signal Processing",
      "original": "2888",
      "page_count": 5,
      "order": 379,
      "p1": "1828",
      "pn": "1832",
      "abstract": [
        "This work investigates the use of a classification approach as a means\nto identify effective depression markers in read speech, i.e., observable\nand measurable traces of the pathology in the way people read a predefined\ntext. This is important because the diagnosis of depression is still\na challenging problem and reliable markers can, at least to a partial\nextent, contribute to address it. The experiments have involved 110\nindividuals and revolve around the tendency of depressed people to\nread slower and display silences that are both longer and more frequent.\nThe results show that features expected to capture such differences\nreduce the error rate of a baseline classifier by more than 50% (from\n31.8% to 15.5%). This is of particular interest when considering that\nthe new features are less than 10% of the original set (3 out of 32).\nFurthermore, the results appear to be in line with the findings of\nneuroscience about brain-level differences between depressed and non-depressed\nindividuals.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2888",
      "author_area_id": "3",
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "kim20d_interspeech": {
      "authors": [
        [
          "Yelin",
          "Kim"
        ],
        [
          "Joshua",
          "Levy"
        ],
        [
          "Yang",
          "Liu"
        ]
      ],
      "title": "Speech Sentiment and Customer Satisfaction Estimation in Socialbot Conversations",
      "original": "2890",
      "page_count": 5,
      "order": 380,
      "p1": "1833",
      "pn": "1837",
      "abstract": [
        "For an interactive agent, such as task-oriented spoken dialog systems\nor chatbots, measuring and adapting to Customer Satisfaction (CSAT)\nis critical in order to understand user perception of an agent&#8217;s\nbehavior and increase user engagement and retention. However, an agent\noften relies on explicit customer feedback for measuring CSAT. Such\nexplicit feedback may result in potential distraction to users and\nit can be challenging to capture continuously changing user&#8217;s\nsatisfaction. To address this challenge, we present a new approach\nto automatically estimate CSAT using acoustic and lexical information\nin the Alexa Prize Socialbot data. We first explore the relationship\nbetween CSAT and sentiment scores at both the utterance and conversation\nlevel. We then investigate static and temporal modeling methods that\nuse estimated sentiment scores as a mid-level representation. The results\nshow that the sentiment scores, particularly valence and satisfaction,\nare correlated with CSAT. We also demonstrate that our proposed temporal\nmodeling approach for estimating CSAT achieves competitive performance,\nrelative to static baselines as well as human performance. This work\nprovides insights into open domain social conversations between real\nusers and socialbots, and the use of both acoustic and lexical information\nfor understanding the relationship between CSAT and sentiment scores.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2890",
      "author_area_id": "3",
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "lepp20_interspeech": {
      "authors": [
        [
          "Haley",
          "Lepp"
        ],
        [
          "Gina-Anne",
          "Levow"
        ]
      ],
      "title": "Pardon the Interruption: An Analysis of Gender and Turn-Taking in U.S. Supreme Court Oral Arguments",
      "original": "2964",
      "page_count": 5,
      "order": 381,
      "p1": "1838",
      "pn": "1842",
      "abstract": [
        "This study presents a corpus of turn changes between speakers in U.S.\nSupreme Court oral arguments. Each turn change is labeled on a spectrum\nof &#8220;cooperative&#8221; to &#8220;competitive&#8221; by a human\nannotator with legal experience in the United States. We analyze the\nrelationship between speech features, the nature of exchanges, and\nthe gender and legal role of the speakers. Finally, we demonstrate\nthat the models can be used to predict the label of an exchange with\nmoderate success. The automatic classification of the nature of exchanges\nindicates that future studies of turn-taking in oral arguments can\nrely on larger, unlabeled corpora.<SUP>1</SUP>\n"
      ],
      "doi": "10.21437/Interspeech.2020-2964",
      "author_area_id": "3",
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "neitsch20_interspeech": {
      "authors": [
        [
          "Jana",
          "Neitsch"
        ],
        [
          "Oliver",
          "Niebuhr"
        ]
      ],
      "title": "Are Germans Better Haters Than Danes? Language-Specific Implicit Prosodies of Types of Hate Speech and How They Relate to Perceived Severity and Societal Rules",
      "original": "1611",
      "page_count": 5,
      "order": 382,
      "p1": "1843",
      "pn": "1847",
      "abstract": [
        "Hate speech, both written and spoken, is a growing source of concern\nas it often discriminates societal minorities for their national origin,\nsexual orientation, gender or disabilities. Despite its destructive\npower, hardly anything is known about whether there are cross-linguistic\nmechanisms and acoustic-phonetic characteristics of hate speech. For\nthis reason, our experiment analyzes the implicit prosodies that are\ncaused by written Twitter and Facebook hate-speech items and made phonetically\n&#8220;tangible&#8221; through a special, introspective reading-aloud\ntask. We compare the elicited (implicit) prosodies of Danish and German\nspeakers with respect to f0, intensity, HNR, and the Hammarberg index.\nWhile we found no evidence for a consistent hate-speech-specific prosody\neither within or between the two languages, our results show clear\nprosodic differences associated with types of hate speech and their\ntargeted minority groups. Moreover, language-specific differences suggest\nthat &#8212; compared to Danish &#8212; German hate speech sounds more\nexpressive and hateful. Results are discussed regarding their implications\nfor the perceived severity and the automatic flagging and deletion\nof hate-speech posts in social media.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1611",
      "author_area_id": "3",
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "chen20g_interspeech": {
      "authors": [
        [
          "Fuling",
          "Chen"
        ],
        [
          "Roberto",
          "Togneri"
        ],
        [
          "Murray",
          "Maybery"
        ],
        [
          "Diana",
          "Tan"
        ]
      ],
      "title": "An Objective Voice Gender Scoring System and Identification of the Salient Acoustic Measures",
      "original": "1627",
      "page_count": 5,
      "order": 383,
      "p1": "1848",
      "pn": "1852",
      "abstract": [
        "Human voices vary in their perceived masculinity or femininity, and\nsubjective gender scores provided by human raters have long been used\nin psychological studies to understand the complex psychosocial relationships\nbetween people. However, there has been limited research on developing\nobjective gender scoring of voices and examining the correlation between\nobjective gender scores (including the weighting of each acoustic factor)\nand subjective gender scores (i.e., perceived masculinity/ femininity).\nIn this work we propose a gender scoring model based on Linear Discriminant\nAnalysis (LDA) and using weakly labelled data to objectively rate speakers&#8217;\nmasculinity and femininity. For 434 speakers, we investigated 29 acoustic\nmeasures of voice characteristics and their relationships to both the\nobjective scores and subjective masculinity/femininity ratings. The\nresults revealed close correspondence between objective scores and\nsubjective ratings of masculinity for males and femininity for females\n(correlations of 0.667 and 0.505 respectively). Among the 29 measures,\nF0 was found to be the most important vocal characteristic influencing\nboth objective and subjective ratings for both sexes. For female voices,\nlocal absolute jitter and Harmonic-to-Noise Ratio (HNR) were moderately\nassociated with objective scores. For male voices, F0 variance influenced\nobjective gender scores more than the subjective ratings provided by\nhuman listeners.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1627",
      "author_area_id": "3",
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "jayawardena20_interspeech": {
      "authors": [
        [
          "Sadari",
          "Jayawardena"
        ],
        [
          "Julien",
          "Epps"
        ],
        [
          "Zhaocheng",
          "Huang"
        ]
      ],
      "title": "How Ordinal Are Your Data?",
      "original": "2030",
      "page_count": 5,
      "order": 384,
      "p1": "1853",
      "pn": "1857",
      "abstract": [
        "Many affective computing datasets are annotated using ordinal scales,\nas are many other forms of ground truth involving subjectivity, e.g.\ndepression severity. When investigating these datasets, the speech\nprocessing community has chosen classification problems in some cases,\nand regression in others, while ordinal regression may also arguably\nbe the correct approach for some. However, there is currently essentially\nno guidance on selecting a suitable machine learning and evaluation\nmethod. To investigate this problem, this paper proposes a neural network-based\nframework which can transition between different modelling methods\nwith the help of a novel multi-term loss function. Experiments on synthetic\ndatasets show that the proposed framework is empirically well-behaved\nand able to correctly identify classification-like, ordinal regression-like\nand regression-like properties within multidimensional datasets. Application\nof the proposed framework to six real datasets widely used in affective\ncomputing and related fields suggests that more focus should be placed\non ordinal regression instead of classifying or predicting, which are\nthe common practices to date.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2030",
      "author_area_id": "3",
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "hughes20_interspeech": {
      "authors": [
        [
          "Vincent",
          "Hughes"
        ],
        [
          "Frantz",
          "Clermont"
        ],
        [
          "Philip",
          "Harrison"
        ]
      ],
      "title": "Correlating Cepstra with Formant Frequencies: Implications for Phonetically-Informed Forensic Voice Comparison",
      "original": "2216",
      "page_count": 5,
      "order": 385,
      "p1": "1858",
      "pn": "1862",
      "abstract": [
        "A significant question for forensic voice comparison, and for speaker\nrecognition more generally, is the extent to which different input\nfeatures capture complementary speaker-specific information. Understanding\ncomplementarity allows us to make predictions about how combining methods\nusing different features may produce better overall performance. In\nforensic contexts, it is also important to be able to explain to courts\nwhat information the underlying features are actually capturing. This\npaper addresses these issues by examining the extent to which MFCCs\nand LPCCs can predict F0, F1, F2, and F3 values using data extracted\nfrom the midpoint of the vocalic portion of the hesitation marker \num for 89 speakers of standard southern British English. By-speaker\ncorrelations were calculated using multiple linear regression and performance\nwas assessed using mean rho (&#961;) values. Results show that the\nfirst two formants were more accurately predicted than F3 or F0. LPCCs\nconsistently produced stronger correlations with the linguistic features\nthan MFCCs, while increasing cepstral order up to 16 also increased\nthe strength of the correlations. There was, however, considerable\nvariability across speakers in terms of the accuracy of the predictions.\nWe discuss the implications of these findings for forensic voice comparison.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2216",
      "author_area_id": "2",
      "author_area_label": "Phonetics, Phonology, and Prosody"
    },
    "neitsch20b_interspeech": {
      "authors": [
        [
          "Jana",
          "Neitsch"
        ],
        [
          "Plinio A.",
          "Barbosa"
        ],
        [
          "Oliver",
          "Niebuhr"
        ]
      ],
      "title": "Prosody and Breathing: A Comparison Between Rhetorical and Information-Seeking Questions in German and Brazilian Portuguese",
      "original": "1607",
      "page_count": 5,
      "order": 386,
      "p1": "1863",
      "pn": "1867",
      "abstract": [
        "Several studies have shown that rhetorical  wh-questions (RQs) and\nstring-identical information-seeking  wh-questions (ISQs) are realized\nwith different prosodic characteristics. In contrast to ISQs, RQs have\nbeen shown to be phonetically realized with a breathier (i.e., softer)\nvoice quality (e.g., German and English) and longer constituent durations\n(e.g., German, English, Icelandic). Based on similar results found\nfor different languages, we investigate  wh-RQs and sting-identical\n wh-ISQs in Brazilian Portuguese (BP) and German (G). We analyze (i)\nwhether specific duration and voice-quality patterns characterize and\nseparate the two illocution types (RQ and ISQ) in BP, and (ii) if direct\nmeasures of the respiratory sub-system reveal differences between illocution\ntypes, given that breathiness involves greater transglottal air flow\nwhich can be observed in the speakers&#8217; chest and/or abdomen movement.<br\nstyle=\"mso-data-placement:same-cell;\" /><br style=\"mso-data-placement:same-cell;\"\n/>\n  Our data suggest that, similar to G, English, and Icelandic, duration\nand voice quality patterns play a role in the realization of RQs compared\nto ISQs in BP, reinforcing the assumption that there are cross-linguistically\nsimilar phonetic features in the realization of RQs compared to ISQs.\nWe also find that speakers of G breathe in more deeply and dynamically\nthan speakers of BP, suggesting a link between breathing and voice\nquality.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1607",
      "author_area_id": "2",
      "author_area_label": "Phonetics, Phonology, and Prosody"
    },
    "defina20_interspeech": {
      "authors": [
        [
          "Rebecca",
          "Defina"
        ],
        [
          "Catalina",
          "Torres"
        ],
        [
          "Hywel",
          "Stoakes"
        ]
      ],
      "title": "Scaling Processes of Clause Chains in Pitjantjatjara",
      "original": "2101",
      "page_count": 5,
      "order": 387,
      "p1": "1868",
      "pn": "1872",
      "abstract": [
        "Clause chains are a syntactic strategy for combining multiple clauses\ninto a single unit. They are reported in many languages, including\nKorean and Turkish. However, they have seen relatively little focused\nresearch. In particular, prosodic features are often mentioned in descriptions\nof clause chaining, however there have been vanishingly few investigations.\nCorpus-based studies of the prosody of clause chains in two unrelated\nlanguages of Papua New Guinea report that they are typically produced\nas a sequence of Intonation phrases united by pitch-scaling of the\nL% boundary tones in each clause with only the final, finite, clause\ndescending to a full L%. The present study is the first experimental\ninvestigation of the prosody of clause chains in Pitjantjatjara.<br\nstyle=\"mso-data-placement:same-cell;\" /><br style=\"mso-data-placement:same-cell;\"\n/>\n  This paper focuses on one type of clause chain found in the Australian\nIndigenous language Pitjantjatjara. We examine a set of 120 clause\nchains read out by three native Pitjantjatjara speakers. Prosodic analysis\nreveals that these Pitjantjatjara clause chains are produced within\na single Intonational Phrase. Speakers do not pause between the clauses\nin the chain, there is consistent linear downstep throughout the phrase\nand additionally phrase final lowering occurs at the end of the utterance.\nThis differs from previous impressionistic studies of the prosody of\nclause chains.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2101",
      "author_area_id": "2",
      "author_area_label": "Phonetics, Phonology, and Prosody"
    },
    "mizoguchi20_interspeech": {
      "authors": [
        [
          "Ai",
          "Mizoguchi"
        ],
        [
          "Ayako",
          "Hashimoto"
        ],
        [
          "Sanae",
          "Matsui"
        ],
        [
          "Setsuko",
          "Imatomi"
        ],
        [
          "Ryunosuke",
          "Kobayashi"
        ],
        [
          "Mafuyu",
          "Kitahara"
        ]
      ],
      "title": "Neutralization of Voicing Distinction of Stops in Tohoku Dialects of Japanese: Field Work and Acoustic Measurements",
      "original": "3191",
      "page_count": 5,
      "order": 388,
      "p1": "1873",
      "pn": "1877",
      "abstract": [
        "Research on Tohoku dialects, which is a variety of Japanese, has found\nthat the voiceless stops /k/ and /t/ in the intervocalic position are\nfrequently realized as voiced stops. However, the phenomenon has mainly\nbeen judged aurally in the Japanese linguistics literature and has\nnot been confirmed by acoustic measurements. We measured the VOT of\ndata originally collected in the survey of Tohoku dialects by [1].\nThe data used in this study includes two age groups from eight sites.\nThe results demonstrate that for word medial stops, the VOT distribution\nof voiced and voiceless stops largely overlapped, while, the laryngeal\ncontrast was maintained for the word initial stops. Intervocalic voicing\nneutralization was confirmed by quantitative acoustic measurements.\nThe effects of neighboring vowels were also investigated to show that\nheight, but not duration, had a significant effect on voicing neutralization.\nOur results shed light on the phonetic nature of Tohoku dialects as\nwell as on their phonological structure, such as the role of voicing\ncontrast.\n"
      ],
      "doi": "10.21437/Interspeech.2020-3191"
    },
    "lee20b_interspeech": {
      "authors": [
        [
          "Lou",
          "Lee"
        ],
        [
          "Denis",
          "Jouvet"
        ],
        [
          "Katarina",
          "Bartkova"
        ],
        [
          "Yvon",
          "Keromnes"
        ],
        [
          "Mathilde",
          "Dargnat"
        ]
      ],
      "title": "Correlation Between Prosody and Pragmatics: Case Study of Discourse Markers in French and English",
      "original": "2204",
      "page_count": 5,
      "order": 389,
      "p1": "1878",
      "pn": "1882",
      "abstract": [
        "This paper investigates the prosodic characteristics of French and\nEnglish discourse markers according to their pragmatic meaning in context.\nThe study focusses on three French discourse markers ( alors [&#8216;so&#8217;],\n bon [&#8216;well&#8217;], and  donc [&#8216;so&#8217;]) and three\nEnglish markers ( now, so, and  well). Hundreds of occurrences of discourse\nmarkers were automatically extracted from French and English speech\ncorpora and manually annotated with pragmatic functions labels. The\npaper compares the prosodic characteristics of discourse markers in\ndifferent speech styles and in two languages. The first comparison\nis carried out with respect to two different speech styles in French:\nspontaneous speech vs. prepared speech. The other comparison of the\nprosodic characteristics is conducted between two languages, French\nvs. English, on the prepared speech. Results show that some pragmatic\nfunctions of discourse markers bring about specific prosodic behaviour\nin terms of presence and position of pauses, and their F0 articulation\nin their immediate context. Moreover, similar pragmatic functions frequently\nshare similar prosodic characteristics, even across languages.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2204",
      "author_area_id": "2",
      "author_area_label": "Phonetics, Phonology, and Prosody"
    },
    "zarka20_interspeech": {
      "authors": [
        [
          "Dina El",
          "Zarka"
        ],
        [
          "Anneliese",
          "Kelterer"
        ],
        [
          "Barbara",
          "Schuppler"
        ]
      ],
      "title": "An Analysis of Prosodic Prominence Cues to Information Structure in Egyptian Arabic",
      "original": "2322",
      "page_count": 5,
      "order": 390,
      "p1": "1883",
      "pn": "1887",
      "abstract": [
        "This study presents the first acoustic examination of prominence relations\nin entire contours associated with different information structures\nin Egyptian Arabic. Previous work has shown that topics and foci are\ntypically associated with different pitch events, whereas it is still\na matter of debate whether and how Egyptian Arabic uses prominence\nrelations to mark narrow focus. The analysis of data from 17 native\nspeakers showed that narrow focus was marked by on-focus pitch expansion\nas well as post-focus compression. Post-focus compression was realized\nas a large downstep after focus, compressed pitch range, lower intensity\nand shorter duration. The results also showed further register lowering\nafter a contrastive focus, but no further pitch boost of the focused\nword. By contrast, a contrastive topic showed higher scaling of the\ntopic as well as an expanded pitch range of the overall contour. The\nfindings of this study stress the significance of whole contours to\nconvey intonational meanings, revealing gradient prominence cues to\nfocus across the utterance, specifically post-focus register lowering\nto enhance the prominence of a contrastive focus.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2322",
      "author_area_id": "2",
      "author_area_label": "Phonetics, Phonology, and Prosody"
    },
    "mumtaz20_interspeech": {
      "authors": [
        [
          "Benazir",
          "Mumtaz"
        ],
        [
          "Tina",
          "B\u00f6gel"
        ],
        [
          "Miriam",
          "Butt"
        ]
      ],
      "title": "Lexical Stress in Urdu",
      "original": "2942",
      "page_count": 5,
      "order": 391,
      "p1": "1888",
      "pn": "1892",
      "abstract": [
        "This study looks at the role of lexical stress in Urdu prosody. The\nliterature on lexical stress is divided, with some authors developing\nalgorithms for stress assignment, while others deny its relevance for\nprosody. We performed three experiments to investigate this issue.\nWe found evidence that a strong increase in the duration of a syllable\nindicates stress and that lexical stress and phrasal intonation interact\nin a non-trivial manner. We also found that stress perception varies\naccording to syllable weight with weight clash being a determining\nfactor.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2942",
      "author_area_id": "2",
      "author_area_label": "Phonetics, Phonology, and Prosody"
    },
    "riad20_interspeech": {
      "authors": [
        [
          "Rachid",
          "Riad"
        ],
        [
          "Hadrien",
          "Titeux"
        ],
        [
          "Laurie",
          "Lemoine"
        ],
        [
          "Justine",
          "Montillot"
        ],
        [
          "Jennifer Hamet",
          "Bagnou"
        ],
        [
          "Xuan-Nga",
          "Cao"
        ],
        [
          "Emmanuel",
          "Dupoux"
        ],
        [
          "Anne-Catherine",
          "Bachoud-L\u00e9vi"
        ]
      ],
      "title": "Vocal Markers from Sustained Phonation in Huntington&#8217;s Disease",
      "original": "1057",
      "page_count": 5,
      "order": 392,
      "p1": "1893",
      "pn": "1897",
      "abstract": [
        "Disease-modifying treatments are currently assessed in neurodegenerative\ndiseases. Huntington&#8217;s Disease represents a unique opportunity\nto design automatic sub-clinical markers, even in premanifest gene\ncarriers. We investigated phonatory impairments as potential clinical\nmarkers and propose them for both diagnosis and gene carriers follow-up.\nWe used two sets of features: Phonatory features and Modulation Power\nSpectrum Features. We found that phonation is not sufficient for the\nidentification of sub-clinical disorders of premanifest gene carriers.\nAccording to our regression results, Phonatory features are suitable\nfor the predictions of clinical performance in Huntington&#8217;s Disease.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1057",
      "author_area_id": "2",
      "author_area_label": "Phonetics, Phonology, and Prosody"
    },
    "dentel20_interspeech": {
      "authors": [
        [
          "Laure",
          "Dentel"
        ],
        [
          "Julien",
          "Meyer"
        ]
      ],
      "title": "How Rhythm and Timbre Encode Moor&#233; Language in Bendr&#233; Drummed Speech",
      "original": "2532",
      "page_count": 5,
      "order": 393,
      "p1": "1898",
      "pn": "1902",
      "abstract": [
        "Human languages have the flexibility to be acoustically adapted to\nthe context of communication, such as in shouting or whispering. Drummed\nforms of languages represent one of the most extreme natural expressions\nof such speech adaptability. A large amount of research has been conducted\non drummed languages in anthropology or linguistics, particularly in\nWest African societies. However, in spite of the clearly rhythmic nature\nof drumming, previous studies have largely neglected exploring systematically\nthe role of speech rhythm. Here, we explore a unique corpus of the\nBendr&#233; drummed speech form of the Mossi people, transcribed published\nin the 80&#8217;s by the anthropologist Kawada Junzo. The analysis\nof this large database in Moor&#233; language reveals that the rhythmic\nunits encoded in the length of pauses between drumbeats match more\nclosely with vowel-to-vowel intervals than with syllable parsing. Meanwhile,\nwe confirm for the first time a result found recently on the drummed\nspeech tradition of the Bora Amazonian language. However, the complex\nacoustic structure of the Bendr&#233; skin drum required much more\nattention than the simple two pitch hollow log drum of the Bora. Thus,\nwe also present here results on how drummed Bendr&#233; timbre encodes\ntones of Moor&#233; language.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2532",
      "author_area_id": "2",
      "author_area_label": "Phonetics, Phonology, and Prosody"
    },
    "lalhminghlui20_interspeech": {
      "authors": [
        [
          "Wendy",
          "Lalhminghlui"
        ],
        [
          "Priyankoo",
          "Sarmah"
        ]
      ],
      "title": "Interaction of Tone and Voicing in Mizo",
      "original": "2695",
      "page_count": 5,
      "order": 395,
      "p1": "1903",
      "pn": "1907",
      "abstract": [
        "Since the production of fundamental frequency and voicing is determined\nby the tension in the vocal folds, it is noticed that VOT is affected\nby the F0 in tone languages. Similarly laryngeal contrasts also affect\nthe F0 of tone. This work studies the interaction between tone and\nvoicing in a lesser-known tone language, Mizo. Mizo has eight stops,\nthat can be categorized into three laryngeal contrasts namely, voiced,\nvoiceless unaspirated, and voiceless aspirated. In the current work,\nwe look into CV syllables produced with the eight Mizo stops with all\nfive vowel categories of the language, produced with four distinct\ntones in Mizo. The results show a predictable effect of onsets on the\nF0 of tone and weak effect of tone on VOT duration.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2695",
      "author_area_id": "2",
      "author_area_label": "Phonetics, Phonology, and Prosody"
    },
    "wu20h_interspeech": {
      "authors": [
        [
          "Yaru",
          "Wu"
        ],
        [
          "Martine",
          "Adda-Decker"
        ],
        [
          "Lori",
          "Lamel"
        ]
      ],
      "title": "Mandarin Lexical Tones: A Corpus-Based Study of Word Length, Syllable Position and Prosodic Position on Duration",
      "original": "1614",
      "page_count": 5,
      "order": 396,
      "p1": "1908",
      "pn": "1912",
      "abstract": [
        "The present study aims to increase our knowledge of Mandarin lexical\ntones in fluent speech, more specifically their occurrence frequency\ndistributions and their duration patterns. First, the occurrence frequency\nof each lexical tone was computed in a large speech corpus (&#126;220\nhours). Then the duration of each lexical tone, as well as the impact\nof word length, syllable position and the prosodic position were investigated.\nOverall, results show that Tone 3 tends to have the longest duration\namong all lexical tones. Nonetheless, the factors word length, syllable\nposition and prosodic position are found to impact tone duration. Monosyllabic\nwords exhibit tone durations closer to those of word-final syllables\n(especially for disyllabic words) than to other syllable positions.\nMoreover, tone duration tends to be the longest at word&#8217;s right\nboundary in Mandarin, regardless of word length. An effect of prosodic\nposition is also found: the duration of Mandarin lexical tones tends\nto increase with higher prosodic level. Tone durations are the longest\nin phrase-final position, followed by word-final position and word-medial\nposition, regardless of the tone nature.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1614"
    },
    "gao20c_interspeech": {
      "authors": [
        [
          "Yingming",
          "Gao"
        ],
        [
          "Xinyu",
          "Zhang"
        ],
        [
          "Yi",
          "Xu"
        ],
        [
          "Jinsong",
          "Zhang"
        ],
        [
          "Peter",
          "Birkholz"
        ]
      ],
      "title": "An Investigation of the Target Approximation Model for Tone Modeling and Recognition in Continuous Mandarin Speech",
      "original": "2823",
      "page_count": 5,
      "order": 397,
      "p1": "1913",
      "pn": "1917",
      "abstract": [
        "The complex f<SUB>0</SUB> variations in continuous speech make it rather\ndifficult to perform automatic recognition of tones in a language like\nMandarin Chinese. In this study, we tested the use of target approximation\nmodel (TAM) for continuous tone recognition on two datasets. TAM simulates\nf<SUB>0</SUB> production from the articulatory point of view and so\nallow to discover the underlying pitch targets from the surface f<SUB>0</SUB>\ncontour. The f<SUB>0</SUB> contour of each tone represented by 30 equidistant\npoints in the first dataset was simulated by the TAM model. Using a\nsupport vector machine (SVM) to classify tones showed that, compared\nto the representation by 30 f<SUB>0</SUB> values, the estimated three-dimensional\nTAM parameters had a comparable performance in characterizing tone\npatterns. The TAM model was further tested on the second dataset containing\nmore complex tonal variations. With equal or a fewer number of features,\nthe TAM parameters provided better performance than the coefficients\nof the cosine transform and a slightly worse performance than the statistical\nf<SUB>0</SUB> parameters for tone recognition. Furthermore, we investigated\nbidirectional LSTM neural network for modelling the sequential tonal\nvariations, which proved to be more powerful than the SVM classifier.\nThe BLSTM system incorporating TAM and statistical f<SUB>0</SUB> parameters\nachieved the best accuracy of 87.56%.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2823",
      "author_area_id": "2",
      "author_area_label": "Phonetics, Phonology, and Prosody"
    },
    "lai20_interspeech": {
      "authors": [
        [
          "Wei",
          "Lai"
        ],
        [
          "Aini",
          "Li"
        ]
      ],
      "title": "Integrating the Application and Realization of Mandarin 3rd Tone Sandhi in the Resolution of Sentence Ambiguity",
      "original": "2073",
      "page_count": 5,
      "order": 398,
      "p1": "1918",
      "pn": "1922",
      "abstract": [
        "Chinese third tone sandhi (T3S) covaries with the prosodic hierarchy\nboth in the probability of application and in the realization of pitch\nslope. This paper evaluates whether Mandarin-speaking listeners integrate\nthe covariation between T3S and prosody to resolute sentence ambiguity.\nTwenty-seven structurally ambiguous sentences were designed, each containing\ntwo consecutive T3 syllables situated across a word boundary, and the\nstrength of the T3-intervening boundary crucially differentiates different\ninterpretations of the sentence. The first T3 was manipulated to bear\neither a low, a shallow-rising, or a sharp-rising pitch. Sixty native\nMandarin-speaking listeners heard each of these sentences and chose\nfrom two written interpretations the one that was consistent with what\nthey heard. The results show that listeners are more likely to report\na major-juncture interpretation when T3S does not apply (low) than\nwhen it applies (rising), and in the latter case, when the T3S variant\nhas a sharper rather than shallower slope. Post-hoc analyses show that\nthe T3S application is a more robust parsing cue for short sentences\n(4&#8211;5 syllables long), whereas the pitch shape of T3S is a more\nefficient parsing cue for longer sentences, indicating that listeners\nmake sophisticated use of tonal variation to facilitate sentence processing.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2073",
      "author_area_id": "2",
      "author_area_label": "Phonetics, Phonology, and Prosody"
    },
    "zhang20p_interspeech": {
      "authors": [
        [
          "Zhenrui",
          "Zhang"
        ],
        [
          "Fang",
          "Hu"
        ]
      ],
      "title": "Neutral Tone in Changde Mandarin",
      "original": "1257",
      "page_count": 5,
      "order": 399,
      "p1": "1923",
      "pn": "1927",
      "abstract": [
        "This paper discusses the phonetics and phonology of tones in Changde\nMandarin and focuses on neutral tone in disyllabic words. Acoustic\nrealizations of both citation and neutral tones are examined in terms\nof fundamental frequencies (F<SUB>0</SUB>), duration, and intensity.\nAnd phonetic and phonological descriptions are given on the basis of\nacoustic data. Acoustic data from 12 speakers show that Changde Mandarin\nhas four lexical tones that distinguish in contour, namely level versus\nrising, and pitch height, namely high versus low. Neutral tone in Changde\nMandarin is different from that in Beijing Mandarin or Standard Chinese.\nNeutral tone in Changde Mandarin is a reduced form of its citation\ntone, which is produced with a neutralized pitch height, and a significantly\nshorter duration and weaker intensity than the control.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1257",
      "author_area_id": "2",
      "author_area_label": "Phonetics, Phonology, and Prosody"
    },
    "cui20_interspeech": {
      "authors": [
        [
          "Ping",
          "Cui"
        ],
        [
          "Jianjing",
          "Kuang"
        ]
      ],
      "title": "Pitch Declination and Final Lowering in Northeastern Mandarin",
      "original": "1987",
      "page_count": 5,
      "order": 400,
      "p1": "1928",
      "pn": "1932",
      "abstract": [
        "Northeastern Mandarin has a similar lexical tone system as Beijing\nMandarin. However, the two dialects significantly diverge at higher\nprosodic structures. T1 in Northeastern Mandarin always changes to\na falling tone in domain-final positions. Previous studies have analyzed\nthis variation as a type of tone sandhi, but we propose it is related\nto more global prosodic processes such as final lowering. We addressed\nthis issue by conducting both production and perception experiments\nwith native bidialectal speakers of Northeastern Mandarin and Beijing\nMandarin. Our findings suggest that T1 variation is essentially a domain-final\nlowering effect. Other tones also show some kind of final lowering\neffects. Compared to Beijing Mandarin, Northeastern Mandarin generally\nhas greater global pitch declination and greater final lowering effects.\nOur perception experiment further showed that both prosodic effects\nplay important roles in identifying the Northeastern Mandarin accent,\nand final lowering cues are more perceptually salient than the global\ndeclination cues. These findings support the notion that pitch declination\nand final lowering effects are linguistically controlled, not just\na by-product of the physiological mechanisms.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1987",
      "author_area_id": "2",
      "author_area_label": "Phonetics, Phonology, and Prosody"
    },
    "rose20_interspeech": {
      "authors": [
        [
          "Phil",
          "Rose"
        ]
      ],
      "title": "Variation in Spectral Slope and Interharmonic Noise in Cantonese Tones",
      "original": "1954",
      "page_count": 5,
      "order": 401,
      "p1": "1933",
      "pn": "1937",
      "abstract": [
        "To provide reference data for studies of voice quality variation in\nlexical tone, an experiment is described to investigate the nature\nof intrinsic variation in spectral slope and interharmonic noise for\nCantonese citation tones. 23 spectral slope and interharmonic noise\nmeasures are extracted with  VoiceSauce from the tones on /o/ Rhymes\nof five male and five female speakers of conservative Cantonese. Significant\ncorrelation between F0 and both spectral slope and interharmonic noise\nis demonstrated. It is shown with probabilistic bivariate discriminant\nanalysis that even tones with no extrinsic voice quality differences\ncan be identified at rates considerably above chance from a combination\nof their spectral slope and interharmonic noise. Male tones, with a\nminimal error rate of 5.7%, are identified twice as well as female,\nwith a minimal error rate of 14.5%. Combinations with uncorrected spectral\nslopes perform better than corrected. The best combinations for both\nsexes involve slope parameters  H2H4 (difference between the 4<SUP>th</SUP>\nand 2<SUP>nd</SUP> harmonic amplitudes); and  H42K (difference between\nthe 4<SUP>th</SUP> harmonic and nearest harmonic to 2 kHz), irrespective\nof noise parameters. The worst combinations involve  CPP (cepstral\npeak prominence) as a noise parameter.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1954",
      "author_area_id": "2",
      "author_area_label": "Phonetics, Phonology, and Prosody"
    },
    "tang20_interspeech": {
      "authors": [
        [
          "Ping",
          "Tang"
        ],
        [
          "Shanpeng",
          "Li"
        ]
      ],
      "title": "The Acoustic Realization of Mandarin Tones in Fast Speech",
      "original": "1274",
      "page_count": 4,
      "order": 402,
      "p1": "1938",
      "pn": "1941",
      "abstract": [
        "Many studies have demonstrated that acoustic contrasts between speech\nsegments (vowels and consonants) were reduced when speaking rate increases,\nwhile it was unclear whether tones in tonal languages also undergo\nsimilar modifications. Mandarin Chinese is a tonal language, while\nresults regarding the rate effect on Mandarin tones in previous studies\nwere mixed, probably driven by the material difference, i.e., the position\nof target tones within a sentence. Therefore, the present study examined\nthe effect of speaking rate on Mandarin tones, comparing the pitch\ncontour and tonal contrast of Mandarin tones between normal and fast\nspeech across utterance initial, medial and final positions. The results\nshowed that, relative to normal speech, lexical tones in Mandarin Chinese\nexhibited overall higher and flatter pitch contours, with smaller tonal\nspace. Moreover, the rate effect on tones did not vary with position.\nThe current results and previous studies on segments thus revealed\na universal pattern of speech reduction in fast speech at both segmental\nand suprasegmental levels.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1274",
      "author_area_id": "2",
      "author_area_label": "Phonetics, Phonology, and Prosody"
    },
    "loukina20_interspeech": {
      "authors": [
        [
          "Anastassia",
          "Loukina"
        ],
        [
          "Keelan",
          "Evanini"
        ],
        [
          "Matthew",
          "Mulholland"
        ],
        [
          "Ian",
          "Blood"
        ],
        [
          "Klaus",
          "Zechner"
        ]
      ],
      "title": "Do Face Masks Introduce Bias in Speech Technologies? The Case of Automated Scoring of Speaking Proficiency",
      "original": "1264",
      "page_count": 5,
      "order": 403,
      "p1": "1942",
      "pn": "1946",
      "abstract": [
        "The COVID-19 pandemic has led to a dramatic increase in the use of\nface masks worldwide. Face coverings can affect both acoustic properties\nof the signal as well as speech patterns and have unintended effects\nif the person wearing the mask attempts to use speech processing technologies.\nIn this paper we explore the impact of wearing face masks on the automated\nassessment of English language proficiency. We use a dataset from a\nlarge-scale speaking test for which test-takers were required to wear\nface masks during the test administration, and we compare it to a matched\ncontrol sample of test-takers who took the same test before the mask\nrequirements were put in place. We find that the two samples differ\nacross a range of acoustic measures and also show a small but significant\ndifference in speech patterns. However, these differences do not lead\nto differences in human or automated scores of English language proficiency.\nSeveral measures of bias showed no differences in scores between the\ntwo groups.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1264",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "mhiri20_interspeech": {
      "authors": [
        [
          "Mohamed",
          "Mhiri"
        ],
        [
          "Samuel",
          "Myer"
        ],
        [
          "Vikrant Singh",
          "Tomar"
        ]
      ],
      "title": "A Low Latency ASR-Free End to End Spoken Language Understanding System",
      "original": "1449",
      "page_count": 5,
      "order": 404,
      "p1": "1947",
      "pn": "1951",
      "abstract": [
        "In recent years, developing a speech understanding system that classifies\na waveform to structured data, such as intents and slots, without first\ntranscribing the speech to text has emerged as an interesting research\nproblem. This work proposes such as system with an additional constraint\nof designing a system that has a small enough footprint to run on small\nmicro-controllers and embedded systems with minimal latency. Given\na streaming input speech signal, the proposed system can process it\nsegment-by-segment without the need to have the entire stream at the\nmoment of processing. The proposed system is evaluated on the publicly\navailable Fluent Speech Commands dataset. Experiments show that the\nproposed system yields state-of-the-art performance with the advantage\nof low latency and a much smaller model when compared to other published\nworks on the same task.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1449",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "wang20s_interspeech": {
      "authors": [
        [
          "Joe",
          "Wang"
        ],
        [
          "Rajath",
          "Kumar"
        ],
        [
          "Mike",
          "Rodehorst"
        ],
        [
          "Brian",
          "Kulis"
        ],
        [
          "Shiv Naga Prasad",
          "Vitaladevuni"
        ]
      ],
      "title": "An Audio-Based Wakeword-Independent Verification System",
      "original": "1843",
      "page_count": 5,
      "order": 405,
      "p1": "1952",
      "pn": "1956",
      "abstract": [
        "We propose an audio-based wakeword-independent verification model to\ndetermine whether a wakeword spotting model correctly woke and should\nrespond or incorrectly woke and should not respond. Our model works\non any wakeword-initiated audio, independent of the wakeword by operating\nonly on the audio surrounding the wakeword, yielding a wakeword agnostic\nmodel. This model is based on two key assumptions: that audio surrounding\nthe wakeword is informative to determine if the user intended to wake\nthe device and that this audio is independent of the wakeword itself.\nWe show experimentally that on wakewords not included in the training\nset, our model trained without examples or knowledge of the wakeword\nis able to achieve verification performance comparable to models trained\non 5,000 to 10,000 annotated examples of the new wakeword.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1843",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "vuong20_interspeech": {
      "authors": [
        [
          "Tyler",
          "Vuong"
        ],
        [
          "Yangyang",
          "Xia"
        ],
        [
          "Richard M.",
          "Stern"
        ]
      ],
      "title": "Learnable Spectro-Temporal Receptive Fields for Robust Voice Type Discrimination",
      "original": "1878",
      "page_count": 5,
      "order": 406,
      "p1": "1957",
      "pn": "1961",
      "abstract": [
        "Voice Type Discrimination (VTD) refers to discrimination between regions\nin a recording where speech was produced by speakers that are physically\nwithin proximity of the recording device (&#8220;Live Speech&#8221;)\nfrom speech and other types of audio that were played back such as\ntraffic noise and television broadcasts (&#8220;Distractor Audio&#8221;).\nIn this work, we propose a deep-learning-based VTD system that features\nan initial layer of learnable spectro-temporal receptive fields (STRFs).\nOur approach is also shown to provide very strong performance on a\nsimilar spoofing detection task in the ASVspoof 2019 challenge. We\nevaluate our approach on a new standardized VTD database that was collected\nto support research in this area. In particular, we study the effect\nof using learnable STRFs compared to static STRFs or unconstrained\nkernels. We also show that our system consistently improves a competitive\nbaseline system across a wide range of signal-to-noise ratios on spoofing\ndetection in the presence of VTD distractor noise.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1878",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "chang20b_interspeech": {
      "authors": [
        [
          "Shuo-Yiin",
          "Chang"
        ],
        [
          "Bo",
          "Li"
        ],
        [
          "David",
          "Rybach"
        ],
        [
          "Yanzhang",
          "He"
        ],
        [
          "Wei",
          "Li"
        ],
        [
          "Tara N.",
          "Sainath"
        ],
        [
          "Trevor",
          "Strohman"
        ]
      ],
      "title": "Low Latency Speech Recognition Using End-to-End Prefetching",
      "original": "1898",
      "page_count": 5,
      "order": 407,
      "p1": "1962",
      "pn": "1966",
      "abstract": [
        "Latency is a crucial metric for streaming speech recognition systems.\nIn this paper, we reduce latency by fetching responses early based\non the  partial recognition results and refer to it as  prefetching.\nSpecifically, prefetching works by submitting partial recognition results\nfor subsequent processing such as obtaining assistant server responses\nor second-pass rescoring  before the recognition result is finalized.\nIf the partial result matches the final recognition result, the early\nfetched response can be delivered to the user instantly. This effectively\nspeeds up the system by saving the execution latency that typically\nhappens after recognition is completed.<br style=\"mso-data-placement:same-cell;\"\n/><br style=\"mso-data-placement:same-cell;\" />\n  Prefetching can be\ntriggered multiple times for a single query, but this leads to multiple\nrounds of downstream processing and increases the computation costs.\nIt is hence desirable to fetch the result sooner but meanwhile limiting\nthe number of prefetches. To achieve the best trade-off between latency\nand computation cost, we investigated a series of prefetching decision\nmodels including decoder silence based prefetching, acoustic silence\nbased prefetching and end-to-end prefetching.<br style=\"mso-data-placement:same-cell;\"\n/><br style=\"mso-data-placement:same-cell;\" />\n  In this paper, we\ndemonstrate the proposed prefetching mechanism reduced latency by &#126;200\nms for a system that consists of a streaming first pass model using\nrecurrent neural network transducer and a non-streaming second pass\nrescoring model using Listen, Attend and Spell. We observe that the\nend-to-end prefetching provides the best trade-off between cost and\nlatency and is 120 ms faster compared to silence based prefetching\nat a fixed prefetch rate.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1898",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "wang20t_interspeech": {
      "authors": [
        [
          "Jingsong",
          "Wang"
        ],
        [
          "Tom",
          "Ko"
        ],
        [
          "Zhen",
          "Xu"
        ],
        [
          "Xiawei",
          "Guo"
        ],
        [
          "Souxiang",
          "Liu"
        ],
        [
          "Wei-Wei",
          "Tu"
        ],
        [
          "Lei",
          "Xie"
        ]
      ],
      "title": "AutoSpeech 2020: The Second Automated Machine Learning Challenge for Speech Classification",
      "original": "1986",
      "page_count": 5,
      "order": 408,
      "p1": "1967",
      "pn": "1971",
      "abstract": [
        "The AutoSpeech challenge calls for automated machine learning (AutoML)\nsolutions to automate the process of applying machine learning to speech\nprocessing tasks. These tasks, which cover a large variety of domains,\nwill be shown to the automated system in a random order. Each time\nwhen the tasks are switched, the information of the new task will be\nhinted with its corresponding training set. Thus, every submitted solution\nshould contain an adaptation routine which adapts the system to the\nnew task. Compared to the first edition, the 2020 edition includes\nadvances of 1) more speech tasks, 2) noisier data in each task, 3)\na modified evaluation metric. This paper outlines the challenge and\ndescribe the competition protocol, datasets, evaluation metric, starting\nkit, and baseline systems.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1986",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "kumar20c_interspeech": {
      "authors": [
        [
          "Rajath",
          "Kumar"
        ],
        [
          "Mike",
          "Rodehorst"
        ],
        [
          "Joe",
          "Wang"
        ],
        [
          "Jiacheng",
          "Gu"
        ],
        [
          "Brian",
          "Kulis"
        ]
      ],
      "title": "Building a Robust Word-Level Wakeword Verification Network",
      "original": "2018",
      "page_count": 5,
      "order": 409,
      "p1": "1972",
      "pn": "1976",
      "abstract": [
        "Wakeword detection is responsible for switching on downstream systems\nin a voice-activated device. To prevent a response when the wakeword\nis detected by mistake, a secondary network is often utilized to verify\nthe detected wakeword. Published verification approaches are formulated\nbased on Automatic Speech Recognition (ASR) biased towards the wakeword.\nThis approach has several drawbacks, including high model complexity\nand the necessity of large vocabulary training data. To address these\nshortcomings, we propose to use a large receptive field (LRF) word-level\nwakeword model, and in particular, a convolutional-recurrent-attention\n(CRA) network. CRA networks use a strided small receptive field convolutional\nfront-end followed by fixed time-step recurrent layers optimized to\nmodel the temporal phonetic dependencies within the wakeword. We experimentally\nshow that this type of modeling helps the system to be robust to errors\nin the location of the wakeword as estimated by the detection network.\nThe proposed CRA network significantly outperforms previous baselines,\nincluding an LRF whole-word convolutional network and a 2-stage DNN-HMM\nsystem. Additionally, we study the importance of pre- and post-wakeword\ncontext. Finally, the CRA network has significantly fewer model parameters\nand multiplies, which makes it suitable for real-world production applications.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2018",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "koizumi20_interspeech": {
      "authors": [
        [
          "Yuma",
          "Koizumi"
        ],
        [
          "Ryo",
          "Masumura"
        ],
        [
          "Kyosuke",
          "Nishida"
        ],
        [
          "Masahiro",
          "Yasuda"
        ],
        [
          "Shoichiro",
          "Saito"
        ]
      ],
      "title": "A Transformer-Based Audio Captioning Model with Keyword Estimation",
      "original": "2087",
      "page_count": 5,
      "order": 410,
      "p1": "1977",
      "pn": "1981",
      "abstract": [
        "One of the problems with automated audio captioning (AAC) is the indeterminacy\nin word selection corresponding to the audio event/scene. Since one\nacoustic event/scene can be described with several words, it results\nin a combinatorial explosion of possible captions and difficulty in\ntraining. To solve this problem, we propose a Transformer-based audio-captioning\nmodel with keyword estimation called  TRACKE. It simultaneously solves\nthe word-selection indeterminacy problem with the main task of AAC\nwhile executing the sub-task of acoustic event detection/acoustic scene\nclassification (i.e., keyword estimation). TRACKE estimates keywords,\nwhich comprise a word set corresponding to audio events/scenes in the\ninput audio, and generates the caption while referring to the estimated\nkeywords to reduce word-selection indeterminacy. Experimental results\non a public AAC dataset indicate that TRACKE achieved state-of-the-art\nperformance and successfully estimated both the caption and its keywords.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2087",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "mo20_interspeech": {
      "authors": [
        [
          "Tong",
          "Mo"
        ],
        [
          "Yakun",
          "Yu"
        ],
        [
          "Mohammad",
          "Salameh"
        ],
        [
          "Di",
          "Niu"
        ],
        [
          "Shangling",
          "Jui"
        ]
      ],
      "title": "Neural Architecture Search for Keyword Spotting",
      "original": "3132",
      "page_count": 5,
      "order": 411,
      "p1": "1982",
      "pn": "1986",
      "abstract": [
        "Deep neural networks have recently become a popular solution to keyword\nspotting systems, which enable the control of smart devices via voice.\nIn this paper, we apply neural architecture search to search for convolutional\nneural network models that can help boost the performance of keyword\nspotting based on features extracted from acoustic signals while maintaining\nan acceptable memory footprint. Specifically, we use differentiable\narchitecture search techniques to search for operators and their connections\nin a predefined cell search space. The found cells are then scaled\nup in both depth and width to achieve competitive performance. We evaluated\nthe proposed method on Google&#8217;s Speech Commands Dataset and achieved\na state-of-the-art accuracy of over 97% on the setting of 12-class\nutterance classification commonly reported in the literature.\n"
      ],
      "doi": "10.21437/Interspeech.2020-3132",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "li20s_interspeech": {
      "authors": [
        [
          "Ximin",
          "Li"
        ],
        [
          "Xiaodong",
          "Wei"
        ],
        [
          "Xiaowei",
          "Qin"
        ]
      ],
      "title": "Small-Footprint Keyword Spotting with Multi-Scale Temporal Convolution",
      "original": "3177",
      "page_count": 5,
      "order": 412,
      "p1": "1987",
      "pn": "1991",
      "abstract": [
        "Keyword Spotting (KWS) plays a vital role in human-computer interaction\nfor smart on-device terminals and service robots. It remains challenging\nto achieve the trade-off between small footprint and high accuracy\nfor KWS task. In this paper, we explore the application of multi-scale\ntemporal modeling to the small-footprint keyword spotting task. We\npropose a multi-branch temporal convolution module (MTConv), a CNN\nblock consisting of multiple temporal convolution filters with different\nkernel sizes, which enriches temporal feature space. Besides, taking\nadvantage of temporal and depthwise convolution, a temporal efficient\nneural network (TENet) is designed for KWS system<SUP>1</SUP>. Based\non the purposed model, we replace standard temporal convolution layers\nwith MTConvs that can be trained for better performance. While at the\ninference stage, the MTConv can be equivalently converted to the base\nconvolution architecture, so that no extra parameters and computational\ncosts are added compared to the base model. The results on Google Speech\nCommand Dataset show that one of our models trained with MTConv performs\nthe accuracy of 96.8% with only 100K parameters.\n"
      ],
      "doi": "10.21437/Interspeech.2020-3177",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "wang20u_interspeech": {
      "authors": [
        [
          "Xin",
          "Wang"
        ],
        [
          "Junichi",
          "Yamagishi"
        ]
      ],
      "title": "Using Cyclic Noise as the Source Signal for Neural Source-Filter-Based Speech Waveform Model",
      "original": "1018",
      "page_count": 5,
      "order": 413,
      "p1": "1992",
      "pn": "1996",
      "abstract": [
        "Neural source-filter (NSF) waveform models generate speech waveforms\nby morphing sine-based source signals through dilated convolution in\nthe time domain. Although the sine-based source signals help the NSF\nmodels to produce voiced sounds with specified pitch, the sine shape\nmay constrain the generated waveform when the target voiced sounds\nare less periodic. In this paper, we propose a more flexible source\nsignal called cyclic noise, a quasi-periodic noise sequence given by\nthe convolution of a pulse train and a static random noise with a trainable\ndecaying rate that controls the signal shape. We further propose a\nmasked spectral loss to guide the NSF models to produce periodic voiced\nsounds from the cyclic noise-based source signal. Results from a large-scale\nlistening test demonstrated the effectiveness of the cyclic noise and\nthe masked spectral loss on speaker-independent NSF models in copy-synthesis\nexperiments on the CMU ARCTIC database.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1018",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "liu20i_interspeech": {
      "authors": [
        [
          "Jen-Yu",
          "Liu"
        ],
        [
          "Yu-Hua",
          "Chen"
        ],
        [
          "Yin-Cheng",
          "Yeh"
        ],
        [
          "Yi-Hsuan",
          "Yang"
        ]
      ],
      "title": "Unconditional Audio Generation with Generative Adversarial Networks and Cycle Regularization",
      "original": "1137",
      "page_count": 5,
      "order": 414,
      "p1": "1997",
      "pn": "2001",
      "abstract": [
        "In a recent paper, we have presented a generative adversarial network\n(GAN)-based model for unconditional generation of the mel-spectrograms\nof singing voices. As the generator of the model is designed to take\na variable-length sequence of noise vectors as input, it can generate\nmel-spectrograms of variable length. However, our previous listening\ntest shows that the quality of the generated audio leaves room for\nimprovement. The present paper extends and expands that previous work\nin the following aspects. First, we employ a hierarchical architecture\nin the generator to induce some structure in the temporal dimension.\nSecond, we introduce a cycle regularization mechanism to the generator\nto avoid mode collapse. Third, we evaluate the performance of the new\nmodel not only for generating singing voices, but also for generating\nspeech voices. Evaluation result shows that new model outperforms the\nprior one both objectively and subjectively. We also employ the model\nto unconditionally generate sequences of piano and violin music and\nfind the result promising. Audio examples, as well as the code for\nimplementing our model, will be publicly available online upon paper\npublication.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1137",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "nakashika20_interspeech": {
      "authors": [
        [
          "Toru",
          "Nakashika"
        ]
      ],
      "title": "Complex-Valued Variational Autoencoder: A Novel Deep Generative Model for Direct Representation of Complex Spectra",
      "original": "1964",
      "page_count": 5,
      "order": 415,
      "p1": "2002",
      "pn": "2006",
      "abstract": [
        "In recent years, variational autoencoders (VAEs) have been attracting\ninterest for many applications and generative tasks. Although the VAE\nis one of the most powerful deep generative models, it still has difficulty\nrepresenting complex-valued data such as the complex spectra of speech.\nIn speech synthesis, we usually use the VAE to encode Mel-cepstra,\nor raw amplitude spectra, from a speech signal into normally distributed\nlatent features and then synthesize the speech from the reconstruction\nby using the Griffin-Lim algorithm or other vocoders. Such inputs are\noriginally calculated from complex spectra but lack the phase information,\nwhich leads to degradation when recovering speech. In this work, we\npropose a novel generative model to directly encode the complex spectra\nby extending the conventional VAE. The proposed model, which we call\nthe complex-valued VAE (CVAE), consists of two complex-valued neural\nnetworks (CVNNs) of an encoder and a decoder. In the CVAE, not only\nthe inputs and the parameters of the encoder and decoder but also the\nlatent features are defined as complex-valued to preserve the phase\ninformation throughout the network. The results of our speech encoding\nexperiments demonstrated the effectiveness of the CVAE compared to\nthe conventional VAE in both objective and subjective criteria.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1964",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "choi20c_interspeech": {
      "authors": [
        [
          "Seungwoo",
          "Choi"
        ],
        [
          "Seungju",
          "Han"
        ],
        [
          "Dongyoung",
          "Kim"
        ],
        [
          "Sungjoo",
          "Ha"
        ]
      ],
      "title": "Attentron: Few-Shot Text-to-Speech Utilizing Attention-Based Variable-Length Embedding",
      "original": "2096",
      "page_count": 5,
      "order": 416,
      "p1": "2007",
      "pn": "2011",
      "abstract": [
        "On account of growing demands for personalization, the need for a so-called\nfew-shot TTS system that clones speakers with only a few data is emerging.\nTo address this issue, we propose Attentron, a few-shot TTS model that\nclones voices of speakers unseen during training. It introduces two\nspecial encoders, each serving different purposes. A fine-grained encoder\nextracts variable-length style information via an attention mechanism,\nand a coarse-grained encoder greatly stabilizes the speech synthesis,\ncircumventing unintelligible gibberish even for synthesizing speech\nof unseen speakers. In addition, the model can scale out to an arbitrary\nnumber of reference audios to improve the quality of the synthesized\nspeech. According to our experiments, including a human evaluation,\nthe proposed model significantly outperforms state-of-the-art models\nwhen generating speech for unseen speakers in terms of speaker similarity\nand quality.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2096",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "ihm20_interspeech": {
      "authors": [
        [
          "Hyeong Rae",
          "Ihm"
        ],
        [
          "Joun Yeop",
          "Lee"
        ],
        [
          "Byoung Jin",
          "Choi"
        ],
        [
          "Sung Jun",
          "Cheon"
        ],
        [
          "Nam Soo",
          "Kim"
        ]
      ],
      "title": "Reformer-TTS: Neural Speech Synthesis with Reformer Network",
      "original": "2189",
      "page_count": 5,
      "order": 417,
      "p1": "2012",
      "pn": "2016",
      "abstract": [
        "Recent End-to-end text-to-speech (TTS) systems based on the deep neural\nnetwork (DNN) have shown the state-of-the-art performance on the speech\nsynthesis field. Especially, the attention-based sequence-to-sequence\nmodels have improved the quality of the alignment between the text\nand spectrogram successfully. Leveraging such improvement, speech synthesis\nusing a Transformer network was reported to generate humanlike speech\naudio. However, such sequence-to-sequence models require intensive\ncomputing power and memory during training. The attention scores are\ncalculated over the entire key at every query sequence, which increases\nmemory usage. To mitigate this issue, we propose Reformer-TTS, the\nmodel using a Reformer network which utilizes the locality-sensitive\nhashing attention and the reversible residual network. As a result,\nwe show that the Reformer network consumes almost twice smaller memory\nmargin as the Transformer, which leads to the fast convergence of training\nend-to-end TTS system. We demonstrate such advantages with memory usage,\nobjective, and subjective performance evaluation.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2189",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "kaneko20_interspeech": {
      "authors": [
        [
          "Takuhiro",
          "Kaneko"
        ],
        [
          "Hirokazu",
          "Kameoka"
        ],
        [
          "Kou",
          "Tanaka"
        ],
        [
          "Nobukatsu",
          "Hojo"
        ]
      ],
      "title": "CycleGAN-VC3: Examining and Improving CycleGAN-VCs for Mel-Spectrogram Conversion",
      "original": "2280",
      "page_count": 5,
      "order": 418,
      "p1": "2017",
      "pn": "2021",
      "abstract": [
        "Non-parallel voice conversion (VC) is a technique for learning mappings\nbetween source and target speeches without using a parallel corpus.\nRecently, cycle-consistent adversarial network (CycleGAN)-VC and CycleGAN-VC2\nhave shown promising results regarding this problem and have been widely\nused as benchmark methods. However, owing to the ambiguity of the effectiveness\nof CycleGAN-VC/VC2 for mel-spectrogram conversion, they are typically\nused for mel-cepstrum conversion even when comparative methods employ\nmel-spectrogram as a conversion target. To address this, we examined\nthe applicability of CycleGAN-VC/VC2 to mel-spectrogram conversion.\nThrough initial experiments, we discovered that their direct applications\ncompromised the time-frequency structure that should be preserved during\nconversion. To remedy this, we propose CycleGAN-VC3, an improvement\nof CycleGAN-VC2 that incorporates time-frequency adaptive normalization\n(TFAN). Using TFAN, we can adjust the scale and bias of the converted\nfeatures while reflecting the time-frequency structure of the source\nmel-spectrogram. We evaluated CycleGAN-VC3 on inter-gender and intra-gender\nnon-parallel VC. A subjective evaluation of naturalness and similarity\nshowed that for every VC pair, CycleGAN-VC3 outperforms or is competitive\nwith the two types of CycleGAN-VC2, one of which was applied to mel-cepstrum\nand the other to mel-spectrogram.<SUP>1</SUP>\n"
      ],
      "doi": "10.21437/Interspeech.2020-2280",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "ellinas20_interspeech": {
      "authors": [
        [
          "Nikolaos",
          "Ellinas"
        ],
        [
          "Georgios",
          "Vamvoukakis"
        ],
        [
          "Konstantinos",
          "Markopoulos"
        ],
        [
          "Aimilios",
          "Chalamandaris"
        ],
        [
          "Georgia",
          "Maniati"
        ],
        [
          "Panos",
          "Kakoulidis"
        ],
        [
          "Spyros",
          "Raptis"
        ],
        [
          "June Sig",
          "Sung"
        ],
        [
          "Hyoungmin",
          "Park"
        ],
        [
          "Pirros",
          "Tsiakoulis"
        ]
      ],
      "title": "High Quality Streaming Speech Synthesis with Low, Sentence-Length-Independent Latency",
      "original": "2464",
      "page_count": 5,
      "order": 419,
      "p1": "2022",
      "pn": "2026",
      "abstract": [
        "This paper presents an end-to-end text-to-speech system with low latency\non a CPU, suitable for real-time applications. The system is composed\nof an autoregressive attention-based sequence-to-sequence acoustic\nmodel and the LPCNet vocoder for waveform generation. An acoustic model\narchitecture that adopts modules from both the Tacotron 1 and 2 models\nis proposed, while stability is ensured by using a recently proposed\npurely location-based attention mechanism, suitable for arbitrary sentence\nlength generation. During inference, the decoder is unrolled and acoustic\nfeature generation is performed in a streaming manner, allowing for\na nearly constant latency which is independent from the sentence length.\nExperimental results show that the acoustic model can produce feature\nsequences with minimal latency about 31 times faster than real-time\non a computer CPU and 6.5 times on a mobile CPU, enabling it to meet\nthe conditions required for real-time applications on both devices.\nThe full end-to-end system can generate almost natural quality speech,\nwhich is verified by listening tests.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2464",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "yu20c_interspeech": {
      "authors": [
        [
          "Chengzhu",
          "Yu"
        ],
        [
          "Heng",
          "Lu"
        ],
        [
          "Na",
          "Hu"
        ],
        [
          "Meng",
          "Yu"
        ],
        [
          "Chao",
          "Weng"
        ],
        [
          "Kun",
          "Xu"
        ],
        [
          "Peng",
          "Liu"
        ],
        [
          "Deyi",
          "Tuo"
        ],
        [
          "Shiyin",
          "Kang"
        ],
        [
          "Guangzhi",
          "Lei"
        ],
        [
          "Dan",
          "Su"
        ],
        [
          "Dong",
          "Yu"
        ]
      ],
      "title": "DurIAN: Duration Informed Attention Network for Speech Synthesis",
      "original": "2968",
      "page_count": 5,
      "order": 420,
      "p1": "2027",
      "pn": "2031",
      "abstract": [
        "In this paper, we present a robust and effective speech synthesis system\nthat generates highly natural speech. The key component of proposed\nsystem is Duration Informed Attention Network (DurIAN), an autoregressive\nmodel in which the alignments between the input text and the output\nacoustic features are inferred from a duration model. This is different\nfrom the attention mechanism used in existing end-to-end speech synthesis\nsystems that accounts for various unavoidable artifacts. To improve\nthe audio generation efficiency of neural vocoders, we also propose\na multi-band audio generation framework exploiting the sparseness characteristics\nof neural network. With proposed multi-band processing framework, the\ntotal computational complexity of WaveRNN model can be effectively\nreduced from 9.8 to 3.6 GFLOPS without any performance loss. Finally,\nwe show that proposed DurIAN system could generate highly natural speech\nthat is on par with current state of the art end-to-end systems, while\nbeing robust and stable at the same time.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2968",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "mitsui20_interspeech": {
      "authors": [
        [
          "Kentaro",
          "Mitsui"
        ],
        [
          "Tomoki",
          "Koriyama"
        ],
        [
          "Hiroshi",
          "Saruwatari"
        ]
      ],
      "title": "Multi-Speaker Text-to-Speech Synthesis Using Deep Gaussian Processes",
      "original": "3167",
      "page_count": 5,
      "order": 421,
      "p1": "2032",
      "pn": "2036",
      "abstract": [
        "Multi-speaker speech synthesis is a technique for modeling multiple\nspeakers&#8217; voices with a single model. Although many approaches\nusing deep neural networks (DNNs) have been proposed, DNNs are prone\nto overfitting when the amount of training data is limited. We propose\na framework for multi-speaker speech synthesis using deep Gaussian\nprocesses (DGPs); a DGP is a deep architecture of Bayesian kernel regressions\nand thus robust to overfitting. In this framework, speaker information\nis fed to duration/acoustic models using speaker codes. We also examine\nthe use of deep Gaussian process latent variable models (DGPLVMs).\nIn this approach, the representation of each speaker is learned simultaneously\nwith other model parameters, and therefore the similarity or dissimilarity\nof speakers is considered efficiently. We experimentally evaluated\ntwo situations to investigate the effectiveness of the proposed methods.\nIn one situation, the amount of data from each speaker is balanced\n(speaker-balanced), and in the other, the data from certain speakers\nare limited (speaker-imbalanced). Subjective and objective evaluation\nresults showed that both the DGP and DGPLVM synthesize multi-speaker\nspeech more effective than a DNN in the speaker-balanced situation.\nWe also found that the DGPLVM outperforms the DGP significantly in\nthe speaker-imbalanced situation.\n"
      ],
      "doi": "10.21437/Interspeech.2020-3167",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "m20_interspeech": {
      "authors": [
        [
          "Mano Ranjith Kumar",
          "M."
        ],
        [
          "Sudhanshu",
          "Srivastava"
        ],
        [
          "Anusha",
          "Prakash"
        ],
        [
          "Hema A.",
          "Murthy"
        ]
      ],
      "title": "A Hybrid HMM-Waveglow Based Text-to-Speech Synthesizer Using Histogram Equalization for Low Resource Indian Languages",
      "original": "3180",
      "page_count": 5,
      "order": 422,
      "p1": "2037",
      "pn": "2041",
      "abstract": [
        "Conventional text-to-speech (TTS) synthesis requires extensive linguistic\nprocessing for producing quality output. The advent of end-to-end (E2E)\nsystems has caused a relocation in the paradigm with better synthesized\nvoices. However, hidden Markov model (HMM) based systems are still\npopular due to their fast synthesis time, robustness to less training\ndata, and flexible adaptation of voice characteristics, speaking styles,\nand emotions.<br style=\"mso-data-placement:same-cell;\" /><br style=\"mso-data-placement:same-cell;\"\n/>\n  This paper proposes a technique that combines the classical parametric\nHMM-based TTS framework (HTS) with the neural-network-based Waveglow\nvocoder using histogram equalization (HEQ) in a low resource environment.\nThe two paradigms are combined by performing HEQ across mel-spectrograms\nextracted from HTS generated audio and source spectra of training data.\nDuring testing, the synthesized mel-spectrograms are mapped to the\nsource spectrograms using the learned HEQ. Experiments are carried\nout on Hindi male and female dataset of the Indic TTS database. Systems\nare evaluated based on degradation mean opinion scores (DMOS). Results\nindicate that the synthesis quality of the hybrid system is better\nthan that of the conventional HTS system. These results are quite promising\nas they pave way to good quality TTS systems with less data compared\nto E2E systems.\n"
      ],
      "doi": "10.21437/Interspeech.2020-3180",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "schuller20_interspeech": {
      "authors": [
        [
          "Bj\u00f6rn W.",
          "Schuller"
        ],
        [
          "Anton",
          "Batliner"
        ],
        [
          "Christian",
          "Bergler"
        ],
        [
          "Eva-Maria",
          "Messner"
        ],
        [
          "Antonia",
          "Hamilton"
        ],
        [
          "Shahin",
          "Amiriparian"
        ],
        [
          "Alice",
          "Baird"
        ],
        [
          "Georgios",
          "Rizos"
        ],
        [
          "Maximilian",
          "Schmitt"
        ],
        [
          "Lukas",
          "Stappen"
        ],
        [
          "Harald",
          "Baumeister"
        ],
        [
          "Alexis Deighton",
          "MacIntyre"
        ],
        [
          "Simone",
          "Hantke"
        ]
      ],
      "title": "The INTERSPEECH 2020 Computational Paralinguistics Challenge: Elderly Emotion, Breathing &amp; Masks",
      "original": "0032",
      "page_count": 5,
      "order": 423,
      "p1": "2042",
      "pn": "2046",
      "abstract": [
        "The INTERSPEECH 2020 Computational Paralinguistics Challenge addresses\nthree different problems for the first time in a research competition\nunder well-defined conditions: In the  Elderly Emotion Sub-Challenge,\narousal and valence in the speech of elderly individuals have to be\nmodelled as a 3-class problem; in the  Breathing Sub-Challenge, breathing\nhas to be assessed as a regression problem; and in the  Mask Sub-Challenge,\nspeech without and with a surgical mask has to be told apart. We describe\nthe Sub-Challenges, baseline feature extraction, and classifiers based\non the &#8216;usual&#8217;  ComParE and BoAW features as well as deep\nunsupervised representation learning using the  auDeep toolkit, and\ndeep feature extraction from pre-trained CNNs using the  Deep Spectrum\ntoolkit; in addition, we partially add deep end-to-end sequential modelling,\nand, for the first time in the challenge, linguistic analysis.\n"
      ],
      "doi": "10.21437/Interspeech.2020-32"
    },
    "koike20_interspeech": {
      "authors": [
        [
          "Tomoya",
          "Koike"
        ],
        [
          "Kun",
          "Qian"
        ],
        [
          "Bj\u00f6rn W.",
          "Schuller"
        ],
        [
          "Yoshiharu",
          "Yamamoto"
        ]
      ],
      "title": "Learning Higher Representations from Pre-Trained Deep Models with Data Augmentation for the COMPARE 2020 Challenge Mask Task",
      "original": "1552",
      "page_count": 5,
      "order": 424,
      "p1": "2047",
      "pn": "2051",
      "abstract": [
        "Human hand-crafted features are always regarded as expensive, time-consuming,\nand difficult in almost all of the machine-learning-related tasks.\nFirst, those well-designed features extremely rely on human expert\ndomain knowledge, which may restrain the collaboration work across\nfields. Second, the features extracted in such a brute-force scenario\nmay not be easy to be transferred to another task, which means a series\nof new features should be designed. To this end, we introduce a method\nbased on a transfer learning strategy combined with data augmentation\ntechniques for the  ComParE 2020 Challenge  Mask Sub-Challenge. Unlike\nthe previous studies mainly based on pre-trained models by image data,\nwe use a pre-trained model based on large scale audio data, i. e.,\nAudioSet. In addition, the  SpecAugment and  mixup methods are used\nto improve the generalisation of the deep models. Experimental results\ndemonstrate that the best-proposed model can significantly (p &#60;\n.001, by one-tailed z-test) improve the unweighted average recall (UAR)\nfrom 71.8% (baseline) to 76.2% on the test set. Finally, the best result,\ni. e., 77.5% of the UAR on the test set, is achieved by a late fusion\nof the two best proposed models and the best single model in the baseline.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1552",
      "author_area_id": "13",
      "author_area_label": "Special Sessions"
    },
    "illium20_interspeech": {
      "authors": [
        [
          "Steffen",
          "Illium"
        ],
        [
          "Robert",
          "M\u00fcller"
        ],
        [
          "Andreas",
          "Sedlmeier"
        ],
        [
          "Claudia",
          "Linnhoff-Popien"
        ]
      ],
      "title": "Surgical Mask Detection with Convolutional Neural Networks and Data Augmentations on Spectrograms",
      "original": "1692",
      "page_count": 5,
      "order": 425,
      "p1": "2052",
      "pn": "2056",
      "abstract": [
        "In many fields of research, labeled data-sets are hard to acquire.\nThis is where data augmentation promises to overcome the lack of training\ndata in the context of neural network engineering and classification\ntasks. The idea here is to reduce model over-fitting to the feature\ndistribution of a small under-descriptive training data-set. We try\nto evaluate such data augmentation techniques to gather insights in\nthe performance boost they provide for several convolutional neural\nnetworks on mel-spectrogram representations of audio data. We show\nthe impact of data augmentation on the binary classification task of\nsurgical mask detection in samples of human voice ( ComParE Challenge\n2020). Also we consider four varying architectures to account for augmentation\nrobustness. Results show that most of the baselines given by  ComParE\nare outperformed.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1692",
      "author_area_id": "13",
      "author_area_label": "Special Sessions"
    },
    "klumpp20_interspeech": {
      "authors": [
        [
          "Philipp",
          "Klumpp"
        ],
        [
          "Tom\u00e1s",
          "Arias-Vergara"
        ],
        [
          "Juan Camilo",
          "V\u00e1squez-Correa"
        ],
        [
          "Paula Andrea",
          "P\u00e9rez-Toro"
        ],
        [
          "Florian",
          "H\u00f6nig"
        ],
        [
          "Elmar",
          "N\u00f6th"
        ],
        [
          "Juan Rafael",
          "Orozco-Arroyave"
        ]
      ],
      "title": "Surgical Mask Detection with Deep Recurrent Phonetic Models",
      "original": "1723",
      "page_count": 5,
      "order": 426,
      "p1": "2057",
      "pn": "2061",
      "abstract": [
        "To solve the task of surgical mask detection from audio recordings\nin the scope of Interspeech&#8217;s ComParE challenge, we introduce\na phonetic recognizer which is able to differentiate between clear\nand mask samples.<br style=\"mso-data-placement:same-cell;\" /><br style=\"mso-data-placement:same-cell;\"\n/>\n  A deep recurrent phoneme recognition model is first trained on\nspectrograms from a German corpus to learn the spectral properties\nof different speech sounds. Under the assumption that each phoneme\nsounds differently among clear and mask speech, the model is then used\nto compute frame-wise phonetic labels for the challenge data, including\ninformation about the presence of a surgical mask. These labels served\nto train a second phoneme recognition model which is finally able to\ndifferentiate between mask and clear phoneme productions. For a single\nutterance, we can compute a functional representation and learn a random\nforest classifier to detect whether a speech sample was produced with\nor without a mask.<br style=\"mso-data-placement:same-cell;\" /><br style=\"mso-data-placement:same-cell;\"\n/>\n  Our method performed better than the baseline methods on both\nvalidation and test set. Furthermore, we could show how wearing a mask\ninfluences the speech signal. Certain phoneme groups were clearly affected\nby the obstruction in front of the vocal tract, while others remained\nalmost unaffected.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1723",
      "author_area_id": "13",
      "author_area_label": "Special Sessions"
    },
    "montacie20_interspeech": {
      "authors": [
        [
          "Claude",
          "Montaci\u00e9"
        ],
        [
          "Marie-Jos\u00e9",
          "Caraty"
        ]
      ],
      "title": "Phonetic, Frame Clustering and Intelligibility Analyses for the INTERSPEECH 2020 ComParE Challenge",
      "original": "2243",
      "page_count": 5,
      "order": 427,
      "p1": "2062",
      "pn": "2066",
      "abstract": [
        "The INTERSPEECH 2020 Compare Mask Sub-Challenge is to determine whether\na speech signal was emitted with or without wearing a surgical mask.\nFor this purpose, we have investigated phonetic context and intelligibility\nmeasurements related to speech changes caused by wearing a mask. Experiments\nwere conducted on the Mask Augsburg Speech Corpus (MASC) and on the\nMask Sorbonne Speech Corpus (MSSC) both in German language. We investigated\nthe effects of mask wearing on the acoustical properties of phonemes\nat frame and segment levels. At the frame level, a phonetic mask detector\nhas been developed to determine the most sensitive phonemes when wearing\na mask. At the segmental level, a perceptual scoring of intelligibility\nhas been developed and assessed on the MSCC. Two mask detector systems\nhave been developed and assessed on the MASC: the first one used two\nlarge composite audio feature sets, the second one used a bottom-up\napproach based on phonetic analysis and frame clustering. Experiments\nhave shown an improvement of 5.9% (absolute) on the Test set compared\nto the official baseline performance of the Challenge (71.8%).\n"
      ],
      "doi": "10.21437/Interspeech.2020-2243",
      "author_area_id": "13",
      "author_area_label": "Special Sessions"
    },
    "juliao20_interspeech": {
      "authors": [
        [
          "Mariana",
          "Juli\u00e3o"
        ],
        [
          "Alberto",
          "Abad"
        ],
        [
          "Helena",
          "Moniz"
        ]
      ],
      "title": "Exploring Text and Audio Embeddings for Multi-Dimension Elderly Emotion Recognition",
      "original": "2290",
      "page_count": 5,
      "order": 428,
      "p1": "2067",
      "pn": "2071",
      "abstract": [
        "This paper investigates the use of audio and text embeddings for the\nclassification of emotion dimensions within the scope of the Elderly\nEmotion Sub-Challenge of the INTERSPEECH 2020 Computational Paralinguistics\nChallenge. We explore speaker and time dependencies on the expression\nof emotions through the combination of well-known acoustic-prosodic\nfeatures and speaker embeddings extracted for different time scales.\nWe consider text information input through transformer language embeddings,\nboth isolated and in combination with acoustic features. The combination\nof acoustic and text information is explored in early and late fusion\nschemes. Overall, early fusion of systems trained on top of hand-crafted\nacoustic-prosodic features (eGeMAPS and ComParE), acoustic model feature\nembeddings (x-vectors), and text feature embeddings provide the best\nclassification results in development for both Arousal and Valence.\nThe combination of modalities allows us to reach a multi-dimension\nemotion classification performance in the development challenge data\nset of up to 48.8% Unweighted Average Recall (UAR) and 61.0% UAR for\nArousal and Valence, respectively. These results correspond to a 16.2%\nand a 8.7% relative UAR improvement.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2290",
      "author_area_id": "13",
      "author_area_label": "Special Sessions"
    },
    "markitantov20_interspeech": {
      "authors": [
        [
          "Maxim",
          "Markitantov"
        ],
        [
          "Denis",
          "Dresvyanskiy"
        ],
        [
          "Danila",
          "Mamontov"
        ],
        [
          "Heysem",
          "Kaya"
        ],
        [
          "Wolfgang",
          "Minker"
        ],
        [
          "Alexey",
          "Karpov"
        ]
      ],
      "title": "Ensembling End-to-End Deep Models for Computational Paralinguistics Tasks: ComParE 2020 Mask and Breathing Sub-Challenges",
      "original": "2666",
      "page_count": 5,
      "order": 429,
      "p1": "2072",
      "pn": "2076",
      "abstract": [
        "This paper describes deep learning approaches for the Mask and Breathing\nSub-Challenges (SCs), which are addressed by the INTERSPEECH 2020 Computational\nParalinguistics Challenge. Motivated by outstanding performance of\nstate-of-the-art end-to-end (E2E) approaches, we explore and compare\neffectiveness of different deep Convolutional Neural Network (CNN)\narchitectures on raw data, log Mel-spectrograms, and Mel-Frequency\nCepstral Coefficients. We apply a transfer learning approach to improve\nmodel&#8217;s efficiency and convergence speed. In the Mask SC, we\nconduct experiments with several pretrained CNN architectures on log-Mel\nspectrograms, as well as Support Vector Machines on baseline features.\nFor the Breathing SC, we propose an ensemble deep learning system that\nexploits E2E learning and sequence prediction. The E2E model is based\non 1D CNN operating on raw speech signals and is coupled with Long\nShort-Term Memory layers for sequence modeling. The second model works\nwith log-Mel features and is based on a pretrained 2D CNN model stacked\nto Gated Recurrent Unit layers. To increase performance of our models\nin both SCs, we use ensembles of the best deep neural models obtained\nfrom N-fold cross-validation on combined challenge training and development\ndatasets. Our results markedly outperform the challenge test set baselines\nin both SCs.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2666",
      "author_area_id": "13",
      "author_area_label": "Special Sessions"
    },
    "mendonca20_interspeech": {
      "authors": [
        [
          "John",
          "Mendon\u00e7a"
        ],
        [
          "Francisco",
          "Teixeira"
        ],
        [
          "Isabel",
          "Trancoso"
        ],
        [
          "Alberto",
          "Abad"
        ]
      ],
      "title": "Analyzing Breath Signals for the Interspeech 2020 ComParE Challenge",
      "original": "2778",
      "page_count": 5,
      "order": 430,
      "p1": "2077",
      "pn": "2081",
      "abstract": [
        "This paper presents our contribution to the INTERSPEECH 2020 Breathing\nSub-challenge. Besides fulfilling the main goal of the challenge, which\ninvolves the automatic prediction from conversational speech of the\nbreath signals obtained from respiratory belts, we also analyse both\noriginal and predicted signals in an attempt to overcome the main pitfalls\nof the proposed systems. In particular, we identify the subsets of\nmost irregular belt signals which yield the worst performance, measured\nby the Pearson correlation coefficient, and show how they affect the\nresults that were obtained by both the baseline end-to-end system and\nvariants such as a Bidirectional LSTM. The performance of this type\nof architecture indicates that future information is also relevant\nwhen predicting breathing patterns.<br style=\"mso-data-placement:same-cell;\"\n/><br style=\"mso-data-placement:same-cell;\" />\n  We also study the\ninformation retained from the AM-FM decomposition of the speech signal\nfor this purpose, showing how the AM component significantly outperforms\nthe FM component on all experiments, but fails to surpass the prediction\nresults obtained using the original speech signal.<br style=\"mso-data-placement:same-cell;\"\n/><br style=\"mso-data-placement:same-cell;\" />\n  Finally, we validate\nthe system&#8217;s performance in video-conferencing conditions by\nusing data augmentation and compare clinically relevant parameters,\nsuch as breathing rate, from both the original belt signals and the\nones predicted from the simulated video-conferencing signals.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2778",
      "author_area_id": "13",
      "author_area_label": "Special Sessions"
    },
    "macintyre20_interspeech": {
      "authors": [
        [
          "Alexis Deighton",
          "MacIntyre"
        ],
        [
          "Georgios",
          "Rizos"
        ],
        [
          "Anton",
          "Batliner"
        ],
        [
          "Alice",
          "Baird"
        ],
        [
          "Shahin",
          "Amiriparian"
        ],
        [
          "Antonia",
          "Hamilton"
        ],
        [
          "Bj\u00f6rn W.",
          "Schuller"
        ]
      ],
      "title": "Deep Attentive End-to-End Continuous Breath Sensing from Speech",
      "original": "2832",
      "page_count": 5,
      "order": 431,
      "p1": "2082",
      "pn": "2086",
      "abstract": [
        "Modelling of the breath signal is of high interest to both healthcare\nprofessionals and computer scientists, as a source of diagnosis-related\ninformation, or a means for curating higher quality datasets in speech\nanalysis research. The formation of a breath signal gold standard is,\nhowever, not a straightforward task, as it requires specialised equipment,\nhuman annotation budget, and even then, it corresponds to lab recording\nsettings, that are not reproducible in-the-wild. Herein, we explore\ndeep learning based methodologies, as an automatic way to predict a\ncontinuous-time breath signal by solely analysing spontaneous speech.\nWe address two task formulations, those of continuous-valued signal\nprediction, as well as inhalation event prediction, that are of great\nuse in various healthcare and Automatic Speech Recognition applications,\nand showcase results that outperform current baselines. Most importantly,\nwe also perform an initial exploration into explaining which parts\nof the input audio signal are important with respect to the prediction.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2832",
      "author_area_id": "13",
      "author_area_label": "Special Sessions"
    },
    "szep20_interspeech": {
      "authors": [
        [
          "Jeno",
          "Szep"
        ],
        [
          "Salim",
          "Hariri"
        ]
      ],
      "title": "Paralinguistic Classification of Mask Wearing by Image Classifiers and Fusion",
      "original": "2857",
      "page_count": 5,
      "order": 432,
      "p1": "2087",
      "pn": "2091",
      "abstract": [
        "In this study, we address the ComParE 2020 Paralinguistics Mask sub-challenge,\nwhere the task is the detection of wearing surgical masks from short\nspeech segments. In our approach, we propose a computer-vision-based\npipeline to utilize the capabilities of deep convolutional neural network-based\nimage classifiers developed in recent years and apply this technology\nto a specific class of spectrograms. Several linear and logarithmic\nscale spectrograms were tested, and the best performance is achieved\non linear-scale, 3-Channel Spectrograms created from the audio segments.\nA single model image classifier provided a 6.1% better result than\nthe best single-dataset baseline model. The ensemble of our models\nfurther improves accuracy and achieves 73.0% UAR by training just on\nthe &#8216;train&#8217; dataset and reaches 80.1% UAR on the test set\nwhen training includes the &#8216;devel&#8217; dataset, which result\nis 8.3% higher than the baseline. We also provide an activation-mapping\nanalysis to identify frequency ranges that are critical in the &#8216;mask&#8217;\nversus &#8216;clear&#8217; classification.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2857",
      "author_area_id": "13",
      "author_area_label": "Special Sessions"
    },
    "yang20c_interspeech": {
      "authors": [
        [
          "Ziqing",
          "Yang"
        ],
        [
          "Zifan",
          "An"
        ],
        [
          "Zehao",
          "Fan"
        ],
        [
          "Chengye",
          "Jing"
        ],
        [
          "Houwei",
          "Cao"
        ]
      ],
      "title": "Exploration of Acoustic and Lexical Cues for the INTERSPEECH 2020 Computational Paralinguistic Challenge",
      "original": "2999",
      "page_count": 5,
      "order": 433,
      "p1": "2092",
      "pn": "2096",
      "abstract": [
        "In this paper, we investigate various acoustic features and lexical\nfeatures for the INTERSPEECH 2020 Computational Paralinguistic Challenge.\nFor the acoustic analysis, we show that the proposed FV-MFCC feature\nis very promising, which has very strong prediction power on its own,\nand can also provide complementary information when fused with other\nacoustic features. For the lexical representation, we find that the\ncorpus-dependent TF.IDF feature is by far the best representation.\nWe also explore several model fusion techniques to combine different\nmodalities together, and propose novel SVM models to aggregate the\nchunk-level predictions to the narrative-level predictions based on\nthe chunk-level decision functionals. Finally we discuss the potential\nfor improving prediction by combining the lexical and acoustic modalities\ntogether, and we find that fusion of lexical and acoustic modalities\ndo not lead to consistent improvements over elderly Arousal, but substantially\nimprove over the Valence. Our methods significantly outperform the\nofficial baselines on the test set in the participated Mask and Elderly\nSub-challenges. We obtain an UAR of 75.1%, 54.3%, and 59.0% on the\nMask, Elderly Arousal and Valence prediction tasks respectively.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2999",
      "author_area_id": "13",
      "author_area_label": "Special Sessions"
    },
    "sogancoglu20_interspeech": {
      "authors": [
        [
          "Gizem",
          "So\u011fanc\u0131o\u011flu"
        ],
        [
          "Oxana",
          "Verkholyak"
        ],
        [
          "Heysem",
          "Kaya"
        ],
        [
          "Dmitrii",
          "Fedotov"
        ],
        [
          "Tobias",
          "Cad\u00e9e"
        ],
        [
          "Albert Ali",
          "Salah"
        ],
        [
          "Alexey",
          "Karpov"
        ]
      ],
      "title": "Is Everything Fine, Grandma? Acoustic and Linguistic Modeling for Robust Elderly Speech Emotion Recognition",
      "original": "3160",
      "page_count": 5,
      "order": 434,
      "p1": "2097",
      "pn": "2101",
      "abstract": [
        "Acoustic and linguistic analysis for elderly emotion recognition is\nan under-studied and challenging research direction, but essential\nfor the creation of digital assistants for the elderly, as well as\nunobtrusive telemonitoring of elderly in their residences for mental\nhealthcare purposes. This paper presents our contribution to the INTERSPEECH\n2020 Computational Paralinguistics Challenge (ComParE) - Elderly Emotion\nSub-Challenge, which is comprised of two ternary classification tasks\nfor arousal and valence recognition. We propose a bi-modal framework,\nwhere these tasks are modeled using state-of-the-art acoustic and linguistic\nfeatures, respectively. In this study, we demonstrate that exploiting\ntask-specific dictionaries and resources can boost the performance\nof linguistic models, when the amount of labeled data is small. Observing\na high mismatch between development and test set performances of various\nmodels, we also propose alternative training and decision fusion strategies\nto better estimate and improve the generalization performance.\n"
      ],
      "doi": "10.21437/Interspeech.2020-3160",
      "author_area_id": "13",
      "author_area_label": "Special Sessions"
    },
    "ristea20_interspeech": {
      "authors": [
        [
          "Nicolae-C\u0103t\u0103lin",
          "Ristea"
        ],
        [
          "Radu Tudor",
          "Ionescu"
        ]
      ],
      "title": "Are you Wearing a Mask? Improving Mask Detection from Speech Using Augmentation by Cycle-Consistent GANs",
      "original": "1329",
      "page_count": 5,
      "order": 435,
      "p1": "2102",
      "pn": "2106",
      "abstract": [
        "The task of detecting whether a person wears a face mask from speech\nis useful in modelling speech in forensic investigations, communication\nbetween surgeons or people protecting themselves against infectious\ndiseases such as COVID-19. In this paper, we propose a novel data augmentation\napproach for mask detection from speech. Our approach is based on (i)\ntraining Generative Adversarial Networks (GANs) with cycle-consistency\nloss to translate unpaired utterances between two classes (with mask\nand without mask), and on (ii) generating new training utterances using\nthe cycle-consistent GANs, assigning opposite labels to each translated\nutterance. Original and translated utterances are converted into spectrograms\nwhich are provided as input to a set of ResNet neural networks with\nvarious depths. The networks are combined into an ensemble through\na Support Vector Machines (SVM) classifier. With this system, we participated\nin the Mask Sub-Challenge (MSC) of the INTERSPEECH 2020 Computational\nParalinguistics Challenge, surpassing the baseline proposed by the\norganizers by 2.8%. Our data augmentation technique provided a performance\nboost of 0.9% on the private test set. Furthermore, we show that our\ndata augmentation approach yields better results than other baseline\nand state-of-the-art augmentation methods. \n"
      ],
      "doi": "10.21437/Interspeech.2020-1329",
      "author_area_id": "13",
      "author_area_label": "Special Sessions"
    },
    "kumar20d_interspeech": {
      "authors": [
        [
          "Kshitiz",
          "Kumar"
        ],
        [
          "Chaojun",
          "Liu"
        ],
        [
          "Yifan",
          "Gong"
        ],
        [
          "Jian",
          "Wu"
        ]
      ],
      "title": "1-D Row-Convolution LSTM: Fast Streaming ASR at Accuracy Parity with LC-BLSTM",
      "original": "2894",
      "page_count": 5,
      "order": 436,
      "p1": "2107",
      "pn": "2111",
      "abstract": [
        "In this work we develop a simple, efficient, and compact automatic\nspeech recognition (ASR) model based on purely 1-dimensional row convolution\n(RC) operation. We refer to our proposed model as 1-dim row-convolution\nLSTM (RC-LSTM), where we embed limited future information to standard\nUniLSTMs in 1-dim RC operation. We target fast streaming ASR solutions\nand establish ASR accuracy parity with latency-control bidirectional-LSTM\n(LC-BLSTM). We develop an application of future information at ASR\nfeatures and hidden layer stages. We study connections with related\ntechniques, analyze tradeoffs and recommend uniform future lookahead\nto all hidden layers. We argue that our architecture implicitly factorizes\ntraining into orthogonal time and &#8220;frequency&#8221; dimensions\nfor an effective learning on large scale tasks. We conduct a series\nof experiments on medium scale with 6k hrs of English corpus, as well\nas, large scale with 60k hrs training. We demonstrate our findings\nacross unified ASR tasks. Compared to UniLSTM model, RC-LSTM achieved\n16% relative reduction in word error rate (WER). RC-LSTM also achieved\naccuracy parity with LC-BLSTM on large scale tasks at significantly\nlower latency and computational cost.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2894",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "wang20v_interspeech": {
      "authors": [
        [
          "Chengyi",
          "Wang"
        ],
        [
          "Yu",
          "Wu"
        ],
        [
          "Liang",
          "Lu"
        ],
        [
          "Shujie",
          "Liu"
        ],
        [
          "Jinyu",
          "Li"
        ],
        [
          "Guoli",
          "Ye"
        ],
        [
          "Ming",
          "Zhou"
        ]
      ],
      "title": "Low Latency End-to-End Streaming Speech Recognition with a Scout Network",
      "original": "1292",
      "page_count": 5,
      "order": 437,
      "p1": "2112",
      "pn": "2116",
      "abstract": [
        "The attention-based Transformer model has achieved promising results\nfor speech recognition (SR) in the offline mode. However, in the streaming\nmode, the Transformer model usually incurs significant latency to maintain\nits recognition accuracy when applying a fixed-length look-ahead window\nin each encoder layer. In this paper, we propose a novel low-latency\nstreaming approach for Transformer models, which consists of a scout\nnetwork and a recognition network. The scout network detects the whole\nword boundary without seeing any future frames, while the recognition\nnetwork predicts the next subword by utilizing the information from\nall the frames before the predicted boundary. Our model achieves the\nbest performance (2.7/6.4 WER) with only an average of 639 ms latency\non the test-clean and test-other data sets of Librispeech.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1292",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "kurata20_interspeech": {
      "authors": [
        [
          "Gakuto",
          "Kurata"
        ],
        [
          "George",
          "Saon"
        ]
      ],
      "title": "Knowledge Distillation from Offline to Streaming RNN Transducer for End-to-End Speech Recognition",
      "original": "2442",
      "page_count": 5,
      "order": 438,
      "p1": "2117",
      "pn": "2121",
      "abstract": [
        "End-to-end training of recurrent neural network transducers (RNN-Ts)\ndoes not require frame-level alignments between audio and output symbols.\nBecause of that, the posterior lattices defined by the predictive distributions\nfrom different RNN-Ts trained on the same data can differ a lot, which\nposes a new set of challenges in knowledge distillation between such\nmodels. These discrepancies are especially prominent in the posterior\nlattices between an offline model and a streaming model, which can\nbe expected from the fact that the streaming RNN-T emits symbols later\nthan the offline RNN-T. We propose a method to train an RNN-T so that\nthe posterior peaks at each node in the posterior lattice are aligned\nwith the ones from a pretrained model for the same utterance. By utilizing\nthis method, we can train an offline RNN-T that can serve as a good\nteacher to train a student streaming RNN-T. Experimental results on\nthe standard Switchboard conversational telephone speech corpus demonstrate\naccuracy improvements for a streaming unidirectional RNN-T by knowledge\ndistillation from an offline bidirectional counterpart.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2442",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "li20t_interspeech": {
      "authors": [
        [
          "Wei",
          "Li"
        ],
        [
          "James",
          "Qin"
        ],
        [
          "Chung-Cheng",
          "Chiu"
        ],
        [
          "Ruoming",
          "Pang"
        ],
        [
          "Yanzhang",
          "He"
        ]
      ],
      "title": "Parallel Rescoring with Transformer for Streaming On-Device Speech Recognition",
      "original": "2875",
      "page_count": 5,
      "order": 439,
      "p1": "2122",
      "pn": "2126",
      "abstract": [
        "Recent advances of end-to-end models have outperformed conventional\nmodels through employing a two-pass model. The two-pass model provides\nbetter speed-quality trade-offs for on-device speech recognition, where\na 1 st-pass model generates hypotheses in a streaming fashion, and\na 2 nd-pass model rescores the hypotheses with full audio sequence\ncontext. The 2 nd-pass model plays a key role in the quality improvement\nof the end-to-end model to surpass the conventional model. One main\nchallenge of the two-pass model is the computation latency introduced\nby the 2 nd-pass model. Specifically, the original design of the two-pass\nmodel uses LSTMs for the 2 nd-pass model, which are subject to long\nlatency as they are constrained by the recurrent nature and have to\nrun inference sequentially. In this work we explore replacing the LSTM\nlayers in the 2 nd-pass rescorer with Transformer layers, which can\nprocess the entire hypothesis sequences  in parallel and can therefore\nutilize the on-device computation resources more efficiently. Compared\nwith an LSTM-based baseline, our proposed Transformer rescorer achieves\nmore than 50% latency reduction with quality improvement.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2875",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "baqueroarnal20_interspeech": {
      "authors": [
        [
          "Pau",
          "Baquero-Arnal"
        ],
        [
          "Javier",
          "Jorge"
        ],
        [
          "Adri\u00e0",
          "Gim\u00e9nez"
        ],
        [
          "Joan Albert",
          "Silvestre-Cerd\u00e0"
        ],
        [
          "Javier",
          "Iranzo-S\u00e1nchez"
        ],
        [
          "Albert",
          "Sanchis"
        ],
        [
          "Jorge",
          "Civera"
        ],
        [
          "Alfons",
          "Juan"
        ]
      ],
      "title": "Improved Hybrid Streaming ASR with Transformer Language Models",
      "original": "2770",
      "page_count": 5,
      "order": 440,
      "p1": "2127",
      "pn": "2131",
      "abstract": [
        "Streaming ASR is gaining momentum due to its wide applicability, though\nit is still unclear how best to come close to the accuracy of state-of-the-art\noff-line ASR systems when the output must come within a short delay\nafter the incoming audio stream. Following our previous work on streaming\none-pass decoding with hybrid ASR systems and LSTM language models,\nin this work we report further improvements by replacing LSTMs with\nTransformer models. First, two key ideas are discussed so as to run\nthese models fast during inference. Then, empirical results on LibriSpeech\nand TED-LIUM are provided showing that Transformer language models\nlead to improved recognition rates on both tasks. ASR systems obtained\nin this work can be seamlessly transferred to a streaming setup with\nminimal quality losses. Indeed, to the best of our knowledge, no better\nresults have been reported on these tasks when assessed under a streaming\nsetup.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2770",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "wu20i_interspeech": {
      "authors": [
        [
          "Chunyang",
          "Wu"
        ],
        [
          "Yongqiang",
          "Wang"
        ],
        [
          "Yangyang",
          "Shi"
        ],
        [
          "Ching-Feng",
          "Yeh"
        ],
        [
          "Frank",
          "Zhang"
        ]
      ],
      "title": "Streaming Transformer-Based Acoustic Models Using Self-Attention with Augmented Memory",
      "original": "2079",
      "page_count": 5,
      "order": 441,
      "p1": "2132",
      "pn": "2136",
      "abstract": [
        "Transformer-based acoustic modeling has achieved great success for\nboth hybrid and sequence-to-sequence speech recognition. However, it\nrequires access to the full sequence, and the computational cost grows\nquadratically with respect to the input sequence length. These factors\nlimit its adoption for streaming applications. In this work, we proposed\na novel augmented memory self-attention, which attends on a short segment\nof the input sequence and a bank of memories. The memory bank stores\nthe embedding information for all the processed segments. On the librispeech\nbenchmark, our proposed method outperforms all the existing streamable\ntransformer methods by a large margin and achieved over 15% relative\nerror reduction, compared with the widely used LC-BLSTM baseline. Our\nfindings are also confirmed on some large internal datasets.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2079",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "inaguma20b_interspeech": {
      "authors": [
        [
          "Hirofumi",
          "Inaguma"
        ],
        [
          "Masato",
          "Mimura"
        ],
        [
          "Tatsuya",
          "Kawahara"
        ]
      ],
      "title": "Enhancing Monotonic Multihead Attention for Streaming ASR",
      "original": "1780",
      "page_count": 5,
      "order": 442,
      "p1": "2137",
      "pn": "2141",
      "abstract": [
        "We investigate a monotonic multihead attention (MMA) by extending hard\nmonotonic attention to Transformer-based automatic speech recognition\n(ASR) for online streaming applications. For streaming inference, all\nmonotonic attention (MA) heads should learn proper alignments because\nthe next token is not generated until all heads detect the corresponding\ntoken boundaries. However, we found not all MA heads learn alignments\nwith a na&#239;ve implementation. To encourage every head to learn\nalignments properly, we propose  HeadDrop regularization by masking\nout a part of heads stochastically during training. Furthermore, we\npropose to prune redundant heads to improve consensus among heads for\nboundary detection and prevent delayed token generation caused by such\nheads. Chunkwise attention on each MA head is extended to the multihead\ncounterpart. Finally, we propose  head-synchronous beam search decoding\nto guarantee stable streaming inference.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1780",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "zhang20q_interspeech": {
      "authors": [
        [
          "Shiliang",
          "Zhang"
        ],
        [
          "Zhifu",
          "Gao"
        ],
        [
          "Haoneng",
          "Luo"
        ],
        [
          "Ming",
          "Lei"
        ],
        [
          "Jie",
          "Gao"
        ],
        [
          "Zhijie",
          "Yan"
        ],
        [
          "Lei",
          "Xie"
        ]
      ],
      "title": "Streaming Chunk-Aware Multihead Attention for Online End-to-End Speech Recognition",
      "original": "1972",
      "page_count": 5,
      "order": 443,
      "p1": "2142",
      "pn": "2146",
      "abstract": [
        "Recently, streaming end-to-end automatic speech recognition (E2E-ASR)\nhas gained more and more attention. Many efforts have been paid to\nturn the non-streaming attention-based E2E-ASR system into streaming\narchitecture. In this work, we propose a novel online E2E-ASR system\nby using  Streaming Chunk-Aware Multihead Attention (SCAMA) and a latency\ncontrol memory equipped self-attention network (LC-SAN-M). LC-SAN-M\nuses chunk-level input to control the latency of encoder. As to SCAMA,\na jointly trained  predictor is used to control the output of encoder\nwhen feeding to decoder, which enables decoder to generate output in\nstreaming manner. Experimental results on the open 170-hour AISHELL-1\nand an industrial-level 20000-hour Mandarin speech recognition tasks\nshow that our approach can significantly outperform the MoChA-based\nbaseline system under comparable setup. On the AISHELL-1 task, our\nproposed method achieves a character error rate (CER) of 7.39%, to\nthe best of our knowledge, which is the best published performance\nfor online ASR.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1972",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "nguyen20b_interspeech": {
      "authors": [
        [
          "Thai-Son",
          "Nguyen"
        ],
        [
          "Ngoc-Quan",
          "Pham"
        ],
        [
          "Sebastian",
          "St\u00fcker"
        ],
        [
          "Alex",
          "Waibel"
        ]
      ],
      "title": "High Performance Sequence-to-Sequence Model for Streaming Speech Recognition",
      "original": "1863",
      "page_count": 5,
      "order": 444,
      "p1": "2147",
      "pn": "2151",
      "abstract": [
        "Recently sequence-to-sequence models have started to achieve state-of-the-art\nperformance on standard speech recognition tasks when processing audio\ndata in batch mode, i.e., the complete audio data is available when\nstarting processing. However, when it comes to performing run-on recognition\non an input stream of audio data while producing recognition results\nin real-time and with low word-based latency, these models face several\nchallenges. For many techniques, the whole audio sequence to be decoded\nneeds to be available at the start of the processing, e.g., for the\nattention mechanism or the bidirectional LSTM (BLSTM). In this paper,\nwe propose several techniques to mitigate these problems. We introduce\nan additional loss function controlling the uncertainty of the attention\nmechanism, a modified beam search identifying partial, stable hypotheses,\nways of working with BLSTM in the encoder, and the use of chunked BLSTM.\nOur experiments show that with the right combination of these techniques,\nit is possible to perform run-on speech recognition with low word-based\nlatency without sacrificing in word error rate performance.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1863",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "joshi20_interspeech": {
      "authors": [
        [
          "Vikas",
          "Joshi"
        ],
        [
          "Rui",
          "Zhao"
        ],
        [
          "Rupesh R.",
          "Mehta"
        ],
        [
          "Kshitiz",
          "Kumar"
        ],
        [
          "Jinyu",
          "Li"
        ]
      ],
      "title": "Transfer Learning Approaches for Streaming End-to-End Speech Recognition System",
      "original": "2345",
      "page_count": 5,
      "order": 445,
      "p1": "2152",
      "pn": "2156",
      "abstract": [
        "Transfer learning (TL) is widely used in conventional hybrid automatic\nspeech recognition (ASR) system, to transfer the knowledge from source\nto target language. TL can be applied to end-to-end (E2E) ASR system\nsuch as recurrent neural network transducer (RNN-T) models, by initializing\nthe encoder and/or prediction network of the target language with the\npre-trained models from source language. In the hybrid ASR system,\ntransfer learning is typically done by initializing the target language\nacoustic model (AM) with source language AM. Several transfer learning\nstrategies exist in the case of the RNN-T framework, depending upon\nthe choice of the initialization model for encoder and prediction networks.\nThis paper presents a comparative study of four different TL methods\nfor RNN-T framework. We show 10%&#8211;17% relative word error rate\nreduction with different TL methods over randomly initialized RNN-T\nmodel. We also study the impact of TL with varying amount of training\ndata ranging from 50 hours to 1000 hours and show the efficacy of TL\nfor languages with a very small amount of training data.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2345",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "martinc20_interspeech": {
      "authors": [
        [
          "Matej",
          "Martinc"
        ],
        [
          "Senja",
          "Pollak"
        ]
      ],
      "title": "Tackling the ADReSS Challenge: A Multimodal Approach to the Automated Recognition of Alzheimer&#8217;s Dementia",
      "original": "2202",
      "page_count": 5,
      "order": 446,
      "p1": "2157",
      "pn": "2161",
      "abstract": [
        "The paper describes a multimodal approach to the automated recognition\nof Alzheimer&#8217;s dementia in order to solve the ADReSS (Alzheimer&#8217;s\nDementia Recognition through Spontaneous Speech) challenge at INTERSPEECH\n2020. The proposed method exploits available audio and textual data\nfrom the benchmark speech dataset to address challenge&#8217;s two\nsubtasks, a classification task that deals with classifying speech\nas dementia or healthy control speech and the regression task of determining\nthe mini-mental state examination scores (MMSE) for each speech segment.\nOur approach is based on evaluating the predictive power of different\ntypes of features and on an exhaustive grid search across several feature\ncombinations and different classification algorithms. Results suggest\nthat even though TF-IDF based textual features generally lead to better\nclassification and regression results, specific types of audio and\nreadability features can boost the overall performance of the classification\nand regression models.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2202",
      "author_area_id": "13",
      "author_area_label": "Special Sessions"
    },
    "yuan20_interspeech": {
      "authors": [
        [
          "Jiahong",
          "Yuan"
        ],
        [
          "Yuchen",
          "Bian"
        ],
        [
          "Xingyu",
          "Cai"
        ],
        [
          "Jiaji",
          "Huang"
        ],
        [
          "Zheng",
          "Ye"
        ],
        [
          "Kenneth",
          "Church"
        ]
      ],
      "title": "Disfluencies and Fine-Tuning Pre-Trained Language Models for Detection of Alzheimer&#8217;s Disease",
      "original": "2516",
      "page_count": 5,
      "order": 447,
      "p1": "2162",
      "pn": "2166",
      "abstract": [
        "Disfluencies and language problems in Alzheimer&#8217;s Disease can\nbe naturally modeled by fine-tuning Transformer-based pre-trained language\nmodels such as BERT and ERNIE. Using this method, we achieved 89.6%\naccuracy on the test set of the ADReSS (Alzheimer&#8217;s Dementia\nRecognition through Spontaneous Speech) Challenge, a considerable improvement\nover the baseline of 75.0%, established by the organizers of the challenge.\nThe best accuracy was obtained with ERNIE, plus an encoding of pauses.\nRobustness is a challenge for large models and small training sets.\nEnsemble over many runs of BERT/ERNIE fine-tuning reduced variance\nand improved accuracy. We found that  um was used much less frequently\nin Alzheimer&#8217;s speech, compared to  uh. We discussed this interesting\nfinding from linguistic and cognitive perspectives.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2516",
      "author_area_id": "13",
      "author_area_label": "Special Sessions"
    },
    "balagopalan20_interspeech": {
      "authors": [
        [
          "Aparna",
          "Balagopalan"
        ],
        [
          "Benjamin",
          "Eyre"
        ],
        [
          "Frank",
          "Rudzicz"
        ],
        [
          "Jekaterina",
          "Novikova"
        ]
      ],
      "title": "To BERT or not to BERT: Comparing Speech and Language-Based Approaches for Alzheimer&#8217;s Disease Detection",
      "original": "2557",
      "page_count": 5,
      "order": 448,
      "p1": "2167",
      "pn": "2171",
      "abstract": [
        "Research related to automatically detecting Alzheimer&#8217;s disease\n(AD) is important, given the high prevalence of AD and the high cost\nof traditional methods. Since AD significantly affects the content\nand acoustics of spontaneous speech, natural language processing and\nmachine learning provide promising techniques for reliably detecting\nAD. We compare and contrast the performance of two such approaches\nfor AD detection on the recent ADReSS challenge dataset [1]: 1) using\ndomain knowledge-based hand-crafted features that capture linguistic\nand acoustic phenomena, and 2) fine-tuning Bidirectional Encoder Representations\nfrom Transformer (BERT)-based sequence classification models. We also\ncompare multiple feature-based regression models for a neuropsychological\nscore task in the challenge. We observe that fine-tuned BERT models,\ngiven the relative importance of linguistics in cognitive impairment\ndetection, outperform feature-based approaches on the AD detection\ntask.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2557",
      "author_area_id": "13",
      "author_area_label": "Special Sessions"
    },
    "luz20_interspeech": {
      "authors": [
        [
          "Saturnino",
          "Luz"
        ],
        [
          "Fasih",
          "Haider"
        ],
        [
          "Sofia de la",
          "Fuente"
        ],
        [
          "Davida",
          "Fromm"
        ],
        [
          "Brian",
          "MacWhinney"
        ]
      ],
      "title": "Alzheimer&#8217;s Dementia Recognition Through Spontaneous Speech: The ADReSS Challenge",
      "original": "2571",
      "page_count": 5,
      "order": 449,
      "p1": "2172",
      "pn": "2176",
      "abstract": [
        "The ADReSS Challenge at INTERSPEECH 2020 defines a shared task through\nwhich different approaches to the automated recognition of Alzheimer&#8217;s\ndementia based on spontaneous speech can be compared. ADReSS provides\nresearchers with a benchmark speech dataset which has been acoustically\npre-processed and balanced in terms of age and gender, defining two\ncognitive assessment tasks, namely: the Alzheimer&#8217;s speech classification\ntask and the neuropsychological score regression task. In the Alzheimer&#8217;s\nspeech classification task, ADReSS challenge participants create models\nfor classifying speech as dementia or healthy control speech. In the\nneuropsychological score regression task, participants create models\nto predict mini-mental state examination scores. This paper describes\nthe ADReSS Challenge in detail and presents a baseline for both tasks,\nincluding feature extraction procedures and results for classification\nand regression models. ADReSS aims to provide the speech and language\nAlzheimer&#8217;s research community with a platform for comprehensive\nmethodological comparisons. This will hopefully contribute to addressing\nthe lack of standardisation that currently affects the field and shed\nlight on avenues for future research and clinical applicability.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2571",
      "author_area_id": "13",
      "author_area_label": "Special Sessions"
    },
    "pappagari20_interspeech": {
      "authors": [
        [
          "Raghavendra",
          "Pappagari"
        ],
        [
          "Jaejin",
          "Cho"
        ],
        [
          "Laureano",
          "Moro-Vel\u00e1zquez"
        ],
        [
          "Najim",
          "Dehak"
        ]
      ],
      "title": "Using State of the Art Speaker Recognition and Natural Language Processing Technologies to Detect Alzheimer&#8217;s Disease and Assess its Severity",
      "original": "2587",
      "page_count": 5,
      "order": 450,
      "p1": "2177",
      "pn": "2181",
      "abstract": [
        "In this study, we analyze the use of state-of-the-art technologies\nfor speaker recognition and natural language processing to detect Alzheimer&#8217;s\nDisease (AD) and to assess its severity predicting Mini-mental status\nevaluation (MMSE) scores. With these purposes, we study the use of\nspeech signals and transcriptions. Our work focuses on the adaptation\nof state-of-the-art models for both modalities individually and together\nto examine its complementarity. We used x-vectors to characterize speech\nsignals and pre-trained BERT models to process human transcriptions\nwith different back-ends in AD diagnosis and assessment. We evaluated\nfeatures based on silence segments of the audio files as a complement\nto x-vectors. We trained and evaluated our systems in the Interspeech\n2020 ADReSS challenge dataset, containing 78 AD patients and 78 sex\nand age-matched controls. Our results indicate that the fusion of scores\nobtained from the acoustic and the transcript-based models provides\nthe best detection and assessment results, suggesting that individual\nmodels for two modalities contain complementary information. The addition\nof the silence-related features improved the fusion system even further.\nA separate analysis of the models suggests that transcript-based models\nprovide better results than acoustic models in the detection task but\nsimilar results in the MMSE prediction task.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2587",
      "author_area_id": "13",
      "author_area_label": "Special Sessions"
    },
    "cummins20_interspeech": {
      "authors": [
        [
          "Nicholas",
          "Cummins"
        ],
        [
          "Yilin",
          "Pan"
        ],
        [
          "Zhao",
          "Ren"
        ],
        [
          "Julian",
          "Fritsch"
        ],
        [
          "Venkata Srikanth",
          "Nallanthighal"
        ],
        [
          "Heidi",
          "Christensen"
        ],
        [
          "Daniel",
          "Blackburn"
        ],
        [
          "Bj\u00f6rn W.",
          "Schuller"
        ],
        [
          "Mathew",
          "Magimai-Doss"
        ],
        [
          "Helmer",
          "Strik"
        ],
        [
          "Aki",
          "H\u00e4rm\u00e4"
        ]
      ],
      "title": "A Comparison of Acoustic and Linguistics Methodologies for Alzheimer&#8217;s Dementia Recognition",
      "original": "2635",
      "page_count": 5,
      "order": 451,
      "p1": "2182",
      "pn": "2186",
      "abstract": [
        "In the light of the current COVID-19 pandemic, the need for remote\ndigital health assessment tools is greater than ever. This statement\nis especially pertinent for elderly and vulnerable populations. In\nthis regard, the INTERSPEECH 2020 Alzheimer&#8217;s Dementia Recognition\nthrough Spontaneous Speech (ADReSS) Challenge offers competitors the\nopportunity to develop speech and language-based systems for the task\nof Alzheimer&#8217;s Dementia (AD) recognition. The challenge data\nconsists of speech recordings and their transcripts, the work presented\nherein is an assessment of different contemporary approaches on these\nmodalities. Specifically, we compared a hierarchical neural network\nwith an attention mechanism trained on linguistic features with three\nacoustic-based systems: (i) Bag-of-Audio-Words (BoAW) quantising different\nlow-level descriptors, (ii) a Siamese Network trained on log-Mel spectrograms,\nand (iii) a Convolutional Neural Network (CNN) end-to-end system trained\non raw waveforms. Key results indicate the strength of the linguistic\napproach over the acoustics systems. Our strongest test-set result\nwas achieved using a late fusion combination of BoAW, End-to-End CNN,\nand hierarchical-attention networks, which outperformed the challenge\nbaseline in both the classification and regression tasks.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2635",
      "author_area_id": "13",
      "author_area_label": "Special Sessions"
    },
    "rohanian20_interspeech": {
      "authors": [
        [
          "Morteza",
          "Rohanian"
        ],
        [
          "Julian",
          "Hough"
        ],
        [
          "Matthew",
          "Purver"
        ]
      ],
      "title": "Multi-Modal Fusion with Gating Using Audio, Lexical and Disfluency Features for Alzheimer&#8217;s Dementia Recognition from Spontaneous Speech",
      "original": "2721",
      "page_count": 5,
      "order": 452,
      "p1": "2187",
      "pn": "2191",
      "abstract": [
        "This paper is a submission to the Alzheimer&#8217;s Dementia Recognition\nthrough Spontaneous Speech (ADReSS) challenge, which aims to develop\nmethods that can assist in the automated prediction of severity of\nAlzheimer&#8217;s Disease from speech data. We focus on acoustic and\nnatural language features for cognitive impairment detection in spontaneous\nspeech in the context of Alzheimer&#8217;s Disease Diagnosis and the\nmini-mental state examination (MMSE) score prediction. We proposed\na model that obtains unimodal decisions from different LSTMs, one for\neach modality of text and audio, and then combines them using a gating\nmechanism for the final prediction. We focused on sequential modelling\nof text and audio and investigated whether the disfluencies present\nin individuals&#8217; speech relate to the extent of their cognitive\nimpairment. Our results show that the proposed classification and regression\nschemes obtain very promising results on both development and test\nsets. This suggests Alzheimer&#8217;s Disease can be detected successfully\nwith sequence modeling of the speech data of medical sessions.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2721",
      "author_area_id": "13",
      "author_area_label": "Special Sessions"
    },
    "searle20_interspeech": {
      "authors": [
        [
          "Thomas",
          "Searle"
        ],
        [
          "Zina",
          "Ibrahim"
        ],
        [
          "Richard",
          "Dobson"
        ]
      ],
      "title": "Comparing Natural Language Processing Techniques for Alzheimer&#8217;s Dementia Prediction in Spontaneous Speech",
      "original": "2729",
      "page_count": 5,
      "order": 453,
      "p1": "2192",
      "pn": "2196",
      "abstract": [
        "Alzheimer&#8217;s Dementia (AD) is an incurable, debilitating, and\nprogressive neurodegenerative condition that affects cognitive function.\nEarly diagnosis is important as therapeutics can delay progression\nand give those diagnosed vital time. Developing models that analyse\nspontaneous speech could eventually provide an efficient diagnostic\nmodality for earlier diagnosis of AD. The Alzheimer&#8217;s Dementia\nRecognition through Spontaneous Speech task offers acoustically pre-processed\nand balanced datasets for the classification and prediction of AD and\nassociated phenotypes through the modelling of spontaneous speech.\nWe exclusively analyse the supplied textual transcripts of the spontaneous\nspeech dataset, building and comparing performance across numerous\nmodels for the classification of AD vs controls and the prediction\nof Mental Mini State Exam scores. We rigorously train and evaluate\nSupport Vector Machines (SVMs), Gradient Boosting Decision Trees (GBDT),\nand Conditional Random Fields (CRFs) alongside deep learning Transformer\nbased models. We find our top performing models to be a simple Term\nFrequency-Inverse Document Frequency (TF-IDF) vectoriser as input into\na SVM model and a pre-trained Transformer based model &#8216;DistilBERT&#8217;\nwhen used as an embedding layer into simple linear models. We demonstrate\ntest set scores of 0.81&#8211;0.82 across classification metrics and\na RMSE of 4.58.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2729",
      "author_area_id": "13",
      "author_area_label": "Special Sessions"
    },
    "edwards20_interspeech": {
      "authors": [
        [
          "Erik",
          "Edwards"
        ],
        [
          "Charles",
          "Dognin"
        ],
        [
          "Bajibabu",
          "Bollepalli"
        ],
        [
          "Maneesh",
          "Singh"
        ]
      ],
      "title": "Multiscale System for Alzheimer&#8217;s Dementia Recognition Through Spontaneous Speech",
      "original": "2781",
      "page_count": 5,
      "order": 454,
      "p1": "2197",
      "pn": "2201",
      "abstract": [
        "This paper describes the Verisk submission to The ADReSS Challenge\n[1]. We analyze the text data at both the word level and phoneme level,\nwhich leads to our best-performing system in combination with audio\nfeatures. Thus, the system is both multi-modal (audio and text) and\nmulti-scale (word and phoneme levels). Experiments with larger neural\nlanguage models did not result in improvement, given the small amount\nof text data available. By contrast, the phoneme representation has\na vocabulary size of only 66 tokens and could be trained from scratch\non the present data. Therefore, we believe this method to be useful\nin cases of limited text data, as in many medical settings.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2781",
      "author_area_id": "13",
      "author_area_label": "Special Sessions"
    },
    "pompili20_interspeech": {
      "authors": [
        [
          "Anna",
          "Pompili"
        ],
        [
          "Thomas",
          "Rolland"
        ],
        [
          "Alberto",
          "Abad"
        ]
      ],
      "title": "The INESC-ID Multi-Modal System for the ADReSS 2020 Challenge",
      "original": "2833",
      "page_count": 5,
      "order": 455,
      "p1": "2202",
      "pn": "2206",
      "abstract": [
        "This paper describes a multi-modal approach for the automatic detection\nof Alzheimer&#8217;s disease proposed in the context of the INESC-ID\nHuman Language Technology Laboratory participation in the ADReSS 2020\nchallenge. Our classification framework takes advantage of both acoustic\nand textual feature embeddings, which are extracted independently and\nlater combined. Speech signals are encoded into acoustic features using\nDNN speaker embeddings extracted from pre-trained models. For textual\ninput, contextual embedding vectors are first extracted using an English\nBert model and then used either to directly compute sentence embeddings\nor to feed a bidirectional LSTM-RNNs with attention. Finally, an SVM\nclassifier with linear kernel is used for the individual evaluation\nof the three systems. Our best system, based on the combination of\nlinguistic and acoustic information, attained a classification accuracy\nof 81.25%. Results have shown the importance of linguistic features\nin the classification of Alzheimer&#8217;s Disease, which outperforms\nthe acoustic ones in terms of accuracy. Early stage features fusion\ndid not provide additional improvements, confirming that the discriminant\nability conveyed by speech in this case is smooth out by linguistic\ndata.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2833",
      "author_area_id": "13",
      "author_area_label": "Special Sessions"
    },
    "farzana20_interspeech": {
      "authors": [
        [
          "Shahla",
          "Farzana"
        ],
        [
          "Natalie",
          "Parde"
        ]
      ],
      "title": "Exploring MMSE Score Prediction Using Verbal and Non-Verbal Cues",
      "original": "3085",
      "page_count": 5,
      "order": 456,
      "p1": "2207",
      "pn": "2211",
      "abstract": [
        "The Mini Mental State Examination (MMSE) is a standardized cognitive\nhealth screening test. It is generally administered by trained clinicians,\nwhich may be time-consuming and costly. An intriguing and scalable\nalternative is to detect changes in cognitive function by automatically\nmonitoring individuals&#8217; memory and language abilities from their\nconversational narratives. We work towards doing so by predicting clinical\nMMSE scores using verbal and non-verbal features extracted from the\ntranscripts of 108 speech samples from the ADReSS Challenge dataset.\nWe achieve a Root Mean Squared Error (RMSE) of 4.34, a percentage decrease\nof 29.3% over the existing performance benchmark. We also explore the\nperformance impacts of acoustic versus linguistic, text-based features\nand find that linguistic features achieve lower RMSE scores, providing\nstrong positive support for their inclusion in future MMSE score prediction\nmodels. Our best-performing model leverages a selection of verbal and\nnon-verbal cues, demonstrating that MMSE score prediction is a rich\nproblem that is best addressed using input from multiple perspectives.\n"
      ],
      "doi": "10.21437/Interspeech.2020-3085",
      "author_area_id": "13",
      "author_area_label": "Special Sessions"
    },
    "sarawgi20_interspeech": {
      "authors": [
        [
          "Utkarsh",
          "Sarawgi"
        ],
        [
          "Wazeer",
          "Zulfikar"
        ],
        [
          "Nouran",
          "Soliman"
        ],
        [
          "Pattie",
          "Maes"
        ]
      ],
      "title": "Multimodal Inductive Transfer Learning for Detection of Alzheimer&#8217;s Dementia and its Severity",
      "original": "3137",
      "page_count": 5,
      "order": 457,
      "p1": "2212",
      "pn": "2216",
      "abstract": [
        "Alzheimer&#8217;s disease is estimated to affect around 50 million\npeople worldwide and is rising rapidly, with a global economic burden\nof nearly a trillion dollars. This calls for scalable, cost-effective,\nand robust methods for detection of Alzheimer&#8217;s dementia (AD).\nWe present a novel architecture that leverages acoustic, cognitive,\nand linguistic features to form a multimodal ensemble system. It uses\nspecialized artificial neural networks with temporal characteristics\nto detect AD and its severity, which is reflected through Mini-Mental\nState Exam (MMSE) scores. We first evaluate it on the ADReSS challenge\ndataset, which is a subject-independent and balanced dataset matched\nfor age and gender to mitigate biases, and is available through DementiaBank.\nOur system achieves state-of-the-art test accuracy, precision, recall,\nand F1-score of 83.3% each for AD classification, and state-of-the-art\ntest root mean squared error (RMSE) of 4.60 for MMSE score regression.\nTo the best of our knowledge, the system further achieves state-of-the-art\nAD classification accuracy of 88.0% when evaluated on the full benchmark\nDementiaBank Pitt database. Our work highlights the applicability and\ntransferability of spontaneous speech to produce a robust inductive\ntransfer learning model, and demonstrates generalizability through\na task-agnostic feature-space. The source code is available at <KBD>https://github.com/wazeerzulfikar/alzheimers-dementia</KBD>\n"
      ],
      "doi": "10.21437/Interspeech.2020-3137",
      "author_area_id": "13",
      "author_area_label": "Special Sessions"
    },
    "koo20_interspeech": {
      "authors": [
        [
          "Junghyun",
          "Koo"
        ],
        [
          "Jie Hwan",
          "Lee"
        ],
        [
          "Jaewoo",
          "Pyo"
        ],
        [
          "Yujin",
          "Jo"
        ],
        [
          "Kyogu",
          "Lee"
        ]
      ],
      "title": "Exploiting Multi-Modal Features from Pre-Trained Networks for Alzheimer&#8217;s Dementia Recognition",
      "original": "3153",
      "page_count": 5,
      "order": 458,
      "p1": "2217",
      "pn": "2221",
      "abstract": [
        "Collecting and accessing a large amount of medical data is very time-consuming\nand laborious, not only because it is difficult to find specific patients\nbut also because it is required to resolve the confidentiality of a\npatient&#8217;s medical records. On the other hand, there are deep\nlearning models, trained on easily collectible, large scale datasets\nsuch as Youtube or Wikipedia, offering useful representations. It could\ntherefore be very advantageous to utilize the features from these pre-trained\nnetworks for handling a small amount of data at hand. In this work,\nwe exploit various multi-modal features extracted from pre-trained\nnetworks to recognize Alzheimer&#8217;s Dementia using a neural network,\nwith a small dataset provided by the ADReSS Challenge at INTERSPEECH\n2020. The challenge regards to discern patients suspicious of Alzheimer&#8217;s\nDementia by providing acoustic and textual data. With the multi-modal\nfeatures, we modify a Convolutional Recurrent Neural Network based\nstructure to perform classification and regression tasks simultaneously\nand is capable of computing conversations with variable lengths. Our\ntest results surpass baseline&#8217;s accuracy by 18.75%, and our validation\nresult for the regression task shows the possibility of classifying\n4 classes of cognitive impairment with an accuracy of 78.70%.\n"
      ],
      "doi": "10.21437/Interspeech.2020-3153",
      "author_area_id": "13",
      "author_area_label": "Special Sessions"
    },
    "syed20_interspeech": {
      "authors": [
        [
          "Muhammad Shehram Shah",
          "Syed"
        ],
        [
          "Zafi Sherhan",
          "Syed"
        ],
        [
          "Margaret",
          "Lech"
        ],
        [
          "Elena",
          "Pirogova"
        ]
      ],
      "title": "Automated Screening for Alzheimer&#8217;s Dementia Through Spontaneous Speech",
      "original": "3158",
      "page_count": 5,
      "order": 459,
      "p1": "2222",
      "pn": "2226",
      "abstract": [
        "Dementia is a neurodegenerative disease that leads to cognitive and\n(eventually) physical impairments. Individuals who are affected by\ndementia experience deterioration in their capacity to perform day-to-day\ntasks thereby significantly affecting their quality of life. This paper\naddresses the Interspeech 2020 Alzheimer&#8217;s Dementia Recognition\nthrough Spontaneous Speech (ADReSS) challenge where the objective is\nto propose methods for two tasks. The first task is to identify speech\nrecordings from individuals with dementia amongst a set of recordings\nwhich also include those from healthy individuals. The second task\nrequires participants to estimate the Mini-Mental State Examination\n(MMSE) score based on an individual&#8217;s speech alone. To this end,\nwe investigated characteristics of speech paralinguistics such as prosody,\nvoice quality, and spectra as well as VGGish based deep acoustic embedding\nfor automated screening for dementia based on the audio modality. In\naddition to this, we also computed deep text embeddings for transcripts\nof speech. For the classification task, our method achieves an accuracy\nof 85.42% compared to the baseline of 62.50% on the test partition,\nmeanwhile, for the regression task, our method achieves an RMSE = 4.30\ncompared to the baseline of 6.14. These results show the promise of\nour proposed methods for the task of automated screening for dementia\nbased on speech alone.\n"
      ],
      "doi": "10.21437/Interspeech.2020-3158",
      "author_area_id": "13",
      "author_area_label": "Special Sessions"
    },
    "lee20c_interspeech": {
      "authors": [
        [
          "Kong Aik",
          "Lee"
        ],
        [
          "Koji",
          "Okabe"
        ],
        [
          "Hitoshi",
          "Yamamoto"
        ],
        [
          "Qiongqiong",
          "Wang"
        ],
        [
          "Ling",
          "Guo"
        ],
        [
          "Takafumi",
          "Koshinaka"
        ],
        [
          "Jiacen",
          "Zhang"
        ],
        [
          "Keisuke",
          "Ishikawa"
        ],
        [
          "Koichi",
          "Shinoda"
        ]
      ],
      "title": "NEC-TT Speaker Verification System for SRE&#8217;19 CTS Challenge",
      "original": "1132",
      "page_count": 5,
      "order": 460,
      "p1": "2227",
      "pn": "2231",
      "abstract": [
        "The series of  speaker recognition evaluations (SREs) organized by\nthe National Institute of Standards and Technology (NIST) is widely\naccepted as the de facto benchmark for speaker recognition technology.\nThis paper describes the NEC-TT speaker verification system developed\nfor the recent SRE&#8217;19 CTS Challenge. Our system is based on an\nx-vector embedding front-end followed by a thin scoring back-end. We\ntrained a very-deep neural network for x-vector extraction by incorporating\nresidual connections, squeeze-and-excitation networks, and angular-margin\nsoftmax at the output layer. We enhanced the back-end with a tandem\napproach leveraging the benefit of supervised and unsupervised domain\nadaptation. We obtained over 30% relative reduction in error rate with\neach of these enhancements at the front-end and back-end, respectively.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1132",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "li20u_interspeech": {
      "authors": [
        [
          "Ruyun",
          "Li"
        ],
        [
          "Tianyu",
          "Liang"
        ],
        [
          "Dandan",
          "Song"
        ],
        [
          "Yi",
          "Liu"
        ],
        [
          "Yangcheng",
          "Wu"
        ],
        [
          "Can",
          "Xu"
        ],
        [
          "Peng",
          "Ouyang"
        ],
        [
          "Xianwei",
          "Zhang"
        ],
        [
          "Xianhong",
          "Chen"
        ],
        [
          "Wei-Qiang",
          "Zhang"
        ],
        [
          "Shouyi",
          "Yin"
        ],
        [
          "Liang",
          "He"
        ]
      ],
      "title": "THUEE System for NIST SRE19 CTS Challenge",
      "original": "1245",
      "page_count": 5,
      "order": 461,
      "p1": "2232",
      "pn": "2236",
      "abstract": [
        "In this paper, we present the system that THUEE submitted to NIST 2019\nSpeaker Recognition Evaluation CTS Challenge (SRE19). Similar to the\nprevious SREs, domain mismatches, such as cross-lingual and cross-channel\nbetween the training sets and evaluation sets, remain the major challenges\nin this evaluation. To improve the robustness of our systems, we develop\ndeeper and wider x-vector architectures. Besides, we use novel speaker\ndiscriminative embedding systems, hybrid multi-task learning architectures\ncombined with phonetic information. To deal with domain mismatches,\nwe follow a heuristic search scheme to select the best back-end strategy\nbased on limited development corpus. An extended and factorized TDNN\nachieves the best single-system results on SRE18 DEV and SRE19 EVAL\nsets. The final system is a fusion of six subsystems, which yields\nEER 2.81% and minimum cost 0.262 on the SRE19 EVAL set.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1245",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "antipov20_interspeech": {
      "authors": [
        [
          "Grigory",
          "Antipov"
        ],
        [
          "Nicolas",
          "Gengembre"
        ],
        [
          "Olivier Le",
          "Blouch"
        ],
        [
          "Ga\u00ebl Le",
          "Lan"
        ]
      ],
      "title": "Automatic Quality Assessment for Audio-Visual Verification Systems. The  LOVe Submission to NIST SRE Challenge 2019",
      "original": "1434",
      "page_count": 5,
      "order": 462,
      "p1": "2237",
      "pn": "2241",
      "abstract": [
        "Fusion of scores is a cornerstone of multimodal biometric systems composed\nof independent unimodal parts. In this work, we focus on quality-dependent\nfusion for speaker-face verification. To this end, we propose a universal\nmodel which can be trained for automatic quality assessment of both\nface and speaker modalities. This model estimates the quality of representations\nproduced by unimodal systems which are then used to enhance the score-level\nfusion of speaker and face verification modules. We demonstrate the\nimprovements brought by this quality-dependent fusion on the recent\nNIST SRE19 Audio-Visual Challenge dataset.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1434",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "tao20b_interspeech": {
      "authors": [
        [
          "Ruijie",
          "Tao"
        ],
        [
          "Rohan Kumar",
          "Das"
        ],
        [
          "Haizhou",
          "Li"
        ]
      ],
      "title": "Audio-Visual Speaker Recognition with a Cross-Modal Discriminative Network",
      "original": "1814",
      "page_count": 5,
      "order": 463,
      "p1": "2242",
      "pn": "2246",
      "abstract": [
        "Audio-visual speaker recognition is one of the tasks in the recent\n2019 NIST speaker recognition evaluation (SRE). Studies in neuroscience\nand computer science all point to the fact that vision and auditory\nneural signals interact in the cognitive process. This motivated us\nto study a cross-modal network, namely voice-face discriminative network\n(VFNet) that establishes the general relation between human voice and\nface. Experiments show that VFNet provides additional speaker discriminative\ninformation. With VFNet, we achieve 16.54% equal error rate relative\nreduction over the score level fusion audio-visual baseline on evaluation\nset of 2019 NIST SRE.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1814",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "shon20_interspeech": {
      "authors": [
        [
          "Suwon",
          "Shon"
        ],
        [
          "James",
          "Glass"
        ]
      ],
      "title": "Multimodal Association for Speaker Verification",
      "original": "1996",
      "page_count": 5,
      "order": 464,
      "p1": "2247",
      "pn": "2251",
      "abstract": [
        "In this paper, we propose a multimodal association on a speaker verification\nsystem for fine-tuning using both voice and face. Inspired by neuroscientific\nfindings, the proposed approach is to mimic the unimodal perception\nsystem benefits from the multisensory association of stimulus pairs.\nTo verify this, we use the SRE18 evaluation protocol for experiments\nand use out-of-domain data, Voxceleb, for the proposed multimodal fine-tuning.\nAlthough the proposed approach relies on voice-face paired multimodal\ndata during the training phase, the face is no more needed after training\nis done and only speech audio is used for the speaker verification\nsystem. In the experiments, we observed that the unimodal model, i.e.\nspeaker verification model, benefits from the multimodal association\nof voice and face and generalized better than before by learning channel\ninvariant speaker representation.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1996",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "chen20h_interspeech": {
      "authors": [
        [
          "Zhengyang",
          "Chen"
        ],
        [
          "Shuai",
          "Wang"
        ],
        [
          "Yanmin",
          "Qian"
        ]
      ],
      "title": "Multi-Modality Matters: A Performance Leap on VoxCeleb",
      "original": "2229",
      "page_count": 5,
      "order": 465,
      "p1": "2252",
      "pn": "2256",
      "abstract": [
        "The information from different modalities usually compensates each\nother. In this paper, we use the audio and visual data in VoxCeleb\ndataset to do person verification. We explored different information\nfusion strategies and loss functions for the audio-visual person verification\nsystem at the embedding level. System performance is evaluated using\nthe public trail lists on VoxCeleb1 dataset. Our best system using\naudio-visual knowledge at the embedding level achieves 0.585%, 0.427%\nand 0.735% EER on the three official trial lists of VoxCeleb1, which\nare the best reported results on this dataset. Moreover, to imitate\nmore complex test environment with one modality corrupted or missing,\nwe construct a noisy evaluation set based on VoxCeleb1 dataset. We\nuse a data augmentation strategy at the embedding level to help our\naudio-visual system to distinguish the noisy and the clean embedding.\nWith such data augmented strategy, the proposed audio-visual person\nverification system is more robust on the noisy evaluation set.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2229",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "wang20w_interspeech": {
      "authors": [
        [
          "Zhenyu",
          "Wang"
        ],
        [
          "Wei",
          "Xia"
        ],
        [
          "John H.L.",
          "Hansen"
        ]
      ],
      "title": "Cross-Domain Adaptation with Discrepancy Minimization for Text-Independent Forensic Speaker Verification",
      "original": "2738",
      "page_count": 5,
      "order": 466,
      "p1": "2257",
      "pn": "2261",
      "abstract": [
        "Forensic audio analysis for speaker verification offers unique challenges\ndue to location/scenario uncertainty and diversity mismatch between\nreference and naturalistic field recordings. The lack of real naturalistic\nforensic audio corpora with ground-truth speaker identity represents\na major challenge in this field. It is also difficult to directly employ\nsmall-scale domain-specific data to train complex neural network architectures\ndue to domain mismatch and loss in performance. Alternatively, cross-domain\nspeaker verification for multiple acoustic environments is a challenging\ntask which could advance research in audio forensics. In this study,\nwe introduce a CRSS-Forensics audio dataset collected in multiple acoustic\nenvironments. We pre-train a CNN-based network using the VoxCeleb data,\nfollowed by an approach which fine-tunes part of the high-level network\nlayers with clean speech from CRSS-Forensics. Based on this fine-tuned\nmodel, we align domain-specific distributions in the embedding space\nwith the discrepancy loss and maximum mean discrepancy (MMD). This\nmaintains effective performance on the clean set, while simultaneously\ngeneralizes the model to other acoustic domains. From the results,\nwe demonstrate that diverse acoustic environments affect the speaker\nverification performance, and that our proposed approach of cross-domain\nadaptation can significantly improve the results in this scenario.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2738",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "sang20_interspeech": {
      "authors": [
        [
          "Mufan",
          "Sang"
        ],
        [
          "Wei",
          "Xia"
        ],
        [
          "John H.L.",
          "Hansen"
        ]
      ],
      "title": "Open-Set Short Utterance Forensic Speaker Verification Using Teacher-Student Network with Explicit Inductive Bias",
      "original": "2868",
      "page_count": 5,
      "order": 467,
      "p1": "2262",
      "pn": "2266",
      "abstract": [
        "In forensic applications, it is very common that only small naturalistic\ndatasets consisting of short utterances in complex or unknown acoustic\nenvironments are available. In this study, we propose a pipeline solution\nto improve speaker verification on a small actual forensic field dataset.\nBy leveraging large-scale out-of-domain datasets, a knowledge distillation\nbased objective function is proposed for teacher-student learning,\nwhich is applied for short utterance forensic speaker verification.\nThe objective function collectively considers speaker classification\nloss, Kullback-Leibler divergence, and similarity of embeddings. In\norder to advance the trained deep speaker embedding network to be robust\nfor a small target dataset, we introduce a novel strategy to fine-tune\nthe pre-trained student model towards a forensic target domain by utilizing\nthe model as a finetuning start point and a reference in regularization.\nThe proposed approaches are evaluated on the 1<SUP>st</SUP> 48-UTD\nforensic corpus, a newly established naturalistic dataset of actual\nhomicide investigations consisting of short utterances recorded in\nuncontrolled conditions. We show that the proposed objective function\ncan efficiently improve the performance of teacher-student learning\non short utterances and that our fine-tuning strategy outperforms the\ncommonly used weight decay method by providing an explicit inductive\nbias towards the pre-trained model.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2868",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "chowdhury20b_interspeech": {
      "authors": [
        [
          "Anurag",
          "Chowdhury"
        ],
        [
          "Austin",
          "Cozzo"
        ],
        [
          "Arun",
          "Ross"
        ]
      ],
      "title": "JukeBox: A Multilingual Singer Recognition Dataset",
      "original": "2972",
      "page_count": 5,
      "order": 468,
      "p1": "2267",
      "pn": "2271",
      "abstract": [
        "A text-independent speaker recognition system relies on successfully\nencoding speech factors such as vocal pitch, intensity, and timbre\nto achieve good performance. A majority of such systems are trained\nand evaluated using spoken voice or everyday conversational voice data.\nSpoken voice, however, exhibits a limited range of possible speaker\ndynamics, thus constraining the utility of the derived speaker recognition\nmodels. Singing voice, on the other hand, covers a broader range of\nvocal and ambient factors and can, therefore, be used to evaluate the\nrobustness of a speaker recognition system. However, a majority of\nexisting speaker recognition datasets only focus on the spoken voice.\nIn comparison, there is a significant shortage of labeled singing voice\ndata suitable for speaker recognition research. To address this issue,\nwe assemble  JukeBox &#8212; a speaker recognition dataset with multilingual\nsinging voice audio annotated with singer identity, gender, and language\nlabels. We use the current state-of-the-art methods to demonstrate\nthe difficulty of performing speaker recognition on singing voice using\nmodels trained on spoken voice alone. We also evaluate the effect of\ngender and language on speaker recognition performance, both in spoken\nand singing voice data. The complete  JukeBox dataset can be accessed\nat <KBD>http://iprobe.cse.msu.edu/datasets/jukebox.html</KBD>\n"
      ],
      "doi": "10.21437/Interspeech.2020-2972",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "li20v_interspeech": {
      "authors": [
        [
          "Ruirui",
          "Li"
        ],
        [
          "Jyun-Yu",
          "Jiang"
        ],
        [
          "Xian",
          "Wu"
        ],
        [
          "Chu-Cheng",
          "Hsieh"
        ],
        [
          "Andreas",
          "Stolcke"
        ]
      ],
      "title": "Speaker Identification for Household Scenarios with Self-Attention and Adversarial Training",
      "original": "3025",
      "page_count": 5,
      "order": 469,
      "p1": "2272",
      "pn": "2276",
      "abstract": [
        "Speaker identification based on voice input is a fundamental capability\nin speech processing enabling versatile downstream applications, such\nas personalization and authentication. With the advent of deep learning,\nmost state-of-the-art methods apply machine learning techniques and\nderive acoustic embeddings from utterances with convolutional neural\nnetworks (CNNs) and recurrent neural networks (RNNs). This paper addresses\ntwo inherent limitations of current approaches. First, voice characteristics\nover long time spans might not be fully captured by CNNs and RNNs,\nas they are designed to focus on local feature extraction and adjacent\ndependencies modeling, respectively. Second, complex deep learning\nmodels can be fragile with regard to subtle but intentional changes\nin model inputs, also known as adversarial perturbations. To distill\ninformative global acoustic embedding representations from utterances\nand be robust to adversarial perturbations, we propose a Self-Attentive\nAdversarial Speaker-Identification method ( SAASI). In experiments\non the VCTK dataset,  SAASI significantly outperforms four state-of-the-art\nbaselines in identifying both known and new speakers.\n"
      ],
      "doi": "10.21437/Interspeech.2020-3025",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "rybakov20_interspeech": {
      "authors": [
        [
          "Oleg",
          "Rybakov"
        ],
        [
          "Natasha",
          "Kononenko"
        ],
        [
          "Niranjan",
          "Subrahmanya"
        ],
        [
          "Mirk\u00f3",
          "Visontai"
        ],
        [
          "Stella",
          "Laurenzo"
        ]
      ],
      "title": "Streaming Keyword Spotting on Mobile Devices",
      "original": "1003",
      "page_count": 5,
      "order": 470,
      "p1": "2277",
      "pn": "2281",
      "abstract": [
        "In this work we explore the latency and accuracy of keyword spotting\n(KWS) models in streaming and non-streaming modes on mobile phones.\nNN model conversion from non-streaming mode (model receives the whole\ninput sequence and then returns the classification result) to streaming\nmode (model receives portion of the input sequence and classifies it\nincrementally) may require manual model rewriting. We address this\nby designing a Tensorflow/Keras based library which allows automatic\nconversion of non-streaming models to streaming ones with minimum effort.\nWith this library we benchmark multiple KWS models in both streaming\nand non-streaming modes on mobile phones and demonstrate different\ntradeoffs between latency and accuracy. We also explore novel KWS models\nwith multi-head attention which reduce the classification error over\nthe state-of-art by 10% on Google speech commands data sets V2. The\nstreaming library with all experiments is open-sourced.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1003",
      "author_area_id": "10",
      "author_area_label": "Speech Recognition: Technologies and Systems for New Applications"
    },
    "liu20j_interspeech": {
      "authors": [
        [
          "Hongyi",
          "Liu"
        ],
        [
          "Apurva",
          "Abhyankar"
        ],
        [
          "Yuriy",
          "Mishchenko"
        ],
        [
          "Thibaud",
          "S\u00e9n\u00e9chal"
        ],
        [
          "Gengshen",
          "Fu"
        ],
        [
          "Brian",
          "Kulis"
        ],
        [
          "Noah D.",
          "Stein"
        ],
        [
          "Anish",
          "Shah"
        ],
        [
          "Shiv Naga Prasad",
          "Vitaladevuni"
        ]
      ],
      "title": "Metadata-Aware End-to-End Keyword Spotting",
      "original": "1262",
      "page_count": 5,
      "order": 471,
      "p1": "2282",
      "pn": "2286",
      "abstract": [
        "As a crucial part of Alexa products, our on-device keyword spotting\nsystem detects the wakeword in conversation and initiates subsequent\nuser-device interactions. Convolutional neural networks (CNNs) have\nbeen widely used to model the relationship between time and frequency\nin the audio spectrum. However, it is not obvious how to appropriately\nleverage the rich descriptive information from device state metadata\n(such as player state, device type, volume, etc) in a CNN architecture.\nIn this paper, we propose to use metadata information as an additional\ninput feature to improve the performance of a single CNN keyword -spotting\nmodel under different conditions. We design a new network architecture\nfor metadata-aware end-to-end keyword spotting which learns to convert\nthe categorical metadata to a fixed length embedding, and then uses\nthe embedding to: 1) modulate convolutional feature maps via conditional\nbatch normalization, and 2) contribute to the fully connected layer\nvia feature concatenation. The experiment shows that the proposed architecture\nis able to learn the meta-specific characteristics from combined datasets,\nand the best candidate achieves an average relative false reject rate\n(FRR) improvement of 14.63% at the same false accept rate (FAR) compared\nwith CNN that does not use device state metadata.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1262",
      "author_area_id": "10",
      "author_area_label": "Speech Recognition: Technologies and Systems for New Applications"
    },
    "kong20_interspeech": {
      "authors": [
        [
          "Yehao",
          "Kong"
        ],
        [
          "Jiliang",
          "Zhang"
        ]
      ],
      "title": "Adversarial Audio: A New Information Hiding Method",
      "original": "1294",
      "page_count": 5,
      "order": 472,
      "p1": "2287",
      "pn": "2291",
      "abstract": [
        "Audio is an important medium in people&#8217;s daily life, hidden information\ncan be embedded into audio for covert communication. Current audio\ninformation hiding techniques can be roughly classified into time domain-based\nand transform domain-based techniques. Time domain-based techniques\nhave large hiding capacity but low imperceptibility. Transform domain-based\ntechniques have better imperceptibility, but the hiding capacity is\npoor. This paper proposes a new audio information hiding technique\nwhich shows high hiding capacity and good imperceptibility. The proposed\naudio information hiding method takes the original audio signal as\ninput and obtains the audio signal embedded with hidden information\n(called stego audio) through the training of our private DNN-based\nautomatic speech recognition (ASR) model. The experimental results\nshow that the proposed audio information hiding technique has a high\nhiding capacity of 48 cps with good imperceptibility and high security.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1294",
      "author_area_id": "10",
      "author_area_label": "Speech Recognition: Technologies and Systems for New Applications"
    },
    "wang20x_interspeech": {
      "authors": [
        [
          "Xinsheng",
          "Wang"
        ],
        [
          "Tingting",
          "Qiao"
        ],
        [
          "Jihua",
          "Zhu"
        ],
        [
          "Alan",
          "Hanjalic"
        ],
        [
          "Odette",
          "Scharenborg"
        ]
      ],
      "title": "S2IGAN: Speech-to-Image Generation via Adversarial Learning",
      "original": "1759",
      "page_count": 5,
      "order": 473,
      "p1": "2292",
      "pn": "2296",
      "abstract": [
        "An estimated half of the world&#8217;s languages do not have a written\nform, making it impossible for these languages to benefit from any\nexisting text-based technologies. In this paper, a speech-to-image\ngeneration (S2IG) framework is proposed which translates speech descriptions\nto photo-realistic images without using any text information, thus\nallowing unwritten languages to potentially benefit from this technology.\nThe proposed S2IG framework, named S2IGAN, consists of a speech embedding\nnetwork (SEN) and a relation-supervised densely-stacked generative\nmodel (RDG). SEN learns the speech embedding with the supervision of\nthe corresponding visual information. Conditioned on the speech embedding\nproduced by SEN, the proposed RDG synthesizes images that are semantically\nconsistent with the corresponding speech descriptions. Extensive experiments\non datasets CUB and Oxford-102 demonstrate the effectiveness of the\nproposed S2IGAN on synthesizing high-quality and semantically-consistent\nimages from the speech signal, yielding a good performance and a solid\nbaseline for the S2IG task.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1759",
      "author_area_id": "10",
      "author_area_label": "Speech Recognition: Technologies and Systems for New Applications"
    },
    "zuluagagomez20_interspeech": {
      "authors": [
        [
          "Juan",
          "Zuluaga-Gomez"
        ],
        [
          "Petr",
          "Motlicek"
        ],
        [
          "Qingran",
          "Zhan"
        ],
        [
          "Karel",
          "Vesel\u00fd"
        ],
        [
          "Rudolf",
          "Braun"
        ]
      ],
      "title": "Automatic Speech Recognition Benchmark for Air-Traffic Communications",
      "original": "2173",
      "page_count": 5,
      "order": 474,
      "p1": "2297",
      "pn": "2301",
      "abstract": [
        "Advances in Automatic Speech Recognition (ASR) over the last decade\nopened new areas of speech-based automation such as in Air-Traffic\nControl (ATC) environments. Currently, voice communication and data\nlinks communications are the only way of contact between pilots and\nAir-Traffic Controllers (ATCo), where the former is the most widely\nused and the latter is a non-spoken method mandatory for oceanic messages\nand limited for some domestic issues. ASR systems on ATCo environments\ninherit increasing complexity due to accents from non-English speakers,\ncockpit noise, speaker-dependent biases and small in-domain ATC databases\nfor training. Hereby, we introduce CleanSky EC-H2020 ATCO2, a project\nthat aims to develop an ASR-based platform to collect, organize and\nautomatically pre-process ATCo speech-data from air space. This paper\nconveys an exploratory benchmark of several state-of-the-art ASR models\ntrained on more than 170 hours of ATCo speech-data. We demonstrate\nthat the cross-accent flaws due to speakers&#8217; accents are minimized\ndue to the amount of data, making the system feasible for ATC environments.\nThe developed ASR system achieves an averaged word error rate (WER)\nof 7.75% across four databases. An additional 35% relative improvement\nin WER is achieved on one test set when training a TDNNF system with\nbyte-pair encoding.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2173",
      "author_area_id": "10",
      "author_area_label": "Speech Recognition: Technologies and Systems for New Applications"
    },
    "gudepu20_interspeech": {
      "authors": [
        [
          "Prithvi R.R.",
          "Gudepu"
        ],
        [
          "Gowtham P.",
          "Vadisetti"
        ],
        [
          "Abhishek",
          "Niranjan"
        ],
        [
          "Kinnera",
          "Saranu"
        ],
        [
          "Raghava",
          "Sarma"
        ],
        [
          "M. Ali Basha",
          "Shaik"
        ],
        [
          "Periyasamy",
          "Paramasivam"
        ]
      ],
      "title": "Whisper Augmented End-to-End/Hybrid Speech Recognition System &#8212; CycleGAN Approach",
      "original": "2639",
      "page_count": 5,
      "order": 475,
      "p1": "2302",
      "pn": "2306",
      "abstract": [
        "Automatic speech recognition (ASR) systems are known to perform poorly\nunder whispered speech conditions. One of the primary reasons is the\nlack of large annotated whisper corpora. To address this challenge,\nwe propose data augmentation with synthetic whisper corpus generated\nfrom normal speech using Cycle-Consistent Generative Adversarial Network\n(CycleGAN). We train CycleGAN model with a limited corpus of parallel\nwhispered and normal speech, aligned using Dynamic Time Warping (DTW).\nThe model learns frame-wise mapping from feature vectors of normal\nspeech to those of whisper. We then augment ASR systems with the generated\nsynthetic whisper corpus. In this paper, we validate our proposed approach\nusing state-of-the-art end-to-end (E2E) and hybrid ASR systems trained\non publicly available Librispeech, wTIMIT and internally recorded far-field\ncorpora. We achieved 23% relative reduction in word error rate (WER)\ncompared to baseline on whisper test sets. In addition, we also achieved\nWER reductions on Librispeech and far-field test sets.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2639",
      "author_area_id": "10",
      "author_area_label": "Speech Recognition: Technologies and Systems for New Applications"
    },
    "sawhney20_interspeech": {
      "authors": [
        [
          "Ramit",
          "Sawhney"
        ],
        [
          "Arshiya",
          "Aggarwal"
        ],
        [
          "Piyush",
          "Khanna"
        ],
        [
          "Puneet",
          "Mathur"
        ],
        [
          "Taru",
          "Jain"
        ],
        [
          "Rajiv Ratn",
          "Shah"
        ]
      ],
      "title": "Risk Forecasting from Earnings Calls Acoustics and Network Correlations",
      "original": "2649",
      "page_count": 5,
      "order": 476,
      "p1": "2307",
      "pn": "2311",
      "abstract": [
        "Stock volatility is a degree of deviations from expected returns, and\nthus, estimates risk, which is crucial for investment decision making.\nVolatility forecasting is complex given the stochastic nature of market\nmicrostructure, where we use frenzied data over various modalities\nto make temporally dependent forecasts. Transcripts of earnings calls\nof companies are well studied for risk modeling as they offer unique\ninvestment insight into stock performance. Anecdotal evidence shows\ncompany CEO&#8217;s vocal cues could be indicative of the stock performance.\nThe recently developing body of work on analyzing earnings calls treat\nstocks as independent of each other, thus not using rich relations\nbetween stocks. To this end, we introduce the first neural model that\nemploys cross inter-modal attention for deep verbal-vocal coherence\nand accounts for stock interdependence through multi-layer network\nembeddings. We show that our approach outperforms state-of-the-art\nmethods by augmenting speech features with correlations from text and\nstock network modalities. Lastly, we analyse the components and financial\nimplications of our method through an ablation and case study.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2649",
      "author_area_id": "10",
      "author_area_label": "Speech Recognition: Technologies and Systems for New Applications"
    },
    "chen20i_interspeech": {
      "authors": [
        [
          "Huili",
          "Chen"
        ],
        [
          "Bita",
          "Darvish"
        ],
        [
          "Farinaz",
          "Koushanfar"
        ]
      ],
      "title": "SpecMark: A Spectral Watermarking Framework for IP Protection of Speech Recognition Systems",
      "original": "2787",
      "page_count": 5,
      "order": 477,
      "p1": "2312",
      "pn": "2316",
      "abstract": [
        "Automatic Speech Recognition (ASR) systems are widely deployed in various\napplications due to their superior performance. However, obtaining\na highly accurate ASR model is non-trivial since it requires the availability\nof a massive amount of proprietary training data and enormous computational\nresources. As such, pre-trained ASR models shall be considered as the\nintellectual property (IP) of the model designer and protected against\ncopyright infringement attacks. In this paper, we propose SpecMark,\nthe first spectral watermarking framework that seamlessly embeds a\n watermark (WM) in the spectrum of the ASR model for  ownership proof.\nSpecMark identifies the significant frequency components of the model\nparameters and encodes the owner&#8217;s WM in the corresponding spectrum\nregion before sharing the model with end-users. The model builder can\nlater extract the spectral WM to verify his ownership of the marked\nASR system. We evaluate SpecMark&#8217;s performance using DeepSpeech\nmodel with three different speech datasets. Empirical results corroborate\nthat SpecMark incurs negligible overhead and preserves the recognition\naccuracy of the original system. Furthermore, SpecMark sustains diverse\nmodel modifications, including parameter pruning and transfer learning.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2787",
      "author_area_id": "10",
      "author_area_label": "Speech Recognition: Technologies and Systems for New Applications"
    },
    "hout20_interspeech": {
      "authors": [
        [
          "Justin van der",
          "Hout"
        ],
        [
          "Zolt\u00e1n",
          "D\u2019Haese"
        ],
        [
          "Mark",
          "Hasegawa-Johnson"
        ],
        [
          "Odette",
          "Scharenborg"
        ]
      ],
      "title": "Evaluating Automatically Generated Phoneme Captions for Images",
      "original": "2870",
      "page_count": 5,
      "order": 478,
      "p1": "2317",
      "pn": "2321",
      "abstract": [
        "Image2Speech is the relatively new task of generating a spoken description\nof an image. This paper presents an investigation into the evaluation\nof this task. For this, first an Image2Speech system was implemented\nwhich generates image captions consisting of phoneme sequences. This\nsystem outperformed the original Image2Speech system on the Flickr8k\ncorpus. Subsequently, these phoneme captions were converted into sentences\nof words. The captions were rated by human evaluators for their goodness\nof describing the image. Finally, several objective metric scores of\nthe results were correlated with these human ratings. Although BLEU4\ndoes not perfectly correlate with human ratings, it obtained the highest\ncorrelation among the investigated metrics, and is the best currently\nexisting metric for the Image2Speech task. Current metrics are limited\nby the fact that they assume their input to be words. A more appropriate\nmetric for the Image2Speech task should assume its input to be parts\nof words, i.e. phonemes, instead.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2870",
      "author_area_id": "10",
      "author_area_label": "Speech Recognition: Technologies and Systems for New Applications"
    },
    "lin20d_interspeech": {
      "authors": [
        [
          "Wei-Cheng",
          "Lin"
        ],
        [
          "Carlos",
          "Busso"
        ]
      ],
      "title": "An Efficient Temporal Modeling Approach for Speech Emotion Recognition by Mapping Varied Duration Sentences into Fixed Number of Chunks",
      "original": "2636",
      "page_count": 5,
      "order": 479,
      "p1": "2322",
      "pn": "2326",
      "abstract": [
        " Speech emotion recognition (SER) plays an important role in multiple\nfields such as healthcare,  human-computer interaction (HCI), and security\nand defense. Emotional labels are often annotated at the sentence-level\n(i.e., one label per sentence), resulting in a sequence-to-one recognition\nproblem. Traditionally, studies have relied on statistical descriptions,\nwhich are computed over time from  low level descriptors (LLDs), creating\na fixed dimension sentence-level feature representation regardless\nof the duration of the sentence. However sentence-level features lack\ntemporal information, which limits the performance of SER systems.\nRecently, new deep learning architectures have been proposed to model\ntemporal data. An important question is how to extract emotion-relevant\nfeatures with temporal information. This study proposes a novel data\nprocessing approach that extracts a fixed number of small chunks over\nsentences of different durations by changing the overlap between these\nchunks. The approach is flexible, providing an ideal framework to combine\ngated network or attention mechanisms with  long short-term memory\n(LSTM) networks. Our experimental results based on the MSP-Podcast\ndataset demonstrate that the proposed method not only significantly\nimproves recognition accuracy over alternative temporal-based models\nrelying on LSTM, but also leads to computational efficiency.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2636",
      "author_area_id": "3",
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "latif20b_interspeech": {
      "authors": [
        [
          "Siddique",
          "Latif"
        ],
        [
          "Rajib",
          "Rana"
        ],
        [
          "Sara",
          "Khalifa"
        ],
        [
          "Raja",
          "Jurdak"
        ],
        [
          "Bj\u00f6rn W.",
          "Schuller"
        ]
      ],
      "title": "Deep Architecture Enhancing Robustness to Noise, Adversarial Attacks, and Cross-Corpus Setting for Speech Emotion Recognition",
      "original": "3190",
      "page_count": 5,
      "order": 480,
      "p1": "2327",
      "pn": "2331",
      "abstract": [
        "Speech emotion recognition systems (SER) can achieve high accuracy\nwhen the training and test data are identically distributed, but this\nassumption is frequently violated in practice and the performance of\nSER systems plummet against unforeseen data shifts. The design of robust\nmodels for accurate SER is challenging, which limits its use in practical\napplications. In this paper we propose a deeper neural network architecture\nwherein we fuse Dense Convolutional Network (DenseNet), Long short-term\nmemory (LSTM) and Highway Network to learn powerful discriminative\nfeatures which are robust to noise. We also propose data augmentation\nwith our network architecture to further improve the robustness. We\ncomprehensively evaluate the architecture coupled with data augmentation\nagainst (1) noise, (2) adversarial attacks and (3) cross-corpus settings.\nOur evaluations on the widely used IEMOCAP and MSP-IMPROV datasets\nshow promising results when compared with existing studies and state-of-the-art\nmodels.\n"
      ],
      "doi": "10.21437/Interspeech.2020-3190",
      "author_area_id": "3",
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "fujioka20_interspeech": {
      "authors": [
        [
          "Takuya",
          "Fujioka"
        ],
        [
          "Takeshi",
          "Homma"
        ],
        [
          "Kenji",
          "Nagamatsu"
        ]
      ],
      "title": "Meta-Learning for Speech Emotion Recognition Considering Ambiguity of Emotion Labels",
      "original": "1082",
      "page_count": 5,
      "order": 481,
      "p1": "2332",
      "pn": "2336",
      "abstract": [
        "Emotion labels in emotion recognition corpora are highly noisy and\nambiguous, due to the annotators&#8217; subjective perception of emotions.\nSuch ambiguity may introduce errors in automatic classification and\naffect the overall performance. We therefore propose a dynamic label\ncorrection and sample contribution weight estimation model. Our model\nis based on a standard BLSTM model with attention with two extra parameters.\nThe first learns a new corrected label distribution and aims to fix\nthe inaccurate labels in the dataset. The other estimates the contribution\nof each sample to the training process and aims to ignore the ambiguous\nand noisy samples while giving higher weights to the clear ones. We\ntrain our model through an alternating optimization method, where in\nthe first epoch we update the neural network parameters, and in the\nsecond we keep them fixed to update the label correction and sample\nimportance parameters. When training and evaluating our model on the\nIEMOCAP dataset, we obtained a weighted accuracy (WA) and unweighted\naccuracy (UA) of 65.9% and 61.4%, respectively. This yielded an absolute\nimprovement of 2.3% and 1.9%, respectively, compared to a BLSTM with\nattention baseline, trained on the corpus gold labels.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1082",
      "author_area_id": "3",
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "liu20k_interspeech": {
      "authors": [
        [
          "Jiaxing",
          "Liu"
        ],
        [
          "Zhilei",
          "Liu"
        ],
        [
          "Longbiao",
          "Wang"
        ],
        [
          "Yuan",
          "Gao"
        ],
        [
          "Lili",
          "Guo"
        ],
        [
          "Jianwu",
          "Dang"
        ]
      ],
      "title": "Temporal Attention Convolutional Network for Speech Emotion Recognition with Latent Representation",
      "original": "1520",
      "page_count": 5,
      "order": 482,
      "p1": "2337",
      "pn": "2341",
      "abstract": [
        "As the fundamental research of affective computing, speech emotion\nrecognition (SER) has gained a lot of attention. Unlike with common\ndeep learning tasks, SER was restricted by the scarcity of emotional\nspeech datasets. In this paper, the vector quantization variational\nautomatic encoder (VQ-VAE) was introduced and trained by massive unlabeled\ndata in an unsupervised manner. Benefiting from the excellent invariant\ndistribution encoding capability and discrete embedding space of VQ-VAE,\nthe pre-trained VQ-VAE could learn latent representation from labeled\ndata. The extracted latent representation could serve as the additional\nsource data to make data abundantly available. While solving data lacking\nissue, sequence information modeling was also taken into account which\nwas considered useful for SER. The proposed sequence model, temporal\nattention convolutional network (TACN) was simple yet good at learning\ncontextual information from limited data which was not friendly to\ncomplicated structures of recurrent neural network (RNN) based sequence\nmodels. To validate the effectiveness of the latent representation,\nt-distributed stochastic neighbor embedding (t-SNE) was introduced\nto analyze the visualizations. To verify the performance of the proposed\nTACN, quantitative classification results of all commonly used sequence\nmodels were provided. Our proposed model achieved state-of-the-art\nperformance on IEMOCAP.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1520",
      "author_area_id": "3",
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "zhu20_interspeech": {
      "authors": [
        [
          "Zhi",
          "Zhu"
        ],
        [
          "Yoshinao",
          "Sato"
        ]
      ],
      "title": "Reconciliation of Multiple Corpora for Speech Emotion Recognition by Multiple Classifiers with an Adversarial Corpus Discriminator",
      "original": "1618",
      "page_count": 5,
      "order": 483,
      "p1": "2342",
      "pn": "2346",
      "abstract": [
        "Research on affective computing has achieved remarkable success with\nthe development of deep learning. One of the major difficulties in\nemotion recognition is inconsistent criteria for emotion categorization\nbetween multiple corpora. Most previous studies using multiple corpora\ndiscard or merge a part of their emotion classes. This prescription\ncauses catastrophic information loss with respect to emotion categorization.\nFurthermore, the influences of corpus-specific factors other than emotions,\nsuch as languages, speech registers, and recording environments, should\nbe eliminated to fully utilize multiple corpora. In this paper, we\naddress the challenge of reconciling multiple emotion corpora by learning\na corpus-independent emotion encoding disentangled from all the remaining\nfactors without causing catastrophic information loss. For this purpose,\nwe propose a model that consists of a shared emotion encoder, multiple\nemotion classifiers, and an adversarial corpus discriminator. This\nmodel is trained with multi-task learning harnessed by adversarial\nlearning. We conducted speech emotion classification experiments with\nour method on two corpora, namely, EmoDB and CREMA-D. The results demonstrate\nthat our method achieves higher accuracies than mono-corpus models.\nIn addition, it is indicated that the proposed method suppresses corpus-dependent\nfactors other than emotions in the embedding space.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1618",
      "author_area_id": "3",
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "lian20c_interspeech": {
      "authors": [
        [
          "Zheng",
          "Lian"
        ],
        [
          "Jianhua",
          "Tao"
        ],
        [
          "Bin",
          "Liu"
        ],
        [
          "Jian",
          "Huang"
        ],
        [
          "Zhanlei",
          "Yang"
        ],
        [
          "Rongjun",
          "Li"
        ]
      ],
      "title": "Conversational Emotion Recognition Using Self-Attention Mechanisms and Graph Neural Networks",
      "original": "1703",
      "page_count": 5,
      "order": 484,
      "p1": "2347",
      "pn": "2351",
      "abstract": [
        "Different from the emotion estimation in individual utterances, context-sensitive\nand speaker-sensitive dependences are vitally pivotal for conversational\nemotion analysis. In this paper, we propose a graph-based neural network\nto model these dependences. Specifically, our approach represents each\nutterance and each speaker as a node. To bridge the context-sensitive\ndependence, each utterance node has edges between immediate utterances\nfrom the same conversation. Meanwhile, the directed edges between each\nutterance node and its speaker node bridge the speaker-sensitive dependence.\nTo verify the effectiveness of our strategy, we conduct experiments\non the MELD dataset. Experimental results demonstrate that our method\nshows an absolute improvement of 1%&#126;2% over state-of-the-art strategies.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1703",
      "author_area_id": "3",
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "mao20c_interspeech": {
      "authors": [
        [
          "Shuiyang",
          "Mao"
        ],
        [
          "P.C.",
          "Ching"
        ],
        [
          "Tan",
          "Lee"
        ]
      ],
      "title": "EigenEmo: Spectral Utterance Representation Using Dynamic Mode Decomposition for Speech Emotion Classification",
      "original": "1762",
      "page_count": 5,
      "order": 485,
      "p1": "2352",
      "pn": "2356",
      "abstract": [
        "Human emotional speech is, by its very nature, a variant signal. This\nresults in dynamics intrinsic to automatic emotion classification based\non speech. In this work, we explore a spectral decomposition method\nstemming from fluid-dynamics, known as Dynamic Mode Decomposition (DMD),\nto computationally represent and analyze the global utterance-level\ndynamics of emotional speech. Specifically, segment-level emotion-specific\nrepresentations are first learned through an Emotion Distillation process.\nThis forms a multi-dimensional signal of emotion flow for each utterance,\ncalled Emotion Profiles (EPs). The DMD algorithm is then applied to\nthe resultant EPs to capture the eigenfrequencies, and hence the fundamental\ntransition dynamics of the emotion flow. Evaluation experiments using\nthe proposed approach, which we call EigenEmo, show promising results.\nMoreover, due to the positive combination of their complementary properties,\nconcatenating the utterance representations generated by EigenEmo with\nsimple EPs averaging yields noticeable gains.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1762",
      "author_area_id": "3",
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "mao20d_interspeech": {
      "authors": [
        [
          "Shuiyang",
          "Mao"
        ],
        [
          "P.C.",
          "Ching"
        ],
        [
          "C.-C. Jay",
          "Kuo"
        ],
        [
          "Tan",
          "Lee"
        ]
      ],
      "title": "Advancing Multiple Instance Learning with Attention Modeling for Categorical Speech Emotion Recognition",
      "original": "1779",
      "page_count": 5,
      "order": 486,
      "p1": "2357",
      "pn": "2361",
      "abstract": [
        "Categorical speech emotion recognition is typically performed as a\nsequence-to-label problem, i. e., to determine the discrete emotion\nlabel of the input utterance as a whole. One of the main challenges\nin practice is that most of the existing emotion corpora do not give\nground truth labels for each segment; instead, we only have labels\nfor whole utterances. To extract segment-level emotional information\nfrom such weakly labeled emotion corpora, we propose using multiple\ninstance learning (MIL) to learn segment embeddings in a weakly supervised\nmanner. Also, for a sufficiently long utterance, not all of the segments\ncontain relevant emotional information. In this regard, three attention-based\nneural network models are then applied to the learned segment embeddings\nto attend the most salient part of a speech utterance. Experiments\non the CASIA corpus and the IEMOCAP database show better or highly\ncompetitive results than other state-of-the-art approaches.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1779"
    },
    "perezramon20_interspeech": {
      "authors": [
        [
          "Rub\u00e9n",
          "P\u00e9rez-Ram\u00f3n"
        ],
        [
          "Mar\u00eda Luisa Garc\u00eda",
          "Lecumberri"
        ],
        [
          "Martin",
          "Cooke"
        ]
      ],
      "title": "The Effect of Language Proficiency on the Perception of Segmental Foreign Accent",
      "original": "1023",
      "page_count": 5,
      "order": 487,
      "p1": "2362",
      "pn": "2366",
      "abstract": [
        "Foreign accent has different effects on speech intelligibility for\nnative and non-native listeners. However, not much is known about the\nimpact of individual foreign-accented segments on listeners with different\nlevels of proficiency in the language. Using a technique developed\nto generate degrees of segmental foreign accent, this study investigates\nhow native and non-native listeners differing in language proficiency\ncategorise and discriminate degrees of accentedness at the segmental\nlevel. Listeners responded to continua ranging from Spanish-accented\ntokens to English tokens, constructed by inserting accented segments\ninto words. Six continua were chosen, based on known problems faced\nby Spanish speakers of English. Whether foreign accent categorisation\nperformance differed across native and non-native listeners was found\nto depend on the status of the segment in the listeners&#8217; first\nlanguage. For certain sounds both high and low proficiency non-native\ngroups resembled native listener responses. For other sounds, categorisation\nrevealed a clear effect of proficiency, with the high-proficiency group\ncloser to native performance than the low proficiency cohort. This\nbehaviour indicates an ongoing process of new second language phonemic\ncategory creation by the more proficient learners.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1023",
      "author_area_id": "1",
      "author_area_label": "Speech Perception, Production and Acquisition"
    },
    "liu20l_interspeech": {
      "authors": [
        [
          "Yi",
          "Liu"
        ],
        [
          "Jinghong",
          "Ning"
        ]
      ],
      "title": "The Effect of Language Dominance on the Selective Attention of Segments and Tones in Urdu-Cantonese Speakers",
      "original": "1678",
      "page_count": 5,
      "order": 488,
      "p1": "2367",
      "pn": "2371",
      "abstract": [
        "To perceive a second language (L2), non-native speakers not only have\nto focus on phonological, lexical and grammatical knowledge, but also\nneed to develop a good mastery of L2 strategic knowledge, including\nselective attention and language planning. Previous research has found\nthat non-tonal speakers are overtly attentive to segments, while tonal\nlanguage speakers give more attention to tones. However, it is unclear\nhow different dominant language speakers distribute their attention\nwhen processing both segments and tones in non-native speeches. In\nthe current study Cantonese native speakers, Cantonese-dominants, and\nUrdu-dominants participated in an attention distribution experiment\nin Cantonese. The results show that the Urdu-dominants retain their\nL1 attentional strategy in the processing of Cantonese stimuli, classifying\nthe stimuli along segments, while the Cantonese native speakers are\nmore attentive to tones. Moreover, the Cantonese-dominants perform\neither in monolingual mode or bilingual mode according to different\ntasks, showing a perceptual flexibility in highly proficient and experienced\nlisteners. The results reveal that language dominance plays a vital\nrole in listeners&#8217; attention distribution. The research also\nsupports the ASP model and hypothesis on bilinguals, proposed by [1].\n"
      ],
      "doi": "10.21437/Interspeech.2020-1678",
      "author_area_id": "1",
      "author_area_label": "Speech Perception, Production and Acquisition"
    },
    "li20w_interspeech": {
      "authors": [
        [
          "Mengrou",
          "Li"
        ],
        [
          "Ying",
          "Chen"
        ],
        [
          "Jie",
          "Cui"
        ]
      ],
      "title": "The Effect of Input on the Production of English Tense and Lax Vowels by Chinese Learners: Evidence from an Elementary School in China",
      "original": "2595",
      "page_count": 5,
      "order": 489,
      "p1": "2372",
      "pn": "2376",
      "abstract": [
        "In both American and British English, tense high vowels /i/ and /u/\nshow extreme positions of the tongue and lips in articulation rather\nthan their lax counterparts /&#618;/ and /&#650;/. However, the tenseness\ncontrast in English is taught to Chinese learners in classroom by most\ninstructors as duration difference &#8212; /i/ and /u/ are longer than\n/&#618;/ and /&#650;/ respectively. The present study therefore examines\nEnglish production of /i/ vs. /&#618;/ and /u/ vs. /&#650;/ by Chinese\nelementary students and investigates how L2 beginners actually realize\nthe target vowels and how their production resembles that of their\nclassroom instructors and talkers who recorded their teaching materials.\nThe results show that the students differentiated /i/ from /&#618;/\nand /u/ from /&#650;/ mainly in duration and marginally in F2 but not\nin F1. Their production was found closer to their English teacher&#8217;s\nthan the textbook recordings&#8217; and native English speakers&#8217;,\nsuggesting the input from the teachers significantly affects the English\nproduction of elementary school students in China.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2595",
      "author_area_id": "1",
      "author_area_label": "Speech Perception, Production and Acquisition"
    },
    "spinu20_interspeech": {
      "authors": [
        [
          "Laura",
          "Spinu"
        ],
        [
          "Jiwon",
          "Hwang"
        ],
        [
          "Nadya",
          "Pincus"
        ],
        [
          "Mariana",
          "Vasilita"
        ]
      ],
      "title": "Exploring the Use of an Artificial Accent of English to Assess Phonetic Learning in Monolingual and Bilingual Speakers",
      "original": "2783",
      "page_count": 5,
      "order": 490,
      "p1": "2377",
      "pn": "2381",
      "abstract": [
        "We designed a production experiment to explore the relatively controversial\nphenomenon of the bilingual advantage. Our focus is on an understudied\naspect of bilingual cognition, specifically phonetic learning. We presented\n36 participants (17 monolinguals and 19 early bilinguals) living in\nNew York City with an artificially constructed accent of English, differing\nin four ways from Standard American English. More precisely, the novel\naccent included a vocalic change (diphthongization of the open-mid\nfront unrounded vowel), consonantal change (tapping of intervocalic\nliquids), syllable structure change (epenthesis in voiceless s-clusters)\nand suprasegmental change (a novel intonation pattern in tag questions).\nAfter recording their baseline accents, the participants first completed\na training task, in which they listened to and then directly imitated\nsentences heard in the novel accent, and then a testing task, in which\nthey were asked to read the baseline sentences in the accent they had\njust learned in the absence of any audio prompts. In this paper, we\npresent acoustic results with diphthongization and tag question intonation.\nOur findings replicate the previously observed bilingual advantage\nin phonetic learning across the board and extend it to novel learning\ncircumstances. \n"
      ],
      "doi": "10.21437/Interspeech.2020-2783",
      "author_area_id": "1",
      "author_area_label": "Speech Perception, Production and Acquisition"
    },
    "chowdhury20c_interspeech": {
      "authors": [
        [
          "Shammur A.",
          "Chowdhury"
        ],
        [
          "Younes",
          "Samih"
        ],
        [
          "Mohamed",
          "Eldesouki"
        ],
        [
          "Ahmed",
          "Ali"
        ]
      ],
      "title": "Effects of Dialectal Code-Switching on Speech Modules: A Study Using Egyptian Arabic Broadcast Speech",
      "original": "2271",
      "page_count": 5,
      "order": 491,
      "p1": "2382",
      "pn": "2386",
      "abstract": [
        "The intra-utterance code-switching (CS) is defined as the alternation\nbetween two or more languages within the same utterance. Despite the\nfact that spoken dialectal code-switching (DCS) is more challenging\nthan CS, it remains largely unexplored. In this study, we describe\na method to build the first spoken DCS corpus. The corpus is annotated\nat the token-level minding both linguistic and acoustic cues for dialectal\nArabic. For detailed analysis, we study Arabic automatic speech recognition\n(ASR), Arabic dialect identification (ADI), and natural language processing\n(NLP) modules for the DCS corpus. Our results highlight the importance\nof lexical information for discriminating the DCS labels. We observe\nthat the performance of different models is highly dependent on the\ndegree of code-mixing at the token-level as well as its complexity\nat the utterance-level.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2271",
      "author_area_id": "1",
      "author_area_label": "Speech Perception, Production and Acquisition"
    },
    "johnson20_interspeech": {
      "authors": [
        [
          "Khia A.",
          "Johnson"
        ],
        [
          "Molly",
          "Babel"
        ],
        [
          "Robert A.",
          "Fuhrman"
        ]
      ],
      "title": "Bilingual Acoustic Voice Variation is Similarly Structured Across Languages",
      "original": "3095",
      "page_count": 5,
      "order": 492,
      "p1": "2387",
      "pn": "2391",
      "abstract": [
        "When a bilingual switches languages, do they switch their &#8220;voice&#8221;?\nUsing a new conversational corpus of speech from early Cantonese-English\nbilinguals (N = 34), this paper examines the talker-specific acoustic\nsignature of bilingual voices. Following prior work in voice quality\nvariation, 24 filter and source-based acoustic measurements are estimated.\nThe analysis summarizes mean differences for these dimensions, in addition\nto identifying the underlying structure of each talker&#8217;s voice\nacross languages with principal components analyses. Canonical redundancy\nanalyses demonstrate that while talkers vary in the degree to which\nthey have the same &#8220;voice&#8221; across languages, all talkers\nshow strong similarity with themselves.\n"
      ],
      "doi": "10.21437/Interspeech.2020-3095",
      "author_area_id": "1",
      "author_area_label": "Speech Perception, Production and Acquisition"
    },
    "zhang20r_interspeech": {
      "authors": [
        [
          "Haobo",
          "Zhang"
        ],
        [
          "Haihua",
          "Xu"
        ],
        [
          "Van Tung",
          "Pham"
        ],
        [
          "Hao",
          "Huang"
        ],
        [
          "Eng Siong",
          "Chng"
        ]
      ],
      "title": "Monolingual Data Selection Analysis for English-Mandarin Hybrid Code-Switching Speech Recognition",
      "original": "1582",
      "page_count": 5,
      "order": 493,
      "p1": "2392",
      "pn": "2396",
      "abstract": [
        "In this paper, we conduct data selection analysis in building an English-Mandarin\ncode-switching (CS) speech recognition (CSSR) system, which is aimed\nfor a real CSSR contest in China. The overall training sets have three\nsubsets, i.e., a code-switching data set, an English (LibriSpeech)\nand a Mandarin data set respectively. The code-switching data are Mandarin\ndominated. First of all, it is found using the overall data yields\nworse results, and hence data selection study is necessary. Then to\nexploit monolingual data, we find data matching is crucial. Mandarin\ndata is closely matched with the Mandarin part in the code-switching\ndata, while English data is not. However, Mandarin data only helps\non those utterances that are significantly Mandarin-dominated. Besides,\nthere is a balance point, over which more monolingual data will divert\nthe CSSR system, degrading results. Finally, we analyze the effectiveness\nof combining monolingual data to train a CSSR system with the HMM-DNN\nhybrid framework. The CSSR system can perform within-utterance code-switch\nrecognition, but it still has a margin with the one trained on code-switching\ndata.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1582",
      "author_area_id": "1",
      "author_area_label": "Speech Perception, Production and Acquisition"
    },
    "du20b_interspeech": {
      "authors": [
        [
          "Dan",
          "Du"
        ],
        [
          "Xianjin",
          "Zhu"
        ],
        [
          "Zhu",
          "Li"
        ],
        [
          "Jinsong",
          "Zhang"
        ]
      ],
      "title": "Perception and Production of Mandarin Initial Stops by Native Urdu Speakers",
      "original": "1921",
      "page_count": 5,
      "order": 494,
      "p1": "2397",
      "pn": "2401",
      "abstract": [
        "In the area of second language (L2) acquisition, studies of stop consonants\nhave focused on those first language (L1) and L2 with a two-way stop\ncontrast and a three-way stop contrast, few are about languages with\na four-way stop contrast. The current study, mainly concerning VOT,\ninvestigates how native speakers of a language with a four-way stop\ncontrast acquire the two-way stop contrast in L2. Mandarin presents\na two-way stop contrast, which is primarily differentiated by VOT,\nwhereas Urdu presents a four-way stop contrast. Speech perception and\nproduction experiments are designed to explore L2 learners with a more\ncomplex language system learning a relatively simple language system,\nand the results show that the speech perception of Mandarin initial\nstops by native Urdu speakers has no significant difference compared\nwith those by Chinese speakers while the speech production by native\nUrdu speakers is significantly different from those by Chinese speakers.\nIt demonstrates that L1 exerts different influences on L2 perception\nand production separately and sometimes good L2 perception doesn&#8217;t\nmean good L2 production. This study has made some contribution about\nthe four-way stop contrast in L2 acquisition and will shed light on\nL2 learning.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1921",
      "author_area_id": "1",
      "author_area_label": "Speech Perception, Production and Acquisition"
    },
    "afouras20_interspeech": {
      "authors": [
        [
          "Triantafyllos",
          "Afouras"
        ],
        [
          "Joon Son",
          "Chung"
        ],
        [
          "Andrew",
          "Zisserman"
        ]
      ],
      "title": "Now You&#8217;re Speaking My Language: Visual Language Identification",
      "original": "2921",
      "page_count": 5,
      "order": 495,
      "p1": "2402",
      "pn": "2406",
      "abstract": [
        "The goal of this work is to train models that can identify a spoken\nlanguage just by interpreting the speaker&#8217;s lip movements. Our\ncontributions are the following: (i) we show that models can learn\nto discriminate among 14 different languages using only visual speech\ninformation; (ii) we compare different designs in sequence modelling\nand utterance-level aggregation in order to determine the best architecture\nfor this task; (iii) we investigate the factors that contribute discriminative\ncues and show that our model indeed solves the problem by finding temporal\npatterns in mouth movements and not by exploiting spurious correlations.\nWe demonstrate this further by evaluating our models on challenging\nexamples from bilingual speakers.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2921",
      "author_area_id": "1",
      "author_area_label": "Speech Perception, Production and Acquisition"
    },
    "rhee20_interspeech": {
      "authors": [
        [
          "Nari",
          "Rhee"
        ],
        [
          "Jianjing",
          "Kuang"
        ]
      ],
      "title": "The Different Enhancement Roles of Covarying Cues in Thai and Mandarin Tones",
      "original": "1685",
      "page_count": 5,
      "order": 496,
      "p1": "2407",
      "pn": "2411",
      "abstract": [
        "Any phonological contrast is distinguished by multiple covarying cues.\nThe role and nature of the cue covariation have been much debated in\nthe literature, identifying the physiological link in production, perceptual\nintegration, and enhancement as the key factors in play. In this study,\nwe test whether the enhancement role of covarying cues are influenced\nby yet another factor: the interaction between cues at multiple layers\nof phonological structure. We hypothesize that further enhancement\nof the cue covariations occurs in contexts where acoustic cues are\nin competition for phonological contrasts at multiple prosodic levels.\nTo test this, we investigate the enhancement role of the covariation\nrelationship between F0 and spectral cues in Mandarin and Thai tones\nin different phrasal positions. Exploratory and multidimensional-scaling\nanalyses suggest that in Mandarin, the phrase-final weakening of F0\ncues are compensated by enhancing the spectral cues, while in Thai,\ntonal category-specific enhancement is observed at all phrasal positions.\nThe context- and category-specific enhancement of covarying cues suggests\nthat the language&#8217;s phonological structure plays an important\nrole in the fine-tuning of cue covariations.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1685",
      "author_area_id": "1",
      "author_area_label": "Speech Perception, Production and Acquisition"
    },
    "shi20c_interspeech": {
      "authors": [
        [
          "Hao",
          "Shi"
        ],
        [
          "Longbiao",
          "Wang"
        ],
        [
          "Sheng",
          "Li"
        ],
        [
          "Chenchen",
          "Ding"
        ],
        [
          "Meng",
          "Ge"
        ],
        [
          "Nan",
          "Li"
        ],
        [
          "Jianwu",
          "Dang"
        ],
        [
          "Hiroshi",
          "Seki"
        ]
      ],
      "title": "Singing Voice Extraction with Attention-Based Spectrograms Fusion",
      "original": "1043",
      "page_count": 5,
      "order": 497,
      "p1": "2412",
      "pn": "2416",
      "abstract": [
        "We propose a novel attention mechanism-based spectrograms fusion system\nwith minimum difference masks (MDMs) estimation for singing voice extraction.\nCompared with previous works that use a fully connected neural network,\nour system takes advantage of the multi-head attention mechanism. Specifically,\nwe 1) try a variety of embedding methods of multiple spectrograms as\nthe input of attention mechanisms, which can provide multi-scale correlation\ninformation between adjacent frames in the spectrograms; 2) add a regular\nterm to loss function to obtain better continuity of spectrogram; 3)\nuse the phase of the linear fusion waveform to reconstruct the final\nwaveform, which can reduce the impact of the inconsistent spectrogram.\nExperiments on the MIR-1K dataset show that our system consistently\nimproves the quantitative evaluation by the perceptual evaluation of\nspeech quality, signal-to-distortion ratio, signal-to-interference\nratio, and signal-to-artifact ratio.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1043",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "lu20d_interspeech": {
      "authors": [
        [
          "Yen-Ju",
          "Lu"
        ],
        [
          "Chien-Feng",
          "Liao"
        ],
        [
          "Xugang",
          "Lu"
        ],
        [
          "Jeih-weih",
          "Hung"
        ],
        [
          "Yu",
          "Tsao"
        ]
      ],
      "title": "Incorporating Broad Phonetic Information for Speech Enhancement",
      "original": "1400",
      "page_count": 5,
      "order": 498,
      "p1": "2417",
      "pn": "2421",
      "abstract": [
        "In noisy conditions, knowing speech contents facilitates listeners\nto more effectively suppress background noise components and to retrieve\npure speech signals. Previous studies have also confirmed the benefits\nof incorporating phonetic information in a speech enhancement (SE)\nsystem to achieve better denoising performance. To obtain the phonetic\ninformation, we usually prepare a phoneme-based acoustic model, which\nis trained using speech waveforms and phoneme labels. Despite performing\nwell in normal noisy conditions, when operating in very noisy conditions,\nhowever, the recognized phonemes may be erroneous and thus misguide\nthe SE process. To overcome the limitation, this study proposes to\nincorporate the broad phonetic class (BPC) information into the SE\nprocess. We have investigated three criteria to build the BPC, including\ntwo knowledge-based criteria: place and manner of articulatory and\none data-driven criterion. Moreover, the recognition accuracies of\nBPCs are much higher than that of phonemes, thus providing more accurate\nphonetic information to guide the SE process under very noisy conditions.\nExperimental results demonstrate that the proposed SE with the BPC\ninformation framework can achieve notable performance improvements\nover the baseline system and an SE system using monophonic information\nin terms of both speech quality intelligibility on the TIMIT dataset.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1400",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "li20x_interspeech": {
      "authors": [
        [
          "Andong",
          "Li"
        ],
        [
          "Chengshi",
          "Zheng"
        ],
        [
          "Cunhang",
          "Fan"
        ],
        [
          "Renhua",
          "Peng"
        ],
        [
          "Xiaodong",
          "Li"
        ]
      ],
      "title": "A Recursive Network with Dynamic Attention for Monaural Speech Enhancement",
      "original": "1513",
      "page_count": 5,
      "order": 499,
      "p1": "2422",
      "pn": "2426",
      "abstract": [
        "For continuous speech processing, dynamic attention is helpful in preferential\nprocessing, which has already been shown by the auditory dynamic attending\ntheory. Accordingly, we propose a framework combining dynamic attention\nand recursive learning together called DARCN for monaural speech enhancement.\nApart from a major noise reduction network, we design a separated sub-network,\nwhich adaptively generates the attention distribution to control the\ninformation flow throughout the major network. Recursive learning is\nintroduced to dynamically reduce the number of trainable parameters\nby reusing a network for multiple stages, where the intermediate output\nin each stage is refined with a memory mechanism. By doing so, a more\nflexible and better estimation can be obtained. We conduct experiments\non TIMIT corpus. Experimental results show that the proposed architecture\nobtains consistently better performance than recent state-of-the-art\nmodels in terms of both PESQ and STOI scores.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1513",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "yu20d_interspeech": {
      "authors": [
        [
          "Hongjiang",
          "Yu"
        ],
        [
          "Wei-Ping",
          "Zhu"
        ],
        [
          "Yuhong",
          "Yang"
        ]
      ],
      "title": "Constrained Ratio Mask for Speech Enhancement Using DNN",
      "original": "1920",
      "page_count": 5,
      "order": 500,
      "p1": "2427",
      "pn": "2431",
      "abstract": [
        "Speech enhancement has found many applications concerning robust speech\nprocessing. A masking based algorithm, as an important method of speech\nenhancement, aims to retain the speech dominant components and suppress\nthe noise dominant parts of the noisy speech. In this paper, we derive\na new type of mask: constrained ratio mask (CRM), which can better\ncontrol the trade-off between speech distortion and residual noise\nin the enhanced speech. A deep neural network (DNN) is then employed\nfor CRM estimation in noisy conditions. The estimated CRM is finally\napplied to the noisy speech for denoising. Experimental results show\nthat the enhanced speech from the new masking scheme yields an improved\nspeech quality over three existing masks under various noisy conditions.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1920",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "lee20d_interspeech": {
      "authors": [
        [
          "Chi-Chang",
          "Lee"
        ],
        [
          "Yu-Chen",
          "Lin"
        ],
        [
          "Hsuan-Tien",
          "Lin"
        ],
        [
          "Hsin-Min",
          "Wang"
        ],
        [
          "Yu",
          "Tsao"
        ]
      ],
      "title": "SERIL: Noise Adaptive Speech Enhancement Using Regularization-Based Incremental Learning",
      "original": "2213",
      "page_count": 5,
      "order": 501,
      "p1": "2432",
      "pn": "2436",
      "abstract": [
        "Numerous noise adaptation techniques have been proposed to fine-tune\ndeep-learning models in speech enhancement (SE) for mismatched noise\nenvironments. Nevertheless, adaptation to a new environment may lead\nto catastrophic forgetting of the previously learned environments.\nThe catastrophic forgetting issue degrades the performance of SE in\nreal-world embedded devices, which often revisit previous noise environments.\nThe nature of embedded devices does not allow solving the issue with\nadditional storage of all pre-trained models or earlier training data.\nIn this paper, we propose a regularization-based incremental learning\nSE (SERIL) strategy, complementing existing noise adaptation strategies\nwithout using additional storage. With a regularization constraint,\nthe parameters are updated to the new noise environment while retaining\nthe knowledge of the previous noise environments. The experimental\nresults show that, when faced with a new noise domain, the SERIL model\noutperforms the unadapted SE model. Meanwhile, compared with the current\nadaptive technique based on fine-tuning, the SERIL model can reduce\nthe forgetting of previous noise environments by 52%. The results verify\nthat the SERIL model can effectively adjust itself to new noise environments\nwhile overcoming the catastrophic forgetting issue. The results make\nSERIL a favorable choice for real-world SE applications, where the\nnoise environment changes frequently.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2213",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "bando20_interspeech": {
      "authors": [
        [
          "Yoshiaki",
          "Bando"
        ],
        [
          "Kouhei",
          "Sekiguchi"
        ],
        [
          "Kazuyoshi",
          "Yoshii"
        ]
      ],
      "title": "Adaptive Neural Speech Enhancement with a Denoising Variational Autoencoder",
      "original": "2291",
      "page_count": 5,
      "order": 502,
      "p1": "2437",
      "pn": "2441",
      "abstract": [
        "This paper presents a neural speech enhancement method that has a statistical\nfeedback mechanism based on a denoising variational autoencoder (VAE).\nDeep generative models of speech signals have been combined with unsupervised\nnoise models for enhancing speech robustly regardless of the condition\nmismatch from the training data. This approach, however, often yields\nunnatural speech-like noise due to the unsuitable prior distribution\non the latent speech representations. To mitigate this problem, we\nuse a denoising VAE whose encoder estimates the latent vectors of clean\nspeech from an input mixture signal. This encoder network is utilized\nas a prior distribution of the probabilistic generative model of the\ninput mixture, and its condition mismatch is handled in a Bayesian\nmanner. The speech signal is estimated by updating the latent vectors\nto fit the input mixture while noise is estimated by a nonnegative\nmatrix factorization model. To efficiently train the encoder network,\nwe also propose a multi-task learning of the denoising VAE with the\nstandard mask-based enhancement. The experimental results show that\nour method outperforms the existing mask-based and generative enhancement\nmethods in unknown conditions.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2291",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "bulut20_interspeech": {
      "authors": [
        [
          "Ahmet E.",
          "Bulut"
        ],
        [
          "Kazuhito",
          "Koishida"
        ]
      ],
      "title": "Low-Latency Single Channel Speech Dereverberation Using U-Net Convolutional Neural Networks",
      "original": "2421",
      "page_count": 5,
      "order": 503,
      "p1": "2442",
      "pn": "2446",
      "abstract": [
        "Speech signal reverberation due to reflections in a physical obstacle\nis one of the main difficulties in speech processing as well as the\npresence of non-stationary background noise. In this study we explore\nDNN-based single-channel speech dereverberation with state-of-the-art\nperformance comparisons. We propose a CNN auto-encoder architecture\nwith skip connections focusing on real-time and low-latency applications.\nThe proposed system is evaluated with the REVERB challenge dataset\nthat includes simulated and real reverberated speech samples. Our experimental\nresults show that the proposed system has superior results on the challenge\nevaluation dataset as opposed to a baseline system that uses deep neural\nnetwork (DNN) based weighted prediction error (WPE) algorithm. We also\nextend the comparison with state of the art systems in terms of most\ncommonly used objective metrics and our system achieves better results\nin the most of objective metrics. Moreover a latency analysis of the\nproposed system is performed and trade-off between processing time\nand performance is examined.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2421",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "tran20b_interspeech": {
      "authors": [
        [
          "Dung N.",
          "Tran"
        ],
        [
          "Kazuhito",
          "Koishida"
        ]
      ],
      "title": "Single-Channel Speech Enhancement by Subspace Affinity Minimization",
      "original": "2982",
      "page_count": 5,
      "order": 504,
      "p1": "2447",
      "pn": "2451",
      "abstract": [
        "In data-driven speech enhancement frameworks, learning informative\nrepresentations is crucial to obtain a high-quality estimate of the\ntarget speech. State-of-the-art speech enhancement methods based on\ndeep neural networks (DNN) commonly learn a single embedding from the\nnoisy input to predict clean speech. This compressed representation\ninevitably contains both noise and speech information leading to speech\ndistortion and poor noise reduction performance. To alleviate this\nissue, we proposed to learn from the noisy input separate embeddings\nfor speech and noise and introduced a subspace affinity loss function\nto prevent information leaking between the two representations. We\nrigorously proved that minimizing this loss function yields maximally\nuncorrelated speech and noise representations, which can block information\nleaking. We empirically showed that our proposed framework outperforms\ntraditional and state-of-the-art speech enhancement methods in various\nunseen nonstationary noise environments. Our results suggest that learning\nuncorrelated speech and noise embeddings can improve noise reduction\nand reduces speech distortion in speech enhancement applications.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2982",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "li20y_interspeech": {
      "authors": [
        [
          "Haoyu",
          "Li"
        ],
        [
          "Junichi",
          "Yamagishi"
        ]
      ],
      "title": "Noise Tokens: Learning Neural Noise Templates for Environment-Aware Speech Enhancement",
      "original": "1030",
      "page_count": 5,
      "order": 505,
      "p1": "2452",
      "pn": "2456",
      "abstract": [
        "In recent years, speech enhancement (SE) has achieved impressive progress\nwith the success of deep neural networks (DNNs). However, the DNN approach\nusually fails to generalize well to unseen environmental noise that\nis not included in the training. To address this problem, we propose\n&#8220;noise tokens&#8221; (NTs), which are a set of neural noise templates\nthat are jointly trained with the SE system. NTs dynamically capture\nthe environment variability and thus enable the DNN model to handle\nvarious environments to produce STFT magnitude with higher quality.\nExperimental results show that using NTs is an effective strategy that\nconsistently improves the generalization ability of SE systems across\ndifferent DNN architectures. Furthermore, we investigate applying a\nstate-of-the-art neural vocoder to generate waveform instead of traditional\ninverse STFT (ISTFT). Subjective listening tests show the residual\nnoise can be significantly suppressed through mel-spectrogram correction\nand vocoder-based waveform synthesis.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1030",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "deng20_interspeech": {
      "authors": [
        [
          "Feng",
          "Deng"
        ],
        [
          "Tao",
          "Jiang"
        ],
        [
          "Xiao-Rui",
          "Wang"
        ],
        [
          "Chen",
          "Zhang"
        ],
        [
          "Yan",
          "Li"
        ]
      ],
      "title": "NAAGN: Noise-Aware Attention-Gated Network for Speech Enhancement",
      "original": "1133",
      "page_count": 5,
      "order": 506,
      "p1": "2457",
      "pn": "2461",
      "abstract": [
        "For single channel speech enhancement, contextual information is very\nimportant for accurate speech estimation. In this paper, to capture\nlong-term temporal contexts, we treat speech enhancement as a sequence-to-sequence\nmapping problem, and propose a noise-aware attention-gated network\n(NAAGN) for speech enhancement. Firstly, by incorporating deep residual\nlearning and dilated convolutions into U-Net architecture, we present\na deep residual U-net (ResUNet), which significantly expand receptive\nfields to aggregate context information systematically. Secondly, the\nattention-gated (AG) network is integrated into the ResUNet architecture\nwith minimal computational overhead while furtherly increasing the\nlong-term contexts sensitivity and prediction accuracy. Thirdly, we\npropose a novel noise-aware multi-task loss function, named weighted\nmean absolute error (WMAE) loss, in which both speech estimation loss\nand noise prediction loss are taken into consideration. Finally, the\nproposed NAAGN model was evaluated on the Voice Bank corpus and DEMAND\ndatabase, which have been widely applied for speech enhancement by\nlots of deep learning models. Experimental results indicate that the\nproposed NAAGN method can achieve a larger segmental SNR improvement,\na better speech quality and a higher speech intelligibility than reference\nmethods.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1133",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "li20z_interspeech": {
      "authors": [
        [
          "Xiaofei",
          "Li"
        ],
        [
          "Radu",
          "Horaud"
        ]
      ],
      "title": "Online Monaural Speech Enhancement Using Delayed Subband LSTM",
      "original": "2091",
      "page_count": 5,
      "order": 507,
      "p1": "2462",
      "pn": "2466",
      "abstract": [
        "This paper proposes a delayed subband LSTM network for online monaural\n(single-channel) speech enhancement. The proposed method is developed\nin the short time Fourier transform (STFT) domain. Online processing\nrequires frame-by-frame signal reception and processing. A paramount\nfeature of the proposed method is that the same LSTM is used across\nfrequencies, which drastically reduces the number of network parameters,\nthe amount of training data and the computational burden. Training\nis performed in a subband manner: the input consists of a frequency\ntogether with a few context frequencies. The network learns a speech-to-noise\ndiscriminative function relying on the signal stationarity and on the\nlocal spectral pattern, based on which it predicts a clean-speech mask\nat each frequency. To exploit future information, i.e. a look-ahead\nstrategy, we propose an output-delayed subband LSTM network, which\nallows the unidirectional forward network to use a few future frames\nto process the current frame. We leverage the proposed method to participate\nto the DNS real-time speech enhancement challenge. Experiments with\nthe DNS dataset show that the proposed method achieves better performance-measuring\nscores than the DNS baseline method, which learns the full-band spectra\nusing a gated recurrent unit network.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2091",
      "author_area_id": "13",
      "author_area_label": "Special Sessions"
    },
    "strake20_interspeech": {
      "authors": [
        [
          "Maximilian",
          "Strake"
        ],
        [
          "Bruno",
          "Defraene"
        ],
        [
          "Kristoff",
          "Fluyt"
        ],
        [
          "Wouter",
          "Tirry"
        ],
        [
          "Tim",
          "Fingscheidt"
        ]
      ],
      "title": "INTERSPEECH 2020 Deep Noise Suppression Challenge: A Fully Convolutional Recurrent Network (FCRN) for Joint Dereverberation and Denoising",
      "original": "2439",
      "page_count": 5,
      "order": 508,
      "p1": "2467",
      "pn": "2471",
      "abstract": [
        "The Interspeech 2020 Deep Noise Suppression (DNS) Challenge focuses\non evaluating low-latency single-channel speech enhancement algorithms\nunder realistic test conditions. Our contribution to the challenge\nis a method for joint dereverberation and denoising based on complex\nspectral mask estimation using a fully convolutional recurrent network\n(FCRN) which relies on a convolutional LSTM layer for temporal modeling.\nSince the effects of reverberation and noise on perceived speech quality\ncan differ notably, a multi-target loss for controlling the weight\non desired dereverberation and denoising is proposed. In the crowdsourced\nsubjective P.808 listening test conducted by the DNS Challenge organizers,\nthe proposed method shows a significant overall improvement of 0.43\nMOS points over the DNS Challenge baseline and ranks amongst the top-3\nsubmissions for both realtime and non-realtime tracks of the challenge.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2439",
      "author_area_id": "13",
      "author_area_label": "Special Sessions"
    },
    "hu20g_interspeech": {
      "authors": [
        [
          "Yanxin",
          "Hu"
        ],
        [
          "Yun",
          "Liu"
        ],
        [
          "Shubo",
          "Lv"
        ],
        [
          "Mengtao",
          "Xing"
        ],
        [
          "Shimin",
          "Zhang"
        ],
        [
          "Yihui",
          "Fu"
        ],
        [
          "Jian",
          "Wu"
        ],
        [
          "Bihong",
          "Zhang"
        ],
        [
          "Lei",
          "Xie"
        ]
      ],
      "title": "DCCRN: Deep Complex Convolution Recurrent Network for Phase-Aware Speech Enhancement",
      "original": "2537",
      "page_count": 5,
      "order": 509,
      "p1": "2472",
      "pn": "2476",
      "abstract": [
        "Speech enhancement has benefited from the success of deep learning\nin terms of intelligibility and perceptual quality. Conventional time-frequency\n(TF) domain methods focus on predicting TF-masks or speech spectrum,\nvia a naive convolution neural network (CNN) or recurrent neural network\n(RNN). Some recent studies use complex-valued spectrogram as a training\ntarget but train in a real-valued network, predicting the magnitude\nand phase component or real and imaginary part, respectively. Particularly,\nconvolution recurrent network (CRN) integrates a convolutional encoder-decoder\n(CED) structure and long short-term memory (LSTM), which has been proven\nto be helpful for complex targets. In order to train the complex target\nmore effectively, in this paper, we design a new network structure\nsimulating the complex-valued operation, called Deep Complex Convolution\nRecurrent Network (DCCRN), where both CNN and RNN structures can handle\ncomplex-valued operation. The proposed DCCRN models are very competitive\nover other previous networks, either on objective or subjective metric.\nWith only 3.7M parameters, our DCCRN models submitted to the Interspeech\n2020 Deep Noise Suppression (DNS) challenge ranked first for the real-time-track\nand second for the non-real-time track in terms of Mean Opinion Score\n(MOS).\n"
      ],
      "doi": "10.21437/Interspeech.2020-2537",
      "author_area_id": "13",
      "author_area_label": "Special Sessions"
    },
    "westhausen20_interspeech": {
      "authors": [
        [
          "Nils L.",
          "Westhausen"
        ],
        [
          "Bernd T.",
          "Meyer"
        ]
      ],
      "title": "Dual-Signal Transformation LSTM Network for Real-Time Noise Suppression",
      "original": "2631",
      "page_count": 5,
      "order": 510,
      "p1": "2477",
      "pn": "2481",
      "abstract": [
        "This paper introduces a dual-signal transformation LSTM network (DTLN)\nfor real-time speech enhancement as part of the Deep Noise Suppression\nChallenge (DNS-Challenge). This approach combines a short-time Fourier\ntransform (STFT) and a learned analysis and synthesis basis in a stacked-network\napproach with less than one million parameters. The model was trained\non 500 h of noisy speech provided by the challenge organizers. The\nnetwork is capable of real-time processing (one frame in, one frame\nout) and reaches competitive results. Combining these two types of\nsignal transformations enables the DTLN to robustly extract information\nfrom magnitude spectra and incorporate phase information from the learned\nfeature basis. The method shows state-of-the-art performance and outperforms\nthe DNS-Challenge baseline by 0.24 points absolute in terms of the\nmean opinion score (MOS).\n"
      ],
      "doi": "10.21437/Interspeech.2020-2631",
      "author_area_id": "13",
      "author_area_label": "Special Sessions"
    },
    "valin20_interspeech": {
      "authors": [
        [
          "Jean-Marc",
          "Valin"
        ],
        [
          "Umut",
          "Isik"
        ],
        [
          "Neerad",
          "Phansalkar"
        ],
        [
          "Ritwik",
          "Giri"
        ],
        [
          "Karim",
          "Helwani"
        ],
        [
          "Arvindh",
          "Krishnaswamy"
        ]
      ],
      "title": "A Perceptually-Motivated Approach for Low-Complexity, Real-Time Enhancement of Fullband Speech",
      "original": "2730",
      "page_count": 5,
      "order": 511,
      "p1": "2482",
      "pn": "2486",
      "abstract": [
        "Over the past few years, speech enhancement methods based on deep learning\nhave greatly surpassed traditional methods based on spectral subtraction\nand spectral estimation. Many of these new techniques operate directly\nin the the short-time Fourier transform (STFT) domain, resulting in\na high computational complexity. In this work, we propose PercepNet,\nan efficient approach that relies on human perception of speech by\nfocusing on the spectral envelope and on the periodicity of the speech.\nWe demonstrate high-quality, real-time enhancement of fullband (48\nkHz) speech with less than 5% of a CPU core.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2730",
      "author_area_id": "13",
      "author_area_label": "Special Sessions"
    },
    "isik20_interspeech": {
      "authors": [
        [
          "Umut",
          "Isik"
        ],
        [
          "Ritwik",
          "Giri"
        ],
        [
          "Neerad",
          "Phansalkar"
        ],
        [
          "Jean-Marc",
          "Valin"
        ],
        [
          "Karim",
          "Helwani"
        ],
        [
          "Arvindh",
          "Krishnaswamy"
        ]
      ],
      "title": "PoCoNet: Better Speech Enhancement with Frequency-Positional Embeddings, Semi-Supervised Conversational Data, and Biased Loss",
      "original": "3027",
      "page_count": 5,
      "order": 512,
      "p1": "2487",
      "pn": "2491",
      "abstract": [
        "Neural network applications generally benefit from larger-sized models,\nbut for current speech enhancement models, larger scale networks often\nsuffer from decreased robustness to the variety of real-world use cases\nbeyond what is encountered in training data. We introduce several innovations\nthat lead to better large neural networks for speech enhancement. The\nnovel PoCoNet architecture is a convolutional neural network that,\nwith the use of frequency-positional embeddings, is able to more efficiently\nbuild frequency-dependent features in the early layers. A semi-supervised\nmethod helps increase the amount of conversational training data by\npre-enhancing noisy datasets, improving performance on real recordings.\nA new loss function biased towards preserving speech quality helps\nthe optimization better match human perceptual opinions on speech quality.\nAblation experiments and objective and human opinion metrics show the\nbenefits of the proposed improvements.\n"
      ],
      "doi": "10.21437/Interspeech.2020-3027",
      "author_area_id": "13",
      "author_area_label": "Special Sessions"
    },
    "reddy20_interspeech": {
      "authors": [
        [
          "Chandan K.A.",
          "Reddy"
        ],
        [
          "Vishak",
          "Gopal"
        ],
        [
          "Ross",
          "Cutler"
        ],
        [
          "Ebrahim",
          "Beyrami"
        ],
        [
          "Roger",
          "Cheng"
        ],
        [
          "Harishchandra",
          "Dubey"
        ],
        [
          "Sergiy",
          "Matusevych"
        ],
        [
          "Robert",
          "Aichner"
        ],
        [
          "Ashkan",
          "Aazami"
        ],
        [
          "Sebastian",
          "Braun"
        ],
        [
          "Puneet",
          "Rana"
        ],
        [
          "Sriram",
          "Srinivasan"
        ],
        [
          "Johannes",
          "Gehrke"
        ]
      ],
      "title": "The INTERSPEECH 2020 Deep Noise Suppression Challenge: Datasets, Subjective Testing Framework, and Challenge Results",
      "original": "3038",
      "page_count": 5,
      "order": 513,
      "p1": "2492",
      "pn": "2496",
      "abstract": [
        "The INTERSPEECH 2020 Deep Noise Suppression (DNS) Challenge is intended\nto promote collaborative research in real-time single-channel Speech\nEnhancement aimed to maximize the subjective (perceptual) quality of\nthe enhanced speech. A typical approach to evaluate the noise suppression\nmethods is to use objective metrics on the test set obtained by splitting\nthe original dataset. While the performance is good on the synthetic\ntest set, often the model performance degrades significantly on real\nrecordings. Also, most of the conventional objective metrics do not\ncorrelate well with subjective tests and lab subjective tests are not\nscalable for a large test set. In this challenge, we open-sourced a\nlarge clean speech and noise corpus for training the noise suppression\nmodels and a representative test set to real-world scenarios consisting\nof both synthetic and real recordings. We also open-sourced an online\nsubjective test framework based on ITU-T P.808 for researchers to reliably\ntest their developments. We evaluated the results using P.808 on a\nblind test set. The results and the key learnings from the challenge\nare discussed.<br style=\"mso-data-placement:same-cell;\" /><br style=\"mso-data-placement:same-cell;\"\n/>\n  The datasets and scripts can be found here for quick access <KBD>https://github.com/microsoft/DNS-Challenge</KBD>\n"
      ],
      "doi": "10.21437/Interspeech.2020-3038",
      "author_area_id": "13",
      "author_area_label": "Special Sessions"
    },
    "akbarzadeh20_interspeech": {
      "authors": [
        [
          "Sara",
          "Akbarzadeh"
        ],
        [
          "Sungmin",
          "Lee"
        ],
        [
          "Chin-Tuan",
          "Tan"
        ]
      ],
      "title": "The Implication of Sound Level on Spatial Selective Auditory Attention for Cochlear Implant Users: Behavioral and Electrophysiological Measurement",
      "original": "2836",
      "page_count": 5,
      "order": 514,
      "p1": "2497",
      "pn": "2501",
      "abstract": [
        "This study investigated the effect of sound level on the spatial selective\nauditory attention of Normal Hearing (NH) and Cochlear Implant (CI)\nlisteners behaviorally and electrophysiologically. Three spatially\nseparated speech streams consisting of one target and two maskers were\npresented to the subjects. While keeping the same target to masker\nratio, the target stimuli were presented at three different sound levels.\nIn the behavioral test, subjects were instructed to attend the target\nspeech, and speech perception score was calculated based on correctly\nrecognized words in percentage. In the electrophysiological test, electroencephalography\n(EEG) signals were recorded while the subjects were listening for target\nspeech in the presence of two maskers. The attended speech envelope\nwas reconstructed from EEG using a linear regression model. The spatial\nauditory attention was detected through comparison of the reconstructed\nspeech envelope with the original envelopes associated with the three\nspeech streams presented. Outcome of both behavioral and electrophysiological\nexperiments showed that an increase in the sound level decreases the\nspatial speech recognition performance of CI listeners, but not NH\nlisteners.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2836",
      "author_area_id": "1",
      "author_area_label": "Speech Perception, Production and Acquisition"
    },
    "wan20b_interspeech": {
      "authors": [
        [
          "Yangyang",
          "Wan"
        ],
        [
          "Huali",
          "Zhou"
        ],
        [
          "Qinglin",
          "Meng"
        ],
        [
          "Nengheng",
          "Zheng"
        ]
      ],
      "title": "Enhancing the Interaural Time Difference of Bilateral Cochlear Implants with the Temporal Limits Encoder",
      "original": "2507",
      "page_count": 5,
      "order": 515,
      "p1": "2502",
      "pn": "2506",
      "abstract": [
        "Normal hearing listeners mainly use interaural time differences (ITDs)\nand interaural level differences (ILDs) to localize sound sources in\nthe horizontal plane. Listeners with bilateral cochlear implants (CIs),\nhowever, have poor sensitivity to ITDs which significantly limits their\nspatial hearing capabilities. Most CI signal processing strategies,\nsuch as the continuous interleaved sampling (CIS) strategy, are temporal-envelope-based,\nand the temporal fine structure (TFS), which contains useful cues for\nITDs, is discarded. Recently, a temporal limits encoder (TLE) CI strategy\nwas proposed to implicitly introduce the TFS while preserving the temporal\nenvelope. It has demonstrated benefits in unilateral CI simulations\nin tasks including speech-in-noise understanding and pitch perception.\nThis study investigates whether the ITD cues could be enhanced by a\nbilateral TLE strategy. Identification of five ITDs respectively associated\nwith five sound source directions was tested with vocoded speech stimuli\nto compare the performance of the bilateral TLE and CIS strategies.\nResults show that the bilateral TLE has better overall performance\nthan the bilateral CIS. This finding suggests that the bilateral TLE\nis promising in providing enhanced ITD cues for bilateral CI users.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2507",
      "author_area_id": "1",
      "author_area_label": "Speech Perception, Production and Acquisition"
    },
    "irino20_interspeech": {
      "authors": [
        [
          "Toshio",
          "Irino"
        ],
        [
          "Soichi",
          "Higashiyama"
        ],
        [
          "Hanako",
          "Yoshigi"
        ]
      ],
      "title": "Speech Clarity Improvement by Vocal Self-Training Using a Hearing Impairment Simulator and its Correlation with an Auditory Modulation Index",
      "original": "1081",
      "page_count": 5,
      "order": 516,
      "p1": "2507",
      "pn": "2511",
      "abstract": [
        "We performed two experiments to ascertain whether vocal self-training\nimproves speech clarity, particularly when the feedback speech is degraded\nby a hearing impairment simulator. Speech sounds before and after the\ntraining were recorded under noisy and quiet conditions and their speech\nclarity was evaluated by subjective listening tests using Scheffe&#8217;s\npaired comparison. We also analyzed the auditory modulation features\nto derive an index to explain the subjective clarity scores. The auditory\nmodulation index highly correlated with subjective scores and seems\na good candidate for predicting speech clarity.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1081",
      "author_area_id": "1",
      "author_area_label": "Speech Perception, Production and Acquisition"
    },
    "zhang20s_interspeech": {
      "authors": [
        [
          "Zhuohuang",
          "Zhang"
        ],
        [
          "Donald S.",
          "Williamson"
        ],
        [
          "Yi",
          "Shen"
        ]
      ],
      "title": "Investigation of Phase Distortion on Perceived Speech Quality for Hearing-Impaired Listeners",
      "original": "1481",
      "page_count": 5,
      "order": 517,
      "p1": "2512",
      "pn": "2516",
      "abstract": [
        "Phase serves as a critical component of speech that influences the\nquality and intelligibility. Current speech enhancement algorithms\nare beginning to address phase distortions, but the algorithms focus\non normal-hearing (NH) listeners. It is not clear whether phase enhancement\nis beneficial for hearing-impaired (HI) listeners. We investigated\nthe influence of phase distortion on speech quality through a listening\nstudy, in which NH and HI listeners provided speech-quality ratings\nusing the MUSHRA procedure. In one set of conditions, the speech was\nmixed with babble noise at 4 different signal-to-noise ratios (SNRs)\nfrom -5 to 10 dB. In another set of conditions, the SNR was fixed at\n10 dB and the noisy speech was presented in a simulated reverberant\nroom with T60s ranging from 100 to 1000 ms. The speech level was kept\nat 65 dB SPL for NH listeners and amplification was applied for HI\nlisteners to ensure audibility. Ideal ratio masking (IRM) was used\nto simulate speech enhancement. Two objective metrics (i.e., PESQ and\nHASQI) were utilized to compare subjective and objective ratings. Results\nindicate that phase distortion has a negative impact on perceived quality\nfor both groups and PESQ is more closely correlated with human ratings.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1481",
      "author_area_id": "1",
      "author_area_label": "Speech Perception, Production and Acquisition"
    },
    "zhang20t_interspeech": {
      "authors": [
        [
          "Zhuo",
          "Zhang"
        ],
        [
          "Gaoyan",
          "Zhang"
        ],
        [
          "Jianwu",
          "Dang"
        ],
        [
          "Shuang",
          "Wu"
        ],
        [
          "Di",
          "Zhou"
        ],
        [
          "Longbiao",
          "Wang"
        ]
      ],
      "title": "EEG-Based Short-Time Auditory Attention Detection Using Multi-Task Deep Learning",
      "original": "2013",
      "page_count": 5,
      "order": 518,
      "p1": "2517",
      "pn": "2521",
      "abstract": [
        "A healthy person can attend to one speech in a multi-speaker scenario,\nhowever, this ability is not available to some people suffering from\nhearing impairments. Therefore, research on auditory attention detection\nbased on electroencephalography (EEG) is a possible way to help hearing-impaired\nlisteners detect the focused speech. Many previous studies used linear\nmodels or deep learning to decode the attended speech, but the cross-subject\ndecoding accuracy is low, especially within a short time duration.\nIn this study, we propose a multi-task learning model based on convolutional\nneural networks (CNN) to simultaneously perform attention decoding\nand reconstruct the attended temporal amplitude envelopes (TAEs) in\na 2s time condition. The experimental results show that, compared to\nthe traditional linear method, both the subject-specific and cross-subject\ndecoding performance showed great improvement. Particularly, the cross-subject\ndecoding accuracy was improved from 56% to 82% in 2s condition in the\ndichotic listening experiment. Furthermore, it was found that the frontal\nand temporal regions of the brain were more important for the detection\nof auditory attention by analyzing the channel contribution map. In\nsummary, the proposed method is promising for nerve-steered hearing\naids which can help hearing-impaired listeners to make faster and accurate\nattention detection.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2013",
      "author_area_id": "1",
      "author_area_label": "Speech Perception, Production and Acquisition"
    },
    "abderrazek20_interspeech": {
      "authors": [
        [
          "Sondes",
          "Abderrazek"
        ],
        [
          "Corinne",
          "Fredouille"
        ],
        [
          "Alain",
          "Ghio"
        ],
        [
          "Muriel",
          "Lalain"
        ],
        [
          "Christine",
          "Meunier"
        ],
        [
          "Virginie",
          "Woisard"
        ]
      ],
      "title": "Towards Interpreting Deep Learning Models to Understand Loss of Speech Intelligibility in Speech Disorders &#8212; Step 1: CNN Model-Based Phone Classification",
      "original": "2239",
      "page_count": 5,
      "order": 519,
      "p1": "2522",
      "pn": "2526",
      "abstract": [
        "Perceptual measurement is still the most common method for assessing\ndisordered speech in clinical practice. The subjectivity of such a\nmeasure, strongly due to human nature, but also to its lack of interpretation\nwith regard to local alterations in speech units, strongly motivates\na sophisticated tool for objective evaluation. Of interest is the increasing\nperformance of Deep Neural Networks in speech applications, but more\nimportantly the fact that they are no longer considered as black boxes.\nThe work carried out here is the first step in a long-term research\nproject, which aims to determine the linguistic units that contribute\nmost to the maintenance or loss of the intelligibility in speech disorders.\nIn this context, we study a CNN trained on normal speech for a classification\ntask of phones and tested on pathological speech. The aim of this first\nstudy is to analyze the response of the CNN model to disordered speech\nin order to study later its effectiveness in providing relevant knowledge\nin terms of speech severity or loss of intelligibility. Compared to\nperceptual severity and intelligibility measures, the results revealed\na very strong correlation between these metrics and our classifier\nperformance scores, which is very promising for future work.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2239",
      "author_area_id": "1",
      "author_area_label": "Speech Perception, Production and Acquisition"
    },
    "mirheidari20_interspeech": {
      "authors": [
        [
          "Bahman",
          "Mirheidari"
        ],
        [
          "Daniel",
          "Blackburn"
        ],
        [
          "Ronan",
          "O\u2019Malley"
        ],
        [
          "Annalena",
          "Venneri"
        ],
        [
          "Traci",
          "Walker"
        ],
        [
          "Markus",
          "Reuber"
        ],
        [
          "Heidi",
          "Christensen"
        ]
      ],
      "title": "Improving Cognitive Impairment Classification by Generative Neural Network-Based Feature Augmentation",
      "original": "2433",
      "page_count": 5,
      "order": 520,
      "p1": "2527",
      "pn": "2531",
      "abstract": [
        "Early detection of cognitive impairment is of great clinical importance.\nCurrent cognitive tests assess language and speech abilities. Recently,\nwe have developed a fully automated system to detect cognitive impairment\nfrom the analysis of conversations between a person and an intelligent\nvirtual agent (IVA). Promising results have been achieved, however\nmore data than is typically available in the medical domain is required\nto train more complex classifiers. Data augmentation using generative\nmodels has been demonstrated to be an effective approach. In this paper,\nwe use a variational autoencoder to augment data at the feature-level\nas opposed to the speech signal-level. We investigate whether this\nsuits some feature types (e.g., acoustic, linguistic) better than others.\nWe evaluate the approach on IVA recordings of people with four different\ncognitive impairment conditions. F-scores of a four-way logistic regression\n(LR) classifier are improved for certain feature types. For a deep\nneural network (DNN) classifier, the improvement is seen for almost\nall feature types. The F-score of the LR classifier on the combined\nfeatures increases from 55% to 60%, and for the DNN classifier from\n49% to 62%. Further improvements are gained by feature selection: 88%\nand 80% F-scores for LR and DNN classifiers respectively.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2433",
      "author_area_id": "1",
      "author_area_label": "Speech Perception, Production and Acquisition"
    },
    "moore20_interspeech": {
      "authors": [
        [
          "Meredith",
          "Moore"
        ],
        [
          "Piyush",
          "Papreja"
        ],
        [
          "Michael",
          "Saxon"
        ],
        [
          "Visar",
          "Berisha"
        ],
        [
          "Sethuraman",
          "Panchanathan"
        ]
      ],
      "title": "UncommonVoice: A Crowdsourced Dataset of Dysphonic Speech",
      "original": "3093",
      "page_count": 5,
      "order": 521,
      "p1": "2532",
      "pn": "2536",
      "abstract": [
        "To facilitate more accessible spoken language technologies and advance\nthe study of dysphonic speech this paper presents UncommonVoice, a\nfreely-available, crowd-sourced speech corpus consisting of 8.5 hours\nof speech from 57 individuals, 48 of whom have spasmodic dysphonia.\nThe speech material consists of non-words (prolonged vowels, and the\nprompt for diadochokinetic rate), sentences (randomly selected from\nTIMIT prompts and the CAPE-V intelligibility analysis), and spontaneous\nimage descriptions. The data was recorded in a crowdsourced manner\nusing a web-based application. This dataset is a fundamental resource\nfor the development of voice-assistive technologies for individuals\nwith dysphonia as well as the enhancement of the accessibility of voice-based\ntechnologies (automatic speech recognition, virtual assistants, etc).\nResearch on articulation differences as well as how best to model and\nrepresent dysphonic speech will greatly benefit from a free and publicly\navailable dataset of dysphonic speech. The dataset will be made freely\nand publicly available at  www.uncommonvoice.org. In the following\nsections, we detail the data collection process as well as provide\nan initial analysis of the speech corpus.\n"
      ],
      "doi": "10.21437/Interspeech.2020-3093",
      "author_area_id": "1",
      "author_area_label": "Speech Perception, Production and Acquisition"
    },
    "barche20_interspeech": {
      "authors": [
        [
          "Purva",
          "Barche"
        ],
        [
          "Krishna",
          "Gurugubelli"
        ],
        [
          "Anil Kumar",
          "Vuppala"
        ]
      ],
      "title": "Towards Automatic Assessment of Voice Disorders: A Clinical Approach",
      "original": "2160",
      "page_count": 5,
      "order": 522,
      "p1": "2537",
      "pn": "2541",
      "abstract": [
        "Automatic detection and assessment of voice disorders is important\nin diagnosis and treatment planning of voice disorders. This work proposes\nan approach for automatic detection and assessment of voice disorders\nfrom a clinical perspective. To accomplish this, a multi-level classification\napproach was explored in which four binary classifiers were used for\nthe assessment of voice disorders. The binary classifiers were trained\nusing support vector machines with excitation source features, vocal-tract\nsystem features, and state-of-art OpenSMILE features. In this study\nsource features namely, glottal parameters obtained from glottal flow\nwaveform, perturbation measures obtained from epoch locations, and\ncepstral features obtained from linear prediction residual and zero\nfrequency filtered signal were explored. The present study used the\nSaarbucken voice disorders database to evaluate the performance of\nproposed approach. The OpenSMILE features namely ComParE and eGEMAPS\nfeature sets shown better performance in terms of classification accuracies\nof 82.8% and 76%, respectively for voice disorder detection. The combination\nof excitation source features with baseline feature sets further improved\nthe performance of detection and assessment systems, that highlight\nthe complimentary nature of exciting source features.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2160",
      "author_area_id": "1",
      "author_area_label": "Speech Perception, Production and Acquisition"
    },
    "shivkumar20_interspeech": {
      "authors": [
        [
          "Abhishek",
          "Shivkumar"
        ],
        [
          "Jack",
          "Weston"
        ],
        [
          "Raphael",
          "Lenain"
        ],
        [
          "Emil",
          "Fristed"
        ]
      ],
      "title": "BlaBla: Linguistic Feature Extraction for Clinical Analysis in Multiple Languages",
      "original": "2880",
      "page_count": 5,
      "order": 523,
      "p1": "2542",
      "pn": "2546",
      "abstract": [
        "We introduce BlaBla, an open-source Python library for extracting linguistic\nfeatures with proven clinical relevance to neurological and psychiatric\ndiseases across many languages. BlaBla is a unifying framework for\naccelerating and simplifying clinical linguistic research. The library\nis built on state-of-the-art NLP frameworks and supports multithreaded/GPU-enabled\nfeature extraction via both native Python calls and a command line\ninterface. We describe BlaBla&#8217;s architecture and clinical validation\nof its features across 12 diseases. We further demonstrate the application\nof BlaBla to a task visualizing and classifying language disorders\nin three languages on real clinical data from the AphasiaBank dataset.\nWe make the codebase freely available to researchers with the hope\nof providing a consistent, well-validated foundation for the next generation\nof clinical linguistic research.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2880",
      "author_area_id": "1",
      "author_area_label": "Speech Perception, Production and Acquisition"
    },
    "xu20d_interspeech": {
      "authors": [
        [
          "Menglong",
          "Xu"
        ],
        [
          "Xiao-Lei",
          "Zhang"
        ]
      ],
      "title": "Depthwise Separable Convolutional ResNet with Squeeze-and-Excitation Blocks for Small-Footprint Keyword Spotting",
      "original": "1045",
      "page_count": 5,
      "order": 524,
      "p1": "2547",
      "pn": "2551",
      "abstract": [
        "One difficult problem of keyword spotting is how to miniaturize its\nmemory footprint while maintain a high precision. Although convolutional\nneural networks have shown to be effective to the small-footprint keyword\nspotting problem, they still need hundreds of thousands of parameters\nto achieve good performance. In this paper, we propose an efficient\nmodel based on depthwise separable convolution layers and squeeze-and-excitation\nblocks. Specifically, we replace the standard convolution by the depthwise\nseparable convolution, which reduces the number of the parameters of\nthe standard convolution without significant performance degradation.\nWe further improve the performance of the depthwise separable convolution\nby reweighting the output feature maps of the first convolution layer\nwith a so-called squeeze-and-excitation block. We compared the proposed\nmethod with five representative models on two experimental settings\nof the Google Speech Commands dataset. Experimental results show that\nthe proposed method achieves the state-of-the-art performance. For\nexample, it achieves a classification error rate of 3.29% with a number\nof parameters of 72K in the first experiment, which significantly outperforms\nthe comparison methods given a similar model size. It achieves an error\nrate of 3.97% with a number of parameters of 10K, which is also slightly\nbetter than the state-of-the-art comparison method given a similar\nmodel size.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1045",
      "author_area_id": "12",
      "author_area_label": "Spoken Language Processing: Translation, Information Retrieval, Summarization, Resources and Evaluation"
    },
    "bluche20_interspeech": {
      "authors": [
        [
          "Th\u00e9odore",
          "Bluche"
        ],
        [
          "Thibault",
          "Gisselbrecht"
        ]
      ],
      "title": "Predicting Detection Filters for Small Footprint Open-Vocabulary Keyword Spotting",
      "original": "1186",
      "page_count": 5,
      "order": 525,
      "p1": "2552",
      "pn": "2556",
      "abstract": [
        "In this paper, we propose a fully-neural approach to open-vocabulary\nkeyword spotting, that allows the users to include a customizable voice\ninterface to their device and that does not require task-specific data.\nWe present a keyword detection neural network weighing less than 250KB,\nin which the topmost layer performing keyword detection is predicted\nby an auxiliary network, that may be run offline to generate a detector\nfor any keyword. We show that the proposed model outperforms acoustic\nkeyword spotting baselines by a large margin on two tasks of detecting\nkeywords in utterances and three tasks of detecting isolated speech\ncommands. We also propose a method to fine-tune the model when specific\ntraining data is available for some keywords, which yields a performance\nsimilar to a standard speech command neural network while keeping the\nability of the model to be applied to new keywords.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1186",
      "author_area_id": "12",
      "author_area_label": "Spoken Language Processing: Translation, Information Retrieval, Summarization, Resources and Evaluation"
    },
    "ylmaz20_interspeech": {
      "authors": [
        [
          "Emre",
          "Y\u0131lmaz"
        ],
        [
          "\u00d6zg\u00fcr Bora",
          "Gevrek"
        ],
        [
          "Jibin",
          "Wu"
        ],
        [
          "Yuxiang",
          "Chen"
        ],
        [
          "Xuanbo",
          "Meng"
        ],
        [
          "Haizhou",
          "Li"
        ]
      ],
      "title": "Deep Convolutional Spiking Neural Networks for Keyword Spotting",
      "original": "1230",
      "page_count": 5,
      "order": 526,
      "p1": "2557",
      "pn": "2561",
      "abstract": [
        "This paper investigates the use of deep convolutional spiking neural\nnetworks (SNN) for keyword spotting (KWS) and wakeword detection tasks.\nThe brain-inspired SNN mimic the spike-based information processing\nof biological neural networks and they can operate on the emerging\nultra-low power neuromorphic chips. Unlike conventional artificial\nneural networks (ANN), SNN process input information asynchronously\nin an event-driven manner. With temporally sparse input information,\nthis event-driven processing substantially reduces the computational\nrequirements compared to the synchronous computation performed in ANN-based\nKWS approaches. To explore the effectiveness and computational complexity\nof SNN on KWS and wakeword detection, we compare the performance and\ncomputational costs of spiking fully-connected and convolutional neural\nnetworks with ANN counterparts under clean and noisy testing conditions.\nThe results obtained on the Speech Commands and Hey Snips corpora have\nshown the effectiveness of the convolutional SNN model compared to\na conventional CNN with comparable performance on KWS and better performance\non the wakeword detection task. With its competitive performance and\nreduced computational complexity, convolutional SNN models running\non energy-efficient neuromorphic hardware offer a low-power and effective\nsolution for mobile KWS applications.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1230",
      "author_area_id": "12",
      "author_area_label": "Spoken Language Processing: Translation, Information Retrieval, Summarization, Resources and Evaluation"
    },
    "wu20j_interspeech": {
      "authors": [
        [
          "Haiwei",
          "Wu"
        ],
        [
          "Yan",
          "Jia"
        ],
        [
          "Yuanfei",
          "Nie"
        ],
        [
          "Ming",
          "Li"
        ]
      ],
      "title": "Domain Aware Training for Far-Field Small-Footprint Keyword Spotting",
      "original": "1412",
      "page_count": 5,
      "order": 527,
      "p1": "2562",
      "pn": "2566",
      "abstract": [
        "In this paper, we focus on the task of small-footprint keyword spotting\nunder the far-field scenario. Far-field environments are commonly encountered\nin real-life speech applications, causing severe degradation of performance\ndue to room reverberation and various kinds of noises. Our baseline\nsystem is built on the convolutional neural network trained with pooled\ndata of both far-field and close-talking speech. To cope with the distortions,\nwe develop three domain aware training systems, including the domain\nembedding system, the deep CORAL system, and the multi-task learning\nsystem. These methods incorporate domain knowledge into network training\nand improve the performance of the keyword classifier on far-field\nconditions. Experimental results show that our proposed methods manage\nto maintain the performance on the close-talking speech and achieve\nsignificant improvement on the far-field test set.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1412",
      "author_area_id": "12",
      "author_area_label": "Spoken Language Processing: Translation, Information Retrieval, Summarization, Resources and Evaluation"
    },
    "zhang20u_interspeech": {
      "authors": [
        [
          "Kun",
          "Zhang"
        ],
        [
          "Zhiyong",
          "Wu"
        ],
        [
          "Daode",
          "Yuan"
        ],
        [
          "Jian",
          "Luan"
        ],
        [
          "Jia",
          "Jia"
        ],
        [
          "Helen",
          "Meng"
        ],
        [
          "Binheng",
          "Song"
        ]
      ],
      "title": "Re-Weighted Interval Loss for Handling Data Imbalance Problem of End-to-End Keyword Spotting",
      "original": "1644",
      "page_count": 5,
      "order": 528,
      "p1": "2567",
      "pn": "2571",
      "abstract": [
        "The training process of end-to-end keyword spotting (KWS) suffers from\ncritical data imbalance problem that positive samples are far less\nthan negative samples where different negative samples are not of equal\nimportance. During decoding, false alarms are mainly caused by a small\nnumber of  important negative samples having pronunciation similar\nto the keyword; however, the training loss is dominated by the majority\nof negative samples whose pronunciation is not related to the keyword,\ncalled  unimportant negative samples. This inconsistency greatly degrades\nthe performance of KWS and existing methods like focal loss don&#8217;t\ndiscriminate between the two kinds of negative samples. To deal with\nthe problem, we propose a novel re-weighted interval loss to re-weight\nsample loss considering the performance of the classifier over local\ninterval of negative utterance, which automatically down-weights the\nlosses of unimportant negative samples and focuses training on important\nnegative samples that are prone to produce false alarms during decoding.\nEvaluations on Hey Snips dataset demonstrate that our approach has\nyielded a superior performance over focal loss baseline with 34% (@0.5\nfalse alarm per hour) relative reduction of false reject rate.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1644",
      "author_area_id": "12",
      "author_area_label": "Spoken Language Processing: Translation, Information Retrieval, Summarization, Resources and Evaluation"
    },
    "zhang20v_interspeech": {
      "authors": [
        [
          "Peng",
          "Zhang"
        ],
        [
          "Xueliang",
          "Zhang"
        ]
      ],
      "title": "Deep Template Matching for Small-Footprint and Configurable Keyword Spotting",
      "original": "1761",
      "page_count": 5,
      "order": 529,
      "p1": "2572",
      "pn": "2576",
      "abstract": [
        "Keyword spotting (KWS) is a very important technique for human&#8211;machine\ninteraction to detect a trigger phrase and voice commands. In practice,\na popular demand for KWS is to conveniently define the keywords by\nconsumers or device vendors. In this paper, we propose a novel template\nmatching approach for KWS based on end-to-end deep learning method,\nwhich utilizes an attention mechanism to match the input voice to the\nkeyword templates in high-level feature space. The proposed approach\nonly requires very limited voice samples (at least only one sample)\nto register a new keyword without any retraining. We conduct experiments\non the publicly available Google speech commands dataset. The experimental\nresults demonstrate that our method outperforms baseline methods while\nallowing for a flexible configuration.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1761",
      "author_area_id": "12",
      "author_area_label": "Spoken Language Processing: Translation, Information Retrieval, Summarization, Resources and Evaluation"
    },
    "yang20d_interspeech": {
      "authors": [
        [
          "Chen",
          "Yang"
        ],
        [
          "Xue",
          "Wen"
        ],
        [
          "Liming",
          "Song"
        ]
      ],
      "title": "Multi-Scale Convolution for Robust Keyword Spotting",
      "original": "2185",
      "page_count": 5,
      "order": 530,
      "p1": "2577",
      "pn": "2581",
      "abstract": [
        "We propose a robust small-footprint keyword spotting system for resource-constrained\ndevices. Small footprint is achieved by the use of depthwise-separable\nconvolutions in a ResNet framework. Noise robustness is achieved with\na multi-scale ensemble of classifiers: each classifier is specialized\nfor a different view of the input, while the whole ensemble remains\ncompact in size by heavy parameter sharing. Extensive experiments on\npublic Google Command dataset demonstrate the effectiveness of our\nproposed method.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2185",
      "author_area_id": "12",
      "author_area_label": "Spoken Language Processing: Translation, Information Retrieval, Summarization, Resources and Evaluation"
    },
    "chen20j_interspeech": {
      "authors": [
        [
          "Yangbin",
          "Chen"
        ],
        [
          "Tom",
          "Ko"
        ],
        [
          "Lifeng",
          "Shang"
        ],
        [
          "Xiao",
          "Chen"
        ],
        [
          "Xin",
          "Jiang"
        ],
        [
          "Qing",
          "Li"
        ]
      ],
      "title": "An Investigation of Few-Shot Learning in Spoken Term Classification",
      "original": "2568",
      "page_count": 5,
      "order": 531,
      "p1": "2582",
      "pn": "2586",
      "abstract": [
        "In this paper, we investigate the feasibility of applying few-shot\nlearning algorithms to a speech task. We formulate a user-defined scenario\nof spoken term classification as a few-shot learning problem. In most\nfew-shot learning studies, it is assumed that all the N classes are\nnew in a N-way problem. We suggest that this assumption can be relaxed\nand define a N+M-way problem where N and M are the number of new classes\nand fixed classes respectively. We propose a modification to the Model-Agnostic\nMeta-Learning (MAML) algorithm to solve the problem. Experiments on\nthe Google Speech Commands dataset show that our approach<SUP>1</SUP>\noutperforms the conventional supervised learning approach and the original\nMAML.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2568",
      "author_area_id": "12",
      "author_area_label": "Spoken Language Processing: Translation, Information Retrieval, Summarization, Resources and Evaluation"
    },
    "zhao20d_interspeech": {
      "authors": [
        [
          "Zeyu",
          "Zhao"
        ],
        [
          "Wei-Qiang",
          "Zhang"
        ]
      ],
      "title": "End-to-End Keyword Search Based on Attention and Energy Scorer for Low Resource Languages",
      "original": "2613",
      "page_count": 5,
      "order": 532,
      "p1": "2587",
      "pn": "2591",
      "abstract": [
        "Keyword search (KWS) means searching for the keywords given by the\nuser from continuous speech. Conventional KWS systems based on automatic\nspeech recognition (ASR) decode input speech by ASR before searching\nfor keywords. With deep neural network (DNN) becoming increasingly\npopular, some end-to-end (E2E) KWS emerged. The main advantage of E2E\nKWS is to avoid speech recognition. Since E2E KWS systems are at the\nvery beginning, the performance is currently not as good as traditional\nmethods, so there is still loads of work to do. To this end, we propose\nan E2E KWS model consists of four parts, including speech encoder-decoder,\nquery encoder-decoder, attention mechanism and energy scorer. Different\nfrom the baseline system using auto-encoder to extract embeddings,\nthe proposed model extracts embeddings that contain character sequence\ninformation by encode-decoder. Attention mechanism and a novel energy\nscorer are also introduced in the model, where the former can locate\nthe keywords, and the latter can make the final decision. We train\nthe models on low resource condition with only about 10-hour training\ndata in various languages. The experiment results show that the proposed\nmodel outperforms the baseline system.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2613",
      "author_area_id": "12",
      "author_area_label": "Spoken Language Processing: Translation, Information Retrieval, Summarization, Resources and Evaluation"
    },
    "higuchi20_interspeech": {
      "authors": [
        [
          "Takuya",
          "Higuchi"
        ],
        [
          "Mohammad",
          "Ghasemzadeh"
        ],
        [
          "Kisun",
          "You"
        ],
        [
          "Chandra",
          "Dhir"
        ]
      ],
      "title": "Stacked 1D Convolutional Networks for End-to-End Small Footprint Voice Trigger Detection",
      "original": "2763",
      "page_count": 5,
      "order": 533,
      "p1": "2592",
      "pn": "2596",
      "abstract": [
        "We propose a stacked 1D convolutional neural network (S1DCNN) for end-to-end\nsmall footprint voice trigger detection in a streaming scenario. Voice\ntrigger detection is an important speech application, with which users\ncan activate their devices by simply saying a keyword or phrase. Due\nto privacy and latency reasons, a voice trigger detection system should\nrun on an always-on processor on device. Therefore, having small memory\nand compute cost is crucial for a voice trigger detection system. Recently,\nsingular value decomposition filters (SVDFs) has been used for end-to-end\nvoice trigger detection. The SVDFs approximate a fully-connected layer\nwith a low rank approximation, which reduces the number of model parameters.\nIn this work, we propose S1DCNN as an alternative approach for end-to-end\nsmall-footprint voice trigger detection. An S1DCNN layer consists of\na 1D convolution layer followed by a depth-wise 1D convolution layer.\nWe show that the SVDF can be expressed as a special case of the S1DCNN\nlayer. Experimental results show that the S1DCNN achieve 19.0% relative\nfalse reject ratio (FRR) reduction with a similar model size and a\nsimilar time delay compared to the SVDF. By using longer time delays,\nthe S1DCNN further improve the FRR up to 12.2% relative.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2763",
      "author_area_id": "12",
      "author_area_label": "Spoken Language Processing: Translation, Information Retrieval, Summarization, Resources and Evaluation"
    },
    "heitkaemper20_interspeech": {
      "authors": [
        [
          "Jens",
          "Heitkaemper"
        ],
        [
          "Joerg",
          "Schmalenstroeer"
        ],
        [
          "Reinhold",
          "Haeb-Umbach"
        ]
      ],
      "title": "Statistical and Neural Network Based Speech Activity Detection in Non-Stationary Acoustic Environments",
      "original": "1252",
      "page_count": 5,
      "order": 534,
      "p1": "2597",
      "pn": "2601",
      "abstract": [
        "Speech activity detection (SAD), which often rests on the fact that\nthe noise is &#8220;more&#8221; stationary than speech, is particularly\nchallenging in non-stationary environments, because the time variance\nof the acoustic scene makes it difficult to discriminate speech from\nnoise. We propose two approaches to SAD, where one is based on statistical\nsignal processing, while the other utilizes neural networks. The former\nemploys sophisticated signal processing to track the noise and speech\nenergies and is meant to support the case for a resource efficient,\nunsupervised signal processing approach. The latter introduces a recurrent\nnetwork layer that operates on short segments of the input speech to\ndo temporal smoothing in the presence of non-stationary noise. The\nsystems are tested on the Fearless Steps challenge database, which\nconsists of the transmission data from the Apollo-11 space mission.\nThe statistical SAD achieves comparable detection performance to earlier\nproposed neural network based SADs, while the neural network based\napproach leads to a decision cost function of 1.07% on the evaluation\nset of the 2020 Fearless Steps Challenge, which sets a new state of\nthe art.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1252",
      "author_area_id": "13",
      "author_area_label": "Special Sessions"
    },
    "zhang20w_interspeech": {
      "authors": [
        [
          "Xueshuai",
          "Zhang"
        ],
        [
          "Wenchao",
          "Wang"
        ],
        [
          "Pengyuan",
          "Zhang"
        ]
      ],
      "title": "Speaker Diarization System Based on DPCA Algorithm for Fearless Steps Challenge Phase-2",
      "original": "1666",
      "page_count": 5,
      "order": 535,
      "p1": "2602",
      "pn": "2606",
      "abstract": [
        "This paper describes the ASRGroup team speaker diarization systems\nsubmitted to the TRACK 2 of the Fearless Steps Challenge Phase-2. In\nthis system, the similarity matrix among all segments of an audio recording\nwas measured by Sequential Bidirectional Long Short-term Memory Networks\n(Bi-LSTM), and a clustering scheme based on Density Peak Cluster Algorithm\n(DPCA) was proposed to clustering the segments. The system was compared\nwith the Kaldi Toolkit diarization system (x-vector based on TDNN with\nPLDA scoring model) and the Spectral system (similarity based on Bi-LSTM\nwith Spectral clustering algorithm). Experiments show that our system\nis significantly outperforms above systems and achieves a Diarization\nError Rate (DER) of 42.75% and 39.52% respectively on the Dev dataset\nand Eval dataset of TRACK 2 (Fearless Steps Challenge Phase-2). Compared\nwith the baseline Kaldi Toolkit diarization system and Spectral Clustering\nalgorithm with Bi-LSTM similarity models, the DER of our system is\nabsolutely reduced 4.64%, 1.84% and 8.85%, 7.57% respectively on the\ntwo datasets.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1666",
      "author_area_id": "13",
      "author_area_label": "Special Sessions"
    },
    "lin20e_interspeech": {
      "authors": [
        [
          "Qingjian",
          "Lin"
        ],
        [
          "Tingle",
          "Li"
        ],
        [
          "Ming",
          "Li"
        ]
      ],
      "title": "The DKU Speech Activity Detection and Speaker Identification Systems for Fearless Steps Challenge Phase-02",
      "original": "1915",
      "page_count": 5,
      "order": 536,
      "p1": "2607",
      "pn": "2611",
      "abstract": [
        "This paper describes the systems developed by the DKU team for the\nFearless Steps Challenge Phase-02 competition. For the Speech Activity\nDetection task, we start with the Long Short-Term Memory (LSTM) system\nand then apply the ResNet-LSTM improvement. Our ResNet-LSTM system\nreduces the DCF error by about 38% relatively in comparison with the\nLSTM baseline. We also discuss the system performance with additional\ntraining corpora included, and the lowest DCF of 1.406% on the Eval\nSet is gained with system pre-training. As for the Speaker Identification\ntask, we employ the Deep ResNet vector system, which receives a variable-length\nfeature sequence and directly generates speaker posteriors. The pretraining\nprocess with Voxceleb is also considered, and our best-performing system\nachieves the Top-5 accuracy of 92.393% on the Eval Set.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1915",
      "author_area_id": "13",
      "author_area_label": "Special Sessions"
    },
    "gorin20_interspeech": {
      "authors": [
        [
          "Arseniy",
          "Gorin"
        ],
        [
          "Daniil",
          "Kulko"
        ],
        [
          "Steven",
          "Grima"
        ],
        [
          "Alex",
          "Glasman"
        ]
      ],
      "title": "&#8220;This is Houston. Say again, please&#8221;. The Behavox System for the Apollo-11 Fearless Steps Challenge (Phase II)",
      "original": "2822",
      "page_count": 5,
      "order": 537,
      "p1": "2612",
      "pn": "2616",
      "abstract": [
        "We describe the speech activity detection (SAD), speaker diarization\n(SD), and automatic speech recognition (ASR) experiments conducted\nby the Behavox team for the Interspeech 2020 Fearless Steps Challenge\n(FSC-2). A relatively small amount of labeled data, a large variety\nof speakers and channel distortions, specific lexicon and speaking\nstyle resulted in high error rates on the systems which involved this\ndata. In addition to approximately 36 hours of annotated NASA mission\nrecordings, the organizers provided a much larger but unlabeled 19k\nhour Apollo-11 corpus that we also explore for semi-supervised training\nof ASR acoustic and language models, observing more than 17% relative\nword error rate improvement compared to training on the FSC-2 data\nonly. We also compare several SAD and SD systems to approach the most\ndifficult tracks of the challenge (track 1 for diarization and ASR),\nwhere long 30-minute audio recordings are provided for evaluation without\nsegmentation or speaker information. For all systems, we report substantial\nperformance improvements compared to the FSC-2 baseline systems, and\nachieved a first-place ranking for SD and ASR and fourth-place for\nSAD in the challenge.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2822",
      "author_area_id": "13",
      "author_area_label": "Special Sessions"
    },
    "joglekar20_interspeech": {
      "authors": [
        [
          "Aditya",
          "Joglekar"
        ],
        [
          "John H.L.",
          "Hansen"
        ],
        [
          "Meena Chandra",
          "Shekar"
        ],
        [
          "Abhijeet",
          "Sangwan"
        ]
      ],
      "title": "FEARLESS STEPS Challenge (FS-2): Supervised Learning with Massive Naturalistic Apollo Data",
      "original": "3054",
      "page_count": 5,
      "order": 538,
      "p1": "2617",
      "pn": "2621",
      "abstract": [
        "The Fearless Steps Initiative by UTDallas-CRSS led to the digitization,\nrecovery, and diarization of 19,000 hours of original analog audio\ndata, as well as the development of algorithms to extract meaningful\ninformation from this multi-channel naturalistic data resource. The\n2020 FEARLESS STEPS (FS-2) Challenge is the second annual challenge\nheld for the Speech and Language Technology community to motivate supervised\nlearning algorithm development for multi-party and multi-stream naturalistic\naudio. In this paper, we present an overview of the challenge sub-tasks,\ndata, performance metrics, and lessons learned from Phase-2 of the\nFearless Steps Challenge (FS-2). We present advancements made in FS-2\nthrough extensive community outreach and feedback. We describe innovations\nin the challenge corpus development, and present revised baseline results.\nWe finally discuss the challenge outcome and general trends in system\ndevelopment across both phases (Phase FS-1 Unsupervised, and Phase\nFS-2 Supervised) of the challenge, and its continuation into multi-channel\nchallenge tasks for the upcoming Fearless Steps Challenge Phase-3.\n"
      ],
      "doi": "10.21437/Interspeech.2020-3054",
      "author_area_id": "13",
      "author_area_label": "Special Sessions"
    },
    "luo20b_interspeech": {
      "authors": [
        [
          "Yi",
          "Luo"
        ],
        [
          "Nima",
          "Mesgarani"
        ]
      ],
      "title": "Separating Varying Numbers of Sources with Auxiliary Autoencoding Loss",
      "original": "0034",
      "page_count": 5,
      "order": 539,
      "p1": "2622",
      "pn": "2626",
      "abstract": [
        "Many recent source separation systems are designed to separate a fixed\nnumber of sources out of a mixture. In the cases where the source activation\npatterns are unknown, such systems have to either adjust the number\nof outputs or to identify invalid outputs from the valid ones. Iterative\nseparation methods have gain much attention in the community as they\ncan flexibly decide the number of outputs, however (1) they typically\nrely on long-term information to determine the stopping time for the\niterations, which makes them hard to operate in a causal setting; (2)\nthey lack a &#8220;fault tolerance&#8221; mechanism when the estimated\nnumber of sources is different from the actual number. In this paper,\nwe propose a simple training method, the auxiliary autoencoding permutation\ninvariant training (A2PIT), to alleviate the two issues. A2PIT assumes\na fixed number of outputs and uses auxiliary autoencoding loss to force\nthe invalid outputs to be the copies of the input mixture, and detects\ninvalid outputs in a fully unsupervised way during inference phase.\nExperiment results show that A2PIT is able to improve the separation\nperformance across various numbers of speakers and effectively detect\nthe number of speakers in a mixture.\n"
      ],
      "doi": "10.21437/Interspeech.2020-34",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "chen20k_interspeech": {
      "authors": [
        [
          "Jingjing",
          "Chen"
        ],
        [
          "Qirong",
          "Mao"
        ],
        [
          "Dong",
          "Liu"
        ]
      ],
      "title": "On Synthesis for Supervised Monaural Speech Separation in Time Domain",
      "original": "1150",
      "page_count": 5,
      "order": 540,
      "p1": "2627",
      "pn": "2631",
      "abstract": [
        "Time-domain approaches for speech separation have achieved great success\nrecently. However, the sources separated by these time-domain approaches\nusually contain some artifacts (broadband noises), especially when\nseparating mixture with noise. In this paper, we incorporate synthesis\nway into the time-domain speech separation approaches to deal with\nabove broadband noises in separated sources, which can be seamlessly\nused in the speech separation system by a &#8216;plug-and-play&#8217;\nway. By directly learning an estimation for each source in encoded\ndomain, synthesis way can reduce artifacts in estimated speeches and\nimprove the speech separation performance. Extensive experiments on\ndifferent state-of-the-art models reveal that the synthesis way acquires\nthe ability to handle with noisy mixture and is more suitable for noisy\nspeech separation. On a new benchmark noisy dataset, the synthesis\nway obtains 0.97 dB (10.1%) SDR relative improvement and respective\ngains on various metrics without extra computation cost.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1150",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "wang20y_interspeech": {
      "authors": [
        [
          "Jun",
          "Wang"
        ]
      ],
      "title": "Learning Better Speech Representations by Worsening Interference",
      "original": "1545",
      "page_count": 5,
      "order": 541,
      "p1": "2632",
      "pn": "2636",
      "abstract": [
        "Can better representations be learnt from worse interfering scenarios?\nTo verify this seeming paradox, we propose a novel framework that performed\ncompositional learning in traditionally independent tasks of speech\nseparation and speaker identification. In this framework, generic pre-training\nand compositional fine-tuning are proposed to mimic the bottom-up and\ntop-down processes of a human&#8217;s cocktail party effect. Moreover,\nwe investigate schemes to prohibit the model from ending up learning\nan easier identity-prediction task. Substantially discriminative and\ngeneralizable representations can be learnt in severely interfering\nconditions. Experiment results on downstream tasks show that our learnt\nrepresentations have superior discriminative power than a standard\nspeaker verification method. Meanwhile, RISE achieves higher SI-SNRi\nconsistently in different inference modes over DPRNN, a state-of-the-art\nspeech separation system.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1545",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "pariente20_interspeech": {
      "authors": [
        [
          "Manuel",
          "Pariente"
        ],
        [
          "Samuele",
          "Cornell"
        ],
        [
          "Joris",
          "Cosentino"
        ],
        [
          "Sunit",
          "Sivasankaran"
        ],
        [
          "Efthymios",
          "Tzinis"
        ],
        [
          "Jens",
          "Heitkaemper"
        ],
        [
          "Michel",
          "Olvera"
        ],
        [
          "Fabian-Robert",
          "St\u00f6ter"
        ],
        [
          "Mathieu",
          "Hu"
        ],
        [
          "Juan M.",
          "Mart\u00edn-Do\u00f1as"
        ],
        [
          "David",
          "Ditter"
        ],
        [
          "Ariel",
          "Frank"
        ],
        [
          "Antoine",
          "Deleforge"
        ],
        [
          "Emmanuel",
          "Vincent"
        ]
      ],
      "title": "Asteroid: The PyTorch-Based Audio Source Separation Toolkit for Researchers",
      "original": "1673",
      "page_count": 5,
      "order": 542,
      "p1": "2637",
      "pn": "2641",
      "abstract": [
        "This paper describes  Asteroid, the  PyTorch-based audio source separation\ntoolkit for researchers. Inspired by the most successful neural source\nseparation systems, it provides all neural building blocks required\nto build such a system. To improve reproducibility, Kaldi-style recipes\non common audio source separation datasets are also provided. This\npaper describes the software architecture of  Asteroid and its most\nimportant features. By showing experimental results obtained with \nAsteroid&#8217;s recipes, we show that our implementations are at least\non par with most results reported in reference papers. The toolkit\nis publicly available at <KBD>https://github.com/mpariente/asteroid</KBD>.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1673",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "chen20l_interspeech": {
      "authors": [
        [
          "Jingjing",
          "Chen"
        ],
        [
          "Qirong",
          "Mao"
        ],
        [
          "Dong",
          "Liu"
        ]
      ],
      "title": "Dual-Path Transformer Network: Direct Context-Aware Modeling for End-to-End Monaural Speech Separation",
      "original": "2205",
      "page_count": 5,
      "order": 543,
      "p1": "2642",
      "pn": "2646",
      "abstract": [
        "The dominant speech separation models are based on complex recurrent\nor convolution neural network that model speech sequences indirectly\nconditioning on context, such as passing information through many intermediate\nstates in recurrent neural network, leading to suboptimal separation\nperformance. In this paper, we propose a dual-path transformer network\n(DPTNet) for end-to-end speech separation, which introduces direct\ncontext-awareness in the modeling for speech sequences. By introduces\na improved transformer, elements in speech sequences can interact directly,\nwhich enables DPTNet can model for the speech sequences with direct\ncontext-awareness. The improved transformer in our approach learns\nthe order information of the speech sequences without positional encodings\nby incorporating a recurrent neural network into the original transformer.\nIn addition, the structure of dual paths makes our model efficient\nfor extremely long speech sequence modeling. Extensive experiments\non benchmark datasets show that our approach outperforms the current\nstate-of-the-arts (20.6 dB SDR on the public WSj0-2mix data corpus).\n"
      ],
      "doi": "10.21437/Interspeech.2020-2205",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "deng20b_interspeech": {
      "authors": [
        [
          "Chengyun",
          "Deng"
        ],
        [
          "Yi",
          "Zhang"
        ],
        [
          "Shiqian",
          "Ma"
        ],
        [
          "Yongtao",
          "Sha"
        ],
        [
          "Hui",
          "Song"
        ],
        [
          "Xiangang",
          "Li"
        ]
      ],
      "title": "Conv-TasSAN: Separative Adversarial Network Based on Conv-TasNet",
      "original": "2371",
      "page_count": 5,
      "order": 544,
      "p1": "2647",
      "pn": "2651",
      "abstract": [
        "Conv-TasNet has showed competitive performance on single-channel speech\nsource separation. In this paper, we investigate to further improve\nseparation performance by optimizing the training mechanism with the\nsame network structure. Motivated by the successful applications of\ngenerative adversarial networks (GANs) on speech enhancement tasks,\nwe propose a novel Separative Adversarial Network called Conv-TasSAN,\nin which the separator is realized by using Conv-TasNet architecture.\nThe discriminator is involved to optimize the separator with respect\nto specific speech objective metric. It makes the separator network\ncapture the distribution information of speech sources more accurately,\nand also prevents over-smoothing problems. Experiments on WSJ0-2mix\ndataset confirm the superior performance of the proposed method over\nConv-TasNet in terms of SI-SNR and PESQ improvement.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2371",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "kinoshita20_interspeech": {
      "authors": [
        [
          "Keisuke",
          "Kinoshita"
        ],
        [
          "Thilo von",
          "Neumann"
        ],
        [
          "Marc",
          "Delcroix"
        ],
        [
          "Tomohiro",
          "Nakatani"
        ],
        [
          "Reinhold",
          "Haeb-Umbach"
        ]
      ],
      "title": "Multi-Path RNN for Hierarchical Modeling of Long Sequential Data and its Application to Speaker Stream Separation",
      "original": "2388",
      "page_count": 5,
      "order": 545,
      "p1": "2652",
      "pn": "2656",
      "abstract": [
        "Recently, the source separation performance was greatly improved by\ntime-domain audio source separation based on dual-path recurrent neural\nnetwork (DPRNN). DPRNN is a simple but effective model for a long sequential\ndata. While DPRNN is quite efficient in modeling a sequential data\nof the length of an utterance, i.e., about 5 to 10 second data, it\nis harder to apply it to longer sequences such as whole conversations\nconsisting of multiple utterances. It is simply because, in such a\ncase, the number of time steps consumed by its internal module called\ninter-chunk RNN becomes extremely large. To mitigate this problem,\nthis paper proposes a multi-path RNN (MPRNN), a generalized version\nof DPRNN, that models the input data in a hierarchical manner. In the\nMPRNN framework, the input data is represented at several (&#8805;3)\ntime-resolutions, each of which is modeled by a specific RNN sub-module.\nFor example, the RNN sub-module that deals with the finest resolution\nmay model temporal relationship only within a phoneme, while the RNN\nsub-module handling the most coarse resolution may capture only the\nrelationship between utterances such as speaker information. We perform\nexperiments using simulated dialogue-like mixtures and show that MPRNN\nhas greater model capacity, and it outperforms the current state-of-the-art\nDPRNN framework especially in online processing scenarios.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2388",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "narayanaswamy20_interspeech": {
      "authors": [
        [
          "Vivek",
          "Narayanaswamy"
        ],
        [
          "Jayaraman J.",
          "Thiagarajan"
        ],
        [
          "Rushil",
          "Anirudh"
        ],
        [
          "Andreas",
          "Spanias"
        ]
      ],
      "title": "Unsupervised Audio Source Separation Using Generative Priors",
      "original": "3115",
      "page_count": 5,
      "order": 546,
      "p1": "2657",
      "pn": "2661",
      "abstract": [
        "State-of-the-art under-determined audio source separation systems rely\non supervised end to end training of carefully tailored neural network\narchitectures operating either in the time or the spectral domain.\nHowever, these methods are severely challenged in terms of requiring\naccess to expensive source level labeled data and being specific to\na given set of sources and the mixing process, which demands complete\nre-training when those assumptions change. This strongly emphasizes\nthe need for unsupervised methods that can leverage the recent advances\nin data-driven modeling, and compensate for the lack of labeled data\nthrough meaningful priors. To this end, we propose a novel approach\nfor audio source separation based on generative priors trained on individual\nsources. Through the use of projected gradient descent optimization,\nour approach simultaneously searches in the source-specific latent\nspaces to effectively recover the constituent sources. Though the generative\npriors can be defined in the time domain directly, e.g. WaveGAN, we\nfind that using spectral domain loss functions for our optimization\nleads to good-quality source estimates. Our empirical studies on standard\nspoken digit and instrument datasets clearly demonstrate the effectiveness\nof our approach over classical as well as state-of-the-art unsupervised\nbaselines.\n"
      ],
      "doi": "10.21437/Interspeech.2020-3115",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "qiu20b_interspeech": {
      "authors": [
        [
          "Yuanhang",
          "Qiu"
        ],
        [
          "Ruili",
          "Wang"
        ]
      ],
      "title": "Adversarial Latent Representation Learning for Speech Enhancement",
      "original": "1593",
      "page_count": 5,
      "order": 547,
      "p1": "2662",
      "pn": "2666",
      "abstract": [
        "This paper proposes a novel adversarial latent representation learning\n(ALRL) method for speech enhancement. Based on adversarial feature\nlearning, ALRL employs an extra encoder to learn an inverse mapping\nfrom the generated data distribution to the latent space. The encoder\nbuilds an inner connection with the generator, and provides relevant\nlatent information for adversarial feature modelling. A new loss function\nis proposed to implement the encoder mapping simultaneously. In addition,\nthe multi-head self-attention is also applied to the encoder for learning\nof long-range dependencies and further effective adversarial representations.\nThe experimental results demonstrate that ALRL outperforms current\nGAN-based speech enhancement methods.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1593",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "xiang20_interspeech": {
      "authors": [
        [
          "Yang",
          "Xiang"
        ],
        [
          "Liming",
          "Shi"
        ],
        [
          "Jesper Lisby",
          "H\u00f8jvang"
        ],
        [
          "Morten H\u00f8jfeldt",
          "Rasmussen"
        ],
        [
          "Mads Gr\u00e6sb\u00f8ll",
          "Christensen"
        ]
      ],
      "title": "An NMF-HMM Speech Enhancement Method Based on Kullback-Leibler Divergence",
      "original": "1047",
      "page_count": 5,
      "order": 548,
      "p1": "2667",
      "pn": "2671",
      "abstract": [
        "In this paper, we present a novel supervised Non-negative Matrix Factorization\n(NMF) speech enhancement method, which is based on Hidden Markov Model\n(HMM) and Kullback-Leibler (KL) divergence (NMF-HMM). Our algorithm\napplies the HMM to capture the timing information, so the temporal\ndynamics of speech signal can be considered by comparing with the traditional\nNMF-based speech enhancement method. More specifically, the sum of\nPoisson, leading to the KL divergence measure, is used as the observation\nmodel for each state of HMM. This ensures that the parameter update\nrule of the proposed algorithm is identical to the multiplicative update\nrule, which is quick and efficient. In the training stage, this update\nrule is applied to train the NMF-HMM model. In the online enhancement\nstage, a novel minimum mean-square error (MMSE) estimator that combines\nthe NMF-HMM is proposed to conduct speech enhancement. The performance\nof the proposed algorithm is evaluated by perceptual evaluation of\nspeech quality (PESQ) and short-time objective intelligibility (STOI).\nThe experimental results indicate that the STOI score of proposed strategy\nis able to outperform 7% than current state-of-the-art NMF-based speech\nenhancement methods.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1047",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "zhang20x_interspeech": {
      "authors": [
        [
          "Lu",
          "Zhang"
        ],
        [
          "Mingjiang",
          "Wang"
        ]
      ],
      "title": "Multi-Scale TCN: Exploring Better Temporal DNN Model for Causal Speech Enhancement",
      "original": "1104",
      "page_count": 5,
      "order": 549,
      "p1": "2672",
      "pn": "2676",
      "abstract": [
        "Capturing the temporal dependence of speech signals is of great importance\nfor numerous speech related tasks. This paper proposes a more effective\ntemporal modeling method for causal speech enhancement system. We design\na forward stacked temporal convolutional network (TCN) model which\nexploits multi-scale temporal analysis in each residual block. This\nmodel incorporates a multi-scale dilated convolution to better track\nthe target speech through its context information from past frames.\nApplying multi-target learning of log power spectrum (LPS) and ideal\nratio mask (IRM) further improves model robustness, due to the complementarity\namong the tasks. Experimental results show that the proposed TCN model\nnot only performs better speech reconstruction ability in terms of\nspeech quality and speech intelligibility, but also has smaller model\nsize than that of long short-term memory (LSTM) network and the gated\nrecurrent units (GRU) network.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1104",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "wang20z_interspeech": {
      "authors": [
        [
          "Quan",
          "Wang"
        ],
        [
          "Ignacio Lopez",
          "Moreno"
        ],
        [
          "Mert",
          "Saglam"
        ],
        [
          "Kevin",
          "Wilson"
        ],
        [
          "Alan",
          "Chiao"
        ],
        [
          "Renjie",
          "Liu"
        ],
        [
          "Yanzhang",
          "He"
        ],
        [
          "Wei",
          "Li"
        ],
        [
          "Jason",
          "Pelecanos"
        ],
        [
          "Marily",
          "Nika"
        ],
        [
          "Alexander",
          "Gruenstein"
        ]
      ],
      "title": "VoiceFilter-Lite: Streaming Targeted Voice Separation for On-Device Speech Recognition",
      "original": "1193",
      "page_count": 5,
      "order": 550,
      "p1": "2677",
      "pn": "2681",
      "abstract": [
        "We introduce VoiceFilter-Lite, a single-channel source separation model\nthat runs on the device to preserve only the speech signals from a\ntarget user, as part of a streaming speech recognition system. Delivering\nsuch a model presents numerous challenges: It should improve the performance\nwhen the input signal consists of overlapped speech, and must not hurt\nthe speech recognition performance under all other acoustic conditions.\nBesides, this model must be tiny, fast, and perform inference in a\nstreaming fashion, in order to have minimal impact on CPU, memory,\nbattery and latency. We propose novel techniques to meet these multi-faceted\nrequirements, including using a new asymmetric loss, and adopting adaptive\nruntime suppression strength. We also show that such a model can be\nquantized as a 8-bit integer model and run in realtime.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1193",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "shi20d_interspeech": {
      "authors": [
        [
          "Ziqiang",
          "Shi"
        ],
        [
          "Rujie",
          "Liu"
        ],
        [
          "Jiqing",
          "Han"
        ]
      ],
      "title": "Speech Separation Based on Multi-Stage Elaborated Dual-Path Deep BiLSTM with Auxiliary Identity Loss",
      "original": "1537",
      "page_count": 5,
      "order": 551,
      "p1": "2682",
      "pn": "2686",
      "abstract": [
        "Deep neural network with dual-path bi-directional long short-term memory\n(BiLSTM) block has been proved to be very effective in sequence modeling,\nespecially in speech separation. This work investigates how to extend\ndual-path BiLSTM to result in a new state-of-the-art approach, called\nTasTas, for multi-talker monaural speech separation (a.k.a cocktail\nparty problem). TasTas introduces two simple but effective improvements,\none is an iterative multi-stage refinement scheme, and the other is\nto correct the speech with imperfect separation through a loss of speaker\nidentity consistency between the separated speech and original speech,\nto boost the performance of dual-path BiLSTM based networks. TasTas\ntakes the mixed utterance of two speakers and maps it to two separated\nutterances, where each utterance contains only one speaker&#8217;s\nvoice. Our experiments on the notable benchmark WSJ0-2mix data corpus\nresult in 20.55dB SDR improvement, 20.35dB SI-SDR improvement, 3.69\nof PESQ, and 94.86% of ESTOI, which shows that our proposed networks\ncan lead to big performance improvement on the speaker separation task.\nWe have open sourced our reimplementation of the DPRNN-TasNet here<SUP>1</SUP>,\nand our TasTas is realized based on this implementation of DPRNN-TasNet,\nit is believed that the results in this paper can be reproduced with\nease.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1537",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "hao20b_interspeech": {
      "authors": [
        [
          "Xiang",
          "Hao"
        ],
        [
          "Shixue",
          "Wen"
        ],
        [
          "Xiangdong",
          "Su"
        ],
        [
          "Yun",
          "Liu"
        ],
        [
          "Guanglai",
          "Gao"
        ],
        [
          "Xiaofei",
          "Li"
        ]
      ],
      "title": "Sub-Band Knowledge Distillation Framework for Speech Enhancement",
      "original": "1539",
      "page_count": 5,
      "order": 552,
      "p1": "2687",
      "pn": "2691",
      "abstract": [
        "In single-channel speech enhancement, methods based on full-band spectral\nfeatures have been widely studying, while only a few methods pay attention\nto non-full-band spectral features. In this paper, we explore a knowledge\ndistillation framework based on sub-band spectral mapping for single-channel\nspeech enhancement. First, we divide the full frequency band into multiple\nsub-bands and pre-train elite-level sub-band enhancement model (teacher\nmodel) for each sub-band. The teacher models are dedicated to processing\ntheir own sub-bands. Next, under the teacher models&#8217; guidance,\nwe train a general sub-band enhancement model (student model) that\nworks for all sub-bands. Without increasing the number of model parameters\nand computational complexity, the student model&#8217;s performance\nis further improved. To evaluate the proposed method, we conducted\na large number of experiments on an open-source data set. The final\nexperimental results show that the guidance from the elite-level teacher\nmodels dramatically improves the student model&#8217;s performance,\nwhich exceeds the full-band model by employing fewer parameters.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1539",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "roy20_interspeech": {
      "authors": [
        [
          "Sujan Kumar",
          "Roy"
        ],
        [
          "Aaron",
          "Nicolson"
        ],
        [
          "Kuldip K.",
          "Paliwal"
        ]
      ],
      "title": "A Deep Learning-Based Kalman Filter for Speech Enhancement",
      "original": "1551",
      "page_count": 5,
      "order": 553,
      "p1": "2692",
      "pn": "2696",
      "abstract": [
        "The existing Kalman filter (KF) suffers from poor estimates of the\nnoise variance and the linear prediction coefficients (LPCs) in real-world\nnoise conditions. This results in a degraded speech enhancement performance.\nIn this paper, a deep learning approach is used to more accurately\nestimate the noise variance and LPCs, enabling the KF to enhance speech\nin various noise conditions. Specifically, a deep learning approach\nto MMSE-based noise power spectral density (PSD) estimation, called\nDeepMMSE, is used. The estimated noise PSD is used to compute the noise\nvariance. We also construct a whitening filter with its coefficients\ncomputed from the estimated noise PSD. It is then applied to the noisy\nspeech, yielding pre-whitened speech for computing the LPCs. The improved\nnoise variance and LPC estimates enable the KF to minimise the  residual\nnoise and  distortion in the enhanced speech. Experimental results\nshow that the proposed method exhibits higher quality and intelligibility\nin the enhanced speech than the benchmark methods in various noise\nconditions for a wide-range of SNR levels.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1551",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "yu20e_interspeech": {
      "authors": [
        [
          "Hongjiang",
          "Yu"
        ],
        [
          "Wei-Ping",
          "Zhu"
        ],
        [
          "Benoit",
          "Champagne"
        ]
      ],
      "title": "Subband Kalman Filtering with DNN Estimated Parameters for Speech Enhancement",
      "original": "1913",
      "page_count": 5,
      "order": 554,
      "p1": "2697",
      "pn": "2701",
      "abstract": [
        "In this paper, we present a novel deep neural network (DNN) assisted\nsubband Kalman filtering system for speech enhancement. In the off-line\nphase, a DNN is trained to explore the relationships between the features\nof the noisy subband speech and the linear prediction coefficients\nof the clean ones, which are the key parameters in Kalman filtering.\nIn the on-line phase, the input noisy speech is firstly decomposed\ninto subbands, and then Kalman filtering is applied to each subband\nspeech for denoising. The final enhanced speech is obtained by synthesizing\nthe enhanced subband speeches. Experimental results show that our proposed\nsystem outperforms three Kalman filtering based methods in terms of\nboth speech quality and intelligibility.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1913",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "li20aa_interspeech": {
      "authors": [
        [
          "Xiaoqi",
          "Li"
        ],
        [
          "Yaxing",
          "Li"
        ],
        [
          "Yuanjie",
          "Dong"
        ],
        [
          "Shan",
          "Xu"
        ],
        [
          "Zhihui",
          "Zhang"
        ],
        [
          "Dan",
          "Wang"
        ],
        [
          "Shengwu",
          "Xiong"
        ]
      ],
      "title": "Bidirectional LSTM Network with Ordered Neurons for Speech Enhancement",
      "original": "2245",
      "page_count": 5,
      "order": 555,
      "p1": "2702",
      "pn": "2706",
      "abstract": [
        "Speech enhancement aims to reduce the noise and improve the quality\nand intelligibility of noisy speech. Long short-term memory (LSTM)\nnetwork frameworks have achieved great success on many speech enhancement\napplications. In this paper, the ordered neurons long short-term memory\n(ON-LSTM) network with a new inductive bias to differential the long/short-term\ninformation in each neuron is proposed for speech enhancement. Comparing\nthe low-ranking neurons with short-term or local information, the high-ranking\nneurons which contain the long-term or global information always update\nless frequently for a wide range of influence. Thus, the ON-LSTM can\nautomatically learn the clean speech information from noisy input and\nshow better expressive ability. We also propose a rearrangement concatenation\nrule to connect the ON-LSTM outputs of forward and backward layers\nto construct the bidirectional ON-LSTM (Bi-ONLSTM) for further performance\nimprovement. The experimental results reveal that the proposed ON-LSTM\nschemes produce better enhancement performance than the vanilla LSTM\nbaseline. And visualization result shows that our proposed model can\neffectively capture clean speech components from noisy inputs.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2245",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "shi20e_interspeech": {
      "authors": [
        [
          "Jing",
          "Shi"
        ],
        [
          "Jiaming",
          "Xu"
        ],
        [
          "Yusuke",
          "Fujita"
        ],
        [
          "Shinji",
          "Watanabe"
        ],
        [
          "Bo",
          "Xu"
        ]
      ],
      "title": "Speaker-Conditional Chain Model for Speech Separation and Extraction",
      "original": "2418",
      "page_count": 5,
      "order": 556,
      "p1": "2707",
      "pn": "2711",
      "abstract": [
        "Speech separation has been extensively explored to tackle the cocktail\nparty problem. However, these studies are still far from having enough\ngeneralization capabilities for real scenarios. In this work, we raise\na common strategy named Speaker-Conditional Chain Model to process\ncomplex speech recordings. In the proposed method, our model first\ninfers the identities of variable numbers of speakers from the observation\nbased on a sequence-to-sequence model. Then, it takes the information\nfrom the inferred speakers as conditions to extract their speech sources.\nWith the predicted speaker information from whole observation, our\nmodel is helpful to solve the problem of conventional speech separation\nand speaker extraction for multi-round long recordings. The experiments\nfrom standard fully-overlapped speech separation benchmarks show comparable\nresults with prior studies, while our proposed model gets better adaptability\nfor multi-round long recordings.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2418",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "nortje20_interspeech": {
      "authors": [
        [
          "Leanne",
          "Nortje"
        ],
        [
          "Herman",
          "Kamper"
        ]
      ],
      "title": "Unsupervised vs. Transfer Learning for Multimodal One-Shot Matching of Speech and Images",
      "original": "0087",
      "page_count": 5,
      "order": 557,
      "p1": "2712",
      "pn": "2716",
      "abstract": [
        "We consider the task of multimodal one-shot speech-image matching.\nAn agent is shown a picture along with a spoken word describing the\nobject in the picture, e.g.  cookie, broccoli and  ice-cream. After\nobserving  one paired speech-image example per class, it is shown a\nnew set of unseen pictures, and asked to pick the &#8220;ice-cream&#8221;.\nPrevious work attempted to tackle this problem using transfer learning:\nsupervised models are trained on labelled background data not containing\nany of the one-shot classes. Here we compare transfer learning to unsupervised\nmodels trained on unlabelled in-domain data. On a dataset of paired\nisolated spoken and visual digits, we specifically compare unsupervised\nautoencoder-like models to supervised classifier and Siamese neural\nnetworks. In both unimodal and multimodal few-shot matching experiments,\nwe find that transfer learning outperforms unsupervised training. We\nalso present experiments towards combining the two methodologies, but\nfind that transfer learning still performs best (despite idealised\nexperiments showing the benefits of unsupervised learning).\n"
      ],
      "doi": "10.21437/Interspeech.2020-87",
      "author_area_id": "10",
      "author_area_label": "Speech Recognition: Technologies and Systems for New Applications"
    },
    "lee20e_interspeech": {
      "authors": [
        [
          "Yoonhyung",
          "Lee"
        ],
        [
          "Seunghyun",
          "Yoon"
        ],
        [
          "Kyomin",
          "Jung"
        ]
      ],
      "title": "Multimodal Speech Emotion Recognition Using Cross Attention with Aligned Audio and Text",
      "original": "2312",
      "page_count": 5,
      "order": 558,
      "p1": "2717",
      "pn": "2721",
      "abstract": [
        "In this paper, we propose a novel speech emotion recognition model\ncalled Cross Attention Network (CAN) that uses aligned audio and text\nsignals as inputs. It is inspired by the fact that humans recognize\nspeech as a combination of simultaneously produced acoustic and textual\nsignals. First, our method segments the audio and the underlying text\nsignals into equal number of steps in an aligned way so that the same\ntime steps of the sequential signals cover the same time span in the\nsignals. Together with this technique, we apply the cross attention\nto aggregate the sequential information from the aligned signals. In\nthe cross attention, each modality is aggregated independently by applying\nthe global attention mechanism onto each modality. Then, the attention\nweights of each modality are applied directly to the other modality\nin a crossed way, so that the CAN gathers the audio and text information\nfrom the same time steps based on each modality. In the experiments\nconducted on the standard IEMOCAP dataset, our model outperforms the\nstate-of-the-art systems by 2.66% and 3.18% relatively in terms of\nthe weighted and unweighted accuracy.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2312",
      "author_area_id": "10",
      "author_area_label": "Speech Recognition: Technologies and Systems for New Applications"
    },
    "csapo20_interspeech": {
      "authors": [
        [
          "Tam\u00e1s G\u00e1bor",
          "Csap\u00f3"
        ]
      ],
      "title": "Speaker Dependent Articulatory-to-Acoustic Mapping Using Real-Time MRI of the Vocal Tract",
      "original": "0015",
      "page_count": 5,
      "order": 559,
      "p1": "2722",
      "pn": "2726",
      "abstract": [
        "Articulatory-to-acoustic (forward) mapping is a technique to predict\nspeech using various articulatory acquisition techniques (e.g. ultrasound\ntongue imaging, lip video). Real-time MRI (rtMRI) of the vocal tract\nhas not been used before for this purpose. The advantage of MRI is\nthat it has a high &#8216;relative&#8217; spatial resolution: it can\ncapture not only lingual, labial and jaw motion, but also the velum\nand the pharyngeal region, which is typically not possible with other\ntechniques. In the current paper, we train various DNNs (fully connected,\nconvolutional and recurrent neural networks) for articulatory-to-speech\nconversion, using rtMRI as input, in a speaker-specific way. We use\ntwo male and two female speakers of the USC-TIMIT articulatory database,\neach of them uttering 460 sentences. We evaluate the results with objective\n(Normalized MSE and MCD) and subjective measures (perceptual test)\nand show that CNN-LSTM networks are preferred which take multiple images\nas input, and achieve MCD scores between 2.8&#8211;4.5 dB. In the experiments,\nwe find that the predictions of speaker &#8216;m1&#8217; are significantly\nweaker than other speakers. We show that this is caused by the fact\nthat 74% of the recordings of speaker &#8216;m1&#8217; are out of sync.\n"
      ],
      "doi": "10.21437/Interspeech.2020-15",
      "author_area_id": "10",
      "author_area_label": "Speech Recognition: Technologies and Systems for New Applications"
    },
    "csapo20b_interspeech": {
      "authors": [
        [
          "Tam\u00e1s G\u00e1bor",
          "Csap\u00f3"
        ],
        [
          "Csaba",
          "Zaink\u00f3"
        ],
        [
          "L\u00e1szl\u00f3",
          "T\u00f3th"
        ],
        [
          "G\u00e1bor",
          "Gosztolya"
        ],
        [
          "Alexandra",
          "Mark\u00f3"
        ]
      ],
      "title": "Ultrasound-Based Articulatory-to-Acoustic Mapping with WaveGlow Speech Synthesis",
      "original": "1031",
      "page_count": 5,
      "order": 560,
      "p1": "2727",
      "pn": "2731",
      "abstract": [
        "For articulatory-to-acoustic mapping using deep neural networks, typically\nspectral and excitation parameters of vocoders have been used as the\ntraining targets. However, vocoding often results in buzzy and muffled\nfinal speech quality. Therefore, in this paper on ultrasound-based\narticulatory-to-acoustic conversion, we use a flow-based neural vocoder\n(WaveGlow) pre-trained on a large amount of English and Hungarian speech\ndata. The inputs of the convolutional neural network are ultrasound\ntongue images. The training target is the 80-dimensional mel-spectrogram,\nwhich results in a finer detailed spectral representation than the\npreviously used 25-dimensional Mel-Generalized Cepstrum. From the output\nof the ultrasound-to-mel-spectrogram prediction, WaveGlow inference\nresults in synthesized speech. We compare the proposed WaveGlow-based\nsystem with a continuous vocoder which does not use strict voiced/unvoiced\ndecision when predicting F0. The results demonstrate that during the\narticulatory-to-acoustic mapping experiments, the WaveGlow neural vocoder\nproduces significantly more natural synthesized speech than the baseline\nsystem. Besides, the advantage of WaveGlow is that F0 is included in\nthe mel-spectrogram representation, and it is not necessary to predict\nthe excitation separately.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1031",
      "author_area_id": "10",
      "author_area_label": "Speech Recognition: Technologies and Systems for New Applications"
    },
    "feng20d_interspeech": {
      "authors": [
        [
          "Siyuan",
          "Feng"
        ],
        [
          "Odette",
          "Scharenborg"
        ]
      ],
      "title": "Unsupervised Subword Modeling Using Autoregressive Pretraining and Cross-Lingual Phone-Aware Modeling",
      "original": "1170",
      "page_count": 5,
      "order": 561,
      "p1": "2732",
      "pn": "2736",
      "abstract": [
        "This study addresses unsupervised subword modeling, i.e., learning\nfeature representations that can distinguish subword units of a language.\nThe proposed approach adopts a two-stage bottleneck feature (BNF) learning\nframework, consisting of autoregressive predictive coding (APC) as\na front-end and a DNN-BNF model as a back-end. APC pretrained features\nare set as input features to a DNN-BNF model. A language-mismatched\nASR system is used to provide cross-lingual phone labels for DNN-BNF\nmodel training. Finally, BNFs are extracted as the subword-discriminative\nfeature representation. A second aim of this work is to investigate\nthe robustness of our approach&#8217;s effectiveness to different amounts\nof training data. The results on Libri-light and the ZeroSpeech 2017\ndatabases show that APC is effective in front-end feature pretraining.\nOur whole system outperforms the state of the art on both databases.\nCross-lingual phone labels for English data by a Dutch ASR outperform\nthose by a Mandarin ASR, possibly linked to the larger similarity of\nDutch compared to Mandarin with English. Our system is less sensitive\nto training data amount when the training data is over 50 hours. APC\npretraining leads to a reduction of needed training material from over\n5,000 hours to around 200 hours with little performance degradation.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1170",
      "author_area_id": "10",
      "author_area_label": "Speech Recognition: Technologies and Systems for New Applications"
    },
    "matsuura20_interspeech": {
      "authors": [
        [
          "Kohei",
          "Matsuura"
        ],
        [
          "Masato",
          "Mimura"
        ],
        [
          "Shinsuke",
          "Sakai"
        ],
        [
          "Tatsuya",
          "Kawahara"
        ]
      ],
      "title": "Generative Adversarial Training Data Adaptation for Very Low-Resource Automatic Speech Recognition",
      "original": "1195",
      "page_count": 5,
      "order": 562,
      "p1": "2737",
      "pn": "2741",
      "abstract": [
        "It is important to transcribe and archive speech data of endangered\nlanguages for preserving heritages of verbal culture and automatic\nspeech recognition (ASR) is a powerful tool to facilitate this process.\nHowever, since endangered languages do not generally have large corpora\nwith many speakers, the performance of ASR models trained on them are\nconsiderably poor in general. Nevertheless, we are often left with\na lot of recordings of spontaneous speech data that have to be transcribed.\nIn this work, for mitigating this speaker sparsity problem, we propose\nto convert the whole training speech data and make it sound like the\ntest speaker in order to develop a highly accurate ASR system for this\nspeaker. For this purpose, we utilize a CycleGAN-based non-parallel\nvoice conversion technology to forge a labeled training data that is\nclose to the test speaker&#8217;s speech. We evaluated this speaker\nadaptation approach on two low-resource corpora, namely, Ainu and Mboshi.\nWe obtained 35&#8211;60% relative improvement in phone error rate on\nthe Ainu corpus, and 40% relative improvement was attained on the Mboshi\ncorpus. This approach outperformed two conventional methods namely\nunsupervised adaptation and multilingual training with these two corpora.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1195",
      "author_area_id": "10",
      "author_area_label": "Speech Recognition: Technologies and Systems for New Applications"
    },
    "tsunematsu20_interspeech": {
      "authors": [
        [
          "Kazuki",
          "Tsunematsu"
        ],
        [
          "Johanes",
          "Effendi"
        ],
        [
          "Sakriani",
          "Sakti"
        ],
        [
          "Satoshi",
          "Nakamura"
        ]
      ],
      "title": "Neural Speech Completion",
      "original": "2110",
      "page_count": 5,
      "order": 563,
      "p1": "2742",
      "pn": "2746",
      "abstract": [
        "During a conversation, humans often predict the end of a sentence even\nwhen the other person has not finished it. In contrast, most current\nautomatic speech recognition systems remain limited to passively recognizing\nwhat is being said. But applications like voice search, simultaneous\nspeech translation, and spoken language communication may require a\nsystem that not only recognizes what has been said but also predicts\nwhat will be said. This paper proposes a speech completion system based\non deep learning and discusses the construction in a text-to-text,\nspeech-to-text, and speech-to-speech framework. We evaluate our system\non domain-specific sentences with synthesized speech utterances that\nare only 25%, 50%, or 75% complete. Our proposed systems provide more\nnatural suggestions than the Bidirectional Encoder Representations\nfrom Transformers (BERT) language representation model.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2110",
      "author_area_id": "10",
      "author_area_label": "Speech Recognition: Technologies and Systems for New Applications"
    },
    "milde20_interspeech": {
      "authors": [
        [
          "Benjamin",
          "Milde"
        ],
        [
          "Chris",
          "Biemann"
        ]
      ],
      "title": "Improving Unsupervised Sparsespeech Acoustic Models with Categorical Reparameterization",
      "original": "2629",
      "page_count": 5,
      "order": 564,
      "p1": "2747",
      "pn": "2751",
      "abstract": [
        "The Sparsespeech model is an unsupervised acoustic model that can generate\ndiscrete pseudo-labels for untranscribed speech. We extend the Sparsespeech\nmodel to allow for sampling over a random discrete variable, yielding\npseudo-posteriorgrams. The degree of sparsity in this posteriorgram\ncan be fully controlled after the model has been trained. We use the\nGumbel-Softmax trick to approximately sample from a discrete distribution\nin the neural network and this allows us to train the network efficiently\nwith standard backpropagation. The new and improved model is trained\nand evaluated on the Libri-Light corpus, a benchmark for ASR with limited\nor no supervision. The model is trained on 600h and 6000h of English\nread speech. We evaluate the improved model using the ABX error measure\nand a semi-supervised setting with 10h of transcribed speech. We observe\na relative improvement of up to 31.3% on ABX error rates within speakers\nand 22.5% across speakers with the improved Sparsespeech model on 600h\nof speech data and further improvements when scaling the model to 6000h.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2629",
      "author_area_id": "10",
      "author_area_label": "Speech Recognition: Technologies and Systems for New Applications"
    },
    "papadimitriou20_interspeech": {
      "authors": [
        [
          "Katerina",
          "Papadimitriou"
        ],
        [
          "Gerasimos",
          "Potamianos"
        ]
      ],
      "title": "Multimodal Sign Language Recognition via Temporal Deformable Convolutional Sequence Learning",
      "original": "2691",
      "page_count": 5,
      "order": 565,
      "p1": "2752",
      "pn": "2756",
      "abstract": [
        "In this paper we address the challenging problem of sign language recognition\n(SLR) from videos, introducing an end-to-end deep learning approach\nthat relies on the fusion of a number of spatio-temporal feature streams,\nas well as a fully convolutional encoder-decoder for prediction. Specifically,\nwe examine the contribution of optical flow, human skeletal features,\nas well as appearance features of handshapes and mouthing, in conjunction\nwith a temporal deformable convolutional attention-based encoder-decoder\nfor SLR. To our knowledge, this is the first use in this task of a\nfully convolutional multi-step attention-based encoder-decoder employing\ntemporal deformable convolutional block structures. We conduct experiments\non three sign language datasets and compare our approach to existing\nstate-of-the-art SLR methods, demonstrating its superiority.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2691",
      "author_area_id": "10",
      "author_area_label": "Speech Recognition: Technologies and Systems for New Applications"
    },
    "pratap20_interspeech": {
      "authors": [
        [
          "Vineel",
          "Pratap"
        ],
        [
          "Qiantong",
          "Xu"
        ],
        [
          "Anuroop",
          "Sriram"
        ],
        [
          "Gabriel",
          "Synnaeve"
        ],
        [
          "Ronan",
          "Collobert"
        ]
      ],
      "title": "MLS: A Large-Scale Multilingual Dataset for Speech Research",
      "original": "2826",
      "page_count": 5,
      "order": 566,
      "p1": "2757",
      "pn": "2761",
      "abstract": [
        "This paper introduces Multilingual LibriSpeech (MLS) dataset, a large\nmultilingual corpus suitable for speech research. The dataset is derived\nfrom read audiobooks from LibriVox and consists of 8 languages, including\nabout 32K hours of English and a total of 4.5K hours for other languages.\nWe provide baseline Automatic Speech Recognition (ASR) models and Language\nModels (LM) for all the languages in our dataset. We believe such a\nlarge transcribed dataset will open new avenues in ASR and Text-To-Speech\n(TTS) research. The dataset will be made freely available for anyone\nat <KBD>http://www.openslr.org</KBD>\n"
      ],
      "doi": "10.21437/Interspeech.2020-2826",
      "author_area_id": "10",
      "author_area_label": "Speech Recognition: Technologies and Systems for New Applications"
    },
    "parmonangan20_interspeech": {
      "authors": [
        [
          "Ivan Halim",
          "Parmonangan"
        ],
        [
          "Hiroki",
          "Tanaka"
        ],
        [
          "Sakriani",
          "Sakti"
        ],
        [
          "Satoshi",
          "Nakamura"
        ]
      ],
      "title": "Combining Audio and Brain Activity for Predicting Speech Quality",
      "original": "1559",
      "page_count": 5,
      "order": 567,
      "p1": "2762",
      "pn": "2766",
      "abstract": [
        "Since the perceived audio quality of the synthesized speech may determine\na system&#8217;s market success, quality evaluations are critical.\nAudio quality evaluations are usually done in either subjectively or\nobjectively. Due to their costly and time-consuming nature, the subjective\napproaches have generally been replaced by the faster, more cost-efficient\nobjective approaches. The primary downside of the objective approaches\nprimarily is that they lack the human influence factors which are crucial\nfor deriving the subjective perception of quality. However, it cannot\nbe observed directly and manifested in individual brain activity. Thus,\nwe combined predictions from single-subject electroencephalograph (EEG)\ninformation and audio features to improve the predictions of the overall\nquality of synthesized speech. Our result shows that by combining the\nresults from both audio and EEG models, a very simple neural network\ncan surpass the performance of the single-modal approach.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1559",
      "author_area_id": "13",
      "author_area_label": "Special Sessions"
    },
    "sharon20_interspeech": {
      "authors": [
        [
          "Rini A.",
          "Sharon"
        ],
        [
          "Hema A.",
          "Murthy"
        ]
      ],
      "title": "The &#8220;Sound of Silence&#8221; in EEG &#8212; Cognitive Voice Activity Detection",
      "original": "2383",
      "page_count": 5,
      "order": 568,
      "p1": "2767",
      "pn": "2771",
      "abstract": [
        "Speech cognition bears potential application as a brain computer interface\nthat can improve the quality of life for the otherwise communication\nimpaired people. While speech and resting state EEG are popularly studied,\nhere we attempt to explore a &#8220;non-speech&#8221; (NS) state of\nbrain activity corresponding to the silence regions of speech audio.\nFirstly, speech perception is studied to inspect the existence of such\na state, followed by its identification in speech imagination. Analogous\nto how voice activity detection is employed to enhance the performance\nof speech recognition, the EEG state activity detection protocol implemented\nhere is applied to boost the confidence of imagined speech EEG decoding.\nClassification of speech and NS state is done using two datasets collected\nfrom laboratory-based and commercial-based devices. The state sequential\ninformation thus obtained is further utilized to reduce the search\nspace of imagined EEG unit recognition. Temporal signal structures\nand topographic maps of NS states are visualized across subjects and\nsessions. The recognition performance and the visual distinction observed\ndemonstrates the existence of silence signatures in EEG.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2383",
      "author_area_id": "13",
      "author_area_label": "Special Sessions"
    },
    "cai20_interspeech": {
      "authors": [
        [
          "Siqi",
          "Cai"
        ],
        [
          "Enze",
          "Su"
        ],
        [
          "Yonghao",
          "Song"
        ],
        [
          "Longhan",
          "Xie"
        ],
        [
          "Haizhou",
          "Li"
        ]
      ],
      "title": "Low Latency Auditory Attention Detection with Common Spatial Pattern Analysis of EEG Signals",
      "original": "2496",
      "page_count": 5,
      "order": 569,
      "p1": "2772",
      "pn": "2776",
      "abstract": [
        "A listener listens to one speech stream at a time in a multi-speaker\nscenario. EEG-based auditory attention detection (AAD) aims to identify\nto which speech stream the listener has attended using EEG signals.\nThe performance of linear modeling approaches is limited due to the\nnon-linear nature of the human auditory perception. Furthermore, the\nreal-world applications call for low latency AAD solutions in noisy\nenvironments. In this paper, we propose to adopt common spatial pattern\n(CSP) analysis to enhance the discriminative ability of EEG signals.\nWe study the use of convolutional neural network (CNN) as the non-linear\nsolution. The experiments show that it is possible to decode auditory\nattention within 2 seconds, with a competitive accuracy of 80.2%, even\nin noisy acoustic environments. The results are encouraging for brain-computer\ninterfaces, such as hearing aids, which require real-time responses,\nand robust AAD in complex acoustic environments.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2496",
      "author_area_id": "13",
      "author_area_label": "Special Sessions"
    },
    "angrick20_interspeech": {
      "authors": [
        [
          "Miguel",
          "Angrick"
        ],
        [
          "Christian",
          "Herff"
        ],
        [
          "Garett",
          "Johnson"
        ],
        [
          "Jerry",
          "Shih"
        ],
        [
          "Dean",
          "Krusienski"
        ],
        [
          "Tanja",
          "Schultz"
        ]
      ],
      "title": "Speech Spectrogram Estimation from Intracranial Brain Activity Using a Quantization Approach",
      "original": "2946",
      "page_count": 5,
      "order": 570,
      "p1": "2777",
      "pn": "2781",
      "abstract": [
        "Direct synthesis from intracranial brain activity into acoustic speech\nmight provide an intuitive and natural communication means for speech-impaired\nusers. In previous studies we have used logarithmic Mel-scaled speech\nspectrograms (logMels) as an intermediate representation in the decoding\nfrom ElectroCorticoGraphic (ECoG) recordings to an audible waveform.\nMel-scaled speech spectrograms have a long tradition in acoustic speech\nprocessing and speech synthesis applications. In the past, we relied\non regression approaches to find a mapping from brain activity to logMel\nspectral coefficients, due to the continuous feature space. However,\nregression tasks are unbounded and thus neuronal fluctuations in brain\nactivity may result in abnormally high amplitudes in a synthesized\nacoustic speech signal. To mitigate these issues, we propose two methods\nfor quantization of power values to discretize the feature space of\nlogarithmic Mel-scaled spectral coefficients by using the median and\nthe logistic formula, respectively, to reduce the complexity and restricting\nthe number of intervals. We evaluate the practicability in a proof-of-concept\nwith one participant through a simple classification based on linear\ndiscriminant analysis and compare the resulting waveform with the original\nspeech. Reconstructed spectrograms achieve Pearson correlation coefficients\nwith a mean of r=0.5 &#177; 0.11 in a 5-fold cross validation.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2946",
      "author_area_id": "13",
      "author_area_label": "Special Sessions"
    },
    "dash20_interspeech": {
      "authors": [
        [
          "Debadatta",
          "Dash"
        ],
        [
          "Paul",
          "Ferrari"
        ],
        [
          "Angel",
          "Hernandez"
        ],
        [
          "Daragh",
          "Heitzman"
        ],
        [
          "Sara G.",
          "Austin"
        ],
        [
          "Jun",
          "Wang"
        ]
      ],
      "title": "Neural Speech Decoding for Amyotrophic Lateral Sclerosis",
      "original": "3071",
      "page_count": 5,
      "order": 571,
      "p1": "2782",
      "pn": "2786",
      "abstract": [
        "Amyotrophic lateral sclerosis (ALS) is a motor neuron disease that\nmay cause locked-in syndrome (completely paralyzed but aware). These\nlocked-in patients can communicate with brain-computer interfaces (BCI),\ne.g. EEG spellers, which have a low communication rate. Recent research\nhas progressed towards neural speech decoding paradigms that have the\npotential for normal communication rates. Yet, current neural decoding\nresearch is limited to typical speakers and the extent to which these\nstudies can be translated to a target population (e.g., ALS) is still\nunexplored. Here, we investigated the decoding of imagined and spoken\nphrases from non-invasive magnetoencephalography (MEG) signals of ALS\nsubjects using several spectral features (band-power of brainwaves:\ndelta, theta, alpha, beta, and gamma frequencies) with seven machine\nlearning decoders (Naive Bayes, K-nearest neighbor, decision tree,\nensemble, support vector machine, linear discriminant analysis, and\nartificial neural network). Experimental results indicated that the\ndecoding performance for ALS patients is lower than healthy subjects\nyet significantly higher than chance level. The best performances were\n75% for decoding five imagined phrases and 88% for five spoken phrases\nfrom ALS patients. To our knowledge, this is the first demonstration\nof neural speech decoding for a speech disordered population.\n"
      ],
      "doi": "10.21437/Interspeech.2020-3071",
      "author_area_id": "13",
      "author_area_label": "Special Sessions"
    },
    "chen20m_interspeech": {
      "authors": [
        [
          "Yang",
          "Chen"
        ],
        [
          "Weiran",
          "Wang"
        ],
        [
          "Chao",
          "Wang"
        ]
      ],
      "title": "Semi-Supervised ASR by End-to-End Self-Training",
      "original": "1280",
      "page_count": 5,
      "order": 572,
      "p1": "2787",
      "pn": "2791",
      "abstract": [
        "While deep learning based end-to-end automatic speech recognition (ASR)\nsystems have greatly simplified modeling pipelines, they suffer from\nthe data sparsity issue. In this work, we propose a self-training method\nwith an end-to-end system for semi-supervised ASR. Starting from a\nConnectionist Temporal Classification (CTC) system trained on the supervised\ndata, we iteratively generate pseudo-labels on a mini-batch of unsupervised\nutterances with the current model, and use the pseudo-labels to augment\nthe supervised data for immediate model update. Our method retains\nthe simplicity of end-to-end ASR systems, and can be seen as performing\nalternating optimization over a well-defined learning objective. We\nalso perform empirical investigations of our method, regarding the\neffect of data augmentation, decoding beamsize for pseudo-label generation,\nand freshness of pseudo-labels. On a commonly used semi-supervised\nASR setting with the Wall Street Journal (WSJ) corpus, our method gives\n14.4% relative WER improvement over a carefully-trained base system\nwith data augmentation, reducing the performance gap between the base\nsystem and the oracle system by 46%.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1280",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "tulsiani20_interspeech": {
      "authors": [
        [
          "Hitesh",
          "Tulsiani"
        ],
        [
          "Ashtosh",
          "Sapru"
        ],
        [
          "Harish",
          "Arsikere"
        ],
        [
          "Surabhi",
          "Punjabi"
        ],
        [
          "Sri",
          "Garimella"
        ]
      ],
      "title": "Improved Training Strategies for End-to-End Speech Recognition in Digital Voice Assistants",
      "original": "2036",
      "page_count": 5,
      "order": 573,
      "p1": "2792",
      "pn": "2796",
      "abstract": [
        "The speech recognition training data corresponding to digital voice\nassistants is dominated by wake-words. Training end-to-end (E2E) speech\nrecognition models without careful attention to such data results in\nsub-optimal performance as models prioritize learning wake-words. To\naddress this problem, we propose a novel discriminative initialization\nstrategy by introducing a regularization term to penalize model for\nincorrectly hallucinating wake-words in early phases of training. We\nalso explore other training strategies such as multi-task learning\nwith listen-attend-spell (LAS), label smoothing via probabilistic modelling\nof silence and use of multiple pronunciations, and show how they can\nbe combined with the proposed initialization technique. In addition,\nwe show the connection between cost function of proposed discriminative\ninitialization technique and minimum word error rate (MWER) criterion.\nWe evaluate our methods on two E2E ASR systems, a phone-based system\nand a word-piece based system, trained on 6500 hours of Alexa&#8217;s\nIndian English speech corpus. We show that proposed techniques yield\n20% word error rate reductions for phone based system and 6% for word-piece\nbased system compared to corresponding baselines trained on the same\ndata.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2036",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "kanda20b_interspeech": {
      "authors": [
        [
          "Naoyuki",
          "Kanda"
        ],
        [
          "Yashesh",
          "Gaur"
        ],
        [
          "Xiaofei",
          "Wang"
        ],
        [
          "Zhong",
          "Meng"
        ],
        [
          "Takuya",
          "Yoshioka"
        ]
      ],
      "title": "Serialized Output Training for End-to-End Overlapped Speech Recognition",
      "original": "0999",
      "page_count": 5,
      "order": 574,
      "p1": "2797",
      "pn": "2801",
      "abstract": [
        "This paper proposes serialized output training (SOT), a novel framework\nfor multi-speaker overlapped speech recognition based on an attention-based\nencoder-decoder approach. Instead of having multiple output layers\nas with the permutation invariant training (PIT), SOT uses a model\nwith only one output layer that generates the transcriptions of multiple\nspeakers one after another. The attention and decoder modules take\ncare of producing multiple transcriptions from overlapped speech. SOT\nhas two advantages over PIT: (1) no limitation in the maximum number\nof speakers, and (2) an ability to model the dependencies among outputs\nfor different speakers. We also propose a simple trick that allows\nSOT to be executed in O(S), where S is the number of the speakers in\nthe training sample, by using the start times of the constituent source\nutterances. Experimental results on LibriSpeech corpus show that the\nSOT models can transcribe overlapped speech with variable numbers of\nspeakers significantly better than PIT-based models. We also show that\nthe SOT models can accurately count the number of speakers in the input\naudio.\n"
      ],
      "doi": "10.21437/Interspeech.2020-999",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "weninger20_interspeech": {
      "authors": [
        [
          "Felix",
          "Weninger"
        ],
        [
          "Franco",
          "Mana"
        ],
        [
          "Roberto",
          "Gemello"
        ],
        [
          "Jes\u00fas",
          "Andr\u00e9s-Ferrer"
        ],
        [
          "Puming",
          "Zhan"
        ]
      ],
      "title": "Semi-Supervised Learning with Data Augmentation for End-to-End ASR",
      "original": "1337",
      "page_count": 5,
      "order": 575,
      "p1": "2802",
      "pn": "2806",
      "abstract": [
        "In this paper, we apply Semi-Supervised Learning (SSL) along with Data\nAugmentation (DA) for improving the accuracy of End-to-End ASR. We\nfocus on the consistency regularization principle, which has been successfully\napplied to image classification tasks, and present sequence-to-sequence\n(seq2seq) versions of the FixMatch and Noisy Student algorithms. Specifically,\nwe generate the pseudo labels for the unlabeled data on-the-fly with\na seq2seq model after perturbing the input features with DA. We also\npropose soft label variants of both algorithms to cope with pseudo\nlabel errors, showing further performance improvements. We conduct\nSSL experiments on a conversational speech data set (doctor-patient\nconversations) with 1.9 kh manually transcribed training data, using\nonly 25% of the original labels (475 h labeled data). In the result,\nthe Noisy Student algorithm with soft labels and consistency regularization\nachieves 10.4% word error rate (WER) reduction when adding 475 h of\nunlabeled data, corresponding to a recovery rate of 92%. Furthermore,\nwhen iteratively adding 950 h more unlabeled data, our best SSL performance\nis within 5% WER increase compared to using the full labeled training\nset (recovery rate: 78%).\n"
      ],
      "doi": "10.21437/Interspeech.2020-1337",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "guo20_interspeech": {
      "authors": [
        [
          "Jinxi",
          "Guo"
        ],
        [
          "Gautam",
          "Tiwari"
        ],
        [
          "Jasha",
          "Droppo"
        ],
        [
          "Maarten Van",
          "Segbroeck"
        ],
        [
          "Che-Wei",
          "Huang"
        ],
        [
          "Andreas",
          "Stolcke"
        ],
        [
          "Roland",
          "Maas"
        ]
      ],
      "title": "Efficient Minimum Word Error Rate Training of RNN-Transducer for End-to-End Speech Recognition",
      "original": "1557",
      "page_count": 5,
      "order": 576,
      "p1": "2807",
      "pn": "2811",
      "abstract": [
        "In this work, we propose a novel and efficient minimum word error rate\n(MWER) training method for RNN-Transducer (RNN-T). Unlike previous\nwork on this topic, which performs on-the-fly limited-size beam-search\ndecoding and generates alignment scores for expected edit-distance\ncomputation, in our proposed method, we re-calculate and sum scores\nof all the possible alignments for each hypothesis in N-best lists.\nThe hypothesis probability scores and back-propagated gradients are\ncalculated efficiently using the forward-backward algorithm. Moreover,\nthe proposed method allows us to decouple the decoding and training\nprocesses, and thus we can perform offline parallel-decoding and MWER\ntraining for each subset iteratively. Experimental results show that\nthis proposed semi-on-the-fly method can speed up the on-the-fly method\nby 6 times and result in a similar WER improvement (3.6%) over a baseline\nRNN-T model. The proposed MWER training can also effectively reduce\nhigh-deletion errors (9.2% WER-reduction) introduced by RNN-T models\nwhen EOS is added for end-pointer. Further improvement can be achieved\nif we use a proposed RNN-T rescoring method to re-rank hypotheses and\nuse external RNN-LM to perform additional rescoring. The best system\nachieves a 5% relative improvement on an English test-set of real far-field\nrecordings and a 11.6% WER reduction on music-domain utterances.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1557",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "zeyer20_interspeech": {
      "authors": [
        [
          "Albert",
          "Zeyer"
        ],
        [
          "Andr\u00e9",
          "Merboldt"
        ],
        [
          "Ralf",
          "Schl\u00fcter"
        ],
        [
          "Hermann",
          "Ney"
        ]
      ],
      "title": "A New Training Pipeline for an Improved Neural Transducer",
      "original": "1855",
      "page_count": 5,
      "order": 577,
      "p1": "2812",
      "pn": "2816",
      "abstract": [
        "The  RNN transducer is a promising end-to-end model candidate. We compare\nthe original training criterion with the full marginalization over\nall alignments, to the commonly used maximum approximation, which simplifies,\nimproves and speeds up our training. We also generalize from the original\nneural network model and study more powerful models, made possible\ndue to the maximum approximation. We further generalize the output\nlabel topology to cover RNN-T, RNA and CTC. We perform several studies\namong all these aspects, including a study on the effect of external\nalignments. We find that the transducer model generalizes much better\non longer sequences than the attention model. Our final transducer\nmodel outperforms our attention model on Switchboard 300h by over 6%\nrelative WER.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1855",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "park20d_interspeech": {
      "authors": [
        [
          "Daniel S.",
          "Park"
        ],
        [
          "Yu",
          "Zhang"
        ],
        [
          "Ye",
          "Jia"
        ],
        [
          "Wei",
          "Han"
        ],
        [
          "Chung-Cheng",
          "Chiu"
        ],
        [
          "Bo",
          "Li"
        ],
        [
          "Yonghui",
          "Wu"
        ],
        [
          "Quoc V.",
          "Le"
        ]
      ],
      "title": "Improved Noisy Student Training for Automatic Speech Recognition",
      "original": "1470",
      "page_count": 5,
      "order": 578,
      "p1": "2817",
      "pn": "2821",
      "abstract": [
        "Recently, a semi-supervised learning method known as &#8220;noisy student\ntraining&#8221; has been shown to improve image classification performance\nof deep networks significantly. Noisy student training is an iterative\nself-training method that leverages augmentation to improve network\nperformance. In this work, we adapt and improve noisy student training\nfor automatic speech recognition, employing (adaptive) SpecAugment\nas the augmentation method. We find effective methods to filter, balance\nand augment the data generated in between self-training iterations.\nBy doing so, we are able to obtain word error rates (WERs) 4.2%/8.6%\non the clean/noisy LibriSpeech test sets by only using the clean 100h\nsubset of LibriSpeech as the supervised set and the rest (860h) as\nthe unlabeled set. Furthermore, we are able to achieve WERs 1.7%/3.4%\non the clean/noisy LibriSpeech test sets by using the unlab-60k subset\nof LibriLight as the unlabeled set for LibriSpeech 960h. We are thus\nable to improve upon the previous state-of-the-art clean/noisy test\nWERs achieved on LibriSpeech 100h (4.74%/12.20%) and LibriSpeech (1.9%/4.1%).\n"
      ],
      "doi": "10.21437/Interspeech.2020-1470",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "masumura20_interspeech": {
      "authors": [
        [
          "Ryo",
          "Masumura"
        ],
        [
          "Naoki",
          "Makishima"
        ],
        [
          "Mana",
          "Ihori"
        ],
        [
          "Akihiko",
          "Takashima"
        ],
        [
          "Tomohiro",
          "Tanaka"
        ],
        [
          "Shota",
          "Orihashi"
        ]
      ],
      "title": "Phoneme-to-Grapheme Conversion Based Large-Scale Pre-Training for End-to-End Automatic Speech Recognition",
      "original": "1930",
      "page_count": 5,
      "order": 579,
      "p1": "2822",
      "pn": "2826",
      "abstract": [
        "This paper describes a simple and efficient pre-training method using\na large number of external texts to enhance end-to-end automatic speech\nrecognition (ASR). Generally, it is essential to prepare speech-to-text\npaired data to construct end-to-end ASR models, but it is difficult\nto collect a large amount of such data in practice. One issue caused\nby data scarcity is that the performance of ASR on out-of-domain tasks\ndifferent from those using the speech-to-text paired data is poor,\nsince the mapping from the speech information to textual information\nis not well learned. To address this problem, we leverage a large number\nof phoneme-to-grapheme (P2G) paired data, which can be easily created\nfrom external texts and a rich pronunciation dictionary. The P2G conversion\nand end-to-end ASR are regarded as similar transformation tasks where\nthe input phonetic information is converted into textual information.\nOur method utilizes the P2G conversion task for pre-training of a decoder\nnetwork in Transformer encoder-decoder based end-to-end ASR. Experiments\nusing 4 billion tokens of Web text demonstrates that the performance\nof ASR on out-of-domain tasks can be significantly improved by our\npre-training.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1930",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "gowda20_interspeech": {
      "authors": [
        [
          "Dhananjaya",
          "Gowda"
        ],
        [
          "Ankur",
          "Kumar"
        ],
        [
          "Kwangyoun",
          "Kim"
        ],
        [
          "Hejung",
          "Yang"
        ],
        [
          "Abhinav",
          "Garg"
        ],
        [
          "Sachin",
          "Singh"
        ],
        [
          "Jiyeon",
          "Kim"
        ],
        [
          "Mehul",
          "Kumar"
        ],
        [
          "Sichen",
          "Jin"
        ],
        [
          "Shatrughan",
          "Singh"
        ],
        [
          "Chanwoo",
          "Kim"
        ]
      ],
      "title": "Utterance Invariant Training for Hybrid Two-Pass End-to-End Speech Recognition",
      "original": "3230",
      "page_count": 5,
      "order": 580,
      "p1": "2827",
      "pn": "2831",
      "abstract": [
        "In this paper, we propose an utterance invariant training (UIT) specifically\ndesigned to improve the performance of a two-pass end-to-end hybrid\nASR. Our proposed hybrid ASR solution uses a shared encoder with a\nmonotonic chunkwise attention (MoChA) decoder for streaming capabilities,\nwhile using a low-latency bidirectional full-attention (BFA) decoder\nfor enhancing the overall ASR accuracy. A modified sequence summary\nnetwork (SSN) based utterance invariant training is used to suit the\ntwo-pass model architecture. The input feature stream self-conditioned\nby scaling and shifting with its own sequence summary is used as a\nconcatenative conditioning on the bidirectional encoder layers sitting\non top of the shared encoder. In effect, the proposed utterance invariant\ntraining combines three different types of conditioning namely, concatenative,\nmultiplicative and additive. Experimental results show that the proposed\napproach shows reduction in word error rates up to 7% relative on Librispeech,\nand 10&#8211;15% on a large scale Korean end-to-end two-pass hybrid\nASR model.\n"
      ],
      "doi": "10.21437/Interspeech.2020-3230",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "wang20aa_interspeech": {
      "authors": [
        [
          "Gary",
          "Wang"
        ],
        [
          "Andrew",
          "Rosenberg"
        ],
        [
          "Zhehuai",
          "Chen"
        ],
        [
          "Yu",
          "Zhang"
        ],
        [
          "Bhuvana",
          "Ramabhadran"
        ],
        [
          "Pedro J.",
          "Moreno"
        ]
      ],
      "title": "SCADA: Stochastic, Consistent and Adversarial Data Augmentation to Improve ASR",
      "original": "2920",
      "page_count": 5,
      "order": 581,
      "p1": "2832",
      "pn": "2836",
      "abstract": [
        "Recent developments in data augmentation has brought great gains in\nimprovement for automatic speech recognition (ASR). Parallel developments\nin augmentation policy search in computer vision domain has shown improvements\nin model performance and robustness. In addition, recent developments\nin semi-supervised learning has shown that consistency measures are\ncrucial for performance and robustness. In this work, we demonstrate\nthat combining augmentation policies with consistency measures and\nmodel regularization can greatly improve speech recognition performance.\nUsing the Librispeech task, we show: 1) symmetric consistency measures\nsuch as the Jensen-Shannon Divergence provide 4% relative improvements\nin ASR performance; 2) Augmented adversarial inputs using Virtual Adversarial\nNoise (VAT) provides 12% relative win; and 3) random sampling from\narbitrary combination of augmentation policies yields the best policy.\nThese contributions result in an overall reduction in Word Error Rate\n(WER) of 15% relative on the Librispeech task presented in this paper.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2920",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "das20b_interspeech": {
      "authors": [
        [
          "Sneha",
          "Das"
        ],
        [
          "Tom",
          "B\u00e4ckstr\u00f6m"
        ],
        [
          "Guillaume",
          "Fuchs"
        ]
      ],
      "title": "Fundamental Frequency Model for Postfiltering at Low Bitrates in a Transform-Domain Speech and Audio Codec",
      "original": "1067",
      "page_count": 5,
      "order": 582,
      "p1": "2837",
      "pn": "2841",
      "abstract": [
        "Speech codecs can use postfilters to improve the quality of the decoded\nsignal. While postfiltering is effective in reducing coding artifacts,\nsuch methods often involve processing in both the encoder and the decoder,\nrely on additional transmitted side information, or are highly dependent\non other codec functions for optimal performance. We propose a low-complexity\npostfiltering method to improve the harmonic structure of the decoded\nsignal, which models the fundamental frequency of the signal. In contrast\nto past approaches, the postfilter operates at the decoder as a standalone\nfunction and does not need the transmission of additional side information.\nIt can thus be used to enhance the output of any codec. We tested the\napproach on a modified version of the EVS codec in TCX mode only, which\nis subject to more pronounced coding artefacts when used at its lowest\nbitrate. Listening test results show an average improvement of 7 MUSHRA\npoints for decoded signals with the proposed harmonic postfilter.<SUP>1</SUP>\n"
      ],
      "doi": "10.21437/Interspeech.2020-1067",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "broucke20_interspeech": {
      "authors": [
        [
          "Arthur Van Den",
          "Broucke"
        ],
        [
          "Deepak",
          "Baby"
        ],
        [
          "Sarah",
          "Verhulst"
        ]
      ],
      "title": "Hearing-Impaired Bio-Inspired Cochlear Models for Real-Time Auditory Applications",
      "original": "2818",
      "page_count": 5,
      "order": 583,
      "p1": "2842",
      "pn": "2846",
      "abstract": [
        "Biophysically realistic models of the cochlea are based on cascaded\ntransmission-line (TL) models which capture longitudinal coupling,\ncochlear nonlinearities, as well as the human frequency selectivity.\nHowever, these models are slow to compute (order of seconds/minutes)\nwhile machine-hearing and hearing-aid applications require a real-time\nsolution. Consequently, real-time applications often adopt more basic\nand less time-consuming descriptions of cochlear processing (gamma-tone,\ndual resonance nonlinear) even though there are clear advantages in\nusing more biophysically correct models. To overcome this, we recently\ncombined nonlinear Deep Neural Networks (DNN) with analytical TL cochlear\nmodel descriptions to build a real-time model of cochlear processing\nwhich captures the biophysical properties associated with the TL model.\nIn this work, we aim to extend the normal-hearing DNN-based cochlear\nmodel (CoNNear) to simulate frequency-specific patterns of hearing\nsensitivity loss, yielding a set of normal and hearing-impaired auditory\nmodels which can be computed in real-time and are differentiable. They\ncan hence be used in backpropagation networks to develop the next generation\nof hearing-aid and machine hearing applications.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2818",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "skoglund20_interspeech": {
      "authors": [
        [
          "Jan",
          "Skoglund"
        ],
        [
          "Jean-Marc",
          "Valin"
        ]
      ],
      "title": "Improving Opus Low Bit Rate Quality with Neural Speech Synthesis",
      "original": "2939",
      "page_count": 5,
      "order": 584,
      "p1": "2847",
      "pn": "2851",
      "abstract": [
        "The voice mode of the Opus audio coder can compress wideband speech\nat bit rates ranging from 6 kb/s to 40 kb/s. However, Opus is at its\ncore a waveform matching coder, and as the rate drops below 10 kb/s,\nquality degrades quickly. As the rate reduces even further, parametric\ncoders tend to perform better than waveform coders. In this paper we\npropose a backward-compatible way of improving low bit rate Opus quality\nby resynthesizing speech from the decoded parameters. We compare two\ndifferent neural generative models, WaveNet and LPCNet. WaveNet is\na powerful, high-complexity, and high-latency architecture that is\nnot feasible for a practical system, yet provides a best known achievable\nquality with generative models. LPCNet is a low-complexity, low-latency\nRNN-based generative model, and practically implementable on mobile\nphones. We apply these systems with parameters from Opus coded at 6\nkb/s as conditioning features for the generative models. A listening\ntest shows that for the same 6 kb/s Opus bit stream, synthesized speech\nusing LPCNet clearly outperforms the output of the standard Opus decoder.\nThis opens up ways to improve the decoding quality of existing speech\nand audio waveform coders without breaking compatibility.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2939",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "manocha20_interspeech": {
      "authors": [
        [
          "Pranay",
          "Manocha"
        ],
        [
          "Adam",
          "Finkelstein"
        ],
        [
          "Richard",
          "Zhang"
        ],
        [
          "Nicholas J.",
          "Bryan"
        ],
        [
          "Gautham J.",
          "Mysore"
        ],
        [
          "Zeyu",
          "Jin"
        ]
      ],
      "title": "A Differentiable Perceptual Audio Metric Learned from Just Noticeable Differences",
      "original": "1191",
      "page_count": 5,
      "order": 585,
      "p1": "2852",
      "pn": "2856",
      "abstract": [
        "Many audio processing tasks require perceptual assessment. The &#8220;gold\nstandard&#8221; of obtaining human judgments is time-consuming, expensive,\nand cannot be used as an optimization criterion. On the other hand,\nautomated metrics are efficient to compute but often correlate poorly\nwith human judgment, particularly for audio differences at the threshold\nof human detection. In this work, we construct a metric by fitting\na deep neural network to a new large dataset of crowdsourced human\njudgments. Subjects are prompted to answer a straightforward, objective\nquestion: are two recordings identical or not? These pairs are algorithmically\ngenerated under a variety of perturbations, including noise, reverb,\nand compression artifacts; the perturbation space is probed with the\ngoal of efficiently identifying the just-noticeable difference (JND)\nlevel of the subject. We show that the resulting learned metric is\nwell-calibrated with human judgments, outperforming baseline methods.\nSince it is a deep network, the metric is differentiable, making it\nsuitable as a loss function for other tasks. Thus, simply replacing\nan existing loss (e.g., deep feature loss) with our metric yields significant\nimprovement in a denoising network, as measured by subjective pairwise\ncomparison.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1191",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "masztalski20_interspeech": {
      "authors": [
        [
          "Piotr",
          "Masztalski"
        ],
        [
          "Mateusz",
          "Matuszewski"
        ],
        [
          "Karol",
          "Piaskowski"
        ],
        [
          "Michal",
          "Romaniuk"
        ]
      ],
      "title": "StoRIR: Stochastic Room Impulse Response Generation for Audio Data Augmentation",
      "original": "2261",
      "page_count": 5,
      "order": 586,
      "p1": "2857",
      "pn": "2861",
      "abstract": [
        "In this paper we introduce StoRIR &#8212; a stochastic room impulse\nresponse generation method dedicated to audio data augmentation in\nmachine learning applications. This technique, in contrary to geometrical\nmethods like image-source or ray tracing, does not require prior definition\nof room geometry, absorption coefficients or microphone and source\nplacement and is dependent solely on the acoustic parameters of the\nroom. The method is intuitive, easy to implement and allows to generate\nRIRs of very complicated enclosures. We show that StoRIR, when used\nfor audio data augmentation in a speech enhancement task, allows deep\nlearning models to achieve better results on a wide range of metrics\nthan when using the conventional image-source method, effectively improving\nmany of them by more than 5%. We publish a Python implementation of\nStoRIR online<SUP>1</SUP>.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2261",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "naderi20_interspeech": {
      "authors": [
        [
          "Babak",
          "Naderi"
        ],
        [
          "Ross",
          "Cutler"
        ]
      ],
      "title": "An Open Source Implementation of ITU-T Recommendation P.808 with Validation",
      "original": "2665",
      "page_count": 5,
      "order": 587,
      "p1": "2862",
      "pn": "2866",
      "abstract": [
        "The ITU-T Recommendation P.808 provides a crowdsourcing approach for\nconducting a subjective assessment of speech quality using the Absolute\nCategory Rating (ACR) method. We provide an open-source implementation\nof the ITU-T Rec. P.808 that runs on the Amazon Mechanical Turk platform.\nWe extended our implementation to include Degradation Category Ratings\n(DCR) and Comparison Category Ratings (CCR) test methods. We also significantly\nspeed up the test process by integrating the participant qualification\nstep into the main rating task compared to a two-stage qualification\nand rating solution. We provide program scripts for creating and executing\nthe subjective test, and data cleansing and analyzing the answers to\navoid operational errors. To validate the implementation, we compare\nthe Mean Opinion Scores (MOS) collected through our implementation\nwith MOS values from a standard laboratory experiment conducted based\non the ITU-T Rec. P.800. We also evaluate the reproducibility of the\nresult of the subjective speech quality assessment through crowdsourcing\nusing our implementation. Finally, we quantify the impact of parts\nof the system designed to improve the reliability: environmental tests,\ngold and trapping questions, rating patterns, and a headset usage test.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2665",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "mittag20b_interspeech": {
      "authors": [
        [
          "Gabriel",
          "Mittag"
        ],
        [
          "Ross",
          "Cutler"
        ],
        [
          "Yasaman",
          "Hosseinkashi"
        ],
        [
          "Michael",
          "Revow"
        ],
        [
          "Sriram",
          "Srinivasan"
        ],
        [
          "Naglakshmi",
          "Chande"
        ],
        [
          "Robert",
          "Aichner"
        ]
      ],
      "title": "DNN No-Reference PSTN Speech Quality Prediction",
      "original": "2760",
      "page_count": 5,
      "order": 588,
      "p1": "2867",
      "pn": "2871",
      "abstract": [
        "Classic public switched telephone networks (PSTN) are often a black\nbox for VoIP network providers, as they have no access to performance\nindicators, such as delay or packet loss. Only the degraded output\nspeech signal can be used to monitor the speech quality of these networks.\nHowever, the current state-of-the-art speech quality models are not\nreliable enough to be used for live monitoring. One of the reasons\nfor this is that PSTN distortions can be unique depending on the provider\nand country, which makes it difficult to train a model that generalizes\nwell for different PSTN networks. In this paper, we present a new open-source\nPSTN speech quality test set with over 1000 crowdsourced real phone\ncalls. Our proposed no-reference model outperforms the full-reference\nPOLQA and no-reference P.563 on the validation and test set. Further,\nwe analyzed the influence of file cropping on the perceived speech\nquality and the influence of the number of ratings and training size\non the model accuracy.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2760",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "moller20_interspeech": {
      "authors": [
        [
          "Sebastian",
          "M\u00f6ller"
        ],
        [
          "Tobias",
          "H\u00fcbschen"
        ],
        [
          "Thilo",
          "Michael"
        ],
        [
          "Gabriel",
          "Mittag"
        ],
        [
          "Gerhard",
          "Schmidt"
        ]
      ],
      "title": "Non-Intrusive Diagnostic Monitoring of Fullband Speech Quality",
      "original": "1125",
      "page_count": 5,
      "order": 589,
      "p1": "2872",
      "pn": "2876",
      "abstract": [
        "With the advent of speech communication systems transmitting the full\naudible frequency band (0&#8211;20,000 Hz), traditional approaches\nfor narrowband (300&#8211;3,400 Hz) speech quality estimation, service\nplanning and monitoring come to their limits. Recently, signal-based\nas well as parametric tools have been developed for fullband speech\nquality prediction. These tools estimate overall quality, but do not\nprovide diagnostic information about the technical causes of degradations.\nIn the present paper, we evaluate approaches for diagnostically monitoring\nthe quality of super-wideband and fullband speech communication services.\nThe aim is, first, to estimate technical causes of degradations from\nthe degraded output signals, and, second, to combine the estimated\ncauses with parametric quality prediction models to obtain a quantitative\ndiagnostic picture of the quality-degrading aspects. We evaluate approaches\nfor non-intrusively identifying coding schemes and packet-loss, and\ncompare estimated quality to the predictions of an intrusive signal-based\nmodel.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1125",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "shahrebabaki20_interspeech": {
      "authors": [
        [
          "Abdolreza Sabzi",
          "Shahrebabaki"
        ],
        [
          "Negar",
          "Olfati"
        ],
        [
          "Sabato Marco",
          "Siniscalchi"
        ],
        [
          "Giampiero",
          "Salvi"
        ],
        [
          "Torbj\u00f8rn",
          "Svendsen"
        ]
      ],
      "title": "Transfer Learning of Articulatory Information Through Phone Information",
      "original": "1139",
      "page_count": 5,
      "order": 590,
      "p1": "2877",
      "pn": "2881",
      "abstract": [
        "Articulatory information has been argued to be useful for several speech\ntasks. However, in most practical scenarios this information is not\nreadily available. We propose a novel transfer learning framework to\nobtain reliable articulatory information in such cases. We demonstrate\nits reliability both in terms of estimating parameters of speech production\nand its ability to enhance the accuracy of an end-to-end phone recognizer.\nArticulatory information is estimated from speaker independent phonemic\nfeatures, using a small speech corpus, with electro-magnetic articulography\n(EMA) measurements. Next, we employ a teacher-student model to learn\nestimation of articulatory features from acoustic features for the\ntargeted phone recognition task. Phone recognition experiments, demonstrate\nthat the proposed transfer learning approach outperforms the baseline\ntransfer learning system acquired directly from an acoustic-to-articulatory\n(AAI) model. The articulatory features estimated by the proposed method,\nin conjunction with acoustic features, improved the phone error rate\n(PER) by 6.7% and 6% on the TIMIT core test and development sets, respectively,\ncompared to standalone static acoustic features. Interestingly, this\nimprovement is slightly higher than what is obtained by static+dynamic\nacoustic features, but with a significantly less. Adding articulatory\nfeatures on top of static+dynamic acoustic features yields a small\nbut positive PER improvement.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1139",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "shahrebabaki20b_interspeech": {
      "authors": [
        [
          "Abdolreza Sabzi",
          "Shahrebabaki"
        ],
        [
          "Sabato Marco",
          "Siniscalchi"
        ],
        [
          "Giampiero",
          "Salvi"
        ],
        [
          "Torbj\u00f8rn",
          "Svendsen"
        ]
      ],
      "title": "Sequence-to-Sequence Articulatory Inversion Through Time Convolution of Sub-Band Frequency Signals",
      "original": "1140",
      "page_count": 5,
      "order": 591,
      "p1": "2882",
      "pn": "2886",
      "abstract": [
        "We propose a new acoustic-to-articulatory inversion (AAI) sequence-to-sequence\nneural architecture, where spectral sub-bands are independently processed\nin time by 1-dimensional (1-D) convolutional filters of different sizes.\nThe learned features maps are then combined and processed by a recurrent\nblock with bi-directional long short-term memory (BLSTM) gates for\npreserving the smoothly varying nature of the articulatory trajectories.\nOur experimental evidence shows that, on a speaker dependent AAI task,\nin spite of the reduced number of parameters, our model demonstrates\nbetter root mean squared error (RMSE) and Pearson&#8217;s correlation\ncoefficient (PCC) than a both a BLSTM model and an FC-BLSTM model where\nthe first stages are fully connected layers. In particular, the average\nRMSE goes from 1.401 when feeding the filterbank features directly\ninto the BLSTM, to 1.328 with the FC-BLSTM model, and to 1.216 with\nthe proposed method. Similarly, the average PCC increases from 0.859\nto 0.877, and 0.895, respectively. On a speaker independent AAI task,\nwe show that our convolutional features outperform the original filterbank\nfeatures, and can be combined with phonetic features bringing independent\ninformation to the solution of the problem. To the best of the authors&#8217;\nknowledge, we report the best results on the given task and data.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1140",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "gatto20_interspeech": {
      "authors": [
        [
          "Bernardo B.",
          "Gatto"
        ],
        [
          "Eulanda M. dos",
          "Santos"
        ],
        [
          "Juan G.",
          "Colonna"
        ],
        [
          "Naoya",
          "Sogi"
        ],
        [
          "Lincon S.",
          "Souza"
        ],
        [
          "Kazuhiro",
          "Fukui"
        ]
      ],
      "title": "Discriminative Singular Spectrum Analysis for Bioacoustic Classification",
      "original": "2134",
      "page_count": 5,
      "order": 592,
      "p1": "2887",
      "pn": "2891",
      "abstract": [
        "Classifying bioacoustic signals is a fundamental task for ecological\nmonitoring. However, this task includes several challenges, such as\nnonuniform signal length, environmental noise, and scarce training\ndata. To tackle these challenges, we present a discriminative mechanism\nto classify bioacoustic signals, which does not require a large amount\nof training data and handles nonuniform signal length. The proposed\nmethod relies on transforming the input signals into subspaces generated\nby the singular spectrum analysis (SSA). Then, the difference between\nthe subspaces is used as a discriminative space, providing discriminative\nfeatures. This formulation allows a segmentation-free approach to represent\nand classify bioacoustic signals, as well as a highly compact descriptor\ninherited from the SSA. We validate the proposed method using challenging\ndatasets containing a variety of bioacoustic signals.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2134",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "mannem20b_interspeech": {
      "authors": [
        [
          "Renuka",
          "Mannem"
        ],
        [
          "Hima Jyothi",
          "R."
        ],
        [
          "Aravind",
          "Illa"
        ],
        [
          "Prasanta Kumar",
          "Ghosh"
        ]
      ],
      "title": "Speech Rate Task-Specific Representation Learning from Acoustic-Articulatory Data",
      "original": "2259",
      "page_count": 5,
      "order": 593,
      "p1": "2892",
      "pn": "2896",
      "abstract": [
        "In this work, speech rate is estimated using the task-specific representations\nwhich are learned from the acoustic-articulatory data, in contrast\nto generic representations which may not be optimal for the speech\nrate estimation. 1-D convolutional filters are used to learn speech\nrate specific acoustic representations from the raw speech. A convolutional\ndense neural network (CDNN) is used to estimate the speech rate from\nthe learned representations. In practice, articulatory data is not\ndirectly available; thus, we use Acoustic-to-Articulatory Inversion\n(AAI) to derive the articulatory representations from acoustics. However,\nthese pseudo-articulatory representations are also generic and not\noptimized for any task. To learn the speech-rate specific pseudo-articulatory\nrepresentations, we propose a joint training of BLSTM-based AAI and\nCDNN using a weighted loss function that considers the losses corresponding\nto speech rate estimation and articulatory prediction. The proposed\nmodel yields an improvement in speech rate estimation by &#126;18.5%\nin terms of pearson correlation coefficient (CC) compared to the baseline\nCDNN model with generic articulatory representations as inputs. To\nutilize complementary information from articulatory features, we further\nperform experiments by concatenating task-specific acoustic and pseudo-articulatory\nrepresentations, which yield an improvement in CC by &#126;2.5% compared\nto the baseline CDNN model.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2259",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "hernandez20_interspeech": {
      "authors": [
        [
          "Abner",
          "Hernandez"
        ],
        [
          "Eun Jung",
          "Yeo"
        ],
        [
          "Sunhee",
          "Kim"
        ],
        [
          "Minhwa",
          "Chung"
        ]
      ],
      "title": "Dysarthria Detection and Severity Assessment Using Rhythm-Based Metrics",
      "original": "2354",
      "page_count": 5,
      "order": 594,
      "p1": "2897",
      "pn": "2901",
      "abstract": [
        "Dysarthria refers to a range of speech disorders mainly affecting articulation.\nHowever, impairments are also seen in suprasegmental elements of speech\nsuch as prosody. In this study, we examine the effect of using rhythm\nmetrics on detecting dysarthria, and for assessing severity level.\nPrevious studies investigating prosodic irregularities in dysarthria\ntend to focus on pitch or voice quality measurements. Rhythm is another\naspect of prosody which refers to the rhythmic division of speech units\ninto relatively equal time. Speakers with dysarthria tend to have irregular\nrhythmic patterns that could be useful for detecting dysarthria. We\ncompare the classification accuracy between solely using standard prosodic\nfeatures against using both standard prosodic features and rhythm-based\nfeatures, using random forest, support vector machine, and feed-forward\nneural network. Our best performing classifiers achieved a relative\npercentage increase of 7.5% and 15% in detection and severity assessment\nrespectively for the QoLT Korean dataset, while the TORGO English dataset\nhad an increase of 4.1% and 3.2%. Results indicate that including rhythmic\ninformation can increase accuracy performance regardless of the classifier.\nFurthermore, we show that rhythm metrics are useful in both Korean\nand English.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2354",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "ma20_interspeech": {
      "authors": [
        [
          "Yi",
          "Ma"
        ],
        [
          "Xinzi",
          "Xu"
        ],
        [
          "Yongfu",
          "Li"
        ]
      ],
      "title": "LungRN+NL: An Improved Adventitious Lung Sound Classification Using Non-Local Block ResNet Neural Network with Mixup Data Augmentation",
      "original": "2487",
      "page_count": 5,
      "order": 595,
      "p1": "2902",
      "pn": "2906",
      "abstract": [
        "Performing an automated adventitious lung sound detection is a challenging\ntask since the sound is susceptible to noises (heartbeat, motion artifacts,\nand audio sound) and there is subtle discrimination among different\ncategories. An adventitious lung sound classification model, LungRN+NL,\nis proposed in this work, which has demonstrated a drastic improvement\ncompared to our previous work and the state-of-the-art models. This\nnew model has incorporated the non-local block in the ResNet architecture.\nTo address the imbalance problem and to improve the robustness of the\nmodel, we have also incorporated the mixup method to augment the training\ndataset. Our model has been implemented and compared with the state-of-the-art\nworks using the official ICBHI 2017 challenge dataset and their evaluation\nmethod. As a result, LungRN+NL has achieved a performance score of\n52.26%, which is improved by 2.1&#8211;12.7% compared to the state-of-the-art\nmodels.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2487",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "singh20b_interspeech": {
      "authors": [
        [
          "Abhayjeet",
          "Singh"
        ],
        [
          "Aravind",
          "Illa"
        ],
        [
          "Prasanta Kumar",
          "Ghosh"
        ]
      ],
      "title": "Attention and Encoder-Decoder Based Models for Transforming Articulatory Movements at Different Speaking Rates",
      "original": "2708",
      "page_count": 5,
      "order": 596,
      "p1": "2907",
      "pn": "2911",
      "abstract": [
        "While speaking at different rates, articulators (like tongue, lips)\ntend to move differently and the enunciations are also of different\ndurations. In the past, affine transformation and DNN have been used\nto transform articulatory movements from neutral to fast(N2F) and neutral\nto slow(N2S) speaking rates [1]. In this work, we improve over the\nexisting transformation techniques by modeling rate specific durations\nand their transformation using AstNet, an encoder-decoder framework\nwith attention. In the current work, we propose an encoder-decoder\narchitecture using LSTMs which generates smoother predicted articulatory\ntrajectories. For modeling duration variations across speaking rates,\nwe deploy attention network, which eliminates the need to align trajectories\nin different rates using DTW. We perform a phoneme specific duration\nanalysis to examine how well duration is transformed using the proposed\nAstNet. As the range of articulatory motions is correlated with speaking\nrate, we also analyze amplitude of the transformed articulatory movements\nat different rates compared to their original counterparts, to examine\nhow well the proposed AstNet predicts the extent of articulatory movements\nin N2F and N2S.We observe that AstNet could model both duration and\nextent of articulatory movements better than the existing transformation\ntechniques resulting in more accurate transformed articulatory trajectories.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2708",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "yang20e_interspeech": {
      "authors": [
        [
          "Zijiang",
          "Yang"
        ],
        [
          "Shuo",
          "Liu"
        ],
        [
          "Meishu",
          "Song"
        ],
        [
          "Emilia",
          "Parada-Cabaleiro"
        ],
        [
          "Bj\u00f6rn W.",
          "Schuller"
        ]
      ],
      "title": "Adventitious Respiratory Classification Using Attentive Residual Neural Networks",
      "original": "2790",
      "page_count": 5,
      "order": 597,
      "p1": "2912",
      "pn": "2916",
      "abstract": [
        "Every year, respiratory diseases affect millions of people worldwide,\nbecoming one of the main causes of death in nowadays society. Currently,\nthe COVID-19 &#8212; known as a novel respiratory illness &#8212; has\ntriggered a global health crisis, which has been identified as the\ngreatest challenge of our time since the Second World War. COVID-19\nand many other respiratory diseases present often common symptoms,\nwhich impairs their early diagnosis; thus, restricting their prevention\nand treatment. In this regard, in order to encourage a faster and more\naccurate detection of these kinds of diseases, the automatic identification\nof respiratory illness through the application of machine learning\nmethods is a very promising area of research aimed to support clinicians.\nWith this in mind, we apply attention-based Convolutional Neural Networks\nfor the recognition of adventitious respiratory cycles on the International\nConference on Biomedical Health Informatics 2017 challenge database.\nExperimental results indicate that the architecture of residual networks\nwith attention mechanism achieves a significant improvement w. r. t.\nthe baseline models.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2790",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "lenain20_interspeech": {
      "authors": [
        [
          "Raphael",
          "Lenain"
        ],
        [
          "Jack",
          "Weston"
        ],
        [
          "Abhishek",
          "Shivkumar"
        ],
        [
          "Emil",
          "Fristed"
        ]
      ],
      "title": "Surfboard: Audio Feature Extraction for Modern Machine Learning",
      "original": "2879",
      "page_count": 5,
      "order": 598,
      "p1": "2917",
      "pn": "2921",
      "abstract": [
        "We introduce Surfboard, an open-source Python library for extracting\naudio features with application to the medical domain. Surfboard is\nwritten with the aim of addressing pain points of existing libraries\nand facilitating joint use with modern machine learning frameworks.\nThe package can be accessed both programmatically in Python and via\nits command line interface, allowing it to be easily integrated within\nmachine learning workflows. It builds on state-of-the-art audio analysis\npackages and offers multiprocessing support for processing large workloads.\nWe review similar frameworks and describe Surfboard&#8217;s architecture,\nincluding the clinical motivation for its features. Using the mPower\ndataset, we illustrate Surfboard&#8217;s application to a Parkinson&#8217;s\ndisease classification task, highlighting common pitfalls in existing\nresearch. The source code is opened up to the research community to\nfacilitate future audio research in the clinical domain.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2879",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "naini20_interspeech": {
      "authors": [
        [
          "Abinay Reddy",
          "Naini"
        ],
        [
          "Malla",
          "Satyapriya"
        ],
        [
          "Prasanta Kumar",
          "Ghosh"
        ]
      ],
      "title": "Whisper Activity Detection Using CNN-LSTM Based Attention Pooling Network Trained for a Speaker Identification Task",
      "original": "3217",
      "page_count": 5,
      "order": 599,
      "p1": "2922",
      "pn": "2926",
      "abstract": [
        "In this work, we proposed a method to detect the whispered speech region\nin a noisy audio file called whisper activity detection (WAD). Due\nto the lack of pitch and noisy nature of whispered speech, it makes\nWAD a way more challenging task than standard voice activity detection\n(VAD). In this work, we proposed a Long-short term memory (LSTM) based\nwhisper activity detection algorithm. However, this LSTM network is\ntrained by keeping it as an attention pooling layer to a Convolutional\nneural network (CNN), which is trained for a speaker identification\ntask. WAD experiments with 186 speakers, with eight noise types in\nseven different signal-to-noise ratio (SNR) conditions, show that the\nproposed method performs better than the best baseline scheme in most\nof the conditions. Particularly in the case of unknown noises and environmental\nconditions, the proposed WAD performs significantly better than the\nbest baseline scheme. Another key advantage of the proposed WAD method\nis that it requires only a small part of the training data with annotation\nto fine-tune the post-processing parameters, unlike the existing baseline\nschemes requiring full training data annotated with the whispered speech\nregions.\n"
      ],
      "doi": "10.21437/Interspeech.2020-3217",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "zhao20e_interspeech": {
      "authors": [
        [
          "Shengkui",
          "Zhao"
        ],
        [
          "Trung Hieu",
          "Nguyen"
        ],
        [
          "Hao",
          "Wang"
        ],
        [
          "Bin",
          "Ma"
        ]
      ],
      "title": "Towards Natural Bilingual and Code-Switched Speech Synthesis Based on Mix of Monolingual Recordings and Cross-Lingual Voice Conversion",
      "original": "1163",
      "page_count": 5,
      "order": 600,
      "p1": "2927",
      "pn": "2931",
      "abstract": [
        "Recent state-of-the-art neural text-to-speech (TTS) synthesis models\nhave dramatically improved intelligibility and naturalness of generated\nspeech from text. However, building a good bilingual or code-switched\nTTS for a particular voice is still a challenge. The main reason is\nthat it is not easy to obtain a bilingual corpus from a speaker who\nachieves native-level fluency in both languages. In this paper, we\nexplore the use of Mandarin speech recordings from a Mandarin speaker,\nand English speech recordings from another English speaker to build\nhigh-quality bilingual and code-switched TTS for both speakers. A Tacotron2-based\ncross-lingual voice conversion system is employed to generate the Mandarin\nspeaker&#8217;s English speech and the English speaker&#8217;s Mandarin\nspeech, which show good naturalness and speaker similarity. The obtained\nbilingual data are then augmented with code-switched utterances synthesized\nusing a Transformer model. With these data, three neural TTS models\n&#8212; Tacotron2, Transformer and FastSpeech are applied for building\nbilingual and code-switched TTS. Subjective evaluation results show\nthat all the three systems can produce (near-)native-level speech in\nboth languages for each of the speaker.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1163",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "liu20m_interspeech": {
      "authors": [
        [
          "Zhaoyu",
          "Liu"
        ],
        [
          "Brian",
          "Mak"
        ]
      ],
      "title": "Multi-Lingual Multi-Speaker Text-to-Speech Synthesis for Voice Cloning with Online Speaker Enrollment",
      "original": "1464",
      "page_count": 5,
      "order": 601,
      "p1": "2932",
      "pn": "2936",
      "abstract": [
        "Recent studies in multi-lingual and multi-speaker text-to-speech synthesis\nproposed approaches that use proprietary corpora of performing artists\nand require fine-tuning to enroll new voices. To reduce these costs,\nwe investigate a novel approach for generating high-quality speeches\nin multiple languages of speakers enrolled in their native language.\nIn our proposed system, we introduce tone/stress embeddings which extend\nthe language embedding to represent tone and stress information. By\nmanipulating the tone/stress embedding input, our system can synthesize\nspeeches in native accent or foreign accent. To support online enrollment\nof new speakers, we condition the Tacotron-based synthesizer on speaker\nembeddings derived from a pre-trained x-vector speaker encoder by transfer\nlearning. We introduce a shared phoneme set to encourage more phoneme\nsharing compared with the IPA. Our MOS results demonstrate that the\nnative speech in all languages is highly intelligible and natural.\nWe also find L2-norm normalization and ZCA-whitening on x-vectors are\nhelpful to improve the system stability and audio quality. We also\nfind that the WaveNet performance is seemingly language-independent:\nthe WaveNet model trained with any of the three supported languages\nin our system can be used to generate speeches in the other two languages\nvery well.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1464",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "fu20b_interspeech": {
      "authors": [
        [
          "Ruibo",
          "Fu"
        ],
        [
          "Jianhua",
          "Tao"
        ],
        [
          "Zhengqi",
          "Wen"
        ],
        [
          "Jiangyan",
          "Yi"
        ],
        [
          "Chunyu",
          "Qiang"
        ],
        [
          "Tao",
          "Wang"
        ]
      ],
      "title": "Dynamic Soft Windowing and Language Dependent Style Token for Code-Switching End-to-End Speech Synthesis",
      "original": "1754",
      "page_count": 5,
      "order": 602,
      "p1": "2937",
      "pn": "2941",
      "abstract": [
        "Most of current end-to-end speech synthesis assumes the input text\nis in a single language situation. However, code-switching in speech\noccurs frequently in routine life, in which speakers switch between\nlanguages in the same utterance. And building a large mixed-language\nspeech database is difficult and uneconomical. In this paper, both\nwindowing technique and style token modeling are designed for the code-switching\nend-to-end speech synthesis. To improve the consistency of speaking\nstyle in bilingual situation, compared with the conventional windowing\ntechniques that used fixed constraints, the dynamic attention reweighting\nsoft windowing mechanism is proposed to ensure the smooth transition\nof code-switching. To compensate the shortage of mixed-language training\ndata, the language dependent style token is designed for the cross-language\nmulti-speaker acoustic modeling, where both the Mandarin and English\nmonolingual data are the extended training data set. The attention\ngating is proposed to adjust style token dynamically based on the language\nand the attended context information. Experimental results show that\nproposed methods lead to an improvement on intelligibility, naturalness\nand similarity.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1754",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "staib20_interspeech": {
      "authors": [
        [
          "Marlene",
          "Staib"
        ],
        [
          "Tian Huey",
          "Teh"
        ],
        [
          "Alexandra",
          "Torresquintero"
        ],
        [
          "Devang S. Ram",
          "Mohan"
        ],
        [
          "Lorenzo",
          "Foglianti"
        ],
        [
          "Raphael",
          "Lenain"
        ],
        [
          "Jiameng",
          "Gao"
        ]
      ],
      "title": "Phonological Features for 0-Shot Multilingual Speech Synthesis",
      "original": "1821",
      "page_count": 5,
      "order": 603,
      "p1": "2942",
      "pn": "2946",
      "abstract": [
        "Code-switching &#8212; the intra-utterance use of multiple languages\n&#8212; is prevalent across the world. Within text-to-speech (TTS),\nmultilingual models have been found to enable code-switching [1&#8211;3].\nBy modifying the linguistic input to sequence-to-sequence TTS, we show\nthat code-switching is possible for languages unseen during training,\neven within monolingual models. We use a small set of phonological\nfeatures derived from the International Phonetic Alphabet (IPA), such\nas vowel height and frontness, consonant place and manner. This allows\nthe model topology to stay unchanged for different languages, and enables\nnew, previously unseen feature combinations to be interpreted by the\nmodel. We show that this allows us to generate intelligible, code-switched\nspeech in a new language at test time, including the approximation\nof sounds never seen in training.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1821",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "xin20_interspeech": {
      "authors": [
        [
          "Detai",
          "Xin"
        ],
        [
          "Yuki",
          "Saito"
        ],
        [
          "Shinnosuke",
          "Takamichi"
        ],
        [
          "Tomoki",
          "Koriyama"
        ],
        [
          "Hiroshi",
          "Saruwatari"
        ]
      ],
      "title": "Cross-Lingual Text-To-Speech Synthesis via Domain Adaptation and Perceptual Similarity Regression in Speaker Space",
      "original": "2070",
      "page_count": 5,
      "order": 604,
      "p1": "2947",
      "pn": "2951",
      "abstract": [
        "We present a method for improving the performance of cross-lingual\ntext-to-speech synthesis. Previous works are able to model speaker\nindividuality in speaker space via speaker encoder but suffer from\nperformance decreasing when synthesizing cross-lingual speech. This\nis because the speaker space formed by all speaker embeddings is completely\nlanguage-dependent. In order to construct a language-independent speaker\nspace, we regard cross-lingual speech synthesis as a domain adaptation\nproblem and propose a training method to let the speaker encoder adapt\nspeaker embedding of different languages into the same space. Furthermore,\nto improve speaker individuality and construct a human-interpretable\nspeaker space, we propose a regression method to construct perceptually\ncorrelated speaker space. Experimental result demonstrates that our\nmethod could not only improve the performance of both cross-lingual\nand intra-lingual speech but also find perceptually similar speakers\nbeyond languages.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2070",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "liu20n_interspeech": {
      "authors": [
        [
          "Ruolan",
          "Liu"
        ],
        [
          "Xue",
          "Wen"
        ],
        [
          "Chunhui",
          "Lu"
        ],
        [
          "Xiao",
          "Chen"
        ]
      ],
      "title": "Tone Learning in Low-Resource Bilingual TTS",
      "original": "2180",
      "page_count": 5,
      "order": 605,
      "p1": "2952",
      "pn": "2956",
      "abstract": [
        "We present a system for low-resource multi-speaker cross-lingual text-to-speech\nsynthesis. In particular, we train with monolingual English and Mandarin\nspeakers and synthesize every speaker in both languages. The Mandarin\ntraining data is limited to 15 minutes of speech by a female Mandarin\nspeaker. We identify accent carry-over and mispronunciation in low-resource\nlanguage as two major challenges in this scenario, and address these\nissues by tone preservation mechanisms and data augmentation, respectively.\nWe apply these techniques to a recent strong multi-lingual baseline\nand achieve higher ratings in intelligibility and target accent, but\nslightly lower ratings in cross-lingual speaker similarity.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2180",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "bansal20_interspeech": {
      "authors": [
        [
          "Shubham",
          "Bansal"
        ],
        [
          "Arijit",
          "Mukherjee"
        ],
        [
          "Sandeepkumar",
          "Satpal"
        ],
        [
          "Rupeshkumar",
          "Mehta"
        ]
      ],
      "title": "On Improving Code Mixed Speech Synthesis with Mixlingual Grapheme-to-Phoneme Model",
      "original": "2654",
      "page_count": 5,
      "order": 606,
      "p1": "2957",
      "pn": "2961",
      "abstract": [
        "Regional entities often occur in a code-mixed text in the non-native\nroman script and synthesizing them with the correct pronunciation and\naccent is a challenging problem. English grapheme-to-phoneme (G2P)\nrules fail for such entities because of the orthographical mistakes\nand phonological differences between the English and regional languages.\nThe traditional approach for this problem involves language identification,\nfollowed by the transliteration of the regional entities to their native\nlanguage and then passing them through a native G2P. In this work,\nwe simplify this module based architecture by learning an end-to-end\nmixlingual G2P in a multi-task type setting. Also, rather than mapping\nthe output phone sequences from our mixlingual G2P to the English phoneset\nor using the &#8220;shared&#8221; phoneset, we use the polyglot data\nand &#8220;separated&#8221; phoneset to train a mixlingual synthesizer\nto improvise the synthesized voice accent for regional entities. We\nhave used Hindi-English as the code-mix scenario and we show absolute\nincremental gains of up to 28% in pronunciation accuracy and a 0.9\ngain in &#8220;overall impression&#8221; mean-opinion-score (MOS) over\nusing a standard English monolingual text-to-speech (TTS).\n"
      ],
      "doi": "10.21437/Interspeech.2020-2654"
    },
    "prakash20_interspeech": {
      "authors": [
        [
          "Anusha",
          "Prakash"
        ],
        [
          "Hema A.",
          "Murthy"
        ]
      ],
      "title": "Generic Indic Text-to-Speech Synthesisers with Rapid Adaptation in an End-to-End Framework",
      "original": "2663",
      "page_count": 5,
      "order": 607,
      "p1": "2962",
      "pn": "2966",
      "abstract": [
        "Building text-to-speech (TTS) synthesisers for Indian languages is\na difficult task owing to a large number of active languages. Indian\nlanguages can be classified into a finite set of families, prominent\namong them, Indo-Aryan and Dravidian. The proposed work exploits this\nproperty to build a generic TTS system using multiple languages from\nthe same family in an end-to-end framework. Generic systems are quite\nrobust as they are capable of capturing a variety of phonotactics across\nlanguages. These systems are then adapted to a new language in the\nsame family using small amounts of adaptation data. Experiments indicate\nthat good quality TTS systems can be built using only 7 minutes of\nadaptation data. An average degradation mean opinion score of 3.98\nis obtained for the adapted TTSes.<br style=\"mso-data-placement:same-cell;\"\n/><br style=\"mso-data-placement:same-cell;\" />\n  Extensive analysis\nof systematic interactions between languages in the generic TTSes is\ncarried out. x-vectors are included as speaker embedding to synthesise\ntext in a particular speaker&#8217;s voice. An interesting observation\nis that the prosody of the target speaker&#8217;s voice is preserved.\nThese results are quite promising as they indicate the capability of\ngeneric TTSes to handle speaker and language switching seamlessly,\nalong with the ease of adaptation to a new language.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2663",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "korte20_interspeech": {
      "authors": [
        [
          "Marcel de",
          "Korte"
        ],
        [
          "Jaebok",
          "Kim"
        ],
        [
          "Esther",
          "Klabbers"
        ]
      ],
      "title": "Efficient Neural Speech Synthesis for Low-Resource Languages Through Multilingual Modeling",
      "original": "2664",
      "page_count": 5,
      "order": 608,
      "p1": "2967",
      "pn": "2971",
      "abstract": [
        "Recent advances in neural TTS have led to models that can produce high-quality\nsynthetic speech. However, these models typically require large amounts\nof training data, which can make it costly to produce a new voice with\nthe desired quality. Although multi-speaker modeling can reduce the\ndata requirements necessary for a new voice, this approach is usually\nnot viable for many low-resource languages for which abundant multi-speaker\ndata is not available. In this paper, we therefore investigated to\nwhat extent multilingual multi-speaker modeling can be an alternative\nto monolingual multi-speaker modeling, and explored how data from foreign\nlanguages may best be combined with low-resource language data. We\nfound that multilingual modeling can increase the naturalness of low-resource\nlanguage speech, showed that multilingual models can produce speech\nwith a naturalness comparable to monolingual multi-speaker models,\nand saw that the target language naturalness was affected by the strategy\nused to add foreign language data.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2664",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "nekvinda20_interspeech": {
      "authors": [
        [
          "Tom\u00e1\u0161",
          "Nekvinda"
        ],
        [
          "Ond\u0159ej",
          "Du\u0161ek"
        ]
      ],
      "title": "One Model, Many Languages: Meta-Learning for Multilingual Text-to-Speech",
      "original": "2679",
      "page_count": 5,
      "order": 609,
      "p1": "2972",
      "pn": "2976",
      "abstract": [
        "We introduce an approach to multilingual speech synthesis which uses\nthe meta-learning concept of contextual parameter generation and produces\nnatural-sounding multilingual speech using more languages and less\ntraining data than previous approaches. Our model is based on Tacotron\n2 with a fully convolutional input text encoder whose weights are predicted\nby a separate parameter generator network. To boost voice cloning,\nthe model uses an adversarial speaker classifier with a gradient reversal\nlayer that removes speaker-specific information from the encoder.<br\nstyle=\"mso-data-placement:same-cell;\" /><br style=\"mso-data-placement:same-cell;\"\n/>\n  We arranged two experiments to compare our model with baselines\nusing various levels of cross-lingual parameter sharing, in order to\nevaluate: (1) stability and performance when training on low amounts\nof data, (2) pronunciation accuracy and voice quality of code-switching\nsynthesis. For training, we used the CSS10 dataset and our new small\ndataset based on Common Voice recordings in five languages. Our model\nis shown to effectively share information across languages and according\nto a subjective evaluation test, it produces more natural and accurate\ncode-switching speech than the baselines.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2679",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "chung20b_interspeech": {
      "authors": [
        [
          "Joon Son",
          "Chung"
        ],
        [
          "Jaesung",
          "Huh"
        ],
        [
          "Seongkyu",
          "Mun"
        ],
        [
          "Minjae",
          "Lee"
        ],
        [
          "Hee-Soo",
          "Heo"
        ],
        [
          "Soyeon",
          "Choe"
        ],
        [
          "Chiheon",
          "Ham"
        ],
        [
          "Sunghwan",
          "Jung"
        ],
        [
          "Bong-Jin",
          "Lee"
        ],
        [
          "Icksang",
          "Han"
        ]
      ],
      "title": "In Defence of Metric Learning for Speaker Recognition",
      "original": "1064",
      "page_count": 5,
      "order": 610,
      "p1": "2977",
      "pn": "2981",
      "abstract": [
        "The objective of this paper is &#8216;open-set&#8217; speaker recognition\nof unseen speakers, where ideal embeddings should be able to condense\ninformation into a compact utterance-level representation that has\nsmall intra-speaker and large inter-speaker distance.<br style=\"mso-data-placement:same-cell;\"\n/><br style=\"mso-data-placement:same-cell;\" />\n  A popular belief in\nspeaker recognition is that networks trained with classification objectives\noutperform metric learning methods. In this paper, we present an extensive\nevaluation of most popular loss functions for speaker recognition on\nthe VoxCeleb dataset. We demonstrate that the vanilla triplet loss\nshows competitive performance compared to classification-based losses,\nand those trained with our proposed metric learning objective outperform\nstate-of-the-art methods.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1064",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "kye20_interspeech": {
      "authors": [
        [
          "Seong Min",
          "Kye"
        ],
        [
          "Youngmoon",
          "Jung"
        ],
        [
          "Hae Beom",
          "Lee"
        ],
        [
          "Sung Ju",
          "Hwang"
        ],
        [
          "Hoirin",
          "Kim"
        ]
      ],
      "title": "Meta-Learning for Short Utterance Speaker Recognition with Imbalance Length Pairs",
      "original": "1283",
      "page_count": 5,
      "order": 611,
      "p1": "2982",
      "pn": "2986",
      "abstract": [
        "In practical settings, a speaker recognition system needs to identify\na speaker given a short utterance, while the enrollment utterance may\nbe relatively long. However, existing speaker recognition models perform\npoorly with such short utterances. To solve this problem, we introduce\na meta-learning framework for imbalance length pairs. Specifically,\nwe use a Prototypical Networks and train it with a support set of long\nutterances and a query set of short utterances of varying lengths.\nFurther, since optimizing only for the classes in the given episode\nmay be insufficient for learning discriminative embeddings for unseen\nclasses, we additionally enforce the model to classify both the support\nand the query set against the entire set of classes in the training\nset. By combining these two learning schemes, our model outperforms\nexisting state-of-the-art speaker verification models learned with\na standard supervised learning framework on short utterance (1-2 seconds)\non the VoxCeleb datasets. We also validate our proposed model for unseen\nspeaker identification, on which it also achieves significant performance\ngains over the existing approaches. The codes are available at <KBD>https://github.com/seongmin-kye/meta-SR</KBD>\n"
      ],
      "doi": "10.21437/Interspeech.2020-1283",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "li20ba_interspeech": {
      "authors": [
        [
          "Kai",
          "Li"
        ],
        [
          "Masato",
          "Akagi"
        ],
        [
          "Yibo",
          "Wu"
        ],
        [
          "Jianwu",
          "Dang"
        ]
      ],
      "title": "Segment-Level Effects of Gender, Nationality and Emotion Information on Text-Independent Speaker Verification",
      "original": "1700",
      "page_count": 5,
      "order": 612,
      "p1": "2987",
      "pn": "2991",
      "abstract": [
        "Speaker embeddings extracted from neural network (NN) achieve excellent\nperformance on general speaker verification (SV) missions. Most current\nSV systems use only speaker labels. Therefore, the interaction between\ndifferent types of domain information decrease the prediction accuracy\nof SV. To overcome this weakness and improve SV performance, four effective\nSV systems were proposed by using gender, nationality, and emotion\ninformation to add more constraints in the NN training stage. More\nspecifically, multitask learning-based systems which including multitask\ngender (MTG), multitask nationality (MTN) and multitask gender and\nnationality (MTGN) were used to enhance gender and nationality information\nlearning. Domain adversarial training-based system which including\nemotion domain adversarial training (EDAT) was used to suppress different\nemotions information learning. Experimental results indicate that encouraging\ngender and nationality information and suppressing emotion information\nlearning improve the performance of SV. In the end, our proposed systems\nachieved 16.4 and 22.9% relative improvements in the equal error rate\nfor MTL- and DAT-based systems, respectively.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1700",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "shi20f_interspeech": {
      "authors": [
        [
          "Yanpei",
          "Shi"
        ],
        [
          "Qiang",
          "Huang"
        ],
        [
          "Thomas",
          "Hain"
        ]
      ],
      "title": "Weakly Supervised Training of Hierarchical Attention Networks for Speaker Identification",
      "original": "1774",
      "page_count": 5,
      "order": 613,
      "p1": "2992",
      "pn": "2996",
      "abstract": [
        "Identifying multiple speakers without knowing where a speaker&#8217;s\nvoice is in a recording is a challenging task. In this paper, a hierarchical\nattention network is proposed to solve a weakly labelled speaker identification\nproblem. The use of a hierarchical structure, consisting of a frame-level\nencoder and a segment-level encoder, aims to learn speaker related\ninformation locally and globally. Speech streams are segmented into\nfragments. The frame-level encoder with attention learns features and\nhighlights the target related frames locally, and output a fragment\nbased embedding. The segment-level encoder works with a second attention\nlayer to emphasize the fragments probably related to target speakers.\nThe global information is finally collected from segment-level module\nto predict speakers via a classifier. To evaluate the effectiveness\nof the proposed approach, artificial datasets based on Switchboard\nCellular part1 (SWBC) and Voxceleb1 are constructed in two conditions,\nwhere speakers&#8217; voices are overlapped and not overlapped. Comparing\nto two baselines the obtained results show that the proposed approach\ncan achieve better performances. Moreover, further experiments are\nconducted to evaluate the impact of utterance segmentation. The results\nshow that a reasonable segmentation can slightly improve identification\nperformances.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1774",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "montalvo20_interspeech": {
      "authors": [
        [
          "Ana",
          "Montalvo"
        ],
        [
          "Jose R.",
          "Calvo"
        ],
        [
          "Jean-Fran\u00e7ois",
          "Bonastre"
        ]
      ],
      "title": "Multi-Task Learning for Voice Related Recognition Tasks",
      "original": "1857",
      "page_count": 5,
      "order": 614,
      "p1": "2997",
      "pn": "3001",
      "abstract": [
        "Speech is a complex signal conveying numerous information about the\nmessage but also various characteristics of the speaker: its sex, age,\naccent, language. Understanding the use of these features by machine\nlearning (ML) systems has two main advantages. First, it could help\nprevent bias and discrimination in ML speech applications. Second,\njoint modeling of this information using multitasking learning approaches\n(MTL) has great potential for improvement. We explore in this paper\nthe use of MTL in non-linguistic tasks. We compare single- and multi-task\nmodels applied to three tasks: (spanish) nativeness, speaker and sex.\nThe effect of training data set size in the performance of both single-\nand multi-task models is investigated as well as the specific contribution\nof nativeness and sex information to speaker recognition. Experimental\nresults show that multi-task (MTL) models outperform single task models.\nWe have also found that MTL is beneficial for small training data sets\nand for low-level acoustic features rather than for pretrained features\nsuch as bottleneck ones. Our results indicate also that more attention\nshould be addressed to the information used by ML approaches in order\nto prevent biases or discrimination.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1857",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "khan20_interspeech": {
      "authors": [
        [
          "Umair",
          "Khan"
        ],
        [
          "Javier",
          "Hernando"
        ]
      ],
      "title": "Unsupervised Training of Siamese Networks for Speaker Verification",
      "original": "1882",
      "page_count": 5,
      "order": 615,
      "p1": "3002",
      "pn": "3006",
      "abstract": [
        "Speaker labeled background data is an essential requirement for most\nstate-of-the-art approaches in speaker recognition, e.g., x-vectors\nand i-vector/PLDA. However, in reality it is difficult to access large\namount of labeled data. In this work, we propose siamese networks for\nspeaker verification without using speaker labels. We propose two different\nsiamese networks having two and three branches, respectively, where\neach branch is a CNN encoder. Since the goal is to avoid speaker labels,\nwe propose to generate the training pairs in an unsupervised manner.\nThe client samples are selected within one database according to highest\ncosine scores with the anchor in i-vector space. The impostor samples\nare selected in the same way but from another database. Our double-branch\nsiamese performs binary classification using cross entropy loss during\ntraining. In testing phase, we obtain speaker verification scores directly\nfrom its output layer. Whereas, our triple-branch siamese is trained\nto learn speaker embeddings using triplet loss. During testing, we\nextract speaker embeddings from its output layer, which are scored\nin the experiments using cosine scoring. The evaluation is performed\non VoxCeleb-1 database, which show that using the proposed unsupervised\nsystems, solely or in fusion, the results get closer to supervised\nbaseline.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1882",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "liu20o_interspeech": {
      "authors": [
        [
          "Ying",
          "Liu"
        ],
        [
          "Yan",
          "Song"
        ],
        [
          "Yiheng",
          "Jiang"
        ],
        [
          "Ian",
          "McLoughlin"
        ],
        [
          "Lin",
          "Liu"
        ],
        [
          "Li-Rong",
          "Dai"
        ]
      ],
      "title": "An Effective Speaker Recognition Method Based on Joint Identification and Verification Supervisions",
      "original": "1922",
      "page_count": 5,
      "order": 616,
      "p1": "3007",
      "pn": "3011",
      "abstract": [
        "Deep embedding learning based speaker verification methods have attracted\nsignificant recent research interest due to their superior performance.\nExisting methods mainly focus on designing frame-level feature extraction\nstructures, utterance-level aggregation methods and loss functions\nto learn discriminative speaker embeddings. The scores of verification\ntrials are then computed using cosine distance or Probabilistic Linear\nDiscriminative Analysis (PLDA) classifiers. This paper proposes an\neffective speaker recognition method which is based on joint identification\nand verification supervisions, inspired by multi-task learning frameworks.\nSpecifically, a deep architecture with convolutional feature extractor,\nattentive pooling and two classifier branches is presented. The first,\nan identification branch, is trained with additive margin softmax loss\n(AM-Softmax) to classify the speaker identities. The second, a verification\nbranch, trains a discriminator with binary cross entropy loss (BCE)\nto optimize a new triplet-based mutual information. To balance the\ntwo losses during different training stages, a ramp-up/ramp-down weighting\nscheme is employed. Furthermore, an attentive bilinear pooling method\nis proposed to improve the effectiveness of embeddings. Extensive experiments\nhave been conducted on VoxCeleb1 to evaluate the proposed method, demonstrating\nresults that relatively reduce the equal error rate (EER) by 22% compared\nto the baseline system using identification supervision only.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1922",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "zheng20c_interspeech": {
      "authors": [
        [
          "Naijun",
          "Zheng"
        ],
        [
          "Xixin",
          "Wu"
        ],
        [
          "Jinghua",
          "Zhong"
        ],
        [
          "Xunying",
          "Liu"
        ],
        [
          "Helen",
          "Meng"
        ]
      ],
      "title": "Speaker-Aware Linear Discriminant Analysis in Speaker Verification",
      "original": "2061",
      "page_count": 5,
      "order": 617,
      "p1": "3012",
      "pn": "3016",
      "abstract": [
        "Linear discriminant analysis (LDA) is an effective and widely used\ndiscriminative technique for speaker verification. However, it only\nutilizes the information on global structure to perform classification.\nSome variants of LDA, such as local pairwise LDA (LPLDA), are proposed\nto preserve more information on the local structure in the linear projection\nmatrix. However, considering that the local structure may vary a lot\nin different regions, summing up related components to construct a\nsingle projection matrix may not be sufficient. In this paper, we present\na speaker-aware strategy focusing on preserving distinct information\non local structure in a set of linear discriminant projection matrices,\nand allocating them to different local regions for dimension reduction\nand classification. Experiments on NIST SRE2010 and NIST SRE2016 show\nthat the speaker-aware strategy can boost the performance of both LDA\nand LPLDA backends in i-vector systems and x-vector systems.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2061",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "chen20n_interspeech": {
      "authors": [
        [
          "Zhengyang",
          "Chen"
        ],
        [
          "Shuai",
          "Wang"
        ],
        [
          "Yanmin",
          "Qian"
        ]
      ],
      "title": "Adversarial Domain Adaptation for Speaker Verification Using Partially Shared Network",
      "original": "2226",
      "page_count": 5,
      "order": 618,
      "p1": "3017",
      "pn": "3021",
      "abstract": [
        "Speaker verification systems usually suffer from large performance\ndegradation when applied to a new dataset from a different domain.\nIn this work, we will study the domain adaption strategy between datasets\nwith different languages using domain adversarial training. We introduce\na partially shared network based domain adversarial training architecture\nto learn an asymmetric mapping for source and target domain embedding\nextractor. This architecture can help the embedding extractor learn\ndomain invariant feature without sacrificing the ability on speaker\ndiscrimination. When doing the evaluation on cross-lingual domain adaption,\nthe source domain data is in English from NIST SRE04-10 and Switchboard,\nand the target domain data is in Cantonese and Tagalog from NIST SRE16.\nOur results show that the usual adversarial training mode will indeed\nharm the speaker discrimination when the source and target domain embedding\nextractors are fully shared, and in contrast the newly proposed architecture\nsolves this problem and achieves &#126;25.0% relative average Equal\nError Rate (EER) improvement on SRE16 Cantonese and Tagalog evaluation.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2226",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "lin20f_interspeech": {
      "authors": [
        [
          "Binghuai",
          "Lin"
        ],
        [
          "Liyuan",
          "Wang"
        ],
        [
          "Xiaoli",
          "Feng"
        ],
        [
          "Jinsong",
          "Zhang"
        ]
      ],
      "title": "Automatic Scoring at Multi-Granularity for L2 Pronunciation",
      "original": "1282",
      "page_count": 5,
      "order": 619,
      "p1": "3022",
      "pn": "3026",
      "abstract": [
        "Automatic pronunciation assessment and error detection play an important\npart of Computer-Assisted Pronunciation Training (CAPT). Traditional\napproaches normally focus on scoring of sentences, words or mispronunciation\ndetection of phonemes independently without considering the hierarchical\nand contextual relationships among them. In this paper, we develop\na hierarchical network which combines scoring at the granularity of\nphoneme, word and sentence jointly. Specifically, we achieve the phoneme\nscores by a semi-supervised phoneme mispronunciation detection method,\nthe words scores by an attention mechanism, and the sentence scores\nby a non-linear regression method. To further model the correlation\nbetween the sentence and phoneme, we optimize the network by a multitask\nlearning framework (MTL). The proposed framework relies on a few sentence-level\nlabeled data and a majority of unlabeled data. We evaluate the network\nperformance on a multi-granular dataset consisting of sentences, words\nand phonemes which was recorded by 1,000 Chinese speakers and labeled\nby three experts. Experimental results show that the proposed method\nis well correlated with human raters with a Pearson correlation coefficient\n(PCC) of 0.88 at sentence level and 0.77 at word level. Furthermore,\nthe semi-supervised phoneme mispronunciation detection achieves a comparable\nresult by F1-measure with our supervised baseline.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1282",
      "author_area_id": "10",
      "author_area_label": "Speech Recognition: Technologies and Systems for New Applications"
    },
    "lo20b_interspeech": {
      "authors": [
        [
          "Tien-Hong",
          "Lo"
        ],
        [
          "Shi-Yan",
          "Weng"
        ],
        [
          "Hsiu-Jui",
          "Chang"
        ],
        [
          "Berlin",
          "Chen"
        ]
      ],
      "title": "An Effective End-to-End Modeling Approach for Mispronunciation Detection",
      "original": "1605",
      "page_count": 5,
      "order": 620,
      "p1": "3027",
      "pn": "3031",
      "abstract": [
        "Recently, end-to-end (E2E) automatic speech recognition (ASR) systems\nhave garnered tremendous attention because of their great success and\nunified modeling paradigms in comparison to conventional hybrid DNN-HMM\nASR systems. Despite the widespread adoption of E2E modeling frameworks\non ASR, there still is a dearth of work on investigating the E2E frameworks\nfor use in computer-assisted pronunciation learning (CAPT), particularly\nfor mispronunciation detection (MD). In response, we first present\na novel use of hybrid CTC-Attention approach to the MD task, taking\nadvantage of the strengths of both CTC and the attention-based model\nmeanwhile getting around the need for phone-level forced-alignment.\nSecond, we perform input augmentation with text prompt information\nto make the resulting E2E model more tailored for the MD task. On the\nother hand, we adopt two MD decision methods so as to better cooperate\nwith the proposed framework: 1) decision-making based on a recognition\nconfidence measure or 2) simply based on speech recognition results.\nA series of Mandarin MD experiments demonstrate that our approach not\nonly simplifies the processing pipeline of existing hybrid DNN-HMM\nsystems but also brings about systematic and substantial performance\nimprovements. Furthermore, input augmentation with text prompts seems\nto hold excellent promise for the E2E-based MD approach.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1605",
      "author_area_id": "10",
      "author_area_label": "Speech Recognition: Technologies and Systems for New Applications"
    },
    "yan20_interspeech": {
      "authors": [
        [
          "Bi-Cheng",
          "Yan"
        ],
        [
          "Meng-Che",
          "Wu"
        ],
        [
          "Hsiao-Tsung",
          "Hung"
        ],
        [
          "Berlin",
          "Chen"
        ]
      ],
      "title": "An End-to-End Mispronunciation Detection System for L2 English Speech Leveraging Novel Anti-Phone Modeling",
      "original": "1616",
      "page_count": 5,
      "order": 621,
      "p1": "3032",
      "pn": "3036",
      "abstract": [
        "Mispronunciation detection and diagnosis (MDD) is a core component\nof computer-assisted pronunciation training (CAPT). Most of the existing\nMDD approaches focus on dealing with categorical errors (viz. one canonical\nphone is substituted by another one, aside from those mispronunciations\ncaused by deletions or insertions). However, accurate detection and\ndiagnosis of non-categorial or distortion errors (viz. approximating\nL2 phones with L1 (first-language) phones, or erroneous pronunciations\nin between) still seems out of reach. In view of this, we propose to\nconduct MDD with a novel end-to-end automatic speech recognition (E2E-based\nASR) approach. In particular, we expand the original L2 phone set with\ntheir corresponding anti-phone set, making the E2E-based MDD approach\nhave a better capability to take in both categorical and non-categorial\nmispronunciations, aiming to provide better mispronunciation detection\nand diagnosis feedback. Furthermore, a novel transfer-learning paradigm\nis devised to obtain the initial model estimate of the E2E-based MDD\nsystem without resource to any phonological rules. Extensive sets of\nexperimental results on the L2-ARCTIC dataset show that our best system\ncan outperform the existing E2E baseline system and pronunciation scoring\nbased method (GOP) in terms of the F1-score, by 11.05% and 27.71%,\nrespectively.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1616",
      "author_area_id": "10",
      "author_area_label": "Speech Recognition: Technologies and Systems for New Applications"
    },
    "duan20_interspeech": {
      "authors": [
        [
          "Richeng",
          "Duan"
        ],
        [
          "Nancy F.",
          "Chen"
        ]
      ],
      "title": "Unsupervised Feature Adaptation Using Adversarial Multi-Task Training for Automatic Evaluation of Children&#8217;s Speech",
      "original": "1657",
      "page_count": 5,
      "order": 622,
      "p1": "3037",
      "pn": "3041",
      "abstract": [
        "Processing children&#8217;s speech is challenging due to high speaker\nvariability arising from vocal tract size and scarce amounts of publicly\navailable linguistic resources. In this work, we tackle such challenges\nby proposing an unsupervised feature adaptation approach based on adversarial\nmulti-task training in a neural framework. A front-end feature transformation\nmodule is positioned prior to an acoustic model trained on adult speech\n(1) to leverage on the readily available linguistic resources on adult\nspeech or existing models, and (2) to reduce the acoustic mismatch\nbetween child and adult speech. Experimental results demonstrate that\nour proposed approach consistently outperforms established baselines\ntrained on adult speech across a variety of tasks ranging from speech\nrecognition to pronunciation assessment and fluency score prediction.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1657",
      "author_area_id": "10",
      "author_area_label": "Speech Recognition: Technologies and Systems for New Applications"
    },
    "yang20f_interspeech": {
      "authors": [
        [
          "Longfei",
          "Yang"
        ],
        [
          "Kaiqi",
          "Fu"
        ],
        [
          "Jinsong",
          "Zhang"
        ],
        [
          "Takahiro",
          "Shinozaki"
        ]
      ],
      "title": "Pronunciation Erroneous Tendency Detection with Language Adversarial Represent Learning",
      "original": "2033",
      "page_count": 5,
      "order": 623,
      "p1": "3042",
      "pn": "3046",
      "abstract": [
        "Pronunciation erroneous tendencies (PETs) are designed to provide instructive\nfeedback to guide non-native language learners to correct their pronunciation\nerrors in language learning thus PET detection plays an important role\nin computer-aided pronunciation training (CAPT) system. However, PET\ndetection suffers data sparsity problem because non-native data collection\nand annotation are time-consuming tasks. In this paper, we propose\nan unsupervised learning framework based on contrastive predictive\ncoding (CPC) to extract knowledge from a large scale of unlabeled speech\nfrom two native languages, and then transfer this knowledge to the\nPET detection task. In this framework, language adversarial training\nis incorporated to guide the model to align the feature distribution\nbetween two languages. In addition, sinc filter is introduced to extract\nformant-like feature that is considered relevant to some kinds of pronunciation\nerrors. Through the experiment on the Japanese part of BLCU inter-Chinese\nspeech corpus, results show that our proposed language adversarial\nrepresent learning is effective to improve the performance of pronunciation\nerroneous tendency detection for non-native language learners.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2033",
      "author_area_id": "10",
      "author_area_label": "Speech Recognition: Technologies and Systems for New Applications"
    },
    "cheng20_interspeech": {
      "authors": [
        [
          "Sitong",
          "Cheng"
        ],
        [
          "Zhixin",
          "Liu"
        ],
        [
          "Lantian",
          "Li"
        ],
        [
          "Zhiyuan",
          "Tang"
        ],
        [
          "Dong",
          "Wang"
        ],
        [
          "Thomas Fang",
          "Zheng"
        ]
      ],
      "title": "ASR-Free Pronunciation Assessment",
      "original": "2623",
      "page_count": 5,
      "order": 624,
      "p1": "3047",
      "pn": "3051",
      "abstract": [
        "Most of the pronunciation assessment methods are based on local features\nderived from automatic speech recognition (ASR), e.g., the Goodness\nof Pronunciation (GOP) score. In this paper, we investigate an ASR-free\nscoring approach that is derived from the marginal distribution of\nraw speech signals. The hypothesis is that even if we have no knowledge\nof the language (so cannot recognize the phones/words), we can still\ntell how good a pronunciation is, by comparatively listening to some\nspeech data from the target language. Our analysis shows that this\nnew scoring approach provides an interesting correction for the phone-competition\nproblem of GOP. Experimental results on the ERJ dataset demonstrated\nthat combining the ASR-free score and GOP can achieve better performance\nthan the GOP baseline.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2623",
      "author_area_id": "10",
      "author_area_label": "Speech Recognition: Technologies and Systems for New Applications"
    },
    "kyriakopoulos20_interspeech": {
      "authors": [
        [
          "Konstantinos",
          "Kyriakopoulos"
        ],
        [
          "Kate M.",
          "Knill"
        ],
        [
          "Mark J.F.",
          "Gales"
        ]
      ],
      "title": "Automatic Detection of Accent and Lexical Pronunciation Errors in Spontaneous Non-Native English Speech",
      "original": "2881",
      "page_count": 5,
      "order": 625,
      "p1": "3052",
      "pn": "3056",
      "abstract": [
        "Detecting individual pronunciation errors and diagnosing pronunciation\nerror tendencies in a language learner based on their speech are important\ncomponents of computer-aided language learning (CALL). The tasks of\nerror detection and error tendency diagnosis become particularly challenging\nwhen the speech in question is spontaneous and particularly given the\nchallenges posed by the inconsistency of human annotation of pronunciation\nerrors. This paper presents an approach to these tasks by distinguishing\nbetween lexical errors, wherein the speaker does not know how a particular\nword is pronounced, and accent errors, wherein the candidate&#8217;s\nspeech exhibits consistent patterns of phone substitution, deletion\nand insertion. Three annotated corpora of non-native English speech\nby speakers of multiple L1s are analysed, the consistency of human\nannotation investigated and a method presented for detecting individual\naccent and lexical errors and diagnosing accent error tendencies at\nthe speaker level.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2881",
      "author_area_id": "10",
      "author_area_label": "Speech Recognition: Technologies and Systems for New Applications"
    },
    "shi20g_interspeech": {
      "authors": [
        [
          "Jiatong",
          "Shi"
        ],
        [
          "Nan",
          "Huo"
        ],
        [
          "Qin",
          "Jin"
        ]
      ],
      "title": "Context-Aware Goodness of Pronunciation for Computer-Assisted Pronunciation Training",
      "original": "2953",
      "page_count": 5,
      "order": 626,
      "p1": "3057",
      "pn": "3061",
      "abstract": [
        "Mispronunciation detection is an essential component of the Computer-Assisted\nPronunciation Training (CAPT) systems. State-of-the-art mispronunciation\ndetection models use Deep Neural Networks (DNN) for acoustic modeling,\nand a Goodness of Pronunciation (GOP) based algorithm for pronunciation\nscoring. However, GOP based scoring models have two major limitations:\ni.e., (i) They depend on forced alignment which splits the speech into\nphonetic segments and independently use them for scoring, which neglects\nthe transitions between phonemes within the segment; (ii) They only\nfocus on phonetic segments, which fails to consider the context effects\nacross phonemes (such as liaison, omission, incomplete plosive sound,\netc.). In this work, we propose the Context-aware Goodness of Pronunciation\n(CaGOP) scoring model. Particularly, two factors namely the transition\nfactor and the duration factor are injected into CaGOP scoring. The\ntransition factor identifies the transitions between phonemes and applies\nthem to weight the frame-wise GOP. Moreover, a self-attention based\nphonetic duration modeling is proposed to introduce the duration factor\ninto the scoring model. The proposed scoring model significantly outperforms\nbaselines, achieving 20% and 12% relative improvement over the GOP\nmodel on the phoneme-level and sentence-level mispronunciation detection\nrespectively.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2953",
      "author_area_id": "10",
      "author_area_label": "Speech Recognition: Technologies and Systems for New Applications"
    },
    "chu20_interspeech": {
      "authors": [
        [
          "Wei",
          "Chu"
        ],
        [
          "Yang",
          "Liu"
        ],
        [
          "Jianwei",
          "Zhou"
        ]
      ],
      "title": "Recognize Mispronunciations to Improve Non-Native Acoustic Modeling Through a Phone Decoder Built from One Edit Distance Finite State Automaton",
      "original": "3109",
      "page_count": 5,
      "order": 627,
      "p1": "3062",
      "pn": "3066",
      "abstract": [
        "This paper proposed a procedure for detecting and recognizing mispronunciations\nin training data, and improved non-native acoustic modeling by training\nwith the corrected phone alignments. To start, an initial phone sequence\nfor an utterance is derived from its word-level transcription and a\ndictionary of canonical pronunciation. Following that, the region of\nmispronunciation is detected through examining phone-level goodness-of-pronunciation\n(GOP) scores. Then over the region, a constrained phone decoder is\nused to recognize the most likely pronounced phone sequence from all\nthe possible phone sequences with one phone edit distance from the\ninitial phone sequence. After updating the phone alignments and GOP\nscores, this detection and recognition procedure is repeated until\nno more mispronunciation is detected. Experiments on a 300-hour non-native\nspontaneous dataset showed that the acoustic model trained from the\nproposed procedure reduced WER by 6% compared to a well optimized context-dependent\nfactorized-TDNN HMM baseline system with the same neural network topology.\nThis work also offered a data-driven approach for generating a list\nof common mispronunciation patterns of non-native English learners\nthat may be useful for speech assessment purpose.\n"
      ],
      "doi": "10.21437/Interspeech.2020-3109",
      "author_area_id": "10",
      "author_area_label": "Speech Recognition: Technologies and Systems for New Applications"
    },
    "gimeno20_interspeech": {
      "authors": [
        [
          "Pablo",
          "Gimeno"
        ],
        [
          "Victoria",
          "Mingote"
        ],
        [
          "Alfonso",
          "Ortega"
        ],
        [
          "Antonio",
          "Miguel"
        ],
        [
          "Eduardo",
          "Lleida"
        ]
      ],
      "title": "Partial AUC Optimisation Using Recurrent Neural Networks for Music Detection with Limited Training Data",
      "original": "1108",
      "page_count": 5,
      "order": 628,
      "p1": "3067",
      "pn": "3071",
      "abstract": [
        "State-of-the-art music detection systems, whose aim is to distinguish\nwhether or not music is present in an audio signal, rely mainly on\ndeep learning approaches. However, these kind of solutions are strongly\ndependent on the amount of data they were trained on. In this paper,\nwe introduce the area under the ROC curve (AUC) and partial AUC (pAUC)\noptimisation techniques, recently developed for neural networks, into\nthe music detection task, seeking to overcome the issues derived from\ndata limitation. Using recurrent neural networks as the main element\nin the system and with a limited training set of around 20 hours of\naudio, we explore different approximations to threshold-independent\ntraining objectives. Furthermore, we propose a novel training objective\nbased on the decomposition of the area under the ROC curve as the sum\nof two partial areas under the ROC curve. Experimental results show\nthat partial AUC optimisation can improve the performance of music\ndetection systems significantly compared to traditional training criteria\nsuch as cross entropy.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1108",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "lavechin20_interspeech": {
      "authors": [
        [
          "Marvin",
          "Lavechin"
        ],
        [
          "Ruben",
          "Bousbib"
        ],
        [
          "Herv\u00e9",
          "Bredin"
        ],
        [
          "Emmanuel",
          "Dupoux"
        ],
        [
          "Alejandrina",
          "Cristia"
        ]
      ],
      "title": "An Open-Source Voice Type Classifier for Child-Centered Daylong Recordings",
      "original": "1690",
      "page_count": 5,
      "order": 629,
      "p1": "3072",
      "pn": "3076",
      "abstract": [
        "Spontaneous conversations in real-world settings such as those found\nin child-centered recordings have been shown to be amongst the most\nchallenging audio files to process. Nevertheless, building speech processing\nmodels handling such a wide variety of conditions would be particularly\nuseful for language acquisition studies in which researchers are interested\nin the quantity and quality of the speech that children hear and produce,\nas well as for early diagnosis and measuring effects of remediation.\nIn this paper, we present our approach to designing an open-source\nneural network to classify audio segments into vocalizations produced\nby the child wearing the recording device, vocalizations produced by\nother children, adult male speech, and adult female speech. To this\nend, we gathered diverse child-centered corpora which sums up to a\ntotal of 260 hours of recordings and covers 10 languages. Our model\ncan be used as input for downstream tasks such as estimating the number\nof words produced by adult speakers, or the number of linguistic units\nproduced by children. Our architecture combines SincNet filters with\na stack of recurrent layers and outperforms by a large margin the state-of-the-art\nsystem, the Language ENvironment Analysis (LENA) that has been used\nin numerous child language studies.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1690",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "peng20_interspeech": {
      "authors": [
        [
          "Chao",
          "Peng"
        ],
        [
          "Xihong",
          "Wu"
        ],
        [
          "Tianshu",
          "Qu"
        ]
      ],
      "title": "Competing Speaker Count Estimation on the Fusion of the Spectral and Spatial Embedding Space",
      "original": "1781",
      "page_count": 5,
      "order": 630,
      "p1": "3077",
      "pn": "3081",
      "abstract": [
        "This paper presents a method for estimating the competing speaker count\nwith deep spectral and spatial embedding fusion. The basic idea is\nthat mixed speech can be projected into an embedding space using neural\nnetworks where embedding vectors are orthogonal for different speakers\nwhile parallel for the same speaker. Therefore, speaker count estimation\ncan be performed by computing the rank of the mean covariance matrix\nof the embedding vectors. It is also a feature combination method in\nspeaker embedding space instead of simply combining features at the\ninput layer of neural networks. Experimental results show that embedding-based\nmethod is better than classification-based method where the network\ndirectly predicts the count of speakers and spatial features help to\nspeaker count estimation. In addition, the features combined in the\nembedding space can achieve more accurate speaker counting than features\ncombined at the input layer of neural networks when tested on anechoic\nand reverberant datasets.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1781",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "lin20g_interspeech": {
      "authors": [
        [
          "Shoufeng",
          "Lin"
        ],
        [
          "Xinyuan",
          "Qian"
        ]
      ],
      "title": "Audio-Visual Multi-Speaker Tracking Based on the GLMB Framework",
      "original": "1969",
      "page_count": 5,
      "order": 631,
      "p1": "3082",
      "pn": "3086",
      "abstract": [
        "Multi-speaker tracking using both audio and video modalities is a key\ntask in human-robot interaction and video conferencing. The complementary\nnature of audio and video signals improves the tracking robustness\nagainst noise and outliers compared to the uni-modal approaches. However,\nthe online tracking of multiple speakers via audio-video fusion, especially\nwithout the target number prior, is still an open challenge. In this\npaper, we propose a Generalized Labelled Multi-Bernoulli (GLMB)-based\nframework that jointly estimates the number of targets and their respective\nstates online. Experimental results using the AV16.3 dataset demonstrate\nthe effectiveness of the proposed method.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1969",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "liu20p_interspeech": {
      "authors": [
        [
          "Shuo",
          "Liu"
        ],
        [
          "Andreas",
          "Triantafyllopoulos"
        ],
        [
          "Zhao",
          "Ren"
        ],
        [
          "Bj\u00f6rn W.",
          "Schuller"
        ]
      ],
      "title": "Towards Speech Robustness for Acoustic Scene Classification",
      "original": "2365",
      "page_count": 5,
      "order": 632,
      "p1": "3087",
      "pn": "3091",
      "abstract": [
        "This work discusses the impact of human voice on acoustic scene classification\n(ASC) systems. Typically, such systems are trained and evaluated on\ndata sets lacking human speech. We show experimentally that the addition\nof speech can be detrimental to system performance. Furthermore, we\npropose two alternative solutions to mitigate that effect in the context\nof deep neural networks (DNNs). We first utilise data augmentation\nto make the algorithm robust against the presence of human speech in\nthe data. We also introduce a voice-suppression algorithm that removes\nhuman speech from audio recordings, and test the DNN classifier on\nthose denoised samples. Experimental results show that both approaches\nreduce the negative effects of human voice in ASC systems. Compared\nto using data augmentation, applying voice suppression achieved better\nclassification accuracy and managed to perform more stably for different\nspeech intensity.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2365",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "zhu20b_interspeech": {
      "authors": [
        [
          "Junzhe",
          "Zhu"
        ],
        [
          "Mark",
          "Hasegawa-Johnson"
        ],
        [
          "Leda",
          "Sar\u0131"
        ]
      ],
      "title": "Identify Speakers in Cocktail Parties with End-to-End Attention",
      "original": "2430",
      "page_count": 5,
      "order": 633,
      "p1": "3092",
      "pn": "3096",
      "abstract": [
        "In scenarios where multiple speakers talk at the same time, it is important\nto be able to identify the talkers accurately. This paper presents\nan end-to-end system that integrates speech source extraction and speaker\nidentification, and proposes a new way to jointly optimize these two\nparts by max-pooling the speaker predictions along the channel dimension.\nResidual attention permits us to learn spectrogram masks that are optimized\nfor the purpose of speaker identification, while residual forward connections\npermit dilated convolution with a sufficiently large context window\nto guarantee correct streaming across syllable boundaries. End-to-end\ntraining results in a system that recognizes one speaker in a two-speaker\nbroadcast speech mixture with 99.9% accuracy and both speakers with\n93.9% accuracy, and that recognizes all speakers in three-speaker scenarios\nwith 81.2% accuracy.<SUP>1</SUP>\n"
      ],
      "doi": "10.21437/Interspeech.2020-2430",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "neumann20_interspeech": {
      "authors": [
        [
          "Thilo von",
          "Neumann"
        ],
        [
          "Christoph",
          "Boeddeker"
        ],
        [
          "Lukas",
          "Drude"
        ],
        [
          "Keisuke",
          "Kinoshita"
        ],
        [
          "Marc",
          "Delcroix"
        ],
        [
          "Tomohiro",
          "Nakatani"
        ],
        [
          "Reinhold",
          "Haeb-Umbach"
        ]
      ],
      "title": "Multi-Talker ASR for an Unknown Number of Sources: Joint Training of Source Counting, Separation and ASR",
      "original": "2519",
      "page_count": 5,
      "order": 634,
      "p1": "3097",
      "pn": "3101",
      "abstract": [
        "Most approaches to multi-talker overlapped speech separation and recognition\nassume that the number of simultaneously active speakers is given,\nbut in realistic situations, it is typically unknown. To cope with\nthis, we extend an iterative speech extraction system with mechanisms\nto count the number of sources and combine it with a single-talker\nspeech recognizer to form the first end-to-end multi-talker automatic\nspeech recognition system for an unknown number of active speakers.\nOur experiments show very promising performance in counting accuracy,\nsource separation and speech recognition on simulated clean mixtures\nfrom WSJ0-2mix and WSJ0-3mix. Among others, we set a new state-of-the-art\nword error rate on the WSJ0-2mix database. Furthermore, our system\ngeneralizes well to a larger number of speakers than it ever saw during\ntraining, as shown in experiments with the WSJ0-4mix database.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2519",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "upadhyay20_interspeech": {
      "authors": [
        [
          "Shreya G.",
          "Upadhyay"
        ],
        [
          "Bo-Hao",
          "Su"
        ],
        [
          "Chi-Chun",
          "Lee"
        ]
      ],
      "title": "Attentive Convolutional Recurrent Neural Network Using Phoneme-Level Acoustic Representation for Rare Sound Event Detection",
      "original": "2585",
      "page_count": 5,
      "order": 635,
      "p1": "3102",
      "pn": "3106",
      "abstract": [
        "A well-trained Acoustic Sound Event Detection system captures the patterns\nof the sound to accurately detect events of interest in an auditory\nscene, which enables applications across domains of multimedia, smart\nliving, and even health monitoring. Due to the scarcity and the weak\nlabelling nature of the sound event data, it is often challenging to\ntrain an accurate and robust acoustic event detection model directly,\nespecially for those rare occurrences. In this paper, we proposed an\narchitecture which takes the advantage of integrating ASR network representations\nas additional input when training a sound event detector. Here we used\nthe convolutional bi-directional recurrent neural network (CBRNN),\nwhich includes both spectral and temporal attentions, as the SED classifier\nand further combined the ASR feature representations when performing\nthe end-to-end CBRNN training. Our experiments on the TUT 2017 rare\nsound event detection dataset showed that with the inclusion of ASR\nfeatures, the overall discriminative performance of the end-to-end\nsound event detection system has improved; the average performance\nof our proposed framework in terms of f-score and error rates are 97%\nand 0.05% respectively.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2585"
    },
    "cornell20_interspeech": {
      "authors": [
        [
          "Samuele",
          "Cornell"
        ],
        [
          "Maurizio",
          "Omologo"
        ],
        [
          "Stefano",
          "Squartini"
        ],
        [
          "Emmanuel",
          "Vincent"
        ]
      ],
      "title": "Detecting and Counting Overlapping Speakers in Distant Speech Scenarios",
      "original": "2671",
      "page_count": 5,
      "order": 636,
      "p1": "3107",
      "pn": "3111",
      "abstract": [
        "We consider the problem of detecting the activity and counting overlapping\nspeakers in distant-microphone recordings. We treat supervised Voice\nActivity Detection (VAD), Overlapped Speech Detection (OSD), joint\nVAD+OSD, and speaker counting as instances of a general Overlapped\nSpeech Detection and Counting (OSDC) task, and we design a Temporal\nConvolutional Network (TCN) based method to address it. We show that\nTCNs significantly outperform state-of-the-art methods on two real-world\ndistant speech datasets. In particular our best architecture obtains,\nfor OSD, 29.1% and 25.5% absolute improvement in Average Precision\nover previous techniques on, respectively, the AMI and CHiME-6 datasets.\nFurthermore, we find that generalization for joint VAD+OSD improves\nby using a speaker counting objective rather than a VAD+OSD objective.\nWe also study the effectiveness of forced alignment based labeling\nand data augmentation, and show that both can improve OSD performance.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2671",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "moritz20_interspeech": {
      "authors": [
        [
          "Niko",
          "Moritz"
        ],
        [
          "Gordon",
          "Wichern"
        ],
        [
          "Takaaki",
          "Hori"
        ],
        [
          "Jonathan Le",
          "Roux"
        ]
      ],
      "title": "All-in-One Transformer: Unifying Speech Recognition, Audio Tagging, and Event Detection",
      "original": "2757",
      "page_count": 5,
      "order": 637,
      "p1": "3112",
      "pn": "3116",
      "abstract": [
        "Automatic speech recognition (ASR), audio tagging (AT), and acoustic\nevent detection (AED) are typically treated as separate problems, where\neach task is tackled using specialized system architectures. This is\nin contrast with the way the human auditory system uses a single (binaural)\npathway to process sound signals from different sources. In addition,\nan acoustic model trained to recognize speech as well as sound events\ncould leverage multi-task learning to alleviate data scarcity problems\nin individual tasks. In this work, an all-in-one (AIO) acoustic model\nbased on the Transformer architecture is trained to solve ASR, AT,\nand AED tasks simultaneously, where model parameters are shared across\nall tasks. For the ASR and AED tasks, the Transformer model is combined\nwith the connectionist temporal classification (CTC) objective to enforce\na monotonic ordering and to utilize timing information. Our experiments\ndemonstrate that the AIO Transformer achieves better performance compared\nto all baseline systems of various recent DCASE challenge tasks and\nis suitable for the  total transcription of an acoustic scene, i.e.,\nto simultaneously transcribe speech and recognize the acoustic events\noccurring in it.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2757",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "diener20_interspeech": {
      "authors": [
        [
          "Lorenz",
          "Diener"
        ],
        [
          "Shahin",
          "Amiriparian"
        ],
        [
          "Catarina",
          "Botelho"
        ],
        [
          "Kevin",
          "Scheck"
        ],
        [
          "Dennis",
          "K\u00fcster"
        ],
        [
          "Isabel",
          "Trancoso"
        ],
        [
          "Bj\u00f6rn W.",
          "Schuller"
        ],
        [
          "Tanja",
          "Schultz"
        ]
      ],
      "title": "Towards Silent Paralinguistics: Deriving Speaking Mode and Speaker ID from Electromyographic Signals",
      "original": "2848",
      "page_count": 5,
      "order": 638,
      "p1": "3117",
      "pn": "3121",
      "abstract": [
        "Silent Computational Paralinguistics (SCP) &#8212; the assessment of\nspeaker states and traits from non-audibly spoken communication &#8212;\nhas rarely been targeted in the rich body of either Computational Paralinguistics\nor Silent Speech Processing. Here, we provide first steps towards this\nchallenging but potentially highly rewarding endeavour: Paralinguistics\ncan enrich spoken language interfaces, while Silent Speech Processing\nenables confidential and unobtrusive spoken communication for everybody,\nincluding mute speakers. We approach SCP by using speech-related biosignals\nstemming from facial muscle activities captured by surface electromyography\n(EMG). To demonstrate the feasibility of SCP, we select one speaker\ntrait (speaker identity) and one speaker state (speaking mode). We\nintroduce two promising strategies for SCP: (1) deriving paralinguistic\nspeaker information directly from EMG of silently produced speech versus\n(2) first converting EMG into an audible speech signal followed by\nconventional computational paralinguistic methods. We compare traditional\nfeature extraction and decision making approaches to more recent deep\nrepresentation and transfer learning by convolutional and recurrent\nneural networks, using openly available EMG data. We find that paralinguistics\ncan be assessed not only from acoustic speech but also from silent\nspeech captured by EMG.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2848",
      "author_area_id": "3",
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "zhong20_interspeech": {
      "authors": [
        [
          "Shun-Chang",
          "Zhong"
        ],
        [
          "Bo-Hao",
          "Su"
        ],
        [
          "Wei",
          "Huang"
        ],
        [
          "Yi-Ching",
          "Liu"
        ],
        [
          "Chi-Chun",
          "Lee"
        ]
      ],
      "title": "Predicting Collaborative Task Performance Using Graph Interlocutor Acoustic Network in Small Group Interaction",
      "original": "1698",
      "page_count": 5,
      "order": 639,
      "p1": "3122",
      "pn": "3126",
      "abstract": [
        "Recent works have demonstrated that the integration of group-level\npersonality and vocal behaviors can provide enhanced prediction power\non task performance for small group interactions. In this work, we\npropose that the impact of member personality for task performance\nprediction in groups should be explicitly modeled from both  intra\nand  inter-group perspectives. Specifically, we propose a Graph Interlocutor\nAcoustic Network (G-IAN) architecture that jointly learns the relationship\nbetween vocal behaviors and personality attributes with intra-group\nattention and inter-group graph convolutional layer. We evaluate our\nproposed G-IAN on two group interaction databases and achieve 78.4%\nand 72.2% group performance classification accuracy, which outperforms\nthe baseline model that models vocal behavior only by 14% absolute.\nFurther, our analysis shows that Agreeableness and Conscientiousness\ndemonstrate a clear positive impact in our model that leverages the\ninter-group personality structure for enhanced task performance prediction.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1698",
      "author_area_id": "3",
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "gosztolya20_interspeech": {
      "authors": [
        [
          "G\u00e1bor",
          "Gosztolya"
        ]
      ],
      "title": "Very Short-Term Conflict Intensity Estimation Using Fisher Vectors",
      "original": "2349",
      "page_count": 5,
      "order": 640,
      "p1": "3127",
      "pn": "3131",
      "abstract": [
        "The automatic detection of conflict situations from human speech has\nseveral applications like obtaining feedback of employees in call centers,\nthe surveillance of public spaces, and other roles in human-computer\ninteractions. Although several methods have been developed to automatic\nconflict detection, they were designed to operate on relatively long\nutterances. In practice, however, it would be beneficial to process\nmuch shorter speech segments. With the traditional workflow of paralinguistic\nspeech processing, this would require properly annotated training and\ntesting material consisting of short clips. In this study we show that\nSupport Vector Regression machine learning models using Fisher vectors\nas features, even when trained on longer utterances, allow us to efficiently\nand accurately detect conflict intensity from very short audio segments.\nEven without having reliable annotations of these such short chunks,\nthe mean scores of the predictions corresponding to short segments\nof the same original, longer utterances correlate well to the reference\nmanual annotation. We also verify the validity of this approach by\ncomparing the SVM predictions of the chunks with a manual annotation\nfor the full and the 5-second-long cases. Our findings allow the construction\nof conflict detection systems having smaller delay, therefore being\nmore useful in practice.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2349",
      "author_area_id": "3",
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "mori20_interspeech": {
      "authors": [
        [
          "Hiroki",
          "Mori"
        ],
        [
          "Yuki",
          "Kikuchi"
        ]
      ],
      "title": "Gaming Corpus for Studying Social Screams",
      "original": "2553",
      "page_count": 4,
      "order": 641,
      "p1": "3132",
      "pn": "3135",
      "abstract": [
        "Screams in everyday conversation, rather than in emergencies, are considered\nas nonverbal behavior that makes our speech communication rich and\nexpressive. This paper focuses on screams in conversation. Identification\nof screams in existing spoken dialog corpora revealed that these corpora\ncontained only a small number of screams, so not adequate for the investigation\nof the screams. In order to obtain more screams that naturally occur\nin conversation, we recorded dialogs while playing highly action-oriented\ngames. Following to our criteria to identify screams, 1437 screams\nwere detected from the whole recordings. The screams in our corpus\nwere 12 times more frequent than the existing gaming corpus. As for\nthe number of screams per minute, a strong positive correlation was\nobserved between two speakers of the same pair, suggesting that the\ninterlocutors produced screams not purely spontaneously, but tried\nto get the screaming behavior closer to the other person. Results of\nthe acoustic analysis showed that the typical scream is produced in\n60&#8211;140 mel higher and 8 dB louder voice than typical normal speech.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2553",
      "author_area_id": "3",
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "afshan20_interspeech": {
      "authors": [
        [
          "Amber",
          "Afshan"
        ],
        [
          "Jody",
          "Kreiman"
        ],
        [
          "Abeer",
          "Alwan"
        ]
      ],
      "title": "Speaker Discrimination in Humans and Machines: Effects of Speaking Style Variability",
      "original": "3004",
      "page_count": 5,
      "order": 642,
      "p1": "3136",
      "pn": "3140",
      "abstract": [
        "Does speaking style variation affect humans&#8217; ability to distinguish\nindividuals from their voices? How do humans compare with automatic\nsystems designed to discriminate between voices? In this paper, we\nattempt to answer these questions by comparing human and machine speaker\ndiscrimination performance for read speech versus casual conversations.\nThirty listeners were asked to perform a same versus different speaker\ntask. Their performance was compared to a state-of-the-art x-vector/PLDA-based\nautomatic speaker verification system. Results showed that both humans\nand machines performed better with style-matched stimuli, and human\nperformance was better when listeners were native speakers of American\nEnglish. Native listeners performed better than machines in the style-matched\nconditions (EERs of 6.96% versus 14.35% for read speech, and 15.12%\nversus 19.87%, for conversations), but for style-mismatched conditions,\nthere was no significant difference between native listeners and machines.\nIn all conditions, fusing human responses with machine results showed\nimprovements compared to each alone, suggesting that humans and machines\nhave different approaches to speaker discrimination tasks. Differences\nin the approaches were further confirmed by examining results for individual\nspeakers which showed that the perception of distinct and confused\nspeakers differed between human listeners and machines.\n"
      ],
      "doi": "10.21437/Interspeech.2020-3004",
      "author_area_id": "3",
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "sabu20_interspeech": {
      "authors": [
        [
          "Kamini",
          "Sabu"
        ],
        [
          "Preeti",
          "Rao"
        ]
      ],
      "title": "Automatic Prediction of Confidence Level from Children&#8217;s Oral Reading Recordings",
      "original": "2276",
      "page_count": 5,
      "order": 643,
      "p1": "3141",
      "pn": "3145",
      "abstract": [
        "Perceived speaker confidence or certainty has been found to correlate\nwith lexical and acoustic-prosodic features in the spontaneous speech\nof children interacting with an adult. We investigate the prediction\nof confidence in the context of oral reading of stories by children\nwith good word recognition skills where we must rely purely on prosodic\nfeatures. We report a dataset of oral reading recordings that has been\nmanually rated for confidence at the level of text paragraphs of 50&#8211;70\nwords. Several acoustic features computed at different time scales\nare evaluated via a trained classifier for the prediction of the subjective\nratings. Features based on pausing, pitch and speech rate are found\nto be important predictors of perceived confidence. Also it is seen\nthat the ratings are influenced by signal properties computed across\nthe utterance. When trained on recordings with strong rater agreement,\nthe system predicts low confidence readers with an F-score of 0.70.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2276",
      "author_area_id": "3",
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "xue20_interspeech": {
      "authors": [
        [
          "W.",
          "Xue"
        ],
        [
          "V. Mendoza",
          "Ramos"
        ],
        [
          "W.",
          "Harmsen"
        ],
        [
          "Catia",
          "Cucchiarini"
        ],
        [
          "R.W.N.M. van",
          "Hout"
        ],
        [
          "Helmer",
          "Strik"
        ]
      ],
      "title": "Towards a Comprehensive Assessment of Speech Intelligibility for Pathological Speech",
      "original": "2693",
      "page_count": 5,
      "order": 644,
      "p1": "3146",
      "pn": "3150",
      "abstract": [
        "Speech intelligibility is an essential though complex construct in\nspeech pathology. It is affected by multiple contextual variables and\nit is often measured in different ways. In this paper, we evaluate\nvarious measures of speech intelligibility based on orthographic transcriptions,\nwith respect to their reliability and validity. For this study, different\nspeech tasks were analyzed together with their respective perceptual\nratings assigned by five experienced speech-language pathologists:\na Visual Analogue Scale (VAS) and two types of orthographic transcriptions,\none in terms of existing words and the other in terms of perceived\nsegments, including nonsense words. Six subword measures concerning\ngraphemes and phonemes were derived automatically from these transcriptions.\nAll measures exhibit high degrees of reliability. Correlations between\nthe six subword measures and three independent measures, VAS, word\naccuracy, and severity level, reveal that the measures extracted automatically\nfrom the orthographic transcriptions are valid predictors of speech\nintelligibility. The results also indicate differences between the\nspeech tasks, suggesting that a comprehensive assessment of speech\nintelligibility requires materials from different speech tasks in combination\nwith measures at different granularity levels: utterance, word, and\nsubword. We discuss these results in relation to those of previous\nresearch and suggest possible avenues for future research.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2693",
      "author_area_id": "3",
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "lin20h_interspeech": {
      "authors": [
        [
          "Yi",
          "Lin"
        ],
        [
          "Hongwei",
          "Ding"
        ]
      ],
      "title": "Effects of Communication Channels and Actor&#8217;s Gender on Emotion Identification by Native Mandarin Speakers",
      "original": "1498",
      "page_count": 5,
      "order": 645,
      "p1": "3151",
      "pn": "3155",
      "abstract": [
        "Communication channels and actor&#8217;s gender have been increasingly\nreported to influence emotion perception, but past literature exploring\nthese two factors has largely been disassociated. The present study\nexamined how emotions expressed by actors of the two genders are perceived\nin three different sensory channels (i.e. face, prosody, and semantics).\nEighty-eight native Mandarin participants (43 females and 45 males)\nwere asked to identify the emotion displayed visually through face,\nor auditorily through prosody or semantics in a fixed-choice format,\nin which accuracy and reaction time were recorded. Results revealed\nthat visual facial expressions were more accurately and rapidly identified,\nparticularly when posed by female actors. Additionally, emotion perception\nin the auditory modality was modulated by actor&#8217;s gender to a\ngreater extent: emotional prosody yielded more accurate and faster\nresponses when expressed by female than male actors, while emotional\nsemantics produced better performances when presented by males. To\nsum up, paralinguistic (i.e., visual and prosodic) dominance effects\nare more evident in emotions expressed by female than male actors.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1498",
      "author_area_id": "3",
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "anjos20_interspeech": {
      "authors": [
        [
          "Ivo",
          "Anjos"
        ],
        [
          "Maxine",
          "Eskenazi"
        ],
        [
          "Nuno",
          "Marques"
        ],
        [
          "Margarida",
          "Grilo"
        ],
        [
          "Isabel",
          "Guimar\u00e3es"
        ],
        [
          "Jo\u00e3o",
          "Magalh\u00e3es"
        ],
        [
          "Sofia",
          "Cavaco"
        ]
      ],
      "title": "Detection of Voicing and Place of Articulation of Fricatives with Deep Learning in a Virtual Speech and Language Therapy Tutor",
      "original": "2821",
      "page_count": 5,
      "order": 646,
      "p1": "3156",
      "pn": "3160",
      "abstract": [
        "Children with fricative distortion errors have to learn how to correctly\nuse the vocal folds, and which place of articulation to use in order\nto correctly produce the different fricatives. Here we propose a virtual\ntutor for fricatives distortion correction. This is a virtual tutor\nfor speech and language therapy that helps children understand their\nfricative production errors and how to correctly use their speech organs.\nThe virtual tutor uses log Mel filter banks and deep learning techniques\nwith spectral-temporal convolutions of the data to classify the fricatives\nin children&#8217;s speech by place of articulation and voicing. It\nachieves an accuracy of 90.40% for place of articulation and 90.93%\nfor voicing with children&#8217;s speech. Furthermore, this paper discusses\na multidimensional advanced data analysis of the first layer convolutional\nkernel filters that validates the usefulness of performing the convolution\non the log Mel filter bank.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2821",
      "author_area_id": "3",
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "zhang20y_interspeech": {
      "authors": [
        [
          "Haitong",
          "Zhang"
        ],
        [
          "Yue",
          "Lin"
        ]
      ],
      "title": "Unsupervised Learning for Sequence-to-Sequence Text-to-Speech for Low-Resource Languages",
      "original": "1403",
      "page_count": 5,
      "order": 647,
      "p1": "3161",
      "pn": "3165",
      "abstract": [
        "Recently, sequence-to-sequence models with attention have been successfully\napplied in Text-to-speech (TTS). These models can generate near-human\nspeech with a large accurately-transcribed speech corpus. However,\npreparing such a large data-set is both expensive and laborious. To\nalleviate the problem of heavy data demand, we propose a novel unsupervised\npre-training mechanism in this paper. Specifically, we first use Vector-quantization\nVariational-Autoencoder (VQ-VAE) to extract the unsupervised linguistic\nunits from large-scale, publicly found, and untranscribed speech. We\nthen pre-train the sequence-to-sequence TTS model by using the &#60;unsupervised\nlinguistic units, audio&#62; pairs. Finally, we fine-tune the model\nwith a small amount of &#60;text, audio&#62; paired data from the target\nspeaker. As a result, both objective and subjective evaluations show\nthat our proposed method can synthesize more intelligible and natural\nspeech with the same amount of paired training data. Besides, we extend\nour proposed method to the hypothesized low-resource languages and\nverify the effectiveness of the method using objective evaluation.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1403",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "palkama20_interspeech": {
      "authors": [
        [
          "Kasperi",
          "Palkama"
        ],
        [
          "Lauri",
          "Juvela"
        ],
        [
          "Alexander",
          "Ilin"
        ]
      ],
      "title": "Conditional Spoken Digit Generation with StyleGAN",
      "original": "1461",
      "page_count": 5,
      "order": 648,
      "p1": "3166",
      "pn": "3170",
      "abstract": [
        "This paper adapts a StyleGAN model for speech generation with minimal\nor no conditioning on text. StyleGAN is a multi-scale convolutional\nGAN capable of hierarchically capturing data structure and latent variation\non multiple spatial (or temporal) levels. The model has previously\nachieved impressive results on facial image generation, and it is appealing\nto audio applications due to similar multi-level structures present\nin the data. In this paper, we train a StyleGAN to generate mel-spectrograms\non the Speech Commands dataset, which contains spoken digits uttered\nby multiple speakers in varying acoustic conditions. In a conditional\nsetting our model is conditioned on the digit identity, while learning\nthe remaining data variation remains an unsupervised task. We compare\nour model to the current unsupervised state-of-the-art speech synthesis\nGAN architecture, the WaveGAN, and show that the proposed model outperforms\naccording to numerical measures and subjective evaluation by listening\ntests.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1461",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "yang20g_interspeech": {
      "authors": [
        [
          "Jingzhou",
          "Yang"
        ],
        [
          "Lei",
          "He"
        ]
      ],
      "title": "Towards Universal Text-to-Speech",
      "original": "1590",
      "page_count": 5,
      "order": 649,
      "p1": "3171",
      "pn": "3175",
      "abstract": [
        "This paper studies a multilingual sequence-to-sequence text-to-speech\nframework towards universal modeling, that is able to synthesize speech\nfor any speaker in any language using a single model. This framework\nconsists of a transformer-based acoustic predictor and a WaveNet neural\nvocoder, with global conditions from speaker and language networks.\nIt is examined on a massive TTS data set with around 1250 hours of\ndata from 50 language locales, and the amount of data in different\nlocales is highly unbalanced. Although the multilingual model exhibits\nthe transfer learning ability to benefit the low-resource languages,\ndata imbalance still undermines the model performance. A data balance\ntraining strategy is successfully applied and effectively improves\nthe voice quality of the low-resource languages. Furthermore, this\npaper examines the modeling capacity of extending to new speakers and\nlanguages, as a key step towards universal modeling. Experiments show\n20 seconds of data is feasible for a new speaker and 6 minutes for\na new language.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1590",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "katsurada20_interspeech": {
      "authors": [
        [
          "Kouichi",
          "Katsurada"
        ],
        [
          "Korin",
          "Richmond"
        ]
      ],
      "title": "Speaker-Independent Mel-Cepstrum Estimation from Articulator Movements Using D-Vector Input",
      "original": "1630",
      "page_count": 5,
      "order": 650,
      "p1": "3176",
      "pn": "3180",
      "abstract": [
        "We describe a speaker-independent mel-cepstrum estimation system which\naccepts electromagnetic articulography (EMA) data as input. The system\ncollects speaker information with d-vectors generated from the EMA\ndata. We have also investigated the effect of speaker independence\nin the input vectors given to the mel-cepstrum estimator. This is accomplished\nby introducing a two-stage network, where the first stage is trained\nto output EMA sequences that are averaged across all speakers on a\nper-triphone basis (and so are speaker-independent) and the second\nreceives these as input for mel-cepstrum estimation. Experimental results\nshow that using the d-vectors can improve the performance of mel-cepstrum\nestimation by 0.19 dB with regard to mel-cepstrum distortion in the\nclosed-speaker test set. Additionally, giving triphone-averaged EMA\ndata to a mel-cepstrum estimator is shown to improve the performance\nby a further 0.16 dB, which indicates that the speaker-independent\ninput has a positive effect on mel-cepstrum estimation.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1630",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "liang20_interspeech": {
      "authors": [
        [
          "Xiangyu",
          "Liang"
        ],
        [
          "Zhiyong",
          "Wu"
        ],
        [
          "Runnan",
          "Li"
        ],
        [
          "Yanqing",
          "Liu"
        ],
        [
          "Sheng",
          "Zhao"
        ],
        [
          "Helen",
          "Meng"
        ]
      ],
      "title": "Enhancing Monotonicity for Robust Autoregressive Transformer TTS",
      "original": "1751",
      "page_count": 5,
      "order": 651,
      "p1": "3181",
      "pn": "3185",
      "abstract": [
        "With the development of sequence-to-sequence modeling algorithms, Text-to-Speech\n(TTS) techniques have achieved significant improvement in speech quality\nand naturalness. These deep learning algorithms, such as recurrent\nneural networks (RNNs) and its memory enhanced variations, have shown\nstrong reconstruction ability from input linguistic features to acoustic\nfeatures. However, the efficiency of these algorithms is limited for\nits sequential process in both training and inference. Recently, Transformer\nwith superiority in parallelism is proposed to TTS. It employs the\npositional embedding instead of recurrent mechanism for position modeling\nand significantly boosts training speed. However, this approach lacks\nmonotonic constraint and is deficient with issues like pronunciation\nskipping. Therefore, in this paper, we propose a monotonicity enhancing\napproach with the combining use of Stepwise Monotonic Attention (SMA)\nand multi-head attention for Transformer based TTS system. Experiments\nshow the proposed approach can reduce bad cases from 53 of 500 sentences\nto 1, together with an improvement on MOS from 4.09 to 4.17 in the\nnaturalness test.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1751",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "mohan20_interspeech": {
      "authors": [
        [
          "Devang S. Ram",
          "Mohan"
        ],
        [
          "Raphael",
          "Lenain"
        ],
        [
          "Lorenzo",
          "Foglianti"
        ],
        [
          "Tian Huey",
          "Teh"
        ],
        [
          "Marlene",
          "Staib"
        ],
        [
          "Alexandra",
          "Torresquintero"
        ],
        [
          "Jiameng",
          "Gao"
        ]
      ],
      "title": "Incremental Text to Speech for Neural Sequence-to-Sequence Models Using Reinforcement Learning",
      "original": "1822",
      "page_count": 5,
      "order": 652,
      "p1": "3186",
      "pn": "3190",
      "abstract": [
        "Modern approaches to text to speech require the entire input character\nsequence to be processed before any audio is synthesised. This latency\nlimits the suitability of such models for time-sensitive tasks like\nsimultaneous interpretation. Interleaving the action of reading a character\nwith that of synthesising audio reduces this latency. However, the\norder of this sequence of interleaved actions varies across sentences,\nwhich raises the question of how the actions should be chosen. We propose\na reinforcement learning based framework to train an agent to make\nthis decision. We compare our performance against that of deterministic,\nrule-based systems. Our results demonstrate that our agent successfully\nbalances the trade-off between the latency of audio generation and\nthe quality of synthesised audio. More broadly, we show that neural\nsequence-to-sequence models can be adapted to run in an incremental\nmanner.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1822",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "tu20b_interspeech": {
      "authors": [
        [
          "Tao",
          "Tu"
        ],
        [
          "Yuan-Jui",
          "Chen"
        ],
        [
          "Alexander H.",
          "Liu"
        ],
        [
          "Hung-yi",
          "Lee"
        ]
      ],
      "title": "Semi-Supervised Learning for Multi-Speaker Text-to-Speech Synthesis Using Discrete Speech Representation",
      "original": "1824",
      "page_count": 5,
      "order": 653,
      "p1": "3191",
      "pn": "3195",
      "abstract": [
        "Recently, end-to-end multi-speaker text-to-speech (TTS) systems gain\nsuccess in the situation where a lot of high-quality speech plus their\ncorresponding transcriptions are available. However, laborious paired\ndata collection processes prevent many institutes from building multi-speaker\nTTS systems of great performance. In this work, we propose a semi-supervised\nlearning approach for multi-speaker TTS. A multi-speaker TTS model\ncan learn from the untranscribed audio via the proposed encoder-decoder\nframework with discrete speech representation. The experiment results\ndemonstrate that with only an hour of paired speech data, whether the\npaired data is from multiple speakers or a single speaker, the proposed\nmodel can generate intelligible speech in different voices. We found\nthe model can benefit from the proposed semi-supervised learning approach\neven when part of the unpaired speech data is noisy. In addition, our\nanalysis reveals that different speaker characteristics of the paired\ndata have an impact on the effectiveness of semi-supervised TTS.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1824",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "saha20_interspeech": {
      "authors": [
        [
          "Pramit",
          "Saha"
        ],
        [
          "Sidney",
          "Fels"
        ]
      ],
      "title": "Learning Joint Articulatory-Acoustic Representations with Normalizing Flows",
      "original": "2004",
      "page_count": 5,
      "order": 654,
      "p1": "3196",
      "pn": "3200",
      "abstract": [
        "The articulatory geometric configurations of the vocal tract and the\nacoustic properties of the resultant speech sound are considered to\nhave a strong causal relationship. This paper aims at finding a joint\nlatent representation between the articulatory and acoustic domain\nfor vowel sounds via invertible neural network models, while simultaneously\npreserving the respective domain-specific features. Our model utilizes\na convolutional autoencoder architecture and normalizing flow-based\nmodels to allow both forward and inverse mappings in a semi-supervised\nmanner, between the mid-sagittal vocal tract geometry of a two degrees-of-freedom\narticulatory synthesizer with 1D acoustic wave model and the Mel-spectrogram\nrepresentation of the synthesized speech sounds. Our approach achieves\nsatisfactory performance in achieving both articulatory-to-acoustic\nas well as acoustic-to-articulatory mapping, thereby demonstrating\nour success in achieving a joint encoding of both the domains.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2004",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "yamashita20_interspeech": {
      "authors": [
        [
          "Yuki",
          "Yamashita"
        ],
        [
          "Tomoki",
          "Koriyama"
        ],
        [
          "Yuki",
          "Saito"
        ],
        [
          "Shinnosuke",
          "Takamichi"
        ],
        [
          "Yusuke",
          "Ijima"
        ],
        [
          "Ryo",
          "Masumura"
        ],
        [
          "Hiroshi",
          "Saruwatari"
        ]
      ],
      "title": "Investigating Effective Additional Contextual Factors in DNN-Based Spontaneous Speech Synthesis",
      "original": "2469",
      "page_count": 5,
      "order": 655,
      "p1": "3201",
      "pn": "3205",
      "abstract": [
        "In this paper, we investigate the effectiveness of using rich annotations\nin deep neural network (DNN)-based statistical speech synthesis. General\ntext-to-speech synthesis frameworks for reading-style speech use text-dependent\ninformation referred to as context. However, to achieve more human-like\nspeech synthesis, we should take paralinguistic and nonlinguistic features\ninto account. We focus on adding contextual features to the input features\nof DNN-based speech synthesis using spontaneous speech corpus with\nrich tags including paralinguistic and nonlinguistic features such\nas prosody, disfluency, and morphological features. Through experimental\nevaluations, we investigate the effectiveness of additional contextual\nfactors and show which factors enhance the naturalness as spontaneous\nspeech. This paper contributes as a guide to data collection for speech\nsynthesis.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2469",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "webber20_interspeech": {
      "authors": [
        [
          "Jacob J.",
          "Webber"
        ],
        [
          "Olivier",
          "Perrotin"
        ],
        [
          "Simon",
          "King"
        ]
      ],
      "title": "Hider-Finder-Combiner: An Adversarial Architecture for General Speech Signal Modification",
      "original": "2558",
      "page_count": 5,
      "order": 656,
      "p1": "3206",
      "pn": "3210",
      "abstract": [
        "We introduce a prototype system for modifying an arbitrary parameter\nof a speech signal. Unlike signal processing approaches that require\ndedicated methods for different parameters, our system can &#8212;\nin principle &#8212; modify any control parameter that the signal can\nbe annotated with. Our system comprises three neural networks. The\n&#8216;hider&#8217; removes all information related to the control\nparameter, outputting a hidden embedding. The &#8216;finder&#8217;\nis an adversary used to train the &#8216;hider&#8217;, attempting to\ndetect the value of the control parameter from the hidden embedding.\nThe &#8216;combiner&#8217; network recombines the hidden embedding\nwith a desired new value of the control parameter. The input and output\nto the system are mel-spectrograms and we employ a neural vocoder to\ngenerate the output speech waveform. As a proof of concept, we use\nF<SUB>0</SUB> as the control parameter. The system was evaluated in\nterms of control parameter accuracy and naturalness against a high\nquality signal processing method of F<SUB>0</SUB> modification that\nalso works in the spectrogram domain. We also show that, with modifications\nonly to training data, the system is capable of modifying the 1<SUP>st</SUP>\nand 2<SUP>nd</SUP> vocal tract formants, showing progress towards universal\nsignal modification.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2558",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "lin20i_interspeech": {
      "authors": [
        [
          "Weiwei",
          "Lin"
        ],
        [
          "Man-Wai",
          "Mak"
        ]
      ],
      "title": "Wav2Spk: A Simple DNN Architecture for Learning Speaker Embeddings from Waveforms",
      "original": "1287",
      "page_count": 5,
      "order": 657,
      "p1": "3211",
      "pn": "3215",
      "abstract": [
        "Speaker recognition has seen impressive advances with the advent of\ndeep neural networks (DNNs). However, state-of-the-art speaker recognition\nsystems still rely on human engineering features such as mel-frequency\ncepstrum coefficients (MFCC). We believe that the handcrafted features\nlimit the potential of the powerful representation of DNNs. Besides,\nthere are also additional steps such as voice activity detection (VAD)\nand cepstral mean and variance normalization (CMVN) after computing\nthe MFCC. In this paper, we show that MFCC, VAD, and CMVN can be replaced\nby the tools available in the standard deep learning toolboxes, such\nas a stacked of stride convolutions, temporal gating, and instance\nnormalization. With these tools, we show that directly learning speaker\nembeddings from waveforms outperforms an x-vector network that uses\nMFCC or filter-bank output as features. We achieve an EER of 1.95%\non the VoxCeleb1 test set using an end-to-end training scheme, which,\nto our best knowledge, is the best performance reported using raw waveforms.\nWhat&#8217;s more, the proposed method is complementary with x-vector\nsystems. The fusion of the proposed method with x-vectors trained on\nfilter-bank features produce an EER of 1.55%.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1287",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "pham20b_interspeech": {
      "authors": [
        [
          "Minh",
          "Pham"
        ],
        [
          "Zeqian",
          "Li"
        ],
        [
          "Jacob",
          "Whitehill"
        ]
      ],
      "title": "How Does Label Noise Affect the Quality of Speaker Embeddings?",
      "original": "1395",
      "page_count": 5,
      "order": 658,
      "p1": "3216",
      "pn": "3220",
      "abstract": [
        "A common assumption when collecting speech datasets is that the accuracy\nof data labels strongly influences the accuracy of speaker embedding\nmodels and verification systems trained from these data. However, we\nshow in experiments<SUP>1</SUP> on the large and diverse VoxCeleb2\ndataset that this is not always the case: Under four different labeling\nmodels (Split, Merge, Permute, and Corrupt), we find that the impact\non trained speaker embedding models, as measured by the Equal Error\nRate (EER) of speaker verification, is mild (just a few percent absolute\nerror increase), except with very large amounts of noise (i.e., every\nminibatch is almost completely corrupted). This suggests that efforts\nto collect speech datasets might benefit more from ensuring large size\nand diversity rather than meticulous labeling.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1395",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "liu20q_interspeech": {
      "authors": [
        [
          "Xuechen",
          "Liu"
        ],
        [
          "Md.",
          "Sahidullah"
        ],
        [
          "Tomi",
          "Kinnunen"
        ]
      ],
      "title": "A Comparative Re-Assessment of Feature Extractors for Deep Speaker Embeddings",
      "original": "1765",
      "page_count": 5,
      "order": 659,
      "p1": "3221",
      "pn": "3225",
      "abstract": [
        "Modern automatic speaker verification relies largely on deep neural\nnetworks (DNNs) trained on mel-frequency cepstral coefficient (MFCC)\nfeatures. While there are alternative feature extraction methods based\non phase, prosody and long-term temporal operations, they have not\nbeen extensively studied with DNN-based methods. We aim to fill this\ngap by providing extensive re-assessment of 14 feature extractors on\nVoxCeleb and SITW datasets. Our findings reveal that features equipped\nwith techniques such as spectral centroids, group delay function, and\nintegrated noise suppression provide promising alternatives to MFCCs\nfor deep speaker embeddings extraction. Experimental results demonstrate\nup to 16.3% (VoxCeleb) and 25.1% (SITW) relative decrease in equal\nerror rate (EER) to the baseline.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1765",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "xia20_interspeech": {
      "authors": [
        [
          "Wei",
          "Xia"
        ],
        [
          "John H.L.",
          "Hansen"
        ]
      ],
      "title": "Speaker Representation Learning Using Global Context Guided Channel and Time-Frequency Transformations",
      "original": "1845",
      "page_count": 5,
      "order": 660,
      "p1": "3226",
      "pn": "3230",
      "abstract": [
        "In this study, we propose the global context guided channel and time-frequency\ntransformations to model the long-range, non-local time-frequency dependencies\nand channel variances in speaker representations. We use the global\ncontext information to enhance important channels and recalibrate salient\ntime-frequency locations by computing the similarity between the global\ncontext and local features. The proposed modules, together with a popular\nResNet based model, are evaluated on the VoxCeleb1 dataset, which is\na large scale speaker verification corpus collected in the wild. This\nlightweight block can be easily incorporated into a CNN model with\nlittle additional computational costs and effectively improves the\nspeaker verification performance compared to the baseline ResNet-LDE\nmodel and the Squeeze&amp;Excitation block by a large margin. Detailed\nablation studies are also performed to analyze various factors that\nmay impact the performance of the proposed modules. We find that by\nemploying the proposed L2-tf-GTFC transformation block, the Equal Error\nRate decreases from 4.56% to 3.07%, a relative 32.68% reduction, and\na relative 27.28% improvement in terms of the DCF score. The results\nindicate that our proposed global context guided transformation modules\ncan efficiently improve the learned speaker representations by achieving\ntime-frequency and channel-wise feature recalibration.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1845",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "kwon20_interspeech": {
      "authors": [
        [
          "Yoohwan",
          "Kwon"
        ],
        [
          "Soo-Whan",
          "Chung"
        ],
        [
          "Hong-Goo",
          "Kang"
        ]
      ],
      "title": "Intra-Class Variation Reduction of Speaker Representation in Disentanglement Framework",
      "original": "2075",
      "page_count": 5,
      "order": 661,
      "p1": "3231",
      "pn": "3235",
      "abstract": [
        "In this paper, we propose an effective training strategy to extract\nrobust speaker representations from a speech signal. One of the key\nchallenges in speaker recognition tasks is to learn latent representations\nor embeddings containing solely speaker characteristic information\nin order to be robust in terms of intra-speaker variations. By modifying\nthe network architecture to generate both speaker-related and speaker-unrelated\nrepresentations, we exploit a learning criterion which minimizes the\nmutual information between these disentangled embeddings. We also introduce\nan identity change loss criterion which utilizes a reconstruction error\nto different utterances spoken by the same speaker. Since the proposed\ncriteria reduce the variation of speaker characteristics caused by\nchanges in background environment or spoken content, the resulting\nembeddings of each speaker become more consistent. The effectiveness\nof the proposed method is demonstrated through two tasks; disentanglement\nperformance, and improvement of speaker recognition accuracy compared\nto the baseline model on a benchmark dataset, VoxCeleb1. Ablation studies\nalso show the impact of each criterion on overall performance.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2075",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "georges20_interspeech": {
      "authors": [
        [
          "Munir",
          "Georges"
        ],
        [
          "Jonathan",
          "Huang"
        ],
        [
          "Tobias",
          "Bocklet"
        ]
      ],
      "title": "Compact Speaker Embedding: lrx-Vector",
      "original": "2106",
      "page_count": 5,
      "order": 662,
      "p1": "3236",
      "pn": "3240",
      "abstract": [
        "Deep neural networks (DNN) have recently been widely used in speaker\nrecognition systems, achieving state-of-the-art performance on various\nbenchmarks. The x-vector architecture is especially popular in this\nresearch community, due to its excellent performance and manageable\ncomputational complexity. In this paper, we present the lrx-vector\nsystem, which is the low-rank factorized version of the x-vector embedding\nnetwork. The primary objective of this topology is to further reduce\nthe memory requirement of the speaker recognition system. We discuss\nthe deployment of knowledge distillation for training the lrx-vector\nsystem and compare against low-rank factorization with SVD. On the\nVOiCES 2019 far-field corpus we were able to reduce the weights by\n28% compared to the full-rank x-vector system while keeping the recognition\nrate constant (1.83% EER).\n"
      ],
      "doi": "10.21437/Interspeech.2020-2106",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "kreyssig20_interspeech": {
      "authors": [
        [
          "Florian L.",
          "Kreyssig"
        ],
        [
          "Philip C.",
          "Woodland"
        ]
      ],
      "title": "Cosine-Distance Virtual Adversarial Training for Semi-Supervised Speaker-Discriminative Acoustic Embeddings",
      "original": "2270",
      "page_count": 5,
      "order": 663,
      "p1": "3241",
      "pn": "3245",
      "abstract": [
        "In this paper, we propose a semi-supervised learning (SSL) technique\nfor training deep neural networks (DNNs) to generate speaker-discriminative\nacoustic embeddings (speaker embeddings). Obtaining large amounts of\nspeaker recognition training data can be difficult for desired target\ndomains, especially under privacy constraints. The proposed technique\nreduces requirements for labelled data by leveraging unlabelled data.\nThe technique is a variant of virtual adversarial training (VAT) [1]\nin the form of a loss that is defined as the robustness of the speaker\nembedding against input perturbations, as measured by the cosine-distance.\nThus, we term the technique cosine-distance virtual adversarial training\n(CD-VAT). In comparison to many existing SSL techniques, the unlabelled\ndata does not have to come from the same set of classes (here speakers)\nas the labelled data. The effectiveness of CD-VAT is shown on the 2750+\nhour VoxCeleb data set, where on a speaker verification task it achieves\na reduction in equal error rate (EER) of 11.1% relative to a purely\nsupervised baseline. This is 32.5% of the improvement that would be\nachieved from supervised training if the speaker labels for the unlabelled\ndata were available.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2270",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "peng20b_interspeech": {
      "authors": [
        [
          "Junyi",
          "Peng"
        ],
        [
          "Rongzhi",
          "Gu"
        ],
        [
          "Yuexian",
          "Zou"
        ]
      ],
      "title": "Deep Speaker Embedding with Long Short Term Centroid Learning for Text-Independent Speaker Verification",
      "original": "2470",
      "page_count": 5,
      "order": 664,
      "p1": "3246",
      "pn": "3250",
      "abstract": [
        "Recently, speaker verification systems using deep neural networks have\nshown their effectiveness on large scale datasets. The widely used\npairwise loss functions only consider the discrimination within a mini-batch\ndata (short-term), while either the speaker identity information or\nthe whole training dataset is not fully exploited. Thus, these pairwise\ncomparisons may suffer from the interferences and variances brought\nby speaker-unrelated factors. To tackle this problem, we introduce\nthe speaker identity information to form long-term speaker embedding\ncentroids, which are determined by all the speakers in the training\nset. During the training process, each centroid dynamically accumulates\nthe statistics of all samples belonging to a specific speaker. Since\nthe long-term speaker embedding centroids are associated with a wide\nrange of training samples, these centroids have the potential to be\nmore robust and discriminative. Finally, these centroids are employed\nto construct a loss function, named long short term speaker loss (LSTSL).\nThe proposed LSTSL constrains that the distances between samples and\ncentroid from the same speaker are compact while those from different\nspeakers are dispersed. Experiments are conducted on VoxCeleb1 and\nVoxCeleb2. Results on the VoxCeleb1 dataset demonstrate the effectiveness\nof our proposed LSTSL.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2470",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "li20ca_interspeech": {
      "authors": [
        [
          "Lantian",
          "Li"
        ],
        [
          "Dong",
          "Wang"
        ],
        [
          "Thomas Fang",
          "Zheng"
        ]
      ],
      "title": "Neural Discriminant Analysis for Deep Speaker Embedding",
      "original": "2542",
      "page_count": 5,
      "order": 665,
      "p1": "3251",
      "pn": "3255",
      "abstract": [
        "Probabilistic Linear Discriminant Analysis (PLDA) is a popular tool\nin open-set classification/verification tasks. However, the Gaussian\nassumption underlying PLDA prevents it from being applied to situations\nwhere the data is clearly non-Gaussian. In this paper, we present a\nnovel nonlinear version of PLDA named as Neural Discriminant Analysis\n(NDA). This model employs an invertible deep neural network to transform\na complex distribution to a simple Gaussian, so that the linear Gaussian\nmodel can be readily established in the transformed space. We tested\nthis NDA model on a speaker recognition task where the deep speaker\nvectors (x-vectors) are presumably non-Gaussian. Experimental results\non two datasets demonstrate that NDA consistently outperforms PLDA,\nby handling the non-Gaussian distributions of the x-vectors.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2542",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "cho20b_interspeech": {
      "authors": [
        [
          "Jaejin",
          "Cho"
        ],
        [
          "Piotr",
          "\u017belasko"
        ],
        [
          "Jes\u00fas",
          "Villalba"
        ],
        [
          "Shinji",
          "Watanabe"
        ],
        [
          "Najim",
          "Dehak"
        ]
      ],
      "title": "Learning Speaker Embedding from Text-to-Speech",
      "original": "2970",
      "page_count": 5,
      "order": 666,
      "p1": "3256",
      "pn": "3260",
      "abstract": [
        "Zero-shot multi-speaker Text-to-Speech (TTS) generates target speaker\nvoices given an input text and the corresponding speaker embedding.\nIn this work, we investigate the effectiveness of the TTS reconstruction\nobjective to improve representation learning for speaker verification.\nWe jointly trained end-to-end Tacotron 2 TTS and speaker embedding\nnetworks in a self-supervised fashion. We hypothesize that the embeddings\nwill contain minimal phonetic information since the TTS decoder will\nobtain that information from the textual input. TTS reconstruction\ncan also be combined with speaker classification to enhance these embeddings\nfurther. Once trained, the speaker encoder computes representations\nfor the speaker verification task, while the rest of the TTS blocks\nare discarded. We investigated training TTS from either manual or ASR-generated\ntranscripts. The latter allows us to train embeddings on datasets without\nmanual transcripts. We compared ASR transcripts and Kaldi phone alignments\nas TTS inputs, showing that the latter performed better due to their\nfiner resolution. Unsupervised TTS embeddings improved EER by 2.06%\nabsolute with regard to i-vectors for the LibriTTS dataset. TTS with\nspeaker classification loss improved EER by 0.28% and 2.88% absolutely\nfrom a model using only speaker classification loss in LibriTTS and\nVoxceleb1 respectively.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2970",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "zhao20f_interspeech": {
      "authors": [
        [
          "Yan",
          "Zhao"
        ],
        [
          "DeLiang",
          "Wang"
        ]
      ],
      "title": "Noisy-Reverberant Speech Enhancement Using DenseUNet with Time-Frequency Attention",
      "original": "2952",
      "page_count": 5,
      "order": 667,
      "p1": "3261",
      "pn": "3265",
      "abstract": [
        "Background noise and room reverberation are two major distortions to\nthe speech signal in real-world environments. Each of them degrades\nspeech intelligibility and quality, and their combined effects are\nespecially detrimental. In this paper, we propose a DenseUNet based\nmodel for noisy-reverberant speech enhancement, where a novel time-frequency\n(T-F) attention mechanism is introduced to aggregate contextual information\namong different T-F units efficiently and a channelwise attention is\ndeveloped to merge sources of information among different feature maps.\nIn addition, we introduce a normalization-activation strategy to alleviate\nthe performance drop for small batch training. Systematic evaluations\ndemonstrate that the proposed algorithm substantially improves objective\nspeech intelligibility and quality in various noisy-reverberant conditions,\nand outperforms other related methods.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2952",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "zhang20z_interspeech": {
      "authors": [
        [
          "Zhuohuang",
          "Zhang"
        ],
        [
          "Chengyun",
          "Deng"
        ],
        [
          "Yi",
          "Shen"
        ],
        [
          "Donald S.",
          "Williamson"
        ],
        [
          "Yongtao",
          "Sha"
        ],
        [
          "Yi",
          "Zhang"
        ],
        [
          "Hui",
          "Song"
        ],
        [
          "Xiangang",
          "Li"
        ]
      ],
      "title": "On Loss Functions and Recurrency Training for GAN-Based Speech Enhancement Systems",
      "original": "1169",
      "page_count": 5,
      "order": 668,
      "p1": "3266",
      "pn": "3270",
      "abstract": [
        "Recent work has shown that it is feasible to use generative adversarial\nnetworks (GANs) for speech enhancement, however, these approaches have\nnot been compared to state-of-the-art (SOTA) non GAN-based approaches.\nAdditionally, many loss functions have been proposed for GAN-based\napproaches, but they have not been adequately compared. In this study,\nwe propose novel convolutional recurrent GAN (CRGAN) architectures\nfor speech enhancement. Multiple loss functions are adopted to enable\ndirect comparisons to other GAN-based systems. The benefits of including\nrecurrent layers are also explored. Our results show that the proposed\nCRGAN model outperforms the SOTA GAN-based models using the same loss\nfunctions and it outperforms other non-GAN based systems, indicating\nthe benefits of using a GAN for speech enhancement. Overall, the CRGAN\nmodel that combines an objective metric loss function with the mean\nsquared error (MSE) provides the best performance over comparison approaches\nacross many evaluation metrics.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1169",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "du20c_interspeech": {
      "authors": [
        [
          "Zhihao",
          "Du"
        ],
        [
          "Ming",
          "Lei"
        ],
        [
          "Jiqing",
          "Han"
        ],
        [
          "Shiliang",
          "Zhang"
        ]
      ],
      "title": "Self-Supervised Adversarial Multi-Task Learning for Vocoder-Based Monaural Speech Enhancement",
      "original": "1496",
      "page_count": 5,
      "order": 669,
      "p1": "3271",
      "pn": "3275",
      "abstract": [
        "In our previous study, we introduce the neural vocoder into monaural\nspeech enhancement, in which a flow-based generative vocoder is used\nto synthesize speech waveforms from the Mel power spectra enhanced\nby a denoising autoencoder. As a result, this vocoder-based enhancement\nmethod outperforms several state-of-the-art models on a speaker-dependent\ndataset. However, we find that there is a big gap between the enhancement\nperformance on the trained and untrained noises. Therefore, in this\npaper, we propose the self-supervised adversarial multi-task learning\n(SAMLE) to improve the noise generalization ability. In addition, the\nspeaker dependence is also evaluated for the vocoder-based methods,\nwhich is important for real-life applications. Experimental results\nshow that the proposed SAMLE further improves the enhancement performance\non both trained and untrained noises, resulting in a better noise generalization\nability. Moreover, we find that vocoder-based enhancement methods can\nbe speaker-independent through a large-scale training.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1496",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "kegler20_interspeech": {
      "authors": [
        [
          "Mikolaj",
          "Kegler"
        ],
        [
          "Pierre",
          "Beckmann"
        ],
        [
          "Milos",
          "Cernak"
        ]
      ],
      "title": "Deep Speech Inpainting of Time-Frequency Masks",
      "original": "1532",
      "page_count": 5,
      "order": 670,
      "p1": "3276",
      "pn": "3280",
      "abstract": [
        "Transient loud intrusions, often occurring in noisy environments, can\ncompletely overpower speech signal and lead to an inevitable loss of\ninformation. While existing algorithms for noise suppression can yield\nimpressive results, their efficacy remains limited for very low signal-to-noise\nratios or when parts of the signal are missing. To address these limitations,\nhere we propose an end-to-end framework for speech inpainting, the\ncontext-based retrieval of missing or severely distorted parts of time-frequency\nrepresentation of speech. The framework is based on a convolutional\nU-Net trained via deep feature losses, obtained using speechVGG, a\ndeep speech feature extractor pre-trained on an auxiliary word classification\ntask. Our evaluation results demonstrate that the proposed framework\ncan recover large portions of missing or distorted time-frequency representation\nof speech, up to 400 ms and 3.2 kHz in bandwidth. In particular, our\napproach provided a substantial increase in STOI &amp; PESQ objective\nmetrics of the initially corrupted speech samples. Notably, using deep\nfeature losses to train the framework led to the best results, as compared\nto conventional approaches.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1532",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "shankar20_interspeech": {
      "authors": [
        [
          "Nikhil",
          "Shankar"
        ],
        [
          "Gautam Shreedhar",
          "Bhat"
        ],
        [
          "Issa M.S.",
          "Panahi"
        ]
      ],
      "title": "Real-Time Single-Channel Deep Neural Network-Based Speech Enhancement on Edge Devices",
      "original": "1901",
      "page_count": 5,
      "order": 671,
      "p1": "3281",
      "pn": "3285",
      "abstract": [
        "In this paper, we present a deep neural network architecture comprising\nof both convolutional neural network (CNN) and recurrent neural network\n(RNN) layers for real-time single-channel speech enhancement (SE).\nThe proposed neural network model focuses on enhancing the noisy speech\nmagnitude spectrum on a frame-by-frame process. The developed model\nis implemented on the smartphone (edge device), to demonstrate the\nreal-time usability of the proposed method. Perceptual evaluation of\nspeech quality (PESQ) and short-time objective intelligibility (STOI)\ntest results are used to compare the proposed algorithm to previously\npublished conventional and deep learning-based SE methods. Subjective\nratings show the performance improvement of the proposed model over\nthe other baseline SE methods.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1901",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "lin20j_interspeech": {
      "authors": [
        [
          "Ju",
          "Lin"
        ],
        [
          "Sufeng",
          "Niu"
        ],
        [
          "Adriaan J. van",
          "Wijngaarden"
        ],
        [
          "Jerome L.",
          "McClendon"
        ],
        [
          "Melissa C.",
          "Smith"
        ],
        [
          "Kuang-Ching",
          "Wang"
        ]
      ],
      "title": "Improved Speech Enhancement Using a Time-Domain GAN with Mask Learning",
      "original": "1946",
      "page_count": 5,
      "order": 672,
      "p1": "3286",
      "pn": "3290",
      "abstract": [
        "Speech enhancement is an essential component in robust automatic speech\nrecognition (ASR) systems. Most speech enhancement methods are nowadays\nbased on neural networks that use feature-mapping or mask-learning.\nThis paper proposes a novel speech enhancement method that integrates\ntime-domain feature mapping and mask learning into a unified framework\nusing a Generative Adversarial Network (GAN). The proposed framework\nprocesses the received waveform and decouples speech and noise signals,\nwhich are fed into two short-time Fourier transform (STFT) convolution\n1-D layers that map the waveforms to spectrograms in the complex domain.\nThese speech and noise spectrograms are then used to compute the speech\nmask loss. The proposed method is evaluated using the TIMIT data set\nfor seen and unseen signal-to-noise ratio conditions. It is shown that\nthe proposed method outperforms the speech enhancement methods that\nuse Deep Neural Network (DNN) based speech enhancement or a Speech\nEnhancement Generative Adversarial Network (SEGAN).\n"
      ],
      "doi": "10.21437/Interspeech.2020-1946",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "defossez20_interspeech": {
      "authors": [
        [
          "Alexandre",
          "D\u00e9fossez"
        ],
        [
          "Gabriel",
          "Synnaeve"
        ],
        [
          "Yossi",
          "Adi"
        ]
      ],
      "title": "Real Time Speech Enhancement in the Waveform Domain",
      "original": "2409",
      "page_count": 5,
      "order": 673,
      "p1": "3291",
      "pn": "3295",
      "abstract": [
        "We present a causal speech enhancement model working on the raw waveform\nthat runs in real-time on a laptop CPU. The proposed model is based\non an encoder-decoder architecture with skip-connections. It is optimized\non both time and frequency domains, using multiple loss functions.\nEmpirical evidence shows that it is capable of removing various kinds\nof background noise including stationary and non-stationary noises,\nas well as room reverb. Additionally, we suggest a set of data augmentation\ntechniques applied directly on the raw waveform which further improve\nmodel performance and its generalization abilities. We perform evaluations\non several standard benchmarks, both using objective metrics and human\njudgements. The proposed model matches state-of-the-art performance\nof both causal and non causal methods while working directly on the\nraw waveform.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2409",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "romaniuk20_interspeech": {
      "authors": [
        [
          "Michal",
          "Romaniuk"
        ],
        [
          "Piotr",
          "Masztalski"
        ],
        [
          "Karol",
          "Piaskowski"
        ],
        [
          "Mateusz",
          "Matuszewski"
        ]
      ],
      "title": "Efficient Low-Latency Speech Enhancement with Mobile Audio Streaming Networks",
      "original": "2443",
      "page_count": 5,
      "order": 674,
      "p1": "3296",
      "pn": "3300",
      "abstract": [
        "We propose Mobile Audio Streaming Networks (MASnet) for efficient low-latency\nspeech enhancement, which is particularly suitable for mobile devices\nand other applications where computational capacity is a limitation.\nMASnet processes linear-scale spectrograms, transforming successive\nnoisy frames into complex-valued ratio masks which are then applied\nto the respective noisy frames. MASnet can operate in a low-latency\nincremental inference mode which matches the complexity of layer-by-layer\nbatch mode. Compared to a similar fully-convolutional architecture,\nMASnet incorporates depthwise and pointwise convolutions for a large\nreduction in fused multiply-accumulate operations per second (FMA/s),\nat the cost of some reduction in SNR.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2443",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "chiba20_interspeech": {
      "authors": [
        [
          "Yuya",
          "Chiba"
        ],
        [
          "Takashi",
          "Nose"
        ],
        [
          "Akinori",
          "Ito"
        ]
      ],
      "title": "Multi-Stream Attention-Based BLSTM with Feature Segmentation for Speech Emotion Recognition",
      "original": "1199",
      "page_count": 5,
      "order": 675,
      "p1": "3301",
      "pn": "3305",
      "abstract": [
        "This paper proposes a speech emotion recognition technique that considers\nthe suprasegmental characteristics and temporal change of individual\nspeech parameters. In recent years, speech emotion recognition using\nBidirectional LSTM (BLSTM) has been studied actively because the model\ncan focus on a particular temporal region that contains strong emotional\ncharacteristics. One of the model&#8217;s weaknesses is that it cannot\nconsider the statistics of speech features, which are known to be effective\nfor speech emotion recognition. Besides, this method cannot train individual\nattention parameters for different descriptors because it handles the\ninput sequence by a single BLSTM. In this paper, we introduce feature\nsegmentation and multi-stream processing into attention-based BLSTM\nto solve these problems. In addition, we employed data augmentation\nbased on emotional speech synthesis in a training step. The classification\nexperiments between four emotions (i.e., anger, joy, neutral, and sadness)\nusing the Japanese Twitter-based Emotional Speech corpus (JTES) showed\nthat the proposed method obtained a recognition accuracy of 73.4%,\nwhich is comparable to human evaluation (75.5%).\n"
      ],
      "doi": "10.21437/Interspeech.2020-1199",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "li20da_interspeech": {
      "authors": [
        [
          "Guanjun",
          "Li"
        ],
        [
          "Shan",
          "Liang"
        ],
        [
          "Shuai",
          "Nie"
        ],
        [
          "Wenju",
          "Liu"
        ],
        [
          "Zhanlei",
          "Yang"
        ],
        [
          "Longshuai",
          "Xiao"
        ]
      ],
      "title": "Microphone Array Post-Filter for Target Speech Enhancement Without a Prior Information of Point Interferers",
      "original": "1351",
      "page_count": 5,
      "order": 676,
      "p1": "3306",
      "pn": "3310",
      "abstract": [
        "The post-filter for microphone array speech enhancement can effectively\nsuppress noise including point interferers. However, the suppression\nof point interferers relies on the accurate estimation of the number\nand directions of point interferers, which is a difficult task in practical\nsituations. In this paper, we propose a post-filtering algorithm, which\nis independent of the number and directions of point interferers. Specifically,\nwe assume that the point interferers are continuously distributed at\neach direction of the plane but the probability of the interferer occurring\nat each direction is different in order to calculate the spatial covariance\nmatrix of the point interferers. Moreover, we assume that the noise\nis additive and uncorrelated with the target signal to obtain the power\nspectral densities (PSDs) of the target signal and noise. Finally,\nthe proposed post-filter is calculated using the estimated PSDs. Experimental\nresults prove that the proposed post-filtering algorithm is superior\nto the comparative algorithms in the scenarios where the number and\ndirections of point interferers are not accurately estimated.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1351",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "hiroe20_interspeech": {
      "authors": [
        [
          "Atsuo",
          "Hiroe"
        ]
      ],
      "title": "Similarity-and-Independence-Aware Beamformer: Method for Target Source Extraction Using Magnitude Spectrogram as Reference",
      "original": "1365",
      "page_count": 5,
      "order": 677,
      "p1": "3311",
      "pn": "3315",
      "abstract": [
        "This study presents a novel method for source extraction, referred\nto as the similarity-and-independence-aware beamformer (SIBF). The\nSIBF extracts the target signal using a rough magnitude spectrogram\nas the reference signal. The advantage of the SIBF is that it can obtain\nan accurate target signal, compared to the spectrogram generated by\ntarget-enhancing methods such as the speech enhancement based on deep\nneural networks (DNNs). For the extraction, we extend the framework\nof the deflationary independent component analysis, by considering\nthe similarity between the reference and extracted target, as well\nas the mutual independence of all potential sources. To solve the extraction\nproblem by maximum-likelihood estimation, we introduce two source model\ntypes that can reflect the similarity. The experimental results from\nthe CHiME3 dataset show that the target signal extracted by the SIBF\nis more accurate than the reference signal generated by the DNN.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1365",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "golokolenko20_interspeech": {
      "authors": [
        [
          "Oleg",
          "Golokolenko"
        ],
        [
          "Gerald",
          "Schuller"
        ]
      ],
      "title": "The Method of Random Directions Optimization for Stereo Audio Source Separation",
      "original": "1409",
      "page_count": 5,
      "order": 678,
      "p1": "3316",
      "pn": "3320",
      "abstract": [
        "In this paper, a novel fast time domain audio source separation technique\nbased on fractional delay filters with low computational complexity\nand small algorithmic delay is presented and evaluated in experiments.\nOur goal is a Blind Source Separation (BSS) technique, which can be\napplicable for the low cost and low power devices where processing\nis done in real-time, e.g. hearing aids or teleconferencing setups.\nThe proposed approach optimizes fractional delays implemented as IIR\nfilters and attenuation factors between microphone signals to minimize\ncrosstalk, the principle of a fractional delay and sum beamformer.\nThe experiments have been carried out for offline separation with stationary\nsound sources and for real-time with randomly moving sound sources.\nExperimental results show that separation performance of the proposed\ntime domain BSS technique is competitive with State-of-the-Art (SoA)\napproaches but has lower computational complexity and no system delay\nlike in frequency domain BSS.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1409",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "fan20b_interspeech": {
      "authors": [
        [
          "Cunhang",
          "Fan"
        ],
        [
          "Jianhua",
          "Tao"
        ],
        [
          "Bin",
          "Liu"
        ],
        [
          "Jiangyan",
          "Yi"
        ],
        [
          "Zhengqi",
          "Wen"
        ]
      ],
      "title": "Gated Recurrent Fusion of Spatial and Spectral Features for Multi-Channel Speech Separation with Deep Embedding Representations",
      "original": "1548",
      "page_count": 5,
      "order": 679,
      "p1": "3321",
      "pn": "3325",
      "abstract": [
        "Multi-channel deep clustering (MDC) has acquired a good performance\nfor speech separation. However, MDC only applies the spatial features\nas the additional information, which does not fuse them with the spectral\nfeatures very well. So it is difficult to learn mutual relationship\nbetween spatial and spectral features. Besides, the training objective\nof MDC is defined at embedding vectors, rather than real separated\nsources, which may damage the separation performance. In this work,\nwe deal with spatial and spectral features as two different modalities.\nWe propose the gated recurrent fusion (GRF) method to adaptively select\nand fuse the relevant information from spectral and spatial features\nby making use of the gate and memory modules. In addition, to solve\nthe training objective problem of MDC, the real separated sources are\nused as the training objectives. Specifically, we apply the deep clustering\nnetwork to extract deep embedding features. Instead of using the unsupervised\nK-means clustering to estimate binary masks, another supervised network\nis utilized to learn soft masks from these deep embedding features.\nOur experiments are conducted on a spatialized reverberant version\nof WSJ0-2mix dataset. Experimental results show that the proposed method\noutperforms MDC baseline and even better than the oracle ideal binary\nmask (IBM).\n"
      ],
      "doi": "10.21437/Interspeech.2020-1548",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "scheibler20_interspeech": {
      "authors": [
        [
          "Robin",
          "Scheibler"
        ]
      ],
      "title": "Generalized Minimal Distortion Principle for Blind Source Separation",
      "original": "2158",
      "page_count": 5,
      "order": 680,
      "p1": "3326",
      "pn": "3330",
      "abstract": [
        "We revisit the source image estimation problem from blind source separation\n(BSS). We generalize the traditional minimum distortion principle to\nmaximum likelihood estimation with a model for the residual spectrograms.\nBecause residual spectrograms typically contain other sources, we propose\nto use a mixed-norm model that lets us finely tune sparsity in time\nand frequency. We propose to carry out the minimization of the mixed-norm\nvia majorization-minimization optimization, leading to an iteratively\nreweighted least-squares algorithm. The algorithm balances well efficiency\nand ease of implementation. We assess the performance of the proposed\nmethod as applied to two well-known determined BSS and one joint BSS-dereverberation\nalgorithms. We find out that it is possible to tune the parameters\nto improve separation by up to 2 dB, with no increase in distortion,\nand at little computational cost. The method thus provides a cheap\nand easy way to boost the performance of blind source separation.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2158",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "zhong20b_interspeech": {
      "authors": [
        [
          "Ying",
          "Zhong"
        ],
        [
          "Ying",
          "Hu"
        ],
        [
          "Hao",
          "Huang"
        ],
        [
          "Wushour",
          "Silamu"
        ]
      ],
      "title": "A Lightweight Model Based on Separable Convolution for Speech Emotion Recognition",
      "original": "2408",
      "page_count": 5,
      "order": 681,
      "p1": "3331",
      "pn": "3335",
      "abstract": [
        "One of the major challenges in Speech Emotion Recognition (SER) is\nto build a lightweight model with limited training data. In this paper,\nwe propose a lightweight architecture with only fewer parameters which\nis based on separable convolution and inverted residuals. Speech samples\nare often annotated by multiple raters. While some sentences with clear\nemotional content are consistently annotated (easy samples), sentences\nwith ambiguous emotional content present important disagreement between\nindividual evaluations (hard samples). We assumed that samples hard\nfor humans are also hard for computers. We address the problem by using\nfocal loss, which focus on learning hard samples and down-weight easy\nsamples. By combining attention mechanism, our proposed network can\nenhance the importing of emotion-salient information. Our proposed\nmodel achieves 71.72% and 90.1% of unweighted accuracy (UA) on the\nwell-known corpora IEMOCAP and Emo-DB respectively. Comparing with\nthe current model having fewest parameters as we know, its model size\nis almost 5 times of our proposed model.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2408",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "cai20b_interspeech": {
      "authors": [
        [
          "Ruichu",
          "Cai"
        ],
        [
          "Kaibin",
          "Guo"
        ],
        [
          "Boyan",
          "Xu"
        ],
        [
          "Xiaoyan",
          "Yang"
        ],
        [
          "Zhenjie",
          "Zhang"
        ]
      ],
      "title": "Meta Multi-Task Learning for Speech Emotion Recognition",
      "original": "2624",
      "page_count": 5,
      "order": 682,
      "p1": "3336",
      "pn": "3340",
      "abstract": [
        "Most existing Speech Emotion Recognition (SER) approaches ignore the\nrelationship between the categorical emotional labels and the dimensional\nlabels in valence, activation or dominance space. Although multi-task\nlearning has recently been introduced to explore such auxiliary tasks\nof SER, existing approaches only share the feature extractor under\nthe traditional multi-task learning framework and can not efficiently\ntransfer the knowledge from the auxiliary tasks to the target task.\nIn order to address these issues, we propose a Meta Multi-task Learning\nmethod for SER by combining the multi-task learning with meta learning.\nOur contributions include: 1) to model the relationship among auxiliary\ntasks, we extend the task generation of meta learning to the form of\nmultiple tasks, and 2) to transfer the knowledge from the auxiliary\ntasks to the target task, we propose a tuning-based transfer training\nmechanism in the meta learning framework. The experiments on IEMOCAP\nshow that our approach outperforms the state-of-the-art solution (UA:\n70.32%, WA: 76.64%).\n"
      ],
      "doi": "10.21437/Interspeech.2020-2624",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "grondin20_interspeech": {
      "authors": [
        [
          "Fran\u00e7ois",
          "Grondin"
        ],
        [
          "Jean-Samuel",
          "Lauzon"
        ],
        [
          "Jonathan",
          "Vincent"
        ],
        [
          "Fran\u00e7ois",
          "Michaud"
        ]
      ],
      "title": "GEV Beamforming Supported by DOA-Based Masks Generated on Pairs of Microphones",
      "original": "2687",
      "page_count": 5,
      "order": 683,
      "p1": "3341",
      "pn": "3345",
      "abstract": [
        "Distant speech processing is a challenging task, especially when dealing\nwith the cocktail party effect. Sound source separation is thus often\nrequired as a preprocessing step prior to speech recognition to improve\nthe signal to distortion ratio (SDR). Recently, a combination of beamforming\nand speech separation networks have been proposed to improve the target\nsource quality in the direction of arrival of interest. However, with\nthis type of approach, the neural network needs to be trained in advance\nfor a specific microphone array geometry, which limits versatility\nwhen adding/removing microphones, or changing the shape of the array.\nThe solution presented in this paper is to train a neural network on\npairs of microphones with different spacing and acoustic environmental\nconditions, and then use this network to estimate a time-frequency\nmask from all the pairs of microphones forming the array with an arbitrary\nshape. Using this mask, the target and noise covariance matrices can\nbe estimated, and then used to perform generalized eigenvalue (GEV)\nbeamforming. Results show that the proposed approach improves the SDR\nfrom 4.78 dB to 7.69 dB on average, for various microphone array geometries\nthat correspond to commercially available hardware.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2687",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "jose20_interspeech": {
      "authors": [
        [
          "Christin",
          "Jose"
        ],
        [
          "Yuriy",
          "Mishchenko"
        ],
        [
          "Thibaud",
          "S\u00e9n\u00e9chal"
        ],
        [
          "Anish",
          "Shah"
        ],
        [
          "Alex",
          "Escott"
        ],
        [
          "Shiv Naga Prasad",
          "Vitaladevuni"
        ]
      ],
      "title": "Accurate Detection of Wake Word Start and End Using a CNN",
      "original": "1491",
      "page_count": 5,
      "order": 684,
      "p1": "3346",
      "pn": "3350",
      "abstract": [
        "Small footprint embedded devices require keyword spotters (KWS) with\nsmall model size and detection latency for enabling voice assistants.\nSuch a keyword is often referred to as  wake word as it is used to\nwake up voice assistant enabled devices. Together with wake word detection,\naccurate estimation of wake word endpoints (start and end) is an important\ntask of KWS. In this paper, we propose two new methods for detecting\nthe endpoints of wake words in neural KWS that use single-stage word-level\nneural networks. Our results show that the new techniques give superior\naccuracy for detecting wake words&#8217; endpoints of up to 50 msec\nstandard error versus human annotations, on par with the conventional\nAcoustic Model plus HMM forced alignment. To our knowledge, this is\nthe first study of wake word endpoints detection methods for single-stage\nneural KWS.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1491",
      "author_area_id": "9",
      "author_area_label": "Speech Recognition: Architecture, Search, and Linguistic Components"
    },
    "adya20_interspeech": {
      "authors": [
        [
          "Saurabh",
          "Adya"
        ],
        [
          "Vineet",
          "Garg"
        ],
        [
          "Siddharth",
          "Sigtia"
        ],
        [
          "Pramod",
          "Simha"
        ],
        [
          "Chandra",
          "Dhir"
        ]
      ],
      "title": "Hybrid Transformer/CTC Networks for Hardware Efficient Voice Triggering",
      "original": "1330",
      "page_count": 5,
      "order": 685,
      "p1": "3351",
      "pn": "3355",
      "abstract": [
        "We consider the design of two-pass voice trigger detection systems.\nWe focus on the networks in the second pass that are used to re-score\ncandidate segments obtained from the first-pass. Our baseline is an\nacoustic model(AM), with BiLSTM layers, trained by minimizing the CTC\nloss. We replace the BiLSTM layers with self-attention layers. Results\non internal evaluation sets show that self-attention networks yield\nbetter accuracy while requiring fewer parameters. We add an auto-regressive\ndecoder network on top of the self-attention layers and jointly minimize\nthe CTC loss on the encoder and the cross-entropy loss on the decoder.\nThis design yields further improvements over the baseline. We retrain\nall the models above in a multi-task learning(MTL) setting, where one\nbranch of a shared network is trained as an AM, while the second branch\nclassifies the whole sequence to be true-trigger or not. Results demonstrate\nthat networks with self-attention layers yield &#126;60% relative reduction\nin false reject rates for a given false-alarm rate, while requiring\n10% fewer parameters. When trained in the MTL setup, self-attention\nnetworks yield further accuracy improvements. On-device measurements\nshow that we observe 70% relative reduction in inference time. Additionally,\nthe proposed network architectures are &#126;5&#215; faster to train.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1330",
      "author_area_id": "9",
      "author_area_label": "Speech Recognition: Architecture, Search, and Linguistic Components"
    },
    "majumdar20_interspeech": {
      "authors": [
        [
          "Somshubra",
          "Majumdar"
        ],
        [
          "Boris",
          "Ginsburg"
        ]
      ],
      "title": "MatchboxNet: 1D Time-Channel Separable Convolutional Neural Network Architecture for Speech Commands Recognition",
      "original": "1058",
      "page_count": 5,
      "order": 686,
      "p1": "3356",
      "pn": "3360",
      "abstract": [
        "We present  MatchboxNet &#8212; an end-to-end neural network for speech\ncommand recognition. MatchboxNet is a deep residual network composed\nfrom blocks of 1D time-channel separable convolution, batch-normalization,\nReLU and dropout layers. MatchboxNet reaches state-of-the art accuracy\non the Google Speech Commands dataset while having significantly fewer\nparameters than similar models. The small footprint of MatchboxNet\nmakes it an attractive candidate for devices with limited computational\nresources. The model is highly scalable, so model accuracy can be improved\nwith modest additional memory and compute. Finally, we show how intensive\ndata augmentation using an auxiliary noise dataset improves robustness\nin the presence of background noise.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1058",
      "author_area_id": "9",
      "author_area_label": "Speech Recognition: Architecture, Search, and Linguistic Components"
    },
    "mehrotra20_interspeech": {
      "authors": [
        [
          "Abhinav",
          "Mehrotra"
        ],
        [
          "\u0141ukasz",
          "Dudziak"
        ],
        [
          "Jinsu",
          "Yeo"
        ],
        [
          "Young-yoon",
          "Lee"
        ],
        [
          "Ravichander",
          "Vipperla"
        ],
        [
          "Mohamed S.",
          "Abdelfattah"
        ],
        [
          "Sourav",
          "Bhattacharya"
        ],
        [
          "Samin",
          "Ishtiaq"
        ],
        [
          "Alberto Gil C.P.",
          "Ramos"
        ],
        [
          "SangJeong",
          "Lee"
        ],
        [
          "Daehyun",
          "Kim"
        ],
        [
          "Nicholas D.",
          "Lane"
        ]
      ],
      "title": "Iterative Compression of End-to-End ASR Model Using AutoML",
      "original": "1894",
      "page_count": 5,
      "order": 687,
      "p1": "3361",
      "pn": "3365",
      "abstract": [
        "Increasing demand for on-device Automatic Speech Recognition (ASR)\nsystems has resulted in renewed interests in developing automatic model\ncompression techniques. Past research have shown that AutoML-based\nLow Rank Factorization (LRF) technique, when applied to an end-to-end\nEncoder-Attention-Decoder style ASR model, can achieve a speedup of\nup to 3.7&#215;, outperforming laborious manual rank-selection approaches.\nHowever, we show that current AutoML-based search techniques only work\nup to a certain compression level, beyond which they fail to produce\ncompressed models with acceptable word error rates (WER). In this work,\nwe propose an iterative AutoML-based LRF approach that achieves over\n5&#215; compression without degrading the WER, thereby advancing the\nstate-of-the-art in ASR compression.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1894",
      "author_area_id": "9",
      "author_area_label": "Speech Recognition: Architecture, Search, and Linguistic Components"
    },
    "nguyen20c_interspeech": {
      "authors": [
        [
          "Hieu Duy",
          "Nguyen"
        ],
        [
          "Anastasios",
          "Alexandridis"
        ],
        [
          "Athanasios",
          "Mouchtaris"
        ]
      ],
      "title": "Quantization Aware Training with Absolute-Cosine Regularization for Automatic Speech Recognition",
      "original": "1991",
      "page_count": 5,
      "order": 688,
      "p1": "3366",
      "pn": "3370",
      "abstract": [
        "Compression and quantization is important to neural networks in general\nand Automatic Speech Recognition (ASR) systems in particular, especially\nwhen they operate in real-time on resource-constrained devices. By\nusing fewer number of bits for the model weights, the model size becomes\nmuch smaller while inference time is reduced significantly, with the\ncost of degraded performance. Such degradation can be potentially addressed\nby the so-called quantization-aware training (QAT). Existing QATs mostly\ntake into account the quantization in forward propagation, while ignoring\nthe quantization loss in gradient calculation during back-propagation.\nIn this work, we introduce a novel QAT scheme based on absolute-cosine\nregularization (ACosR), which enforces a prior, quantization-friendly\ndistribution to the model weights. We apply this novel approach into\nASR task assuming a recurrent neural network transducer (RNN-T) architecture.\nThe results show that there is zero to little degradation between floating-point,\n8-bit, and 6-bit ACosR models. Weight distributions further confirm\nthat in-training weights are very close to quantization levels when\nACosR is applied.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1991",
      "author_area_id": "9",
      "author_area_label": "Speech Recognition: Architecture, Search, and Linguistic Components"
    },
    "garg20b_interspeech": {
      "authors": [
        [
          "Abhinav",
          "Garg"
        ],
        [
          "Gowtham P.",
          "Vadisetti"
        ],
        [
          "Dhananjaya",
          "Gowda"
        ],
        [
          "Sichen",
          "Jin"
        ],
        [
          "Aditya",
          "Jayasimha"
        ],
        [
          "Youngho",
          "Han"
        ],
        [
          "Jiyeon",
          "Kim"
        ],
        [
          "Junmo",
          "Park"
        ],
        [
          "Kwangyoun",
          "Kim"
        ],
        [
          "Sooyeon",
          "Kim"
        ],
        [
          "Young-yoon",
          "Lee"
        ],
        [
          "Kyungbo",
          "Min"
        ],
        [
          "Chanwoo",
          "Kim"
        ]
      ],
      "title": "Streaming On-Device End-to-End ASR System for Privacy-Sensitive Voice-Typing",
      "original": "3172",
      "page_count": 5,
      "order": 689,
      "p1": "3371",
      "pn": "3375",
      "abstract": [
        "In this paper, we present our streaming on-device end-to-end speech\nrecognition solution for a privacy sensitive voice-typing application\nwhich primarily involves typing user private details and passwords.\nWe highlight challenges specific to voice-typing scenario in the Korean\nlanguage and propose solutions to these problems within the framework\nof a streaming attention-based speech recognition system. Some important\nchallenges in voice-typing are the choice of output units, coupling\nof multiple characters into longer byte-pair encoded units, lack of\nsufficient training data. Apart from customizing a high accuracy open\ndomain streaming speech recognition model for voice-typing applications,\nwe retain the performance of the model for open domain tasks without\nsignificant degradation. We also explore domain biasing using a shallow\nfusion with a weighted finite state transducer (WFST). We obtain approximately\n13% relative word error rate (WER) improvement on our internal Korean\nvoice-typing dataset without a WFST and about 30% additional WER improvement\nwith a WFST fusion.\n"
      ],
      "doi": "10.21437/Interspeech.2020-3172",
      "author_area_id": "9",
      "author_area_label": "Speech Recognition: Architecture, Search, and Linguistic Components"
    },
    "pratap20b_interspeech": {
      "authors": [
        [
          "Vineel",
          "Pratap"
        ],
        [
          "Qiantong",
          "Xu"
        ],
        [
          "Jacob",
          "Kahn"
        ],
        [
          "Gilad",
          "Avidov"
        ],
        [
          "Tatiana",
          "Likhomanenko"
        ],
        [
          "Awni",
          "Hannun"
        ],
        [
          "Vitaliy",
          "Liptchinsky"
        ],
        [
          "Gabriel",
          "Synnaeve"
        ],
        [
          "Ronan",
          "Collobert"
        ]
      ],
      "title": "Scaling Up Online Speech Recognition Using ConvNets",
      "original": "2840",
      "page_count": 5,
      "order": 690,
      "p1": "3376",
      "pn": "3380",
      "abstract": [
        "We design an online end-to-end speech recognition system based on Time-Depth\nSeparable (TDS) convolutions and Connectionist Temporal Classification\n(CTC). We improve the core TDS architecture in order to limit the future\ncontext and hence reduce latency while maintaining accuracy. The system\nhas almost three times the throughput of a well tuned hybrid ASR baseline\nwhile also having lower latency and a better word error rate. Also\nimportant to the efficiency of the recognizer is our highly optimized\nbeam search decoder. To show the impact of our design choices, we analyze\nthroughput, latency, accuracy, and discuss how these metrics can be\ntuned based on the user requirements.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2840",
      "author_area_id": "9",
      "author_area_label": "Speech Recognition: Architecture, Search, and Linguistic Components"
    },
    "bai20_interspeech": {
      "authors": [
        [
          "Ye",
          "Bai"
        ],
        [
          "Jiangyan",
          "Yi"
        ],
        [
          "Jianhua",
          "Tao"
        ],
        [
          "Zhengkun",
          "Tian"
        ],
        [
          "Zhengqi",
          "Wen"
        ],
        [
          "Shuai",
          "Zhang"
        ]
      ],
      "title": "Listen Attentively, and Spell Once: Whole Sentence Generation via a Non-Autoregressive Architecture for Low-Latency Speech Recognition",
      "original": "1600",
      "page_count": 5,
      "order": 691,
      "p1": "3381",
      "pn": "3385",
      "abstract": [
        "Although attention based end-to-end models have achieved promising\nperformance in speech recognition, the multi-pass forward computation\nin beam-search increases inference time cost, which limits their practical\napplications. To address this issue, we propose a non-autoregressive\nend-to-end speech recognition system called LASO (listen attentively,\nand spell once). Because of the non-autoregressive property, LASO predicts\na textual token in the sequence without the dependence on other tokens.\nWithout beam-search, the one-pass propagation much reduces inference\ntime cost of LASO. And because the model is based on the attention\nbased feedforward structure, the computation can be implemented in\nparallel efficiently. We conduct experiments on publicly available\nChinese dataset AISHELL-1. LASO achieves a character error rate of\n6.4%, which outperforms the state-of-the-art autoregressive transformer\nmodel (6.7%). The average inference latency is 21 ms, which is 1/50\nof the autoregressive transformer model.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1600",
      "author_area_id": "9",
      "author_area_label": "Speech Recognition: Architecture, Search, and Linguistic Components"
    },
    "strimel20_interspeech": {
      "authors": [
        [
          "Grant P.",
          "Strimel"
        ],
        [
          "Ariya",
          "Rastrow"
        ],
        [
          "Gautam",
          "Tiwari"
        ],
        [
          "Adrien",
          "Pi\u00e9rard"
        ],
        [
          "Jon",
          "Webb"
        ]
      ],
      "title": "Rescore in a Flash: Compact, Cache Efficient Hashing Data Structures for n-Gram Language Models",
      "original": "1939",
      "page_count": 5,
      "order": 692,
      "p1": "3386",
      "pn": "3390",
      "abstract": [
        "We introduce DashHashLM, an efficient data structure that stores an\nn-gram language model compactly while making minimal trade-offs on\nruntime lookup latency. The data structure implements a finite state\ntransducer with a lossless structural compression and outperforms comparable\nimplementations when considering lookup speed in the small-footprint\nsetting. DashHashLM introduces several optimizations to language model\ncompression which are designed to minimize expected memory accesses.\nWe also present variations of DashHashLM appropriate for scenarios\nwith different memory and latency constraints. We detail the algorithm\nand justify our design choices with comparative experiments on a speech\nrecognition task. Specifically, we show that with roughly a 10% increase\nin memory size, compared to a highly optimized, compressed baseline\nn-gram representation, our proposed data structure can achieve up to\na 6&#215; query speedup.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1939",
      "author_area_id": "9",
      "author_area_label": "Speech Recognition: Architecture, Search, and Linguistic Components"
    },
    "shankar20b_interspeech": {
      "authors": [
        [
          "Ravi",
          "Shankar"
        ],
        [
          "Hsi-Wei",
          "Hsieh"
        ],
        [
          "Nicolas",
          "Charon"
        ],
        [
          "Archana",
          "Venkataraman"
        ]
      ],
      "title": "Multi-Speaker Emotion Conversion via Latent Variable Regularization and a Chained Encoder-Decoder-Predictor Network",
      "original": "1323",
      "page_count": 5,
      "order": 693,
      "p1": "3391",
      "pn": "3395",
      "abstract": [
        "We propose a novel method for emotion conversion in speech based on\na chained encoder-decoder-predictor neural network architecture. The\nencoder constructs a latent embedding of the fundamental frequency\n(F0) contour and the spectrum, which we regularize using the Large\nDiffeomorphic Metric Mapping (LDDMM) registration framework. The decoder\nuses this embedding to predict the modified F0 contour in a target\nemotional class. Finally, the predictor uses the original spectrum\nand the modified F0 contour to generate a corresponding target spectrum.\nOur joint objective function simultaneously optimizes the parameters\nof three model blocks. We show that our method outperforms the existing\nstate-of-the-art approaches on both, the saliency of emotion conversion\nand the quality of resynthesized speech. In addition, the LDDMM regularization\nallows our model to convert phrases that were not present in training,\nthus providing evidence for out-of-sample generalization.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1323",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "shankar20c_interspeech": {
      "authors": [
        [
          "Ravi",
          "Shankar"
        ],
        [
          "Jacob",
          "Sager"
        ],
        [
          "Archana",
          "Venkataraman"
        ]
      ],
      "title": "Non-Parallel Emotion Conversion Using a Deep-Generative Hybrid Network and an Adversarial Pair Discriminator",
      "original": "1325",
      "page_count": 5,
      "order": 694,
      "p1": "3396",
      "pn": "3400",
      "abstract": [
        "We introduce a novel method for emotion conversion in speech that does\nnot require parallel training data. Our approach loosely relies on\na cycle-GAN schema to minimize the reconstruction error from converting\nback and forth between emotion pairs. However, unlike the conventional\ncycle-GAN, our discriminator classifies whether a pair of input real\nand generated samples corresponds to the desired emotion conversion\n(e.g., A&#8594;B) or to its inverse (B&#8594;A). We will show that\nthis setup, which we refer to as a variational cycle-GAN (VCGAN), is\nequivalent to minimizing the empirical KL divergence between the source\nfeatures and their cyclic counterpart. In addition, our generator combines\na trainable deep network with a fixed generative block to implement\na smooth and invertible transformation on the input features, in our\ncase, the fundamental frequency (F0) contour. This hybrid architecture\nregularizes our adversarial training procedure. We use crowd sourcing\nto evaluate both the emotional saliency and the quality of synthesized\nspeech. Finally, we show that our model generalizes to new speakers\nby modifying speech produced by Wavenet.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1325",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "tits20b_interspeech": {
      "authors": [
        [
          "No\u00e9",
          "Tits"
        ],
        [
          "Kevin El",
          "Haddad"
        ],
        [
          "Thierry",
          "Dutoit"
        ]
      ],
      "title": "Laughter Synthesis: Combining Seq2seq Modeling with Transfer Learning",
      "original": "1423",
      "page_count": 5,
      "order": 695,
      "p1": "3401",
      "pn": "3405",
      "abstract": [
        "Despite the growing interest for expressive speech synthesis, synthesis\nof nonverbal expressions is an under-explored area. In this paper we\npropose an audio laughter synthesis system based on a sequence-to-sequence\nTTS synthesis system. We leverage transfer learning by training a deep\nlearning model to learn to generate both speech and laughs from annotations.\nWe evaluate our model with a listening test, comparing its performance\nto an HMM-based laughter synthesis one and assess that it reaches higher\nperceived naturalness. Our solution is a first step towards a TTS system\nthat would be able to synthesize speech with a control on amusement\nlevel with laughter integration.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1423",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "cao20b_interspeech": {
      "authors": [
        [
          "Yuexin",
          "Cao"
        ],
        [
          "Zhengchen",
          "Liu"
        ],
        [
          "Minchuan",
          "Chen"
        ],
        [
          "Jun",
          "Ma"
        ],
        [
          "Shaojun",
          "Wang"
        ],
        [
          "Jing",
          "Xiao"
        ]
      ],
      "title": "Nonparallel Emotional Speech Conversion Using VAE-GAN",
      "original": "1647",
      "page_count": 5,
      "order": 696,
      "p1": "3406",
      "pn": "3410",
      "abstract": [
        "This paper proposes a nonparallel emotional speech conversion (ESC)\nmethod based on Variational AutoEncoder-Generative Adversarial Network\n(VAE-GAN). Emotional speech conversion aims at transforming speech\nfrom one source emotion to that of a target emotion without changing\nthe speaker&#8217;s identity and linguistic content. In this work,\nan encoder is trained to elicit the content-related representations\nfrom acoustic features. Emotion-related representations are extracted\nin a supervised manner. Then the transformation between emotion-related\nrepresentations from different domains is learned using an improved\ncycle-consistent Generative Adversarial Network (CycleGAN). Finally,\nemotion conversion is performed by eliciting and recombining the content-related\nrepresentations of the source speech and the emotion-related representations\nof the target emotion. Subjective evaluation experiments are conducted\nand the results show that the proposed method outperforms the baseline\nin terms of voice quality and emotion conversion ability.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1647",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "sorin20_interspeech": {
      "authors": [
        [
          "Alexander",
          "Sorin"
        ],
        [
          "Slava",
          "Shechtman"
        ],
        [
          "Ron",
          "Hoory"
        ]
      ],
      "title": "Principal Style Components: Expressive Style Control and Cross-Speaker Transfer in Neural TTS",
      "original": "1854",
      "page_count": 5,
      "order": 697,
      "p1": "3411",
      "pn": "3415",
      "abstract": [
        "We propose a novel semi-supervised technique that enables expressive\nstyle control and cross-speaker transfer in neural text to speech (TTS),\nwhen available training data contains a limited amount of labeled expressive\nspeech from a single speaker. The technique is based on unsupervised\nlearning of a style-related latent space, generated by a previously\nproposed reference audio encoding technique, and transforming it by\nmeans of Principal Component Analysis to another low-dimensional space.\nThe latter space represents style information in a purified form, disentangled\nfrom text and speaker-related information. Encodings for expressive\nstyles that are present in the training data are easily constructed\nin this space. Furthermore, this technique provides control over the\nspeech rate, pitch level, and articulation type that can be used for\nTTS voice transformation. <br style=\"mso-data-placement:same-cell;\"\n/><br style=\"mso-data-placement:same-cell;\" />\n  We present the results\nof subjective crowd evaluations confirming that the synthesized speech\nconvincingly conveys the desired expressive styles and preserves a\nhigh level of quality.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1854",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "zhou20d_interspeech": {
      "authors": [
        [
          "Kun",
          "Zhou"
        ],
        [
          "Berrak",
          "Sisman"
        ],
        [
          "Mingyang",
          "Zhang"
        ],
        [
          "Haizhou",
          "Li"
        ]
      ],
      "title": "Converting Anyone&#8217;s Emotion: Towards Speaker-Independent Emotional Voice Conversion",
      "original": "2014",
      "page_count": 5,
      "order": 698,
      "p1": "3416",
      "pn": "3420",
      "abstract": [
        "Emotional voice conversion aims to convert the emotion of speech from\none state to another while preserving the linguistic content and speaker\nidentity. The prior studies on emotional voice conversion are mostly\ncarried out under the assumption that emotion is speaker-dependent.\nWe consider that there is a common code between speakers for emotional\nexpression in a spoken language, therefore, a speaker-independent mapping\nbetween emotional states is possible. In this paper, we propose a speaker-independent\nemotional voice conversion framework, that can convert anyone&#8217;s\nemotion without the need for parallel data. We propose a VAW-GAN based\nencoder-decoder structure to learn the spectrum and prosody mapping.\nWe perform prosody conversion by using continuous wavelet transform\n(CWT) to model the temporal dependencies. We also investigate the use\nof F0 as an additional input to the decoder to improve emotion conversion\nperformance. Experiments show that the proposed speaker-independent\nframework achieves competitive results for both seen and unseen speakers.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2014",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "matsumoto20_interspeech": {
      "authors": [
        [
          "Kento",
          "Matsumoto"
        ],
        [
          "Sunao",
          "Hara"
        ],
        [
          "Masanobu",
          "Abe"
        ]
      ],
      "title": "Controlling the Strength of Emotions in Speech-Like Emotional Sound Generated by WaveNet",
      "original": "2064",
      "page_count": 5,
      "order": 699,
      "p1": "3421",
      "pn": "3425",
      "abstract": [
        "This paper proposes a method to enhance the controllability of a Speech-like\nEmotional Sound (SES). In our previous study, we proposed an algorithm\nto generate SES by employing WaveNet as a sound generator and confirmed\nthat SES can successfully convey emotional information. The proposed\nalgorithm generates SES using only emotional IDs, which results in\nhaving no linguistic information. We call the generated sounds &#8220;speech-like&#8221;\nbecause they sound as if they are uttered by human beings although\nthey contain no linguistic information. We could synthesize natural\nsounding acoustic signals that are fairly different from vocoder sounds\nto make the best use of WaveNet. To flexibly control the strength of\nemotions, this paper proposes to use a state of voiced, unvoiced, and\nsilence (VUS) as auxiliary features. Three types of emotional speech,\nnamely, neutral, angry, and happy, were generated and subjectively\nevaluated. Experimental results reveal the following: (1) VUS can control\nthe strength of SES by changing the durations of VUS states, (2) VUS\nwith narrow F0 distribution can express stronger emotions than that\nwith wide F0 distribution, and (3) the smaller the unvoiced percentage\nis, the stronger the emotional impression is.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2064",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "zhang20aa_interspeech": {
      "authors": [
        [
          "Guangyan",
          "Zhang"
        ],
        [
          "Ying",
          "Qin"
        ],
        [
          "Tan",
          "Lee"
        ]
      ],
      "title": "Learning Syllable-Level Discrete Prosodic Representation for Expressive Speech Generation",
      "original": "2228",
      "page_count": 5,
      "order": 700,
      "p1": "3426",
      "pn": "3430",
      "abstract": [
        "This paper presents an extension of the Tacotron 2 end-to-end speech\nsynthesis architecture, which aims to learn syllable-level discrete\nprosodic representations from speech data. The learned representations\ncan be used for transferring or controlling prosody in expressive speech\ngeneration. The proposed design starts with a syllable-level text encoder\nthat encodes input text at syllable level instead of phoneme level.\nThe continuous prosodic representation for each syllable is then extracted.\nA Vector-Quantised Variational Auto-Encoder (VQ-VAE) is used to discretize\nthe learned continuous prosodic representations. The discrete representations\nare finally concatenated with text encoder output to achieve prosody\ntransfer or control. Subjective evaluation is carried out on the syllable-level\nTTS system, and the effectiveness of prosody transfer. The results\nshow that the proposed Syllable-level neural TTS system produce more\nnatural speech than conventional phoneme-level TTS system. It is also\nshown that prosody transfer could be achieved and the latent prosody\ncodes are explainable with relation to specific prosody variation.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2228",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "kishida20_interspeech": {
      "authors": [
        [
          "Takuya",
          "Kishida"
        ],
        [
          "Shin",
          "Tsukamoto"
        ],
        [
          "Toru",
          "Nakashika"
        ]
      ],
      "title": "Simultaneous Conversion of Speaker Identity and Emotion Based on Multiple-Domain Adaptive RBM",
      "original": "2262",
      "page_count": 5,
      "order": 701,
      "p1": "3431",
      "pn": "3435",
      "abstract": [
        "In this paper, we propose a multiple-domain adaptive restricted Boltzmann\nmachine (MDARBM) for simultaneous conversion of speaker identity and\nemotion. This study is motivated by the assumption that representing\nmultiple domains (e.g., speaker identity, emotion, accent) of speech\nexplicitly in a single model is beneficial to reduce the effects from\nother domains when the model learns one domain&#8217;s characteristics.\nThe MDARBM decomposes the visible-hidden connections of an RBM into\ndomain-specific factors and a domain-independent factor to make it\nadaptable to multiple domains of speech. By switching the domain-specific\nfactors from the source speaker and emotion to the target ones, the\nmodel can perform a simultaneous conversion. Experimental results showed\nthat the target domain conversion task was enhanced by the other in\nthe simultaneous conversion framework. In a two-domain conversion task,\nthe MDARBM outperformed a combination of ARBMs independently trained\nwith speaker-identity and emotion units.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2262",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "yang20h_interspeech": {
      "authors": [
        [
          "Fengyu",
          "Yang"
        ],
        [
          "Shan",
          "Yang"
        ],
        [
          "Qinghua",
          "Wu"
        ],
        [
          "Yujun",
          "Wang"
        ],
        [
          "Lei",
          "Xie"
        ]
      ],
      "title": "Exploiting Deep Sentential Context for Expressive End-to-End Speech Synthesis",
      "original": "2423",
      "page_count": 5,
      "order": 702,
      "p1": "3436",
      "pn": "3440",
      "abstract": [
        "Attention-based seq2seq text-to-speech systems, especially those use\nself-attention networks (SAN), have achieved state-of-art performance.\nBut an expressive corpus with rich prosody is still challenging to\nmodel as 1) prosodic aspects, which span across different sentential\ngranularities and mainly determine acoustic expressiveness, are difficult\nto quantize and label and 2) the current seq2seq framework extracts\nprosodic information solely from a text encoder, which is easily collapsed\nto an averaged expression for expressive contents. In this paper, we\npropose a context extractor, which is built upon SAN-based text encoder,\nto sufficiently exploit the sentential context over an expressive corpus\nfor seq2seq-based TTS. Our context extractor first collects prosodic-related\nsentential context information from different SAN layers and then aggregates\nthem to learn a comprehensive sentence representation to enhance the\nexpressiveness of the final generated speech. Specifically, we investigate\ntwo methods of context aggregation: 1)  direct aggregation which directly\nconcatenates the outputs of different SAN layers, and 2)  weighted\naggregation which uses multi-head attention to automatically learn\ncontributions for different SAN layers. Experiments on two expressive\ncorpora show that our approach can produce more natural speech with\nmuch richer prosodic variations, and weighted aggregation is more superior\nin modeling expressivity.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2423",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "hono20_interspeech": {
      "authors": [
        [
          "Yukiya",
          "Hono"
        ],
        [
          "Kazuna",
          "Tsuboi"
        ],
        [
          "Kei",
          "Sawada"
        ],
        [
          "Kei",
          "Hashimoto"
        ],
        [
          "Keiichiro",
          "Oura"
        ],
        [
          "Yoshihiko",
          "Nankaku"
        ],
        [
          "Keiichi",
          "Tokuda"
        ]
      ],
      "title": "Hierarchical Multi-Grained Generative Model for Expressive Speech Synthesis",
      "original": "2477",
      "page_count": 5,
      "order": 703,
      "p1": "3441",
      "pn": "3445",
      "abstract": [
        "This paper proposes a hierarchical generative model with a multi-grained\nlatent variable to synthesize expressive speech. In recent years, fine-grained\nlatent variables are introduced into the text-to-speech synthesis that\nenable the fine control of the prosody and speaking styles of synthesized\nspeech. However, the naturalness of speech degrades when these latent\nvariables are obtained by sampling from the standard Gaussian prior.\nTo solve this problem, we propose a novel framework for modeling the\nfine-grained latent variables, considering the dependence on an input\ntext, a hierarchical linguistic structure, and a temporal structure\nof latent variables. This framework consists of a multi-grained variational\nautoencoder, a conditional prior, and a multi-level auto-regressive\nlatent converter to obtain the different time-resolution latent variables\nand sample the finer-level latent variables from the coarser-level\nones by taking into account the input text. Experimental results indicate\nan appropriate method of sampling fine-grained latent variables without\nthe reference signal at the synthesis stage. Our proposed framework\nalso provides the controllability of speaking style in an entire utterance.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2477",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "eskimez20_interspeech": {
      "authors": [
        [
          "Sefik Emre",
          "Eskimez"
        ],
        [
          "Dimitrios",
          "Dimitriadis"
        ],
        [
          "Robert",
          "Gmyr"
        ],
        [
          "Kenichi",
          "Kumanati"
        ]
      ],
      "title": "GAN-Based Data Generation for Speech Emotion Recognition",
      "original": "2898",
      "page_count": 5,
      "order": 704,
      "p1": "3446",
      "pn": "3450",
      "abstract": [
        "In this work, we propose a GAN-based method to generate synthetic data\nfor speech emotion recognition. Specifically, we investigate the usage\nof GANs for capturing the data manifold when the data is  eyes-off,\ni.e., where we can train networks using the data but cannot copy it\nfrom the clients. We propose a CNN-based GAN with spectral normalization\non both the generator and discriminator, both of which are pre-trained\non large unlabeled speech corpora. We show that our method provides\nbetter speech emotion recognition performance than a strong baseline.\nFurthermore, we show that even after the data on the client is lost,\nour model can generate similar data that can be used for model bootstrapping\nin the future. Although we evaluated our method for speech emotion\nrecognition, it can be applied to other tasks.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2898",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "dhamyal20_interspeech": {
      "authors": [
        [
          "Hira",
          "Dhamyal"
        ],
        [
          "Shahan Ali",
          "Memon"
        ],
        [
          "Bhiksha",
          "Raj"
        ],
        [
          "Rita",
          "Singh"
        ]
      ],
      "title": "The Phonetic Bases of Vocal Expressed Emotion: Natural versus Acted",
      "original": "3046",
      "page_count": 5,
      "order": 705,
      "p1": "3451",
      "pn": "3455",
      "abstract": [
        " Can vocal emotions be emulated? This question has been a recurrent\nconcern of the speech community, and has also been vigorously investigated.\nIt has been fueled further by its link to the issue of validity of\nacted emotion databases. Much of the speech and vocal emotion research\nhas relied on acted emotion databases as valid  proxies for studying\nnatural emotions. To create models that generalize to natural settings,\nit is crucial to work with  valid prototypes &#8212; ones that can\nbe assumed to reliably represent natural emotions. More concretely,\nit is important to study emulated emotions against natural emotions\nin terms of their physiological, and psychological concomitants. In\nthis paper, we present an on-scale systematic study of the differences\nbetween natural and acted vocal emotions. We use a self-attention based\nemotion classification model to understand the phonetic bases of emotions\nby discovering the most  &#8216;attended&#8217; phonemes for each class\nof emotions. We then compare these attended-phonemes in their importance\nand distribution across acted and natural classes. Our tests show significant\ndifferences in the manner and choice of phonemes in acted and natural\nspeech, concluding moderate to low validity and value in using acted\nspeech databases for emotion classification tasks.\n"
      ],
      "doi": "10.21437/Interspeech.2020-3046",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "qin20_interspeech": {
      "authors": [
        [
          "Xiaoyi",
          "Qin"
        ],
        [
          "Ming",
          "Li"
        ],
        [
          "Hui",
          "Bu"
        ],
        [
          "Wei",
          "Rao"
        ],
        [
          "Rohan Kumar",
          "Das"
        ],
        [
          "Shrikanth",
          "Narayanan"
        ],
        [
          "Haizhou",
          "Li"
        ]
      ],
      "title": "The INTERSPEECH 2020 Far-Field Speaker Verification Challenge",
      "original": "1249",
      "page_count": 5,
      "order": 706,
      "p1": "3456",
      "pn": "3460",
      "abstract": [
        "The INTERSPEECH 2020 Far-Field Speaker Verification Challenge (FFSVC\n2020) addresses three different research problems under well-defined\nconditions: far-field text-dependent speaker verification from single\nmicrophone array, far-field text-independent speaker verification from\nsingle microphone array, and far-field text-dependent speaker verification\nfrom distributed microphone arrays. All three tasks pose a cross-channel\nchallenge to the participants. To simulate the real-life scenario,\nthe enrollment utterances are recorded from close-talk cellphone, while\nthe test utterances are recorded from the far-field microphone arrays.\nIn this paper, we describe the database, the challenge, and the baseline\nsystem, which is based on a ResNet-based deep speaker network with\ncosine similarity scoring. For a given utterance, the speaker embeddings\nof different channels are equally averaged as the final embedding.\nThe baseline system achieves minDCFs of 0.62, 0.66, and 0.64 and EERs\nof 6.27%, 6.55%, and 7.18% for task 1, task 2, and task 3, respectively.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1249",
      "author_area_id": "13",
      "author_area_label": "Special Sessions"
    },
    "zhang20ba_interspeech": {
      "authors": [
        [
          "Peng",
          "Zhang"
        ],
        [
          "Peng",
          "Hu"
        ],
        [
          "Xueliang",
          "Zhang"
        ]
      ],
      "title": "Deep Embedding Learning for Text-Dependent Speaker Verification",
      "original": "1354",
      "page_count": 5,
      "order": 707,
      "p1": "3461",
      "pn": "3465",
      "abstract": [
        "In this paper we present an effective deep embedding learning architecture\nfor speaker verification task. Compared with the widely used residual\nneural network (ResNet) and time-delay neural network (TDNN) based\narchitectures, two main improvements are proposed: 1) We use densely\nconnected convolutional network (DenseNet) to encode the short term\ncontext information of the speaker. 2) A bidirectional attentive pooling\nstrategy is proposed to further model the long term temporal context\nand aggregate the important frames which reflect the speaker identity.\nWe evaluate the proposed architecture on the task of text-dependent\nspeaker verification in the Interspeech 2020 Far Field Speaker Verification\nChallenge (FFSVC2020). Result shows that the proposed algorithm outperforms\nthe official baseline of FFSVC2020 with 8.06%, 19.70% minDCFs and 9.26%,\n16.16% EERs relative reductions on the evaluation set of Task 1 and\nTask 3 respectively.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1354",
      "author_area_id": "13",
      "author_area_label": "Special Sessions"
    },
    "gusev20_interspeech": {
      "authors": [
        [
          "Aleksei",
          "Gusev"
        ],
        [
          "Vladimir",
          "Volokhov"
        ],
        [
          "Alisa",
          "Vinogradova"
        ],
        [
          "Tseren",
          "Andzhukaev"
        ],
        [
          "Andrey",
          "Shulipa"
        ],
        [
          "Sergey",
          "Novoselov"
        ],
        [
          "Timur",
          "Pekhovsky"
        ],
        [
          "Alexander",
          "Kozlov"
        ]
      ],
      "title": "STC-Innovation Speaker Recognition Systems for Far-Field Speaker Verification Challenge 2020",
      "original": "2580",
      "page_count": 5,
      "order": 708,
      "p1": "3466",
      "pn": "3470",
      "abstract": [
        "This paper presents speaker recognition (SR) systems submitted by the\nSpeech Technology Center (STC) team to the Far-Field Speaker Verification\nChallenge 2020. SR tasks of the challenge are focused on the problem\nof far-field text-dependent speaker verification from single microphone\narray (Track 1), far-field text-independent speaker verification from\nsingle microphone array (Track 2) and far-field text-dependent speaker\nverification from distributed microphone arrays (Track 3).<br style=\"mso-data-placement:same-cell;\"\n/><br style=\"mso-data-placement:same-cell;\" />\n  In this paper, we\npresent techniques and ideas underlying our best performing models.\nA number of experiments on x-vector-based and ResNet-like architectures\nshow that ResNet-based networks outperform x-vector-based systems.\nSubmitted systems are the fusions of ResNet34-based extractors, trained\non 80 Log Mel-filter bank energies (MFBs) post-processed with U-net-like\nvoice activity detector (VAD). The best systems for the Track 1, Track\n2 and Track 3 achieved 5.08% EER and 0.500 C<SUP>min</SUP><SUB>det</SUB>,\n5.39% EER and 0.541 C<SUP>min</SUP><SUB>det</SUB> and 5.53% EER and\n0.458 C<SUP>min</SUP><SUB>det</SUB> on the challenge evaluation sets\nrespectively.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2580",
      "author_area_id": "13",
      "author_area_label": "Special Sessions"
    },
    "zhang20ca_interspeech": {
      "authors": [
        [
          "Li",
          "Zhang"
        ],
        [
          "Jian",
          "Wu"
        ],
        [
          "Lei",
          "Xie"
        ]
      ],
      "title": "NPU Speaker Verification System for INTERSPEECH 2020 Far-Field Speaker Verification Challenge",
      "original": "2688",
      "page_count": 5,
      "order": 709,
      "p1": "3471",
      "pn": "3475",
      "abstract": [
        "This paper describes the NPU system submitted to Interspeech 2020 Far-Field\nSpeaker Verification Challenge (FFSVC). We particularly focus on far-field\ntext-dependent SV from single (task1) and multiple microphone arrays\n(task3). The major challenges in such scenarios are  short utterance\nand  cross-channel and distance mismatch for enrollment and test. With\nthe belief that better speaker embedding can alleviate the effects\nfrom short utterance, we introduce a new speaker embedding architecture\n&#8212; ResNet-BAM, which integrates a bottleneck attention module\nwith ResNet as a simple and efficient way to further improve representation\npower of ResNet. This contribution brings up to 1% EER reduction. We\nfurther address the mismatch problem in three directions. First,  domain\nadversarial training, which aims to learn domain-invariant features,\ncan yield to 0.8% EER reduction. Second,  front-end signal processing,\nincluding WPE and beamforming, has no obvious contribution, but together\nwith data selection and domain adversarial training, can further contribute\nto 0.5% EER reduction. Finally, data augmentation, which works with\na specifically-designed data selection strategy, can lead to 2% EER\nreduction. Together with the above contributions, in the middle challenge\nresults, our single submission system (without multi-system fusion)\nachieves the first and second place on task 1 and task 3, respectively.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2688",
      "author_area_id": "13",
      "author_area_label": "Special Sessions"
    },
    "tong20_interspeech": {
      "authors": [
        [
          "Ying",
          "Tong"
        ],
        [
          "Wei",
          "Xue"
        ],
        [
          "Shanluo",
          "Huang"
        ],
        [
          "Lu",
          "Fan"
        ],
        [
          "Chao",
          "Zhang"
        ],
        [
          "Guohong",
          "Ding"
        ],
        [
          "Xiaodong",
          "He"
        ]
      ],
      "title": "The JD AI Speaker Verification System for the FFSVC 2020 Challenge",
      "original": "3062",
      "page_count": 5,
      "order": 710,
      "p1": "3476",
      "pn": "3480",
      "abstract": [
        "This paper presents the development of our systems for the Interspeech\n2020 Far-Field Speaker Verification Challenge (FFSVC). Our focus is\nthe task 2 of the challenge, which is to perform far-field text-independent\nspeaker verification using a single microphone array. The FFSVC training\nset provided by the challenge is augmented by pre-processing the far-field\ndata with both beamforming, voice channel switching, and a combination\nof weighted prediction error (WPE) and beamforming. Two open-access\ncorpora, CHData in Mandarin and VoxCeleb2 in English, are augmented\nusing multiple methods and mixed with the augmented FFSVC data to form\nthe final training data. Four different model structures are used to\nmodel speaker characteristics: ResNet, extended time-delay neural network\n(ETDNN), Transformer, and factorized TDNN (FTDNN), whose output values\nare pooled across time using the self-attentive structure, the statistic\npooling structure, and the GVLAD structure. The final results are derived\nby fusing the adaptively normalized scores of the four systems with\na two-stage fusion method, which achieves a minimum of the detection\ncost function (minDCF) of 0.3407 and an equal error rate (EER) of 2.67%\non the development set of the challenge.\n"
      ],
      "doi": "10.21437/Interspeech.2020-3062",
      "author_area_id": "13",
      "author_area_label": "Special Sessions"
    },
    "chung20c_interspeech": {
      "authors": [
        [
          "Soo-Whan",
          "Chung"
        ],
        [
          "Soyeon",
          "Choe"
        ],
        [
          "Joon Son",
          "Chung"
        ],
        [
          "Hong-Goo",
          "Kang"
        ]
      ],
      "title": "FaceFilter: Audio-Visual Speech Separation Using Still Images",
      "original": "1065",
      "page_count": 5,
      "order": 711,
      "p1": "3481",
      "pn": "3485",
      "abstract": [
        "The objective of this paper is to separate a target speaker&#8217;s\nspeech from a mixture of two speakers using a deep audio-visual speech\nseparation network. Unlike previous works that used lip movement on\nvideo clips or pre-enrolled speaker information as an auxiliary conditional\nfeature, we use a single face image of the target speaker. In this\ntask, the conditional feature is obtained from facial appearance in\ncross-modal biometric task, where audio and visual identity representations\nare shared in latent space. Learnt identities from facial images enforce\nthe network to isolate matched speakers and extract the voices from\nmixed speech. It solves the permutation problem caused by swapped channel\noutputs, frequently occurred in speech separation tasks. The proposed\nmethod is far more practical than video-based speech separation since\nuser profile images are readily available on many platforms. Also,\nunlike speaker-aware separation methods, it is applicable on separation\nwith unseen speakers who have never been enrolled before. We show strong\nqualitative and quantitative results on challenging real-world examples.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1065",
      "author_area_id": "10",
      "author_area_label": "Speech Recognition: Technologies and Systems for New Applications"
    },
    "chung20d_interspeech": {
      "authors": [
        [
          "Soo-Whan",
          "Chung"
        ],
        [
          "Hong-Goo",
          "Kang"
        ],
        [
          "Joon Son",
          "Chung"
        ]
      ],
      "title": "Seeing Voices and Hearing Voices: Learning Discriminative Embeddings Using Cross-Modal Self-Supervision",
      "original": "1113",
      "page_count": 5,
      "order": 712,
      "p1": "3486",
      "pn": "3490",
      "abstract": [
        "The goal of this work is to train discriminative cross-modal embeddings\nwithout access to manually annotated data. Recent advances in self-supervised\nlearning have shown that effective representations can be learnt from\nnatural cross-modal synchrony. We build on earlier work to train embeddings\nthat are more discriminative for uni-modal downstream tasks. To this\nend, we propose a novel training strategy that not only optimises metrics\nacross modalities, but also enforces intra-class feature separation\nwithin each of the modalities. The effectiveness of the method is demonstrated\non two downstream tasks: lip reading using the features trained on\naudio-visual synchronisation, and speaker recognition using the features\ntrained for cross-modal biometric matching. The proposed method outperforms\nstate-of-the-art self-supervised baselines by a significant margin.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1113",
      "author_area_id": "10",
      "author_area_label": "Speech Recognition: Technologies and Systems for New Applications"
    },
    "wand20_interspeech": {
      "authors": [
        [
          "Michael",
          "Wand"
        ],
        [
          "J\u00fcrgen",
          "Schmidhuber"
        ]
      ],
      "title": "Fusion Architectures for Word-Based Audiovisual Speech Recognition",
      "original": "2117",
      "page_count": 5,
      "order": 713,
      "p1": "3491",
      "pn": "3495",
      "abstract": [
        "In this study we investigate architectures for modality fusion in audiovisual\nspeech recognition, where one aims to alleviate the adverse effect\nof acoustic noise on the speech recognition accuracy by using video\nimages of the speaker&#8217;s face as an additional modality. Starting\nfrom an established neural network fusion system, we substantially\nimprove the recognition accuracy by taking single-modality losses into\naccount: late fusion (at the output logits level) is substantially\nmore robust than the baseline, in particular for unseen acoustic noise,\nat the expense of having to determine the optimal weighting of the\ninput streams. The latter requirement can be removed by making the\nfusion itself a trainable part of the network.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2117",
      "author_area_id": "10",
      "author_area_label": "Speech Recognition: Technologies and Systems for New Applications"
    },
    "yu20f_interspeech": {
      "authors": [
        [
          "Jianwei",
          "Yu"
        ],
        [
          "Bo",
          "Wu"
        ],
        [
          "Rongzhi",
          "Gu"
        ],
        [
          "Shi-Xiong",
          "Zhang"
        ],
        [
          "Lianwu",
          "Chen"
        ],
        [
          "Yong",
          "Xu"
        ],
        [
          "Meng",
          "Yu"
        ],
        [
          "Dan",
          "Su"
        ],
        [
          "Dong",
          "Yu"
        ],
        [
          "Xunying",
          "Liu"
        ],
        [
          "Helen",
          "Meng"
        ]
      ],
      "title": "Audio-Visual Multi-Channel Recognition of Overlapped Speech",
      "original": "2346",
      "page_count": 5,
      "order": 714,
      "p1": "3496",
      "pn": "3500",
      "abstract": [
        "Automatic speech recognition (ASR) of overlapped speech remains a highly\nchallenging task to date. To this end, multi-channel microphone array\ndata are widely used in state-of-the-art ASR systems. Motivated by\nthe invariance of visual modality to acoustic signal corruption, this\npaper presents an audio-visual multi-channel overlapped speech recognition\nsystem featuring tightly integrated separation front-end and recognition\nbackend. A series of audio-visual multi-channel speech separation front-end\ncomponents based on  TF masking,  filter &amp; sum and  mask-based\nMVDR beamforming approaches were developed. To reduce the error cost\nmismatch between the separation and recognition components, they were\njointly fine-tuned using the connectionist temporal classification\n(CTC) loss function, or a multi-task criterion interpolation with scale-invariant\nsignal to noise ratio (Si-SNR) error cost. Experiments suggest that\nthe proposed multi-channel AVSR system outperforms the baseline audio-only\nASR system by up to 6.81% (26.83% relative) and 22.22% (56.87% relative)\nabsolute word error rate (WER) reduction on overlapped speech constructed\nusing either simulation or replaying of the lipreading sentence 2 (LRS2)\ndataset respectively.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2346",
      "author_area_id": "10",
      "author_area_label": "Speech Recognition: Technologies and Systems for New Applications"
    },
    "li20ea_interspeech": {
      "authors": [
        [
          "Wubo",
          "Li"
        ],
        [
          "Dongwei",
          "Jiang"
        ],
        [
          "Wei",
          "Zou"
        ],
        [
          "Xiangang",
          "Li"
        ]
      ],
      "title": "TMT: A Transformer-Based Modal Translator for Improving Multimodal Sequence Representations in Audio Visual Scene-Aware Dialog",
      "original": "2359",
      "page_count": 5,
      "order": 715,
      "p1": "3501",
      "pn": "3505",
      "abstract": [
        "Audio Visual Scene-aware Dialog (AVSD) is a task to generate responses\nwhen discussing about a given video. The previous state-of-the-art\nmodel shows superior performance for this task using Transformer-based\narchitecture. However, there remain some limitations in learning better\nrepresentation of modalities. Inspired by Neural Machine Translation\n(NMT), we propose the Transformer-based Modal Translator (TMT) to learn\nthe representations of the source modal sequence by translating the\nsource modal sequence to the related target modal sequence in a supervised\nmanner. Based on Multimodal Transformer Networks (MTN), we apply TMT\nto video and dialog, proposing MTN-TMT for the video-grounded dialog\nsystem. On the AVSD track of the Dialog System Technology Challenge\n7, MTN-TMT outperforms the MTN and other submission models in both\nVideo and Text task and Text Only task. Compared with MTN, MTN-TMT\nimproves all metrics, especially, achieving relative improvement up\nto 14.1% on CIDEr.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2359",
      "author_area_id": "10",
      "author_area_label": "Speech Recognition: Technologies and Systems for New Applications"
    },
    "sterpu20_interspeech": {
      "authors": [
        [
          "George",
          "Sterpu"
        ],
        [
          "Christian",
          "Saam"
        ],
        [
          "Naomi",
          "Harte"
        ]
      ],
      "title": "Should we Hard-Code the Recurrence Concept or Learn it Instead ? Exploring the Transformer Architecture for Audio-Visual Speech Recognition",
      "original": "2480",
      "page_count": 4,
      "order": 716,
      "p1": "3506",
      "pn": "3509",
      "abstract": [
        "The audio-visual speech fusion strategy AV Align has shown significant\nperformance improvements in audio-visual speech recognition (AVSR)\non the challenging LRS2 dataset. Performance improvements range between\n7% and 30% depending on the noise level when leveraging the visual\nmodality of speech in addition to the auditory one. This work presents\na variant of AV Align where the recurrent Long Short-term Memory (LSTM)\ncomputation block is replaced by the more recently proposed Transformer\nblock. We compare the two methods, discussing in greater detail their\nstrengths and weaknesses. We find that Transformers also learn cross-modal\nmonotonic alignments, but suffer from the same visual convergence problems\nas the LSTM model, calling for a deeper investigation into the dominant\nmodality problem in machine learning.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2480",
      "author_area_id": "10",
      "author_area_label": "Speech Recognition: Technologies and Systems for New Applications"
    },
    "koumparoulis20_interspeech": {
      "authors": [
        [
          "Alexandros",
          "Koumparoulis"
        ],
        [
          "Gerasimos",
          "Potamianos"
        ],
        [
          "Samuel",
          "Thomas"
        ],
        [
          "Edmilson da Silva",
          "Morais"
        ]
      ],
      "title": "Resource-Adaptive Deep Learning for Visual Speech Recognition",
      "original": "3003",
      "page_count": 5,
      "order": 717,
      "p1": "3510",
      "pn": "3514",
      "abstract": [
        "We focus on the problem of efficient architectures for lipreading that\nallow trading-off computational resources for visual speech recognition\naccuracy. In particular, we make two contributions: First, we introduce\nMobiLipNetV3, an efficient and accurate lipreading model, based on\nour earlier work on MobiLipNetV2 and incorporating recent advances\nin convolutional neural network architectures. Second, we propose a\nnovel recognition paradigm, called MultiRate Ensemble (MRE), that combines\na &#8220;lean&#8221; and a &#8220;full&#8221; MobiLipNetV3 in the lipreading\npipeline, with the latter applied at a lower frame rate. This architecture\nyields a family of systems offering multiple accuracy vs. efficiency\noperating points depending on the frame-rate decimation of the &#8220;full&#8221;\nmodel, thus allowing adaptation to the available device resources.\nWe evaluate our approach on the TCD-TIMIT corpus, popular in speaker-independent\nlipreading of continuous speech. The proposed MRE family of systems\ncan be up to 73 times more efficient compared to residual neural network\nbased lipreading, and up to twice as MobiLipNetV2, while in both cases\nreaching up to 8% absolute WER reduction, depending on the MRE chosen\noperating point. For example, a temporal decimation of three yields\na 7% absolute WER reduction and a 26% relative decrease in computations\nover MobiLipNetV2.\n"
      ],
      "doi": "10.21437/Interspeech.2020-3003",
      "author_area_id": "10",
      "author_area_label": "Speech Recognition: Technologies and Systems for New Applications"
    },
    "mortazavi20_interspeech": {
      "authors": [
        [
          "Masood S.",
          "Mortazavi"
        ]
      ],
      "title": "Speech-Image Semantic Alignment Does Not Depend on Any Prior Classification Tasks",
      "original": "3024",
      "page_count": 5,
      "order": 718,
      "p1": "3515",
      "pn": "3519",
      "abstract": [
        "Semantically-aligned ( speech; image) datasets can be used to explore\n&#8220;visually-grounded speech&#8221;. In a majority of existing investigations,\nfeatures of an image signal are extracted using neural networks &#8220;pre-trained&#8221;\non other tasks (e.g., classification on ImageNet). In still others,\npre-trained networks are used to extract audio features prior to semantic\nembedding. Without &#8220;transfer learning&#8221; through pre-trained\ninitialization or pre-trained feature extraction, previous results\nhave tended to show low rates of recall in  speech &#8594;  image and\n image &#8594;  speech queries.<br style=\"mso-data-placement:same-cell;\"\n/><br style=\"mso-data-placement:same-cell;\" />\n  Choosing appropriate\nneural architectures for encoders in the speech and image branches\nand using large datasets, one can obtain competitive recall rates without\nany reliance on any pre-trained initialization or feature extraction:\n( speech; image) semantic alignment and  speech &#8594;  image and\n image &#8594;  speech retrieval are canonical tasks worthy of independent\ninvestigation of their own and allow one to explore other questions\n&#8212; e.g., the size of the audio embedder can be reduced significantly\nwith little loss of recall rates in  speech &#8594;  image and  image\n&#8594;  speech queries.\n"
      ],
      "doi": "10.21437/Interspeech.2020-3024",
      "author_area_id": "10",
      "author_area_label": "Speech Recognition: Technologies and Systems for New Applications"
    },
    "liu20r_interspeech": {
      "authors": [
        [
          "Hong",
          "Liu"
        ],
        [
          "Zhan",
          "Chen"
        ],
        [
          "Bing",
          "Yang"
        ]
      ],
      "title": "Lip Graph Assisted Audio-Visual Speech Recognition Using Bidirectional Synchronous Fusion",
      "original": "3146",
      "page_count": 5,
      "order": 719,
      "p1": "3520",
      "pn": "3524",
      "abstract": [
        "Current studies have shown that extracting representative visual features\nand efficiently fusing audio and visual modalities are vital for audio-visual\nspeech recognition (AVSR), but these are still challenging. To this\nend, we propose a lip graph assisted AVSR method with bidirectional\nsynchronous fusion. First, a hybrid visual stream combines the image\nbranch and graph branch to capture discriminative visual features.\nSpecially, the lip graph exploits the natural and dynamic connections\nbetween the lip key points to model the lip shape, and the temporal\nevolution of the lip graph is captured by the graph convolutional networks\nfollowed by bidirectional gated recurrent units. Second, the hybrid\nvisual stream is combined with the audio stream by an attention-based\nbidirectional synchronous fusion which allows bidirectional information\ninteraction to resolve the asynchrony between the two modalities during\nfusion. The experimental results on LRW-BBC dataset show that our method\noutperforms the end-to-end AVSR baseline method in both clean and noisy\nconditions.\n"
      ],
      "doi": "10.21437/Interspeech.2020-3146",
      "author_area_id": "10",
      "author_area_label": "Speech Recognition: Technologies and Systems for New Applications"
    },
    "konda20_interspeech": {
      "authors": [
        [
          "Vighnesh Reddy",
          "Konda"
        ],
        [
          "Mayur",
          "Warialani"
        ],
        [
          "Rakesh Prasanth",
          "Achari"
        ],
        [
          "Varad",
          "Bhatnagar"
        ],
        [
          "Jayaprakash",
          "Akula"
        ],
        [
          "Preethi",
          "Jyothi"
        ],
        [
          "Ganesh",
          "Ramakrishnan"
        ],
        [
          "Gholamreza",
          "Haffari"
        ],
        [
          "Pankaj",
          "Singh"
        ]
      ],
      "title": "Caption Alignment for Low Resource Audio-Visual Data",
      "original": "3157",
      "page_count": 5,
      "order": 720,
      "p1": "3525",
      "pn": "3529",
      "abstract": [
        "Understanding videos via captioning has gained a lot of traction recently.\nWhile captions are provided alongside videos, the information about\nwhere a caption aligns within a video is missing, which could be particularly\nuseful for indexing and retrieval. Existing work on learning to infer\nalignments has mostly exploited visual features and ignored the audio\nsignal. Video understanding applications often underestimate the importance\nof the audio modality. We focus on how to make effective use of the\naudio modality for temporal localization of captions within videos.\nWe release a new audio-visual dataset that has captions time-aligned\nby (i) carefully listening to the audio and watching the video, and\n(ii) watching only the video. Our dataset is audio-rich and contains\ncaptions in two languages, English and Marathi (a low-resource language).\nWe further propose an attention-driven multimodal model, for effective\nutilization of both audio and video for temporal localization. We then\ninvestigate (i) the effects of audio in both data preparation and model\ndesign, and (ii) effective pretraining strategies (Audioset, ASR-bottleneck\nfeatures, PASE, etc.) handling low-resource setting to help extract\nrich audio representations.\n"
      ],
      "doi": "10.21437/Interspeech.2020-3157",
      "author_area_id": "10",
      "author_area_label": "Speech Recognition: Technologies and Systems for New Applications"
    },
    "michelsanti20_interspeech": {
      "authors": [
        [
          "Daniel",
          "Michelsanti"
        ],
        [
          "Olga",
          "Slizovskaia"
        ],
        [
          "Gloria",
          "Haro"
        ],
        [
          "Emilia",
          "G\u00f3mez"
        ],
        [
          "Zheng-Hua",
          "Tan"
        ],
        [
          "Jesper",
          "Jensen"
        ]
      ],
      "title": "Vocoder-Based Speech Synthesis from Silent Videos",
      "original": "1026",
      "page_count": 5,
      "order": 722,
      "p1": "3530",
      "pn": "3534",
      "abstract": [
        "Both acoustic and visual information influence human perception of\nspeech. For this reason, the lack of audio in a video sequence determines\nan extremely low speech intelligibility for untrained lip readers.\nIn this paper, we present a way to synthesise speech from the silent\nvideo of a talker using deep learning. The system learns a mapping\nfunction from raw video frames to acoustic features and reconstructs\nthe speech with a vocoder synthesis algorithm. To improve speech reconstruction\nperformance, our model is also trained to predict text information\nin a multi-task learning fashion and it is able to simultaneously reconstruct\nand recognise speech in real time. The results in terms of estimated\nspeech quality and intelligibility show the effectiveness of our method,\nwhich exhibits an improvement over existing video-to-speech approaches.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1026",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "wu20k_interspeech": {
      "authors": [
        [
          "Yi-Chiao",
          "Wu"
        ],
        [
          "Tomoki",
          "Hayashi"
        ],
        [
          "Takuma",
          "Okamoto"
        ],
        [
          "Hisashi",
          "Kawai"
        ],
        [
          "Tomoki",
          "Toda"
        ]
      ],
      "title": "Quasi-Periodic Parallel WaveGAN Vocoder: A Non-Autoregressive Pitch-Dependent Dilated Convolution Model for Parametric Speech Generation",
      "original": "1070",
      "page_count": 5,
      "order": 723,
      "p1": "3535",
      "pn": "3539",
      "abstract": [
        "In this paper, we propose a parallel WaveGAN (PWG)-like neural vocoder\nwith a quasi-periodic (QP) architecture to improve the pitch controllability\nof PWG. PWG is a compact non-autoregressive (non-AR) speech generation\nmodel, whose generative speed is much faster than real time. While\nutilizing PWG as a vocoder to generate speech on the basis of acoustic\nfeatures such as spectral and prosodic features, PWG generates high-fidelity\nspeech. However, when the input acoustic features include unseen pitches,\nthe pitch accuracy of PWG-generated speech degrades because of the\nfixed and generic network of PWG without prior knowledge of speech\nperiodicity. The proposed QPPWG adopts a pitch-dependent dilated convolution\nnetwork (PDCNN) module, which introduces the pitch information into\nPWG via the dynamically changed network architecture, to improve the\npitch controllability and speech modeling capability of vanilla PWG.\nBoth objective and subjective evaluation results show the higher pitch\naccuracy and comparable speech quality of QPPWG-generated speech when\nthe QPPWG model size is only 70% of that of vanilla PWG.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1070",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "wu20l_interspeech": {
      "authors": [
        [
          "Yi-Chiao",
          "Wu"
        ],
        [
          "Patrick Lumban",
          "Tobing"
        ],
        [
          "Kazuki",
          "Yasuhara"
        ],
        [
          "Noriyuki",
          "Matsunaga"
        ],
        [
          "Yamato",
          "Ohtani"
        ],
        [
          "Tomoki",
          "Toda"
        ]
      ],
      "title": "A Cyclical Post-Filtering Approach to Mismatch Refinement of Neural Vocoder for Text-to-Speech Systems",
      "original": "1072",
      "page_count": 5,
      "order": 724,
      "p1": "3540",
      "pn": "3544",
      "abstract": [
        "Recently, the effectiveness of text-to-speech (TTS) systems combined\nwith neural vocoders to generate high-fidelity speech has been shown.\nHowever, collecting the required training data and building these advanced\nsystems from scratch are time and resource consuming. An economical\napproach is to develop a neural vocoder to enhance the speech generated\nby existing or low-cost TTS systems. Nonetheless, this approach usually\nsuffers from two issues: 1) temporal mismatches between TTS and natural\nwaveforms and 2) acoustic mismatches between training and testing data.\nTo address these issues, we adopt a cyclic voice conversion (VC) model\nto generate temporally matched pseudo-VC data for training and acoustically\nmatched enhanced data for testing the neural vocoders. Because of the\ngenerality, this framework can be applied to arbitrary TTS systems\nand neural vocoders. In this paper, we apply the proposed method with\na state-of-the-art WaveNet vocoder for two different basic TTS systems,\nand both objective and subjective experimental results confirm the\neffectiveness of the proposed framework.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1072",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "yoon20_interspeech": {
      "authors": [
        [
          "Hyun-Wook",
          "Yoon"
        ],
        [
          "Sang-Hoon",
          "Lee"
        ],
        [
          "Hyeong-Rae",
          "Noh"
        ],
        [
          "Seong-Whan",
          "Lee"
        ]
      ],
      "title": "Audio Dequantization for High Fidelity Audio Generation in Flow-Based Neural Vocoder",
      "original": "1226",
      "page_count": 5,
      "order": 725,
      "p1": "3545",
      "pn": "3549",
      "abstract": [
        "In recent works, a flow-based neural vocoder has shown significant\nimprovement in real-time speech generation task. The sequence of invertible\nflow operations allows the model to convert samples from simple distribution\nto audio samples. However, training a continuous density model on discrete\naudio data can degrade model performance due to the topological difference\nbetween latent and actual distribution. To resolve this problem, we\npropose audio dequantization methods in flow-based neural vocoder for\nhigh fidelity audio generation. Data dequantization is a well-known\nmethod in image generation but has not yet been studied in the audio\ndomain. For this reason, we implement various audio dequantization\nmethods in flow-based neural vocoder and investigate the effect on\nthe generated audio. We conduct various objective performance assessments\nand subjective evaluation to show that audio dequantization can improve\naudio generation quality. From our experiments, using audio dequantization\nproduces waveform audio with better harmonic structure and fewer digital\nartifacts.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1226",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "sharma20b_interspeech": {
      "authors": [
        [
          "Manish",
          "Sharma"
        ],
        [
          "Tom",
          "Kenter"
        ],
        [
          "Rob",
          "Clark"
        ]
      ],
      "title": "StrawNet: Self-Training WaveNet for TTS in Low-Data Regimes",
      "original": "1437",
      "page_count": 5,
      "order": 726,
      "p1": "3550",
      "pn": "3554",
      "abstract": [
        "Recently, WaveNet has become a popular choice of neural network to\nsynthesize speech audio. Autoregressive WaveNet is capable of producing\nhigh-fidelity audio, but is too slow for real-time synthesis. As a\nremedy, Parallel WaveNet was proposed, which can produce audio faster\nthan real time through distillation of an autoregressive teacher into\na feedforward student network. A shortcoming of this approach, however,\nis that a large amount of recorded speech data is required to produce\nhigh-quality student models, and this data is not always available.\nIn this paper, we propose StrawNet: a self-training approach to train\na Parallel WaveNet. Self-training is performed using the synthetic\nexamples generated by the autoregressive WaveNet teacher. We show that,\nin low-data regimes, training on high-fidelity synthetic data from\nan autoregressive teacher model is superior to training the student\nmodel on (much fewer) examples of recorded speech. We compare StrawNet\nto a baseline Parallel WaveNet, using both side-by-side tests and Mean\nOpinion Score evaluations. To our knowledge, synthetic speech has not\nbeen used to train neural text-to-speech before.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1437",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "cui20b_interspeech": {
      "authors": [
        [
          "Yang",
          "Cui"
        ],
        [
          "Xi",
          "Wang"
        ],
        [
          "Lei",
          "He"
        ],
        [
          "Frank K.",
          "Soong"
        ]
      ],
      "title": "An Efficient Subband Linear Prediction for LPCNet-Based Neural Synthesis",
      "original": "1463",
      "page_count": 5,
      "order": 727,
      "p1": "3555",
      "pn": "3559",
      "abstract": [
        "LPCNet neural vocoder and its variants have shown the ability to synthesize\nhigh-quality speech in small footprint by exploiting domain knowledge\nin speech. In this paper, we introduce subband linear prediction in\nLPCNet for producing high fidelity speech more efficiently with consideration\nof subband correlation. Speech is decomposed into multiple subband\nsignals with linear prediction to reduce the complexity of neural vocoder.\nA novel subband-based autoregressive model is proposed to learn the\njoint distribution of the subband sequences by introducing a reasonable\nassumption, which keeps the dependence between subbands while accelerating\nthe inference speed. Based upon the human auditory perception sensitivity\nto the harmonic speech components in the baseband, we allocate more\ncomputational resources to model the low-frequency subband to synthesize\nnatural phase and magnitude of the synthesized speech. Both objective\nand subjective tests show the proposed subband LPCNet neural vocoder\ncan synthesize higher quality speech than the original fullband one\n(MOS 4.62 vs. 4.54), at a rate nearly three times faster.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1463",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "ai20b_interspeech": {
      "authors": [
        [
          "Yang",
          "Ai"
        ],
        [
          "Xin",
          "Wang"
        ],
        [
          "Junichi",
          "Yamagishi"
        ],
        [
          "Zhen-Hua",
          "Ling"
        ]
      ],
      "title": "Reverberation Modeling for Source-Filter-Based Neural Vocoder",
      "original": "1613",
      "page_count": 5,
      "order": 728,
      "p1": "3560",
      "pn": "3564",
      "abstract": [
        "This paper presents a reverberation module for source-filter-based\nneural vocoders that improves the performance of reverberant effect\nmodeling. This module uses the output waveform of neural vocoders as\nan input and produces a reverberant waveform by convolving the input\nwith a room impulse response (RIR). We propose two approaches to parameterizing\nand estimating the RIR. The first approach assumes a global time-invariant\n(GTI) RIR and directly learns the values of the RIR on a training dataset.\nThe second approach assumes an utterance-level time-variant (UTV) RIR,\nwhich is invariant within one utterance but varies across utterances,\nand uses another neural network to predict the RIR values. We add the\nproposed reverberation module to the phase spectrum predictor (PSP)\nof a HiNet vocoder and jointly train the model. Experimental results\ndemonstrate that the proposed module was helpful for modeling the reverberation\neffect and improving the perceived quality of generated reverberant\nspeech. The UTV-RIR was shown to be more robust than the GTI-RIR to\nunknown reverberation conditions and achieved a perceptually better\nreverberation effect.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1613",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "vipperla20_interspeech": {
      "authors": [
        [
          "Ravichander",
          "Vipperla"
        ],
        [
          "Sangjun",
          "Park"
        ],
        [
          "Kihyun",
          "Choo"
        ],
        [
          "Samin",
          "Ishtiaq"
        ],
        [
          "Kyoungbo",
          "Min"
        ],
        [
          "Sourav",
          "Bhattacharya"
        ],
        [
          "Abhinav",
          "Mehrotra"
        ],
        [
          "Alberto Gil C.P.",
          "Ramos"
        ],
        [
          "Nicholas D.",
          "Lane"
        ]
      ],
      "title": "Bunched LPCNet: Vocoder for Low-Cost Neural Text-To-Speech Systems",
      "original": "2041",
      "page_count": 5,
      "order": 729,
      "p1": "3565",
      "pn": "3569",
      "abstract": [
        "LPCNet is an efficient vocoder that combines linear prediction and\ndeep neural network modules to keep the computational complexity low.\nIn this work, we present two techniques to further reduce it&#8217;s\ncomplexity, aiming for a low-cost LPCNet vocoder-based neural Text-to-Speech\n(TTS) System. These techniques are: 1) Sample-bunching, which allows\nLPCNet to generate more than one audio sample per inference; and 2)\nBit-bunching, which reduces the computations in the final layer of\nLPCNet. With the proposed bunching techniques, LPCNet, in conjunction\nwith a Deep Convolutional TTS (DCTTS) acoustic model, shows a 2.19&#215;\nimprovement over the baseline run-time when running on a mobile device,\nwith a less than 0.1 decrease in TTS mean opinion score (MOS).\n"
      ],
      "doi": "10.21437/Interspeech.2020-2041",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "song20c_interspeech": {
      "authors": [
        [
          "Eunwoo",
          "Song"
        ],
        [
          "Min-Jae",
          "Hwang"
        ],
        [
          "Ryuichi",
          "Yamamoto"
        ],
        [
          "Jin-Seob",
          "Kim"
        ],
        [
          "Ohsung",
          "Kwon"
        ],
        [
          "Jae-Min",
          "Kim"
        ]
      ],
      "title": "Neural Text-to-Speech with a Modeling-by-Generation Excitation Vocoder",
      "original": "2116",
      "page_count": 5,
      "order": 730,
      "p1": "3570",
      "pn": "3574",
      "abstract": [
        "This paper proposes a modeling-by-generation (MbG) excitation vocoder\nfor a neural text-to-speech (TTS) system. Recently proposed neural\nexcitation vocoders can realize qualified waveform generation by combining\na vocal tract filter with a WaveNet-based glottal excitation generator.\nHowever, when these vocoders are used in a TTS system, the quality\nof synthesized speech is often degraded owing to a mismatch between\ntraining and synthesis steps. Specifically, the vocoder is separately\ntrained from an acoustic model front-end. Therefore, estimation errors\nof the acoustic model are inevitably boosted throughout the synthesis\nprocess of the vocoder backend. To address this problem, we propose\nto incorporate an MbG structure into the vocoder&#8217;s training process.\nIn the proposed method, the excitation signal is extracted by the acoustic\nmodel&#8217;s generated spectral parameters, and the neural vocoder\nis then optimized not only to learn the target excitation&#8217;s distribution\nbut also to compensate for the estimation errors occurring from the\nacoustic model. Furthermore, as the generated spectral parameters are\nshared in the training and synthesis steps, their mismatch conditions\ncan be reduced effectively. The experimental results verify that the\nproposed system provides high-quality synthetic speech by achieving\na mean opinion score of 4.57 within the TTS framework.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2116",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "vainer20_interspeech": {
      "authors": [
        [
          "Jan",
          "Vainer"
        ],
        [
          "Ond\u0159ej",
          "Du\u0161ek"
        ]
      ],
      "title": "SpeedySpeech: Efficient Neural Speech Synthesis",
      "original": "2867",
      "page_count": 5,
      "order": 731,
      "p1": "3575",
      "pn": "3579",
      "abstract": [
        "While recent neural sequence-to-sequence models have greatly improved\nthe quality of speech synthesis, there has not been a system capable\nof fast training, fast inference and high-quality audio synthesis at\nthe same time. We propose a student-teacher network capable of high-quality\nfaster-than-real-time spectrogram synthesis, with low requirements\non computational resources and fast training time. We show that self-attention\nlayers are not necessary for generation of high quality audio. We utilize\nsimple convolutional blocks with residual connections in both student\nand teacher networks and use only a single attention layer in the teacher\nmodel. Coupled with a MelGAN vocoder, our model&#8217;s voice quality\nwas rated significantly higher than Tacotron 2. Our model can be efficiently\ntrained on a single GPU and can run in real time even on a CPU. We\nprovide both our source code and audio samples in our GitHub repository.<SUP>1</SUP>\n"
      ],
      "doi": "10.21437/Interspeech.2020-2867",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "zhang20da_interspeech": {
      "authors": [
        [
          "Zi-qiang",
          "Zhang"
        ],
        [
          "Yan",
          "Song"
        ],
        [
          "Jian-shu",
          "Zhang"
        ],
        [
          "Ian",
          "McLoughlin"
        ],
        [
          "Li-Rong",
          "Dai"
        ]
      ],
      "title": "Semi-Supervised End-to-End ASR via Teacher-Student Learning with Conditional Posterior Distribution",
      "original": "1574",
      "page_count": 5,
      "order": 732,
      "p1": "3580",
      "pn": "3584",
      "abstract": [
        "Encoder-decoder based methods have become popular for automatic speech\nrecognition (ASR), thanks to their simplified processing stages and\nlow reliance on prior knowledge. However, large amounts of acoustic\ndata with paired transcriptions is generally required to train an effective\nencoder-decoder model, which is expensive, time-consuming to be collected\nand not always readily available. However unpaired speech data is abundant,\nhence several semi-supervised learning methods, such as teacher-student\n(T/S) learning and pseudo-labeling, have recently been proposed to\nutilize this potentially valuable resource. In this paper, a novel\nT/S learning with conditional posterior distribution for encoder-decoder\nbased ASR is proposed. Specifically, the 1-best hypotheses and the\nconditional posterior distribution from the teacher are exploited to\nprovide more effective supervision. Combined with model perturbation\ntechniques, the proposed method reduces WER by 19.2% relatively on\nthe LibriSpeech benchmark, compared with a system trained using only\npaired data. This outperforms previous reported 1-best hypothesis results\non the same task.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1574",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "sapru20_interspeech": {
      "authors": [
        [
          "Ashtosh",
          "Sapru"
        ],
        [
          "Sri",
          "Garimella"
        ]
      ],
      "title": "Leveraging Unlabeled Speech for Sequence Discriminative Training of Acoustic Models",
      "original": "2056",
      "page_count": 5,
      "order": 733,
      "p1": "3585",
      "pn": "3589",
      "abstract": [
        "State-of-the-art Acoustic Modeling (AM) techniques use long short term\nmemory (LSTM) networks, and apply multiple phases of training on large\namount of labeled acoustic data &#8212; initial cross-entropy (CE)\ntraining or connectionist temporal classification (CTC) training followed\nby sequence discriminative training, such as state-level Minimum Bayes\nRisk (sMBR). Recently, there is considerable interest in applying Semi-Supervised\nLearning (SSL) methods that leverage substantial amount of unlabeled\nspeech for improving AM. This paper proposes a novel Teacher-Student\nbased knowledge distillation (KD) approach for sequence discriminative\ntraining, where reference state sequence of unlabeled data are estimated\nusing a strong Bi-directional LSTM Teacher model which is then used\nto guide the sMBR training of a LSTM Student model. We build a strong\nsupervised LSTM AM baseline by using 45000 hours of labeled multi-dialect\nEnglish data for initial CE or CTC training stage, and 11000 hours\nof its British English subset for sMBR training phase. To demonstrate\nthe efficacy of the proposed approach, we leverage an additional 38000\nhours of unlabeled British English data at only sMBR stage, which yields\na relative Word Error Rate (WER) improvement in the range of 6%&#8211;11%\nover supervised baselines in clean and noisy test conditions.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2056",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "li20fa_interspeech": {
      "authors": [
        [
          "Jinyu",
          "Li"
        ],
        [
          "Rui",
          "Zhao"
        ],
        [
          "Zhong",
          "Meng"
        ],
        [
          "Yanqing",
          "Liu"
        ],
        [
          "Wenning",
          "Wei"
        ],
        [
          "Sarangarajan",
          "Parthasarathy"
        ],
        [
          "Vadim",
          "Mazalov"
        ],
        [
          "Zhenghao",
          "Wang"
        ],
        [
          "Lei",
          "He"
        ],
        [
          "Sheng",
          "Zhao"
        ],
        [
          "Yifan",
          "Gong"
        ]
      ],
      "title": "Developing RNN-T Models Surpassing High-Performance Hybrid Models with Customization Capability",
      "original": "3016",
      "page_count": 5,
      "order": 734,
      "p1": "3590",
      "pn": "3594",
      "abstract": [
        "Because of its streaming nature, recurrent neural network transducer\n(RNN-T) is a very promising end-to-end (E2E) model that may replace\nthe popular hybrid model for automatic speech recognition. In this\npaper, we describe our recent development of RNN-T models with reduced\nGPU memory consumption during training, better initialization strategy,\nand advanced encoder modeling with future lookahead. When trained with\nMicrosoft&#8217;s 65 thousand hours of anonymized training data, the\ndeveloped RNN-T model surpasses a very well trained hybrid model with\nboth better recognition accuracy and lower latency. We further study\nhow to customize RNN-T models to a new domain, which is important for\ndeploying E2E models to practical scenarios. By comparing several methods\nleveraging text-only data in the new domain, we found that updating\nRNN-T&#8217;s prediction and joint networks using text-to-speech generated\nfrom domain-specific text is the most effective.\n"
      ],
      "doi": "10.21437/Interspeech.2020-3016",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "chang20c_interspeech": {
      "authors": [
        [
          "Xuankai",
          "Chang"
        ],
        [
          "Aswin Shanmugam",
          "Subramanian"
        ],
        [
          "Pengcheng",
          "Guo"
        ],
        [
          "Shinji",
          "Watanabe"
        ],
        [
          "Yuya",
          "Fujita"
        ],
        [
          "Motoi",
          "Omachi"
        ]
      ],
      "title": "End-to-End ASR with Adaptive Span Self-Attention",
      "original": "2816",
      "page_count": 5,
      "order": 735,
      "p1": "3595",
      "pn": "3599",
      "abstract": [
        "Transformers have demonstrated state-of-the-art performance on many\ntasks in natural language processing and speech processing. One of\nthe key components in Transformers is self-attention, which attends\nto the whole input sequence at every layer. However, the computational\nand memory cost of self-attention is square of the input sequence length,\nwhich is a major concern in automatic speech recognition (ASR) where\nthe input sequence can be very long. In this paper, we propose to use\na technique called adaptive span self-attention for ASR tasks, which\nis originally proposed for language modeling. Our method enables the\nnetwork to learn an appropriate size and position of the window for\neach layer and head, and our newly introduced scheme can further control\nthe window size depending on the future and past contexts. Thus, it\ncan save both computational complexity and memory size from the square\norder of the input length to the adaptive linear order. We show the\neffectiveness of the proposed method by using several ASR tasks, and\nthe proposed adaptive span methods consistently improved the performance\nfrom the conventional fixed span methods.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2816",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "lakomkin20_interspeech": {
      "authors": [
        [
          "Egor",
          "Lakomkin"
        ],
        [
          "Jahn",
          "Heymann"
        ],
        [
          "Ilya",
          "Sklyar"
        ],
        [
          "Simon",
          "Wiesler"
        ]
      ],
      "title": "Subword Regularization: An Analysis of Scalability and Generalization for End-to-End Automatic Speech Recognition",
      "original": "1569",
      "page_count": 5,
      "order": 736,
      "p1": "3600",
      "pn": "3604",
      "abstract": [
        "Subwords are the most widely used output units in end-to-end speech\nrecognition. They combine the best of two worlds by modeling the majority\nof frequent words directly and at the same time allow open vocabulary\nspeech recognition by backing off to shorter units or characters to\nconstruct words unseen during training. However, mapping text to subwords\nis ambiguous and often multiple segmentation variants are possible.\nYet, many systems are trained using only the most likely segmentation.\nRecent research suggests that sampling subword segmentations during\ntraining acts as a regularizer for neural machine translation and speech\nrecognition models, leading to performance improvements. In this work,\nwe conduct a principled investigation on the regularizing effect of\nthe subword segmentation sampling method for a streaming end-to-end\nspeech recognition task. In particular, we evaluate the subword regularization\ncontribution depending on the size of the training dataset. Our results\nsuggest that subword regularization provides a consistent improvement\nof 2&#8211;8% relative word-error-rate reduction, even in a large-scale\nsetting with datasets up to a size of 20k hours. Further, we analyze\nthe effect of subword regularization on recognition of unseen words\nand its implications on beam diversity.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1569",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "michel20_interspeech": {
      "authors": [
        [
          "Wilfried",
          "Michel"
        ],
        [
          "Ralf",
          "Schl\u00fcter"
        ],
        [
          "Hermann",
          "Ney"
        ]
      ],
      "title": "Early Stage LM Integration Using Local and Global Log-Linear Combination",
      "original": "2675",
      "page_count": 5,
      "order": 737,
      "p1": "3605",
      "pn": "3609",
      "abstract": [
        "Sequence-to-sequence models with an implicit alignment mechanism (e.g.\nattention) are closing the performance gap towards traditional hybrid\nhidden Markov models (HMM) for the task of automatic speech recognition.\nOne important factor to improve word error rate in both cases is the\nuse of an external language model (LM) trained on large text-only corpora.\nLanguage model integration is straightforward with the clear separation\nof acoustic model and language model in classical HMM-based modeling.\nIn contrast, multiple integration schemes have been proposed for attention\nmodels.<br style=\"mso-data-placement:same-cell;\" /><br style=\"mso-data-placement:same-cell;\"\n/>\n  In this work, we present a novel method for language model integration\ninto implicit-alignment based sequence-to-sequence models. Log-linear\nmodel combination of acoustic and language model is performed with\na per-token renormalization. This allows us to compute the full normalization\nterm efficiently both in training and in testing.<br style=\"mso-data-placement:same-cell;\"\n/><br style=\"mso-data-placement:same-cell;\" />\n  This is compared to\na global renormalization scheme which is equivalent to applying shallow\nfusion in training.<br style=\"mso-data-placement:same-cell;\" /><br\nstyle=\"mso-data-placement:same-cell;\" />\n  The proposed methods show\ngood improvements over standard model combination (shallow fusion)\non our state-of-the-art Librispeech system. Furthermore, the improvements\nare persistent even if the LM is exchanged for a more powerful one\nafter training.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2675",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "han20_interspeech": {
      "authors": [
        [
          "Wei",
          "Han"
        ],
        [
          "Zhengdong",
          "Zhang"
        ],
        [
          "Yu",
          "Zhang"
        ],
        [
          "Jiahui",
          "Yu"
        ],
        [
          "Chung-Cheng",
          "Chiu"
        ],
        [
          "James",
          "Qin"
        ],
        [
          "Anmol",
          "Gulati"
        ],
        [
          "Ruoming",
          "Pang"
        ],
        [
          "Yonghui",
          "Wu"
        ]
      ],
      "title": "ContextNet: Improving Convolutional Neural Networks for Automatic Speech Recognition with Global Context",
      "original": "2059",
      "page_count": 5,
      "order": 738,
      "p1": "3610",
      "pn": "3614",
      "abstract": [
        "Convolutional neural networks (CNN) have shown promising results for\nend-to-end speech recognition, albeit still behind RNN/transformer\nbased models in performance. In this paper, we study how to bridge\nthis gap and go beyond with a novel CNN-RNN-transducer architecture,\nwhich we call ContextNet. ContextNet features a fully convolutional\nencoder that incorporates global context information into convolution\nlayers by adding squeeze-and-excitation modules. In addition, we propose\na simple scaling method that scales the widths of ContextNet that achieves\ngood trade-off between computation and accuracy.<br style=\"mso-data-placement:same-cell;\"\n/><br style=\"mso-data-placement:same-cell;\" />\n  We demonstrate that\non the widely used Librispeech benchmark, ContextNet achieves a word\nerror rate (WER) of 2.1%/4.6% without external language model (LM),\n1.9%/4.1% with LM and 2.9%/7.0% with only 10M parameters on the clean/noisy\nLibriSpeech test sets. This compares to the best previously published\nmodel of 2.0%/4.6% with LM and 3.9%/11.3% with 20M parameters. The\nsuperiority of the proposed ContextNet model is also verified on a\nmuch larger internal dataset.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2059",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "sainath20_interspeech": {
      "authors": [
        [
          "Tara N.",
          "Sainath"
        ],
        [
          "Ruoming",
          "Pang"
        ],
        [
          "David",
          "Rybach"
        ],
        [
          "Basi",
          "Garc\u00eda"
        ],
        [
          "Trevor",
          "Strohman"
        ]
      ],
      "title": "Emitting Word Timings with End-to-End Models",
      "original": "1059",
      "page_count": 5,
      "order": 739,
      "p1": "3615",
      "pn": "3619",
      "abstract": [
        "Having end-to-end (E2E) models emit the start and end times of words\n on-device is important for various applications. This unsolved problem\npresents challenges with respect to model size, latency and accuracy.\nIn this paper, we present an approach to word timings by constraining\nthe attention head of the Listen, Attend, Spell (LAS) 2nd-pass rescorer\n[1]. On a Voice-Search task, we show that this approach does not degrade\naccuracy compared to when no attention head is constrained. In addition,\nit meets on-device size and latency constraints. In comparison, constraining\nthe alignment with a 1st-pass Recurrent Neural Network Transducer (RNN-T)\nmodel to emit word timings results in quality degradation. Furthermore,\na low-frame-rate conventional acoustic model [2], which is trained\nwith a constrained alignment and is used in many applications for word\ntimings, is slower to detect start and end times compared to our proposed\n2nd-pass LAS approach.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1059",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "liu20s_interspeech": {
      "authors": [
        [
          "Danni",
          "Liu"
        ],
        [
          "Gerasimos",
          "Spanakis"
        ],
        [
          "Jan",
          "Niehues"
        ]
      ],
      "title": "Low-Latency Sequence-to-Sequence Speech Recognition and Translation by Partial Hypothesis Selection",
      "original": "2897",
      "page_count": 5,
      "order": 740,
      "p1": "3620",
      "pn": "3624",
      "abstract": [
        "Encoder-decoder models provide a generic architecture for sequence-to-sequence\ntasks such as speech recognition and translation. While offline systems\nare often evaluated on quality metrics like word error rates (WER)\nand BLEU scores, latency is also a crucial factor in many practical\nuse-cases. We propose three latency reduction techniques for chunk-based\nincremental inference and evaluate their accuracy-latency tradeoff.\nOn the 300-hour How2 dataset, we reduce latency by 83% to 0.8 second\nby sacrificing 1% WER (6% rel.) compared to offline transcription.\nAlthough our experiments use the Transformer, the partial hypothesis\nselection strategies are applicable to other encoder-decoder models.\nTo reduce expensive re-computation as new chunks arrive, we propose\nto use a unidirectionally-attending encoder. After an adaptation procedure\nto partial sequences, the unidirectional model performs on par with\nthe original model. We further show that our approach is also applicable\nto speech translation. On the How2 English-Portuguese speech translation\ndataset, we reduce latency to 0.7 second (-84% rel.) while incurring\na loss of 2.4 BLEU points (5% rel.) compared to the offline system.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2897",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "li20ga_interspeech": {
      "authors": [
        [
          "Ke",
          "Li"
        ],
        [
          "Daniel",
          "Povey"
        ],
        [
          "Sanjeev",
          "Khudanpur"
        ]
      ],
      "title": "Neural Language Modeling with Implicit Cache Pointers",
      "original": "3020",
      "page_count": 5,
      "order": 741,
      "p1": "3625",
      "pn": "3629",
      "abstract": [
        "A cache-inspired approach is proposed for neural language models (LMs)\nto improve long-range dependency and better predict rare words from\nlong contexts. This approach is a simpler alternative to attention-based\npointer mechanism that enables neural LMs to reproduce words from recent\nhistory. Without using attention and mixture structure, the method\nonly involves appending extra tokens that represent words in history\nto the output layer of a neural LM and modifying training supervisions\naccordingly. A memory-augmentation unit is introduced to learn words\nthat are particularly likely to repeat. We experiment with both recurrent\nneural network- and Transformer-based LMs. Perplexity evaluation on\nPenn Treebank and WikiText-2 shows the proposed model outperforms both\nLSTM and LSTM with attention-based pointer mechanism and is more effective\non rare words. N-best rescoring experiments on Switchboard indicate\nthat it benefits both very rare and frequent words. However, it is\nchallenging for the proposed model as well as two other models with\nattention-based pointer mechanism to obtain good overall WER reductions.\n"
      ],
      "doi": "10.21437/Interspeech.2020-3020",
      "author_area_id": "9",
      "author_area_label": "Speech Recognition: Architecture, Search, and Linguistic Components"
    },
    "jain20b_interspeech": {
      "authors": [
        [
          "Abhilash",
          "Jain"
        ],
        [
          "Aku",
          "Rouhe"
        ],
        [
          "Stig-Arne",
          "Gr\u00f6nroos"
        ],
        [
          "Mikko",
          "Kurimo"
        ]
      ],
      "title": "Finnish ASR with Deep Transformer Models",
      "original": "1784",
      "page_count": 5,
      "order": 742,
      "p1": "3630",
      "pn": "3634",
      "abstract": [
        "Recently, BERT and Transformer-XL based architectures have achieved\nstrong results in a range of NLP applications. In this paper, we explore\nTransformer architectures &#8212; BERT and Transformer-XL &#8212; as\na language model for a Finnish ASR task with different rescoring schemes.<br\nstyle=\"mso-data-placement:same-cell;\" /><br style=\"mso-data-placement:same-cell;\"\n/>\n  We achieve strong results in both an intrinsic and an extrinsic\ntask with Transformer-XL. Achieving 29% better perplexity and 3% better\nWER than our previous best LSTM-based approach. We also introduce a\nnovel three-pass decoding scheme which improves the ASR performance\nby 8%. To the best of our knowledge, this is also the first work (i)\nto formulate an alpha smoothing framework to use the non-autoregressive\nBERT language model for an ASR task, and (ii) to explore sub-word units\nwith Transformer-XL for an agglutinative language like Finnish.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1784",
      "author_area_id": "9",
      "author_area_label": "Speech Recognition: Architecture, Search, and Linguistic Components"
    },
    "futami20_interspeech": {
      "authors": [
        [
          "Hayato",
          "Futami"
        ],
        [
          "Hirofumi",
          "Inaguma"
        ],
        [
          "Sei",
          "Ueno"
        ],
        [
          "Masato",
          "Mimura"
        ],
        [
          "Shinsuke",
          "Sakai"
        ],
        [
          "Tatsuya",
          "Kawahara"
        ]
      ],
      "title": "Distilling the Knowledge of BERT for Sequence-to-Sequence ASR",
      "original": "1179",
      "page_count": 5,
      "order": 743,
      "p1": "3635",
      "pn": "3639",
      "abstract": [
        "Attention-based sequence-to-sequence (seq2seq) models have achieved\npromising results in automatic speech recognition (ASR). However, as\nthese models decode in a left-to-right way, they do not have access\nto context on the right. We leverage both left and right context by\napplying BERT as an external language model to seq2seq ASR through\nknowledge distillation. In our proposed method, BERT generates soft\nlabels to guide the training of seq2seq ASR. Furthermore, we leverage\ncontext beyond the current utterance as input to BERT. Experimental\nevaluations show that our method significantly improves the ASR performance\nfrom the seq2seq baseline on the Corpus of Spontaneous Japanese (CSJ).\nKnowledge distillation from BERT outperforms that from a transformer\nLM that only looks at left context. We also show the effectiveness\nof leveraging context beyond the current utterance. Our method outperforms\nother LM application approaches such as n-best rescoring and shallow\nfusion, while it does not require extra inference cost.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1179",
      "author_area_id": "9",
      "author_area_label": "Speech Recognition: Architecture, Search, and Linguistic Components"
    },
    "chien20_interspeech": {
      "authors": [
        [
          "Jen-Tzung",
          "Chien"
        ],
        [
          "Yu-Min",
          "Huang"
        ]
      ],
      "title": "Stochastic Convolutional Recurrent Networks for Language Modeling",
      "original": "1493",
      "page_count": 5,
      "order": 744,
      "p1": "3640",
      "pn": "3644",
      "abstract": [
        "Sequential learning using recurrent neural network (RNN) has been popularly\ndeveloped for language modeling. An alternative sequential learning\nwas implemented by the temporal convolutional network (TCN) which is\nseen as a variant of one-dimensional convolutional neural network (CNN).\nIn general, RNN and TCN are fitted to capture the long-term and the\nshort-term features over natural sentences, respectively. This paper\nis motivated to fulfill TCN as the encoder to extract short-term dependencies\nand then use RNN as the decoder for language modeling where the dependencies\nare integrated in a long-term semantic fashion for word prediction.\nA new sequential learning based on the convolutional recurrent network\n(CRN) is developed to characterize the  local dependencies as well\nas the  global semantics in word sequences. Importantly, the stochastic\nmodeling for CRN is proposed to facilitate model capacity in neural\nlanguage model where the uncertainties in training sentences are represented\nfor variational inference. The complementary benefits of CNN and RNN\nare merged in sequential learning where the latent variable space is\nconstructed as a generative model for sequential prediction. Experiments\non language modeling demonstrate the effectiveness of stochastic convolutional\nrecurrent network relative to the other sequential machines in terms\nof perplexity and word error rate.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1493",
      "author_area_id": "9",
      "author_area_label": "Speech Recognition: Architecture, Search, and Linguistic Components"
    },
    "huo20_interspeech": {
      "authors": [
        [
          "Jingjing",
          "Huo"
        ],
        [
          "Yingbo",
          "Gao"
        ],
        [
          "Weiyue",
          "Wang"
        ],
        [
          "Ralf",
          "Schl\u00fcter"
        ],
        [
          "Hermann",
          "Ney"
        ]
      ],
      "title": "Investigation of Large-Margin Softmax in Neural Language Modeling",
      "original": "1849",
      "page_count": 5,
      "order": 745,
      "p1": "3645",
      "pn": "3649",
      "abstract": [
        "To encourage intra-class compactness and inter-class separability among\ntrainable feature vectors, large-margin softmax methods are developed\nand widely applied in the face recognition community. The introduction\nof the large-margin concept into the softmax is reported to have good\nproperties such as enhanced discriminative power, less overfitting\nand well-defined geometric intuitions. Nowadays, language modeling\nis commonly approached with neural networks using softmax and cross\nentropy. In this work, we are curious to see if introducing large-margins\nto neural language models would improve the perplexity and consequently\nword error rate in automatic speech recognition. Specifically, we first\nimplement and test various types of conventional margins following\nthe previous works in face recognition. To address the distribution\nof natural language data, we then compare different strategies for\nword vector norm-scaling. After that, we apply the best norm-scaling\nsetup in combination with various margins and conduct neural language\nmodels rescoring experiments in automatic speech recognition. We find\nthat although perplexity is slightly deteriorated, neural language\nmodels with large-margin softmax can yield word error rate similar\nto that of the standard softmax baseline. Finally, expected margins\nare analyzed through visualization of word vectors, showing that the\nsyntactic and semantic relationships are also preserved.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1849",
      "author_area_id": "9",
      "author_area_label": "Speech Recognition: Architecture, Search, and Linguistic Components"
    },
    "liu20t_interspeech": {
      "authors": [
        [
          "Da-Rong",
          "Liu"
        ],
        [
          "Chunxi",
          "Liu"
        ],
        [
          "Frank",
          "Zhang"
        ],
        [
          "Gabriel",
          "Synnaeve"
        ],
        [
          "Yatharth",
          "Saraf"
        ],
        [
          "Geoffrey",
          "Zweig"
        ]
      ],
      "title": "Contextualizing ASR Lattice Rescoring with Hybrid Pointer Network Language Model",
      "original": "1344",
      "page_count": 5,
      "order": 746,
      "p1": "3650",
      "pn": "3654",
      "abstract": [
        "Videos uploaded on social media are often accompanied with textual\ndescriptions. In building automatic speech recognition (ASR) systems\nfor videos, we can exploit the contextual information provided by such\nvideo metadata. In this paper, we explore ASR lattice rescoring by\nselectively attending to the video descriptions. We first use an attention\nbased method to extract contextual vector representations of video\nmetadata, and use these representations as part of the inputs to a\nneural language model during lattice rescoring. Secondly, we propose\na hybrid pointer network approach to explicitly interpolate the word\nprobabilities of the word occurrences in metadata. We perform experimental\nevaluations on both language modeling and ASR tasks, and demonstrate\nthat both proposed methods provide performance improvements by selectively\nleveraging the video metadata.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1344",
      "author_area_id": "9",
      "author_area_label": "Speech Recognition: Architecture, Search, and Linguistic Components"
    },
    "higuchi20b_interspeech": {
      "authors": [
        [
          "Yosuke",
          "Higuchi"
        ],
        [
          "Shinji",
          "Watanabe"
        ],
        [
          "Nanxin",
          "Chen"
        ],
        [
          "Tetsuji",
          "Ogawa"
        ],
        [
          "Tetsunori",
          "Kobayashi"
        ]
      ],
      "title": "Mask CTC: Non-Autoregressive End-to-End ASR with CTC and Mask Predict",
      "original": "2404",
      "page_count": 5,
      "order": 747,
      "p1": "3655",
      "pn": "3659",
      "abstract": [
        "We present Mask CTC, a novel  non-autoregressive end-to-end automatic\nspeech recognition (ASR) framework, which generates a sequence by refining\noutputs of the connectionist temporal classification (CTC). Neural\nsequence-to-sequence models are usually  autoregressive: each output\ntoken is generated by conditioning on previously generated tokens,\nat the cost of requiring as many iterations as the output length. On\nthe other hand, non-autoregressive models can simultaneously generate\ntokens within a constant number of iterations, which results in significant\ninference time reduction and better suits end-to-end ASR model for\nreal-world scenarios. In this work, Mask CTC model is trained using\na Transformer encoder-decoder with joint training of mask prediction\nand CTC. During inference, the target sequence is initialized with\nthe greedy CTC outputs and low-confidence tokens are masked based on\nthe CTC probabilities. Based on the conditional dependence between\noutput tokens, these masked low-confidence tokens are then predicted\nconditioning on the high-confidence tokens. Experimental results on\ndifferent speech recognition tasks show that Mask CTC outperforms the\nstandard CTC model (e.g., 17.9% &#8594; 12.1% WER on WSJ) and approaches\nthe autoregressive model, requiring much less inference time using\nCPUs (0.07 RTF in Python implementation). All of our codes are publicly\navailable at <KBD>https://github.com/espnet/espnet</KBD>\n"
      ],
      "doi": "10.21437/Interspeech.2020-2404",
      "author_area_id": "9",
      "author_area_label": "Speech Recognition: Architecture, Search, and Linguistic Components"
    },
    "fujita20_interspeech": {
      "authors": [
        [
          "Yuya",
          "Fujita"
        ],
        [
          "Shinji",
          "Watanabe"
        ],
        [
          "Motoi",
          "Omachi"
        ],
        [
          "Xuankai",
          "Chang"
        ]
      ],
      "title": "Insertion-Based Modeling for End-to-End Automatic Speech Recognition",
      "original": "1619",
      "page_count": 5,
      "order": 748,
      "p1": "3660",
      "pn": "3664",
      "abstract": [
        "End-to-end (E2E) models have gained attention in the research field\nof automatic speech recognition (ASR). Many E2E models proposed so\nfar assume left-to-right autoregressive generation of an output token\nsequence except for connectionist temporal classification (CTC) and\nits variants. However, left-to-right decoding cannot consider the future\noutput context, and it is not always optimal for ASR. One of the non-left-to-right\nmodels is known as non-autoregressive Transformer (NAT) and has been\nintensively investigated in the area of neural machine translation\n(NMT) research. One NAT model, mask-predict, has been applied to ASR\nbut the model needs some heuristics or additional component to estimate\nthe length of the output token sequence. This paper proposes to apply\nanother type of NAT called insertion-based models, that were originally\nproposed for NMT, to ASR tasks. Insertion-based models solve the above\nmask-predict issues and can generate an arbitrary generation order\nof an output sequence. In addition, we introduce a new formulation\nof joint training of the insertion-based models and CTC. This formulation\nreinforces CTC by making it dependent on insertion-based token generation\nin a non-autoregressive manner. We conducted experiments on three public\nbenchmarks and achieved competitive performance to strong autoregressive\nTransformer with a similar decoding condition.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1619"
    },
    "chen20o_interspeech": {
      "authors": [
        [
          "Yefei",
          "Chen"
        ],
        [
          "Heinrich",
          "Dinkel"
        ],
        [
          "Mengyue",
          "Wu"
        ],
        [
          "Kai",
          "Yu"
        ]
      ],
      "title": "Voice Activity Detection in the Wild via Weakly Supervised Sound Event Detection",
      "original": "0995",
      "page_count": 5,
      "order": 749,
      "p1": "3665",
      "pn": "3669",
      "abstract": [
        "Traditional supervised voice activity detection (VAD) methods work\nwell in clean and controlled scenarios, with performance severely degrading\nin real-world applications. One possible bottleneck is that speech\nin the wild contains unpredictable noise types, hence frame-level label\nprediction is difficult, which is required for traditional supervised\nVAD training. In contrast, we propose a general-purpose VAD (GPVAD)\nframework, which can be easily trained from noisy data in a weakly\nsupervised fashion, requiring only clip-level labels. We proposed two\nGPVAD models, one full (GPV-F), trained on 527 Audioset sound events,\nand one binary (GPV-B), only distinguishing speech and noise. We evaluate\nthe two GPV models against a CRNN based standard VAD model (VAD-C)\non three different evaluation protocols (clean, synthetic noise, real\ndata). Results show that our proposed GPV-F demonstrates competitive\nperformance in clean and synthetic scenarios compared to traditional\nVAD-C. Further, in real-world evaluation, GPV-F largely outperforms\nVAD-C in terms of frame-level evaluation metrics as well as segment-level\nones. With a much lower requirement for frame-labeled data, the naive\nbinary clip-level GPV-B model can still achieve comparable performance\nto VAD-C in real-world scenarios.\n"
      ],
      "doi": "10.21437/Interspeech.2020-995",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "lee20f_interspeech": {
      "authors": [
        [
          "Joohyung",
          "Lee"
        ],
        [
          "Youngmoon",
          "Jung"
        ],
        [
          "Hoirin",
          "Kim"
        ]
      ],
      "title": "Dual Attention in Time and Frequency Domain for Voice Activity Detection",
      "original": "0997",
      "page_count": 5,
      "order": 750,
      "p1": "3670",
      "pn": "3674",
      "abstract": [
        "Voice activity detection (VAD) is a challenging task in low signal-to-noise\nratio (SNR) environment, especially in non-stationary noise. To deal\nwith this issue, we propose a novel attention module that can be integrated\nin Long Short-Term Memory (LSTM). Our proposed attention module refines\neach LSTM layer&#8217;s hidden states so as to make it possible to\nadaptively focus on both time and frequency domain. Experiments are\nconducted on various noisy conditions using Aurora 4 database. Our\nproposed method obtains the 95.58% area under the ROC curve (AUC),\nachieving 22.05%relative improvement compared to baseline, with only\n2.44% increase in the number of parameters. Besides, we utilize focal\nloss for alleviating the performance degradation caused by imbalance\nbetween speech and non-speech sections in training sets. The results\nshow that the focal loss can improve the performance in various imbalance\nsituations compared to the cross entropy loss, a commonly used loss\nfunction in VAD.\n"
      ],
      "doi": "10.21437/Interspeech.2020-997",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "xu20e_interspeech": {
      "authors": [
        [
          "Tianjiao",
          "Xu"
        ],
        [
          "Hui",
          "Zhang"
        ],
        [
          "Xueliang",
          "Zhang"
        ]
      ],
      "title": "Polishing the Classical Likelihood Ratio Test by Supervised Learning for Voice Activity Detection",
      "original": "1177",
      "page_count": 5,
      "order": 751,
      "p1": "3675",
      "pn": "3679",
      "abstract": [
        "Voice activity detection (VAD) is essential for speech signal processing\nsystem, which desires low computational cost and high real-time processing.\nLikelihood ratio test (LRT) based VAD is a widely used and effective\napproach in many applications. However, it is still a challenge in\nlow signal-to-noise ratio (SNR) and non-stationary noisy scenario.\nTo cope with this challenge, we propose a supervised masking-based\nparameter estimation module with an adaptive threshold to improve the\nperformance of a state-of-the-art LRT based VAD. Moreover, considering\nreal-time processing, we compared the proposed with corresponding end-to-end\nsupervised learning approaches in various model sizes. Experimental\nresults show that the proposed method leads to consistently better\nperformance than both of the existing LRT based method and end-to-end\nsupervised learning based approaches.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1177",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "kumar20e_interspeech": {
      "authors": [
        [
          "Avinash",
          "Kumar"
        ],
        [
          "S.",
          "Shahnawazuddin"
        ],
        [
          "Waquar",
          "Ahmad"
        ]
      ],
      "title": "A Noise Robust Technique for Detecting Vowels in Speech Signals",
      "original": "1204",
      "page_count": 5,
      "order": 752,
      "p1": "3680",
      "pn": "3684",
      "abstract": [
        "In this work, we propose a novel and noise robust method for the detection\nof vowels in speech signals. The proposed approach combines variational\nmode decomposition (VMD) and non-local means (NLM) estimation for the\ndetection of vowels in a speech sequence. The VMD algorithm is used\nto determine a number of variational mode functions (VMFs). The lower-order\nVMFs represent the frequency contents corresponding to vowel regions.\nThus by combining the lower-order VMFs and reconstructing the speech\nsignal back, the energy corresponding to the vowel regions is enhanced\nwhile the non-vowel regions are suppressed. At the same time, the ill-effect\nof noise is also reduced. Finally, as reported in an earlier work,\napplication of NLM followed by convolution with first-order difference\nof Gaussian window is performed on the reconstructed signal to determine\nthe vowel region. The performance of proposed approach for the task\nof detecting vowels in speech is compared with three existing techniques\nand observed to be superior under clean as well as noisy test conditions.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1204",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "lavechin20b_interspeech": {
      "authors": [
        [
          "Marvin",
          "Lavechin"
        ],
        [
          "Marie-Philippe",
          "Gill"
        ],
        [
          "Ruben",
          "Bousbib"
        ],
        [
          "Herv\u00e9",
          "Bredin"
        ],
        [
          "Leibny Paola",
          "Garcia-Perera"
        ]
      ],
      "title": "End-to-End Domain-Adversarial Voice Activity Detection",
      "original": "2285",
      "page_count": 5,
      "order": 753,
      "p1": "3685",
      "pn": "3689",
      "abstract": [
        "Voice activity detection is the task of detecting speech regions in\na given audio stream or recording. First, we design a neural network\ncombining trainable filters and recurrent layers to tackle voice activity\ndetection directly from the waveform. Experiments on the challenging\nDIHARD dataset show that the proposed end-to-end model reaches state-of-the-art\nperformance and outperforms a variant where trainable filters are replaced\nby standard cepstral coefficients. Our second contribution aims at\nmaking the proposed voice activity detection model robust to domain\nmismatch. To that end, a domain classification branch is added to the\nnetwork and trained in an adversarial manner. The same DIHARD dataset,\ndrawn from 11 different domains is used for evaluation under two scenarios.\nIn the  in-domain scenario where the training and test sets cover the\nexact same domains, we show that the domain-adversarial approach does\nnot degrade performance of the proposed end-to-end model. In the  out-domain\nscenario where the test domain is different from training domains,\nit brings a relative improvement of more than 10%. Finally, our last\ncontribution is the provision of a fully reproducible open-source pipeline\nthan can be easily adapted to other datasets.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2285",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "agarwal20_interspeech": {
      "authors": [
        [
          "Ayush",
          "Agarwal"
        ],
        [
          "Jagabandhu",
          "Mishra"
        ],
        [
          "S.R. Mahadeva",
          "Prasanna"
        ]
      ],
      "title": "VOP Detection in Variable Speech Rate Condition",
      "original": "2326",
      "page_count": 5,
      "order": 754,
      "p1": "3690",
      "pn": "3694",
      "abstract": [
        "The Vowel onset point (VOP) is the location where the onset of vowel\ntakes place in a given speech segment. Many speech processing applications\nneed the information of VOP to extract features from the speech signal.\nIn such cases the overall performance largely depends on the exact\ndetection of VOP location. There are many algorithms proposed in the\nliterature for the automatic detection of VOPs. Most of these methods\nassume that the given speech signal is produced at normal speech rate.\nAll the parameters for smoothing speech signal evidence as well as\nhypothesizing VOPs are set accordingly. However, these parameter settings\nmay not work well for variable speech rate conditions. This work proposes\na dynamic first order Gaussian differentiator (FOGD) window based approach\nto overcome this issue. The proposed approach is evaluated using a\nsubset of TIMIT dataset with manually marked ground truth VOPs. The\nevaluated performance of VOP detection by using the proposed approach\nshows improvement when compared with the existing approach at higher\nand lower speech rate conditions.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2326",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "zheng20d_interspeech": {
      "authors": [
        [
          "Zhenpeng",
          "Zheng"
        ],
        [
          "Jianzong",
          "Wang"
        ],
        [
          "Ning",
          "Cheng"
        ],
        [
          "Jian",
          "Luo"
        ],
        [
          "Jing",
          "Xiao"
        ]
      ],
      "title": "MLNET: An Adaptive Multiple Receptive-Field Attention Neural Network for Voice Activity Detection",
      "original": "2392",
      "page_count": 5,
      "order": 755,
      "p1": "3695",
      "pn": "3699",
      "abstract": [
        "Voice activity detection (VAD) makes a distinction between speech and\nnon-speech and its performance is of crucial importance for speech\nbased services. Recently, deep neural network (DNN)-based VADs have\nachieved better performance than conventional signal processing methods.\nThe existed DNN-based models always handcrafted a fixed window to make\nuse of the contextual speech information to improve the performance\nof VAD. However, the fixed window of contextual speech information\ncan&#8217;t handle various unpredictable noise environments and highlight\nthe critical speech information to VAD task. In order to solve this\nproblem, this paper proposed an adaptive multiple receptive-field attention\nneural network, called MLNET, to finish VAD task. The MLNET leveraged\nmulti-branches to extract multiple contextual speech information and\ninvestigated an effective attention block to weight the most crucial\nparts of the context for final classification. Experiments in real-world\nscenarios demonstrated that the proposed MLNET-based model outperformed\nother baselines.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2392",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "kreuk20_interspeech": {
      "authors": [
        [
          "Felix",
          "Kreuk"
        ],
        [
          "Joseph",
          "Keshet"
        ],
        [
          "Yossi",
          "Adi"
        ]
      ],
      "title": "Self-Supervised Contrastive Learning for Unsupervised Phoneme Segmentation",
      "original": "2398",
      "page_count": 5,
      "order": 756,
      "p1": "3700",
      "pn": "3704",
      "abstract": [
        "We propose a self-supervised representation learning model for the\ntask of unsupervised phoneme boundary detection. The model is a convolutional\nneural network that operates directly on the raw waveform. It is optimized\nto identify spectral changes in the signal using the Noise-Contrastive\nEstimation principle. At test time, a peak detection algorithm is applied\nover the model outputs to produce the final boundaries. As such, the\nproposed model is trained in a fully unsupervised manner with no manual\nannotations in the form of target boundaries nor phonetic transcriptions.\nWe compare the proposed approach to several unsupervised baselines\nusing both TIMIT and Buckeye corpora. Results suggest that our approach\nsurpasses the baseline models and reaches state-of-the-art performance\non both data sets. Furthermore, we experimented with expanding the\ntraining set with additional examples from the Librispeech corpus.\nWe evaluated the resulting model on distributions and languages that\nwere not seen during the training phase (English, Hebrew and German)\nand showed that utilizing additional untranscribed data is beneficial\nfor model performance. Our implementation is available at: <KBD>https://github.com/felixkreuk/UnsupSeg</KBD>\n"
      ],
      "doi": "10.21437/Interspeech.2020-2398",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "zelasko20_interspeech": {
      "authors": [
        [
          "Piotr",
          "\u017belasko"
        ],
        [
          "Laureano",
          "Moro-Vel\u00e1zquez"
        ],
        [
          "Mark",
          "Hasegawa-Johnson"
        ],
        [
          "Odette",
          "Scharenborg"
        ],
        [
          "Najim",
          "Dehak"
        ]
      ],
      "title": "That Sounds Familiar: An Analysis of Phonetic Representations Transfer Across Languages",
      "original": "2513",
      "page_count": 5,
      "order": 757,
      "p1": "3705",
      "pn": "3709",
      "abstract": [
        "Only a handful of the world&#8217;s languages are abundant with the\nresources that enable practical applications of speech processing technologies.\nOne of the methods to overcome this problem is to use the resources\nexisting in other languages to train a multilingual automatic speech\nrecognition (ASR) model, which, intuitively, should learn some universal\nphonetic representations. In this work, we focus on gaining a deeper\nunderstanding of how general these representations might be, and how\nindividual phones are getting improved in a multilingual setting. To\nthat end, we select a phonetically diverse set of languages, and perform\na series of monolingual, multilingual and crosslingual (zero-shot)\nexperiments. The ASR is trained to recognize the International Phonetic\nAlphabet (IPA) token sequences. We observe significant improvements\nacross all languages in the multilingual setting, and stark degradation\nin the crosslingual setting, where the model, among other errors, considers\nJavanese as a tone language. Notably, as little as 10 hours of the\ntarget language training data tremendously reduces ASR error rates.\nOur analysis uncovered that even the phones that are unique to a single\nlanguage can benefit greatly from adding training data from other languages\n&#8212; an encouraging result for the low-resource speech community.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2513",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "limonard20_interspeech": {
      "authors": [
        [
          "S.",
          "Limonard"
        ],
        [
          "Catia",
          "Cucchiarini"
        ],
        [
          "R.W.N.M. van",
          "Hout"
        ],
        [
          "Helmer",
          "Strik"
        ]
      ],
      "title": "Analyzing Read Aloud Speech by Primary School Pupils: Insights for Research and Development",
      "original": "2804",
      "page_count": 5,
      "order": 758,
      "p1": "3710",
      "pn": "3714",
      "abstract": [
        "Reading software based on Automatic Speech Recognition (ASR) has been\nproposed as a possible supplement to traditional classroom instruction\nto help pupils achieve the required level of reading proficiency. However,\nthe knowledge required to develop such software is not always available,\nespecially for languages other than English. To this end, we analyzed\na corpus containing speech material from Dutch native primary school\npupils who read texts aloud at their mastery reading level. We investigated\nreading strategies, reading miscues, a novel reading miscue index and\ntheir relationship with AVI level (reading level) and gender. We found\na significant effect of AVI level on reading miscue index, but did\nnot find a decrease of reading miscue index as AVI level increased.\nPupils mostly used lexical reading strategies, which seem to increase\nwhen AVI level increases. Miscues most frequently concerned low-frequency\nwords with at least two syllables, and omitted and inserted words were\ngenerally high frequent, unstressed function words. These results provide\ninsights that help design the content of reading interventions and\nthat can contribute to developing and improving ASR-based reading software.\nWe discuss the results in view of current trends in education and technology,\nand their implications for future research and development.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2804",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "rasilo20_interspeech": {
      "authors": [
        [
          "Heikki",
          "Rasilo"
        ],
        [
          "Yannick",
          "Jadoul"
        ]
      ],
      "title": "Discovering Articulatory Speech Targets from Synthesized Random Babble",
      "original": "3186",
      "page_count": 5,
      "order": 759,
      "p1": "3715",
      "pn": "3719",
      "abstract": [
        "In several areas of speech research, articulatory models able to produce\na wide variety of speech sounds, not specific to any language, are\nneeded as a starting point. Such research fields include the studies\nof sound system emergence in populations, infant speech acquisition\nresearch, and speech inversion research. Here we approach the problem\nof exploring the possible acoustic outcomes of a dynamic articulatory\nmodel efficiently, and provide an entropy based measure for the diversity\nof the explored articulations. Our exploration algorithm incrementally\nclusters produced babble into a number of target articulations, aiming\nto produce maximally interesting acoustic outcomes. Consonant gestures\nare defined as a subset of articulatory parameters and are thus superposed\non vowel context, to provide a coarticulation effect. We show that\nthe proposed algorithm explores the acoustic domain more efficiently\nthan random target selection, and clusters the articulatory domain\ninto a number of usable articulatory targets.\n"
      ],
      "doi": "10.21437/Interspeech.2020-3186",
      "author_area_id": "1",
      "author_area_label": "Speech Perception, Production and Acquisition"
    },
    "csapo20c_interspeech": {
      "authors": [
        [
          "Tam\u00e1s G\u00e1bor",
          "Csap\u00f3"
        ]
      ],
      "title": "Speaker Dependent Acoustic-to-Articulatory Inversion Using Real-Time MRI of the Vocal Tract",
      "original": "0016",
      "page_count": 5,
      "order": 760,
      "p1": "3720",
      "pn": "3724",
      "abstract": [
        "Acoustic-to-articulatory inversion (AAI) methods estimate articulatory\nmovements from the acoustic speech signal, which can be useful in several\ntasks such as speech recognition, synthesis, talking heads and language\ntutoring. Most earlier inversion studies are based on point-tracking\narticulatory techniques (e.g. EMA or XRMB). The advantage of rtMRI\nis that it provides dynamic information about the full midsagittal\nplane of the upper airway, with a high &#8216;relative&#8217; spatial\nresolution. In this work, we estimated midsagittal rtMRI images of\nthe vocal tract for speaker dependent AAI, using MGC-LSP spectral features\nas input. We applied FC-DNNs, CNNs and recurrent neural networks, and\nhave shown that LSTMs are the most suitable for this task. As objective\nevaluation we measured normalized MSE, Structural Similarity Index\n(SSIM) and its complex wavelet version (CW-SSIM). The results indicate\nthat the combination of FC-DNNs and LSTMs can achieve smooth generated\nMR images of the vocal tract, which are similar to the original MRI\nrecordings (average CW-SSIM: 0.94).\n"
      ],
      "doi": "10.21437/Interspeech.2020-16",
      "author_area_id": "1",
      "author_area_label": "Speech Perception, Production and Acquisition"
    },
    "bozorg20_interspeech": {
      "authors": [
        [
          "Narjes",
          "Bozorg"
        ],
        [
          "Michael T.",
          "Johnson"
        ]
      ],
      "title": "Acoustic-to-Articulatory Inversion with Deep Autoregressive Articulatory-WaveNet",
      "original": "1875",
      "page_count": 5,
      "order": 761,
      "p1": "3725",
      "pn": "3729",
      "abstract": [
        "This paper presents a novel deep autoregressive method for Acoustic-to-Articulatory\nInversion called Articulatory-WaveNet. In traditional methods such\nas Gaussian Mixture Model-Hidden Markov Model (GMM-HMM), mapping the\nframe-level interdependency of observations has not been considered.\nWe address this problem by introducing the Articulatory-WaveNet with\ndilated causal convolutional layers to predict the articulatory trajectories\nfrom acoustic feature sequences. This new model has an average Root\nMean Square Error (RMSE) of 1.08mm and a correlation of 0.82 on the\nEnglish speaker subset of the ElectroMagnetic Articulography-Mandarin\nAccented English (EMA-MAE) corpus. Articulatory-WaveNet represents\nan improvement of 59% for RMSE and 30% for correlation over the previous\nGMM-HMM based inversion model. To the best of our knowledge, this paper\nintroduces the first application of a WaveNet synthesis approach to\nthe problem of Acoustic-to-Articulatory Inversion, and results are\ncomparable to or better than the best currently published systems.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1875",
      "author_area_id": "1",
      "author_area_label": "Speech Perception, Production and Acquisition"
    },
    "douros20_interspeech": {
      "authors": [
        [
          "Ioannis K.",
          "Douros"
        ],
        [
          "Ajinkya",
          "Kulkarni"
        ],
        [
          "Chrysanthi",
          "Dourou"
        ],
        [
          "Yu",
          "Xie"
        ],
        [
          "Jacques",
          "Felblinger"
        ],
        [
          "Karyna",
          "Isaieva"
        ],
        [
          "Pierre-Andr\u00e9",
          "Vuissoz"
        ],
        [
          "Yves",
          "Laprie"
        ]
      ],
      "title": "Using Silence MR Image to Synthesise Dynamic MRI Vocal Tract Data of CV",
      "original": "1173",
      "page_count": 5,
      "order": 762,
      "p1": "3730",
      "pn": "3734",
      "abstract": [
        "In this work we present an algorithm for synthesising pseudo rtMRI\ndata of the vocal tract. rtMRI data on the midsagittal plane were used\nto synthesise target consonant-vowel (CV) using only a silence frame\nof the target speaker. For this purpose, several single speaker models\nwere created. The input of the algorithm is a silence frame of both\ntrain and target speaker and the rtMRI data of the target CV. An image\ntransformation is computed from each CV frame to the next one, creating\na set of transformations that describe the dynamics of the CV production.\nAnother image transformation is computed from the silence frame of\ntrain speaker to the silence frame of the target speaker and is used\nto adapt the set of transformations computed previously to the target\nspeaker. The adapted set of transformations is applied to the silence\nof the target speaker to synthesise his/her CV pseudo rtMRI data. Synthesised\nimages from multiple single speaker models are frame aligned and then\naveraged to create the final version of synthesised images. Synthesised\nimages are compared with the original ones using image cross-correlation.\nResults show good agreement between the synthesised and the original\nimages.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1173",
      "author_area_id": "1",
      "author_area_label": "Speech Perception, Production and Acquisition"
    },
    "csapo20d_interspeech": {
      "authors": [
        [
          "Tam\u00e1s G\u00e1bor",
          "Csap\u00f3"
        ],
        [
          "Kele",
          "Xu"
        ]
      ],
      "title": "Quantification of Transducer Misalignment in Ultrasound Tongue Imaging",
      "original": "1672",
      "page_count": 5,
      "order": 763,
      "p1": "3735",
      "pn": "3739",
      "abstract": [
        "In speech production research, different imaging modalities have been\nemployed to obtain accurate information about the movement and shaping\nof the vocal tract. Ultrasound is an affordable and non-invasive imaging\nmodality with relatively high temporal and spatial resolution to study\nthe dynamic behavior of tongue during speech production. However, a\nlong-standing problem for ultrasound tongue imaging is the transducer\nmisalignment during longer data recording sessions. In this paper,\nwe propose a simple, yet effective, misalignment quantification approach.\nThe analysis employs MSE distance and two similarity measurement metrics\nto identify the relative displacement between the chin and the transducer.\nWe visualize these measures as a function of the timestamp of the utterances.\nExtensive experiments are conducted on a Hungarian and Scottish English\nchild dataset. The results suggest that large values of Mean Square\nError (MSE) and small values of Structural Similarity Index (SSIM)\nand Complex Wavelet SSIM indicate corruptions or issues during the\ndata recordings, which can either be caused by transducer misalignment\nor lack of gel.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1672",
      "author_area_id": "1",
      "author_area_label": "Speech Perception, Production and Acquisition"
    },
    "parrot20_interspeech": {
      "authors": [
        [
          "Maud",
          "Parrot"
        ],
        [
          "Juliette",
          "Millet"
        ],
        [
          "Ewan",
          "Dunbar"
        ]
      ],
      "title": "Independent and Automatic Evaluation of Speaker-Independent Acoustic-to-Articulatory Reconstruction",
      "original": "1746",
      "page_count": 5,
      "order": 764,
      "p1": "3740",
      "pn": "3744",
      "abstract": [
        "Reconstruction of articulatory trajectories from the acoustic speech\nsignal has been proposed for improving speech recognition and text-to-speech\nsynthesis. However, to be useful in these settings, articulatory reconstruction\nmust be speaker-independent. Furthermore, as most research focuses\non single, small data sets with few speakers, robust articulatory reconstruction\ncould profit from combining data sets. Standard evaluation measures\nsuch as root mean squared error and Pearson correlation are inappropriate\nfor evaluating the speaker-independence of models or the usefulness\nof combining data sets. We present a new evaluation for articulatory\nreconstruction which is independent of the articulatory data set used\nfor training: the  phone discrimination ABX task. We use the ABX measure\nto evaluate a bi-LSTM based model trained on three data sets (14 speakers),\nand show that it gives information complementary to standard measures,\nenabling us to evaluate the effects of data set merging, as well as\nthe speaker independence of the model.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1746",
      "author_area_id": "1",
      "author_area_label": "Speech Perception, Production and Acquisition"
    },
    "diener20b_interspeech": {
      "authors": [
        [
          "Lorenz",
          "Diener"
        ],
        [
          "Mehrdad Roustay",
          "Vishkasougheh"
        ],
        [
          "Tanja",
          "Schultz"
        ]
      ],
      "title": "CSL-EMG_Array: An Open Access Corpus for EMG-to-Speech Conversion",
      "original": "2859",
      "page_count": 5,
      "order": 765,
      "p1": "3745",
      "pn": "3749",
      "abstract": [
        "We present a new open access corpus for the training and evaluation\nof EMG-to-Speech conversion systems based on array electromyographic\nrecordings. The corpus is recorded with a recording paradigm closely\nmirroring realistic EMG-to-Speech usage scenarios, and includes evaluation\ndata recorded from both audible as well as silent speech. The corpus\nconsists of 9.5 hours of data, split into 12 sessions recorded from\n8 speakers. Based on this corpus, we present initial benchmark results\nwith a realistic online EMG-to-Speech conversion use case, both for\nthe audible and silent speech subsets. We also present a method for\ndrastically improving EMG-to-Speech system stability and performance\nin the presence of time-related artifacts.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2859",
      "author_area_id": "1",
      "author_area_label": "Speech Perception, Production and Acquisition"
    },
    "penney20_interspeech": {
      "authors": [
        [
          "Joshua",
          "Penney"
        ],
        [
          "Felicity",
          "Cox"
        ],
        [
          "Anita",
          "Szakay"
        ]
      ],
      "title": "Links Between Production and Perception of Glottalisation in Individual Australian English Speaker/Listeners",
      "original": "1175",
      "page_count": 5,
      "order": 766,
      "p1": "3750",
      "pn": "3754",
      "abstract": [
        "Glottalisation of coda stops is a recent change in Australian English.\nPrevious studies have shown that speakers use glottalisation to signal\ncoda stop voicelessness in production, and that listeners interpret\nglottalisation as cueing coda stop voicelessness in perception. As\nis to be expected for a recent change, younger speakers glottalise\nmore than older speakers, but in perception both age groups appear\nto use glottalisation similarly. This study examines whether links\nbetween the production and perception of glottalisation exist at the\nlevel of the individual. We determined how frequently individuals used\nglottalisation in production, and analysed this against how heavily\nthe same individuals weighted glottalisation in perception. Although\ndifferences have previously been found at the age group level, at the\nlevel of the individual we found no correlation between how heavily\nlisteners weighted glottalisation in perception and how frequently\nthey used glottalisation in production for either the younger or the\nolder listeners. Nevertheless, we did find a small number of individuals\nwho exhibited an alignment of their production and perception repertoires,\nwhich may suggest that only a small proportion of individuals exhibit\na strong production-perception link, and we propose that these individuals\nmay be important for driving the progression of change.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1175",
      "author_area_id": "1",
      "author_area_label": "Speech Perception, Production and Acquisition"
    },
    "siriwardhana20_interspeech": {
      "authors": [
        [
          "Shamane",
          "Siriwardhana"
        ],
        [
          "Andrew",
          "Reis"
        ],
        [
          "Rivindu",
          "Weerasekera"
        ],
        [
          "Suranga",
          "Nanayakkara"
        ]
      ],
      "title": "Jointly Fine-Tuning &#8220;BERT-Like&#8221; Self Supervised Models to Improve Multimodal Speech Emotion Recognition",
      "original": "1212",
      "page_count": 5,
      "order": 767,
      "p1": "3755",
      "pn": "3759",
      "abstract": [
        "Multimodal emotion recognition from the speech is an important area\nin affective computing. Fusing multiple data modalities and learning\nrepresentations with limited amounts of labeled data is a challenging\ntask. In this paper, we explore the use of modality specific &#8220;BERT-like&#8221;\npretrained Self Supervised Learning (SSL) architectures to represent\nboth speech and text modalities for the task of multimodal speech emotion\nrecognition. By conducting experiments on three publicly available\ndatasets (IEMOCAP, CMU-MOSEI, and CMU-MOSI), we show that jointly fine-tuning\n&#8220;BERT-like&#8221; SSL architectures achieve state-of-the-art\n(SOTA) results. We also evaluate two methods of fusing speech and text\nmodalities and show that a simple fusion mechanism can outperform more\ncomplex ones when using SSL models that have similar architectural\nproperties to BERT.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1212",
      "author_area_id": "13",
      "author_area_label": "Special Sessions"
    },
    "chung20e_interspeech": {
      "authors": [
        [
          "Yu-An",
          "Chung"
        ],
        [
          "Hao",
          "Tang"
        ],
        [
          "James",
          "Glass"
        ]
      ],
      "title": "Vector-Quantized Autoregressive Predictive Coding",
      "original": "1228",
      "page_count": 5,
      "order": 768,
      "p1": "3760",
      "pn": "3764",
      "abstract": [
        "Autoregressive Predictive Coding (APC), as a self-supervised objective,\nhas enjoyed success in learning representations from large amounts\nof unlabeled data, and the learned representations are rich for many\ndownstream tasks. However, the connection between low self-supervised\nloss and strong performance in downstream tasks remains unclear. In\nthis work, we propose Vector-Quantized Autoregressive Predictive Coding\n(VQ-APC), a novel model that produces quantized representations, allowing\nus to explicitly control the amount of information encoded in the representations.\nBy studying a sequence of increasingly limited models, we reveal the\nconstituents of the learned representations. In particular, we confirm\nthe presence of information with probing tasks, while showing the \nabsence of information with mutual information, uncovering the model&#8217;s\npreference in preserving speech information as its capacity becomes\nconstrained. We find that there exists a point where phonetic and speaker\ninformation are amplified to maximize a self-supervised objective.\nAs a byproduct, the learned codes for a particular model capacity correspond\nwell to English phones.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1228",
      "author_area_id": "13",
      "author_area_label": "Special Sessions"
    },
    "song20d_interspeech": {
      "authors": [
        [
          "Xingchen",
          "Song"
        ],
        [
          "Guangsen",
          "Wang"
        ],
        [
          "Yiheng",
          "Huang"
        ],
        [
          "Zhiyong",
          "Wu"
        ],
        [
          "Dan",
          "Su"
        ],
        [
          "Helen",
          "Meng"
        ]
      ],
      "title": "Speech-XLNet: Unsupervised Acoustic Model Pretraining for Self-Attention Networks",
      "original": "1511",
      "page_count": 5,
      "order": 769,
      "p1": "3765",
      "pn": "3769",
      "abstract": [
        "Self-attention network (SAN) can benefit significantly from the bi-directional\nrepresentation learning through unsupervised pre-training paradigms\nsuch as BERT and XLNet. In this paper, we present an XLNet-like pretraining\nscheme &#8220;Speech-XLNet&#8221; to learn speech representations with\nself-attention networks (SANs). Firstly, we find that by shuffling\nthe speech frame orders, Speech-XLNet serves as a strong regularizer\nwhich encourages the SAN network to make inferences by focusing on\nglobal structures through its attention weights. Secondly, Speech-XLNet\nalso allows the model to explore bi-directional context information\nwhile maintaining the autoregressive training manner. Visualization\nresults show that our approach can generalize better with more flattened\nand widely distributed optimas compared to the conventional approach.\nExperimental results on TIMIT demonstrate that Speech-XLNet greatly\nimproves hybrid SAN/HMM in terms of both convergence speed and recognition\naccuracy. Our best systems achieve a relative improvement of 15.2%\non the TIMIT task. Besides, we also apply our pretrained model to an\nEnd-to-End SAN with WSJ dataset and WER is reduced by up to 68% when\nonly a few hours of transcribed data is used.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1511",
      "author_area_id": "13",
      "author_area_label": "Special Sessions"
    },
    "singh20c_interspeech": {
      "authors": [
        [
          "Kritika",
          "Singh"
        ],
        [
          "Vimal",
          "Manohar"
        ],
        [
          "Alex",
          "Xiao"
        ],
        [
          "Sergey",
          "Edunov"
        ],
        [
          "Ross",
          "Girshick"
        ],
        [
          "Vitaliy",
          "Liptchinsky"
        ],
        [
          "Christian",
          "Fuegen"
        ],
        [
          "Yatharth",
          "Saraf"
        ],
        [
          "Geoffrey",
          "Zweig"
        ],
        [
          "Abdelrahman",
          "Mohamed"
        ]
      ],
      "title": "Large Scale Weakly and Semi-Supervised Learning for Low-Resource Video ASR",
      "original": "1917",
      "page_count": 5,
      "order": 770,
      "p1": "3770",
      "pn": "3774",
      "abstract": [
        "Many semi- and weakly-supervised approaches have been investigated\nfor overcoming the labeling cost of building high-quality speech recognition\nsystems. On the challenging task of transcribing social media videos\nin low-resource conditions, we conduct a large scale systematic comparison\nbetween two self-labeling methods on one hand, and weakly-supervised\npretraining using contextual metadata on the other. We investigate\ndistillation methods at the frame level and the sequence level for\nhybrid, encoder-only Connectionist Temporal Classification (CTC) based,\nand encoder-decoder speech recognition systems on Dutch and Romanian\nlanguages using 27,000 and 58,000 hours of unlabeled audio respectively.\nAlthough all approaches improved upon their respective baseline word\nerror rates (WER) by more than 8%, sequence-level distillation for\nencoder-decoder models provided the largest relative WER reduction\nof 20% compared to the strongest data-augmented supervised baseline.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1917",
      "author_area_id": "13",
      "author_area_label": "Special Sessions"
    },
    "kumatani20_interspeech": {
      "authors": [
        [
          "Kenichi",
          "Kumatani"
        ],
        [
          "Dimitrios",
          "Dimitriadis"
        ],
        [
          "Yashesh",
          "Gaur"
        ],
        [
          "Robert",
          "Gmyr"
        ],
        [
          "Sefik Emre",
          "Eskimez"
        ],
        [
          "Jinyu",
          "Li"
        ],
        [
          "Michael",
          "Zeng"
        ]
      ],
      "title": "Sequence-Level Self-Learning with Multiple Hypotheses",
      "original": "2020",
      "page_count": 5,
      "order": 771,
      "p1": "3775",
      "pn": "3779",
      "abstract": [
        "In this work, we develop new self-learning techniques with an attention-based\nsequence-to-sequence (seq2seq) model for automatic speech recognition\n(ASR). For untranscribed speech data, the hypothesis from an ASR system\nmust be used as a label. However, the imperfect ASR result makes unsupervised\nlearning difficult to consistently improve recognition performance\nespecially in the case that multiple powerful teacher models are unavailable.\nIn contrast to conventional unsupervised learning approaches, we adopt\nthe  multi-task learning (MTL) framework where the n-th best ASR hypothesis\nis used as the label of each task. The seq2seq network is updated through\nthe MTL framework so as to find the common representation that can\ncover multiple hypotheses. By doing so, the effect of the  hard-decision\nerrors can be alleviated. We first demonstrate the effectiveness of\nour self-learning methods through ASR experiments in an accent adaptation\ntask between the US and British English speech. Our experiment results\nshow that our method can reduce the WER on the British speech data\nfrom 14.55% to 10.36% compared to the baseline model trained with the\nUS English data only. Moreover, we investigate the effect of our proposed\nmethods in a federated learning scenario.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2020",
      "author_area_id": "13",
      "author_area_label": "Special Sessions"
    },
    "wu20m_interspeech": {
      "authors": [
        [
          "Haibin",
          "Wu"
        ],
        [
          "Andy T.",
          "Liu"
        ],
        [
          "Hung-yi",
          "Lee"
        ]
      ],
      "title": "Defense for Black-Box Attacks on Anti-Spoofing Models by Self-Supervised Learning",
      "original": "2026",
      "page_count": 5,
      "order": 772,
      "p1": "3780",
      "pn": "3784",
      "abstract": [
        "High-performance anti-spoofing models for automatic speaker verification\n(ASV), have been widely used to protect ASV by identifying and filtering\nspoofing audio that is deliberately generated by text-to-speech, voice\nconversion, audio replay, etc. However, it has been shown that high-performance\nanti-spoofing models are vulnerable to adversarial attacks. Adversarial\nattacks, that are indistinguishable from original data but result in\nthe incorrect predictions, are dangerous for anti-spoofing models and\nnot in dispute we should detect them at any cost. To explore this issue,\nwe proposed to employ Mockingjay, a self-supervised learning based\nmodel, to protect anti-spoofing models against adversarial attacks\nin the black-box scenario. Self-supervised learning models are effective\nin improving downstream task performance like phone classification\nor ASR. However, their effect in defense for adversarial attacks has\nnot been explored yet. In this work, we explore the robustness of self-supervised\nlearned high-level representations by using them in the defense against\nadversarial attacks. A layerwise noise-to-signal ratio (LNSR) is proposed\nto quantize and measure the effectiveness of deep models in countering\nadversarial noise. Experimental results on the ASVspoof 2019 dataset\ndemonstrate that high-level representations extracted by Mockingjay\ncan prevent the transferability of adversarial examples, and successfully\ncounter black-box attacks.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2026",
      "author_area_id": "13",
      "author_area_label": "Special Sessions"
    },
    "yang20i_interspeech": {
      "authors": [
        [
          "Shu-wen",
          "Yang"
        ],
        [
          "Andy T.",
          "Liu"
        ],
        [
          "Hung-yi",
          "Lee"
        ]
      ],
      "title": "Understanding Self-Attention of Self-Supervised Audio Transformers",
      "original": "2231",
      "page_count": 5,
      "order": 773,
      "p1": "3785",
      "pn": "3789",
      "abstract": [
        "Self-supervised Audio Transformers (SAT) enable great success in many\ndownstream speech applications like ASR, but how they work has not\nbeen widely explored yet. In this work, we present multiple strategies\nfor the analysis of attention mechanisms in SAT. We categorize attentions\ninto explainable categories, where we discover each category possesses\nits own unique functionality. We provide a visualization tool for understanding\nmulti-head self-attention, importance ranking strategies for identifying\ncritical attention, and attention refinement techniques to improve\nmodel performance.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2231",
      "author_area_id": "13",
      "author_area_label": "Special Sessions"
    },
    "khurana20_interspeech": {
      "authors": [
        [
          "Sameer",
          "Khurana"
        ],
        [
          "Antoine",
          "Laurent"
        ],
        [
          "Wei-Ning",
          "Hsu"
        ],
        [
          "Jan",
          "Chorowski"
        ],
        [
          "Adrian",
          "Lancucki"
        ],
        [
          "Ricard",
          "Marxer"
        ],
        [
          "James",
          "Glass"
        ]
      ],
      "title": "A Convolutional Deep Markov Model for Unsupervised Speech Representation Learning",
      "original": "3084",
      "page_count": 5,
      "order": 774,
      "p1": "3790",
      "pn": "3794",
      "abstract": [
        "Probabilistic Latent Variable Models (LVMs) provide an alternative\nto self-supervised learning approaches for linguistic representation\nlearning from speech. LVMs admit an intuitive probabilistic interpretation\nwhere the latent structure shapes the information extracted from the\nsignal. Even though LVMs have recently seen a renewed interest due\nto the introduction of Variational Autoencoders (VAEs), their use for\nspeech representation learning remains largely unexplored. In this\nwork, we propose Convolutional Deep Markov Model (ConvDMM), a Gaussian\nstate-space model with non-linear emission and transition functions\nmodelled by deep neural networks. This unsupervised model is trained\nusing black box variational inference. A deep convolutional neural\nnetwork is used as an inference network for structured variational\napproximation. When trained on a large scale speech dataset (LibriSpeech),\nConvDMM produces features that significantly outperform multiple self-supervised\nfeature extracting methods on linear phone classification and recognition\non the Wall Street Journal dataset. Furthermore, we found that ConvDMM\ncomplements self-supervised methods like Wav2Vec and PASE, improving\non the results achieved with any of the methods alone. Lastly, we find\nthat ConvDMM features enable learning better phone recognizers than\nany other features in an extreme low-resource regime with few labelled\ntraining examples.\n"
      ],
      "doi": "10.21437/Interspeech.2020-3084",
      "author_area_id": "13",
      "author_area_label": "Special Sessions"
    },
    "abulimiti20_interspeech": {
      "authors": [
        [
          "Ayimunishagu",
          "Abulimiti"
        ],
        [
          "Jochen",
          "Weiner"
        ],
        [
          "Tanja",
          "Schultz"
        ]
      ],
      "title": "Automatic Speech Recognition for ILSE-Interviews: Longitudinal Conversational Speech Recordings Covering Aging and Cognitive Decline",
      "original": "2829",
      "page_count": 5,
      "order": 775,
      "p1": "3795",
      "pn": "3799",
      "abstract": [
        "The  Interdisciplinary Longitudinal Study on Adult Development and\nAging (ILSE) was initiated with the aim to investigate satisfying and\nhealthy aging. Over 20 years, about 4200 hours of biographic interviews\nfrom more than 1,000 participants were recorded. Spoken language is\na strong indicator for declining cognitive resources, as it is affected\nin early stage. Hence, various research topics related to aging like\ndementia, could be analyzed based on data such as the ILSE interviews.\nThe analysis of language capabilities requires transcribed speech.\nSince manual transcriptions are time and cost consuming, we aim to\nautomatically transcribing the ILSE data using Automatic Speech Recognition\n(ASR). The recognition of ILSE interviews is very demanding due to\nthe combination of various challenges: 20 year old analog two-speaker\none-channel recordings of low signal quality, emotional and personal\ninterviews between doctor and participant, and repeated recordings\nof aging, partly fragile individuals. In this study, we describe ongoing\nwork to develop hybrid Hidden Markov Model (HMM)- Deep Neural Network\n(DNN) based ASR system for the ILSE corpus. So far, the best ASR system\nis obtained by second-pass decoding of a hybrid HMM-DNN model using\nrecurrent neural network based language models with a word error rate\nof 50.39%.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2829",
      "author_area_id": "13",
      "author_area_label": "Special Sessions"
    },
    "zhou20e_interspeech": {
      "authors": [
        [
          "Dao",
          "Zhou"
        ],
        [
          "Longbiao",
          "Wang"
        ],
        [
          "Kong Aik",
          "Lee"
        ],
        [
          "Yibo",
          "Wu"
        ],
        [
          "Meng",
          "Liu"
        ],
        [
          "Jianwu",
          "Dang"
        ],
        [
          "Jianguo",
          "Wei"
        ]
      ],
      "title": "Dynamic Margin Softmax Loss for Speaker Verification",
      "original": "1106",
      "page_count": 5,
      "order": 776,
      "p1": "3800",
      "pn": "3804",
      "abstract": [
        "We propose a dynamic-margin softmax loss for the training of deep speaker\nembedding neural network. Our proposal is inspired by the additive-margin\nsoftmax (AM-Softmax) loss reported earlier. In AM-Softmax loss, a constant\nmargin is used for all training samples. However, the angle between\nthe feature vector and the ground-truth class center is rarely the\nsame for all samples. Furthermore, the angle also changes during training.\nThus, it is more reasonable to set a dynamic margin for each training\nsample. In this paper, we propose to dynamically set the margin of\neach training sample commensurate with the cosine angle of that sample,\nhence, the name dynamic-additive-margin softmax (DAM-Softmax) loss.\nMore specifically, the smaller the cosine angle is, the larger the\nmargin between the training sample and the corresponding class in the\nfeature space should be to promote intra-class compactness. Experimental\nresults show that the proposed DAM-Softmax loss achieves state-of-the-art\nperformance on the VoxCeleb dataset by 1.94% in equal error rate (EER).\nIn addition, our method also outperforms AM-Softmax loss when evaluated\non the Speakers in the Wild (SITW) corpus.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1106",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "rybicka20_interspeech": {
      "authors": [
        [
          "Magdalena",
          "Rybicka"
        ],
        [
          "Konrad",
          "Kowalczyk"
        ]
      ],
      "title": "On Parameter Adaptation in Softmax-Based Cross-Entropy Loss for Improved Convergence Speed and Accuracy in DNN-Based Speaker Recognition",
      "original": "2264",
      "page_count": 5,
      "order": 777,
      "p1": "3805",
      "pn": "3809",
      "abstract": [
        "In various classification tasks the major challenge is in generating\ndiscriminative representation of classes. By proper selection of deep\nneural network (DNN) loss function we can encourage it to produce embeddings\nwith increased inter-class separation and smaller intra-class distances.\nIn this paper, we develop softmax-based cross-entropy loss function\nwhich adapts its parameters to the current training phase. The proposed\nsolution improves accuracy up to 24% in terms of Equal Error Rate (EER)\nand minimum Detection Cost Function (minDCF). In addition, our proposal\nalso accelerates network convergence compared with other state-of-the-art\nsoftmax-based losses. As an additional contribution of this paper,\nwe adopt and subsequently modify the ResNet DNN structure for the speaker\nrecognition task. The proposed ResNet network achieves relative gains\nof up to 32% and 15% in terms of EER and minDCF respectively, compared\nwith the well-established Time Delay Neural Network (TDNN) architecture\nfor x-vector extraction.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2264",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "mingote20_interspeech": {
      "authors": [
        [
          "Victoria",
          "Mingote"
        ],
        [
          "Antonio",
          "Miguel"
        ],
        [
          "Alfonso",
          "Ortega"
        ],
        [
          "Eduardo",
          "Lleida"
        ]
      ],
      "title": "Training Speaker Enrollment Models by Network Optimization",
      "original": "2325",
      "page_count": 5,
      "order": 778,
      "p1": "3810",
      "pn": "3814",
      "abstract": [
        "In this paper, we present a new approach for the enrollment process\nin a deep neural network (DNN) system which learns the speaker model\nby an optimization process. Most Speaker Verification (SV) systems\nextract representations for both the enrollment and test utterances\ncalled embeddings, and then, these systems usually apply a similarity\nmetric or complex back-ends to carry out the verification process.\nUnlike previous works, we propose to take advantage of the knowledge\nacquired by a DNN to model the speakers from the training set since\nthe last layer of the DNN can be seen as an embedding dictionary which\nrepresents train speakers. Thus, after the initial training phase,\nwe introduce a new learnable vector for each enrollment speaker. Furthermore,\nto lead this training process, we employ a loss function more appropriate\nfor verification, the approximated Detection Cost Function ( aDCF)\nloss function. The new strategy to produce enrollment models for each\ntarget speaker was tested on the RSR-Part II database for text-dependent\nspeaker verification, where the proposed approach outperforms the reference\nsystem based on directly averaging of the embeddings extracted from\nthe enroll data using the network and the application of cosine similarity.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2325",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "sarfjoo20_interspeech": {
      "authors": [
        [
          "Seyyed Saeed",
          "Sarfjoo"
        ],
        [
          "Srikanth",
          "Madikeri"
        ],
        [
          "Petr",
          "Motlicek"
        ],
        [
          "S\u00e9bastien",
          "Marcel"
        ]
      ],
      "title": "Supervised Domain Adaptation for Text-Independent Speaker Verification Using Limited Data",
      "original": "2342",
      "page_count": 5,
      "order": 779,
      "p1": "3815",
      "pn": "3819",
      "abstract": [
        "To adapt the speaker verification (SV) system to a target domain with\nlimited data, this paper investigates the transfer learning of the\nmodel pre-trained on the source domain data. To that end, layer-by-layer\nadaptation with transfer learning from the initial and final layers\nof the pre-trained model is investigated. We show that the model adapted\nfrom the initial layers outperforms the model adapted from the final\nlayers. Based on this evidence, and inspired by the works in image\nrecognition field, we hypothesize that low-level convolutional neural\nnetwork (CNN) layers characterize domain-specific component while high-level\nCNN layers are domain-independent and have more discriminative power.\nFor adapting these domain-specific components, angular margin softmax\n(AMSoftmax) applied on the CNN-based implementation of the x-vector\narchitecture. In addition, to reduce the problem of over-fitting on\nthe limited target data, transfer learning on the batch norm layers\nis investigated. Mean shift and covariance estimation of batch norm\nallows to map the represented components of the target domain to the\nsource domain. Using TDNN and E-TDNN versions of the x-vectors as baseline\nmodels, the adapted models on the development set of NIST SRE 2018\noutperformed the baselines with relative improvements of 11.0 and 13.8%,\nrespectively.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2342",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "wei20b_interspeech": {
      "authors": [
        [
          "Yuheng",
          "Wei"
        ],
        [
          "Junzhao",
          "Du"
        ],
        [
          "Hui",
          "Liu"
        ]
      ],
      "title": "Angular Margin Centroid Loss for Text-Independent Speaker Recognition",
      "original": "2538",
      "page_count": 5,
      "order": 780,
      "p1": "3820",
      "pn": "3824",
      "abstract": [
        "Speaker recognition for unseen speakers out of the training dataset\nrelies on the discrimination of speaker embedding. Recent studies use\nthe angular softmax losses with angular margin penalties to enhance\nthe intra-class compactness of speaker embedding, which achieve obvious\nperformance improvement. However, the classification layer encounters\nthe problem of dimension explosion in these losses with the growth\nof training speakers. In this paper, like the prototype network loss\nin the few-short learning and the generalized end-to-end loss, we optimize\nthe cosine distances between speaker embeddings and their corresponding\ncentroids rather than the weight vectors in the classification layer.\nFor the intra-class compactness, we impose the additive angular margin\nto shorten the cosine distance between speaker embeddings belonging\nto the same speaker. Meanwhile, we also explicitly improve the inter-class\nseparability by enlarging the cosine distance between different speaker\ncentroids. Experiments show that our loss achieves comparable performance\nwith the stat-of-the-art angular margin softmax loss in both verification\nand identification tasks and markedly reduces the training iterations.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2538",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "kang20_interspeech": {
      "authors": [
        [
          "Jiawen",
          "Kang"
        ],
        [
          "Ruiqi",
          "Liu"
        ],
        [
          "Lantian",
          "Li"
        ],
        [
          "Yunqi",
          "Cai"
        ],
        [
          "Dong",
          "Wang"
        ],
        [
          "Thomas Fang",
          "Zheng"
        ]
      ],
      "title": "Domain-Invariant Speaker Vector Projection by Model-Agnostic Meta-Learning",
      "original": "2562",
      "page_count": 5,
      "order": 781,
      "p1": "3825",
      "pn": "3829",
      "abstract": [
        "Domain generalization remains a critical problem for speaker recognition,\neven with the state-of-the-art architectures based on deep neural nets.\nFor example, a model trained on reading speech may largely fail when\napplied to scenarios of singing or movie. In this paper, we propose\na domain-invariant projection to improve the generalizability of speaker\nvectors. This projection is a simple neural net and is trained following\nthe Model-Agnostic Meta-Learning (MAML) principle, for which the objective\nis to classify speakers in one domain if it had been updated with speech\ndata in another domain. We tested the proposed method on CNCeleb, a\nnew dataset consisting of single-speaker multi-condition (SSMC) data.\nThe results demonstrated that the MAML-based domain-invariant projection\ncan produce more generalizable speaker vectors, and effectively improve\nthe performance in unseen domains.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2562",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "desplanques20_interspeech": {
      "authors": [
        [
          "Brecht",
          "Desplanques"
        ],
        [
          "Jenthe",
          "Thienpondt"
        ],
        [
          "Kris",
          "Demuynck"
        ]
      ],
      "title": "ECAPA-TDNN: Emphasized Channel Attention, Propagation and Aggregation in TDNN Based Speaker Verification",
      "original": "2650",
      "page_count": 5,
      "order": 782,
      "p1": "3830",
      "pn": "3834",
      "abstract": [
        "Current speaker verification techniques rely on a neural network to\nextract speaker representations. The successful x-vector architecture\nis a Time Delay Neural Network (TDNN) that applies statistics pooling\nto project variable-length utterances into fixed-length speaker characterizing\nembeddings. In this paper, we propose multiple enhancements to this\narchitecture based on recent trends in the related fields of face verification\nand computer vision. Firstly, the initial frame layers can be restructured\ninto 1-dimensional Res2Net modules with impactful skip connections.\nSimilarly to SE-ResNet, we introduce Squeeze-and-Excitation blocks\nin these modules to explicitly model channel interdependencies. The\nSE block expands the temporal context of the frame layer by rescaling\nthe channels according to global properties of the recording. Secondly,\nneural networks are known to learn hierarchical features, with each\nlayer operating on a different level of complexity. To leverage this\ncomplementary information, we aggregate and propagate features of different\nhierarchical levels. Finally, we improve the statistics pooling module\nwith channel-dependent frame attention. This enables the network to\nfocus on different subsets of frames during each of the channel&#8217;s\nstatistics estimation. The proposed ECAPA-TDNN architecture significantly\noutperforms state-of-the-art TDNN based systems on the VoxCeleb test\nsets and the 2019 VoxCeleb Speaker Recognition Challenge.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2650",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "chen20p_interspeech": {
      "authors": [
        [
          "Wenda",
          "Chen"
        ],
        [
          "Jonathan",
          "Huang"
        ],
        [
          "Tobias",
          "Bocklet"
        ]
      ],
      "title": "Length- and Noise-Aware Training Techniques for Short-Utterance Speaker Recognition",
      "original": "2872",
      "page_count": 5,
      "order": 783,
      "p1": "3835",
      "pn": "3839",
      "abstract": [
        "Speaker recognition performance has been greatly improved with the\nemergence of deep learning. Deep neural networks show the capacity\nto effectively deal with impacts of noise and reverberation, making\nthem attractive to far-field speaker recognition systems. The x-vector\nframework is a popular choice for generating speaker embeddings in\nrecent literature due to its robust training mechanism and excellent\nperformance in various test sets. In this paper, we start with early\nwork on including invariant representation learning (IRL) to the loss\nfunction and modify the approach with centroid alignment (CA) and length\nvariability cost (LVC) techniques to further improve robustness in\nnoisy, far-field applications. This work mainly focuses on improvements\nfor short-duration test utterances (1-8s). We also present improved\nresults on long-duration tasks. In addition, this work discusses a\nnovel self-attention mechanism. On the VOiCES far-field corpus, the\ncombination of the proposed techniques achieves relative improvements\nof 7.0% for extremely short and 8.2% for full-duration test utterances\non equal error rate (EER) over our baseline system.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2872",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "lu20e_interspeech": {
      "authors": [
        [
          "Yiting",
          "Lu"
        ],
        [
          "Mark J.F.",
          "Gales"
        ],
        [
          "Yu",
          "Wang"
        ]
      ],
      "title": "Spoken Language &#8216;Grammatical Error Correction&#8217;",
      "original": "1852",
      "page_count": 5,
      "order": 784,
      "p1": "3840",
      "pn": "3844",
      "abstract": [
        "Spoken language &#8216;grammatical error correction&#8217; (GEC) is\nan important mechanism to help learners of a foreign language, here\nEnglish, improve their spoken grammar. GEC is challenging for non-native\nspoken language due to interruptions from disfluent speech events such\nas repetitions and false starts and issues in strictly defining what\nis acceptable in spoken language. Furthermore there is little labelled\ndata to train models. One way to mitigate the impact of speech events\nis to use a disfluency detection (DD) model. Removing the detected\ndisfluencies converts the speech transcript to be closer to written\nlanguage, which has significantly more labelled training data. This\npaper considers two types of approaches to leveraging DD models to\nboost spoken GEC performance. One is sequential, a separately trained\nDD model acts as a pre-processing module providing a more structured\ninput to the GEC model. The second approach is to train DD and GEC\nmodels in an end-to-end fashion, simultaneously optimising both modules.\nEmbeddings enable end-to-end models to have a richer information flow.\nExperimental results show that DD effectively regulates GEC input;\nend-to-end training works well when fine-tuned on limited labelled\nin-domain data; and improving DD by incorporating acoustic information\nhelps improve spoken GEC.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1852",
      "author_area_id": "10",
      "author_area_label": "Speech Recognition: Technologies and Systems for New Applications"
    },
    "papi20_interspeech": {
      "authors": [
        [
          "Sara",
          "Papi"
        ],
        [
          "Edmondo",
          "Trentin"
        ],
        [
          "Roberto",
          "Gretter"
        ],
        [
          "Marco",
          "Matassoni"
        ],
        [
          "Daniele",
          "Falavigna"
        ]
      ],
      "title": "Mixtures of Deep Neural Experts for Automated Speech Scoring",
      "original": "1055",
      "page_count": 5,
      "order": 785,
      "p1": "3845",
      "pn": "3849",
      "abstract": [
        "The paper copes with the task of automatic assessment of second language\nproficiency from the language learners&#8217; spoken responses to test\nprompts. The task has significant relevance to the field of computer\nassisted language learning. The approach presented in the paper relies\non two separate modules: (1) an automatic speech recognition system\nthat yields text transcripts of the spoken interactions involved, and\n(2) a multiple classifier system based on deep learners that ranks\nthe transcripts into proficiency classes. Different deep neural network\narchitectures (both feed-forward and recurrent) are specialized over\ndiverse representations of the texts in terms of: a reference grammar,\nthe outcome of probabilistic language models, several word embeddings,\nand two bag-of-word models. Combination of the individual classifiers\nis realized either via a probabilistic pseudo-joint model, or via a\nneural mixture of experts. Using the data of the third Spoken CALL\nShared Task challenge, the highest values to date were obtained in\nterms of three popular evaluation metrics.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1055",
      "author_area_id": "10",
      "author_area_label": "Speech Recognition: Technologies and Systems for New Applications"
    },
    "wang20ba_interspeech": {
      "authors": [
        [
          "Xinhao",
          "Wang"
        ],
        [
          "Klaus",
          "Zechner"
        ],
        [
          "Christopher",
          "Hamill"
        ]
      ],
      "title": "Targeted Content Feedback in Spoken Language Learning and Assessment",
      "original": "1766",
      "page_count": 5,
      "order": 786,
      "p1": "3850",
      "pn": "3854",
      "abstract": [
        "This study aims to develop automatic models to provide accurate and\nactionable diagnostic feedback within the context of spoken language\nlearning and assessment, in particular, targeting the content development\nskill. We focus on one type of test question widely used in speaking\nassessment where test takers are required to first listen to and/or\nread stimulus material and then create a spontaneous response to a\nquestion related to the stimulus. In a high-proficiency response, critical\ncontent from the source material &#8212; referred to as &#8220;key\npoints&#8221; &#8212; should be properly covered. We propose Transformer-based\nmodels to automatically detect absent key points or location spans\nof key points present in a response. Furthermore, we introduce a multi-task\nlearning approach to measure how well a key point is rendered within\na response (quality score). Experimental results show that automatic\nmodels can surpass human expert performance on both tasks: for span\ndetection, the system performance reached an F1 score of 74.5% (vs.\nhuman agreement of 68.3%); for quality score prediction, system performance\nreached a Pearson correlation coefficient (r) of 0.744 (vs. human agreement\nof 0.712). Finally, the proposed key point-based features can be used\nto predict speaking proficiency scores with a correlation of 0.730.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1766",
      "author_area_id": "10",
      "author_area_label": "Speech Recognition: Technologies and Systems for New Applications"
    },
    "raina20_interspeech": {
      "authors": [
        [
          "Vyas",
          "Raina"
        ],
        [
          "Mark J.F.",
          "Gales"
        ],
        [
          "Kate M.",
          "Knill"
        ]
      ],
      "title": "Universal Adversarial Attacks on Spoken Language Assessment Systems",
      "original": "1890",
      "page_count": 5,
      "order": 787,
      "p1": "3855",
      "pn": "3859",
      "abstract": [
        "There is an increasing demand for automated spoken language assessment\n(SLA) systems, partly driven by the performance improvements that have\ncome from deep learning based approaches. One aspect of deep learning\nsystems is that they do not require expert derived features, operating\ndirectly on the original signal such as a speech recognition (ASR)\ntranscript. This, however, increases their potential susceptibility\nto adversarial attacks as a form of candidate malpractice. In this\npaper the sensitivity of SLA systems to a universal black-box attack\non the ASR text output is explored. The aim is to obtain a single,\nuniversal phrase to maximally increase any candidate&#8217;s score.\nFour approaches to detect such adversarial attacks are also described.\nAll the systems, and associated detection approaches, are evaluated\non a free (spontaneous) speaking section from a Business English test.\nIt is shown that on deep learning based SLA systems the average candidate\nscore can be increased by almost one grade level using a single six\nword phrase appended to the end of the response hypothesis. Although\nthese large gains can be obtained, they can be easily detected based\non detection shifts from the scores of a &#8220;traditional&#8221;\nGaussian Process based grader.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1890",
      "author_area_id": "10",
      "author_area_label": "Speech Recognition: Technologies and Systems for New Applications"
    },
    "wu20n_interspeech": {
      "authors": [
        [
          "Xixin",
          "Wu"
        ],
        [
          "Kate M.",
          "Knill"
        ],
        [
          "Mark J.F.",
          "Gales"
        ],
        [
          "Andrey",
          "Malinin"
        ]
      ],
      "title": "Ensemble Approaches for Uncertainty in Spoken Language Assessment",
      "original": "2238",
      "page_count": 5,
      "order": 788,
      "p1": "3860",
      "pn": "3864",
      "abstract": [
        "Deep learning has dramatically improved the performance of automated\nsystems on a range of tasks including spoken language assessment. One\nof the issues with these deep learning approaches is that they tend\nto be overconfident in the decisions that they make, with potentially\nserious implications for deployment of systems for high-stakes examinations.\nThis paper examines the use of ensemble approaches to improve both\nthe reliability of the scores that are generated, and the ability to\ndetect where the system has made predictions beyond acceptable errors.\nIn this work assessment is treated as a regression problem. Deep density\nnetworks, and ensembles of these models, are used as the predictive\nmodels. Given an ensemble of models measures of uncertainty, for example\nthe variance of the predicted distributions, can be obtained and used\nfor detecting outlier predictions. However, these ensemble approaches\nincrease the computational and memory requirements of the system. To\naddress this problem the ensemble is distilled into a single mixture\ndensity network. The performance of the systems is evaluated on a free\nspeaking prompt-response style spoken language assessment test. Experiments\nshow that the ensembles and the distilled model yield performance gains\nover a single model, and have the ability to detect outliers.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2238",
      "author_area_id": "10",
      "author_area_label": "Speech Recognition: Technologies and Systems for New Applications"
    },
    "lin20k_interspeech": {
      "authors": [
        [
          "Zhenchao",
          "Lin"
        ],
        [
          "Ryo",
          "Takashima"
        ],
        [
          "Daisuke",
          "Saito"
        ],
        [
          "Nobuaki",
          "Minematsu"
        ],
        [
          "Noriko",
          "Nakanishi"
        ]
      ],
      "title": "Shadowability Annotation with Fine Granularity on L2 Utterances and its Improvement with Native Listeners&#8217; Script-Shadowing",
      "original": "2550",
      "page_count": 5,
      "order": 789,
      "p1": "3865",
      "pn": "3869",
      "abstract": [
        "Language teachers often claim that the goal of speech training should\nbe intelligible enough pronunciations, not native-sounding ones, because\nsome types of accented pronunciations are intelligible or comprehensible\nenough. However, if one aims to provide a technical framework of automatic\nassessment based on intelligibility or comprehensibility, s/he has\nto be faced with a big technical challenge. That is collection of L2\nutterances with annotations based on these metrics. Further, learners\nalways want to know which parts (words, morphemes, or syllables) in\ntheir speech should be corrected. This means that data collection needs\na valid method of intelligibility annotation with fine granularity.\nIn our previous studies, a new metric of  shadowability was introduced,\nand it was shown experimentally to be highly correlated to perceived\nintelligibility or comprehensibility as well as it was explained theoretically\nto be potential to give annotations with fine granularity. In this\npaper, shadowability annotation with fine granularity is examined experimentally,\nand a new and more valid method of collecting shadowing utterances\nis introduced. Finally, we tentatively derive frame-based shadowability\nannotation for L2 utterances.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2550",
      "author_area_id": "10",
      "author_area_label": "Speech Recognition: Technologies and Systems for New Applications"
    },
    "bai20b_interspeech": {
      "authors": [
        [
          "Yu",
          "Bai"
        ],
        [
          "Ferdy",
          "Hubers"
        ],
        [
          "Catia",
          "Cucchiarini"
        ],
        [
          "Helmer",
          "Strik"
        ]
      ],
      "title": "ASR-Based Evaluation and Feedback for Individualized Reading Practice",
      "original": "2842",
      "page_count": 5,
      "order": 790,
      "p1": "3870",
      "pn": "3874",
      "abstract": [
        "Learning to read is a prerequisite to participate in our knowledge\nsociety. Developing reading skills requires intensive practice with\nindividual evaluation and guidance by teachers, which is not always\nfeasible in traditional classroom instruction. Automatic Speech Recognition\n(ASR) technology could offer a solution, but so far it has been mostly\nused to follow children while reading and to provide correct word forms\nthrough text-to-speech technology. However, ASR could possibly be employed\nat earlier stages of learning to read when children are still in the\nprocess of developing decoding skills. Early evaluation through ASR\nand individualized feedback could help achieve more personalized and\npossibly more effective guidance, thus preventing reading problems\nand improving the process of reading development. <br style=\"mso-data-placement:same-cell;\"\n/><br style=\"mso-data-placement:same-cell;\" />\n  In this paper we report\non an explorative study in which an ASR-based system equipped with\nlogging capabilities was developed and employed to evaluate decoding\nskills in Dutch first graders reading aloud, and to provide them with\ndetailed, individualized feedback. The results indicate that ASR-based\nfeedback leads to improved reading accuracy and speed and that the\nlog-files provide useful information to enhance practice and feedback,\nthus paving the way for more personalized, technology-enriched approaches\nto reading instruction.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2842",
      "author_area_id": "10",
      "author_area_label": "Speech Recognition: Technologies and Systems for New Applications"
    },
    "woszczyk20_interspeech": {
      "authors": [
        [
          "Dominika",
          "Woszczyk"
        ],
        [
          "Stavros",
          "Petridis"
        ],
        [
          "David",
          "Millard"
        ]
      ],
      "title": "Domain Adversarial Neural Networks for Dysarthric Speech Recognition",
      "original": "2845",
      "page_count": 5,
      "order": 791,
      "p1": "3875",
      "pn": "3879",
      "abstract": [
        "Speech recognition systems have improved dramatically over the last\nfew years, however, their performance is significantly degraded for\nthe cases of accented or impaired speech. This work explores domain\nadversarial neural networks (DANN) for speaker-independent speech recognition\non the UAS dataset of dysarthric speech. The classification task on\n10 spoken digits is performed using an end-to-end CNN taking raw audio\nas input. The results are compared to a speaker-adaptive (SA) model\nas well as speaker-dependent (SD) and multi-task learning models (MTL).\nThe experiments conducted in this paper show that DANN achieves an\nabsolute recognition rate of 74.91% and outperforms the baseline by\n12.18%. Additionally, the DANN model achieves comparable results to\nthe SA model&#8217;s recognition rate of 77.65%. We also observe that\nwhen labelled dysarthric speech data is available DANN and MTL perform\nsimilarly, but when they are not DANN performs better than MTL.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2845"
    },
    "hidaka20_interspeech": {
      "authors": [
        [
          "Shunsuke",
          "Hidaka"
        ],
        [
          "Yogaku",
          "Lee"
        ],
        [
          "Kohei",
          "Wakamiya"
        ],
        [
          "Takashi",
          "Nakagawa"
        ],
        [
          "Tokihiko",
          "Kaburagi"
        ]
      ],
      "title": "Automatic Estimation of Pathological Voice Quality Based on Recurrent Neural Network Using Amplitude and Phase Spectrogram",
      "original": "3228",
      "page_count": 5,
      "order": 792,
      "p1": "3880",
      "pn": "3884",
      "abstract": [
        "Perceptual evaluation of voice quality is widely used in laryngological\npractice, but it lacks reproducibility caused by inter- and intra-rater\nvariability. This problem can be solved by automatic estimation of\nvoice quality using machine learning. In the previous studies, conventional\nacoustic features, such as jitter, have often been employed as inputs.\nHowever, many of them are vulnerable to severe hoarseness because they\nassume a quasi-periodicity of voice. This paper investigated non-parametric\nfeatures derived from amplitude and phase spectrograms. We applied\nthe instantaneous phase correction proposed by Yatabe et al. (2018)\nto extract features that could be interpreted as indicators of non-sinusoidality.\nSpecifically, we compared log amplitude, temporal phase variation,\ntemporal complex value variation, and mel-scale versions of them. A\ndeep neural network with a bidirectional GRU was constructed for each\nitem of GRBAS Scale, a hoarseness evaluation method. The dataset was\ncomposed of 2545 samples of sustained vowel /a/ with the GRBAS scores\nlabeled by an otolaryngologist. The results showed that the Hz-mel\nconversion improved the performance in almost all the case. The best\nscores were obtained when using temporal phase variation along the\nmel scale for Grade, Rough, Breathy, and Strained, and when using log\nmel amplitude for Asthenic.\n"
      ],
      "doi": "10.21437/Interspeech.2020-3228",
      "author_area_id": "10",
      "author_area_label": "Speech Recognition: Technologies and Systems for New Applications"
    },
    "chien20b_interspeech": {
      "authors": [
        [
          "Jen-Tzung",
          "Chien"
        ],
        [
          "Po-Chien",
          "Hsu"
        ]
      ],
      "title": "Stochastic Curiosity Exploration for Dialogue Systems",
      "original": "1313",
      "page_count": 5,
      "order": 793,
      "p1": "3885",
      "pn": "3889",
      "abstract": [
        "Traditionally, task-oriented dialogue system is built by an autonomous\nagent which can be trained by reinforcement learning where the reward\nfrom environment is maximized. The agent is learned by updating the\npolicy when the goal state is observed. However, in real world, the\nextrinsic reward is usually sparse or missing. The training efficiency\nis bounded. The system performance is degraded. It is challenging to\ntackle the issue of sample efficiency in sparse reward scenario for\nspoken dialogues. Accordingly, a dialogue agent needs additional information\nto update its policy even in the period when reward is absent in the\nenvironment. This paper presents a new dialogue agent which is learned\nby incorporating the intrinsic reward based on the information-theoretic\napproach via stochastic curiosity exploration. This agent encourages\nthe exploration for future diversity based on a latent dynamic architecture\nwhich consists of encoder network, curiosity network, information network\nand policy network. The latent states and actions are drawn to predict\nstochastic transition for future. The curiosity learning are implemented\nwith intrinsic reward in a metric of mutual information and prediction\nerror in the predicted states and actions. Experiments on dialogue\nmanagement using PyDial demonstrate the benefit by using the stochastic\ncuriosity exploration.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1313",
      "author_area_id": "11",
      "author_area_label": "Spoken dialog systems and conversational analysis"
    },
    "jeong20_interspeech": {
      "authors": [
        [
          "Myeongho",
          "Jeong"
        ],
        [
          "Seungtaek",
          "Choi"
        ],
        [
          "Hojae",
          "Han"
        ],
        [
          "Kyungho",
          "Kim"
        ],
        [
          "Seung-won",
          "Hwang"
        ]
      ],
      "title": "Conditional Response Augmentation for Dialogue Using Knowledge Distillation",
      "original": "1968",
      "page_count": 5,
      "order": 794,
      "p1": "3890",
      "pn": "3894",
      "abstract": [
        "This paper studies dialogue response selection task. As state-of-the-arts\nare neural models requiring a large training set, data augmentation\nis essential to overcome the sparsity of observational annotation,\nwhere one observed response is annotated as gold. In this paper, we\npropose counterfactual augmentation, of considering whether unobserved\nutterances would &#8220;counterfactually&#8221; replace the labelled\nresponse, for the given context, and augment only if that is the case.\nWe empirically show that our pipeline improves BERT-based models in\ntwo different response selection tasks without incurring annotation\noverheads.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1968",
      "author_area_id": "11",
      "author_area_label": "Spoken dialog systems and conversational analysis"
    },
    "luo20c_interspeech": {
      "authors": [
        [
          "Hongyin",
          "Luo"
        ],
        [
          "Shang-Wen",
          "Li"
        ],
        [
          "James",
          "Glass"
        ]
      ],
      "title": "Prototypical Q Networks for Automatic Conversational Diagnosis and Few-Shot New Disease Adaption",
      "original": "1865",
      "page_count": 5,
      "order": 795,
      "p1": "3895",
      "pn": "3899",
      "abstract": [
        "Spoken dialog systems have seen applications in many domains, including\nmedical for automatic conversational diagnosis. State-of-the-art dialog\nmanagers are usually driven by deep reinforcement learning models,\nsuch as deep Q networks (DQNs), which learn by interacting with a simulator\nto explore the entire action space since real conversations are limited.\nHowever, the DQN-based automatic diagnosis models do not achieve satisfying\nperformances when adapted to new, unseen diseases with only a few training\nsamples. In this work, we propose the Prototypical Q Networks (ProtoQN)\nas the dialog manager for the automatic diagnosis systems. The model\ncalculates prototype embeddings with real conversations between doctors\nand patients, learning from them and simulator-augmented dialogs more\nefficiently. We create both supervised and few-shot learning tasks\nwith the Muzhi corpus. Experiments showed that the ProtoQN significantly\noutperformed the baseline DQN model in both supervised and few-shot\nlearning scenarios, and achieves state-of-the-art few-shot learning\nperformances.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1865",
      "author_area_id": "11",
      "author_area_label": "Spoken dialog systems and conversational analysis"
    },
    "hong20b_interspeech": {
      "authors": [
        [
          "Teakgyu",
          "Hong"
        ],
        [
          "Oh-Woog",
          "Kwon"
        ],
        [
          "Young-Kil",
          "Kim"
        ]
      ],
      "title": "End-to-End Task-Oriented Dialog System Through Template Slot Value Generation",
      "original": "2011",
      "page_count": 5,
      "order": 796,
      "p1": "3900",
      "pn": "3904",
      "abstract": [
        "To overcome the limitations of conventional pipeline-based task-oriented\ndialog systems, an end-to-end approach has been introduced. To date,\nmany end-to-end task-oriented dialog systems have been proposed and\nthese have shown good performance in various domains. However, those\nhave some limitations such as the need for dialog state annotations.\nAnd there is also room for improvement for those systems. In this paper,\nwe examine the issues of recent end-to-end task-oriented dialog systems\nand present a model that can handle these issues. The proposed model\nclassifies a system utterance template in a retrieval-based manner\nand then generates the slot values in the template through a decoder.\nAlso, we propose an unsupervised learning based template generation\nmethod that allows model training even in a domain where the templates\nare not given and the dialog information is not tagged. Our model obtains\nnew state-of-the-art results on a restaurant search domain.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2011",
      "author_area_id": "11",
      "author_area_label": "Spoken dialog systems and conversational analysis"
    },
    "he20_interspeech": {
      "authors": [
        [
          "Zhenhao",
          "He"
        ],
        [
          "Jiachun",
          "Wang"
        ],
        [
          "Jian",
          "Chen"
        ]
      ],
      "title": "Task-Oriented Dialog Generation with Enhanced Entity Representation",
      "original": "1037",
      "page_count": 5,
      "order": 797,
      "p1": "3905",
      "pn": "3909",
      "abstract": [
        "Recent advances in neural sequence-to-sequence models have led to promising\nresults for end-to-end task-oriented dialog generation. Such frameworks\nenable a decoder to retrieve knowledge from the dialog history and\nthe knowledge base during generation. However, these models usually\nrely on learned word embeddings as entity representation, which is\ndifficult to deal with the rare and unknown entities. In this work,\nwe propose a novel enhanced entity representation (EER) to simultaneously\nobtain context-sensitive and structure-aware entity representation.\nOur proposed method enables the decoder to facilitate both the ability\nto fetch the relevant knowledge and the effectiveness of incorporating\ngrounding knowledge into the dialog generation. Experimental results\non two publicly available dialog datasets show that our model outperforms\nthe state-of-the-art data-driven task-oriented dialog models. Moreover,\nwe conduct an Out-of-Vocabulary (OOV) test to demonstrate the superiority\nof EER in handling common OOV problem.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1037",
      "author_area_id": "11",
      "author_area_label": "Spoken dialog systems and conversational analysis"
    },
    "dang20_interspeech": {
      "authors": [
        [
          "Viet-Trung",
          "Dang"
        ],
        [
          "Tianyu",
          "Zhao"
        ],
        [
          "Sei",
          "Ueno"
        ],
        [
          "Hirofumi",
          "Inaguma"
        ],
        [
          "Tatsuya",
          "Kawahara"
        ]
      ],
      "title": "End-to-End Speech-to-Dialog-Act Recognition",
      "original": "1062",
      "page_count": 5,
      "order": 798,
      "p1": "3910",
      "pn": "3914",
      "abstract": [
        "Spoken language understanding, which extracts intents and/or semantic\nconcepts in utterances, is conventionally formulated as a post-processing\nof automatic speech recognition. It is usually trained with oracle\ntranscripts, but needs to deal with errors by ASR. Moreover, there\nare acoustic features which are related with intents but not represented\nwith the transcripts. In this paper, we present an end-to-end model\nthat directly converts speech into dialog acts without the deterministic\ntranscription process. In the proposed model, the dialog act recognition\nnetwork is conjunct with an acoustic-to-word ASR model at its latent\nlayer before the softmax layer, which provides a distributed representation\nof word-level ASR decoding information. Then, the entire network is\nfine-tuned in an end-to-end manner. This allows for stable training\nas well as robustness against ASR errors. The model is further extended\nto conduct DA segmentation jointly. Evaluations with the Switchboard\ncorpus demonstrate that the proposed method significantly improves\ndialog act recognition accuracy from the conventional pipeline framework.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1062",
      "author_area_id": "11",
      "author_area_label": "Spoken dialog systems and conversational analysis"
    },
    "qian20_interspeech": {
      "authors": [
        [
          "Yao",
          "Qian"
        ],
        [
          "Yu",
          "Shi"
        ],
        [
          "Michael",
          "Zeng"
        ]
      ],
      "title": "Discriminative Transfer Learning for Optimizing ASR and Semantic Labeling in Task-Oriented Spoken Dialog",
      "original": "1962",
      "page_count": 5,
      "order": 799,
      "p1": "3915",
      "pn": "3919",
      "abstract": [
        "Spoken language understanding (SLU) tries to decode an input speech\nutterance such that effective semantic actions can be taken to continue\nmeaningful and interactive spoken dialog (SD). The performance of SLU,\nhowever, can be adversely affected by automatic speech recognition\n(ASR) errors. In this paper, we exploit transfer learning in a Generative\npre-trained Transformer (GPT) to jointly optimize ASR error correction\nand semantic labeling in terms of dialog act and slot-value for a given\nuser&#8217;s spoken response in the context of SD system (SDS). With\nthe encoded ASR output and dialog history as context, a conditional\ngenerative model is trained to generate transcripts correction, dialog\nact, and slot-values successively. The proposed generation model is\njointly optimized as a classification task, which utilizes the ground-truth\nand N-best hypotheses in a multi-task, discriminative learning. We\nevaluate its effectiveness on a public SD corpus used in the Second\nDialog State Tracking Challenge. The results show that our generation\nmodel can achieve a relative word error rate reduction of 25.12% from\nthat in the original ASR 1-best result, and a sentence error rate (SER)\nlower than the oracle result from the 10-best ASR hypotheses. The proposed\napproach of generating dialog acts and slot-values, instead of classification\nand tagging, is promising. The refined ASR hypotheses are critical\nfor improving semantic label generation.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1962",
      "author_area_id": "11",
      "author_area_label": "Spoken dialog systems and conversational analysis"
    },
    "xu20f_interspeech": {
      "authors": [
        [
          "Xinnuo",
          "Xu"
        ],
        [
          "Yizhe",
          "Zhang"
        ],
        [
          "Lars",
          "Liden"
        ],
        [
          "Sungjin",
          "Lee"
        ]
      ],
      "title": "Datasets and Benchmarks for Task-Oriented Log Dialogue Ranking Task",
      "original": "1341",
      "page_count": 5,
      "order": 800,
      "p1": "3920",
      "pn": "3924",
      "abstract": [
        "Although the data-driven approaches of some recent bot building platforms\nmake it possible for a wide range of users to easily create dialogue\nsystems, those platforms don&#8217;t offer tools for quickly identifying\nwhich log dialogues contain problems. Thus, in this paper, we (1) introduce\na new task, log dialogue ranking, where the ranker places problematic\ndialogues higher (2) provide a collection of human-bot conversations\nin the restaurant inquiry task labelled with dialogue quality for ranker\ntraining and evaluation (3) present a detailed description of the data\ncollection pipeline, which is entirely based on crowd-sourcing (4)\nfinally report a benchmark result of dialogue ranking, which shows\nthe usability of the data and sets a baseline for future studies.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1341",
      "author_area_id": "11",
      "author_area_label": "Spoken dialog systems and conversational analysis"
    },
    "wang20ca_interspeech": {
      "authors": [
        [
          "Ziteng",
          "Wang"
        ],
        [
          "Yueyue",
          "Na"
        ],
        [
          "Zhang",
          "Liu"
        ],
        [
          "Yun",
          "Li"
        ],
        [
          "Biao",
          "Tian"
        ],
        [
          "Qiang",
          "Fu"
        ]
      ],
      "title": "A Semi-Blind Source Separation Approach for Speech Dereverberation",
      "original": "1307",
      "page_count": 5,
      "order": 801,
      "p1": "3925",
      "pn": "3929",
      "abstract": [
        "This paper presents a novel semi-blind source separation approach for\nspeech dereverberation. Based on a time independence assumption of\nthe clean speech signals, direct sound and late reverberation are treated\nas separate sources and are separated using the auxiliary function\nbased independent component analysis (Aux-ICA) algorithm. We show that\nthe dereverberation performance is closely related to the underlying\nsource probability density prior and the proposed approach generalizes\nto the popular weighted prediction error (WPE) algorithm, if the direct\nsound follows a Gaussian distribution with time-varying variances.\nThe efficacy of the proposed approach is fully validated by speech\nquality and speech recognition experiments conducted on the REVERB\nChallenge dataset.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1307",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "yang20j_interspeech": {
      "authors": [
        [
          "Joon-Young",
          "Yang"
        ],
        [
          "Joon-Hyuk",
          "Chang"
        ]
      ],
      "title": "Virtual Acoustic Channel Expansion Based on Neural Networks for Weighted Prediction Error-Based Speech Dereverberation",
      "original": "1553",
      "page_count": 5,
      "order": 802,
      "p1": "3930",
      "pn": "3934",
      "abstract": [
        "In this study, we propose a neural-network-based virtual acoustic channel\nexpansion (VACE) framework for weighted prediction error (WPE)-based\nspeech dereverberation. Specifically, for the situation in which only\na single microphone observation is available, we aim to build a neural\nnetwork capable of generating a virtual signal that can be exploited\nas the secondary input for the dual-channel WPE algorithm, thus making\nits dereverberation performance superior to the single-channel WPE.\nTo implement the VACE-WPE, the neural network for the VACE is initialized\nand integrated to the pre-trained neural WPE algorithm. The entire\nsystem is then trained in a supervised manner to output a dereverberated\nsignal that is close to the oracle early arriving speech. Experimental\nresults show that the proposed VACE-WPE method outperforms the single-channel\nWPE in a real room impulse response shortening task.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1553",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "kothapally20_interspeech": {
      "authors": [
        [
          "Vinay",
          "Kothapally"
        ],
        [
          "Wei",
          "Xia"
        ],
        [
          "Shahram",
          "Ghorbani"
        ],
        [
          "John H.L.",
          "Hansen"
        ],
        [
          "Wei",
          "Xue"
        ],
        [
          "Jing",
          "Huang"
        ]
      ],
      "title": "SkipConvNet: Skip Convolutional Neural Network for Speech Dereverberation Using Optimally Smoothed Spectral Mapping",
      "original": "2048",
      "page_count": 5,
      "order": 803,
      "p1": "3935",
      "pn": "3939",
      "abstract": [
        "The reliability of using fully convolutional networks (FCNs) has been\nsuccessfully demonstrated by recent studies in many speech applications.\nOne of the most popular variants of these FCNs is the &#8216;U-Net&#8217;,\nwhich is an encoder-decoder network with skip connections. In this\nstudy, we propose &#8216;SkipConvNet&#8217; where we replace each skip\nconnection with multiple convolutional modules to provide decoder with\nintuitive feature maps rather than encoder&#8217;s output to improve\nthe learning capacity of the network. We also propose the use of optimal\nsmoothing of power spectral density (PSD) as a pre-processing step,\nwhich helps to further enhance the efficiency of the network. To evaluate\nour proposed system, we use the REVERB challenge corpus to assess the\nperformance of various enhancement approaches under the same conditions.\nWe focus solely on monitoring improvements in speech quality and their\ncontribution to improving the efficiency of back-end speech systems,\nsuch as speech recognition and speaker verification, trained on only\nclean speech. Experimental findings show that the proposed system consistently\noutperforms other approaches.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2048",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "zhang20ea_interspeech": {
      "authors": [
        [
          "Chenggang",
          "Zhang"
        ],
        [
          "Xueliang",
          "Zhang"
        ]
      ],
      "title": "A Robust and Cascaded Acoustic Echo Cancellation Based on Deep Learning",
      "original": "1260",
      "page_count": 5,
      "order": 804,
      "p1": "3940",
      "pn": "3944",
      "abstract": [
        "Acoustic echo cancellation (AEC) is used to cancel feedback between\na loudspeaker and a microphone. Ideally, AEC is a linear problem and\ncan be solved by adaptive filtering. However, in practice, two important\nproblems severely affect the performance of AEC, i.e. 1) double-talk\nproblem and 2) nonlinear distortion mainly caused by loudspeakers and/or\npower amplifiers. Considering these two problems in AEC, we propose\na novel cascaded AEC which integrates adaptive filtering and deep learning.\nSpecifically, two long short-term memory networks (LSTM) are employed\nfor double-talk detection (DTD) and nonlinearity modeling, respectively.\nThe adaptive filtering is employed to remove the linear part of echo.\nExperimental results show that the proposed method outperforms conventional\nmethods in terms of the objective evaluation metrics by a considerable\nmargin in the matched scenario. Moreover, the proposed method has much\nbetter generalization ability in the unmatched scenarios, compared\nwith end-to-end deep learning method.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1260",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "zhang20fa_interspeech": {
      "authors": [
        [
          "Yi",
          "Zhang"
        ],
        [
          "Chengyun",
          "Deng"
        ],
        [
          "Shiqian",
          "Ma"
        ],
        [
          "Yongtao",
          "Sha"
        ],
        [
          "Hui",
          "Song"
        ],
        [
          "Xiangang",
          "Li"
        ]
      ],
      "title": "Generative Adversarial Network Based Acoustic Echo Cancellation",
      "original": "1454",
      "page_count": 5,
      "order": 805,
      "p1": "3945",
      "pn": "3949",
      "abstract": [
        "Generative adversarial networks (GANs) have become a popular research\ntopic in speech enhancement like noise suppression. By training the\nnoise suppression algorithm in an adversarial scenario, GAN based solutions\noften yield good performance. In this paper, a convolutional recurrent\nGAN architecture (CRGAN-EC) is proposed to address both linear and\nnonlinear echo scenarios. The proposed architecture is trained in frequency\ndomain and predicts the time-frequency (TF) mask for the target speech.\nSeveral metric loss functions are deployed and their influence on echo\ncancellation performance is studied. Experimental results suggest that\nthe proposed method outperforms the existing methods for unseen speakers\nin terms of echo return loss enhancement (ERLE) and perceptual evaluation\nof speech quality (PESQ). Moreover, multiple metric loss functions\nprovide more freedom to achieve specific goals, e.g., more echo suppression\nor less distortion.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1454",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "pfeifenberger20_interspeech": {
      "authors": [
        [
          "Lukas",
          "Pfeifenberger"
        ],
        [
          "Franz",
          "Pernkopf"
        ]
      ],
      "title": "Nonlinear Residual Echo Suppression Using a Recurrent Neural Network",
      "original": "1473",
      "page_count": 5,
      "order": 806,
      "p1": "3950",
      "pn": "3954",
      "abstract": [
        "The acoustic front-end of hands-free communication devices introduces\na variety of distortions to the linear echo path between the loudspeaker\nand the microphone. While the amplifiers may introduce a memory-less\nnon-linearity, mechanical vibrations transmitted from the loudspeaker\nto the microphone via the housing of the device introduce non-linearities\nwith memory, which are much harder to compensate. These distortions\nsignificantly limit the performance of linear Acoustic Echo Cancellation\n(AEC) algorithms. While there already exists a wide range of Residual\nEcho Suppressor (RES) techniques for individual use cases, our contribution\nspecifically aims at a low-resource implementation that is also real-time\ncapable. The proposed approach is based on a small Recurrent Neural\nNetwork (RNN) which adds memory to the residual echo suppressor, enabling\nit to compensate both types of non-linear distortions. We evaluate\nthe performance of our system in terms of Echo Return Loss Enhancement\n(ERLE), Signal to Distortion Ratio (SDR) and Word Error Rate (WER),\nobtained during realistic double-talk situations. Further, we compare\nthe postfilter against a state-of-the-art implementation. Finally,\nwe analyze the numerical complexity of the overall system.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1473",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "gao20d_interspeech": {
      "authors": [
        [
          "Yi",
          "Gao"
        ],
        [
          "Ian",
          "Liu"
        ],
        [
          "J.",
          "Zheng"
        ],
        [
          "Cheng",
          "Luo"
        ],
        [
          "Bin",
          "Li"
        ]
      ],
      "title": "Independent Echo Path Modeling for Stereophonic Acoustic Echo Cancellation",
      "original": "2131",
      "page_count": 4,
      "order": 807,
      "p1": "3955",
      "pn": "3958",
      "abstract": [
        "As stereophonic audio devices, such as smart speakers and cellphones,\nevolve to be daily essentials, stereophonic acoustic echo cancellation\nbecomes more important for voice and audio applications. The cross-correlation\nbetween the far-end channels and the associated ambiguity in the estimated\necho path transfer functions lead the misalignment and instability\nissues with conventional stereophonic acoustic echo cancellers (SAEC).\nIn this paper, we propose a novel SAEC algorithm, which can better\nmodel the acoustic echo path between each loudspeaker and microphone.\nSpecifically, filter adaptations are modeled independently by applying\npre-whitening in solving the misalignment problem. Improvement in echo\nsuppression capability is evaluated in terms of echo return loss enhancement(ERLE)\nand wakeup word detection accuracy.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2131",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "chen20q_interspeech": {
      "authors": [
        [
          "Hongsheng",
          "Chen"
        ],
        [
          "Teng",
          "Xiang"
        ],
        [
          "Kai",
          "Chen"
        ],
        [
          "Jing",
          "Lu"
        ]
      ],
      "title": "Nonlinear Residual Echo Suppression Based on Multi-Stream Conv-TasNet",
      "original": "2234",
      "page_count": 5,
      "order": 808,
      "p1": "3959",
      "pn": "3963",
      "abstract": [
        "Acoustic echo cannot be entirely removed by linear adaptive filters\ndue to the nonlinear relationship between the echo and far-end signal.\nUsually a post processing module is required to further suppress the\necho. In this paper, we propose a residual echo suppression method\nbased on the modification of fully convolutional time-domain audio\nseparation network (Conv-TasNet). Both the residual signal of the linear\nacoustic echo cancellation system, and the output of the adaptive filter\nare adopted to form multiple streams for the Conv-TasNet, resulting\nin more effective echo suppression while keeping a lower latency of\nthe whole system. Simulation results validate the efficacy of the proposed\nmethod in both single-talk and double-talk situations.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2234",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "fan20c_interspeech": {
      "authors": [
        [
          "Wenzhi",
          "Fan"
        ],
        [
          "Jing",
          "Lu"
        ]
      ],
      "title": "Improving Partition-Block-Based Acoustic Echo Canceler in Under-Modeling Scenarios",
      "original": "2479",
      "page_count": 5,
      "order": 809,
      "p1": "3964",
      "pn": "3968",
      "abstract": [
        "Recently, a partitioned-block-based frequency-domain Kalman filter\n(PFKF) has been proposed for acoustic echo cancellation. Compared with\nthe normal frequency-domain Kalman filter, the PFKF utilizes the partitioned-block\nstructure, resulting in both fast convergence and low time-latency.\nWe present an analysis of the steady-state behavior of the PFKF and\nfound that it suffers from a biased steady-state solution when the\nfilter is of deficient length. Accordingly, we propose an effective\nmodification that has the benefit of the guaranteed optimal steady-state\nbehavior. Simulations are conducted to validate the improved performance\nof the proposed method.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2479",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "kim20e_interspeech": {
      "authors": [
        [
          "Jung-Hee",
          "Kim"
        ],
        [
          "Joon-Hyuk",
          "Chang"
        ]
      ],
      "title": "Attention Wave-U-Net for Acoustic Echo Cancellation",
      "original": "3200",
      "page_count": 5,
      "order": 810,
      "p1": "3969",
      "pn": "3973",
      "abstract": [
        "In this paper, a Wave-U-Net based acoustic echo cancellation (AEC)\nwith an attention mechanism is proposed to jointly suppress acoustic\necho and background noise. The proposed approach consists of the Wave-U-Net,\nan auxiliary encoder, and an attention network. In the proposed approach,\nthe Wave-U-Net yields the estimated near-end speech from the mixture,\nthe auxiliary encoder extracts the latent features of the far-end speech,\namong which the relevant features are provided to the Wave-U-Net by\nusing the attention mechanism. With the attention network, the echo\ncan be effectively suppressed from the mixture. Experimental results\non TIMIT dataset show that the proposed approach outperforms the existing\nmethods in terms of the echo return loss enhancement (ERLE) for the\nsingle-talk period and the perceptual evaluation of speech quality\n(PESQ) score for the double-talk period. Furthermore, the robustness\nof the proposed approach against unseen noise condition is also validated\nfrom the experimental results.\n"
      ],
      "doi": "10.21437/Interspeech.2020-3200",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "cai20c_interspeech": {
      "authors": [
        [
          "Zexin",
          "Cai"
        ],
        [
          "Chuxiong",
          "Zhang"
        ],
        [
          "Ming",
          "Li"
        ]
      ],
      "title": "From Speaker Verification to Multispeaker Speech Synthesis, Deep Transfer with Feedback Constraint",
      "original": "1032",
      "page_count": 5,
      "order": 811,
      "p1": "3974",
      "pn": "3978",
      "abstract": [
        "High-fidelity speech can be synthesized by end-to-end text-to-speech\nmodels in recent years. However, accessing and controlling speech attributes\nsuch as speaker identity, prosody, and emotion in a text-to-speech\nsystem remains a challenge. This paper presents a system involving\nfeedback constraints for multispeaker speech synthesis. We manage to\nenhance the knowledge transfer from the speaker verification to the\nspeech synthesis by engaging the speaker verification network. The\nconstraint is taken by an added loss related to the speaker identity,\nwhich is centralized to improve the speaker similarity between the\nsynthesized speech and its natural reference audio. The model is trained\nand evaluated on publicly available datasets. Experimental results,\nincluding visualization on speaker embedding space, show significant\nimprovement in terms of speaker identity cloning in the spectrogram\nlevel. In addition, synthesized samples are available online for listening.<SUP>1</SUP>\n"
      ],
      "doi": "10.21437/Interspeech.2020-1032"
    },
    "cooper20_interspeech": {
      "authors": [
        [
          "Erica",
          "Cooper"
        ],
        [
          "Cheng-I",
          "Lai"
        ],
        [
          "Yusuke",
          "Yasuda"
        ],
        [
          "Junichi",
          "Yamagishi"
        ]
      ],
      "title": "Can Speaker Augmentation Improve Multi-Speaker End-to-End TTS?",
      "original": "1229",
      "page_count": 5,
      "order": 812,
      "p1": "3979",
      "pn": "3983",
      "abstract": [
        "Previous work on speaker adaptation for end-to-end speech synthesis\nstill falls short in speaker similarity. We investigate an orthogonal\napproach to the current speaker adaptation paradigms,  speaker augmentation,\nby creating artificial speakers and by taking advantage of low-quality\ndata. The base Tacotron2 model is modified to account for the channel\nand dialect factors inherent in these corpora. In addition, we describe\na warm-start training strategy that we adopted for Tacotron2 training.\nA large-scale listening test is conducted, and a distance metric is\nadopted to evaluate synthesis of dialects. This is followed by an analysis\non synthesis quality, speaker and dialect similarity, and a remark\non the effectiveness of our speaker augmentation approach. Audio samples\nare available online<SUP>1</SUP>.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1229",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "wang20da_interspeech": {
      "authors": [
        [
          "Tao",
          "Wang"
        ],
        [
          "Xuefei",
          "Liu"
        ],
        [
          "Jianhua",
          "Tao"
        ],
        [
          "Jiangyan",
          "Yi"
        ],
        [
          "Ruibo",
          "Fu"
        ],
        [
          "Zhengqi",
          "Wen"
        ]
      ],
      "title": "Non-Autoregressive End-to-End TTS with Coarse-to-Fine Decoding",
      "original": "1662",
      "page_count": 5,
      "order": 813,
      "p1": "3984",
      "pn": "3988",
      "abstract": [
        "Most end-to-end neural text-to-speech (TTS) systems generate acoustic\nfeatures autoregressively from left to right, which still suffer from\ntwo problems: 1) low efficiency during inference; 2) the limitation\nof &#8220;exposure bias&#8221;. To overcome these shortcomings, this\npaper proposes a non-autoregressive speech synthesis model which is\nbased on the transformer structure. During training, the ground truth\nof acoustic features is schedule masked. The decoder needs to predict\nthe entire acoustic features by taking text and the masked ground truth.\nDuring inference, we just need a text as input, the network will predict\nthe acoustic features in one step. Additionally, we decompose the decoding\nprocess into two stages so that the model can consider the information\nin the context. Given an input text embedding, we first generate coarse\nacoustic features, which focus on the meaning of sentences. Then, we\nfill in missing details of acoustic features by taking into account\nthe text information and the coarse acoustic features. Experiments\non a Chinese female corpus illustrate that our approach can achieve\ncompetitive results in speech naturalness relative to autoregressive\nmodel. Most importantly, our model speed up the acoustic features generation\nby 296&#215; compared with the autoregressive model based on transformer\nstructure.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1662",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "wang20ea_interspeech": {
      "authors": [
        [
          "Tao",
          "Wang"
        ],
        [
          "Jianhua",
          "Tao"
        ],
        [
          "Ruibo",
          "Fu"
        ],
        [
          "Jiangyan",
          "Yi"
        ],
        [
          "Zhengqi",
          "Wen"
        ],
        [
          "Chunyu",
          "Qiang"
        ]
      ],
      "title": "Bi-Level Speaker Supervision for One-Shot Speech Synthesis",
      "original": "1737",
      "page_count": 5,
      "order": 814,
      "p1": "3989",
      "pn": "3993",
      "abstract": [
        "The gap between speaker characteristics of reference speech and synthesized\nspeech remains a challenging problem in one-shot speech synthesis.\nIn this paper, we propose a bi-level speaker supervision framework\nto close the speaker characteristics gap via supervising the synthesized\nspeech at speaker feature level and speaker identity level. The speaker\nfeature extraction and speaker identity reconstruction are integrated\nin an end-to-end speech synthesis network, with the one on speaker\nfeature level for closing speaker characteristics and the other on\nspeaker identity level for preserving identity information. This framework\nguarantees that the synthesized speech has similar speaker characteristics\nto original speech, and it also ensures the distinguishability between\ndifferent speakers. Additionally, to solve the influence of speech\ncontent on speaker feature extraction task, we propose a text-independent\nreference encoder (ti-reference encoder) module to extract speaker\nfeature. Experiments on LibriTTS dataset show that our model is able\nto generate the speech similar to target speaker. Furthermore, we demonstrate\nthat this model can learn meaningful speaker representations by bi-level\nspeaker supervision and ti-reference encoder module.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1737",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "peirolilja20_interspeech": {
      "authors": [
        [
          "Alex",
          "Peir\u00f3-Lilja"
        ],
        [
          "Mireia",
          "Farr\u00fas"
        ]
      ],
      "title": "Naturalness Enhancement with Linguistic Information in End-to-End TTS Using Unsupervised Parallel Encoding",
      "original": "1788",
      "page_count": 5,
      "order": 815,
      "p1": "3994",
      "pn": "3998",
      "abstract": [
        "State-of-the-art end-to-end speech synthesis models have reached levels\nof quality close to human capabilities. However, there is still room\nfor improvement in terms of naturalness, related to prosody, which\nis essential for human-machine interaction. Therefore, part of current\nresearch has shift its focus on improving this aspect with many solutions,\nwhich mainly involve prosody adaptability or control. In this work,\nwe explored a way to include linguistic features into the sequence-to-sequence\nTacotron2 system to improve the naturalness of the generated voice.\nThat is, making the prosody of the synthesis looking more like the\nreal human speaker. Specifically we embedded with an additional encoder\npart-of-speech tags and punctuation mark locations of the input text\nto condition Tacotron2 generation. We propose two different architectures\nfor this parallel encoder: one based on a stack of convolutional plus\nrecurrent layers, and another formed by a stack of bidirectional recurrent\nplus linear layers. To evaluate the similarity between real read-speech\nand synthesis, we carried out an objective test using signal processing\nmetrics and a perceptual test. The presented results show that we achieved\nan improvement in naturalness.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1788",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "li20ha_interspeech": {
      "authors": [
        [
          "Naihan",
          "Li"
        ],
        [
          "Shujie",
          "Liu"
        ],
        [
          "Yanqing",
          "Liu"
        ],
        [
          "Sheng",
          "Zhao"
        ],
        [
          "Ming",
          "Liu"
        ],
        [
          "Ming",
          "Zhou"
        ]
      ],
      "title": "MoBoAligner: A Neural Alignment Model for Non-Autoregressive TTS with Monotonic Boundary Search",
      "original": "1976",
      "page_count": 5,
      "order": 816,
      "p1": "3999",
      "pn": "4003",
      "abstract": [
        "To speed up the inference of neural speech synthesis, non-autoregressive\nmodels receive increasing attention recently. In non-autoregressive\nmodels, additional durations of text tokens are required to make a\nhard alignment between the encoder and the decoder. The duration-based\nalignment plays a crucial role since it controls the correspondence\nbetween text tokens and spectrum frames and determines the rhythm and\nspeed of synthesized audio. To get better duration-based alignment\nand improve the quality of non-autoregressive speech synthesis, in\nthis paper, we propose a novel neural alignment model named MoBoAligner.\nGiven the pairs of the text and mel spectrum, MoBoAligner tries to\nidentify the boundaries of text tokens in the given mel spectrum frames\nbased on the token-frame similarity in the neural semantic space with\nan end-to-end framework. With these boundaries, durations can be extracted\nand used in the training of non-autoregressive TTS models. Compared\nwith the duration extracted by TransformerTTS, MoBoAligner brings improvement\nfor the non-autoregressive TTS model on MOS (3.74 comparing to FastSpeech&#8217;s\n3.44). Besides, MoBoAligner is task-specified and lightweight, which\nreduces the parameter number by 45% and the training time consuming\nby 30%.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1976",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "lim20_interspeech": {
      "authors": [
        [
          "Dan",
          "Lim"
        ],
        [
          "Won",
          "Jang"
        ],
        [
          "Gyeonghwan",
          "O"
        ],
        [
          "Heayoung",
          "Park"
        ],
        [
          "Bongwan",
          "Kim"
        ],
        [
          "Jaesam",
          "Yoon"
        ]
      ],
      "title": "JDI-T: Jointly Trained Duration Informed Transformer for Text-To-Speech without Explicit Alignment",
      "original": "2123",
      "page_count": 5,
      "order": 817,
      "p1": "4004",
      "pn": "4008",
      "abstract": [
        "We propose Jointly trained Duration Informed Transformer (JDI-T), a\nfeed-forward Transformer with a duration predictor jointly trained\nwithout explicit alignments in order to generate an acoustic feature\nsequence from an input text. In this work, inspired by the recent success\nof the duration informed networks such as FastSpeech and DurIAN, we\nfurther simplify its sequential, two-stage training pipeline to a single-stage\ntraining. Specifically, we extract the phoneme duration from the autoregressive\nTransformer on the fly during the joint training instead of pretraining\nthe autoregressive model and using it as a phoneme duration extractor.\nTo our best knowledge, it is the first implementation to jointly train\nthe feed-forward Transformer without relying on a pre-trained phoneme\nduration extractor in a single training pipeline. We evaluate the effectiveness\nof the proposed model on the publicly available Korean Single speaker\nSpeech (KSS) dataset compared to the baseline text-to-speech (TTS)\nmodels trained by ESPnet-TTS.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2123",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "aso20_interspeech": {
      "authors": [
        [
          "Masashi",
          "Aso"
        ],
        [
          "Shinnosuke",
          "Takamichi"
        ],
        [
          "Hiroshi",
          "Saruwatari"
        ]
      ],
      "title": "End-to-End Text-to-Speech Synthesis with Unaligned Multiple Language Units Based on Attention",
      "original": "2347",
      "page_count": 5,
      "order": 818,
      "p1": "4009",
      "pn": "4013",
      "abstract": [
        "This paper presents the use of unaligned multiple language units for\nend-to-end text-to-speech (TTS). End-to-end TTS is a promising technology\nin that it does not require intermediate representation such as prosodic\ncontexts. However, it causes mispronunciation and unnatural prosody.\nTo alleviate this problem, previous methods have used multiple language\nunits, e.g., phonemes and characters, but required the units to be\nhard-aligned. In this paper, we propose a multi-input attention structure\nthat simultaneously accepts multiple language units without alignments\namong them. We consider using not only traditional phonemes and characters\nbut also subwords tokenized in a language-independent manner. We also\npropose a progressive training strategy to deal with the unaligned\nmultiple language units. The experimental results demonstrated that\nour model and training strategy improve speech quality.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2347",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "dou20_interspeech": {
      "authors": [
        [
          "Qingyun",
          "Dou"
        ],
        [
          "Joshua",
          "Efiong"
        ],
        [
          "Mark J.F.",
          "Gales"
        ]
      ],
      "title": "Attention Forcing for Speech Synthesis",
      "original": "2520",
      "page_count": 5,
      "order": 819,
      "p1": "4014",
      "pn": "4018",
      "abstract": [
        "Auto-regressive sequence-to-sequence models with attention mechanisms\nhave achieved state-of-the-art performance in various tasks including\nspeech synthesis. Training these models can be difficult. The standard\napproach guides a model with the reference output history during training.\nHowever during synthesis the generated output history must be used.\nThis mismatch can impact performance. Several approaches have been\nproposed to handle this, normally by selectively using the generated\noutput history. To make training stable, these approaches often require\na heuristic schedule or an auxiliary classifier. This paper introduces\nattention forcing, which guides the model with the generated output\nhistory and reference attention. This approach reduces the training-evaluation\nmismatch without the need for a schedule or a classifier. Additionally,\nfor standard training approaches, the frame rate is often reduced to\nprevent models from copying the output history. As attention forcing\ndoes not feed the reference output history to the model, it allows\nusing a higher frame rate, which improves the speech quality. Finally,\nattention forcing allows the model to generate output sequences aligned\nwith the references, which is important for some down-stream tasks\nsuch as training neural vocoders. Experiments show that attention forcing\nallows doubling the frame rate, and yields significant gain in speech\nquality.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2520",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "fong20_interspeech": {
      "authors": [
        [
          "Jason",
          "Fong"
        ],
        [
          "Jason",
          "Taylor"
        ],
        [
          "Simon",
          "King"
        ]
      ],
      "title": "Testing the Limits of Representation Mixing for Pronunciation Correction in End-to-End Speech Synthesis",
      "original": "2618",
      "page_count": 5,
      "order": 820,
      "p1": "4019",
      "pn": "4023",
      "abstract": [
        "Accurate pronunciation is an essential requirement for text-to-speech\n(TTS) systems. Systems trained on raw text exhibit pronunciation errors\nin output speech due to ambiguous letter-to-sound relations. Without\nan intermediate phonemic representation, it is difficult to intervene\nand correct these errors. Retaining explicit control over pronunciation\nruns counter to the current drive toward end-to-end (E2E) TTS using\nsequence-to-sequence models. On the one hand, E2E TTS aims to eliminate\nmanual intervention, especially expert skill such as phonemic transcription\nof words in a lexicon. On the other, a system making difficult-to-correct\npronunciation errors is of little practical use. Some intervention\nis necessary. We explore the minimal amount of linguistic features\nrequired to correct pronunciation errors in an otherwise E2E TTS system\nthat accepts graphemic input. We use representation-mixing: within\neach sequence the system accepts either graphemic and/or phonemic input.\nWe quantify how little training data needs to be phonemically labelled\n&#8212; that is, how small a lexicon must be written &#8212; to ensure\ncontrol over pronunciation. We find modest correction is possible with\n500 phonemised word types from the LJ speech dataset but correction\nworks best when the majority of word types are phonemised with syllable\nboundaries.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2618",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "chen20r_interspeech": {
      "authors": [
        [
          "Mingjian",
          "Chen"
        ],
        [
          "Xu",
          "Tan"
        ],
        [
          "Yi",
          "Ren"
        ],
        [
          "Jin",
          "Xu"
        ],
        [
          "Hao",
          "Sun"
        ],
        [
          "Sheng",
          "Zhao"
        ],
        [
          "Tao",
          "Qin"
        ]
      ],
      "title": "MultiSpeech: Multi-Speaker Text to Speech with Transformer",
      "original": "3139",
      "page_count": 5,
      "order": 821,
      "p1": "4024",
      "pn": "4028",
      "abstract": [
        "Transformer-based text to speech (TTS) model (e.g., Transformer TTS\n[1], FastSpeech [2]) has shown the advantages of training and inference\nefficiency over RNN-based model (e.g., Tacotron [3]) due to its parallel\ncomputation in training and/or inference. However, the parallel computation\nincreases the difficulty while learning the alignment between text\nand speech in Transformer, which is further magnified in the multi-speaker\nscenario with noisy data and diverse speakers, and hinders the applicability\nof Transformer for multi-speaker TTS. In this paper, we develop a robust\nand high-quality multi-speaker Transformer TTS system called MultiSpeech,\nwith several specially designed components/techniques to improve text-to-speech\nalignment: 1) a diagonal constraint on the weight matrix of encoder-decoder\nattention in both training and inference; 2) layer normalization on\nphoneme embedding in encoder to better preserve position information;\n3) a bottleneck in decoder pre-net to prevent copy between consecutive\nspeech frames. Experiments on VCTK and LibriTTS multi-speaker datasets\ndemonstrate the effectiveness of MultiSpeech: 1) it synthesizes more\nrobust and better quality multi-speaker voice than naive Transformer\nbased TTS; 2) with a MultiSpeech model as the teacher, we obtain a\nstrong multi-speaker FastSpeech model with almost zero quality degradation\nwhile enjoying extremely fast inference speed.\n"
      ],
      "doi": "10.21437/Interspeech.2020-3139",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "papadopoulos20_interspeech": {
      "authors": [
        [
          "Pavlos",
          "Papadopoulos"
        ],
        [
          "Shrikanth",
          "Narayanan"
        ]
      ],
      "title": "Exploiting Conic Affinity Measures to Design Speech Enhancement Systems Operating in Unseen Noise Conditions",
      "original": "1269",
      "page_count": 5,
      "order": 822,
      "p1": "4029",
      "pn": "4033",
      "abstract": [
        "Speech enhancement under unseen noise conditions is a challenging task,\nbut essential for meeting the increasing demand for speech technologies\nto operate in diverse and dynamic real world environments. A method\nthat has been widely used to enhance speech signals is nonnegative\nmatrix factorization (NMF). In the training phase NMF produces speech\nand noise dictionaries which are represented as matrices with nonnegative\nentries. The quality of the enhanced signal depends on the reconstruction\nability of the dictionaries. A geometric interpretation of these nonnegative\nmatrices enables us to cast them as convex polyhedral cones in the\npositive orthant. In this work, we employ conic affinity measures to\ndesign systems able to operate in unseen noise conditions, by selecting\nan appropriate noise dictionary amongst a pool of potential candidates.\nWe show that such a method yields results similar to those that would\nbe produced if the oracle noise dictionary was used.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1269",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "ji20_interspeech": {
      "authors": [
        [
          "Yunyun",
          "Ji"
        ],
        [
          "Longting",
          "Xu"
        ],
        [
          "Wei-Ping",
          "Zhu"
        ]
      ],
      "title": "Adversarial Dictionary Learning for Monaural Speech Enhancement",
      "original": "2500",
      "page_count": 5,
      "order": 823,
      "p1": "4034",
      "pn": "4038",
      "abstract": [
        "In this paper, we propose an adversarial dictionary learning method\nto train a speaker independent speech dictionary and a universal noise\ndictionary for improving the generality of the dictionary learning\nbased speech enhancement system. In the learning stage, two discriminators\nare employed separately to identify the components in speech and noise\nwhich are highly correlated with each other. The residuals in the speech\nand noise magnitude spectral matrices are then utilized to train the\nspeech and noise dictionaries via the alternating direction method\nof multiplier algorithm, which can effectively reduce the mutual coherence\nbetween speech and noise. In the enhancement stage, a new optimization\ntechnique is proposed for enhancing the speech based on the low-rank\ndecomposition and sparse coding. Experimental results show that our\nproposed method achieves better performance in improving the speech\nquality and intelligibility than the reference methods in terms of\nthree objective performance evaluation measures.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2500",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "seki20_interspeech": {
      "authors": [
        [
          "Shogo",
          "Seki"
        ],
        [
          "Moe",
          "Takada"
        ],
        [
          "Tomoki",
          "Toda"
        ]
      ],
      "title": "Semi-Supervised Self-Produced Speech Enhancement and Suppression Based on Joint Source Modeling of Air- and Body-Conducted Signals Using Variational Autoencoder",
      "original": "2055",
      "page_count": 5,
      "order": 824,
      "p1": "4039",
      "pn": "4043",
      "abstract": [
        "This paper proposes a semi-supervised method for enhancing and suppressing\nself-produced speech, using a variational autoencoder (VAE) to jointly\nmodel self-produced speech recorded with air- and body-conductive microphones.\nIn speech enhancement and suppression for self-produced speech, body-conducted\nsignals can be used as an acoustical clue since they are robust against\nexternal noise and include self-produced speech predominantly. We have\npreviously developed a semi-supervised method taking an improved source\nmodeling approach called the joint source modeling, which can capture\na nonlinear correspondence of air- and body-conducted signals using\nnon-negative matrix factorization (NMF). This allows enhanced and suppressed\nair-conducted self-produced speech to be prevented from contaminating\nby the characteristics of body-conducted signals. However, our previous\nmethod employs a rank-1 spatial model, which is effective but difficult\nto consider in more practical situations. Furthermore, joint source\nmodeling depends on the representation capability of NMF. As a result,\nenhancement and suppression performances are limited. To overcome these\nlimitations, this paper employs a full-rank spatial model and proposes\na joint source modeling of air- and body-conducted signals using a\nVAE, which has shown to represent source signals more accurately than\nNMF. Experimental results revealed that the proposed method outperformed\nbaseline methods.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2055",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "weisman20_interspeech": {
      "authors": [
        [
          "Ran",
          "Weisman"
        ],
        [
          "Vladimir",
          "Tourbabin"
        ],
        [
          "Paul",
          "Calamia"
        ],
        [
          "Boaz",
          "Rafaely"
        ]
      ],
      "title": "Spatial Covariance Matrix Estimation for Reverberant Speech with Application to Speech Enhancement",
      "original": "2224",
      "page_count": 5,
      "order": 825,
      "p1": "4044",
      "pn": "4048",
      "abstract": [
        "A wide range of applications in speech and audio signal processing\nincorporate a model of room reverberation based on the spatial covariance\nmatrix (SCM). Typically, a diffuse sound field model is used, but although\nthe diffuse model simplifies formulations, it may lead to limited accuracy\nin realistic sound fields, resulting in potential degradation in performance.\nWhile some extensions to the diffuse field SCM recently have been presented,\naccurate modeling for real sound fields remains an open problem. In\nthis paper, a method for estimating the SCM of reverberant speech is\nproposed, based on the selection of time-frequency bins dominated by\nreverberation. The method is data-based and estimates the SCM for a\nspecific acoustic scene. It is therefore applicable to realistic reverberant\nfields. An application of the proposed method to optimal beamforming\nfor speech enhancement is presented, using the plane wave density function\nin the spherical harmonics (SH) domain. It is shown that the use of\nthe proposed SCM outperforms the commonly used diffuse field SCM, suggesting\nthe method is more successful in capturing the statistics of the late\npart of the reverberation.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2224",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "ho20_interspeech": {
      "authors": [
        [
          "Minh Tri",
          "Ho"
        ],
        [
          "Jinyoung",
          "Lee"
        ],
        [
          "Bong-Ki",
          "Lee"
        ],
        [
          "Dong Hoon",
          "Yi"
        ],
        [
          "Hong-Goo",
          "Kang"
        ]
      ],
      "title": "A Cross-Channel Attention-Based Wave-U-Net for Multi-Channel Speech Enhancement",
      "original": "2548",
      "page_count": 5,
      "order": 826,
      "p1": "4049",
      "pn": "4053",
      "abstract": [
        "In this paper, we present a novel architecture for multi-channel speech\nenhancement using a cross-channel attention-based Wave-U-Net structure.\nDespite the advantages of utilizing spatial information as well as\nspectral information, it is challenging to effectively train a multi-channel\ndeep learning system in an end-to-end framework. With a channel-independent\nencoding architecture for spectral estimation and a strategy to extract\nspatial information through an inter-channel attention mechanism, we\nimplement a multi-channel speech enhancement system that has high performance\neven in reverberant and extremely noisy environments. Experimental\nresults show that the proposed architecture has superior performance\nin terms of signal-to-distortion ratio improvement (SDRi), short-time\nobjective intelligence (STOI), and phoneme error rate (PER) for speech\nrecognition.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2548",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "fedorov20_interspeech": {
      "authors": [
        [
          "Igor",
          "Fedorov"
        ],
        [
          "Marko",
          "Stamenovic"
        ],
        [
          "Carl",
          "Jensen"
        ],
        [
          "Li-Chia",
          "Yang"
        ],
        [
          "Ari",
          "Mandell"
        ],
        [
          "Yiming",
          "Gan"
        ],
        [
          "Matthew",
          "Mattina"
        ],
        [
          "Paul N.",
          "Whatmough"
        ]
      ],
      "title": "TinyLSTMs: Efficient Neural Speech Enhancement for Hearing Aids",
      "original": "1864",
      "page_count": 5,
      "order": 827,
      "p1": "4054",
      "pn": "4058",
      "abstract": [
        "Modern speech enhancement algorithms achieve remarkable noise suppression\nby means of large recurrent neural networks (RNNs). However, large\nRNNs limit practical deployment in hearing aid hardware (HW) form-factors,\nwhich are battery powered and run on resource-constrained microcontroller\nunits (MCUs) with limited memory capacity and compute capability. In\nthis work, we use model compression techniques to bridge this gap.\nWe define the constraints imposed on the RNN by the HW and describe\na method to satisfy them. Although model compression techniques are\nan active area of research, we are the first to demonstrate their efficacy\nfor RNN speech enhancement, using pruning and integer quantization\nof weights/activations. We also demonstrate state update skipping,\nwhich reduces the computational load. Finally, we conduct a perceptual\nevaluation of the compressed models to verify audio quality on human\nraters. Results show a reduction in model size and operations of 11.9&#215;\nand 2.9&#215;, respectively, over the baseline for compressed models,\nwithout a statistical difference in listening preference and only exhibiting\na loss of 0.55dB SDR. Our model achieves a computational latency of\n2.39ms, well within the 10ms target and 351&#215; better than previous\nwork.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1864",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "hikosaka20_interspeech": {
      "authors": [
        [
          "Shu",
          "Hikosaka"
        ],
        [
          "Shogo",
          "Seki"
        ],
        [
          "Tomoki",
          "Hayashi"
        ],
        [
          "Kazuhiro",
          "Kobayashi"
        ],
        [
          "Kazuya",
          "Takeda"
        ],
        [
          "Hideki",
          "Banno"
        ],
        [
          "Tomoki",
          "Toda"
        ]
      ],
      "title": "Intelligibility Enhancement Based on Speech Waveform Modification Using Hearing Impairment",
      "original": "2062",
      "page_count": 5,
      "order": 828,
      "p1": "4059",
      "pn": "4063",
      "abstract": [
        "In this paper, we propose a speech waveform modification method which\nincorporates a hearing impairment simulator, to improve speech intelligibility\nfor the hearing-impaired. The settings of hearing aid devices usually\nneed to be manually adjusted to suit the needs of each user, which\ncreates a significant burden. To address this issue, the proposed method\ncreates a spectral shaping filter, using a hearing impairment simulator\ncapable of estimating speech signals as perceived by a specific hearing-impaired\nperson. We conduct objective and subjective evaluations through simulations\nusing the hearing impairment simulator. Our experimental results demonstrate\nthat; 1) the proposed spectral shaping filter can significantly improve\nboth speech intelligibility and quality, 2) the filter can be combined\nwith a well-known speech intelligibility enhancement technique based\non power compensation using dynamic range compression (DRC), and 3)\nspeech intelligibility can be further improved by controlling the trade-off\nbetween filtering and DRC-based power compensation.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2062"
    },
    "hou20c_interspeech": {
      "authors": [
        [
          "Nana",
          "Hou"
        ],
        [
          "Chenglin",
          "Xu"
        ],
        [
          "Van Tung",
          "Pham"
        ],
        [
          "Joey Tianyi",
          "Zhou"
        ],
        [
          "Eng Siong",
          "Chng"
        ],
        [
          "Haizhou",
          "Li"
        ]
      ],
      "title": "Speaker and Phoneme-Aware Speech Bandwidth Extension with Residual Dual-Path Network",
      "original": "1994",
      "page_count": 5,
      "order": 829,
      "p1": "4064",
      "pn": "4068",
      "abstract": [
        "Speech bandwidth extension aims to generate a wideband signal from\na narrowband (low-band) input by predicting the missing high-frequency\ncomponents. It is believed that the general knowledge about the speaker\nand phonetic content strengthens the prediction. In this paper, we\npropose to augment the low-band acoustic features with i-vector and\nphonetic posteriorgram (PPG), which represent speaker and phonetic\ncontent of the speech, respectively. We also propose a residual dual-path\nnetwork (RDPN) as the core module to process the augmented features,\nwhich fully utilizes the utterance-level temporal continuity information\nand avoids gradient vanishing. Experiments show that the proposed method\nachieves 20.2% and 7.0% relative improvements over the best baseline\nin terms of log-spectral distortion (LSD) and signal-to-noise ratio\n(SNR), respectively. Furthermore, our method is 16 times more compact\nthan the best baseline in terms of the number of parameters.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1994",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "hou20d_interspeech": {
      "authors": [
        [
          "Nana",
          "Hou"
        ],
        [
          "Chenglin",
          "Xu"
        ],
        [
          "Joey Tianyi",
          "Zhou"
        ],
        [
          "Eng Siong",
          "Chng"
        ],
        [
          "Haizhou",
          "Li"
        ]
      ],
      "title": "Multi-Task Learning for End-to-End Noise-Robust Bandwidth Extension",
      "original": "2022",
      "page_count": 5,
      "order": 830,
      "p1": "4069",
      "pn": "4073",
      "abstract": [
        "Bandwidth extension aims to reconstruct wideband speech signals from\nnarrowband inputs to improve perceptual quality. Prior studies mostly\nperform bandwidth extension under the assumption that the narrowband\nsignals are clean without noise. The use of such extension techniques\nis greatly limited in practice when signals are corrupted by noise.\nTo alleviate such problem, we propose an end-to-end time-domain framework\nfor noise-robust bandwidth extension, that jointly optimizes a mask-based\nspeech enhancement and an ideal bandwidth extension module with multi-task\nlearning. The proposed framework avoids decomposing the signals into\nmagnitude and phase spectra, therefore, requires no phase estimation.\nExperimental results show that the proposed method achieves 14.3% and\n15.8% relative improvements over the best baseline in terms of perceptual\nevaluation of speech quality (PESQ) and log-spectral distortion (LSD),\nrespectively. Furthermore, our method is 3 times more compact than\nthe best baseline in terms of the number of parameters.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2022",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "hu20h_interspeech": {
      "authors": [
        [
          "Shichao",
          "Hu"
        ],
        [
          "Bin",
          "Zhang"
        ],
        [
          "Beici",
          "Liang"
        ],
        [
          "Ethan",
          "Zhao"
        ],
        [
          "Simon",
          "Lui"
        ]
      ],
      "title": "Phase-Aware Music Super-Resolution Using Generative Adversarial Networks",
      "original": "2605",
      "page_count": 5,
      "order": 831,
      "p1": "4074",
      "pn": "4078",
      "abstract": [
        "Audio super-resolution is a challenging task of recovering the missing\nhigh-resolution features from a low-resolution signal. To address this,\ngenerative adversarial networks (GAN) have been used to achieve promising\nresults by training the mappings between magnitudes of the low and\nhigh-frequency components. However, phase information is not well-considered\nfor waveform reconstruction in conventional methods. In this paper,\nwe tackle the problem of music super-resolution and conduct a thorough\ninvestigation on the importance of phase for this task. We use GAN\nto predict the magnitudes of the high-frequency components. The corresponding\nphase information can be extracted using either a GAN-based waveform\nsynthesis system or a modified Griffin-Lim algorithm. Experimental\nresults show that phase information plays an important role in the\nimprovement of the reconstructed music quality. Moreover, our proposed\nmethod significantly outperforms other state-of-the-art methods in\nterms of objective evaluations.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2605",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "huang20d_interspeech": {
      "authors": [
        [
          "Jian",
          "Huang"
        ],
        [
          "Jianhua",
          "Tao"
        ],
        [
          "Bin",
          "Liu"
        ],
        [
          "Zheng",
          "Lian"
        ]
      ],
      "title": "Learning Utterance-Level Representations with Label Smoothing for Speech Emotion Recognition",
      "original": "1391",
      "page_count": 5,
      "order": 832,
      "p1": "4079",
      "pn": "4083",
      "abstract": [
        "Emotion is high-level paralinguistic information characteristics in\nspeech. The most essential part of speech emotion recognition is to\ngenerate robust utterance-level emotional feature representations.\nThe commonly used approaches are pooling methods based on various models,\nwhich may lead to the loss of detailed information for emotion classification.\nIn this paper, we utilize the NetVLAD as trainable discriminative clustering\nto aggregate frame-level descriptors into a single utterance-level\nvector. In addition, to relieve the influence of imbalanced emotional\nclasses, we utilize unigram label smoothing with prior emotional class\ndistribution to regularize the model. Our experimental results on the\nInteractive Emotional Motion Capture (IEMOCAP) database reveal that\nour proposed methods are beneficial to performance improvement, which\nis 3% better than other models.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1391",
      "author_area_id": "3",
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "jalal20_interspeech": {
      "authors": [
        [
          "Md. Asif",
          "Jalal"
        ],
        [
          "Rosanna",
          "Milner"
        ],
        [
          "Thomas",
          "Hain"
        ],
        [
          "Roger K.",
          "Moore"
        ]
      ],
      "title": "Removing Bias with Residual Mixture of Multi-View Attention for Speech Emotion Recognition",
      "original": "3005",
      "page_count": 5,
      "order": 833,
      "p1": "4084",
      "pn": "4088",
      "abstract": [
        "Speech emotion recognition is essential for obtaining emotional intelligence\nwhich affects the understanding of context and meaning of speech. The\nfundamental challenges of speech emotion recognition from a machine\nlearning standpoint is to extract patterns which carry maximum correlation\nwith the emotion information encoded in this signal, and to be as insensitive\nas possible to other types of information carried by speech. In this\npaper, a novel recurrent residual temporal context modelling framework\nis proposed. The framework includes mixture of multi-view attention\nsmoothing and high dimensional feature projection for context expansion\nand learning feature representations. The framework is designed to\nbe robust to changes in speaker and other distortions, and it provides\nstate-of-the-art results for speech emotion recognition. Performance\nof the proposed approach is compared with a wide range of current architectures\nin a standard 4-class classification task on the widely used IEMOCAP\ncorpus. A significant improvement of 4% unweighted accuracy over state-of-the-art\nsystems is observed. Additionally, the attention vectors have been\naligned with the input segments and plotted at two different attention\nlevels to demonstrate the effectiveness.\n"
      ],
      "doi": "10.21437/Interspeech.2020-3005",
      "author_area_id": "3",
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "fan20d_interspeech": {
      "authors": [
        [
          "Weiquan",
          "Fan"
        ],
        [
          "Xiangmin",
          "Xu"
        ],
        [
          "Xiaofen",
          "Xing"
        ],
        [
          "Dongyan",
          "Huang"
        ]
      ],
      "title": "Adaptive Domain-Aware Representation Learning for Speech Emotion Recognition",
      "original": "2572",
      "page_count": 5,
      "order": 834,
      "p1": "4089",
      "pn": "4093",
      "abstract": [
        "Speech emotion recognition is a crucial part in human-computer interaction.\nHowever, representation learning is challenging due to much variability\nfrom speech emotion signals across diverse domains, such as gender,\nage, languages, and social cultural context. Many approaches focus\non domain-invariant representation learning which loses the domain-specific\nknowledge and results in unsatisfactory speech emotion recognition\nacross domains. In this paper, we propose an adaptive domain-aware\nrepresentation learning that leverages the domain knowledge to extract\ndomain aware features. The proposed approach applies attention model\non frequency to embed the domain knowledge in the emotion representation\nspace. Experiments demonstrate that our approach on IEMOCAP achieves\nthe state-of-the-art performance under the same experimental conditions\nwith WA of 73.02% and UA of 65.86%.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2572",
      "author_area_id": "3",
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "zhou20f_interspeech": {
      "authors": [
        [
          "Huan",
          "Zhou"
        ],
        [
          "Kai",
          "Liu"
        ]
      ],
      "title": "Speech Emotion Recognition with Discriminative Feature Learning",
      "original": "2237",
      "page_count": 4,
      "order": 835,
      "p1": "4094",
      "pn": "4097",
      "abstract": [
        "The performance of a speech emotion recognition (SER) system heavily\nrelies on the deep feature learned from the speeches. Most state of\nthe art has focused on developing various deep architectures for effective\nfeature learning. In this study, we make the first attempt to explore\nfeature discriminability instead. Based on our SER baseline system,\nwe propose three approaches, two on loss functions and one on combined\nattentive pooling, to enhance feature discriminability. Evaluations\non IEMOCAP database consistently validate the effectiveness of all\nour proposals. Compared to the baseline system, the proposed three\nsystems demonstrated at least +4.0% absolute improvements in accuracy,\nwith no increment in the total number of parameters.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2237",
      "author_area_id": "3",
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "zhou20g_interspeech": {
      "authors": [
        [
          "Hengshun",
          "Zhou"
        ],
        [
          "Jun",
          "Du"
        ],
        [
          "Yan-Hui",
          "Tu"
        ],
        [
          "Chin-Hui",
          "Lee"
        ]
      ],
      "title": "Using Speech Enhancement Preprocessing for Speech Emotion Recognition in Realistic Noisy Conditions",
      "original": "2472",
      "page_count": 5,
      "order": 836,
      "p1": "4098",
      "pn": "4102",
      "abstract": [
        "In this study, we investigate the effects of deep learning (DL)-based\nspeech enhancement (SE) on speech emotion recognition (SER) in realistic\nenvironments. First, we use emotion speech data to train regression-based\nspeech enhancement models which is shown to be beneficial to noisy\nspeech emotion recognition. Next, to improve the model generalization\ncapability of the regression model, an LSTM architecture with a design\nof hidden layers via simply densely-connected progressive learning,\nis adopted for the enhancement model. Finally, a post-processor utilizing\nan improved speech presence probability to estimate masks from the\nabove proposed LSTM structure is shown to further improves recognition\naccuracies. Experiments results on the IEMOCAP and CHEAVD 2.0 corpora\ndemonstrate that the proposed framework can yield consistent and significant\nimprovements over the systems using unprocessed noisy speech.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2472",
      "author_area_id": "3",
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "li20ia_interspeech": {
      "authors": [
        [
          "Yongwei",
          "Li"
        ],
        [
          "Jianhua",
          "Tao"
        ],
        [
          "Bin",
          "Liu"
        ],
        [
          "Donna",
          "Erickson"
        ],
        [
          "Masato",
          "Akagi"
        ]
      ],
      "title": "Comparison of Glottal Source Parameter Values in Emotional Vowels",
      "original": "1536",
      "page_count": 5,
      "order": 837,
      "p1": "4103",
      "pn": "4107",
      "abstract": [
        "Since glottal source plays an important role for expressing emotions\nin speech, it is crucial to compare a set of glottal source parameter\nvalues to find differences in these expressions of emotions for emotional\nspeech recognition and synthesis. This paper focuses on comparing a\nset of glottal source parameter values among varieties of emotional\nvowels /a/ (joy, neutral, anger, and sadness) using an improved ARX-LF\nmodel algorithm. The set of glottal source parameters included in the\ncomparison were T<SUB>p</SUB>, T<SUB>e</SUB>, T<SUB>a</SUB>, E<SUB>e</SUB>,\nand F<SUB>0</SUB>(1/T<SUB>0</SUB>) in the LF model; parameter values\nwere divided into 5 levels according to that of neutral vowel. Results\nshowed that each emotion has its own levels for each set of the glottal\nsource parameter value. These findings could be used for emotional\nspeech recognition and synthesis.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1536",
      "author_area_id": "3",
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "chou20_interspeech": {
      "authors": [
        [
          "Huang-Cheng",
          "Chou"
        ],
        [
          "Chi-Chun",
          "Lee"
        ]
      ],
      "title": "Learning to Recognize Per-Rater&#8217;s Emotion Perception Using Co-Rater Training Strategy with Soft and Hard Labels",
      "original": "1714",
      "page_count": 5,
      "order": 838,
      "p1": "4108",
      "pn": "4112",
      "abstract": [
        "An individual&#8217;s emotion perception plays a key role in affecting\nour decision-making and task performances. Previous speech emotion\nrecognition research focuses mainly on recognizing the emotion label\nderived from the majority vote (hard label) of the speaker (i.e., producer)\nbut not on recognizing per-rater&#8217;s emotion perception. In this\nwork, we propose a framework that integrates different viewpoints of\nemotion perception from other co-raters (exclude target rater) using\nsoft and hard label learning to improve target rater&#8217;s emotion\nperception recognition. Our methods achieve [3.97%, 1.48%] and [1.71%,\n2.87%] improvement on average unweighted accuracy recall (UAR) on the\nthree-class (low, middle, and high class) [valence, activation (arousal)]\nemotion recognition task for four different raters on the IEMOCAP and\nthe NNIME databases, respectively. Further analyses show that learning\nfrom the soft label of co-raters provides the most robust accuracy\neven without obtaining the target rater&#8217;s labels. By simply adding\n50% of a target raters annotation, our framework performance mostly\nsurpasses the model trained with 100% of raters annotations.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1714",
      "author_area_id": "3",
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "jalal20b_interspeech": {
      "authors": [
        [
          "Md. Asif",
          "Jalal"
        ],
        [
          "Rosanna",
          "Milner"
        ],
        [
          "Thomas",
          "Hain"
        ]
      ],
      "title": "Empirical Interpretation of Speech Emotion Perception with Attention Based Model for Speech Emotion Recognition",
      "original": "3007",
      "page_count": 5,
      "order": 839,
      "p1": "4113",
      "pn": "4117",
      "abstract": [
        "Speech emotion recognition is essential for obtaining emotional intelligence\nwhich affects the understanding of context and meaning of speech. Harmonically\nstructured vowel and consonant sounds add indexical and linguistic\ncues in spoken information. Previous research argued whether vowel\nsound cues were more important in carrying the emotional context from\na psychological and linguistic point of view. Other research also claimed\nthat emotion information could exist in small overlapping acoustic\ncues. However, these claims are not corroborated in computational speech\nemotion recognition systems. In this research, a convolution-based\nmodel and a long-short-term memory-based model, both using attention,\nare applied to investigate these theories of speech emotion on computational\nmodels. The role of acoustic context and word importance is demonstrated\nfor the task of speech emotion recognition. The IEMOCAP corpus is evaluated\nby the proposed models, and 80.1% unweighted accuracy is achieved on\npure acoustic data which is higher than current state-of-the-art models\non this task. The phones and words are mapped to the attention vectors\nand it is seen that the vowel sounds are more important for defining\nemotion acoustic cues than the consonants, and the model can assign\nword importance based on acoustic context.\n"
      ],
      "doi": "10.21437/Interspeech.2020-3007",
      "author_area_id": "3",
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "gessinger20_interspeech": {
      "authors": [
        [
          "Iona",
          "Gessinger"
        ],
        [
          "Bernd",
          "M\u00f6bius"
        ],
        [
          "Bistra",
          "Andreeva"
        ],
        [
          "Eran",
          "Raveh"
        ],
        [
          "Ingmar",
          "Steiner"
        ]
      ],
      "title": "Phonetic Accommodation of L2 German Speakers to the Virtual Language Learning Tutor Mirabella",
      "original": "2701",
      "page_count": 5,
      "order": 840,
      "p1": "4118",
      "pn": "4122",
      "abstract": [
        "The present paper compares phonetic accommodation of L1 French speakers\nin interaction with the simulated virtual language learning tutor for\nGerman, Mirabella, to that of L1 German speakers from a previous study.\nIn a question-and-answer exchange, the L1 French speakers adapted the\nintonation contours of wh-questions as falling or rising according\nto the variant produced by Mirabella. However, they were not sensitive\nto a change of the nuclear pitch accent placement. In a map task, the\nL1 French speakers increased the number of dispreferred variants for\nthe allophonic contrast [&#618;&#231;] vs. [&#618;k] in the word ending\n &#x27e8;-ig&#x27e9; when Mirabella used this variant. For the contrast\n[&#949;&#720;] vs. [e&#720;] as a realization of stressed &#x27e8;-&#228;-&#x27e9;,\nsuch a convergence effect was not found. Overall, the non-native speakers\nshowed a similar degree of accommodative behavior towards Mirabella\nas the L1 German speakers. This suggests that incidental inductive\nlearning through accommodation is possible. However, phenomena of the\ntarget language that deviate too radically from the native pattern\nseem to require more explicit training. \n"
      ],
      "doi": "10.21437/Interspeech.2020-2701",
      "author_area_id": "2",
      "author_area_label": "Phonetics, Phonology, and Prosody"
    },
    "gu20b_interspeech": {
      "authors": [
        [
          "Yuling",
          "Gu"
        ],
        [
          "Nancy F.",
          "Chen"
        ]
      ],
      "title": "Characterization of Singaporean Children&#8217;s English: Comparisons to American and British Counterparts Using Archetypal Analysis",
      "original": "3166",
      "page_count": 5,
      "order": 841,
      "p1": "4123",
      "pn": "4127",
      "abstract": [
        "In this work, we investigate pronunciation differences in English spoken\nby Singaporean children in relation to their American and British counterparts\nby conducting archetypal clustering and formant space analysis on selected\nvowel pairs. Given that Singapore adopts British English as the institutional\nstandard due to historical reasons, one might expect Singaporean children\nto follow British pronunciation patterns, but interestingly we observe\nthat Singaporean children present similar patterns to American children\nwhen it comes to TRAP&#8211;BATH split vowels and /&#230;/ vs. /&#949;/\nproductions: Singaporean and American speakers both exhibit more fronted\ncharacteristics (p &#60; 0.001) for vowels in these vowel pairs, resulting\nin less contrast compared to British speakers. In addition, when producing\nthese vowels, the first formant frequency estimates of Singaporean\nchildren is consistently lower, suggesting a higher tongue position,\ndistinguishing them from American and British speakers (p &#60; 0.05).\n"
      ],
      "doi": "10.21437/Interspeech.2020-3166",
      "author_area_id": "2",
      "author_area_label": "Phonetics, Phonology, and Prosody"
    },
    "kaminskaia20_interspeech": {
      "authors": [
        [
          "Svetlana",
          "Kaminska\u00efa"
        ]
      ],
      "title": "Rhythmic Convergence in Canadian French Varieties?",
      "original": "2963",
      "page_count": 5,
      "order": 842,
      "p1": "4128",
      "pn": "4132",
      "abstract": [
        "Studies of prosodic rhythm in a minority Ontario French using rhythm\nmetrics did not demonstrate the effect of contact with English; moreover,\nthey demonstrated an even more syllable-timed (French) pattern in this\ncontact variety than in majority Canadian and European ones. To understand\nthese results and further explore regional variation in Canadian French\nand the effect of linguistic contact, syllabic typology, length and\nduration of the stress group, syllable duration ratios, and vowel intensity\nare explored here through a comparison of a minority variety with a\nmajority Canadian French (Quebec). Spontaneous samples show the same\nsyllabic typology and distribution, stress group length and duration,\nsimilar syllable ratios, and a regular rhythmic pattern in both Canadian\nvarieties. The analysis of intensity of stressed syllables, however,\nsuggested divergence of the datasets from both traditional description\nof French and from each other. Thus, intensity accompanies primary\nstress in Ontario but not in Quebec, and both varieties use intensity\nto mark secondary stress. These results suggest a convergence to the\nneighboring English language and need to be confirmed in a controlled\nsetting.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2963"
    },
    "manghat20_interspeech": {
      "authors": [
        [
          "Sreeja",
          "Manghat"
        ],
        [
          "Sreeram",
          "Manghat"
        ],
        [
          "Tanja",
          "Schultz"
        ]
      ],
      "title": "Malayalam-English Code-Switched: Grapheme to Phoneme System",
      "original": "1936",
      "page_count": 5,
      "order": 843,
      "p1": "4133",
      "pn": "4137",
      "abstract": [
        "Grapheme to phoneme conversion is an integral aspect of speech processing.\nConversational speech in Malayalam &#8212; a low resource Indic language\nhas inter-sentential, intra-sentential code-switching as well as frequent\nintra-word code-switching with English. Monolingual G2P systems cannot\nprocess such special intra-word code-switching scenarios. A G2P system\nwhich can handle code-switching developed based on Malayalam-English\ncode-switch speech and text corpora is presented. Since neither Malayalam\nnor English are phonetic subset of each other, the overlapping phonemes\nfor English&#8211;Malayalam are identified and analysed. Additional\nrules used to handle special cases of Malayalam phonemes and intra-word\ncode-switching in the G2P system is also presented specifically.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1936",
      "author_area_id": "2",
      "author_area_label": "Phonetics, Phonology, and Prosody"
    },
    "hutin20_interspeech": {
      "authors": [
        [
          "Mathilde",
          "Hutin"
        ],
        [
          "Ad\u00e8le",
          "Jatteau"
        ],
        [
          "Ioana",
          "Vasilescu"
        ],
        [
          "Lori",
          "Lamel"
        ],
        [
          "Martine",
          "Adda-Decker"
        ]
      ],
      "title": "Ongoing Phonologization of Word-Final Voicing Alternations in Two Romance Languages: Romanian and French",
      "original": "1460",
      "page_count": 5,
      "order": 844,
      "p1": "4138",
      "pn": "4142",
      "abstract": [
        "Phonologization is a process whereby phonetic substance becomes phonological\nstructure [1]. The process involves at least two steps: (i) a universal\nphonetic (&#8216;automatic&#8217;) variation becomes a language-specific\n(&#8216;speaker-controlled&#8217;) pattern, (ii) the language-specific\npattern becomes a phonological (&#8216;structured&#8217;) object. This\npaper will focus on the first step and ask the question of whether\nthree universal phonetic variations of the laryngeal feature of word-final\ncodas (final devoicing, voicelessness assimilation and voicing assimilation)\nare becoming language-specific patterns in two Romance languages, Romanian\nand French. Our results suggest that neutralization processes (final\ndevoicing) might be beginning their phonologization process in both\nFrench and Romanian whereas assimilation processes (regressive assimilation\nof voicing and voicelessness) remain universal phonetic tendencies.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1460",
      "author_area_id": "2",
      "author_area_label": "Phonetics, Phonology, and Prosody"
    },
    "hope20_interspeech": {
      "authors": [
        [
          "Maxwell",
          "Hope"
        ],
        [
          "Jason",
          "Lilley"
        ]
      ],
      "title": "Cues for Perception of Gender in Synthetic Voices and the Role of Identity",
      "original": "2657",
      "page_count": 5,
      "order": 845,
      "p1": "4143",
      "pn": "4147",
      "abstract": [
        "Perception of gender in voice is not an under-researched area. Previous\nstudies have been conducted in the hopes of pinpointing what aspects\nof voice (e.g. fundamental frequency, intonation, etc.) carry the largest\ncues for skewing gender perception. These studies have to date been\nconducted within the framework of the gender binary, i.e. men&#8217;s\nvs. women&#8217;s voices, which have left out the exploration of perception\nof something besides simply femininity and masculinity.<br style=\"mso-data-placement:same-cell;\"\n/><br style=\"mso-data-placement:same-cell;\" />\n  The literature thus\nfar has not endeavored to keep pitch in the &#8220;androgynous&#8221;\nzone while manipulating other aspects such as the F0 contour or other\nacoustic parameters. Additionally, past literature on speech perception\nhas neglected to explicitly include members of the gender expansive\ncommunity. Hence, we recruited participants of all genders and first\nsought to identify cues for gender perception in synthetically made\nvoices and then examine the relationship between one&#8217;s own sense\nof gender identity and the perception of gender in synthetically made\nvoices for native speakers of American English. We found that vocal\ntract acoustics are most important for swaying perception of gender\nand one&#8217;s own gender identity influences gender perception in\nvoice.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2657",
      "author_area_id": "2",
      "author_area_label": "Phonetics, Phonology, and Prosody"
    },
    "menshikova20_interspeech": {
      "authors": [
        [
          "Alla",
          "Menshikova"
        ],
        [
          "Daniil",
          "Kocharov"
        ],
        [
          "Tatiana",
          "Kachkovskaia"
        ]
      ],
      "title": "Phonetic Entrainment in Cooperative Dialogues: A Case of Russian",
      "original": "2696",
      "page_count": 5,
      "order": 846,
      "p1": "4148",
      "pn": "4152",
      "abstract": [
        "It has been shown for a number of languages that speakers accommodate\nto each other in conversation. Such accommodation, or entrainment,\nreveals itself in many modalities including speech: interlocutors are\nfound to entrain in intensity, fundamental frequency, tempo and other\nacoustic features. This paper presents data on speech entrainment in\nRussian using the standard measures for speech entrainment: proximity,\nconvergence and synchrony. The research uses 49 dialogues from the\nSibLing speech corpus where speakers played a card-matching game. The\nlist of acoustic features includes various measures of pitch, energy,\nspectral slope, HNR, jitter, and shimmer. The results for Russian are\ncompared with those published previously for other languages.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2696",
      "author_area_id": "2",
      "author_area_label": "Phonetics, Phonology, and Prosody"
    },
    "xu20g_interspeech": {
      "authors": [
        [
          "Chengwei",
          "Xu"
        ],
        [
          "Wentao",
          "Gu"
        ]
      ],
      "title": "Prosodic Characteristics of Genuine and Mock (Im)polite Mandarin Utterances",
      "original": "3231",
      "page_count": 5,
      "order": 847,
      "p1": "4153",
      "pn": "4157",
      "abstract": [
        "As specialized social affects in speech communication, mock politeness\nand mock impoliteness are usually characterized by unique prosodic\npatterns that conflict with the literal meanings. To give a quantitative\nanalysis of prosodic characteristics, a context-elicited discourse\ncompletion task was conducted to collect genuine and mock (im)polite\nMandarin utterances in both imperative and interrogative modes. Results\nrevealed that prosodic features played roles in a complex way. Mock\npolite speech showed a higher maximum F<SUB>0</SUB> and intensity,\na wider range as well as a higher variability of F<SUB>0</SUB> and\nintensity, a lower HNR, and a higher jitter than genuine polite speech,\nwhereas mock impolite speech showed a lower mean/maximum F<SUB>0</SUB>\nand intensity, a narrower range as well as a lower variability of F<SUB>0</SUB>\nand intensity, a slower speech rate, a higher HNR, and lower jitter,\nshimmer and H1-H2 than genuine impolite speech. In the perceptual experiment,\nthe lower identification rates on mock (im)politeness indicated that\nperceptual judgement was influenced by literal meanings. Politeness\nratings further showed that mock (im)polite speech was less (im)polite\nthan genuine (im)polite speech, suggesting a good correspondence between\nprosodic manifestations and perceived politeness. Moreover, interrogatives\nsounded more polite than imperatives, also verifying the Tact Maxim\nprinciple for politeness.\n"
      ],
      "doi": "10.21437/Interspeech.2020-3231",
      "author_area_id": "2",
      "author_area_label": "Phonetics, Phonology, and Prosody"
    },
    "li20ja_interspeech": {
      "authors": [
        [
          "Yanping",
          "Li"
        ],
        [
          "Catherine T.",
          "Best"
        ],
        [
          "Michael D.",
          "Tyler"
        ],
        [
          "Denis",
          "Burnham"
        ]
      ],
      "title": "Tone Variations in Regionally Accented Mandarin",
      "original": "1235",
      "page_count": 5,
      "order": 848,
      "p1": "4158",
      "pn": "4162",
      "abstract": [
        "The present study investigated tone variations in regionally accented\nMandarin (i.e., Standard Mandarin [SM] spoken by dialectal Chinese\nspeakers) as influenced by the varying tone systems of their native\ndialects. 12 female speakers, four each from Guangzhou, Shanghai and\nYantai, were recruited to produce monosyllabic words in SM that included\nminimal contrasts among the four Mandarin lexical tones. Since SM developed\nfrom the Beijing dialect, their pronunciations were compared to the\nsame Mandarin words produced by four Beijing female speakers. Regional\nMandarin speakers successfully produced the four Mandarin lexical tones,\nbut their productions varied from SM. Two crucial acoustic measures\nfor Mandarin lexical tones, F0 (fundamental frequency) and duration\nvalues, were fitted into linear mixed-effects models on differences\nbetween regional and Beijing accents. Regional speakers had longer\nword duration and different F0 height when producing SM, resulting\nin variations in Mandarin lexical tones across the regional accents.\nThese findings shed light on regional accent variations in Mandarin\nlexical tones and lay a foundation for deeper understanding of their\nimpact on perception of accented Mandarin lexical tones by native (Beijing)\nMandarin listeners.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1235",
      "author_area_id": "2",
      "author_area_label": "Phonetics, Phonology, and Prosody"
    },
    "yang20k_interspeech": {
      "authors": [
        [
          "Yike",
          "Yang"
        ],
        [
          "Si",
          "Chen"
        ],
        [
          "Xi",
          "Chen"
        ]
      ],
      "title": "F0 Patterns in Mandarin Statements of Mandarin and Cantonese Speakers",
      "original": "2549",
      "page_count": 5,
      "order": 849,
      "p1": "4163",
      "pn": "4167",
      "abstract": [
        "Cross-linguistic differences of F0 patterns have been found from both\nmonolingual and bilingual speakers. However, previous studies either\nworked on intonation languages or compared an intonation language with\na tone language. It still remains unknown whether there are F0 differences\nin bilingual speakers of tone languages. This study compared second\nlanguage (L2) Mandarin with Cantonese and first language (L1) Mandarin,\nto test whether the L2 speakers of Mandarin have acquired the F0 patterns\nof Mandarin and whether there are influences from their L1 Cantonese.\nDifferent F0 measurements (including maximum F0, minimum F0, mean F0\nand F0 range) were examined with linear mixed-effects models. Cantonese\nand Mandarin showed different F0 patterns, the source of which still\nrequires further investigation. The L2 Mandarin data resembled the\nF0 patterns of Cantonese and were different from L1 Mandarin, for which\nwe provided different explanations: assimilation of L1 Cantonese and\nL2 Mandarin, the negative transfer from native Cantonese, and similarities\nin the nature of tone languages. Suggestions for testing these assumptions\nare proposed. Lastly, our data provided conflicting results concerning\nthe role of gender in F0 pattern realisation.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2549",
      "author_area_id": "2",
      "author_area_label": "Phonetics, Phonology, and Prosody"
    },
    "chuang20b_interspeech": {
      "authors": [
        [
          "Yung-Sung",
          "Chuang"
        ],
        [
          "Chi-Liang",
          "Liu"
        ],
        [
          "Hung-yi",
          "Lee"
        ],
        [
          "Lin-shan",
          "Lee"
        ]
      ],
      "title": "SpeechBERT: An Audio-and-Text Jointly Learned Language Model for End-to-End Spoken Question Answering",
      "original": "1570",
      "page_count": 5,
      "order": 850,
      "p1": "4168",
      "pn": "4172",
      "abstract": [
        "While various end-to-end models for spoken language understanding tasks\nhave been explored recently, this paper is probably the first known\nattempt to challenge the very difficult task of end-to-end spoken question\nanswering (SQA). Learning from the very successful BERT model for various\ntext processing tasks, here we proposed an audio-and-text jointly learned\nSpeechBERT model. This model outperformed the conventional approach\nof cascading ASR with the following text question answering (TQA) model\non datasets including ASR errors in answer spans, because the end-to-end\nmodel was shown to be able to extract information out of audio data\nbefore ASR produced errors. When ensembling the proposed end-to-end\nmodel with the cascade architecture, even better performance was achieved.\nIn addition to the potential of end-to-end SQA, the SpeechBERT can\nalso be considered for many other spoken language understanding tasks\njust as BERT for many text processing tasks.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1570",
      "author_area_id": "11",
      "author_area_label": "Spoken dialog systems and conversational analysis"
    },
    "kuo20b_interspeech": {
      "authors": [
        [
          "Chia-Chih",
          "Kuo"
        ],
        [
          "Shang-Bao",
          "Luo"
        ],
        [
          "Kuan-Yu",
          "Chen"
        ]
      ],
      "title": "An Audio-Enriched BERT-Based Framework for Spoken Multiple-Choice Question Answering",
      "original": "1763",
      "page_count": 5,
      "order": 851,
      "p1": "4173",
      "pn": "4177",
      "abstract": [
        "In a spoken multiple-choice question answering (SMCQA) task, given\na passage, a question, and multiple choices all in the form of speech,\nthe machine needs to pick the correct choice to answer the question.\nWhile the audio could contain useful cues for SMCQA, usually only the\nauto-transcribed text is utilized in system development. Thanks to\nthe large-scaled pre-trained language representation models, such as\nthe bidirectional encoder representations from transformers (BERT),\nsystems with only auto-transcribed text can still achieve a certain\nlevel of performance. However, previous studies have evidenced that\nacoustic-level statistics can offset text inaccuracies caused by the\nautomatic speech recognition systems or representation inadequacy lurking\nin word embedding generators, thereby making the SMCQA system robust.\nAlong the line of research, this study concentrates on designing a\nBERT-based SMCQA framework, which not only inherits the advantages\nof contextualized language representations learned by BERT, but integrates\nthe complementary acoustic-level information distilled from audio with\nthe text-level information. Consequently, an audio-enriched BERT-based\nSMCQA framework is proposed. A series of experiments demonstrates remarkable\nimprovements in accuracy over selected baselines and SOTA systems on\na published Chinese SMCQA dataset.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1763",
      "author_area_id": "11",
      "author_area_label": "Spoken dialog systems and conversational analysis"
    },
    "huang20e_interspeech": {
      "authors": [
        [
          "Binxuan",
          "Huang"
        ],
        [
          "Han",
          "Wang"
        ],
        [
          "Tong",
          "Wang"
        ],
        [
          "Yue",
          "Liu"
        ],
        [
          "Yang",
          "Liu"
        ]
      ],
      "title": "Entity Linking for Short Text Using Structured Knowledge Graph via Multi-Grained Text Matching",
      "original": "1934",
      "page_count": 5,
      "order": 852,
      "p1": "4178",
      "pn": "4182",
      "abstract": [
        "Entity Linking (EL) recognizes textual mentions of entities and maps\nthem to the corresponding entities in a Knowledge Graph (KG). In this\npaper, we propose a novel method for EL on short text using entity\nrepresentations base on their name labels, descriptions, and other\nrelated entities in the KG. We then leverage a pre-trained BERT model\nto calculate the semantic similarity between the entity and the text.\nThis method does not require a large volume of data to jointly train\nword and entity representations, and is easily portable to a new domain\nwith a KG. We demonstrate that our approach outperforms previous methods\non a public benchmark dataset with a large margin.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1934",
      "author_area_id": "11",
      "author_area_label": "Spoken dialog systems and conversational analysis"
    },
    "zhang20ga_interspeech": {
      "authors": [
        [
          "Mingxin",
          "Zhang"
        ],
        [
          "Tomohiro",
          "Tanaka"
        ],
        [
          "Wenxin",
          "Hou"
        ],
        [
          "Shengzhou",
          "Gao"
        ],
        [
          "Takahiro",
          "Shinozaki"
        ]
      ],
      "title": "Sound-Image Grounding Based Focusing Mechanism for Efficient Automatic Spoken Language Acquisition",
      "original": "2027",
      "page_count": 5,
      "order": 853,
      "p1": "4183",
      "pn": "4187",
      "abstract": [
        "The process of spoken language acquisition based on sound-image grounding\nhas been one of the topics that has attracted the most significant\ninterest of linguists and human scientists for decades. To understand\nthe process and enable new possibilities for intelligent robots, we\ndesigned a spoken-language acquisition task in which a software robot\nlearns to fulfill its desire by correctly identifying and uttering\nthe name of its preferred object from the given images, without relying\non any labeled dataset. We propose an unsupervised vision-based focusing\nstrategy and a pre-training approach based on sound-image grounding\nto boost the efficiency of reinforcement learning. These ideas are\nmotivated by the introspection that human babies first observe the\nworld and then try actions to realize their desires. Our experiments\nshow that the software robot can successfully acquire spoken language\nfrom spoken indications with images and dialogues. Moreover, the learning\nspeed of reinforcement learning is significantly improved compared\nto several baseline approaches.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2027",
      "author_area_id": "11",
      "author_area_label": "Spoken dialog systems and conversational analysis"
    },
    "yamamoto20_interspeech": {
      "authors": [
        [
          "Kenta",
          "Yamamoto"
        ],
        [
          "Koji",
          "Inoue"
        ],
        [
          "Tatsuya",
          "Kawahara"
        ]
      ],
      "title": "Semi-Supervised Learning for Character Expression of Spoken Dialogue Systems",
      "original": "2293",
      "page_count": 5,
      "order": 854,
      "p1": "4188",
      "pn": "4192",
      "abstract": [
        "We address character expression for spoken dialogue systems (e.g. extrovert).\nWhile conventional studies focused on controlling linguistic expressions,\nwe focus on spoken dialogue behaviors. Specifically, the proposed model\nmaps three character traits: extroversion, emotional instability, and\npoliteness to four spoken dialogue behaviors: utterance amount, backchannel,\nfiller, and switching pause length. It is costly to collect annotated\ndata for training this kind of models. Therefore, we propose a semi-supervised\nlearning approach to utilize not only a character impression data (labeled\ndata) but also a corpus data (unlabeled data). Experimental results\nshow that the proposed model expresses the target character traits\nthrough the behaviors more precisely than a baseline model that corresponds\nto the case of supervised learning only. Besides, we also investigate\nhow to model unlabeled behavior (e.g. speech rate) by utilizing the\nadvantage of semi-supervised learning.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2293",
      "author_area_id": "11",
      "author_area_label": "Spoken dialog systems and conversational analysis"
    },
    "shi20h_interspeech": {
      "authors": [
        [
          "Xiaohan",
          "Shi"
        ],
        [
          "Sixia",
          "Li"
        ],
        [
          "Jianwu",
          "Dang"
        ]
      ],
      "title": "Dimensional Emotion Prediction Based on Interactive Context in Conversation",
      "original": "1820",
      "page_count": 5,
      "order": 855,
      "p1": "4193",
      "pn": "4197",
      "abstract": [
        "Emotion prediction in conversation is important for humans to conduct\na fluent conversation, which is an underexplored research topic in\nthe affective computing area. In previous studies, predicting the coming\nemotion only considered the context information from one single speaker.\nHowever, there are two sides of the speaker and listener in interlocutors,\nand their emotions are influenced by one another during the conversation.\nFor this reason, we propose a dimensional emotion prediction model\nbased on interactive information in conversation from both interlocutors.\nWe investigate the effects of interactive information in four conversation\nsituations on emotion prediction, in which emotional tendencies of\ninterlocutors are consistent or inconsistent in both valence and arousal.\nThe results showed that the proposed method performance better by considering\nthe interactive context information than the ones considering one single\nside alone. The prediction result is affected by the conversation situations.\nIn the situation interlocutors have consistent emotional tendency in\nvalence and inconsistent tendency in arousal, the prediction performance\nof valence is the best. In the situation that interlocutors&#8217;\nemotional tendency is inconsistent in both valence and arousal, the\nprediction performance of arousal is the best.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1820",
      "author_area_id": "11",
      "author_area_label": "Spoken dialog systems and conversational analysis"
    },
    "atamna20_interspeech": {
      "authors": [
        [
          "Asma",
          "Atamna"
        ],
        [
          "Chlo\u00e9",
          "Clavel"
        ]
      ],
      "title": "HRI-RNN: A User-Robot Dynamics-Oriented RNN for Engagement Decrease Detection",
      "original": "1261",
      "page_count": 5,
      "order": 856,
      "p1": "4198",
      "pn": "4202",
      "abstract": [
        "Natural and fluid human-robot interaction (HRI) systems rely on the\nrobot&#8217;s ability to accurately assess the user&#8217;s  engagement\nin the interaction. Current HRI systems for engagement analysis, and\nmore broadly emotion recognition, only consider user data while discarding\nrobot data which, in many cases, affects the user state. We present\na novel recurrent neural architecture for online detection of user\nengagement decrease in a spontaneous HRI setting that exploits the\nrobot data. Our architecture models the user as a distinct party in\nthe conversation and uses the robot data as contextual information\nto help assess engagement. We evaluate our approach on a real-world\nhighly imbalanced data set, where we observe up to 2.13% increase in\nF1 score compared to a standard gated recurrent unit (GRU).\n"
      ],
      "doi": "10.21437/Interspeech.2020-1261",
      "author_area_id": "11",
      "author_area_label": "Spoken dialog systems and conversational analysis"
    },
    "fuscone20_interspeech": {
      "authors": [
        [
          "Simone",
          "Fuscone"
        ],
        [
          "Benoit",
          "Favre"
        ],
        [
          "Laurent",
          "Pr\u00e9vot"
        ]
      ],
      "title": "Neural Representations of Dialogical History for Improving Upcoming Turn Acoustic Parameters Prediction",
      "original": "2785",
      "page_count": 5,
      "order": 857,
      "p1": "4203",
      "pn": "4207",
      "abstract": [
        "Predicting the acoustic and linguistic parameters of an upcoming conversational\nturn is important for dialogue systems aiming to include low-level\nadaptation with the user. It is known that during an interaction speakers\ncould influence each other speech production. However, the precise\ndynamics of the phenomena is not well-established, especially in the\ncontext of natural conversations. We developed a model based on an\nRNN architecture that predicts speech variables (Energy, F0 range and\nSpeech Rate) of the upcoming turn using a representation vector describing\nspeech information of previous turns. We compare the prediction performances\nwhen using a dialogical history (from both participants) vs. monological\nhistory (from only upcoming turn&#8217;s speaker). We found that the\ninformation contained in previous turns produced by both the speaker\nand his interlocutor reduce the error in predicting current acoustic\ntarget variable. In addition the error in prediction decreases as increases\nthe number of previous turns taken into account.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2785",
      "author_area_id": "11",
      "author_area_label": "Spoken dialog systems and conversational analysis"
    },
    "hu20i_interspeech": {
      "authors": [
        [
          "Shengli",
          "Hu"
        ]
      ],
      "title": "Detecting Domain-Specific Credibility and Expertise in Text and Speech",
      "original": "1518",
      "page_count": 5,
      "order": 858,
      "p1": "4208",
      "pn": "4212",
      "abstract": [
        "We investigate and explore the interplay of credibility and expertise\nlevel in text and speech. We collect a unique domain-specific multimodal\ndataset and analyze a set of acoustic-prosodic and linguistic features\nin both credible and less credible speech by professionals of varying\nexpertise levels. Our analyses shed light on potential indicators of\ndomain-specific perceived credibility and expertise, as well as the\ninterplay in-between. Moreover, we build multimodal and multi-task\ndeep learning models that outperform human performance by 6.2% in credibility\nand 3.8% in expertise level, building upon state-of-the-art self-supervised\npre-trained language models. To our knowledge, this is the first multimodal\nmulti-task study that analyzes and predicts domain-specific credibility\nand expertise level at the same time.<SUP>1</SUP>\n"
      ],
      "doi": "10.21437/Interspeech.2020-1518",
      "author_area_id": "11",
      "author_area_label": "Spoken dialog systems and conversational analysis"
    },
    "das20c_interspeech": {
      "authors": [
        [
          "Rohan Kumar",
          "Das"
        ],
        [
          "Xiaohai",
          "Tian"
        ],
        [
          "Tomi",
          "Kinnunen"
        ],
        [
          "Haizhou",
          "Li"
        ]
      ],
      "title": "The Attacker&#8217;s Perspective on Automatic Speaker Verification: An Overview",
      "original": "1052",
      "page_count": 5,
      "order": 859,
      "p1": "4213",
      "pn": "4217",
      "abstract": [
        "Security of automatic speaker verification (ASV) systems is compromised\nby various spoofing attacks. While many types of  non-proactive attacks\n(and their defenses) have been studied in the past,  attacker&#8217;s\nperspective on ASV, represents a far less explored direction. It can\npotentially help to identify the weakest parts of ASV systems and be\nused to develop attacker-aware systems. We present an overview on this\nemerging research area by focusing on potential threats of adversarial\nattacks on ASV, spoofing countermeasures, or both. We conclude the\nstudy with discussion on selected attacks and leveraging from such\nknowledge to improve defense mechanisms against adversarial attacks.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1052",
      "author_area_id": "13",
      "author_area_label": "Special Sessions"
    },
    "sholokhov20_interspeech": {
      "authors": [
        [
          "Alexey",
          "Sholokhov"
        ],
        [
          "Tomi",
          "Kinnunen"
        ],
        [
          "Ville",
          "Vestman"
        ],
        [
          "Kong Aik",
          "Lee"
        ]
      ],
      "title": "Extrapolating False Alarm Rates in Automatic Speaker Verification",
      "original": "1090",
      "page_count": 5,
      "order": 860,
      "p1": "4218",
      "pn": "4222",
      "abstract": [
        "Automatic speaker verification (ASV) vendors and corpus providers would\nboth benefit from tools to reliably extrapolate performance metrics\nfor large speaker populations  without collecting new speakers. We\naddress false alarm rate extrapolation under a worst-case model whereby\nan adversary identifies the closest impostor for a given target speaker\nfrom a large population. Our models are generative and allow sampling\nnew speakers. The models are formulated in the ASV detection score\nspace to facilitate analysis of arbitrary ASV systems.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1090",
      "author_area_id": "13",
      "author_area_label": "Special Sessions"
    },
    "jiang20b_interspeech": {
      "authors": [
        [
          "Ziyue",
          "Jiang"
        ],
        [
          "Hongcheng",
          "Zhu"
        ],
        [
          "Li",
          "Peng"
        ],
        [
          "Wenbing",
          "Ding"
        ],
        [
          "Yanzhen",
          "Ren"
        ]
      ],
      "title": "Self-Supervised Spoofing Audio Detection Scheme",
      "original": "1760",
      "page_count": 5,
      "order": 861,
      "p1": "4223",
      "pn": "4227",
      "abstract": [
        "With the development of deep generation technology, spoofing audio\ntechnology based on speech synthesis and speech conversion is closer\nto reality, which challenges the credibility of the media in social\nnetworks. This paper proposes a self-supervised spoofing audio detection\nscheme(SSAD). In SSAD, eight convolutional blocks are used to capture\nthe local feature of the audio signal. The temporal convolutional network\n(TCN) is used to capture the context features and realize the operation\nin parallel. Three regression workers and one binary worker are designed\nto achieve better performance in fake and spoofing audio detection.\nThe experimental results on ASVspoof 2019 dataset show that the detection\naccuracy of SSAD outperforms the state-of-art. It shows that the self-supervised\nmethod is effective for the task of spoofing audio detection.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1760",
      "author_area_id": "13",
      "author_area_label": "Special Sessions"
    },
    "wang20fa_interspeech": {
      "authors": [
        [
          "Qing",
          "Wang"
        ],
        [
          "Pengcheng",
          "Guo"
        ],
        [
          "Lei",
          "Xie"
        ]
      ],
      "title": "Inaudible Adversarial Perturbations for Targeted Attack in Speaker Recognition",
      "original": "1955",
      "page_count": 5,
      "order": 862,
      "p1": "4228",
      "pn": "4232",
      "abstract": [
        "Speaker recognition is a popular topic in biometric authentication\nand many deep learning approaches have achieved extraordinary performances.\nHowever, it has been shown in both image and speech applications that\ndeep neural networks are vulnerable to adversarial examples. In this\nstudy, we aim to exploit this weakness to perform targeted adversarial\nattacks against the x-vector based speaker recognition system. We propose\nto generate inaudible adversarial perturbations based on the psychoacoustic\nprinciple of frequency masking, achieving targeted white-box attacks\nto speaker recognition system. Specifically, we constrict the perturbation\nunder the masking threshold of original audio, instead of using a common\nl<SUB>p</SUB> norm to measure the perturbations. Experiments on Aishell-1\ncorpus show that our approach yields up to 98.5% attack success rate\nto arbitrary gender speaker targets, while retaining indistinguishable\nattribute to listeners. Furthermore, we also achieve an effective speaker\nattack when applying the proposed approach to a completely irrelevant\nwaveform, such as music.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1955",
      "author_area_id": "13",
      "author_area_label": "Special Sessions"
    },
    "villalba20_interspeech": {
      "authors": [
        [
          "Jes\u00fas",
          "Villalba"
        ],
        [
          "Yuekai",
          "Zhang"
        ],
        [
          "Najim",
          "Dehak"
        ]
      ],
      "title": "x-Vectors Meet Adversarial Attacks: Benchmarking Adversarial Robustness in Speaker Verification",
      "original": "2458",
      "page_count": 5,
      "order": 863,
      "p1": "4233",
      "pn": "4237",
      "abstract": [
        "Automatic Speaker Verification (ASV) enables high-security applications\nlike user authentication or criminal investigation. However, ASV can\nbe subjected to malicious attacks, which could compromise that security.\nThe ASV literature mainly studies spoofing (a.k.a impersonation) attacks\nsuch as voice replay, synthesis or conversion. Meanwhile, other kinds\nof attacks, known as adversarial attacks, have become a threat to all\nkind of machine learning systems. Adversarial attacks introduce an\nimperceptible perturbation in the input signal that radically changes\nthe behavior of the system. These attacks have been intensively studied\nin the image domain but less in the speech domain.<br style=\"mso-data-placement:same-cell;\"\n/><br style=\"mso-data-placement:same-cell;\" />\n  In this work, we investigate\nthe vulnerability of state-of-the-art ASV systems to adversarial attacks.\nWe consider a threat model consisting in adding a perturbation noise\nto the test waveform to alter the ASV decision. We also discuss the\nmethodology and metrics to benchmark adversarial attacks and defenses\nin ASV. We evaluated three x-vector architectures, which performed\namong the best in recent ASV evaluations, against fast gradient sign\nand Carlini-Wagner attacks. All networks were highly vulnerable in\nthe white-box attack scenario, even for high SNR (30&#8211;60 dB).\nFurthermore, we successfully transferred attacks generated with smaller\nwhite-box networks to attack a larger black-box network.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2458",
      "author_area_id": "13",
      "author_area_label": "Special Sessions"
    },
    "zhang20ha_interspeech": {
      "authors": [
        [
          "Yuekai",
          "Zhang"
        ],
        [
          "Ziyan",
          "Jiang"
        ],
        [
          "Jes\u00fas",
          "Villalba"
        ],
        [
          "Najim",
          "Dehak"
        ]
      ],
      "title": "Black-Box Attacks on Spoofing Countermeasures Using Transferability of Adversarial Examples",
      "original": "2834",
      "page_count": 5,
      "order": 864,
      "p1": "4238",
      "pn": "4242",
      "abstract": [
        "Spoofing countermeasure systems protect Automatic Speaker Verification\n(ASV) systems from spoofing attacks such as replay, synthesis, and\nconversion. However, research has shown spoofing countermeasures are\nvulnerable to adversarial attacks. Previous literature mainly uses\nadversarial attacks on spoofing countermeasures under a white-box scenario,\nwhere attackers could access all the information of the victim networks.\nBlackbox attacks would be a more serious threat than white-box attacks.<br\nstyle=\"mso-data-placement:same-cell;\" /><br style=\"mso-data-placement:same-cell;\"\n/>\n  In this paper, our objective is to black-box attack spoofing countermeasures\nusing adversarial examples with high transferability. We used MI-FGSM\nto improve the transferability of adversarial examples. We propose\nan iterative ensemble method (IEM) to further improve the transferability.\nComparing with previous ensemble-based attacks, our proposed IEM method,\ncombined with MI-FGSM, could effectively generate adversarial examples\nwith higher transferability. In our experiments, we evaluated the attacks\non four black-box networks. For each black-box model, we used the other\nthree as a white-box ensemble to generate the adversarial examples.\nThe proposed IEM with MI-FGSM improved attack success rate by 4&#8211;30%\nrelative (depending on black-box model) w.r.t. the baseline logit ensemble.\nTherefore, we conclude that spoofing countermeasure models are also\nvulnerable to black-box attacks.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2834",
      "author_area_id": "13",
      "author_area_label": "Special Sessions"
    },
    "n20_interspeech": {
      "authors": [
        [
          "Krishna D.",
          "N."
        ],
        [
          "Ankita",
          "Patil"
        ]
      ],
      "title": "Multimodal Emotion Recognition Using Cross-Modal Attention and 1D Convolutional Neural Networks",
      "original": "1190",
      "page_count": 5,
      "order": 865,
      "p1": "4243",
      "pn": "4247",
      "abstract": [
        "In this work, we propose a new approach for multimodal emotion recognition\nusing cross-modal attention and raw waveform based convolutional neural\nnetworks. Our approach uses audio and text information to predict the\nemotion label. We use an audio encoder to process the raw audio waveform\nto extract high-level features from the audio, and we use text encoder\nto extract high-level semantic information from text. We use cross-modal\nattention where the features from audio encoder attend to the features\nfrom text encoder and vice versa. This helps in developing interaction\nbetween speech and text sequences to extract most relevant features\nfor emotion recognition. Our experiments show that the proposed approach\nobtains the state of the art results on IEMOCAP dataset [1]. We obtain\n1.9% absolute improvement in accuracy compared to the previous state\nof the art method [2]. Our proposed approach uses 1D convolutional\nneural network to process the raw waveform instead of spectrogram features.\nOur experiments also shows that processing raw waveform gives a 0.54%\nimprovement over spectrogram based modal.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1190",
      "author_area_id": "12",
      "author_area_label": "Spoken Language Processing: Translation, Information Retrieval, Summarization, Resources and Evaluation"
    },
    "manakul20_interspeech": {
      "authors": [
        [
          "Potsawee",
          "Manakul"
        ],
        [
          "Mark J.F.",
          "Gales"
        ],
        [
          "Linlin",
          "Wang"
        ]
      ],
      "title": "Abstractive Spoken Document Summarization Using Hierarchical Model with Multi-Stage Attention Diversity Optimization",
      "original": "1683",
      "page_count": 5,
      "order": 866,
      "p1": "4248",
      "pn": "4252",
      "abstract": [
        "Abstractive summarization is a standard task for written documents,\nsuch as news articles. Applying summarization schemes to spoken documents\nis more challenging, especially in situations involving human interactions,\nsuch as meetings. Here, utterances tend not to form complete sentences\nand sometimes contain little information. Moreover, speech disfluencies\nwill be present as well as recognition errors for automated systems.\nFor current attention-based sequence-to-sequence summarization systems,\nthese additional challenges can yield a poor attention distribution\nover the spoken document words and utterances, impacting performance.\nIn this work, we propose a multi-stage method based on a hierarchical\nencoder-decoder model to explicitly model utterance-level attention\ndistribution at training time; and enforce diversity at inference time\nusing a unigram diversity term. Furthermore, multitask learning tasks\nincluding dialogue act classification and extractive summarization\nare incorporated. The performance of the system is evaluated on the\nAMI meeting corpus. The inclusion of both training and inference diversity\nterms improves performance, outperforming current state-of-the-art\nsystems in terms of ROUGE scores. Additionally, the impact of ASR errors,\nas well as performance on the multitask learning tasks, is evaluated.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1683",
      "author_area_id": "12",
      "author_area_label": "Spoken Language Processing: Translation, Information Retrieval, Summarization, Resources and Evaluation"
    },
    "zhang20ia_interspeech": {
      "authors": [
        [
          "Yichi",
          "Zhang"
        ],
        [
          "Yinpei",
          "Dai"
        ],
        [
          "Zhijian",
          "Ou"
        ],
        [
          "Huixin",
          "Wang"
        ],
        [
          "Junlan",
          "Feng"
        ]
      ],
      "title": "Improved Learning of Word Embeddings with Word Definitions and Semantic Injection",
      "original": "1702",
      "page_count": 5,
      "order": 867,
      "p1": "4253",
      "pn": "4257",
      "abstract": [
        "Recently, two categories of linguistic knowledge sources, word definitions\nfrom monolingual dictionaries and linguistic relations (e.g. synonymy\nand antonymy), have been leveraged separately to improve the traditional\nco-occurrence based methods for learning word embeddings. In this paper,\nwe investigate to leverage these two kinds of resources together. Specifically,\nwe propose a new method for word embedding specialization, named Definition\nAutoencoder with Semantic Injection (DASI). In our experiments<SUP>1</SUP>,\nDASI outperforms its single-knowledge-source counterparts on two semantic\nsimilarity benchmarks, and the improvements are further justified on\na downstream task of dialog state tracking. We also show that DASI\nis superior over simple combinations of existing methods in incorporating\nthe two knowledge sources.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1702",
      "author_area_id": "12",
      "author_area_label": "Spoken Language Processing: Translation, Information Retrieval, Summarization, Resources and Evaluation"
    },
    "wang20ga_interspeech": {
      "authors": [
        [
          "Yiming",
          "Wang"
        ],
        [
          "Hang",
          "Lv"
        ],
        [
          "Daniel",
          "Povey"
        ],
        [
          "Lei",
          "Xie"
        ],
        [
          "Sanjeev",
          "Khudanpur"
        ]
      ],
      "title": "Wake Word Detection with Alignment-Free Lattice-Free MMI",
      "original": "1811",
      "page_count": 5,
      "order": 868,
      "p1": "4258",
      "pn": "4262",
      "abstract": [
        "Always-on spoken language interfaces, e.g. personal digital assistants,\nrely on a  wake word to start processing spoken input. We present novel\nmethods to train a hybrid DNN/HMM wake word detection system from partially\nlabeled training data, and to use it in on-line applications: (i) we\nremove the prerequisite of frame-level alignments in the LF-MMI training\nalgorithm, permitting the use of un-transcribed training examples that\nare annotated only for the presence/absence of the wake word; (ii)\nwe show that the classical keyword/filler model must be supplemented\nwith an explicit non-speech (silence) model for good performance; (iii)\nwe present an FST-based decoder to perform online detection. We evaluate\nour methods on two real data sets, showing 50%&#8211;90% reduction\nin false rejection rates at pre-specified false alarm rates over the\nbest previously published figures, and re-validate them on a third\n(large) data set.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1811",
      "author_area_id": "12",
      "author_area_label": "Spoken Language Processing: Translation, Information Retrieval, Summarization, Resources and Evaluation"
    },
    "nguyen20d_interspeech": {
      "authors": [
        [
          "Thai Binh",
          "Nguyen"
        ],
        [
          "Quang Minh",
          "Nguyen"
        ],
        [
          "Thi Thu Hien",
          "Nguyen"
        ],
        [
          "Quoc Truong",
          "Do"
        ],
        [
          "Chi Mai",
          "Luong"
        ]
      ],
      "title": "Improving Vietnamese Named Entity Recognition from Speech Using Word Capitalization and Punctuation Recovery Models",
      "original": "1896",
      "page_count": 5,
      "order": 869,
      "p1": "4263",
      "pn": "4267",
      "abstract": [
        "Studies on the Named Entity Recognition (NER) task have shown outstanding\nresults that reach human parity on input texts with correct text formattings,\nsuch as with proper punctuation and capitalization. However, such conditions\nare not available in applications where the input is speech, because\nthe text is generated from a speech recognition system (ASR), and that\nthe system does not consider the text formatting. In this paper, we\n(1) presented the first Vietnamese speech dataset for NER task, and\n(2) the first pre-trained public large-scale monolingual language model\nfor Vietnamese that achieved the new state-of-the-art for the Vietnamese\nNER task by 1.3% absolute F1 score comparing to the latest study. And\nfinally, (3) we proposed a new pipeline for NER task from speech that\novercomes the text formatting problem by introducing a text capitalization\nand punctuation recovery model (CaPu) into the pipeline. The model\ntakes input text from an ASR system and performs two tasks at the same\ntime, producing proper text formatting that helps to improve NER performance.\nExperimental results indicated that the CaPu model helps to improve\nby nearly 4% of F1-score.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1896",
      "author_area_id": "12",
      "author_area_label": "Spoken Language Processing: Translation, Information Retrieval, Summarization, Resources and Evaluation"
    },
    "yadav20b_interspeech": {
      "authors": [
        [
          "Hemant",
          "Yadav"
        ],
        [
          "Sreyan",
          "Ghosh"
        ],
        [
          "Yi",
          "Yu"
        ],
        [
          "Rajiv Ratn",
          "Shah"
        ]
      ],
      "title": "End-to-End Named Entity Recognition from English Speech",
      "original": "2482",
      "page_count": 5,
      "order": 870,
      "p1": "4268",
      "pn": "4272",
      "abstract": [
        "Named entity recognition (NER) from text has been a widely studied\nproblem and usually extracts semantic information from text. Until\nnow, NER from speech is mostly studied in a two-step pipeline process\nthat includes first applying an automatic speech recognition (ASR)\nsystem on an audio sample and then passing the predicted transcript\nto a NER tagger. In such cases, the error does not propagate from one\nstep to another as both the tasks are not optimized in an end-to-end\n(E2E) fashion. Recent studies confirm that integrated approaches (e.g.,\nE2E ASR) outperform sequential ones (e.g., phoneme based ASR). In this\npaper, we introduce a first publicly available NER annotated dataset\nfor English speech and present an E2E approach, which jointly optimizes\nthe ASR and NER tagger components. Experimental results show that the\nproposed E2E approach outperforms the classical two-step approach.\nWe also discuss how NER from speech can be used to handle out of vocabulary\n(OOV) words in an ASR system.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2482",
      "author_area_id": "12",
      "author_area_label": "Spoken Language Processing: Translation, Information Retrieval, Summarization, Resources and Evaluation"
    },
    "mckenna20_interspeech": {
      "authors": [
        [
          "Joseph P.",
          "McKenna"
        ],
        [
          "Samridhi",
          "Choudhary"
        ],
        [
          "Michael",
          "Saxon"
        ],
        [
          "Grant P.",
          "Strimel"
        ],
        [
          "Athanasios",
          "Mouchtaris"
        ]
      ],
      "title": "Semantic Complexity in End-to-End Spoken Language Understanding",
      "original": "2929",
      "page_count": 5,
      "order": 871,
      "p1": "4273",
      "pn": "4277",
      "abstract": [
        "End-to-end spoken language understanding (SLU) models are a class of\nmodel architectures that predict semantics directly from speech. Because\nof their input and output types, we refer to them as speech-to-interpretation\n(STI) models. Previous works have successfully applied STI models to\ntargeted use cases, such as recognizing home automation commands, however\nno study has yet addressed how these models generalize to broader use\ncases. In this work, we analyze the relationship between the performance\nof STI models and the difficulty of the use case to which they are\napplied. We introduce empirical measures of dataset  semantic complexity\nto quantify the difficulty of the SLU tasks. We show that near-perfect\nperformance metrics for STI models reported in the literature were\nobtained with datasets that have low semantic complexity values. We\nperform experiments where we vary the semantic complexity of a large,\nproprietary dataset and show that STI model performance correlates\nwith our semantic complexity measures, such that performance increases\nas complexity values decrease. Our results show that it is important\nto contextualize an STI model&#8217;s performance with the complexity\nvalues of its training dataset to reveal the scope of its applicability.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2929",
      "author_area_id": "12",
      "author_area_label": "Spoken Language Processing: Translation, Information Retrieval, Summarization, Resources and Evaluation"
    },
    "tran20c_interspeech": {
      "authors": [
        [
          "Trang",
          "Tran"
        ],
        [
          "Morgan",
          "Tinkler"
        ],
        [
          "Gary",
          "Yeung"
        ],
        [
          "Abeer",
          "Alwan"
        ],
        [
          "Mari",
          "Ostendorf"
        ]
      ],
      "title": "Analysis of Disfluency in Children&#8217;s Speech",
      "original": "3037",
      "page_count": 5,
      "order": 872,
      "p1": "4278",
      "pn": "4282",
      "abstract": [
        "Disfluencies are prevalent in spontaneous speech, as shown in many\nstudies of adult speech. Less is understood about children&#8217;s\nspeech, especially in pre-school children who are still developing\ntheir language skills. We present a novel dataset with annotated disfluencies\nof spontaneous explanations from 26 children (ages 5&#8211;8), interviewed\ntwice over a year-long period. Our preliminary analysis reveals significant\ndifferences between children&#8217;s speech in our corpus and adult\nspontaneous speech from two corpora (Switchboard and CallHome). Children\nhave higher disfluency and filler rates, tend to use nasal filled pauses\nmore frequently, and on average exhibit longer reparandums than repairs,\nin contrast to adult speakers. Despite the differences, an automatic\ndisfluency detection system trained on adult (Switchboard) speech transcripts\nperforms reasonably well on children&#8217;s speech, achieving an F1\nscore that is 10% higher than the score on an adult out-of-domain dataset\n(CallHome).\n"
      ],
      "doi": "10.21437/Interspeech.2020-3037",
      "author_area_id": "12",
      "author_area_label": "Spoken Language Processing: Translation, Information Retrieval, Summarization, Resources and Evaluation"
    },
    "mittal20_interspeech": {
      "authors": [
        [
          "Ashish",
          "Mittal"
        ],
        [
          "Samarth",
          "Bharadwaj"
        ],
        [
          "Shreya",
          "Khare"
        ],
        [
          "Saneem",
          "Chemmengath"
        ],
        [
          "Karthik",
          "Sankaranarayanan"
        ],
        [
          "Brian",
          "Kingsbury"
        ]
      ],
      "title": "Representation Based Meta-Learning for Few-Shot Spoken Intent Recognition",
      "original": "3208",
      "page_count": 5,
      "order": 873,
      "p1": "4283",
      "pn": "4287",
      "abstract": [
        "Spoken intent detection has become a popular approach to interface\nwith various smart devices with ease. However, such systems are limited\nto the preset list of  intents-terms or  commands, which restricts\nthe quick customization of personal devices to new intents. This paper\npresents a few-shot spoken intent classification approach with task-agnostic\nrepresentations via meta-learning paradigm. Specifically, we leverage\nthe popular representation based meta-learning learning to build a\ntask-agnostic representation of utterances, that then use a linear\nclassifier for prediction. We evaluate three such approaches on our\nnovel experimental protocol developed on two popular spoken intent\nclassification datasets: Google Commands and the Fluent Speech Commands\ndataset. For a 5-shot (1-shot) classification of novel classes, the\nproposed framework provides an average classification accuracy of 88.6%\n(76.3%) on the Google Commands dataset, and 78.5% (64.2%) on the Fluent\nSpeech Commands dataset. The performance is comparable to traditionally\nsupervised classification models with abundant training samples.\n"
      ],
      "doi": "10.21437/Interspeech.2020-3208",
      "author_area_id": "12",
      "author_area_label": "Spoken Language Processing: Translation, Information Retrieval, Summarization, Resources and Evaluation"
    },
    "agarwal20b_interspeech": {
      "authors": [
        [
          "Rishika",
          "Agarwal"
        ],
        [
          "Xiaochuan",
          "Niu"
        ],
        [
          "Pranay",
          "Dighe"
        ],
        [
          "Srikanth",
          "Vishnubhotla"
        ],
        [
          "Sameer",
          "Badaskar"
        ],
        [
          "Devang",
          "Naik"
        ]
      ],
      "title": "Complementary Language Model and Parallel Bi-LRNN for False Trigger Mitigation",
      "original": "3238",
      "page_count": 5,
      "order": 874,
      "p1": "4288",
      "pn": "4292",
      "abstract": [
        "False triggers in voice assistants are unintended invocations of the\nassistant, which not only degrade the user experience but may also\ncompromise privacy. False trigger mitigation (FTM) is a process to\ndetect the false trigger events and respond appropriately to the user.\nIn this paper, we propose a novel solution to the FTM problem by introducing\na parallel ASR decoding process with a special language model trained\nfrom &#8220;out-of-domain&#8221; data sources. Such language model\nis complementary to the existing language model optimized for the assistant\ntask. A bidirectional lattice RNN (Bi-LRNN) classifier trained from\nthe lattices generated by the complementary language model shows a\n38.34% relative reduction of the false trigger (FT) rate at the fixed\nrate of 0.4% false suppression (FS) of correct invocations, compared\nto the current Bi-LRNN model. In addition, we propose to train a parallel\nBi-LRNN model based on the decoding lattices from both language models,\nand examine various ways of implementation. The resulting model leads\nto further reduction in the false trigger rate by 10.8%.\n"
      ],
      "doi": "10.21437/Interspeech.2020-3238",
      "author_area_id": "12",
      "author_area_label": "Spoken Language Processing: Translation, Information Retrieval, Summarization, Resources and Evaluation"
    },
    "liu20u_interspeech": {
      "authors": [
        [
          "Tianchi",
          "Liu"
        ],
        [
          "Rohan Kumar",
          "Das"
        ],
        [
          "Maulik",
          "Madhavi"
        ],
        [
          "Shengmei",
          "Shen"
        ],
        [
          "Haizhou",
          "Li"
        ]
      ],
      "title": "Speaker-Utterance Dual Attention for Speaker and Utterance Verification",
      "original": "1818",
      "page_count": 5,
      "order": 875,
      "p1": "4293",
      "pn": "4297",
      "abstract": [
        "In this paper, we study a novel technique that exploits the interaction\nbetween speaker traits and linguistic content to improve both speaker\nverification and utterance verification performance. We implement an\nidea of speaker-utterance dual attention (SUDA) in a unified neural\nnetwork. The dual attention refers to an attention mechanism for the\ntwo tasks of speaker and utterance verification. The proposed SUDA\nfeatures an attention mask mechanism to learn the interaction between\nthe speaker and utterance information streams. This helps to focus\nonly on the required information for respective task by masking the\nirrelevant counterparts. The studies conducted on RSR2015 corpus confirm\nthat the proposed SUDA outperforms the framework without attention\nmask as well as several competitive systems for both speaker and utterance\nverification.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1818",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "yi20b_interspeech": {
      "authors": [
        [
          "Lu",
          "Yi"
        ],
        [
          "Man-Wai",
          "Mak"
        ]
      ],
      "title": "Adversarial Separation and Adaptation Network for Far-Field Speaker Verification",
      "original": "2372",
      "page_count": 5,
      "order": 876,
      "p1": "4298",
      "pn": "4302",
      "abstract": [
        "Typically, speaker verification systems are highly optimized on the\nspeech collected by close-talking microphones. However, these systems\nwill perform poorly when the users use far-field microphones during\nverification. In this paper, we propose an adversarial separation and\nadaptation network (ADSAN) to extract speaker discriminative and domain-invariant\nfeatures through adversarial learning. The idea is based on the notion\nthat speaker embedding comprises domain-specific components and domain-shared\ncomponents, and that the two components can be disentangled by the\ninterplay of the separation network and the adaptation network in the\nADSAN. We also propose to incorporate a mutual information neural estimator\ninto the domain adaptation network to retain speaker discriminative\ninformation. Experiments on the VOiCES Challenge 2019 demonstrate that\nthe proposed approaches can produce more domain-invariant and speaker\ndiscriminative representations, which could help to reduce the domain\nshift caused by different types of microphones and reverberant environments.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2372",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "han20b_interspeech": {
      "authors": [
        [
          "Hyewon",
          "Han"
        ],
        [
          "Soo-Whan",
          "Chung"
        ],
        [
          "Hong-Goo",
          "Kang"
        ]
      ],
      "title": "MIRNet: Learning Multiple Identities Representations in Overlapped Speech",
      "original": "2076",
      "page_count": 5,
      "order": 877,
      "p1": "4303",
      "pn": "4307",
      "abstract": [
        "Many approaches can derive information about a single speaker&#8217;s\nidentity from the speech by learning to recognize consistent characteristics\nof acoustic parameters. However, it is challenging to determine identity\ninformation when there are multiple concurrent speakers in a given\nsignal. In this paper, we propose a novel deep speaker representation\nstrategy that can reliably extract multiple speaker identities from\nan overlapped speech. We design a network that can extract a high-level\nembedding that contains information about each speaker&#8217;s identity\nfrom a given mixture. Unlike conventional approaches that need reference\nacoustic features for training, our proposed algorithm only requires\nthe speaker identity labels of the overlapped speech segments. We demonstrate\nthe effectiveness and usefulness of our algorithm in a speaker verification\ntask and a speech separation system conditioned on the target speaker\nembeddings obtained through the proposed method.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2076",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "lin20l_interspeech": {
      "authors": [
        [
          "Weiwei",
          "Lin"
        ],
        [
          "Man-Wai",
          "Mak"
        ],
        [
          "Jen-Tzung",
          "Chien"
        ]
      ],
      "title": "Strategies for End-to-End Text-Independent Speaker Verification",
      "original": "2092",
      "page_count": 5,
      "order": 878,
      "p1": "4308",
      "pn": "4312",
      "abstract": [
        "State-of-the-art speaker verification (SV) systems typically consist\nof two distinct components: a deep neural network (DNN) for creating\nspeaker embeddings and a backend for improving the embeddings&#8217;\ndiscriminative ability. The question which arises is: Can we train\nan SV system without a backend? We believe that the backend is to compensate\nfor the fact that the network is trained entirely on short speech segments.\nThis paper shows that with several modifications to the x-vector system,\nDNN embeddings can be directly used for verification. The proposed\nmodifications include: (1) a mask-pooling layer that augments the training\nsamples by randomly masking the frame-level activations and then computing\ntemporal statistics, (2) a sampling scheme that produces diverse training\nsamples by randomly splicing several speech segments from each utterance,\nand (3) additional convolutional layers designed to reduce the temporal\nresolution to save computational cost. Experiments on NIST SRE 2016\nand 2018 show that our method can achieve state-of-the-art performance\nwith simple cosine similarity and requires only half of the computational\ncost of the x-vector network.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2092",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "hautamaki20_interspeech": {
      "authors": [
        [
          "Rosa Gonz\u00e1lez",
          "Hautam\u00e4ki"
        ],
        [
          "Tomi",
          "Kinnunen"
        ]
      ],
      "title": "Why Did the x-Vector System Miss a Target Speaker? Impact of Acoustic Mismatch Upon Target Score on VoxCeleb Data",
      "original": "2715",
      "page_count": 5,
      "order": 879,
      "p1": "4313",
      "pn": "4317",
      "abstract": [
        "Modern automatic speaker verification (ASV) relies heavily on machine\nlearning implemented through deep neural networks. It can be difficult\nto interpret the output of these black boxes. In line with interpretative\nmachine learning, we model the dependency of ASV detection score upon\nacoustic mismatch of the enrollment and test utterances. We aim to\nidentify mismatch factors that explain target speaker misses (false\nrejections). We use distance in the first- and second-order statistics\nof selected acoustic features as the predictors in a linear mixed effects\nmodel, while a standard Kaldi x-vector system forms our ASV black-box.\nOur results on the VoxCeleb data reveal the most prominent mismatch\nfactor to be in F0 mean, followed by mismatches associated with formant\nfrequencies. Our findings indicate that x-vector systems lack robustness\nto intra-speaker variations.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2715",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "afshan20b_interspeech": {
      "authors": [
        [
          "Amber",
          "Afshan"
        ],
        [
          "Jinxi",
          "Guo"
        ],
        [
          "Soo Jin",
          "Park"
        ],
        [
          "Vijay",
          "Ravi"
        ],
        [
          "Alan",
          "McCree"
        ],
        [
          "Abeer",
          "Alwan"
        ]
      ],
      "title": "Variable Frame Rate-Based Data Augmentation to Handle Speaking-Style Variability for Automatic Speaker Verification",
      "original": "3006",
      "page_count": 5,
      "order": 880,
      "p1": "4318",
      "pn": "4322",
      "abstract": [
        "The effects of speaking-style variability on automatic speaker verification\nwere investigated using the UCLA Speaker Variability database which\ncomprises multiple speaking styles per speaker. An x-vector/PLDA (probabilistic\nlinear discriminant analysis) system was trained with the SRE and Switchboard\ndatabases with standard augmentation techniques and evaluated with\nutterances from the UCLA database. The equal error rate (EER) was low\nwhen enrollment and test utterances were of the same style (e.g., 0.98%\nand 0.57% for read and conversational speech, respectively), but it\nincreased substantially when styles were mismatched between enrollment\nand test utterances. For instance, when enrolled with conversation\nutterances, the EER increased to 3.03%, 2.96% and 22.12% when tested\non read, narrative, and pet-directed speech, respectively. To reduce\nthe effect of style mismatch, we propose an entropy-based variable\nframe rate technique to artificially generate style-normalized representations\nfor PLDA adaptation. The proposed system significantly improved performance.\nIn the aforementioned conditions, the EERs improved to 2.69% (conversation\n&#8211; read), 2.27% (conversation &#8211; narrative), and 18.75% (pet-directed\n&#8211; read). Overall, the proposed technique performed comparably\nto multi-style PLDA adaptation without the need for training data in\ndifferent speaking styles per speaker.\n"
      ],
      "doi": "10.21437/Interspeech.2020-3006",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "seurin20_interspeech": {
      "authors": [
        [
          "Mathieu",
          "Seurin"
        ],
        [
          "Florian",
          "Strub"
        ],
        [
          "Philippe",
          "Preux"
        ],
        [
          "Olivier",
          "Pietquin"
        ]
      ],
      "title": "A Machine of Few Words: Interactive Speaker Recognition with Reinforcement Learning",
      "original": "2892",
      "page_count": 5,
      "order": 881,
      "p1": "4323",
      "pn": "4327",
      "abstract": [
        "Speaker recognition is a well known and studied task in the speech\nprocessing domain. It has many applications, either for security or\nspeaker adaptation of personal devices. In this paper, we present a\nnew paradigm for automatic speaker recognition that we call Interactive\nSpeaker Recognition (ISR). In this paradigm, the recognition system\naims to incrementally build a representation of the speakers by requesting\npersonalized utterances to be spoken in contrast to the standard text-dependent\nor text-independent schemes. To do so, we cast the speaker recognition\ntask into a sequential decision-making problem that we solve with Reinforcement\nLearning. Using a standard dataset, we show that our method achieves\nexcellent performance while using little speech signal amounts. This\nmethod could also be applied as an utterance selection mechanism for\nbuilding speech synthesis systems.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2892",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "granqvist20_interspeech": {
      "authors": [
        [
          "Filip",
          "Granqvist"
        ],
        [
          "Matt",
          "Seigel"
        ],
        [
          "Rogier van",
          "Dalen"
        ],
        [
          "\u00c1ine",
          "Cahill"
        ],
        [
          "Stephen",
          "Shum"
        ],
        [
          "Matthias",
          "Paulik"
        ]
      ],
      "title": "Improving On-Device Speaker Verification Using Federated Learning with Privacy",
      "original": "2944",
      "page_count": 5,
      "order": 882,
      "p1": "4328",
      "pn": "4332",
      "abstract": [
        "Information on speaker characteristics can be useful as side information\nin improving speaker recognition accuracy. However, such information\nis often private. This paper investigates how privacy-preserving learning\ncan improve a speaker verification system, by enabling the use of privacy-sensitive\nspeaker data to train an auxiliary classification model that predicts\nvocal characteristics of speakers. In particular, this paper explores\nthe utility achieved by approaches which combine different federated\nlearning and differential privacy mechanisms. These approaches make\nit possible to train a central model while protecting user privacy,\nwith users&#8217; data remaining on their devices. Furthermore, they\nmake learning on a large population of speakers possible, ensuring\ngood coverage of speaker characteristics when training a model. The\nauxiliary model described here uses features extracted from phrases\nwhich trigger a speaker verification system. From these features, the\nmodel predicts speaker characteristic labels considered useful as side\ninformation. The knowledge of the auxiliary model is distilled into\na speaker verification system using multi-task learning, with the side\ninformation labels predicted by this auxiliary model being the additional\ntask. This approach results in a 6% relative improvement in equal error\nrate over a baseline system.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2944",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "ramoji20_interspeech": {
      "authors": [
        [
          "Shreyas",
          "Ramoji"
        ],
        [
          "Prashant",
          "Krishnan"
        ],
        [
          "Sriram",
          "Ganapathy"
        ]
      ],
      "title": "Neural PLDA Modeling for End-to-End Speaker Verification",
      "original": "2699",
      "page_count": 5,
      "order": 883,
      "p1": "4333",
      "pn": "4337",
      "abstract": [
        "While deep learning models have made significant advances in supervised\nclassification problems, the application of these models for out-of-set\nverification tasks like speaker recognition has been limited to deriving\nfeature embeddings. The state-of-the-art x-vector PLDA based speaker\nverification systems use a generative model based on probabilistic\nlinear discriminant analysis (PLDA) for computing the verification\nscore. Recently, we had proposed a neural network approach for backend\nmodeling in speaker verification called the neural PLDA (NPLDA) where\nthe likelihood ratio score of the generative PLDA model is posed as\na discriminative similarity function and the learnable parameters of\nthe score function are optimized using a verification cost. In this\npaper, we extend this work to achieve joint optimization of the embedding\nneural network (x-vector network) with the NPLDA network in an end-to-end\n(E2E) fashion. This proposed end-to-end model is optimized directly\nfrom the acoustic features with a verification cost function and during\ntesting, the model directly outputs the likelihood ratio score. With\nvarious experiments using the NIST speaker recognition evaluation (SRE)\n2018 and 2019 datasets, we show that the proposed E2E model improves\nsignificantly over the x-vector PLDA baseline speaker verification\nsystem.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2699",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "opatka20_interspeech": {
      "authors": [
        [
          "Kuba",
          "\u0141opatka"
        ],
        [
          "Tobias",
          "Bocklet"
        ]
      ],
      "title": "State Sequence Pooling Training of Acoustic Models for Keyword Spotting",
      "original": "2722",
      "page_count": 5,
      "order": 884,
      "p1": "4338",
      "pn": "4342",
      "abstract": [
        "We propose a new training method to improve HMM-based keyword spotting.\nThe loss function is based on a score computed with the keyword/filler\nmodel from the entire input sequence. It is equivalent to max/attention\npooling but is based on prior acoustic knowledge. We also employ a\nmulti-task learning setup by predicting both LVCSR and keyword posteriors.\nWe compare our model to a baseline trained on frame-wise cross entropy,\nwith and without per-class weighting. We employ a low-footprint TDNN\nfor acoustic modeling. The proposed training yields significant and\nconsistent improvement over the baseline in adverse noise conditions.\nThe FRR on cafeteria noise is reduced from 13.07% to 5.28% at 9 dB\nSNR and from 37.44% to 6.78% at 5 dB SNR. We obtain these results with\nonly 600 unique training keyword samples. The training method is independent\nof the frontend and acoustic model topology.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2722",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "hard20_interspeech": {
      "authors": [
        [
          "Andrew",
          "Hard"
        ],
        [
          "Kurt",
          "Partridge"
        ],
        [
          "Cameron",
          "Nguyen"
        ],
        [
          "Niranjan",
          "Subrahmanya"
        ],
        [
          "Aishanee",
          "Shah"
        ],
        [
          "Pai",
          "Zhu"
        ],
        [
          "Ignacio Lopez",
          "Moreno"
        ],
        [
          "Rajiv",
          "Mathews"
        ]
      ],
      "title": "Training Keyword Spotting Models on Non-IID Data with Federated Learning",
      "original": "3023",
      "page_count": 5,
      "order": 885,
      "p1": "4343",
      "pn": "4347",
      "abstract": [
        "We demonstrate that a production-quality keyword-spotting model can\nbe trained on-device using federated learning and achieve comparable\nfalse accept and false reject rates to a centrally-trained model. To\novercome the algorithmic constraints associated with fitting on-device\ndata (which are inherently non-independent and identically distributed),\nwe conduct thorough empirical studies of optimization algorithms and\nhyperparameter configurations using large-scale federated simulations.\nTo overcome resource constraints, we replace memory-intensive MTR data\naugmentation with SpecAugment, which reduces the false reject rate\nby 56%. Finally, to label examples (given the zero visibility into\non-device data), we explore teacher-student training.\n"
      ],
      "doi": "10.21437/Interspeech.2020-3023",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "huang20f_interspeech": {
      "authors": [
        [
          "Rongqing",
          "Huang"
        ],
        [
          "Ossama",
          "Abdel-hamid"
        ],
        [
          "Xinwei",
          "Li"
        ],
        [
          "Gunnar",
          "Evermann"
        ]
      ],
      "title": "Class LM and Word Mapping for Contextual Biasing in End-to-End ASR",
      "original": "1787",
      "page_count": 4,
      "order": 886,
      "p1": "4348",
      "pn": "4351",
      "abstract": [
        "In recent years, all-neural, end-to-end (E2E) ASR systems gained rapid\ninterest in the speech recognition community. They convert speech input\nto text units in a single trainable Neural Network model. In ASR, many\nutterances contain rich named entities. Such named entities may be\nuser or location specific and they are not seen during training. A\nsingle model makes it inflexible to utilize dynamic contextual information\nduring inference. In this paper, we propose to train a context aware\nE2E model and allow the beam search to traverse into the context FST\nduring inference. We also propose a simple method to adjust the cost\ndiscrepancy between the context FST and the base model. This algorithm\nis able to reduce the named entity utterance WER by 57% with little\naccuracy degradation on regular utterances. Although an E2E model does\nnot need a pronunciation dictionary, it&#8217;s interesting to make\nuse of existing pronunciation knowledge to improve accuracy. In this\npaper, we propose an algorithm to map the rare entity words to common\nwords via pronunciation and treat the mapped words as an alternative\nform to the original word during recognition. This algorithm further\nreduces the WER on the named entity utterances by another 31%.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1787",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "borgholt20_interspeech": {
      "authors": [
        [
          "Lasse",
          "Borgholt"
        ],
        [
          "Jakob D.",
          "Havtorn"
        ],
        [
          "\u017deljko",
          "Agi\u0107"
        ],
        [
          "Anders",
          "S\u00f8gaard"
        ],
        [
          "Lars",
          "Maal\u00f8e"
        ],
        [
          "Christian",
          "Igel"
        ]
      ],
      "title": "Do End-to-End Speech Recognition Models Care About Context?",
      "original": "1750",
      "page_count": 5,
      "order": 887,
      "p1": "4352",
      "pn": "4356",
      "abstract": [
        "The two most common paradigms for end-to-end speech recognition are\nconnectionist temporal classification (CTC) and attention-based encoder-decoder\n(AED) models. It has been argued that the latter is better suited for\nlearning an implicit language model. We test this hypothesis by measuring\ntemporal context sensitivity and evaluate how the models perform when\nwe constrain the amount of contextual information in the audio input.\nWe find that the AED model is indeed more context sensitive, but that\nthe gap can be closed by adding self-attention to the CTC model. Furthermore,\nthe two models perform similarly when contextual information is constrained.\nFinally, in contrast to previous research, our results show that the\nCTC model is highly competitive on WSJ and LibriSpeech without the\nhelp of an external language model.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1750",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "kumar20f_interspeech": {
      "authors": [
        [
          "Ankur",
          "Kumar"
        ],
        [
          "Sachin",
          "Singh"
        ],
        [
          "Dhananjaya",
          "Gowda"
        ],
        [
          "Abhinav",
          "Garg"
        ],
        [
          "Shatrughan",
          "Singh"
        ],
        [
          "Chanwoo",
          "Kim"
        ]
      ],
      "title": "Utterance Confidence Measure for End-to-End Speech Recognition with Applications to Distributed Speech Recognition Scenarios",
      "original": "3216",
      "page_count": 5,
      "order": 888,
      "p1": "4357",
      "pn": "4361",
      "abstract": [
        "In this paper, we present techniques to compute confidence score on\nthe predictions made by an end-to-end speech recognition model. Our\nproposed neural confidence measure (NCM) is trained as a binary classification\ntask to accept or reject an end-to-end speech recognition result. We\nincorporate features from an encoder, a decoder, and an attention block\nof the attention-based end-to-end speech recognition model to improve\nNCM significantly. We observe that using information from multiple\nbeams further improves the performance. As a case study of this NCM,\nwe consider an application of the utterance-level confidence score\nin a distributed speech recognition environment with two or more speech\nrecognition systems running on different platforms with varying resource\ncapabilities. We show that around 57% computation on a resource-rich\nhigh-end platform (e.g. a cloud platform) can be saved without sacrificing\naccuracy compared to the high-end only solution. Around 70&#8211;80%\nof computations can be saved if we allow a degradation of word error\nrates to within 5&#8211;10% relative to the high-end solution.\n"
      ],
      "doi": "10.21437/Interspeech.2020-3216"
    },
    "wu20o_interspeech": {
      "authors": [
        [
          "Huaxin",
          "Wu"
        ],
        [
          "Genshun",
          "Wan"
        ],
        [
          "Jia",
          "Pan"
        ]
      ],
      "title": "Speaker Code Based Speaker Adaptive Training Using Model Agnostic Meta-Learning",
      "original": "2296",
      "page_count": 5,
      "order": 889,
      "p1": "4362",
      "pn": "4366",
      "abstract": [
        "The performance of automatic speech recognition systems can be improved\nby speaker adaptive training (SAT), which adapts an acoustic model\nto compensate for the mismatch between training and testing conditions.\nSpeaker code learning is one of the useful ways for speaker adaptive\ntraining. It learns a set of speaker dependent codes together with\nspeaker independent acoustic model in order to remove speaker variation.\nConventionally, speaker dependent codes and speaker independent acoustic\nmodel are jointly optimized. However, this could make it difficult\nto decouple the speaker code from the acoustic model. In this paper,\nwe take the speaker code based SAT as a meta-learning task. The acoustic\nmodel is considered as meta-knowledge, while speaker code is considered\nas task specific knowledge. Experiments on the Switchboard task show\nthat our method can not only learn a good speaker code, but also improve\nthe performance of the acoustic model even without speaker code.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2296",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "zhu20c_interspeech": {
      "authors": [
        [
          "Han",
          "Zhu"
        ],
        [
          "Jiangjiang",
          "Zhao"
        ],
        [
          "Yuling",
          "Ren"
        ],
        [
          "Li",
          "Wang"
        ],
        [
          "Pengyuan",
          "Zhang"
        ]
      ],
      "title": "Domain Adaptation Using Class Similarity for Robust Speech Recognition",
      "original": "3087",
      "page_count": 5,
      "order": 890,
      "p1": "4367",
      "pn": "4371",
      "abstract": [
        "When only limited target domain data is available, domain adaptation\ncould be used to promote performance of deep neural network (DNN) acoustic\nmodel by leveraging well-trained source model and target domain data.\nHowever, suffering from domain mismatch and data sparsity, domain adaptation\nis very challenging. This paper proposes a novel adaptation method\nfor DNN acoustic model using class similarity. Since the output distribution\nof DNN model contains the knowledge of similarity among classes, which\nis applicable to both source and target domain, it could be transferred\nfrom source to target model for the performance improvement. In our\napproach, we first compute the frame level posterior probabilities\nof source samples using source model. Then, for each class, probabilities\nof this class are used to compute a mean vector, which we refer to\nas mean soft labels. During adaptation, these mean soft labels are\nused in a regularization term to train the target model. Experiments\nshowed that our approach outperforms fine-tuning using one-hot labels\non both accent and noise adaptation task, especially when source and\ntarget domain are highly mismatched.\n"
      ],
      "doi": "10.21437/Interspeech.2020-3087",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "novitasari20_interspeech": {
      "authors": [
        [
          "Sashi",
          "Novitasari"
        ],
        [
          "Andros",
          "Tjandra"
        ],
        [
          "Tomoya",
          "Yanagita"
        ],
        [
          "Sakriani",
          "Sakti"
        ],
        [
          "Satoshi",
          "Nakamura"
        ]
      ],
      "title": "Incremental Machine Speech Chain Towards Enabling Listening While Speaking in Real-Time",
      "original": "2034",
      "page_count": 5,
      "order": 891,
      "p1": "4372",
      "pn": "4376",
      "abstract": [
        "Inspired by a human speech chain mechanism, a machine speech chain\nframework based on deep learning was recently proposed for the semi-supervised\ndevelopment of automatic speech recognition (ASR) and text-to-speech\nsynthesis (TTS) systems. However, the mechanism to listen while speaking\ncan be done only after receiving entire input sequences. Thus, there\nis a significant delay when encountering long utterances. By contrast,\nhumans can listen to what they speak in real-time, and if there is\na delay in hearing, they won&#8217;t be able to continue speaking.\nIn this work, we propose an incremental machine speech chain towards\nenabling machine to listen while speaking in real-time. Specifically,\nwe construct incremental ASR (ISR) and incremental TTS (ITTS) by letting\nboth systems improve together through a short-term loop. Our experimental\nresults reveal that our proposed framework is able to reduce delays\ndue to long utterances while keeping a comparable performance to the\nnon-incremental basic machine speech chain.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2034",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "raissi20_interspeech": {
      "authors": [
        [
          "Tina",
          "Raissi"
        ],
        [
          "Eugen",
          "Beck"
        ],
        [
          "Ralf",
          "Schl\u00fcter"
        ],
        [
          "Hermann",
          "Ney"
        ]
      ],
      "title": "Context-Dependent Acoustic Modeling Without Explicit Phone Clustering",
      "original": "1244",
      "page_count": 5,
      "order": 892,
      "p1": "4377",
      "pn": "4381",
      "abstract": [
        "Phoneme-based acoustic modeling of large vocabulary automatic speech\nrecognition takes advantage of phoneme context. The large number of\ncontext-dependent (CD) phonemes and their highly varying statistics\nrequire tying or smoothing to enable robust training. Usually, Classification\nand Regression Trees are used for phonetic clustering, which is standard\nin Hidden Markov Model (HMM)-based systems. However, this solution\nintroduces a secondary training objective and does not allow for end-to-end\ntraining. In this work, we address a direct phonetic context modeling\nfor the hybrid Deep Neural Network (DNN)/HMM, that does not build on\nany phone clustering algorithm for the determination of the HMM state\ninventory. By performing different decompositions of the joint probability\nof the center phoneme state and its left and right contexts, we obtain\na factorized network consisting of different components, trained jointly.\nMoreover, the representation of the phonetic context for the network\nrelies on phoneme embeddings. The recognition accuracy of our proposed\nmodels on the Switchboard task is comparable and outperforms slightly\nthe hybrid model using the standard state-tying decision trees.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1244",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "shahnawazuddin20_interspeech": {
      "authors": [
        [
          "S.",
          "Shahnawazuddin"
        ],
        [
          "Nagaraj",
          "Adiga"
        ],
        [
          "Kunal",
          "Kumar"
        ],
        [
          "Aayushi",
          "Poddar"
        ],
        [
          "Waquar",
          "Ahmad"
        ]
      ],
      "title": "Voice Conversion Based Data Augmentation to Improve Children&#8217;s Speech Recognition in Limited Data Scenario",
      "original": "1112",
      "page_count": 5,
      "order": 893,
      "p1": "4382",
      "pn": "4386",
      "abstract": [
        "Automatic recognition of children&#8217;s speech is a challenging research\nproblem due to several reasons. One among those is unavailability of\nlarge amounts of speech data from child speakers to develop automatic\nspeech recognition (ASR) systems employing deep learning architectures.\nUsing a limited amount of training data limits the power of the learned\nsystem. To overcome this issue, we have explored means to effectively\nmake use of adults&#8217; speech data for training an ASR system. For\nthat purpose, generative adversarial network (GAN) based voice conversion\n(VC) is exploited to modify the acoustic attributes of adults&#8217;\nspeech making it perceptually similar to that of children&#8217;s speech.\nThe original and converted speech samples from adult speakers are then\npooled together to learn the statistical model parameters. Significantly\nimproved recognition rate for children&#8217;s speech is noted due\nto VC-based data augmentation. To further enhance the recognition rate,\na limited amount of children&#8217;s speech data is also pooled into\ntraining. Large reduction in error rate is observed in this case as\nwell. It is worth mentioning that GAN-based VC does not change the\nspeaking-rate. To demonstrate the need to deal with speaking-rate differences\nwe report the results of time-scale modification of children&#8217;s\nspeech test data.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1112",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "karlapati20_interspeech": {
      "authors": [
        [
          "Sri",
          "Karlapati"
        ],
        [
          "Alexis",
          "Moinet"
        ],
        [
          "Arnaud",
          "Joly"
        ],
        [
          "Viacheslav",
          "Klimkov"
        ],
        [
          "Daniel",
          "S\u00e1ez-Trigueros"
        ],
        [
          "Thomas",
          "Drugman"
        ]
      ],
      "title": "CopyCat: Many-to-Many Fine-Grained Prosody Transfer for Neural Text-to-Speech",
      "original": "1251",
      "page_count": 5,
      "order": 894,
      "p1": "4387",
      "pn": "4391",
      "abstract": [
        "Prosody Transfer (PT) is a technique that aims to use the prosody from\na source audio as a reference while synthesising speech. Fine-grained\nPT aims at capturing prosodic aspects like rhythm, emphasis, melody,\nduration, and loudness, from a source audio at a very granular level\nand transferring them when synthesising speech in a different target\nspeaker&#8217;s voice. Current approaches for fine-grained PT suffer\nfrom source speaker leakage, where the synthesised speech has the voice\nidentity of the source speaker as opposed to the target speaker. In\norder to mitigate this issue, they compromise on the quality of PT.\nIn this paper, we propose CopyCat, a novel, many-to-many PT system\nthat is robust to source speaker leakage, without using parallel data.\nWe achieve this through a novel reference encoder architecture capable\nof capturing temporal prosodic representations which are robust to\nsource speaker leakage. We compare CopyCat against a state-of-the-art\nfine-grained PT model through various subjective evaluations, where\nwe show a relative improvement of 47% in the quality of prosody transfer\nand 14% in preserving the target speaker identity, while still maintaining\nthe same naturalness.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1251",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "lin20m_interspeech": {
      "authors": [
        [
          "Binghuai",
          "Lin"
        ],
        [
          "Liyuan",
          "Wang"
        ],
        [
          "Xiaoli",
          "Feng"
        ],
        [
          "Jinsong",
          "Zhang"
        ]
      ],
      "title": "Joint Detection of Sentence Stress and Phrase Boundary for Prosody",
      "original": "1284",
      "page_count": 5,
      "order": 895,
      "p1": "4392",
      "pn": "4396",
      "abstract": [
        "Prosodic event detection plays an important role in spoken language\nprocessing tasks and Computer-Assisted Pronunciation Training (CAPT)\nsystems [1]. Traditional methods for the detection of sentence stress\nand phrase boundaries rely on machine learning methods that model limited\ncontextual information and account little for interaction between these\ntwo prosodic events. In this paper, we propose a hierarchical network\nmodeling the contextual factors at the granularity of phoneme, syllable\nand word based on bidirectional Long Short-Term Memory (BLSTM). Moreover,\nto account for the inherent connection between sentence stress and\nphrase boundaries, we perform a joint modeling of these two important\nprosodic events with a multitask learning framework (MTL) which shares\ncommon prosodic features. We evaluate the network performance based\non Aix-Machine Readable Spoken English Corpus (Aix-MARSEC). Experimental\nresults show our proposed method obtains the F1-measure of 90% for\nsentence stress detection and 91% for phrase boundary detection, which\noutperforms the baseline utilizing conditional random field (CRF) by\nabout 4% and 9% respectively.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1284",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "kulkarni20_interspeech": {
      "authors": [
        [
          "Ajinkya",
          "Kulkarni"
        ],
        [
          "Vincent",
          "Colotte"
        ],
        [
          "Denis",
          "Jouvet"
        ]
      ],
      "title": "Transfer Learning of the Expressivity Using FLOW Metric Learning in Multispeaker Text-to-Speech Synthesis",
      "original": "1297",
      "page_count": 5,
      "order": 896,
      "p1": "4397",
      "pn": "4401",
      "abstract": [
        "In this paper, we present a novel flow metric learning architecture\nin a parametric multispeaker expressive text-to-speech (TTS) system.\nWe proposed inverse autoregressive flow (IAF) as a way to perform the\nvariational inference, thus providing flexible approximate posterior\ndistribution. The proposed approach condition the text-to-speech system\non speaker embeddings so that latent space represents the emotion as\nsemantic characteristics. For representing the speaker, we extracted\nspeaker embeddings from the x-vector based speaker recognition model\ntrained on speech data from many speakers. To predict the vocoder features,\nwe used the acoustic model conditioned on the textual features as well\nas on the speaker embedding. We transferred the expressivity by using\nthe mean of the latent variables for each emotion to generate expressive\nspeech in different speaker&#8217;s voices for which no expressive\nspeech data is available.<br style=\"mso-data-placement:same-cell;\"\n/><br style=\"mso-data-placement:same-cell;\" />\n  We compared the results\nobtained using flow-based variational inference with variational autoencoder\nas a baseline model. The performance measured by mean opinion score\n(MOS), speaker MOS, and expressive MOS shows that N-pair loss based\ndeep metric learning along with IAF model improves the transfer of\nexpressivity in the desired speaker&#8217;s voice in synthesized speech.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1297",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "bae20_interspeech": {
      "authors": [
        [
          "Jae-Sung",
          "Bae"
        ],
        [
          "Hanbin",
          "Bae"
        ],
        [
          "Young-Sun",
          "Joo"
        ],
        [
          "Junmo",
          "Lee"
        ],
        [
          "Gyeong-Hoon",
          "Lee"
        ],
        [
          "Hoon-Young",
          "Cho"
        ]
      ],
      "title": "Speaking Speed Control of End-to-End Speech Synthesis Using Sentence-Level Conditioning",
      "original": "1361",
      "page_count": 5,
      "order": 897,
      "p1": "4402",
      "pn": "4406",
      "abstract": [
        "This paper proposes a controllable end-to-end text-to-speech (TTS)\nsystem to control the speaking speed (speed-controllable TTS; SCTTS)\nof synthesized speech with sentence-level speaking-rate value as an\nadditional input. The speaking-rate value, the ratio of the number\nof input phonemes to the length of input speech, is adopted in the\nproposed system to control the speaking speed. Furthermore, the proposed\nSCTTS system can control the speaking speed while retaining other speech\nattributes, such as the pitch, by adopting the global style token-based\nstyle encoder. The proposed SCTTS does not require any additional well-trained\nmodel or an external speech database to extract phoneme-level duration\ninformation and can be trained in an end-to-end manner. In addition,\nour listening tests on fast-, normal-, and slow-speed speech showed\nthat the SCTTS can generate more natural speech than other phoneme\nduration control approaches which increase or decrease duration at\nthe same rate for the entire sentence, especially in the case of slow-speed\nspeech.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1361",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "tyagi20_interspeech": {
      "authors": [
        [
          "Shubhi",
          "Tyagi"
        ],
        [
          "Marco",
          "Nicolis"
        ],
        [
          "Jonas",
          "Rohnke"
        ],
        [
          "Thomas",
          "Drugman"
        ],
        [
          "Jaime",
          "Lorenzo-Trueba"
        ]
      ],
      "title": "Dynamic Prosody Generation for Speech Synthesis Using Linguistics-Driven Acoustic Embedding Selection",
      "original": "1411",
      "page_count": 5,
      "order": 898,
      "p1": "4407",
      "pn": "4411",
      "abstract": [
        "Recent advances in Text-to-Speech (TTS) have improved quality and naturalness\nto near-human capabilities. But something which is still lacking in\norder to achieve human-like communication is the dynamic variations\nand adaptability of human speech in more complex scenarios. This work\nattempts to solve the problem of achieving a more dynamic and natural\nintonation in TTS systems, particularly for stylistic speech such as\nthe newscaster speaking style. We propose a novel way of exploiting\nlinguistic information in VAE systems to drive dynamic prosody generation.\nWe analyze the contribution of both semantic and syntactic features.\nOur results show that the approach improves the prosody and naturalness\nfor complex utterances as well as in Long Form Reading (LFR).\n"
      ],
      "doi": "10.21437/Interspeech.2020-1411",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "kenter20_interspeech": {
      "authors": [
        [
          "Tom",
          "Kenter"
        ],
        [
          "Manish",
          "Sharma"
        ],
        [
          "Rob",
          "Clark"
        ]
      ],
      "title": "Improving the Prosody of RNN-Based English Text-To-Speech Synthesis by Incorporating a BERT Model",
      "original": "1430",
      "page_count": 5,
      "order": 899,
      "p1": "4412",
      "pn": "4416",
      "abstract": [
        "The prosody of currently available speech synthesis systems can be\nunnatural due to the systems only having access to the text, possibly\nenriched by linguistic information such as part-of-speech tags and\nparse trees. We show that incorporating a BERT model in an RNN-based\nspeech synthesis model &#8212; where the BERT model is pretrained on\nlarge amounts of unlabeled data, and fine-tuned to the speech domain\n&#8212; improves prosody. Additionally, we propose a way of handling\narbitrarily long sequences with BERT. Our findings indicate that small\nBERT models work better than big ones, and that fine-tuning the BERT\npart of the model is pivotal for getting good results.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1430",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "zhao20g_interspeech": {
      "authors": [
        [
          "Yi",
          "Zhao"
        ],
        [
          "Haoyu",
          "Li"
        ],
        [
          "Cheng-I",
          "Lai"
        ],
        [
          "Jennifer",
          "Williams"
        ],
        [
          "Erica",
          "Cooper"
        ],
        [
          "Junichi",
          "Yamagishi"
        ]
      ],
      "title": "Improved Prosody from Learned F0 Codebook Representations for VQ-VAE Speech Waveform Reconstruction",
      "original": "1615",
      "page_count": 5,
      "order": 900,
      "p1": "4417",
      "pn": "4421",
      "abstract": [
        "Vector Quantized Variational AutoEncoders (VQ-VAE) are a powerful representation\nlearning framework that can discover discrete groups of features from\na speech signal without supervision. Until now, the VQ-VAE architecture\nhas previously modeled individual types of speech features, such as\nonly phones or only F0. This paper introduces an important extension\nto VQ-VAE for learning F0-related suprasegmental information simultaneously\nalong with traditional phone features. The proposed framework uses\ntwo encoders such that the F0 trajectory and speech waveform are both\ninput to the system, therefore two separate codebooks are learned.\nWe used a WaveRNN vocoder as the decoder component of VQ-VAE. Our speaker-independent\nVQ-VAE was trained with raw speech waveforms from multi-speaker Japanese\nspeech databases. Experimental results show that the proposed extension\nreduces F0 distortion of reconstructed speech for all unseen test speakers,\nand results in significantly higher preference scores from a listening\ntest. We additionally conducted experiments using single-speaker Mandarin\nspeech to demonstrate advantages of our architecture in another language\nwhich relies heavily on F0.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1615",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "zeng20b_interspeech": {
      "authors": [
        [
          "Zhen",
          "Zeng"
        ],
        [
          "Jianzong",
          "Wang"
        ],
        [
          "Ning",
          "Cheng"
        ],
        [
          "Jing",
          "Xiao"
        ]
      ],
      "title": "Prosody Learning Mechanism for Speech Synthesis System Without Text Length Limit",
      "original": "2053",
      "page_count": 5,
      "order": 901,
      "p1": "4422",
      "pn": "4426",
      "abstract": [
        "Recent neural speech synthesis systems have gradually focused on the\ncontrol of prosody to improve the quality of synthesized speech, but\nthey rarely consider the variability of prosody and the correlation\nbetween prosody and semantics together. In this paper, a prosody learning\nmechanism is proposed to model the prosody of speech based on TTS system,\nwhere the prosody information of speech is extracted from the mel-spectrum\nby a prosody learner and combined with the phoneme sequence to reconstruct\nthe mel-spectrum. Meanwhile, the semantic features of text from the\npre-trained language model is introduced to improve the prosody prediction\nresults. In addition, a novel self-attention structure, named as local\nattention, is proposed to lift this restriction of input text length,\nwhere the relative position information of the sequence is modeled\nby the relative position matrices so that the position encodings is\nno longer needed. Experiments on English and Mandarin show that speech\nwith more satisfactory prosody has obtained in our model. Especially\nin Mandarin synthesis, our proposed model outperforms baseline model\nwith a MOS gap of 0.08, and the overall naturalness of the synthesized\nspeech has been significantly improved.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2053",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "shirahata20_interspeech": {
      "authors": [
        [
          "Yuma",
          "Shirahata"
        ],
        [
          "Daisuke",
          "Saito"
        ],
        [
          "Nobuaki",
          "Minematsu"
        ]
      ],
      "title": "Discriminative Method to Extract Coarse Prosodic Structure and its Application for Statistical Phrase/Accent Command Estimation",
      "original": "2566",
      "page_count": 5,
      "order": 902,
      "p1": "4427",
      "pn": "4431",
      "abstract": [
        "This paper introduces a method of extracting coarse prosodic structure\nfrom fundamental frequency (F<SUB>0</SUB>) contours by using a discriminative\napproach such as deep neural networks (DNN), and applies the method\nfor the parameter estimation of the Fujisaki model. In the conventional\nmethods for the parameter estimation of the Fujisaki model, generative\napproaches, in which the estimation is treated as an inverse problem\nof the generation process, have been adopted. On the other hand, recent\ndevelopment of the discriminative approaches would enable us to treat\nthe problem in a direct manner. To introduce a discriminative approach\nto the parameter estimation of the Fujisaki model in which the precise\nlabels for the parameter are expensive, this study focuses on the similarities\nbetween the acoustic realization of the prosodic structure in F<SUB>0</SUB>\ncontours and the sentence structure of the read text. In the proposed\nmethod, the sentence structure obtained from the text is utilized as\nthe labels for the discriminative model, and the model estimates the\n coarse prosodic structure. Finally this structure is refined by using\na conventional method for the parameter estimation. Experimental results\ndemonstrate that the proposed method improves the estimation accuracy\nby 18% in terms of detection rate without using any auxiliary features\nat inference.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2566",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "raitio20_interspeech": {
      "authors": [
        [
          "Tuomo",
          "Raitio"
        ],
        [
          "Ramya",
          "Rasipuram"
        ],
        [
          "Dan",
          "Castellani"
        ]
      ],
      "title": "Controllable Neural Text-to-Speech Synthesis Using Intuitive Prosodic Features",
      "original": "2861",
      "page_count": 5,
      "order": 903,
      "p1": "4432",
      "pn": "4436",
      "abstract": [
        "Modern neural text-to-speech (TTS) synthesis can generate speech that\nis indistinguishable from natural speech. However, the prosody of generated\nutterances often represents the average prosodic style of the database\ninstead of having wide prosodic variation. Moreover, the generated\nprosody is solely defined by the input text, which does not allow for\ndifferent styles for the same sentence. In this work, we train a sequence-to-sequence\nneural network conditioned on acoustic speech features to learn a latent\nprosody space with intuitive and meaningful dimensions. Experiments\nshow that a model conditioned on sentence-wise pitch, pitch range,\nphone duration, energy, and spectral tilt can effectively control each\nprosodic dimension and generate a wide variety of speaking styles,\nwhile maintaining similar mean opinion score (4.23) to our Tacotron\nbaseline (4.26).\n"
      ],
      "doi": "10.21437/Interspeech.2020-2861",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "morrison20_interspeech": {
      "authors": [
        [
          "Max",
          "Morrison"
        ],
        [
          "Zeyu",
          "Jin"
        ],
        [
          "Justin",
          "Salamon"
        ],
        [
          "Nicholas J.",
          "Bryan"
        ],
        [
          "Gautham J.",
          "Mysore"
        ]
      ],
      "title": "Controllable Neural Prosody Synthesis",
      "original": "2918",
      "page_count": 5,
      "order": 904,
      "p1": "4437",
      "pn": "4441",
      "abstract": [
        "Speech synthesis has recently seen significant improvements in fidelity,\ndriven by the advent of neural vocoders and neural prosody generators.\nHowever, these systems lack intuitive user controls over prosody, making\nthem unable to rectify prosody errors (e.g., misplaced emphases and\ncontextually inappropriate emotions) or generate prosodies with diverse\nspeaker excitement levels and emotions. We address these limitations\nwith a user-controllable, context-aware neural prosody generator. Given\na real or synthesized speech recording, our model allows a user to\ninput prosody constraints for certain time frames and generates the\nremaining time frames from input text and contextual prosody. We also\npropose a pitch-shifting neural vocoder to modify input speech to match\nthe synthesized prosody. Through objective and subjective evaluations\nwe show that we can successfully incorporate user control into our\nprosody generation model without sacrificing the overall naturalness\nof the synthesized speech.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2918",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "whitehill20_interspeech": {
      "authors": [
        [
          "Matt",
          "Whitehill"
        ],
        [
          "Shuang",
          "Ma"
        ],
        [
          "Daniel",
          "McDuff"
        ],
        [
          "Yale",
          "Song"
        ]
      ],
      "title": "Multi-Reference Neural TTS Stylization with Adversarial Cycle Consistency",
      "original": "2985",
      "page_count": 5,
      "order": 905,
      "p1": "4442",
      "pn": "4446",
      "abstract": [
        "Current multi-reference style transfer models for Text-to-Speech (TTS)\nperform sub-optimally on disjoints datasets, where one dataset contains\nonly a single style class for one of the style dimensions. These models\ngenerally fail to produce style transfer for the dimension that is\nunderrepresented in the dataset. In this paper, we propose an adversarial\ncycle consistency training scheme with paired and unpaired triplets\nto ensure the use of information from all style dimensions. During\ntraining, we incorporate  unpaired triplets with randomly selected\nreference audio samples and encourage the synthesized speech to preserve\nthe appropriate styles using adversarial cycle consistency. We use\nthis method to transfer emotion from a dataset containing four emotions\nto a dataset with only a single emotion. This results in a 78% improvement\nin style transfer (based on emotion classification) with minimal reduction\nin fidelity and naturalness. In subjective evaluations our method was\nconsistently rated as closer to the reference style than the baseline.\nSynthesized speech samples are available at: <KBD>https://sites.google.com/view/adv-cycle-consistent-tts</KBD>\n"
      ],
      "doi": "10.21437/Interspeech.2020-2985",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "gao20e_interspeech": {
      "authors": [
        [
          "Yang",
          "Gao"
        ],
        [
          "Weiyi",
          "Zheng"
        ],
        [
          "Zhaojun",
          "Yang"
        ],
        [
          "Thilo",
          "K\u00f6hler"
        ],
        [
          "Christian",
          "Fuegen"
        ],
        [
          "Qing",
          "He"
        ]
      ],
      "title": "Interactive Text-to-Speech System via Joint Style Analysis",
      "original": "3069",
      "page_count": 5,
      "order": 906,
      "p1": "4447",
      "pn": "4451",
      "abstract": [
        "While modern TTS technologies have made significant advancements in\naudio quality, there is still a lack of behavior naturalness compared\nto conversing with people. We propose a style-embedded TTS system that\ngenerates styled responses based on the speech query style. To achieve\nthis, the system includes a style extraction model that extracts a\nstyle embedding from the speech query, which is then used by the TTS\nto produce a matching response. We faced two main challenges: 1) only\na small portion of the TTS training dataset has style labels, which\nis needed to train a multi-style TTS that respects different style\nembeddings during inference. 2) The TTS system and the style extraction\nmodel have disjoint training datasets. We need consistent style labels\nacross these two datasets so that the TTS can learn to respect the\nlabels produced by the style extraction model during inference. To\nsolve these, we adopted a semi-supervised approach that uses the style\nextraction model to create style labels for the TTS dataset and applied\ntransfer learning to learn the style embedding jointly. Our experiment\nresults show user preference for the styled TTS responses and demonstrate\nthe style-embedded TTS system&#8217;s capability of mimicking the speech\nquery style.\n"
      ],
      "doi": "10.21437/Interspeech.2020-3069",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "hirschi20_interspeech": {
      "authors": [
        [
          "Kevin",
          "Hirschi"
        ],
        [
          "Okim",
          "Kang"
        ],
        [
          "Catia",
          "Cucchiarini"
        ],
        [
          "John H.L.",
          "Hansen"
        ],
        [
          "Keelan",
          "Evanini"
        ],
        [
          "Helmer",
          "Strik"
        ]
      ],
      "title": "Mobile-Assisted Prosody Training for Limited English Proficiency: Learner Background and Speech Learning Pattern",
      "original": "2901",
      "page_count": 5,
      "order": 907,
      "p1": "4452",
      "pn": "4456",
      "abstract": [
        "The use of Mobile-Assisted Pronunciation Training (MAPT) has been increasing\ndrastically due to the personal and interactive nature of mobile devices.\nHowever, MAPT applications lack support from empirical evidence as\nresearch on MAPT-based acquisition, particularly related to prosody,\nhas been rare. The present study employs a MAPT application with lessons\non lexical stress and prominence with Limited English Proficiency (LEP)\nusers (n = 31) of mixed ages and first languages. Then, 16 experienced\nraters conducted discourse-based prosodic analysis on unconstrained\nspeech collected at the beginning and the end of the intervention.\nA series of mixed-effect model analyses were conducted on learner effort,\nimprovement and learner background to investigate their relationship\nwith accentedness and comprehensibility. The results indicated that\npresent MAPT prosody interventions were effective for comprehensibility\nbut not accentedness, however, learner effort on lexical stress and\nprominence exhibit differing patterns. Similar to previous findings,\nlearner age impacts production more than the length of residency or\nhistory of language study. Implications include a prosody-based MAPT\napplication; support for the treatment of accentedness and comprehensibility\nas separate, but related constructs; and a further understanding of\nthe role of learner-related factors in prosody intervention.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2901",
      "author_area_id": "1",
      "author_area_label": "Speech Perception, Production and Acquisition"
    },
    "niekerk20_interspeech": {
      "authors": [
        [
          "Daniel R. van",
          "Niekerk"
        ],
        [
          "Anqi",
          "Xu"
        ],
        [
          "Branislav",
          "Gerazov"
        ],
        [
          "Paul K.",
          "Krug"
        ],
        [
          "Peter",
          "Birkholz"
        ],
        [
          "Yi",
          "Xu"
        ]
      ],
      "title": "Finding Intelligible Consonant-Vowel Sounds Using High-Quality Articulatory Synthesis",
      "original": "2545",
      "page_count": 5,
      "order": 908,
      "p1": "4457",
      "pn": "4461",
      "abstract": [
        "In this study, a state-of-the-art articulatory speech synthesiser was\nused as the basis for simulating the exploration of CV sounds imitating\nspeech stimuli. By adopting a relevant kinematic model and systematically\nreducing the search space of consonant articulatory targets, intelligible\nCV sounds can be found. Derivative-free optimisation strategies were\nevaluated to speed up the process of exploring articulatory space and\nthe possibility of using automatic speech recognition as a means of\nevaluating intelligibility was explored.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2545",
      "author_area_id": "1",
      "author_area_label": "Speech Perception, Production and Acquisition"
    },
    "krishnamohan20_interspeech": {
      "authors": [
        [
          "Venkat",
          "Krishnamohan"
        ],
        [
          "Akshara",
          "Soman"
        ],
        [
          "Anshul",
          "Gupta"
        ],
        [
          "Sriram",
          "Ganapathy"
        ]
      ],
      "title": "Audiovisual Correspondence Learning in Humans and Machines",
      "original": "2674",
      "page_count": 5,
      "order": 909,
      "p1": "4462",
      "pn": "4466",
      "abstract": [
        "Audiovisual correspondence learning is the task of acquiring the association\nbetween images and its corresponding audio. In this paper, we propose\na novel experimental paradigm in which unfamiliar pseudo images and\npseudowords in audio form are introduced to both humans and machine\nsystems. The task is to learn the association between the pairs of\nimage and audio which is later evaluated with a retrieval task. The\nmachine system used in the study is pretrained with the ImageNet corpus\nalong with the corresponding audio labels. This model is transfer learned\nfor the new image-audio pairs. Using the proposed paradigm, we perform\na direct comparison of one-shot, two-shot and three-shot learning performance\nfor humans and machine systems. The human behavioral experiment confirms\nthat the majority of the correspondence learning happens in the first\nexposure of the audio-visual pair. This paper proposes a machine model\nwhich performs on par with the humans in audiovisual correspondence\nlearning. But compared to the machine model, humans exhibited better\ngeneralization ability for new input samples with a single exposure.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2674",
      "author_area_id": "1",
      "author_area_label": "Speech Perception, Production and Acquisition"
    },
    "lan20_interspeech": {
      "authors": [
        [
          "Yizhou",
          "Lan"
        ]
      ],
      "title": "Perception of English Fricatives and Affricates by Advanced Chinese Learners of English",
      "original": "1120",
      "page_count": 4,
      "order": 910,
      "p1": "4467",
      "pn": "4470",
      "abstract": [
        "70 Mandarin-speaking advanced learners of English (level B2 and above)\nparticipated in a perceptual identification experiment eliciting their\npreferred Mandarin equivalent classifications of English fricatives\nand affricates (/s, &#643;, &#679;, &#676;, tr, dr, &#658;/) along\nwith fitness rates. The degree of mapping between Mandarin and English\nconsonants, ranging from poor to fair, and good, were compared against\npredictions by the Perceptual Learning Model, a theoretic model that\npredicts learning outcomes by phonetic distances. Overall, the perceived\nphonetic distances between Mandarin and English consonants predicted\nthe learners&#8217; correct identification of the L2 consonants except\nfor a few number of individual sounds. The Findings suggest that phonetic\nsimilarity do predict most mappings as the learning models postulate,\nbut other factors such as articulatory proximity and orthographic influences\nshould be considered, too.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1120",
      "author_area_id": "1",
      "author_area_label": "Speech Perception, Production and Acquisition"
    },
    "tsukada20_interspeech": {
      "authors": [
        [
          "Kimiko",
          "Tsukada"
        ],
        [
          "Joo-Yeon",
          "Kim"
        ],
        [
          "Jeong-Im",
          "Han"
        ]
      ],
      "title": "Perception of Japanese Consonant Length by Native Speakers of Korean Differing in Japanese Learning Experience",
      "original": "1068",
      "page_count": 5,
      "order": 911,
      "p1": "4471",
      "pn": "4475",
      "abstract": [
        "The perception of Japanese consonant length contrasts (i.e. short/singleton\nvs long/geminate) by native and non-native listeners was compared to\nexamine the extent to which difficult foreign language (FL) sounds\nare processed accurately by native speakers of Korean (NK). Three NK\ngroups differed in their experience with Japanese: non-learners, intermediate\nand advanced. Via the AXB task, the NK speakers&#8217; discrimination\naccuracy of Japanese consonant length contrasts was assessed and compared\nto that of a group of 10 native speakers of Japanese (NJ) who served\nas controls. On average, the NK advanced group did not significantly\ndiffer from the NJ group and outperformed the NK non-learner (but not\nthe NK intermediate) group. The NK intermediate and non-learner groups\ndid not differ from each other. However, regardless of experience with\nJapanese, the NK speakers may benefit from the first language (L1)\nlaryngeal contrasts, associating L1 Korean fortis consonants with Japanese\ngeminates. The NK advanced group appeared less affected than the other\ntwo NK groups by Japanese pitch accent patterns in their consonant\nlength perception. The NK advanced learners&#8217; results demonstrate\nthat it is possible for non-native speakers to acquire native-like\ndiscrimination of consonant length in adulthood.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1068",
      "author_area_id": "1",
      "author_area_label": "Speech Perception, Production and Acquisition"
    },
    "ng20b_interspeech": {
      "authors": [
        [
          "Si-Ioi",
          "Ng"
        ],
        [
          "Tan",
          "Lee"
        ]
      ],
      "title": "Automatic Detection of Phonological Errors in Child Speech Using Siamese Recurrent Autoencoder",
      "original": "2145",
      "page_count": 5,
      "order": 912,
      "p1": "4476",
      "pn": "4480",
      "abstract": [
        "Speech sound disorder (SSD) refers to the developmental disorder in\nwhich children encounter persistent difficulties in correctly pronouncing\nwords. Assessment of SSD has been relying largely on trained speech\nand language pathologists (SLPs). With the increasing demand for and\nlong-lasting shortage of SLPs, automated assessment of speech disorder\nbecomes a highly desirable approach to assisting clinical work. This\npaper describes a study on automatic detection of phonological errors\nin Cantonese speech of kindergarten children, based on a newly collected\nlarge speech corpus. The proposed approach to speech error detection\ninvolves the use of a Siamese recurrent autoencoder, which is trained\nto learn the similarity and discrepancy between phone segments in the\nembedding space. Training of the model requires only speech data from\ntypically developing (TD) children. To distinguish disordered speech\nfrom typical one, cosine distance between the embeddings of the test\nsegment and the reference segment is computed. Different model architectures\nand training strategies are experimented. Results on detecting the\n6 most common consonant errors demonstrate satisfactory performance\nof the proposed model, with the average precision value from 0.82 to\n0.93.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2145",
      "author_area_id": "1",
      "author_area_label": "Speech Perception, Production and Acquisition"
    },
    "ding20e_interspeech": {
      "authors": [
        [
          "Hongwei",
          "Ding"
        ],
        [
          "Binghuai",
          "Lin"
        ],
        [
          "Liyuan",
          "Wang"
        ],
        [
          "Hui",
          "Wang"
        ],
        [
          "Ruomei",
          "Fang"
        ]
      ],
      "title": "A Comparison of English Rhythm Produced by Native American Speakers and Mandarin ESL Primary School Learners",
      "original": "2207",
      "page_count": 5,
      "order": 913,
      "p1": "4481",
      "pn": "4485",
      "abstract": [
        "Prosodic speech characteristics are important in the evaluation of\nboth intelligibility and naturalness of oral English proficiency levels\nfor learners of English as a Second Language (ESL). Different stress\npatterns between English and Mandarin Chinese have been an important\nresearch topic for L2 (second language) English speech learning. However,\nprevious studies seldom employed children as ESL learners on this topic.\nSince more and more children start to learn English in the primary\nschool in China, the current study aims to examine the L2 English rhythm\nof these child learners. We carefully selected 273 English utterances\nfrom a speech database produced by both native speakers and Mandarin\nchild learners, and measured the rhythmic correlates. Results suggested\nthat vowel-related metrics (e.g.  nPVI) are better indexes for L2 rhythmic\nevaluation, which is similar for ESL adults; pause-related fluency\nis another indication for prosodic assessment, especially for child\nESL learners. This investigation could shed some light on the rhythmic\ndifficulties for Mandarin ESL child learners and provide some implications\nfor ESL prosody teaching for school children.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2207",
      "author_area_id": "1",
      "author_area_label": "Speech Perception, Production and Acquisition"
    },
    "zhou20h_interspeech": {
      "authors": [
        [
          "Chao",
          "Zhou"
        ],
        [
          "Silke",
          "Hamann"
        ]
      ],
      "title": "Cross-Linguistic Interaction Between Phonological Categorization and Orthography Predicts Prosodic Effects in the Acquisition of Portuguese Liquids by L1-Mandarin Learners",
      "original": "2689",
      "page_count": 5,
      "order": 914,
      "p1": "4486",
      "pn": "4490",
      "abstract": [
        "Prior research has revealed that L1-Mandarin learners employed position-dependent\nrepair strategies for European Portuguese /l/ and /&#638;/. In this\nstudy we examined whether this L2 prosodic effect can be attributed\nto a cross-linguistic influence and whether the replacement of the\nPortuguese rhotic by the Mandarin [&#633;] is due to perception or\northography. We performed a delayed imitation task with na&#239;ve\nMandarin listeners and manipulated the presented input types (auditory\nform alone or a combination of auditory and written forms). Results\nshowed that na&#239;ve responses were reminiscent of L1-Mandarin learners&#8217;\nbehaviour, and that [&#633;] was used almost exclusively in the presence\nof written input, suggesting that the prosodic effect attested in L2\nacquisition of European Portuguese /l/ and /&#638;/ stems from cross-linguistic\ninteraction between phonological categorization and orthography.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2689",
      "author_area_id": "1",
      "author_area_label": "Speech Perception, Production and Acquisition"
    },
    "li20ka_interspeech": {
      "authors": [
        [
          "Wenqian",
          "Li"
        ],
        [
          "Jung-Yueh",
          "Tu"
        ]
      ],
      "title": "Cross-Linguistic Perception of Utterances with Willingness and Reluctance in Mandarin by Korean L2 Learners",
      "original": "1640",
      "page_count": 5,
      "order": 915,
      "p1": "4491",
      "pn": "4495",
      "abstract": [
        "This study investigated the cross-linguistic perception of attitudinal\nintonation with willingness and reluctance in Mandarin by Korean L2\nlearners. In the current study, 20 Korean L2 learners of Mandarin (KL2)\nand 20 native Mandarin listeners (CL1) were instructed to rate perceived\ndegree of willingness (1&#8211;5 Likert scale) from the utterances\n(with willingness, reluctance, and neutrality) produced by 2 native\nMandarin speakers (one male and one female). The rating results showed\nthat 1) the rating scores of willing attitude were significantly higher\nthan those of reluctant attitude by KL2; 2) utterances of willingness\nand neutrality tend to be perceived less willing by KL2 than by CL1;\n3) KL2 had a narrower rating range on the perception of attitudinal\nintonation than CL1. Specifically, Korean females had a wider rating\nrange than Korean males. The findings indicated that 1) utterances\nof willingness, neutrality, and reluctance in Mandarin were accurately\nperceived by KL2; 2) willingness carried by attitudinal intonation\nwas weakened through L2 pragmatic comprehension by KL2; 3) Korean females\nwere more sensitive than Korean males on the perception of attitudinal\nintonation. The overall results suggest significant effects of language\nexperience and gender difference on the perception of Chinese utterances\nwith willingness and reluctance.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1640",
      "author_area_id": "1",
      "author_area_label": "Speech Perception, Production and Acquisition"
    },
    "cheng20b_interspeech": {
      "authors": [
        [
          "Rui",
          "Cheng"
        ],
        [
          "Changchun",
          "Bao"
        ]
      ],
      "title": "Speech Enhancement Based on Beamforming and Post-Filtering by Combining Phase Information",
      "original": "0990",
      "page_count": 5,
      "order": 916,
      "p1": "4496",
      "pn": "4500",
      "abstract": [
        "Speech enhancement is an indispensable technology in the field of speech\ninteraction. With the development of microphone array signal processing\ntechnology and deep learning, the beamforming combined with neural\nnetwork has provided a more diverse solution for this field. In this\npaper, a multi-channel speech enhancement method is proposed, which\ncombines beamforming and post-filtering based on neural network. The\nspatial features and phase information of target speech are incorporated\ninto the beamforming by neural network, and a neural network based\nsingle-channel post-filtering with the phase correction is further\ncombined to improve the performance. The experiments at different signal-to-noise\nratio (SNR) levels confirmed that the proposed method results in an\nobvious improvement on speech quality and intelligibility compared\nto the reference methods.\n"
      ],
      "doi": "10.21437/Interspeech.2020-990",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "wang20ha_interspeech": {
      "authors": [
        [
          "Yu-Xuan",
          "Wang"
        ],
        [
          "Jun",
          "Du"
        ],
        [
          "Li",
          "Chai"
        ],
        [
          "Chin-Hui",
          "Lee"
        ],
        [
          "Jia",
          "Pan"
        ]
      ],
      "title": "A Noise-Aware Memory-Attention Network Architecture for Regression-Based Speech Enhancement",
      "original": "2037",
      "page_count": 5,
      "order": 917,
      "p1": "4501",
      "pn": "4505",
      "abstract": [
        "We propose a novel noise-aware memory-attention network (NAMAN) for\nregression-based speech enhancement, aiming at improving quality of\nenhanced speech in unseen noise conditions. The NAMAN architecture\nconsists of three parts, a main regression network, a memory block\nand an attention block. First, a long short-term memory recurrent neural\nnetwork (LSTM-RNN) is adopted as the main network to well model the\nacoustic context of neighboring frames. Next, the memory block is built\nwith an extensive set of noise feature vectors as the prior noise bases.\nFinally, the attention block serves as an auxiliary network to improve\nthe noise awareness of the main network by encoding the dynamic noise\ninformation at frame level through additional features obtained by\nweighing the existing noise basis vectors in the memory block. Our\nexperiments show that the proposed NAMAN framework is compact and outperforms\nthe state-of-the-art dynamic noise-aware training approaches in low\nSNR conditions.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2037",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "su20b_interspeech": {
      "authors": [
        [
          "Jiaqi",
          "Su"
        ],
        [
          "Zeyu",
          "Jin"
        ],
        [
          "Adam",
          "Finkelstein"
        ]
      ],
      "title": "HiFi-GAN: High-Fidelity Denoising and Dereverberation Based on Speech Deep Features in Adversarial Networks",
      "original": "2143",
      "page_count": 5,
      "order": 918,
      "p1": "4506",
      "pn": "4510",
      "abstract": [
        "Real-world audio recordings are often degraded by factors such as noise,\nreverberation, and equalization distortion. This paper introduces HiFi-GAN,\na deep learning method to transform recorded speech to sound as though\nit had been recorded in a studio. We use an end-to-end feed-forward\nWaveNet architecture, trained with multi-scale adversarial discriminators\nin both the time domain and the time-frequency domain. It relies on\nthe deep feature matching losses of the discriminators to improve the\nperceptual quality of enhanced speech. The proposed model generalizes\nwell to new speakers, new speech content, and new environments. It\nsignificantly outperforms state-of-the-art baseline methods in both\nobjective and subjective experiments.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2143",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "pandey20_interspeech": {
      "authors": [
        [
          "Ashutosh",
          "Pandey"
        ],
        [
          "DeLiang",
          "Wang"
        ]
      ],
      "title": "Learning Complex Spectral Mapping for Speech Enhancement with Improved Cross-Corpus Generalization",
      "original": "2561",
      "page_count": 5,
      "order": 919,
      "p1": "4511",
      "pn": "4515",
      "abstract": [
        "It is recently revealed that deep learning based speech enhancement\nsystems do not generalize to untrained corpora in low signal-to-noise\nratio (SNR) conditions, mainly due to the channel mismatch between\ntrained and untrained corpora. In this study, we investigate techniques\nto improve cross-corpus generalization of complex spectrogram enhancement.\nFirst, we propose a long short-term memory (LSTM) network for complex\nspectral mapping. Evaluated on untrained noises and corpora, the proposed\nnetwork substantially outperforms a state-of-the-art gated convolutional\nrecurrent network (GCRN). Next, we examine the importance of training\ncorpus for cross-corpus generalization. It is found that a training\ncorpus that contains utterances with different channels can significantly\nimprove performance on untrained corpora. Finally, we observe that\nusing a smaller frame shift in short-time Fourier transform (STFT)\nis a simple but highly effective technique to improve cross-corpus\ngeneralization.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2561",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "richter20_interspeech": {
      "authors": [
        [
          "Julius",
          "Richter"
        ],
        [
          "Guillaume",
          "Carbajal"
        ],
        [
          "Timo",
          "Gerkmann"
        ]
      ],
      "title": "Speech Enhancement with Stochastic Temporal Convolutional Networks",
      "original": "2588",
      "page_count": 5,
      "order": 920,
      "p1": "4516",
      "pn": "4520",
      "abstract": [
        "We consider the problem of speech modeling in speech enhancement. Recently,\ndeep generative approaches based on variational autoencoders have been\nproposed to model speech spectrograms. However, these approaches are\nbased either on hierarchical or temporal dependencies of stochastic\nlatent variables. In this paper, we propose a generative approach to\nspeech enhancement based on a stochastic temporal convolutional network,\nwhich combines both hierarchical and temporal dependencies of stochastic\nvariables. We evaluate our method with real recordings of different\nnoisy environments. The proposed speech enhancement method outperforms\na previous non-sequential approach based on feed-forward fully-connected\nnetworks in terms of speech distortion, instrumental speech quality\nand intelligibility. At the same time, the computational cost of the\nproposed generative speech model remains feasible, due to inherent\nparallelism of the convolutional architecture.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2588",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "gogate20_interspeech": {
      "authors": [
        [
          "Mandar",
          "Gogate"
        ],
        [
          "Kia",
          "Dashtipour"
        ],
        [
          "Amir",
          "Hussain"
        ]
      ],
      "title": "Visual Speech In Real Noisy Environments (VISION): A Novel Benchmark Dataset and Deep Learning-Based Baseline System",
      "original": "2935",
      "page_count": 5,
      "order": 921,
      "p1": "4521",
      "pn": "4525",
      "abstract": [
        "In this paper, we present VIsual Speech In real nOisy eNvironments\n(VISION), a first of its kind audio-visual (AV) corpus comprising 2500\nutterances from 209 speakers, recorded in real noisy environments including\nsocial gatherings, streets, cafeterias and restaurants. While a number\nof speech enhancement frameworks have been proposed in the literature\nthat exploit AV cues, there are no visual speech corpora recorded in\nreal environments with a sufficient variety of speakers, to enable\nevaluation of AV frameworks&#8217; generalisation capability in a wide\nrange of background visual and acoustic noises. The main purpose of\nour AV corpus is to foster research in the area of AV signal processing\nand to provide a benchmark corpus that can be used for reliable evaluation\nof AV speech enhancement systems in everyday noisy settings. In addition,\nwe present a baseline deep neural network (DNN) based spectral mask\nestimation model for speech enhancement. Comparative simulation results\nwith subjective listening tests demonstrate significant performance\nimprovement of the baseline DNN compared to state-of-the-art speech\nenhancement approaches.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2935"
    },
    "sivaraman20_interspeech": {
      "authors": [
        [
          "Aswin",
          "Sivaraman"
        ],
        [
          "Minje",
          "Kim"
        ]
      ],
      "title": "Sparse Mixture of Local Experts for Efficient Speech Enhancement",
      "original": "2989",
      "page_count": 5,
      "order": 922,
      "p1": "4526",
      "pn": "4530",
      "abstract": [
        "This work proposes a novel approach for reducing the computational\ncomplexity of speech denoising neural networks by using a sparsely\nactive ensemble topology. In our ensemble networks, a gating module\nclassifies an input noisy speech signal either by identifying speaker\ngender or by estimating signal degradation, and exclusively assigns\nit to a best-case specialist module, optimized to denoise a particular\nsubset of the training data. This approach extends the hypothesis that\nspeech denoising can be simplified if it is split into non-overlapping\nsubproblems, contrasting earlier approaches that train large generalist\nneural networks to address a wide range of noisy speech data. We compare\na baseline recurrent network against an ensemble of similarly designed,\nbut smaller networks. Each network module is trained independently\nand combined to form a na&#239;ve ensemble. This can be further fine-tuned\nusing a sparsity parameter to improve performance. Our experiments\non noisy speech data &#8212; generated by mixing LibriSpeech and MUSAN\ndatasets &#8212; demonstrate that a fine-tuned sparsely active ensemble\ncan outperform a generalist using significantly fewer calculations.\nThe key insight of this paper, leveraging model selection as a form\nof network compression, may be used to supplement already-existing\ndeep learning methods for speech denoising.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2989",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "kishore20_interspeech": {
      "authors": [
        [
          "Vinith",
          "Kishore"
        ],
        [
          "Nitya",
          "Tiwari"
        ],
        [
          "Periyasamy",
          "Paramasivam"
        ]
      ],
      "title": "Improved Speech Enhancement Using TCN with Multiple Encoder-Decoder Layers",
      "original": "3122",
      "page_count": 5,
      "order": 923,
      "p1": "4531",
      "pn": "4535",
      "abstract": [
        "A deep learning based time domain single-channel speech enhancement\ntechnique using multilayer encoder-decoder and a temporal convolutional\nnetwork is proposed for use in applications such as smart speakers\nand voice assistants. The technique uses encoder-decoder with convolutional\nlayers for obtaining representation suitable for speech enhancement\nand a temporal convolutional network (TCN) based separator between\nthe encoder and decoder to learn long-range dependencies. The technique\nderives inspiration from speech separation techniques that use TCN\nbased separator between a single layer encoder-decoder. We propose\nto use a multilayer encoder-decoder to obtain a noise-independent representation\nuseful for separating clean speech and noise. We present t-SNE-based\nanalysis of the representation learned using different architectures\nfor selecting the optimal number of encoder-decoder layers. We evaluate\nthe proposed architectures using an objective measure of speech quality,\nscale-invariant source-to-noise ratio, and by obtaining word error\nrate on a speech recognition platform. The proposed two-layer encoder-decoder\narchitecture resulted in 48% improvement in WER over unprocessed noisy\ndata and 33% and 44% improvement in WER over two baselines.\n"
      ],
      "doi": "10.21437/Interspeech.2020-3122",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "fan20e_interspeech": {
      "authors": [
        [
          "Cunhang",
          "Fan"
        ],
        [
          "Jianhua",
          "Tao"
        ],
        [
          "Bin",
          "Liu"
        ],
        [
          "Jiangyan",
          "Yi"
        ],
        [
          "Zhengqi",
          "Wen"
        ]
      ],
      "title": "Joint Training for Simultaneous Speech Denoising and Dereverberation with Deep Embedding Representations",
      "original": "1225",
      "page_count": 5,
      "order": 924,
      "p1": "4536",
      "pn": "4540",
      "abstract": [
        "Monaural speech dereverberation is a very challenging task because\nno spatial cues can be used. When the additive noises exist, this task\nbecomes more challenging. In this paper, we propose a joint training\nmethod for simultaneous speech denoising and dereverberation using\ndeep embedding representations. Firstly, at the denoising stage, the\ndeep clustering (DC) network is used to extract noise-free deep embedding\nrepresentations from the anechoic speech and residual reverberation\nsignals. These deep embedding representations are represent the inferred\nspectral masking patterns of the desired signals so that they could\ndiscriminate the anechoic speech and the reverberant signals very well.\nSecondly, at the dereverberation stage, we utilize another supervised\nneural network to estimate the mask of anechoic speech from these deep\nembedding representations. Finally, the joint training algorithm is\nused to train the speech denoising and dereverberation network. Therefore,\nthe noise reduction and dereverberation can be simultaneously optimized.\nOur experiments are conducted on the TIMIT dataset. Experimental results\nshow that the proposed method outperforms the WPE and BLSTM baselines.\nEspecially in the low SNR (-5 dB) condition, our proposed method produces\na relative improvement of 7.8% for PESQ compared with BLSTM method\nand relative reductions of 16.3% and 19.3% for CD and LLR measures.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1225",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "fontaine20_interspeech": {
      "authors": [
        [
          "Mathieu",
          "Fontaine"
        ],
        [
          "Kouhei",
          "Sekiguchi"
        ],
        [
          "Aditya Arie",
          "Nugraha"
        ],
        [
          "Kazuyoshi",
          "Yoshii"
        ]
      ],
      "title": "Unsupervised Robust Speech Enhancement Based on Alpha-Stable Fast Multichannel Nonnegative Matrix Factorization",
      "original": "3202",
      "page_count": 5,
      "order": 925,
      "p1": "4541",
      "pn": "4545",
      "abstract": [
        "This paper describes multichannel speech enhancement based on a probabilistic\nmodel of complex source spectrograms for improving the intelligibility\nof speech corrupted by undesired noise. The univariate complex Gaussian\nmodel with the reproductive property supports the additivity of source\ncomplex spectrograms and forms the theoretical basis of nonnegative\nmatrix factorization (NMF). Multichannel NMF (MNMF) is an extension\nof NMF based on the multivariate complex Gaussian model with spatial\ncovariance matrices (SCMs), and its state-of-the-art variant called\nFastMNMF with jointly-diagonalizable SCMs achieves faster decomposition\nbased on the univariate Gaussian model in the transformed domain where\nall time-frequency-channel elements are independent. Although a heavy-tailed\nextension of FastMNMF has been proposed to improve the robustness against\nimpulsive noise, the source additivity has never been considered. The\nmultivariate &#945;-stable distribution does not have the reproductive\nproperty for the shape matrix parameter. This paper, therefore, proposes\na heavy-tailed extension called &#945;-stable FastMNMF which works\nin the transformed domain to use a univariate complex &#945;-stable\nmodel, satisfying the reproductive property for any tail lightness\nparameter &#945; and allowing the &#945;-fractional Wiener filtering\nbased on the element-wise source additivity. The experimental results\nshow that &#945;-stable FastMNMF with &#945; = 1.8 significantly outperforms\nGaussian FastMNMF (&#945;=2).\n"
      ],
      "doi": "10.21437/Interspeech.2020-3202",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "albes20_interspeech": {
      "authors": [
        [
          "Merlin",
          "Albes"
        ],
        [
          "Zhao",
          "Ren"
        ],
        [
          "Bj\u00f6rn W.",
          "Schuller"
        ],
        [
          "Nicholas",
          "Cummins"
        ]
      ],
      "title": "Squeeze for Sneeze: Compact Neural Networks for Cold and Flu Recognition",
      "original": "2531",
      "page_count": 5,
      "order": 926,
      "p1": "4546",
      "pn": "4550",
      "abstract": [
        "In digital health applications, speech offers advantages over other\nphysiological signals, in that it can be easily collected, transmitted,\nand stored using mobile and Internet of Things (IoT) technologies.\nHowever, to take full advantage of this positioning, speech-based machine\nlearning models need to be deployed on devices that can have considerable\nmemory and power constraints. These constraints are particularly apparent\nwhen attempting to deploy deep learning models, as they require substantial\namounts of memory and data movement operations. Herein, we test the\nsuitability of pruning and quantisation as two methods to compress\nthe overall size of neural networks trained for a health-driven speech\nclassification task. Key results presented on the Upper Respiratory\nTract Infection Corpus indicate that pruning, then quantising a network\ncan reduce the number of operational weights by almost 90%. They also\ndemonstrate the overall size of the network can be reduced by almost\n95%, as measured in MB, without affecting overall recognition performance.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2531",
      "author_area_id": "3",
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "seneviratne20_interspeech": {
      "authors": [
        [
          "Nadee",
          "Seneviratne"
        ],
        [
          "James R.",
          "Williamson"
        ],
        [
          "Adam C.",
          "Lammert"
        ],
        [
          "Thomas F.",
          "Quatieri"
        ],
        [
          "Carol",
          "Espy-Wilson"
        ]
      ],
      "title": "Extended Study on the Use of Vocal Tract Variables to Quantify Neuromotor Coordination in Depression",
      "original": "2758",
      "page_count": 5,
      "order": 927,
      "p1": "4551",
      "pn": "4555",
      "abstract": [
        "Changes in speech production that occur as a result of psychomotor\nslowing, a key feature of Major Depressive Disorder (MDD), are used\nto non-invasively diagnose MDD. In previous work using data from seven\nsubjects, we showed that using speech-inverted vocal tract variables\n(TVs) as a direct measure of articulation to quantify changes in the\nway speech is produced when depressed relative to being not depressed\noutperforms formant information as a proxy for articulatory information.\nIn this paper, we made significant extensions by using more subjects,\ntaking into account more eigenvalue features and incorporating TVs\nrelated to (1) place of articulation and (2) the glottal source. These\nadditions result in a significant improvement in accuracy, particularly\nfor free speech. As a baseline, we perform a similar analysis using\nhigher-dimensional Mel Frequency Cepstral Coefficients (MFCCs).\n"
      ],
      "doi": "10.21437/Interspeech.2020-2758",
      "author_area_id": "3",
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "xezonaki20_interspeech": {
      "authors": [
        [
          "Danai",
          "Xezonaki"
        ],
        [
          "Georgios",
          "Paraskevopoulos"
        ],
        [
          "Alexandros",
          "Potamianos"
        ],
        [
          "Shrikanth",
          "Narayanan"
        ]
      ],
      "title": "Affective Conditioning on Hierarchical Attention Networks Applied to Depression Detection from Transcribed Clinical Interviews",
      "original": "2819",
      "page_count": 5,
      "order": 928,
      "p1": "4556",
      "pn": "4560",
      "abstract": [
        "In this work we propose a machine learning model for depression detection\nfrom transcribed clinical interviews. Depression is a mental disorder\nthat impacts not only the subject&#8217;s mood but also the use of\nlanguage. To this end we use a Hierarchical Attention Network to classify\ninterviews of depressed subjects. We augment the attention layer of\nour model with a conditioning mechanism on linguistic features, extracted\nfrom affective lexica. Our analysis shows that individuals diagnosed\nwith depression use affective language to a greater extent than not-depressed.\nOur experiments show that external affective information improves the\nperformance of the proposed architecture in the General Psychotherapy\nCorpus and the DAIC-WoZ 2017 depression datasets, achieving state-of-the-art\n71.6 and 70.3 using the test set, F1-scores respectively.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2819",
      "author_area_id": "3",
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "huang20g_interspeech": {
      "authors": [
        [
          "Zhaocheng",
          "Huang"
        ],
        [
          "Julien",
          "Epps"
        ],
        [
          "Dale",
          "Joachim"
        ],
        [
          "Brian",
          "Stasak"
        ],
        [
          "James R.",
          "Williamson"
        ],
        [
          "Thomas F.",
          "Quatieri"
        ]
      ],
      "title": "Domain Adaptation for Enhancing Speech-Based Depression Detection in Natural Environmental Conditions Using Dilated CNNs",
      "original": "3135",
      "page_count": 5,
      "order": 929,
      "p1": "4561",
      "pn": "4565",
      "abstract": [
        "Depression disorders are a major growing concern worldwide, especially\ngiven the unmet need for widely deployable depression screening for\nuse in real-world environments. Speech-based depression screening technologies\nhave shown promising results, but primarily in systems that are trained\nusing laboratory-based recorded speech. They do not generalize well\non data from more naturalistic settings. This paper addresses the generalizability\nissue by proposing multiple adaptation strategies that update pre-trained\nmodels based on a dilated convolutional neural network (CNN) framework,\nwhich improve depression detection performance in both clean and naturalistic\nenvironments. Experimental results on two depression corpora show that\nfeature representations in CNN layers need to be adapted to accommodate\nenvironmental changes, and that increases in data quantity and quality\nare helpful for pre-training models for adaptation. The cross-corpus\nadapted systems produce relative improvements of 29.4% and 17.2% in\nunweighted average recall over non-adapted systems for both clean and\nnaturalistic corpora, respectively.\n"
      ],
      "doi": "10.21437/Interspeech.2020-3135",
      "author_area_id": "3",
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "gosztolya20b_interspeech": {
      "authors": [
        [
          "G\u00e1bor",
          "Gosztolya"
        ],
        [
          "Anita",
          "Bagi"
        ],
        [
          "Szilvia",
          "Szal\u00f3ki"
        ],
        [
          "Istv\u00e1n",
          "Szendi"
        ],
        [
          "Ildik\u00f3",
          "Hoffmann"
        ]
      ],
      "title": "Making a Distinction Between Schizophrenia and Bipolar Disorder Based on Temporal Parameters in Spontaneous Speech",
      "original": "0049",
      "page_count": 5,
      "order": 930,
      "p1": "4566",
      "pn": "4570",
      "abstract": [
        "Schizophrenia is a heterogeneous chronic and severe mental disorder.\nThere are several different theories for the development of schizophrenia\nfrom an etiological point of view: neurochemical, neuroanatomical,\npsychological and genetic factors may also be present in the background\nof the disease. In this study, we examined spontaneous speech productions\nby patients suffering from schizophrenia (SCH) and bipolar disorder\n(BD). We extracted 15 temporal parameters from the speech excerpts\nand used machine learning techniques for distinguishing the SCH and\nBD groups, their subgroups (SCH-S and SCH-Z) and subtypes (BD-I and\nBD-II). Our results indicated, that there is a notable difference between\nspontaneous speech productions of certain subgroups, while some appears\nto be indistinguishable for the used classification model. Firstly,\nSCH and BD groups were found to be different. Secondly, the results\nof SCH-S subgroup were distinct from BD. Thirdly, the spontaneous speech\nof the SCH-Z subgroup was found to be very similar to the BD-I, however,\nit was sharply distinct from BD-II. Our detailed examination highlighted\nthe indistinguishable subgroups and led to us to make our S and Z theory\nmore clarified.\n"
      ],
      "doi": "10.21437/Interspeech.2020-49",
      "author_area_id": "3",
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "huckvale20_interspeech": {
      "authors": [
        [
          "Mark",
          "Huckvale"
        ],
        [
          "Andr\u00e1s",
          "Beke"
        ],
        [
          "Mirei",
          "Ikushima"
        ]
      ],
      "title": "Prediction of Sleepiness Ratings from Voice by Man and Machine",
      "original": "1601",
      "page_count": 5,
      "order": 931,
      "p1": "4571",
      "pn": "4575",
      "abstract": [
        "This paper looks in more detail at the Interspeech 2019 computational\nparalinguistics challenge on the prediction of sleepiness ratings from\nspeech. In this challenge, teams were asked to train a regression model\nto predict sleepiness from samples of the D&#252;sseldorf Sleepy Language\nCorpus (DSLC). This challenge was notable because the performance of\nall entrants was uniformly poor, with even the winning system only\nachieving a correlation of r=0.37. We look at whether the task itself\nis achievable, and whether the corpus is suited to training a machine\nlearning system for the task. We perform a listening experiment using\nsamples from the corpus and show that a group of human listeners can\nachieve a correlation of r=0.7 on this task, although this is mainly\nby classifying the recordings into one of three sleepiness groups.\nWe show that the corpus, because of its construction, confounds variation\nwith sleepiness and variation with speaker identity, and this was the\nreason that machine learning systems failed to perform well. We conclude\nthat sleepiness rating prediction from voice is not an impossible task,\nbut that good performance requires more information about sleepy speech\nand its variability across listeners than is available in the DSLC\ncorpus.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1601",
      "author_area_id": "3",
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "teplansky20_interspeech": {
      "authors": [
        [
          "Kristin J.",
          "Teplansky"
        ],
        [
          "Alan",
          "Wisler"
        ],
        [
          "Beiming",
          "Cao"
        ],
        [
          "Wendy",
          "Liang"
        ],
        [
          "Chad W.",
          "Whited"
        ],
        [
          "Ted",
          "Mau"
        ],
        [
          "Jun",
          "Wang"
        ]
      ],
      "title": "Tongue and Lip Motion Patterns in Alaryngeal Speech",
      "original": "2854",
      "page_count": 5,
      "order": 932,
      "p1": "4576",
      "pn": "4580",
      "abstract": [
        "A laryngectomy is the surgical removal of the larynx which results\nin the loss of phonation. The aim of this study was to characterize\ntongue and lip movements during speech produced by individuals who\nhave had a laryngectomy. EMA (electromagnetic articulography) was used\nto derive movement data from the tongue and lips of nine speakers (four\nalaryngeal and five typical). The kinematic metrics included movement\nduration, range, speed, and cumulative path distance. We also used\na support vector machine (SVM) to classify alaryngeal and healthy speech\nmovement patterns. Our preliminary results indicated that alaryngeal\narticulation is longer in duration than healthy speakers. Alaryngeal\nspeakers also use larger lateral tongue movements and move the tongue\nback at a slower speed than healthy speakers. The results from the\nSVM model also indicates that alaryngeal articulatory movement patterns\nare distinct from healthy speakers. Taken together, these findings\nsuggest that there are differences in articulatory behavior that occur\nafter the removal of the larynx. It may be helpful to consider the\ndistinct articulatory motion patterns of alaryngeal speech in clinical\npractice and in the development of technologies (e.g., silent speech\ninterfaces) that assist to provide an intelligible form of speech for\nthis patient population.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2854",
      "author_area_id": "3",
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "yue20b_interspeech": {
      "authors": [
        [
          "Zhengjun",
          "Yue"
        ],
        [
          "Heidi",
          "Christensen"
        ],
        [
          "Jon",
          "Barker"
        ]
      ],
      "title": "Autoencoder Bottleneck Features with Multi-Task Optimisation for Improved Continuous Dysarthric Speech Recognition",
      "original": "2746",
      "page_count": 5,
      "order": 933,
      "p1": "4581",
      "pn": "4585",
      "abstract": [
        "Automatic recognition of dysarthric speech is a very challenging research\nproblem where performances still lag far behind those achieved for\ntypical speech. The main reason is the lack of suitable training data\nto accommodate for the large mismatch seen between dysarthric and typical\nspeech. Only recently has focus moved from single-word tasks to exploring\ncontinuous speech ASR needed for dictation and most voice-enabled interfaces.\nThis paper investigates improvements to dysarthric continuous ASR.\nIn particular, we demonstrate the effectiveness of using unsupervised\nautoencoder-based bottleneck (AE-BN) feature extractor trained on out-of-domain\n(OOD) LibriSpeech data. We further explore multi-task optimisation\ntechniques shown to benefit typical speech ASR. We propose a 5-fold\ncross-training setup on the widely used TORGO dysarthric database.\nA setup we believe is more suitable for this low-resource data domain.\nResults show that adding the proposed AE-BN features achieves an average\nabsolute (word error rate) WER improvement of 2.63% compared to the\nbaseline system. A further reduction of 2.33% and 0.65% absolute WER\nis seen when applying monophone regularisation and joint optimisation\ntechniques, respectively. In general, the ASR system employing monophone\nregularisation trained on AE-BN features exhibits the best performance.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2746",
      "author_area_id": "3",
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "mallela20_interspeech": {
      "authors": [
        [
          "Jhansi",
          "Mallela"
        ],
        [
          "Aravind",
          "Illa"
        ],
        [
          "Yamini",
          "Belur"
        ],
        [
          "Nalini",
          "Atchayaram"
        ],
        [
          "Ravi",
          "Yadav"
        ],
        [
          "Pradeep",
          "Reddy"
        ],
        [
          "Dipanjan",
          "Gope"
        ],
        [
          "Prasanta Kumar",
          "Ghosh"
        ]
      ],
      "title": "Raw Speech Waveform Based Classification of Patients with ALS, Parkinson&#8217;s Disease and Healthy Controls Using CNN-BLSTM",
      "original": "2221",
      "page_count": 5,
      "order": 934,
      "p1": "4586",
      "pn": "4590",
      "abstract": [
        "Analysis of speech waveform through automated methods in patients with\nAmyotrophic Lateral Sclerosis (ALS), and Parkinson&#8217;s disease\n(PD) can be used for early diagnosis and monitoring disease progression.\nMany works in the past have used different acoustic features for the\nclassification of patients with ALS and PD with healthy controls (HC).\nIn this work, we propose a data-driven approach to learn representations\nfrom raw speech waveform. Our model comprises of 1-D CNN layer to extract\nrepresentations from raw speech followed by BLSTM layers for the classification\ntasks. We consider 3 different classification tasks (ALS vs HC), (PD\nvs HC), and (ALS vs PD). We perform each classification task using\nfour different speech stimuli in two scenarios: i) trained and tested\nin a stimulus-specific manner, ii) trained on data pooled from all\nstimuli, and test on each stimulus separately. Experiments with 60\nALS, 60 PD, and 60 HC show that the frequency responses of the learned\n1-D CNN filters are low pass in nature, and the center frequencies\nlie below 1kHz. The learned representations form raw speech perform\nbetter than MFCC which is considered as baseline. Experiments with\npooled models yield a better result compared to the task-specific models.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2221",
      "author_area_id": "3",
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "pompili20b_interspeech": {
      "authors": [
        [
          "Anna",
          "Pompili"
        ],
        [
          "Rub\u00e9n",
          "Solera-Ure\u00f1a"
        ],
        [
          "Alberto",
          "Abad"
        ],
        [
          "Rita",
          "Cardoso"
        ],
        [
          "Isabel",
          "Guimar\u00e3es"
        ],
        [
          "Margherita",
          "Fabbri"
        ],
        [
          "Isabel P.",
          "Martins"
        ],
        [
          "Joaquim",
          "Ferreira"
        ]
      ],
      "title": "Assessment of Parkinson&#8217;s Disease Medication State Through Automatic Speech Analysis",
      "original": "2726",
      "page_count": 5,
      "order": 935,
      "p1": "4591",
      "pn": "4595",
      "abstract": [
        "Parkinson&#8217;s disease (PD) is a progressive degenerative disorder\nof the central nervous system characterized by motor and non-motor\nsymptoms. As the disease progresses, patients alternate periods in\nwhich motor symptoms are mitigated due to medication intake (ON state)\nand periods with motor complications (OFF state). The time that patients\nspend in the OFF condition is currently the main parameter employed\nto assess pharmacological interventions and to evaluate the efficacy\nof different active principles. In this work, we present a system that\ncombines automatic speech processing and deep learning techniques to\nclassify the medication state of PD patients by leveraging personal\nspeech-based bio-markers. We devise a speaker-dependent approach and\ninvestigate the relevance of different acoustic-prosodic feature sets.\nResults show an accuracy of 90.54% in a test task with mixed speech\nand an accuracy of 95.27% in a semi-spontaneous speech task. Overall,\nthe experimental assessment shows the potentials of this approach towards\nthe development of reliable, remote daily monitoring and scheduling\nof medication intake of PD patients.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2726",
      "author_area_id": "3",
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "zhang20ja_interspeech": {
      "authors": [
        [
          "Chao",
          "Zhang"
        ],
        [
          "Junjie",
          "Cheng"
        ],
        [
          "Yanmei",
          "Gu"
        ],
        [
          "Huacan",
          "Wang"
        ],
        [
          "Jun",
          "Ma"
        ],
        [
          "Shaojun",
          "Wang"
        ],
        [
          "Jing",
          "Xiao"
        ]
      ],
      "title": "Improving Replay Detection System with Channel Consistency DenseNeXt for the ASVspoof 2019 Challenge",
      "original": "1044",
      "page_count": 5,
      "order": 936,
      "p1": "4596",
      "pn": "4600",
      "abstract": [
        "In this paper we describe a novel replay detection system for the ASVspoof\n2019 challenge. The objective of this challenge is to distinguish arbitrarily\naudio files from bona fide or spoofing attacks, where spoofing attacking\nincludes replay attacks, text-to-speech and voice conversions. Our\nreplay detection system is a pipeline system with three aspects: feature\nengineering, DNN models, and score fusion. Firstly, logspec is extracted\nas input features according to previous research works where spectrum\naugmentation is applied during training stage to boost performance\nunder limited training data. Secondly, DNN models part includes three\nmajor models: SEnet, DenseNet, and our proposed model, channel consistency\nDenseNeXt, where binary cross entropy loss and center loss are applied\nas training objectives. Finally, score fusion is applied to all three\nDNN models in order to obtain primary system results. The experiment\nresults show that for our best single system, channel consistency DenseNeXt,\nt-DCF and EER are 0.0137 and 0.46% on physical access evaluation set\nrespectively. The performance of primary system obtains 0.00785 and\n0.282% in terms of t-DCF and EER respectively. This is a 96.8% improvement\ncompared to the baseline system CQCC-GMM and it achieves state-of-the-art\nperformance in PA challenge.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1044",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "falkowskigilski20_interspeech": {
      "authors": [
        [
          "Przemyslaw",
          "Falkowski-Gilski"
        ],
        [
          "Grzegorz",
          "Debita"
        ],
        [
          "Marcin",
          "Habrych"
        ],
        [
          "Bogdan",
          "Miedzinski"
        ],
        [
          "Przemyslaw",
          "Jedlikowski"
        ],
        [
          "Bartosz",
          "Polnik"
        ],
        [
          "Jan",
          "Wandzio"
        ],
        [
          "Xin",
          "Wang"
        ]
      ],
      "title": "Subjective Quality Evaluation of Speech Signals Transmitted via BPL-PLC Wired System",
      "original": "1077",
      "page_count": 5,
      "order": 937,
      "p1": "4601",
      "pn": "4605",
      "abstract": [
        "The broadband over power line &#8211; power line communication (BPL-PLC)\ncable is resistant to electricity stoppage and partial damage of phase\nconductors. It maintains continuity of transmission in case of an emergency.\nThese features make it an ideal solution for delivering data, e.g.\nin an underground mine environment, especially clear and easily understandable\nvoice messages. This paper describes a subjective quality evaluation\nof such a system. The solution was designed and tested in real-time\noperating conditions. It consists of a one-way transmission system,\ndedicated to delivering speech signals and voice commands. The study\ninvolved signal samples in three languages: English, German, and Polish,\nprocessed at different bitrates: 8, 16, and 24 kbps. Obtained results\nconfirmed the usefulness of BPL-PLC technology for speech transmission\npurposes. Even in a narrowband scenario, with bitrates smaller than\n1 Mbps, it proved to be a potentially life-saving communication system.\nResults of this study may aid researchers and parties from the mining\nand oil industry, as well as professionals involved in rescue operations.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1077",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "chiu20_interspeech": {
      "authors": [
        [
          "Waito",
          "Chiu"
        ],
        [
          "Yan",
          "Xu"
        ],
        [
          "Andrew",
          "Abel"
        ],
        [
          "Chun",
          "Lin"
        ],
        [
          "Zhengzheng",
          "Tu"
        ]
      ],
      "title": "Investigating the Visual Lombard Effect with Gabor Based Features",
      "original": "1291",
      "page_count": 5,
      "order": 938,
      "p1": "4606",
      "pn": "4610",
      "abstract": [
        "The Lombard Effect shows that speakers increase their vocal effort\nin the presence of noise, and research into acoustic speech, has demonstrated\nvarying effects, depending on the noise level and speaker, with several\ndifferences, including timing and vocal effort. Research also identified\nseveral differences, including between gender, and noise type. However,\nmost research has focused on the audio domain, with very limited focus\non the visual effect. This paper presents a detailed study of the visual\nLombard Effect, using a pilot Lombard Speech corpus developed for our\nneeds, and a recently developed Gabor based lip feature extraction\napproach. Using Kernel Density Estimation, we identify clear differences\nbetween genders, and also show that speakers handle different noise\ntypes differently.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1291",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "huang20h_interspeech": {
      "authors": [
        [
          "Qiang",
          "Huang"
        ],
        [
          "Thomas",
          "Hain"
        ]
      ],
      "title": "Exploration of Audio Quality Assessment and Anomaly Localisation Using Attention Models",
      "original": "1885",
      "page_count": 5,
      "order": 939,
      "p1": "4611",
      "pn": "4615",
      "abstract": [
        "Many applications of speech technology require more and more audio\ndata. Automatic assessment of the quality of the collected recordings\nis important to ensure they meet the requirements of the related applications.\nHowever, effective and high performing assessment remains a challenging\ntask without a clean reference. In this paper, a novel model for audio\nquality assessment is proposed by jointly using bidirectional long\nshort-term memory and an attention mechanism. The former is to mimic\na human auditory perception ability to learn information from a recording,\nand the latter is to further discriminate interferences from desired\nsignals by highlighting target related features. To evaluate our proposed\napproach, the TIMIT dataset is used and augmented by mixing with various\nnatural sounds. In our experiments, two tasks are explored. The first\ntask is to predict an utterance quality score, and the second is to\nidentify where an anomalous distortion takes place in a recording.\nThe obtained results show that the use of our proposed approach outperforms\na strong baseline method and gains about 5% improvements after being\nmeasured by three metrics, Linear Correlation Coefficient and Spearman&#8217;s\nRank Correlation Coefficient, and F1.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1885",
      "author_area_id": "1",
      "author_area_label": "Speech Perception, Production and Acquisition"
    },
    "ragano20_interspeech": {
      "authors": [
        [
          "Alessandro",
          "Ragano"
        ],
        [
          "Emmanouil",
          "Benetos"
        ],
        [
          "Andrew",
          "Hines"
        ]
      ],
      "title": "Development of a Speech Quality Database Under Uncontrolled Conditions",
      "original": "1899",
      "page_count": 5,
      "order": 940,
      "p1": "4616",
      "pn": "4620",
      "abstract": [
        "Objective audio quality assessment is preferred to avoid time-consuming\nand costly listening tests. The development of objective quality metrics\ndepends on the availability of datasets appropriate to the application\nunder study. Currently, a suitable human-annotated dataset for developing\nquality metrics in archive audio is missing. Given the online availability\nof archival recordings, we propose to develop a real-world audio quality\ndataset. We present a methodology used to curate a speech quality database\nusing the archive recordings from the Apollo Space Program. The proposed\nprocedure is based on two steps: a pilot listening test and an exploratory\ndata analysis. The pilot listening test shows that we can extract audio\nclips through the control of speech-to-text performance metrics to\nprevent data repetition. Through unsupervised exploratory data analysis,\nwe explore the characteristics of the degradations. We classify distinct\ndegradations and we study spectral, intensity, tonality and overall\nquality properties of the data through clustering techniques. These\nresults provide the necessary foundation to support the subsequent\ndevelopment of large-scale crowdsourced datasets for audio quality.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1899",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "algayres20_interspeech": {
      "authors": [
        [
          "Robin",
          "Algayres"
        ],
        [
          "Mohamed Salah",
          "Zaiem"
        ],
        [
          "Beno\u00eet",
          "Sagot"
        ],
        [
          "Emmanuel",
          "Dupoux"
        ]
      ],
      "title": "Evaluating the Reliability of Acoustic Speech Embeddings",
      "original": "2362",
      "page_count": 5,
      "order": 941,
      "p1": "4621",
      "pn": "4625",
      "abstract": [
        "Speech embeddings are fixed-size acoustic representations of variable-length\nspeech sequences. They are increasingly used for a variety of tasks\nranging from information retrieval to unsupervised term discovery and\nspeech segmentation. However, there is currently no clear methodology\nto compare or optimize the quality of these embeddings in a task-neutral\nway. Here, we systematically compare two popular metrics, ABX discrimination\nand Mean Average Precision (MAP), on 5 languages across 17 embedding\nmethods, ranging from supervised to fully unsupervised, and using different\nloss functions (autoencoders, correspondance autoencoders, siamese).\nThen we use the ABX and MAP to predict performances on a new downstream\ntask: the unsupervised estimation of the frequencies of speech segments\nin a given corpus. We find that overall, ABX and MAP correlate with\none another and with frequency estimation. However, substantial discrepancies\nappear in the fine-grained distinctions across languages and/or embedding\nmethods. This makes it unrealistic at present to propose a task-independent\nsilver bullet method for computing the intrinsic quality of speech\nembeddings. There is a need for more detailed analysis of the metrics\ncurrently used to evaluate such embeddings.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2362"
    },
    "li20la_interspeech": {
      "authors": [
        [
          "Hao",
          "Li"
        ],
        [
          "DeLiang",
          "Wang"
        ],
        [
          "Xueliang",
          "Zhang"
        ],
        [
          "Guanglai",
          "Gao"
        ]
      ],
      "title": "Frame-Level Signal-to-Noise Ratio Estimation Using Deep Learning",
      "original": "2475",
      "page_count": 5,
      "order": 942,
      "p1": "4626",
      "pn": "4630",
      "abstract": [
        "This study investigates deep learning based signal-to-noise ratio (SNR)\nestimation at the frame level. We propose to employ recurrent neural\nnetworks (RNNs) with long short-term memory (LSTM) in order to leverage\ncontextual information for this task. As acoustic features are important\nfor deep learning algorithms, we also examine a variety of monaural\nfeatures and investigate feature combinations using Group Lasso and\nsequential floating forward selection. By replacing LSTM with bidirectional\nLSTM, the proposed algorithm naturally leads to a long-term SNR estimator.\nSystematical evaluations demonstrate that the proposed SNR estimators\nsignificantly outperform other frame-level and long-term SNR estimators.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2475",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "dong20_interspeech": {
      "authors": [
        [
          "Xuan",
          "Dong"
        ],
        [
          "Donald S.",
          "Williamson"
        ]
      ],
      "title": "A Pyramid Recurrent Network for Predicting Crowdsourced Speech-Quality Ratings of Real-World Signals",
      "original": "2809",
      "page_count": 5,
      "order": 943,
      "p1": "4631",
      "pn": "4635",
      "abstract": [
        "The real-world capabilities of objective speech quality measures are\nlimited since current measures (1) are developed from simulated data\nthat does not adequately model real environments; or they (2) predict\nobjective scores that are not always strongly correlated with subjective\nratings. Additionally, a large dataset of real-world signals with listener\nquality ratings does not currently exist, which would help facilitate\nreal-world assessment. In this paper, we collect and predict the perceptual\nquality of real-world speech signals that are evaluated by human listeners.\nWe first collect a large quality rating dataset by conducting crowdsourced\nlistening studies on two real-world corpora. We further develop a novel\napproach that predicts human quality ratings using a pyramid bidirectional\nlong short term memory (pBLSTM) network with an attention mechanism.\nThe results show that the proposed model achieves statistically lower\nestimation errors than prior assessment approaches, where the predicted\nscores strongly correlate with human judgments.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2809",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "brueggeman20_interspeech": {
      "authors": [
        [
          "Avamarie",
          "Brueggeman"
        ],
        [
          "John H.L.",
          "Hansen"
        ]
      ],
      "title": "Effect of Spectral Complexity Reduction and Number of Instruments on Musical Enjoyment with Cochlear Implants",
      "original": "3034",
      "page_count": 5,
      "order": 944,
      "p1": "4636",
      "pn": "4640",
      "abstract": [
        "Although speech recognition technology for cochlear implants has continued\nto improve, music accessibility remains a challenge. Previous studies\nhave shown that cochlear implant users may prefer listening to music\nthat has been reengineered to be less complex. In this paper, we consider\nthe combined effect of spectral complexity reduction and number of\ninstruments playing on musical enjoyment with cochlear implants. Nine\nnormal hearing listeners rated 200 10-second music samples on three\nenjoyment modalities (musicality, pleasantness, and naturalness) with\nand without the use of cochlear implant simulation. The music samples\nincluded 20 versions of the song &#8220;Twinkle Twinkle Little Star&#8221;\nsynthesized using one of five different instruments and with one to\nfour instruments playing at once. The remaining 180 versions were created\nby reducing each sample&#8217;s spectral complexity to nine different\nlevels using principal component analysis. The results showed a preference\nfor less amounts of spectral complexity reduction for samples without\ncochlear implant simulation (P&#60;.001), as well as a preference for\nfewer instruments for samples with cochlear implant simulation (P&#60;.001).\nHowever, spectral complexity reduction was not a significant factor\nfor samples with cochlear implant simulation, and a significant interaction\neffect between spectral complexity reduction and number of instruments\nwas not found.\n"
      ],
      "doi": "10.21437/Interspeech.2020-3034",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "kosmider20_interspeech": {
      "authors": [
        [
          "Micha\u0142",
          "Ko\u015bmider"
        ]
      ],
      "title": "Spectrum Correction: Acoustic Scene Classification with Mismatched Recording Devices",
      "original": "3088",
      "page_count": 5,
      "order": 945,
      "p1": "4641",
      "pn": "4645",
      "abstract": [
        "Machine learning algorithms, when trained on audio recordings from\na limited set of devices, may not generalize well to samples recorded\nusing other devices with different frequency responses. In this work,\na relatively straightforward method is introduced to address this problem.\nTwo variants of the approach are presented. First requires aligned\nexamples from multiple devices, the second approach alleviates this\nrequirement. This method works for both time and frequency domain representations\nof audio recordings. Further, a relation to standardization and Cepstral\nMean Subtraction is analysed. The proposed approach becomes effective\neven when very few examples are provided. This method was developed\nduring the  Detection and Classification of Acoustic Scenes and Events\n(DCASE) 2019 challenge and won the 1st place in the scenario with mismatched\nrecording devices with the accuracy of 75%. Source code for the experiments\ncan be found online.\n"
      ],
      "doi": "10.21437/Interspeech.2020-3088",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "oconnor20_interspeech": {
      "authors": [
        [
          "Matt",
          "O\u2019Connor"
        ],
        [
          "W. Bastiaan",
          "Kleijn"
        ]
      ],
      "title": "Distributed Summation Privacy for Speech Enhancement",
      "original": "1977",
      "page_count": 5,
      "order": 946,
      "p1": "4646",
      "pn": "4650",
      "abstract": [
        "Speech privacy in modern sensor network environments is necessary for\nwidespread adoption and public trust of collaborative acoustic signal\nprocessing. Most current distributed privacy research deals with ensuring\nlocal node observations are not accessible by neighbouring nodes while\nstill solving shared tasks. In this work we develop the concept of\ndistributed task privacy in unbounded public networks, where linear\ncodes are used to create limits on the number of nodes contributing\nto a distributed summation task, such as beamforming. We accomplish\nthis by wrapping local observations in a linear code and intentionally\napplying symbol errors prior to transmission. If many nodes join a\ndistributed speech enhancement task, a proportional number of symbol\nerrors are introduced into the aggregated code leading to decoding\nfailure if the code&#8217;s predefined symbol error limit is exceeded.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1977",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "leschanowsky20_interspeech": {
      "authors": [
        [
          "Anna",
          "Leschanowsky"
        ],
        [
          "Sneha",
          "Das"
        ],
        [
          "Tom",
          "B\u00e4ckstr\u00f6m"
        ],
        [
          "Pablo P\u00e9rez",
          "Zarazaga"
        ]
      ],
      "title": "Perception of Privacy Measured in the Crowd &#8212; Paired Comparison on the Effect of Background Noises",
      "original": "2299",
      "page_count": 5,
      "order": 947,
      "p1": "4651",
      "pn": "4655",
      "abstract": [
        "Voice based devices and virtual assistants are widely integrated into\nour daily life, but the growing popularity has also raised concerns\nabout data privacy in processing and storage. While improvements in\ntechnology and data protection regulations have been made to provide\nusers a more secure experience, the concept of privacy continues to\nbe subject to enormous challenges. We can observe that people intuitively\nadjust their way of talking in a human-to-human conversation, an intuition\nthat devices could benefit from to increase their level of privacy.\nIn order to enable devices to quantify privacy in an acoustic scenario,\nthis paper focuses on how people perceive privacy with respect to environmental\nnoise. We measured privacy scores on a crowdsourcing platform with\na paired comparison listening test and obtained reliable and consistent\nresults. Our measurements show that the experience of privacy varies\ndepending on the acoustic features of the ambient noise. Furthermore,\nmultiple probabilistic choice models were fitted to the data to obtain\na meaningful ordering of noise scenarios conveying listeners&#8217;\npreferences. A preference tree model was found to fit best, indicating\nthat subjects change their decision strategy depending on the scenarios\nunder test.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2299",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "kreuk20b_interspeech": {
      "authors": [
        [
          "Felix",
          "Kreuk"
        ],
        [
          "Yossi",
          "Adi"
        ],
        [
          "Bhiksha",
          "Raj"
        ],
        [
          "Rita",
          "Singh"
        ],
        [
          "Joseph",
          "Keshet"
        ]
      ],
      "title": "Hide and Speak: Towards Deep Neural Networks for Speech Steganography",
      "original": "2380",
      "page_count": 5,
      "order": 948,
      "p1": "4656",
      "pn": "4660",
      "abstract": [
        "Steganography is the science of hiding a secret message within an ordinary\npublic message, which is referred to as Carrier. Traditionally, digital\nsignal processing techniques, such as least significant bit encoding,\nwere used for hiding messages. In this paper, we explore the use of\ndeep neural networks as steganographic functions for speech data. We\nshowed that steganography models proposed for vision are less suitable\nfor speech, and propose a new model that includes the short-time Fourier\ntransform and inverse-short-time Fourier transform as differentiable\nlayers within the network, thus imposing a vital constraint on the\nnetwork outputs. We empirically demonstrated the effectiveness of the\nproposed method comparing to deep learning based on several speech\ndatasets and analyzed the results quantitatively and qualitatively.\nMoreover, we showed that the proposed approach could be applied to\nconceal multiple messages in a single carrier using multiple decoders\nor a single conditional decoder. Lastly, we evaluated our model under\ndifferent channel distortions. Qualitative experiments suggest that\nmodifications to the carrier are unnoticeable by human listeners and\nthat the decoded messages are highly intelligible.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2380",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "daubener20_interspeech": {
      "authors": [
        [
          "Sina",
          "D\u00e4ubener"
        ],
        [
          "Lea",
          "Sch\u00f6nherr"
        ],
        [
          "Asja",
          "Fischer"
        ],
        [
          "Dorothea",
          "Kolossa"
        ]
      ],
      "title": "Detecting Adversarial Examples for Speech Recognition via Uncertainty Quantification",
      "original": "2734",
      "page_count": 5,
      "order": 949,
      "p1": "4661",
      "pn": "4665",
      "abstract": [
        "Machine learning systems and also, specifically, automatic speech recognition\n(ASR) systems are vulnerable against adversarial attacks, where an\nattacker maliciously changes the input. In the case of ASR systems,\nthe most interesting cases are  targeted attacks, in which an attacker\naims to force the system into recognizing given target transcriptions\nin an arbitrary audio sample. The increasing number of sophisticated,\nquasi imperceptible attacks raises the question of countermeasures.<br\nstyle=\"mso-data-placement:same-cell;\" /><br style=\"mso-data-placement:same-cell;\"\n/>\n  In this paper, we focus on hybrid ASR systems and compare four\nacoustic models regarding their ability to indicate uncertainty under\nattack: a feed-forward neural network and three neural networks specifically\ndesigned for uncertainty quantification, namely a Bayesian neural network,\nMonte Carlo dropout, and a deep ensemble.<br style=\"mso-data-placement:same-cell;\"\n/><br style=\"mso-data-placement:same-cell;\" />\n  We employ uncertainty\nmeasures of the acoustic model to construct a simple one-class classification\nmodel for assessing whether inputs are benign or adversarial. Based\non this approach, we are able to detect adversarial examples with an\narea under the receiving operator curve score of more than 0.99. The\nneural networks for uncertainty quantification simultaneously diminish\nthe vulnerability to the attack, which is reflected in a lower recognition\naccuracy of the malicious target text in comparison to a standard hybrid\nASR system.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2734",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "adelani20_interspeech": {
      "authors": [
        [
          "David Ifeoluwa",
          "Adelani"
        ],
        [
          "Ali",
          "Davody"
        ],
        [
          "Thomas",
          "Kleinbauer"
        ],
        [
          "Dietrich",
          "Klakow"
        ]
      ],
      "title": "Privacy Guarantees for De-Identifying Text Transformations",
      "original": "2208",
      "page_count": 5,
      "order": 950,
      "p1": "4666",
      "pn": "4670",
      "abstract": [
        "Machine Learning approaches to Natural Language Processing tasks benefit\nfrom a comprehensive collection of real-life user data. At the same\ntime, there is a clear need for protecting the privacy of the users\nwhose data is collected and processed. For text collections, such as,\ne.g., transcripts of voice interactions or patient records, replacing\nsensitive parts with benign alternatives can provide de-identification.\nHowever, how much privacy is actually guaranteed by such text transformations,\nand are the resulting texts still useful for machine learning?<br style=\"mso-data-placement:same-cell;\"\n/><br style=\"mso-data-placement:same-cell;\" />\n  In this paper, we\nderive formal privacy guarantees for general text transformation-based\nde-identification methods on the basis of  Differential Privacy.<br\nstyle=\"mso-data-placement:same-cell;\" /><br style=\"mso-data-placement:same-cell;\"\n/>\n  We also measure the effect that different ways of masking private\ninformation in dialog transcripts have on a subsequent machine learning\ntask. To this end, we formulate different masking strategies and compare\ntheir privacy-utility trade-offs. In particular, we compare a simple\n redact approach with more sophisticated  word-by-word replacement\nusing deep learning models on multiple natural language understanding\ntasks like named entity recognition, intent detection, and dialog act\nclassification. We find that only word-by-word replacement is robust\nagainst performance drops in various tasks.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2208",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "jayashankar20_interspeech": {
      "authors": [
        [
          "Tejas",
          "Jayashankar"
        ],
        [
          "Jonathan Le",
          "Roux"
        ],
        [
          "Pierre",
          "Moulin"
        ]
      ],
      "title": "Detecting Audio Attacks on ASR Systems with Dropout Uncertainty",
      "original": "1846",
      "page_count": 5,
      "order": 951,
      "p1": "4671",
      "pn": "4675",
      "abstract": [
        "Various adversarial audio attacks have recently been developed to fool\nautomatic speech recognition (ASR) systems. We here propose a defense\nagainst such attacks based on the uncertainty introduced by dropout\nin neural networks. We show that our defense is able to detect attacks\ncreated through optimized perturbations and frequency masking on a\nstate-of-the-art end-to-end ASR system. Furthermore, the defense can\nbe made robust against attacks that are immune to noise reduction.\nWe test our defense on Mozilla&#8217;s CommonVoice dataset, the UrbanSound\ndataset, and an excerpt of the LibriSpeech dataset, showing that it\nachieves high detection accuracy in a wide range of scenarios.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1846",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "huang20i_interspeech": {
      "authors": [
        [
          "Wen-Chin",
          "Huang"
        ],
        [
          "Tomoki",
          "Hayashi"
        ],
        [
          "Yi-Chiao",
          "Wu"
        ],
        [
          "Hirokazu",
          "Kameoka"
        ],
        [
          "Tomoki",
          "Toda"
        ]
      ],
      "title": "Voice Transformer Network: Sequence-to-Sequence Voice Conversion Using Transformer with Text-to-Speech Pretraining",
      "original": "1066",
      "page_count": 5,
      "order": 952,
      "p1": "4676",
      "pn": "4680",
      "abstract": [
        "We introduce a novel sequence-to-sequence (seq2seq) voice conversion\n(VC) model based on the Transformer architecture with text-to-speech\n(TTS) pretraining. Seq2seq VC models are attractive owing to their\nability to convert prosody. While seq2seq models based on recurrent\nneural networks (RNNs) and convolutional neural networks (CNNs) have\nbeen successfully applied to VC, the use of the Transformer network,\nwhich has shown promising results in various speech processing tasks,\nhas not yet been investigated. Nonetheless, their data-hungry property\nand the mispronunciation of converted speech make seq2seq models far\nfrom practical. To this end, we propose a simple yet effective pretraining\ntechnique to transfer knowledge from learned TTS models, which benefit\nfrom large-scale, easily accessible TTS corpora. VC models initialized\nwith such pretrained model parameters are able to generate effective\nhidden representations for high-fidelity, highly intelligible converted\nspeech. Experimental results show that such a pretraining scheme can\nfacilitate data-efficient training and outperform an RNN-based seq2seq\nVC model in terms of intelligibility, naturalness, and similarity.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1066",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "suda20_interspeech": {
      "authors": [
        [
          "Hitoshi",
          "Suda"
        ],
        [
          "Gaku",
          "Kotani"
        ],
        [
          "Daisuke",
          "Saito"
        ]
      ],
      "title": "Nonparallel Training of Exemplar-Based Voice Conversion System Using INCA-Based Alignment Technique",
      "original": "1232",
      "page_count": 5,
      "order": 953,
      "p1": "4681",
      "pn": "4685",
      "abstract": [
        "This paper proposes a new voice conversion (VC) framework, which can\nbe trained with nonparallel corpora, using non-negative matrix factorization\n(NMF). While nonparallel VC frameworks have already been studied widely,\nthe conventional frameworks require huge background knowledge or plenty\nof training utterances. This is because of difficulty in disentanglement\nof linguistic and speaker information without a large amount of data.\nThis work tackles the problem by utilizing NMF, which can factorize\nacoustic features into time-variant and time-invariant components in\nan unsupervised manner. To preserve linguistic consistency between\nsource and target speakers, the proposed method performs soft alignment\nbetween the acoustic features of the source speaker and the exemplars\nof the target speaker. The method adopts the alignment technique of\nINCA algorithm, which is an iterative method to obtain alignment of\nnonparallel corpora. The results of subjective experiments showed that\nthe proposed framework outperformed not only the NMF-based parallel\nVC framework but also the CycleGAN-based nonparallel VC framework.\nThe results also showed that the proposed method achieved high-quality\nconversion even if the number of training utterances for the source\nspeaker was extremely limited.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1232",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "chen20s_interspeech": {
      "authors": [
        [
          "Chen-Yu",
          "Chen"
        ],
        [
          "Wei-Zhong",
          "Zheng"
        ],
        [
          "Syu-Siang",
          "Wang"
        ],
        [
          "Yu",
          "Tsao"
        ],
        [
          "Pei-Chun",
          "Li"
        ],
        [
          "Ying-Hui",
          "Lai"
        ]
      ],
      "title": "Enhancing Intelligibility of Dysarthric Speech Using Gated Convolutional-Based Voice Conversion System",
      "original": "1367",
      "page_count": 5,
      "order": 954,
      "p1": "4686",
      "pn": "4690",
      "abstract": [
        "The voice conversion (VC) system is a well-known approach to improve\nthe communication efficiency of patients with dysarthria. In this study,\nwe used a gated convolutional neural network (Gated CNN) with the phonetic\nposteriorgrams (PPGs) features to perform VC for patients with dysarthria,\nwith WaveRNN vocoder used to synthesis converted speech. In addition,\ntwo well-known deep learning-based models, convolution neural network\n(CNN) and bidirectional long short-term memory (BLSTM) were used to\ncompare with the Gated CNN in the proposed VC system. The results from\nthe evaluation of speech intelligibility metric of Google ASR and listening\ntest showed that the proposed system performed better than the original\ndysarthric speech. Meanwhile, the Gated CNN model performs better than\nthe other models and requires fewer parameters compared to BLSTM. The\nresults suggested that Gated CNN can be used as a communication assistive\nsystem to overcome the degradation of speech intelligibility caused\nby dysarthria.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1367",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "wu20p_interspeech": {
      "authors": [
        [
          "Da-Yi",
          "Wu"
        ],
        [
          "Yen-Hao",
          "Chen"
        ],
        [
          "Hung-yi",
          "Lee"
        ]
      ],
      "title": "VQVC+: One-Shot Voice Conversion by Vector Quantization and U-Net Architecture",
      "original": "1443",
      "page_count": 5,
      "order": 955,
      "p1": "4691",
      "pn": "4695",
      "abstract": [
        "Voice conversion (VC) is a task that transforms the source speaker&#8217;s\ntimbre, accent, and tones in audio into another one&#8217;s while preserving\nthe linguistic content. It is still a challenging work, especially\nin a one-shot setting. Auto-encoder-based VC methods disentangle the\nspeaker and the content in input speech without explicit information\nabout the speaker&#8217;s identity, so these methods can further generalize\nto unseen speakers. The disentangle capability is achieved by vector\nquantization (VQ), adversarial training, or instance normalization\n(IN). However, the imperfect disentanglement may harm the quality of\noutput speech. In this work, to further improve audio quality, we use\nthe U-Net architecture within an auto-encoder-based VC system. We find\nthat to leverage the U-Net architecture, a strong information bottleneck\nis necessary. The VQ-based method, which quantizes the latent vectors,\ncan serve the purpose. The objective and the subjective evaluations\nshow that the proposed method performs well in both audio naturalness\nand speaker similarity.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1443",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "park20e_interspeech": {
      "authors": [
        [
          "Seung-won",
          "Park"
        ],
        [
          "Doo-young",
          "Kim"
        ],
        [
          "Myun-chul",
          "Joe"
        ]
      ],
      "title": "Cotatron: Transcription-Guided Speech Encoder for Any-to-Many Voice Conversion Without Parallel Data",
      "original": "1542",
      "page_count": 5,
      "order": 956,
      "p1": "4696",
      "pn": "4700",
      "abstract": [
        "We propose  Cotatron, a transcription-guided speech encoder for speaker-independent\nlinguistic representation. Cotatron is based on the multispeaker TTS\narchitecture and can be trained with conventional TTS datasets. We\ntrain a voice conversion system to reconstruct speech with Cotatron\nfeatures, which is similar to the previous methods based on Phonetic\nPosteriorgram (PPG). By training and evaluating our system with 108\nspeakers from the VCTK dataset, we outperform the previous method in\nterms of both naturalness and speaker similarity. Our system can also\nconvert speech from speakers that are unseen during training, and utilize\nASR to automate the transcription with minimal reduction of the performance.\nAudio samples are available at <KBD>https://mindslab-ai.github.io/cotatron</KBD>,\nand the code with a pre-trained model will be made available soon.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1542",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "fu20c_interspeech": {
      "authors": [
        [
          "Ruibo",
          "Fu"
        ],
        [
          "Jianhua",
          "Tao"
        ],
        [
          "Zhengqi",
          "Wen"
        ],
        [
          "Jiangyan",
          "Yi"
        ],
        [
          "Tao",
          "Wang"
        ],
        [
          "Chunyu",
          "Qiang"
        ]
      ],
      "title": "Dynamic Speaker Representations Adjustment and Decoder Factorization for Speaker Adaptation in End-to-End Speech Synthesis",
      "original": "1623",
      "page_count": 5,
      "order": 957,
      "p1": "4701",
      "pn": "4705",
      "abstract": [
        "End-to-end speech synthesis can reach high quality and naturalness\nwith low-resource adaptation data. However, the generalization of out-domain\ntexts and the improving modeling accuracy of speaker representations\nare still challenging tasks. The limited adaptation data leads to unacceptable\nerrors and low similarity of the synthetic speech. In this paper, both\nspeaker representations modeling and acoustic model structure are improved\nfor the speaker adaptation task. On the one hand, compared with the\nconventional methods that focused on using fixed global speaker representations,\nthe attention gating is proposed to adjust speaker representations\ndynamically based on the attended context and prosody information,\nwhich can describe more pronunciation characteristics in phoneme level.\nOn the other hand, to improve the robustness and avoid over-fitting,\nthe decoder model is factored into average-net and adaptation-net,\nwhich are designed for learning speaker independent acoustic features\nand target speaker timbre imitation respectively. And the context discriminator\nis pre-trained by large ASR data to supervise the average-net generating\nproper speaker independent acoustic features for different phoneme.\nExperimental results on Mandarin dataset show that proposed methods\nlead to an improvement on intelligibility, naturalness and similarity.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1623",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "lian20d_interspeech": {
      "authors": [
        [
          "Zheng",
          "Lian"
        ],
        [
          "Zhengqi",
          "Wen"
        ],
        [
          "Xinyong",
          "Zhou"
        ],
        [
          "Songbai",
          "Pu"
        ],
        [
          "Shengkai",
          "Zhang"
        ],
        [
          "Jianhua",
          "Tao"
        ]
      ],
      "title": "ARVC: An Auto-Regressive Voice Conversion System Without Parallel Training Data",
      "original": "1715",
      "page_count": 5,
      "order": 958,
      "p1": "4706",
      "pn": "4710",
      "abstract": [
        "Voice conversion (VC) is to convert the source speaker&#8217;s voice\nto sound like that of the target speaker without changing the linguistic\ncontent. Recent work shows that phonetic posteriorgrams (PPGs) based\nVC frameworks have achieved promising results in speaker similarity\nand speech quality. However, in practice, we find that the trajectory\nof some generated waveforms is not smooth, thus causing some voice\nerror problems and degrading the sound quality of the converted speech.\nIn this paper, we propose to advance the existing PPGs based voice\nconversion methods to achieve better performance. Specifically, we\npropose a new auto-regressive model for any-to-one VC, called Auto-Regressive\nVoice Conversion (ARVC). Compared with conventional PPGs based VC,\nARVC takes previous step acoustic features as the inputs to produce\nthe next step outputs via the auto-regressive structure. Experimental\nresults on the CMU-ARCTIC dataset show that our method can improve\nthe speech quality and speaker similarity of the converted speech.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1715",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "nercessian20_interspeech": {
      "authors": [
        [
          "Shahan",
          "Nercessian"
        ]
      ],
      "title": "Improved Zero-Shot Voice Conversion Using Explicit Conditioning Signals",
      "original": "1889",
      "page_count": 5,
      "order": 959,
      "p1": "4711",
      "pn": "4715",
      "abstract": [
        "In this paper, we propose a zero-shot voice conversion algorithm adding\na number of conditioning signals to explicitly transfer prosody, linguistic\ncontent, and dynamics to conversion results. We show that the proposed\napproach improves overall conversion quality and generalization to\nout-of-domain samples relative to a baseline implementation of AutoVC,\nas the inclusion of conditioning signals can help reduce the burden\non the model&#8217;s encoder to implicitly learn all of the different\naspects involved in speech production. An ablation analysis illustrates\nthe effectiveness of the proposed method.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1889",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "chen20t_interspeech": {
      "authors": [
        [
          "Minchuan",
          "Chen"
        ],
        [
          "Weijian",
          "Hou"
        ],
        [
          "Jun",
          "Ma"
        ],
        [
          "Shaojun",
          "Wang"
        ],
        [
          "Jing",
          "Xiao"
        ]
      ],
      "title": "Non-Parallel Voice Conversion with Fewer Labeled Data by Conditional Generative Adversarial Networks",
      "original": "2162",
      "page_count": 5,
      "order": 960,
      "p1": "4716",
      "pn": "4720",
      "abstract": [
        "Recent studies have shown remarkable success in voice conversion (VC)\nbased on generative adversarial networks (GANs) without parallel data.\nIn this paper, based on the conditional generative adversarial networks\n(CGANs), we propose a self- and semi-supervised method combined with\nmixup and data augmentation that allows non-parallel many-to-many voice\nconversion with fewer labeled data. In this method, the discriminator\nof CGANs learns to not only distinguish real/fake samples, but also\nclassify attribute domains. We augment the discriminator with an auxiliary\ntask to improve representation learning and introduce a training task\nto predict labels for the unlabeled samples. The proposed approach\nreduces the appetite for labeled data in voice conversion, which enables\nsingle generative network to implement many-to-many mapping between\ndifferent voice domains. Experiment results show that the proposed\nmethod is able to achieve comparable voice quality and speaker similarity\nwith only 10% of the labeled data.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2162",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "liu20v_interspeech": {
      "authors": [
        [
          "Songxiang",
          "Liu"
        ],
        [
          "Yuewen",
          "Cao"
        ],
        [
          "Shiyin",
          "Kang"
        ],
        [
          "Na",
          "Hu"
        ],
        [
          "Xunying",
          "Liu"
        ],
        [
          "Dan",
          "Su"
        ],
        [
          "Dong",
          "Yu"
        ],
        [
          "Helen",
          "Meng"
        ]
      ],
      "title": "Transferring Source Style in Non-Parallel Voice Conversion",
      "original": "2412",
      "page_count": 5,
      "order": 961,
      "p1": "4721",
      "pn": "4725",
      "abstract": [
        "Voice conversion (VC) techniques aim to modify speaker identity of\nan utterance while preserving the underlying linguistic information.\nMost VC approaches ignore modeling of the speaking style (e.g. emotion\nand emphasis), which may contain the factors intentionally added by\nthe speaker and should be retained during conversion. This study proposes\na sequence-to-sequence based non-parallel VC approach, which has the\ncapability of transferring the speaking style from the source speech\nto the converted speech by explicitly modeling. Objective evaluation\nand subjective listening tests show superiority of the proposed VC\napproach in terms of speech naturalness and speaker similarity of the\nconverted speech. Experiments are also conducted to show the source-style\ntransferability of the proposed approach.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2412",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "albadawy20_interspeech": {
      "authors": [
        [
          "Ehab A.",
          "AlBadawy"
        ],
        [
          "Siwei",
          "Lyu"
        ]
      ],
      "title": "Voice Conversion Using Speech-to-Speech Neuro-Style Transfer",
      "original": "3056",
      "page_count": 5,
      "order": 962,
      "p1": "4726",
      "pn": "4730",
      "abstract": [
        "An impressionist is the one who tries to mimic other people&#8217;s\nvoices and their style of speech. Humans have mastered such a task\nthroughout the years. In this work, we introduce a deep learning-based\napproach to do voice conversion with speech style transfer across different\nspeakers. In our work, we use a combination of Variational Auto-Encoder\n(VAE) and Generative Adversarial Network (GAN) as the main components\nof our proposed model followed by a WaveNet-based vocoder. We use three\nobjective metrics to evaluate our model using the ASVspoof 2019 for\nmeasuring the difficulty of differentiating between human and synthesized\nsamples, content verification for transcription accuracy, and speaker\nencoding for identity verification. Our results show the efficacy of\nour proposed model in producing a high quality synthesized speech on\nFlickr8k audio corpus.\n"
      ],
      "doi": "10.21437/Interspeech.2020-3056",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "wang20ia_interspeech": {
      "authors": [
        [
          "Changhan",
          "Wang"
        ],
        [
          "Juan",
          "Pino"
        ],
        [
          "Jiatao",
          "Gu"
        ]
      ],
      "title": "Improving Cross-Lingual Transfer Learning for End-to-End Speech Recognition with Speech Translation",
      "original": "2955",
      "page_count": 5,
      "order": 963,
      "p1": "4731",
      "pn": "4735",
      "abstract": [
        "Transfer learning from high-resource languages is known to be an efficient\nway to improve end-to-end automatic speech recognition (ASR) for low-resource\nlanguages. Pre-trained or jointly trained encoder-decoder models, however,\ndo not share the language modeling (decoder) for the same language,\nwhich is likely to be inefficient for distant target languages. We\nintroduce speech-to-text translation (ST) as an auxiliary task to incorporate\nadditional knowledge of the target language and enable transferring\nfrom that target language. Specifically, we first translate high-resource\nASR transcripts into a target low-resource language, with which a ST\nmodel is trained. Both ST and target ASR share the same attention-based\nencoder-decoder architecture and vocabulary. The former task then provides\na fully pre-trained model for the latter, bringing up to 24.6% word\nerror rate (WER) reduction to the baseline (direct transfer from high-resource\nASR). We show that training ST with human translations is not necessary.\nST trained with machine translation (MT) pseudo-labels brings consistent\ngains. It can even outperform those using human labels when transferred\nto target ASR by leveraging only 500K MT examples. Even with pseudo-labels\nfrom low-resource MT (200K examples), ST-enhanced transfer brings up\nto 8.9% WER reduction to direct transfer.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2955",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "thomas20_interspeech": {
      "authors": [
        [
          "Samuel",
          "Thomas"
        ],
        [
          "Kartik",
          "Audhkhasi"
        ],
        [
          "Brian",
          "Kingsbury"
        ]
      ],
      "title": "Transliteration Based Data Augmentation for Training Multilingual ASR Acoustic Models in Low Resource Settings",
      "original": "2593",
      "page_count": 5,
      "order": 964,
      "p1": "4736",
      "pn": "4740",
      "abstract": [
        "Multilingual acoustic models are often used to build automatic speech\nrecognition (ASR) systems for low-resource languages. We propose a\nnovel data augmentation technique to improve the performance of an\nend-to-end (E2E) multilingual acoustic model by transliterating data\ninto the various languages that are part of the multilingual training\nset. Along with two metrics for data selection, this technique can\nalso improve recognition performance of the model on unsupervised and\ncross-lingual data. On a set of four low-resource languages, we show\nthat word error rates (WER) can be reduced by up to 12% and 5% relative\ncompared to monolingual and multilingual baselines respectively. We\nalso demonstrate how a multilingual network constructed within this\nframework can be extended to a new training language. With the proposed\nmethods, the new model has WER reductions of up to 24% and 13% respectively\ncompared to monolingual and multilingual baselines.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2593",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "zhu20d_interspeech": {
      "authors": [
        [
          "Yun",
          "Zhu"
        ],
        [
          "Parisa",
          "Haghani"
        ],
        [
          "Anshuman",
          "Tripathi"
        ],
        [
          "Bhuvana",
          "Ramabhadran"
        ],
        [
          "Brian",
          "Farris"
        ],
        [
          "Hainan",
          "Xu"
        ],
        [
          "Han",
          "Lu"
        ],
        [
          "Hasim",
          "Sak"
        ],
        [
          "Isabel",
          "Leal"
        ],
        [
          "Neeraj",
          "Gaur"
        ],
        [
          "Pedro J.",
          "Moreno"
        ],
        [
          "Qian",
          "Zhang"
        ]
      ],
      "title": "Multilingual Speech Recognition with Self-Attention Structured Parameterization",
      "original": "2847",
      "page_count": 5,
      "order": 965,
      "p1": "4741",
      "pn": "4745",
      "abstract": [
        "Multilingual automatic speech recognition systems can transcribe utterances\nfrom different languages. These systems are attractive from different\nperspectives: they can provide quality improvements, specially for\nlower resource languages, and simplify the training and deployment\nprocedure. End-to-end speech recognition has further simplified multilingual\nmodeling as one model, instead of several components of a classical\nsystem, have to be unified. In this paper, we investigate a streamable\nend-to-end multilingual system based on the Transformer Transducer\n[1]. We propose several techniques for adapting the self-attention\narchitecture based on the language id. We analyze the trade-offs of\neach method with regards to quality gains and number of additional\nparameters introduced. We conduct experiments in a real-world task\nconsisting of five languages. Our experimental results demonstrate\n&#126;8% to &#126;20% relative gain over the baseline multilingual\nmodel.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2847",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "madikeri20_interspeech": {
      "authors": [
        [
          "Srikanth",
          "Madikeri"
        ],
        [
          "Banriskhem K.",
          "Khonglah"
        ],
        [
          "Sibo",
          "Tong"
        ],
        [
          "Petr",
          "Motlicek"
        ],
        [
          "Herv\u00e9",
          "Bourlard"
        ],
        [
          "Daniel",
          "Povey"
        ]
      ],
      "title": "Lattice-Free Maximum Mutual Information Training of Multilingual Speech Recognition Systems",
      "original": "2919",
      "page_count": 5,
      "order": 966,
      "p1": "4746",
      "pn": "4750",
      "abstract": [
        "Multilingual acoustic model training combines data from multiple languages\nto train an automatic speech recognition system. Such a system is beneficial\nwhen training data for a target language is limited. Lattice-Free Maximum\nMutual Information (LF-MMI) training performs sequence discrimination\nby introducing competing hypotheses through a denominator graph in\nthe cost function. The standard approach to train a multilingual model\nwith LF-MMI is to combine the acoustic units from all languages and\nuse a common denominator graph. The resulting model is either used\nas a feature extractor to train an acoustic model for the target language\nor directly fine-tuned. In this work, we propose a scalable approach\nto train the multilingual acoustic model using a typical multitask\nnetwork for the LF-MMI framework. A set of language-dependent denominator\ngraphs is used to compute the cost function. The proposed approach\nis evaluated under typical multilingual ASR tasks using GlobalPhone\nand BABEL datasets. Relative improvements up to 13.2% in WER are obtained\nwhen compared to the corresponding monolingual LF-MMI baselines. The\nimplementation is made available as a part of the Kaldi speech recognition\ntoolkit.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2919",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "pratap20c_interspeech": {
      "authors": [
        [
          "Vineel",
          "Pratap"
        ],
        [
          "Anuroop",
          "Sriram"
        ],
        [
          "Paden",
          "Tomasello"
        ],
        [
          "Awni",
          "Hannun"
        ],
        [
          "Vitaliy",
          "Liptchinsky"
        ],
        [
          "Gabriel",
          "Synnaeve"
        ],
        [
          "Ronan",
          "Collobert"
        ]
      ],
      "title": "Massively Multilingual ASR: 50 Languages, 1 Model, 1 Billion Parameters",
      "original": "2831",
      "page_count": 5,
      "order": 967,
      "p1": "4751",
      "pn": "4755",
      "abstract": [
        "We study training a single acoustic model for multiple languages with\nthe aim of improving automatic speech recognition (ASR) performance\non low-resource languages, and overall simplifying deployment of ASR\nsystems that support diverse languages. We perform an extensive benchmark\non 51 languages, with varying amount of training data by language (from\n100 hours to 1100 hours). We compare three variants of multilingual\ntraining from a single joint model without knowing the input language,\nto using this information, to multiple heads (one per language &#8220;cluster&#8221;).\nWe show that multilingual training of ASR models on several languages\ncan improve recognition performance, in particular, on low resource\nlanguages. We see 20.9%, 23% and 28.8% average WER relative reduction\ncompared to monolingual baselines on joint model, joint model with\nlanguage input and multi head model respectively. To our knowledge,\nthis is the first work studying multilingual ASR at massive scale,\nwith more than 50 languages and more than 16,000 hours of audio across\nthem.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2831",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "sailor20_interspeech": {
      "authors": [
        [
          "Hardik B.",
          "Sailor"
        ],
        [
          "Thomas",
          "Hain"
        ]
      ],
      "title": "Multilingual Speech Recognition Using Language-Specific Phoneme Recognition as Auxiliary Task for Indian Languages",
      "original": "2739",
      "page_count": 5,
      "order": 968,
      "p1": "4756",
      "pn": "4760",
      "abstract": [
        "This paper proposes a multilingual acoustic modeling approach for Indian\nlanguages using a Multitask Learning (MTL) framework. Language-specific\nphoneme recognition is explored as an auxiliary task in MTL framework\nalong with the primary task of multilingual senone classification.\nThis auxiliary task regularizes the primary task with both the context-independent\nphonemes and language identities induced by language-specific phoneme.\nThe MTL network is also extended by structuring the primary and auxiliary\ntask outputs in the form of a Structured Output Layer (SOL) such that\nboth depend on each other. The experiments are performed using a database\nof the three Indian languages Gujarati, Tamil, and Telugu. The experimental\nresults show that the proposed MTL-SOL framework performed well compared\nto baseline monolingual systems with a relative reduction of 3.1&#8211;4.4\nand 2.9&#8211;4.1% in word error rate for the development and evaluation\nsets, respectively.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2739",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "chandu20_interspeech": {
      "authors": [
        [
          "Khyathi Raghavi",
          "Chandu"
        ],
        [
          "Alan W.",
          "Black"
        ]
      ],
      "title": "Style Variation as a Vantage Point for Code-Switching",
      "original": "2574",
      "page_count": 5,
      "order": 969,
      "p1": "4761",
      "pn": "4765",
      "abstract": [
        "Code-Switching (CS) is a prevalent phenomenon observed in bilingual\nand multilingual communities, especially in digital and social media\nplatforms. A major problem in this domain is the dearth of substantial\ncorpora to train large scale neural models. Generating vast amounts\nof quality synthetic text assists several downstream tasks that heavily\nrely on language modeling such as speech recognition, text-to-speech\nsynthesis etc,. We present a novel vantage point of CS to be style\nvariations between both the participating languages. Our approach does\nnot need any external dense annotations such as lexical language ids.\nIt relies on easily obtainable monolingual corpora without any parallel\nalignment and a limited set of naturally CS sentences. We propose a\ntwo-stage generative adversarial training approach where the first\nstage generates competitive negative examples for CS and the second\nstage generates more realistic CS sentences. We present our experiments\non the following pairs of languages: Spanish-English, Mandarin-English,\nHindi-English and Arabic-French. We show that the trends in metrics\nfor generated CS move closer to real CS data in the above language\npairs through the dual stage training process. We believe this viewpoint\nof CS as style variations opens new perspectives for modeling various\ntasks in CS text.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2574",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "lu20f_interspeech": {
      "authors": [
        [
          "Yizhou",
          "Lu"
        ],
        [
          "Mingkun",
          "Huang"
        ],
        [
          "Hao",
          "Li"
        ],
        [
          "Jiaqi",
          "Guo"
        ],
        [
          "Yanmin",
          "Qian"
        ]
      ],
      "title": "Bi-Encoder Transformer Network for Mandarin-English Code-Switching Speech Recognition Using Mixture of Experts",
      "original": "2485",
      "page_count": 5,
      "order": 970,
      "p1": "4766",
      "pn": "4770",
      "abstract": [
        "Code-switching speech recognition is a challenging task which has been\nstudied in many previous work, and one main challenge for this task\nis the lack of code-switching data. In this paper, we study end-to-end\nmodels for Mandarin-English code-switching automatic speech recognition.\nExternal monolingual data are utilized to alleviate the data sparsity\nproblem. More importantly, we propose a bi-encoder transformer network\nbased Mixture of Experts (MoE) architecture to better leverage these\ndata. We decouple Mandarin and English modeling with two separate encoders\nto better capture language-specific information, and a gating network\nis employed to explicitly handle the language identification task.\nFor the gating network, different models and training modes are explored\nto learn the better MoE interpolation coefficients. Experimental results\nshow that compared with the baseline transformer model, the proposed\nnew MoE architecture can obtain up to 10.4% relative error reduction\non the code-switching test set.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2485",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "sharma20c_interspeech": {
      "authors": [
        [
          "Yash",
          "Sharma"
        ],
        [
          "Basil",
          "Abraham"
        ],
        [
          "Karan",
          "Taneja"
        ],
        [
          "Preethi",
          "Jyothi"
        ]
      ],
      "title": "Improving Low Resource Code-Switched ASR Using Augmented Code-Switched TTS",
      "original": "2402",
      "page_count": 5,
      "order": 971,
      "p1": "4771",
      "pn": "4775",
      "abstract": [
        "Building Automatic Speech Recognition (ASR) systems for code-switched\nspeech has recently gained renewed attention due to the widespread\nuse of speech technologies in multilingual communities worldwide. End-to-end\nASR systems are a natural modeling choice due to their ease of use\nand superior performance in monolingual settings. However, it is well-known\nthat end-to-end systems require large amounts of labeled speech. In\nthis work, we investigate improving code-switched ASR in low resource\nsettings via data augmentation using code-switched text-to-speech (TTS)\nsynthesis. We propose two targeted techniques to effectively leverage\nTTS speech samples: 1) Mixup, an existing technique to create new training\nsamples via linear interpolation of existing samples, applied to TTS\nand real speech samples, and 2) a new loss function, used in conjunction\nwith TTS samples, to encourage code-switched predictions. We report\nsignificant improvements in ASR performance achieving absolute word\nerror rate (WER) reductions of up to 5%, and measurable improvement\nin code switching using our proposed techniques on a Hindi-English\ncode-switched ASR task.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2402",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "qiu20c_interspeech": {
      "authors": [
        [
          "Zimeng",
          "Qiu"
        ],
        [
          "Yiyuan",
          "Li"
        ],
        [
          "Xinjian",
          "Li"
        ],
        [
          "Florian",
          "Metze"
        ],
        [
          "William M.",
          "Campbell"
        ]
      ],
      "title": "Towards Context-Aware End-to-End Code-Switching Speech Recognition",
      "original": "1980",
      "page_count": 5,
      "order": 972,
      "p1": "4776",
      "pn": "4780",
      "abstract": [
        "Code-switching (CS) speech recognition is drawing increasing attention\nin recent years as it is a common situation in speech where speakers\nalternate between languages in the context of a single utterance or\ndiscourse. In this work, we propose Hierarchical Attention-based Recurrent\nDecoder (HARD) to build a context-aware end-to-end code-switching speech\nrecognition system. HARD is an attention-based decoder model which\nemploys a hierarchical recurrent network to enhance model&#8217;s awareness\nof previous generated historical sequence (sub-sequence) at decoding.\nThis architecture has two LSTMs to model encoder hidden states at both\nthe character level and sub-sequence level, therefore enables us to\ngenerate utterances that switch between languages more precisely from\nspeech. We also employ language identification (LID) as an auxiliary\ntask in multi-task learning (MTL) to boost speech recognition performance.\nWe evaluate the effectiveness of our model on the SEAME dataset, results\nshow that our multi-task learning HARD (MTL-HARD) model improves over\nthe baseline Listen, Attend and Spell (LAS) model by reducing character\nerror rate (CER) from 29.91% to 26.56% and mixed error rate (MER) from\n38.99% to 34.50%, and case study shows MTL-HARD can carry historical\ninformation in the sub-sequences.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1980",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "dinh20b_interspeech": {
      "authors": [
        [
          "Tuan",
          "Dinh"
        ],
        [
          "Alexander",
          "Kain"
        ],
        [
          "Robin",
          "Samlan"
        ],
        [
          "Beiming",
          "Cao"
        ],
        [
          "Jun",
          "Wang"
        ]
      ],
      "title": "Increasing the Intelligibility and Naturalness of Alaryngeal Speech Using Voice Conversion and Synthetic Fundamental Frequency",
      "original": "1196",
      "page_count": 5,
      "order": 973,
      "p1": "4781",
      "pn": "4785",
      "abstract": [
        "Individuals who undergo a laryngectomy lose their ability to phonate.\nYet current treatment options allow alaryngeal speech, they struggle\nin their daily communication and social life due to the low intelligibility\nof their speech. In this paper, we presented two conversion methods\nfor increasing intelligibility and naturalness of speech produced by\nlaryngectomees (LAR). The first method used a deep neural network for\npredicting binary voicing/unvoicing or the degree of aperiodicity.\nThe second method used a conditional generative adversarial network\nto learn the mapping from LAR speech spectra to clearly-articulated\nspeech spectra. We also created a synthetic fundamental frequency trajectory\nwith an intonation model consisting of phrase and accent curves. For\nthe two conversion methods, we showed that adaptation always increased\nthe performance of pre-trained models, objectively. In subjective testing\ninvolving four LAR speakers, we significantly improved the naturalness\nof two speakers, and we also significantly improved the intelligibility\nof one speaker.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1196",
      "author_area_id": "1",
      "author_area_label": "Speech Perception, Production and Acquisition"
    },
    "tong20b_interspeech": {
      "authors": [
        [
          "Han",
          "Tong"
        ],
        [
          "Hamid",
          "Sharifzadeh"
        ],
        [
          "Ian",
          "McLoughlin"
        ]
      ],
      "title": "Automatic Assessment of Dysarthric Severity Level Using Audio-Video Cross-Modal Approach in Deep Learning",
      "original": "1997",
      "page_count": 5,
      "order": 974,
      "p1": "4786",
      "pn": "4790",
      "abstract": [
        "Dysarthria is a speech disorder that can significantly impact a person&#8217;s\ndaily life, and yet may be amenable to therapy. To automatically detect\nand classify dysarthria, researchers have proposed various computational\napproaches ranging from traditional speech processing methods focusing\non speech rate, intelligibility, intonation, etc. to more advanced\nmachine learning techniques. Recently developed machine learning systems\nrely on audio features for classification; however, research in other\nfields has shown that audio-video cross-modal frameworks can improve\nclassification accuracy while simultaneously reducing the amount of\ntraining data required compared to uni-modal systems (i.e. audio- or\nvideo-only).<br style=\"mso-data-placement:same-cell;\" /><br style=\"mso-data-placement:same-cell;\"\n/>\n  In this paper, we propose an audio-video cross-modal deep learning\nframework that takes both audio and video data as input to classify\ndysarthria severity levels. Our novel cross-modal framework achieves\nover 99% test accuracy on the UASPEECH dataset &#8212; significantly\noutperforming current uni-modal systems that utilise audio data alone.\nMore importantly, it is able to accelerate training time while improving\naccuracy, and to do so with reduced training data requirements.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1997",
      "author_area_id": "1",
      "author_area_label": "Speech Perception, Production and Acquisition"
    },
    "lin20n_interspeech": {
      "authors": [
        [
          "Yuqin",
          "Lin"
        ],
        [
          "Longbiao",
          "Wang"
        ],
        [
          "Sheng",
          "Li"
        ],
        [
          "Jianwu",
          "Dang"
        ],
        [
          "Chenchen",
          "Ding"
        ]
      ],
      "title": "Staged Knowledge Distillation for End-to-End Dysarthric Speech Recognition and Speech Attribute Transcription",
      "original": "1755",
      "page_count": 5,
      "order": 975,
      "p1": "4791",
      "pn": "4795",
      "abstract": [
        "This study proposes a staged knowledge distillation method to build\nEnd-to-End (E2E) automatic speech recognition (ASR) and automatic speech\nattribute transcription (ASAT) systems for patients with dysarthria\ncaused by either cerebral palsy (CP) or amyotrophic lateral sclerosis\n(ALS). Compared with traditional methods, our proposed method can use\nlimited dysarthric speech more effectively. And the dysarthric E2E-ASR\nand ASAT systems enhanced by the proposed method can achieve 38.28%\nrelative phone error rate (PER%) reduction and 48.33% relative attribute\ndetection error rate (DER%) reduction over their baselines respectively\non the TORGO dataset. The experiments show that our system offers potential\nas a rehabilitation tool and medical diagnostic aid.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1755",
      "author_area_id": "1",
      "author_area_label": "Speech Perception, Production and Acquisition"
    },
    "takashima20_interspeech": {
      "authors": [
        [
          "Yuki",
          "Takashima"
        ],
        [
          "Ryoichi",
          "Takashima"
        ],
        [
          "Tetsuya",
          "Takiguchi"
        ],
        [
          "Yasuo",
          "Ariki"
        ]
      ],
      "title": "Dysarthric Speech Recognition Based on Deep Metric Learning",
      "original": "2267",
      "page_count": 5,
      "order": 976,
      "p1": "4796",
      "pn": "4800",
      "abstract": [
        "We present in this paper an automatic speech recognition (ASR) system\nfor a person with an articulation disorder resulting from athetoid\ncerebral palsy. Because their utterances are often unstable or unclear,\nspeech recognition systems have difficulty recognizing the speech of\nthose with this disorder. For example, their speech styles often fluctuate\ngreatly even when they are repeating the same sentences. For this reason,\ntheir speech tends to have great variation even within recognition\nclasses. To alleviate this intra-class variation problem, we propose\nan ASR system based on deep metric learning. This system learns an\nembedded representation that is characterized by a small distance between\ninput utterances of the same class, while the distance of the input\nutterances of different classes is large. Therefore, our method makes\nit easy for the ASR system to distinguish dysarthric speech. Experimental\nresults show that our proposed approach using deep metric learning\nimproves the word-recognition accuracy consistently. Moreover, we also\nevaluate the combination of our proposed method and transfer learning\nfrom unimpaired speech to alleviate the low-resource problem associated\nwith impaired speech.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2267",
      "author_area_id": "1",
      "author_area_label": "Speech Perception, Production and Acquisition"
    },
    "degala20_interspeech": {
      "authors": [
        [
          "Divya",
          "Degala"
        ],
        [
          "Achuth Rao",
          "M.V."
        ],
        [
          "Rahul",
          "Krishnamurthy"
        ],
        [
          "Pebbili",
          "Gopikishore"
        ],
        [
          "Veeramani",
          "Priyadharshini"
        ],
        [
          "Prakash",
          "T.K."
        ],
        [
          "Prasanta Kumar",
          "Ghosh"
        ]
      ],
      "title": "Automatic Glottis Detection and Segmentation in Stroboscopic Videos Using Convolutional Networks",
      "original": "2599",
      "page_count": 5,
      "order": 977,
      "p1": "4801",
      "pn": "4805",
      "abstract": [
        "Laryngeal videostroboscopy is widely used for the analysis of glottal\nvibration patterns. This analysis plays a crucial role in the diagnosis\nof voice disorders. It is essential to study these patterns using automatic\nglottis segmentation methods to avoid subjectiveness in diagnosis.\nGlottis detection is an essential step before glottis segmentation.\nThis paper considers the problem of automatic glottis segmentation\nusing U-Net based deep convolutional networks. For accurate glottis\ndetection, we train a fully convolutional network with a large amount\nof glottal and non-glottal images. In glottis segmentation, we consider\nU-Net with three different weight initialization schemes: 1) Random\nweight Initialization (RI), 2) Detection Network weight Initialization\n(DNI) and 3) Detection Network encoder frozen weight Initialization\n(DNIFr), using two different architectures: 1) U-Net without skip connection\n(UWSC) 2) U-Net with skip connection (USC). Experiments with 22 subjects&#8217;\ndata reveal that the performance of glottis segmentation network can\nbe increased by initializing its weights using those of the glottis\ndetection network. Among all schemes, when DNI is used, the USC yields\nan average localization accuracy of 81.3% and a Dice score of 0.73,\nwhich are better than those from the baseline approach by 15.87% and\n0.07 (absolute), respectively.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2599",
      "author_area_id": "1",
      "author_area_label": "Speech Perception, Production and Acquisition"
    },
    "pan20c_interspeech": {
      "authors": [
        [
          "Yilin",
          "Pan"
        ],
        [
          "Bahman",
          "Mirheidari"
        ],
        [
          "Zehai",
          "Tu"
        ],
        [
          "Ronan",
          "O\u2019Malley"
        ],
        [
          "Traci",
          "Walker"
        ],
        [
          "Annalena",
          "Venneri"
        ],
        [
          "Markus",
          "Reuber"
        ],
        [
          "Daniel",
          "Blackburn"
        ],
        [
          "Heidi",
          "Christensen"
        ]
      ],
      "title": "Acoustic Feature Extraction with Interpretable Deep Neural Network for Neurodegenerative Related Disorder Classification",
      "original": "2684",
      "page_count": 5,
      "order": 978,
      "p1": "4806",
      "pn": "4810",
      "abstract": [
        "Speech-based automatic approaches for detecting neurodegenerative disorders\n(ND) and mild cognitive impairment (MCI) have received more attention\nrecently due to being non-invasive and potentially more sensitive than\ncurrent pen-and-paper tests. The performance of such systems is highly\ndependent on the choice of features in the classification pipeline.\nIn particular for acoustic features, arriving at a consensus for a\nbest feature set has proven challenging. This paper explores using\ndeep neural network for extracting features directly from the speech\nsignal as a solution to this. Compared with hand-crafted features,\nmore information is present in the raw waveform, but the feature extraction\nprocess becomes more complex and less interpretable which is often\nundesirable in medical domains. Using a SincNet as a first layer allows\nfor some analysis of learned features. We propose and evaluate the\nSinc-CLA (with SincNet, Convolutional, Long Short-Term Memory and Attention\nlayers) as a task-driven acoustic feature extractor for classifying\nMCI, ND and healthy controls (HC). Experiments are carried out on an\nin-house dataset. Compared with the popular hand-crafted feature sets,\nthe learned task-driven features achieve a superior classification\naccuracy. The filters of the SincNet is inspected and acoustic differences\nbetween HC, MCI and ND are found.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2684",
      "author_area_id": "1",
      "author_area_label": "Speech Perception, Production and Acquisition"
    },
    "sharma20d_interspeech": {
      "authors": [
        [
          "Neeraj",
          "Sharma"
        ],
        [
          "Prashant",
          "Krishnan"
        ],
        [
          "Rohit",
          "Kumar"
        ],
        [
          "Shreyas",
          "Ramoji"
        ],
        [
          "Srikanth Raj",
          "Chetupalli"
        ],
        [
          "Nirmala",
          "R."
        ],
        [
          "Prasanta Kumar",
          "Ghosh"
        ],
        [
          "Sriram",
          "Ganapathy"
        ]
      ],
      "title": "Coswara &#8212; A Database of Breathing, Cough, and Voice Sounds for COVID-19 Diagnosis",
      "original": "2768",
      "page_count": 5,
      "order": 979,
      "p1": "4811",
      "pn": "4815",
      "abstract": [
        "The COVID-19 pandemic presents global challenges transcending boundaries\nof country, race, religion, and economy. The current gold standard\nmethod for COVID-19 detection is the reverse transcription polymerase\nchain reaction (RT-PCR) testing. However, this method is expensive,\ntime-consuming, and violates social distancing. Also, as the pandemic\nis expected to stay for a while, there is a need for an alternate diagnosis\ntool which overcomes these limitations, and is deployable at a large\nscale. The prominent symptoms of COVID-19 include cough and breathing\ndifficulties. We foresee that respiratory sounds, when analyzed using\nmachine learning techniques, can provide useful insights, enabling\nthe design of a diagnostic tool. Towards this, the paper presents an\nearly effort in creating (and analyzing) a database, called Coswara,\nof respiratory sounds, namely, cough, breath, and voice. The sound\nsamples are collected via worldwide crowdsourcing using a website application.\nThe curated dataset is released as open access. As the pandemic is\nevolving, the data collection and analysis is a work in progress. We\nbelieve that insights from analysis of Coswara can be effective in\nenabling sound based technology solutions for point-of-care diagnosis\nof respiratory infection, and in the near future this can help to diagnose\nCOVID-19.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2768",
      "author_area_id": "1",
      "author_area_label": "Speech Perception, Production and Acquisition"
    },
    "rowe20_interspeech": {
      "authors": [
        [
          "Hannah P.",
          "Rowe"
        ],
        [
          "Sarah E.",
          "Gutz"
        ],
        [
          "Marc F.",
          "Maffei"
        ],
        [
          "Jordan R.",
          "Green"
        ]
      ],
      "title": "Acoustic-Based Articulatory Phenotypes of Amyotrophic Lateral Sclerosis and Parkinson&#8217;s Disease: Towards an Interpretable, Hypothesis-Driven Framework of Motor Control",
      "original": "1459",
      "page_count": 5,
      "order": 980,
      "p1": "4816",
      "pn": "4820",
      "abstract": [
        "The purpose of this study was to determine the articulatory phenotypes\nof amyotrophic lateral sclerosis (ALS) and Parkinson&#8217;s disease\n(PD) using a novel acoustic-based framework that assesses five key\ncomponents of motor performance:  Coordination, Consistency, Speed,\nPrecision, and  Rate. The use of interpretable, hypothesis-driven features\nhas the potential to inform impairment-based automatic speech recognition\n(ASR) models and improve classification algorithms for disorders with\ndivergent articulatory profiles. Acoustic features were extracted from\naudio recordings of 18 healthy controls, 18 participants with ALS,\nand 18 participants with PD producing syllable sequences. Results revealed\nsignificantly different articulatory phenotypes for each disorder group.\nUpon stratification into Early Stage and Late Stage in disease progression,\nresults from individual receiver operating characteristic (ROC) curves\nand decision tree analyses showed high diagnostic accuracy for impaired\n Coordination in the Early Stage and impaired  Rate in the Late Stage.\nWith additional research, articulatory phenotypes characterized using\nthis framework may lead to advancements in ASR for dysarthric speech\nand diagnostic accuracy at different disease stages for individuals\nwith distinct articulatory deficits.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1459",
      "author_area_id": "1",
      "author_area_label": "Speech Perception, Production and Acquisition"
    },
    "alhinti20_interspeech": {
      "authors": [
        [
          "Lubna",
          "Alhinti"
        ],
        [
          "Stuart",
          "Cunningham"
        ],
        [
          "Heidi",
          "Christensen"
        ]
      ],
      "title": "Recognising Emotions in Dysarthric Speech Using Typical Speech Data",
      "original": "1825",
      "page_count": 5,
      "order": 981,
      "p1": "4821",
      "pn": "4825",
      "abstract": [
        "Effective communication relies on the comprehension of both verbal\nand nonverbal information. People with dysarthria may lose their ability\nto produce intelligible and audible speech sounds which in time may\naffect their way of conveying emotions, that are mostly expressed using\nnonverbal signals. Recent research shows some promise on automatically\nrecognising the verbal part of dysarthric speech. However, this is\nthe first study that investigates the ability to automatically recognise\nthe nonverbal part. A parallel database of dysarthric and typical emotional\nspeech is collected, and approaches to discriminating between emotions\nusing models trained on either dysarthric (speaker dependent,  matched)\nor typical (speaker independent,  unmatched) speech are investigated\nfor four speakers with dysarthria caused by cerebral palsy and Parkinson&#8217;s\ndisease. Promising results are achieved in both scenarios using SVM\nclassifiers, opening new doors to improved, more expressive voice input\ncommunication aids.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1825",
      "author_area_id": "1",
      "author_area_label": "Speech Perception, Production and Acquisition"
    },
    "halpern20_interspeech": {
      "authors": [
        [
          "Bence Mark",
          "Halpern"
        ],
        [
          "Rob van",
          "Son"
        ],
        [
          "Michiel van den",
          "Brekel"
        ],
        [
          "Odette",
          "Scharenborg"
        ]
      ],
      "title": "Detecting and Analysing Spontaneous Oral Cancer Speech in the Wild",
      "original": "1598",
      "page_count": 5,
      "order": 982,
      "p1": "4826",
      "pn": "4830",
      "abstract": [
        "Oral cancer speech is a disease which impacts more than half a million\npeople worldwide every year. Analysis of oral cancer speech has so\nfar focused on read speech. In this paper, we 1) present and 2) analyse\na three-hour long spontaneous oral cancer speech dataset collected\nfrom YouTube. 3) We set baselines for an oral cancer speech detection\ntask on this dataset. The analysis of these explainable machine learning\nbaselines shows that sibilants and stop consonants are the most important\nindicators for spontaneous oral cancer speech detection.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1598",
      "author_area_id": "1",
      "author_area_label": "Speech Perception, Production and Acquisition"
    },
    "dunbar20_interspeech": {
      "authors": [
        [
          "Ewan",
          "Dunbar"
        ],
        [
          "Julien",
          "Karadayi"
        ],
        [
          "Mathieu",
          "Bernard"
        ],
        [
          "Xuan-Nga",
          "Cao"
        ],
        [
          "Robin",
          "Algayres"
        ],
        [
          "Lucas",
          "Ondel"
        ],
        [
          "Laurent",
          "Besacier"
        ],
        [
          "Sakriani",
          "Sakti"
        ],
        [
          "Emmanuel",
          "Dupoux"
        ]
      ],
      "title": "The Zero Resource Speech Challenge 2020: Discovering Discrete Subword and Word Units",
      "original": "2743",
      "page_count": 5,
      "order": 983,
      "p1": "4831",
      "pn": "4835",
      "abstract": [
        "We present the Zero Resource Speech Challenge 2020, which aims at learning\nspeech representations from raw audio signals without any labels. It\ncombines the data sets and metrics from two previous benchmarks (2017\nand 2019) and features two tasks which tap into two levels of speech\nrepresentation. The first task is to discover low bit-rate subword\nrepresentations that optimize the quality of speech synthesis; the\nsecond one is to discover word-like units from unsegmented raw speech.\nWe present the results of the twenty submitted models and discuss the\nimplications of the main findings for unsupervised speech learning.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2743",
      "author_area_id": "13",
      "author_area_label": "Special Sessions"
    },
    "niekerk20b_interspeech": {
      "authors": [
        [
          "Benjamin van",
          "Niekerk"
        ],
        [
          "Leanne",
          "Nortje"
        ],
        [
          "Herman",
          "Kamper"
        ]
      ],
      "title": "Vector-Quantized Neural Networks for Acoustic Unit Discovery in the ZeroSpeech 2020 Challenge",
      "original": "1693",
      "page_count": 5,
      "order": 984,
      "p1": "4836",
      "pn": "4840",
      "abstract": [
        "In this paper, we explore vector quantization for acoustic unit discovery.\nLeveraging unlabelled data, we aim to learn discrete representations\nof speech that separate phonetic content from speaker-specific details.\nWe propose two neural models to tackle this challenge &#8212; both\nuse vector quantization to map continuous features to a finite set\nof codes. The first model is a type of vector-quantized variational\nautoencoder (VQ-VAE). The VQ-VAE encodes speech into a sequence of\ndiscrete units before reconstructing the audio waveform. Our second\nmodel combines vector quantization with contrastive predictive coding\n(VQ-CPC). The idea is to learn a representation of speech by predicting\nfuture acoustic units. We evaluate the models on English and Indonesian\ndata for the  ZeroSpeech 2020 challenge. In ABX phone discrimination\ntests, both models outperform all submissions to the 2019 and 2020\nchallenges, with a relative improvement of more than 30%. The models\nalso perform competitively on a downstream voice conversion task. Of\nthe two, VQ-CPC performs slightly better in general and is simpler\nand faster to train. Finally, probing experiments show that vector\nquantization is an effective bottleneck, forcing the models to discard\nspeaker information.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1693",
      "author_area_id": "13",
      "author_area_label": "Special Sessions"
    },
    "ds20_interspeech": {
      "authors": [
        [
          "Karthik Pandia",
          "D.S."
        ],
        [
          "Anusha",
          "Prakash"
        ],
        [
          "Mano Ranjith Kumar",
          "M."
        ],
        [
          "Hema A.",
          "Murthy"
        ]
      ],
      "title": "Exploration of End-to-End Synthesisers for Zero Resource Speech Challenge 2020",
      "original": "2731",
      "page_count": 5,
      "order": 985,
      "p1": "4841",
      "pn": "4845",
      "abstract": [
        "A Spoken dialogue system for an unseen language is referred to as Zero\nresource speech. It is especially beneficial for developing applications\nfor languages that have low digital resources. Zero resource speech\nsynthesis is the task of building text-to-speech (TTS) models in the\nabsence of transcriptions.<br style=\"mso-data-placement:same-cell;\"\n/><br style=\"mso-data-placement:same-cell;\" />\n  In this work, speech\nis modelled as a sequence of transient and steady-state acoustic units,\nand a unique set of acoustic units is discovered by iterative training.\nUsing the acoustic unit sequence, TTS models are trained.<br style=\"mso-data-placement:same-cell;\"\n/><br style=\"mso-data-placement:same-cell;\" />\n  The main goal of this\nwork is to improve the synthesis quality of zero resource TTS system.\nFour different systems are proposed. All the systems consist of three\nstages &#8212; unit discovery, followed by unit sequence to spectrogram\nmapping, and finally spectrogram to speech inversion. Modifications\nare proposed to the spectrogram mapping stage. These modifications\ninclude training the mapping on voice data, using x-vectors to improve\nthe mapping, two-stage learning, and gender-specific modelling. Evaluation\nof the proposed systems in the Zerospeech 2020 challenge shows that\nquite good quality synthesis can be achieved.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2731",
      "author_area_id": "13",
      "author_area_label": "Special Sessions"
    },
    "gundogdu20_interspeech": {
      "authors": [
        [
          "Batuhan",
          "Gundogdu"
        ],
        [
          "Bolaji",
          "Yusuf"
        ],
        [
          "Mansur",
          "Yesilbursa"
        ],
        [
          "Murat",
          "Saraclar"
        ]
      ],
      "title": "Vector Quantized Temporally-Aware Correspondence Sparse Autoencoders for Zero-Resource Acoustic Unit Discovery",
      "original": "2765",
      "page_count": 5,
      "order": 986,
      "p1": "4846",
      "pn": "4850",
      "abstract": [
        "A recent task posed by the Zerospeech challenge is the unsupervised\nlearning of the basic acoustic units that exist in an unknown language.\nPreviously, we introduced recurrent sparse autoencoders fine-tuned\nwith corresponding speech segments obtained by unsupervised term discovery.\nThere, the clustering was obtained on the intermediate layer where\nthe nodes represent the acoustic unit assignments. In this paper, we\nextend this system by incorporating vector quantization and an adaptation\nof the winner-take-all networks. This way, symbol continuity could\nbe enforced by excitatory and inhibitory weights along the temporal\naxis. Furthermore, in this work, we utilized the speaker information\nin a speaker adversarial training on the encoder. The ABX discriminability\nand the low bitrate results of our proposed approach on the Zerospeech\n2020 challenge demonstrate the effect of the enhanced continuity of\nthe encoding brought by the temporal-awareness and sparsity techniques\nproposed in this work.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2765",
      "author_area_id": "13",
      "author_area_label": "Special Sessions"
    },
    "tjandra20_interspeech": {
      "authors": [
        [
          "Andros",
          "Tjandra"
        ],
        [
          "Sakriani",
          "Sakti"
        ],
        [
          "Satoshi",
          "Nakamura"
        ]
      ],
      "title": "Transformer VQ-VAE for Unsupervised Unit Discovery and Speech Synthesis: ZeroSpeech 2020 Challenge",
      "original": "3033",
      "page_count": 5,
      "order": 987,
      "p1": "4851",
      "pn": "4855",
      "abstract": [
        "In this paper, we report our submitted system for the ZeroSpeech 2020\nchallenge on Track 2019. The main theme in this challenge is to build\na speech synthesizer without any textual information or phonetic labels.\nIn order to tackle those challenges, we build a system that must address\ntwo major components such as 1) given speech audio, extract subword\nunits in an unsupervised way and 2) re-synthesize the audio from novel\nspeakers. The system also needs to balance the codebook performance\nbetween the ABX error rate and the bitrate compression rate. Our main\ncontribution here is we proposed Transformer-based VQ-VAE for unsupervised\nunit discovery and Transformer-based inverter for the speech synthesis\ngiven the extracted codebook. Additionally, we also explored several\nregularization methods to improve performance even further.\n"
      ],
      "doi": "10.21437/Interspeech.2020-3033",
      "author_area_id": "13",
      "author_area_label": "Special Sessions"
    },
    "morita20_interspeech": {
      "authors": [
        [
          "Takashi",
          "Morita"
        ],
        [
          "Hiroki",
          "Koda"
        ]
      ],
      "title": "Exploring TTS Without T Using Biologically/Psychologically Motivated Neural Network Modules (ZeroSpeech 2020)",
      "original": "3127",
      "page_count": 5,
      "order": 988,
      "p1": "4856",
      "pn": "4860",
      "abstract": [
        "In this study, we reported our exploration of Text-To-Speech without\nText (TTS without T) in the Zero Resource Speech Challenge 2020, in\nwhich participants proposed an end-to-end, unsupervised system that\nlearned speech recognition and TTS together. We addressed the challenge\nusing biologically/psychologically motivated modules of Artificial\nNeural Networks (ANN), with a particular interest in unsupervised learning\nof human language as a biological/psychological problem. The system\nfirst processes Mel Frequency Cepstral Coefficient (MFCC) frames with\nan Echo-State Network (ESN), and simulates computations in cortical\nmicrocircuits. The outcome is discretized by our original Variational\nAutoencoder (VAE) that implements the Dirichlet-based Bayesian clustering\nwidely accepted in computational linguistics and cognitive science.\nThe discretized signal is then reverted into sound waveform via a neural-network\nimplementation of the source-filter model for speech production.\n"
      ],
      "doi": "10.21437/Interspeech.2020-3127",
      "author_area_id": "13",
      "author_area_label": "Special Sessions"
    },
    "tobing20_interspeech": {
      "authors": [
        [
          "Patrick Lumban",
          "Tobing"
        ],
        [
          "Tomoki",
          "Hayashi"
        ],
        [
          "Yi-Chiao",
          "Wu"
        ],
        [
          "Kazuhiro",
          "Kobayashi"
        ],
        [
          "Tomoki",
          "Toda"
        ]
      ],
      "title": "Cyclic Spectral Modeling for Unsupervised Unit Discovery into Voice Conversion with Excitation and Waveform Modeling",
      "original": "2559",
      "page_count": 5,
      "order": 989,
      "p1": "4861",
      "pn": "4865",
      "abstract": [
        "We present a novel approach of cyclic spectral modeling for unsupervised\ndiscovery of speech units into voice conversion with excitation network\nand waveform modeling. Specifically, we propose two spectral modeling\ntechniques: 1) cyclic vector-quantized autoencoder (CycleVQVAE), and\n2) cyclic variational autoencoder (CycleVAE). In CycleVQVAE, a discrete\nlatent space is used for the speech units, whereas, in CycleVAE, a\ncontinuous latent space is used. The cyclic structure is developed\nusing the reconstruction flow and the cyclic reconstruction flow of\nspectral features, where the latter is obtained by recycling the converted\nspectral features. This method is used to obtain a possible speaker-independent\nlatent space because of marginalization on all possible speaker conversion\npairs during training. On the other hand, speaker-dependent space is\nconditioned with a one-hot speaker-code. Excitation modeling is developed\nin a separate manner for CycleVQVAE, while it is in a joint manner\nfor CycleVAE. To generate speech waveform, WaveNet-based waveform modeling\nis used. The proposed framework is entried for the ZeroSpeech Challenge\n2020, and is capable of reaching a character error rate of 0.21, a\nspeaker similarity score of 3.91, a mean opinion score of 3.84 for\nthe naturalness of the converted speech in the 2019 voice conversion\ntask.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2559",
      "author_area_id": "13",
      "author_area_label": "Special Sessions"
    },
    "chen20u_interspeech": {
      "authors": [
        [
          "Mingjie",
          "Chen"
        ],
        [
          "Thomas",
          "Hain"
        ]
      ],
      "title": "Unsupervised Acoustic Unit Representation Learning for Voice Conversion Using WaveNet Auto-Encoders",
      "original": "1785",
      "page_count": 5,
      "order": 990,
      "p1": "4866",
      "pn": "4870",
      "abstract": [
        "Unsupervised representation learning of speech has been of keen interest\nin recent years, which is for example evident in the wide interest\nof the ZeroSpeech challenges. This work presents a new method for learning\nframe level representations based on WaveNet auto-encoders. Of particular\ninterest in the ZeroSpeech Challenge 2019 were models with discrete\nlatent variable such as the Vector Quantized Variational Auto-Encoder\n(VQVAE). However these models generate speech with relatively poor\nquality. In this work we aim to address this with two approaches: first\nWaveNet is used as the decoder and to generate waveform data directly\nfrom the latent representation; second, the low complexity of latent\nrepresentations is improved with two alternative disentanglement learning\nmethods, namely instance normalization and sliced vector quantization.\nThe method was developed and tested in the context of the recent ZeroSpeech\nchallenge 2020. The system output submitted to the challenge obtained\nthe top position for naturalness (Mean Opinion Score 4.06), top position\nfor intelligibility (Character Error Rate 0.15), and third position\nfor the quality of the representation (ABX test score 12.5). These\nand further analysis in this paper illustrates that quality of the\nconverted speech and the acoustic units representation can be well\nbalanced.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1785",
      "author_area_id": "13",
      "author_area_label": "Special Sessions"
    },
    "rasanen20_interspeech": {
      "authors": [
        [
          "Okko",
          "R\u00e4s\u00e4nen"
        ],
        [
          "Mar\u00eda Andrea Cruz",
          "Bland\u00f3n"
        ]
      ],
      "title": "Unsupervised Discovery of Recurring Speech Patterns Using Probabilistic Adaptive Metrics",
      "original": "1738",
      "page_count": 5,
      "order": 991,
      "p1": "4871",
      "pn": "4875",
      "abstract": [
        "Unsupervised spoken term discovery (UTD) aims at finding recurring\nsegments of speech from a corpus of acoustic speech data. One potential\napproach to this problem is to use dynamic time warping (DTW) to find\nwell-aligning patterns from the speech data. However, automatic selection\nof initial candidate segments for the DTW-alignment and detection of\n&#8220;sufficiently good&#8221; alignments among those require some\ntype of predefined criteria, often operationalized as threshold parameters\nfor pair-wise distance metrics between signal representations. In the\nexisting UTD systems, the optimal hyperparameters may differ across\ndatasets, limiting their applicability to new corpora and truly low-resource\nscenarios. In this paper, we propose a novel probabilistic approach\nto DTW-based UTD named as PDTW. In PDTW, distributional characteristics\nof the processed corpus are utilized for adaptive evaluation of alignment\nquality, thereby enabling systematic discovery of pattern pairs that\nhave similarity what would be expected by coincidence. We test PDTW\non Zero Resource Speech Challenge 2017 datasets as a part of 2020 implementation\nof the challenge. The results show that the system performs consistently\non all five tested languages using fixed hyperparameters, clearly outperforming\nthe earlier DTW-based system in terms of coverage of the detected patterns.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1738",
      "author_area_id": "13",
      "author_area_label": "Special Sessions"
    },
    "bhati20_interspeech": {
      "authors": [
        [
          "Saurabhchand",
          "Bhati"
        ],
        [
          "Jes\u00fas",
          "Villalba"
        ],
        [
          "Piotr",
          "\u017belasko"
        ],
        [
          "Najim",
          "Dehak"
        ]
      ],
      "title": "Self-Expressing Autoencoders for Unsupervised Spoken Term Discovery",
      "original": "3000",
      "page_count": 5,
      "order": 992,
      "p1": "4876",
      "pn": "4880",
      "abstract": [
        "Unsupervised spoken term discovery consists of two tasks: finding the\nacoustic segment boundaries and labeling acoustically similar segments\nwith the same labels. We perform segmentation based on the assumption\nthat the frame feature vectors are more similar within a segment than\nacross the segments. Therefore, for strong segmentation performance,\nit is crucial that the features represent the phonetic properties of\na frame more than other factors of variability. We achieve this via\na self-expressing autoencoder framework. It consists of a single encoder\nand two decoders with shared weights. The encoder projects the input\nfeatures into a latent representation. One of the decoders tries to\nreconstruct the input from these latent representations and the other\nfrom the self-expressed version of them. We use the obtained features\nto segment and cluster the speech data. We evaluate the performance\nof the proposed method in the Zero Resource 2020 challenge unit discovery\ntask. The proposed system consistently outperforms the baseline, demonstrating\nthe usefulness of the method in learning representations.\n"
      ],
      "doi": "10.21437/Interspeech.2020-3000",
      "author_area_id": "13",
      "author_area_label": "Special Sessions"
    },
    "millet20_interspeech": {
      "authors": [
        [
          "Juliette",
          "Millet"
        ],
        [
          "Ewan",
          "Dunbar"
        ]
      ],
      "title": "Perceptimatic: A Human Speech Perception Benchmark for Unsupervised Subword Modelling",
      "original": "1671",
      "page_count": 5,
      "order": 993,
      "p1": "4881",
      "pn": "4885",
      "abstract": [
        "In this paper, we present a data set and methods to compare speech\nprocessing models and human behaviour on a phone discrimination task.\nWe provide Perceptimatic, an open data set which consists of French\nand English speech stimuli, as well as the results of 91 English- and\n93 French-speaking listeners. The stimuli test a wide range of French\nand English contrasts, and are extracted directly from corpora of natural\nrunning read speech, used for the 2017 Zero Resource Speech Challenge.\nWe provide a method to compare humans&#8217; perceptual space with\nmodels&#8217; representational space, and we apply it to models previously\nsubmitted to the Challenge. We show that, unlike unsupervised models\nand supervised multilingual models, a standard supervised monolingual\nHMM-GMM phone recognition system, while good at discriminating phones,\nyields a representational space very different from that of human native\nlisteners.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1671",
      "author_area_id": "13",
      "author_area_label": "Special Sessions"
    },
    "clayton20_interspeech": {
      "authors": [
        [
          "Jonathan",
          "Clayton"
        ],
        [
          "Scott",
          "Wellington"
        ],
        [
          "Cassia",
          "Valentini-Botinhao"
        ],
        [
          "Oliver",
          "Watts"
        ]
      ],
      "title": "Decoding Imagined, Heard, and Spoken Speech: Classification and Regression of EEG Using a 14-Channel Dry-Contact Mobile Headset",
      "original": "2745",
      "page_count": 5,
      "order": 994,
      "p1": "4886",
      "pn": "4890",
      "abstract": [
        "We investigate the use of a 14-channel, mobile EEG device in the decoding\nof heard, imagined, and articulated English phones from brainwave data.\nTo this end we introduce a dataset that fills a current gap in the\nrange of available open-access EEG datasets for speech processing with\nlightweight, affordable EEG devices made for the consumer market. We\ninvestigate the effectiveness of two classification models and a regression\nmodel for reconstructing spectral features of the original speech signal.\nWe report that our classification performance is almost on a par with\nsimilar findings that use EEG data collected with research-grade devices.\nWe conclude that commercial-grade devices can be used as speech-decoding\nBCIs with minimal signal processing.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2745",
      "author_area_id": "13",
      "author_area_label": "Special Sessions"
    },
    "m20b_interspeech": {
      "authors": [
        [
          "Gurunath Reddy",
          "M."
        ],
        [
          "K. Sreenivasa",
          "Rao"
        ],
        [
          "Partha Pratim",
          "Das"
        ]
      ],
      "title": "Glottal Closure Instants Detection from EGG Signal by Classification Approach",
      "original": "1189",
      "page_count": 5,
      "order": 995,
      "p1": "4891",
      "pn": "4895",
      "abstract": [
        "Electroglottography is a non-invasive technique to acquire the vocal\nfolds activity across the larynx called EGG signal. The EGG is a clean\nsignal free from vocal tract resonances, the parameters extracted from\nsuch a signal finds many applications in clinical and speech processing\ntechnology. In this paper, we propose a classification based approach\nto detect the significant parameter of the EGG such as glottal closure\ninstant (GCI). We train deep convolutional neural networks (CNN) to\npredict if a frame of samples contain GCI location. Further, the GCI\nlocation within the frame is obtained by exploiting its unique manifestation\nfrom its first order derivative. We train several CNN models to determine\nthe suitable input feature representation to efficiently detect the\nGCI location. Further, we train and evaluate the models on multiple\nspeaker dataset to determine and eliminate any bias towards the speaker.\nWe also show that the GCI identification rate can be improved significantly\nby the model trained with joint EGG and derivative (dEGG) signal. The\ndeep models are trained with manually annotated GCI markers obtained\nfrom dEGG as reference. The objective evaluation measures confirmed\nthat the proposed method is comparable and better than the traditional\nsignal processing GCI detection methods.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1189",
      "author_area_id": "13",
      "author_area_label": "Special Sessions"
    },
    "li20ma_interspeech": {
      "authors": [
        [
          "Hua",
          "Li"
        ],
        [
          "Fei",
          "Chen"
        ]
      ],
      "title": "Classify Imaginary Mandarin Tones with Cortical EEG Signals",
      "original": "1248",
      "page_count": 5,
      "order": 996,
      "p1": "4896",
      "pn": "4900",
      "abstract": [
        "Speech synthesis system based on non-invasive brain-computer interface\ntechnology has the potential to restore communication abilities to\npatients with communication disorders. To this end, electroencephalogram\n(EEG) based speech imagery technology is fast evolving largely due\nto its advantages of simple implementation and low dependence on external\nstimuli. This work studied possible factors accounting for the classification\naccuracies of EEG-based imaginary Mandarin tones, which has significance\nto the development of BCI-based Mandarin speech synthesis system. Specially,\na Mandarin tone imagery experiment was designed, and this work studied\nthe effects of electrode configuration and tone cuing on accurately\nclassifying four Mandarin tones from cortical EEG signals. Results\nshowed that the involvement of more activated brain regions (i.e.,\nBroca&#8217;s area, Wernicke&#8217;s area, and primary motor cortex)\nprovided a more accurate classification of imaginary Mandarin tones\nthan that of one specific region. At the tone cue stage, using audio-visual\nstimuli led to a much stronger and more separable activation of brain\nregions than using visual-only stimuli. In addition, the classification\naccuracies of tone 1 and tone 4 were significantly higher than those\nof tone 2 and tone 3.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1248",
      "author_area_id": "13",
      "author_area_label": "Special Sessions"
    },
    "effendi20_interspeech": {
      "authors": [
        [
          "Johanes",
          "Effendi"
        ],
        [
          "Andros",
          "Tjandra"
        ],
        [
          "Sakriani",
          "Sakti"
        ],
        [
          "Satoshi",
          "Nakamura"
        ]
      ],
      "title": "Augmenting Images for ASR and TTS Through Single-Loop and Dual-Loop Multimodal Chain Framework",
      "original": "2001",
      "page_count": 5,
      "order": 997,
      "p1": "4901",
      "pn": "4905",
      "abstract": [
        "Previous research has proposed a machine speech chain to enable automatic\nspeech recognition (ASR) and text-to-speech synthesis (TTS) to assist\neach other in semi-supervised learning and to avoid the need for a\nlarge amount of paired speech and text data. However, that framework\nstill requires a large amount of unpaired (speech or text) data. A\nprototype multimodal machine chain was then explored to further reduce\nthe need for a large amount of unpaired data, which could improve ASR\nor TTS even when no more speech or text data were available. Unfortunately,\nthis framework relied on the image retrieval (IR) model, and thus it\nwas limited to handling only those images that were already known during\ntraining. Furthermore, the performance of this framework was only investigated\nwith single-speaker artificial speech data. In this study, we revamp\nthe multimodal machine chain framework with image generation (IG) and\ninvestigate the possibility of augmenting image data for ASR and TTS\nusing single-loop and dual-loop architectures on multispeaker natural\nspeech data. Experimental results revealed that both single-loop and\ndual-loop multimodal chain frameworks enabled ASR and TTS to improve\ntheir performance using an image-only dataset.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2001",
      "author_area_id": "9",
      "author_area_label": "Speech Recognition: Architecture, Search, and Linguistic Components"
    },
    "augustyniak20_interspeech": {
      "authors": [
        [
          "\u0141ukasz",
          "Augustyniak"
        ],
        [
          "Piotr",
          "Szyma\u0144ski"
        ],
        [
          "Miko\u0142aj",
          "Morzy"
        ],
        [
          "Piotr",
          "\u017belasko"
        ],
        [
          "Adrian",
          "Szymczak"
        ],
        [
          "Jan",
          "Mizgajski"
        ],
        [
          "Yishay",
          "Carmiel"
        ],
        [
          "Najim",
          "Dehak"
        ]
      ],
      "title": "Punctuation Prediction in Spontaneous Conversations: Can We Mitigate ASR Errors with Retrofitted Word Embeddings?",
      "original": "1250",
      "page_count": 5,
      "order": 998,
      "p1": "4906",
      "pn": "4910",
      "abstract": [
        "Automatic Speech Recognition (ASR) systems introduce word errors, which\noften confuse punctuation prediction models, turning punctuation restoration\ninto a challenging task. These errors usually take the form of homophones\n(words which share exact or almost exact pronunciation but differ in\nmeaning) and oronyms (homophones which consist of multiple words).\nWe show how retrofitting of the word embeddings on the domain-specific\ndata can mitigate ASR errors. Our main contribution is a method for\na better alignment of homophone embeddings and the validation of the\npresented method on the punctuation prediction task. We record the\nabsolute improvement in punctuation prediction accuracy between 6.2%\n(for question marks) to 9% (for periods) when compared with the state-of-the-art\nmodel.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1250",
      "author_area_id": "9",
      "author_area_label": "Speech Recognition: Architecture, Search, and Linguistic Components"
    },
    "sunkara20_interspeech": {
      "authors": [
        [
          "Monica",
          "Sunkara"
        ],
        [
          "Srikanth",
          "Ronanki"
        ],
        [
          "Dhanush",
          "Bekal"
        ],
        [
          "Sravan",
          "Bodapati"
        ],
        [
          "Katrin",
          "Kirchhoff"
        ]
      ],
      "title": "Multimodal Semi-Supervised Learning Framework for Punctuation Prediction in Conversational Speech",
      "original": "3074",
      "page_count": 5,
      "order": 999,
      "p1": "4911",
      "pn": "4915",
      "abstract": [
        "In this work, we explore a multimodal semi-supervised learning approach\nfor punctuation prediction by learning representations from large amounts\nof unlabelled audio and text data. Conventional approaches in speech\nprocessing typically use forced alignment to encoder per frame acoustic\nfeatures to word level features and perform multimodal fusion of the\nresulting acoustic and lexical representations. As an alternative,\nwe explore attention based multimodal fusion and compare its performance\nwith forced alignment based fusion. Experiments conducted on the Fisher\ncorpus show that our proposed approach achieves &#126;6&#8211;9% and\n&#126;3&#8211;4% absolute improvement (F1 score) over the baseline\nBLSTM model on reference transcripts and ASR outputs respectively.\nWe further improve the model robustness to ASR errors by performing\ndata augmentation with N-best lists which achieves up to an additional\n&#126;2&#8211;6% improvement on ASR outputs. We also demonstrate the\neffectiveness of semi-supervised learning approach by performing ablation\nstudy on various sizes of the corpus. When trained on 1 hour of speech\nand text data, the proposed model achieved &#126;9&#8211;18% absolute\nimprovement over baseline model.\n"
      ],
      "doi": "10.21437/Interspeech.2020-3074",
      "author_area_id": "9",
      "author_area_label": "Speech Recognition: Architecture, Search, and Linguistic Components"
    },
    "huang20j_interspeech": {
      "authors": [
        [
          "Ruizhe",
          "Huang"
        ],
        [
          "Ke",
          "Li"
        ],
        [
          "Ashish",
          "Arora"
        ],
        [
          "Daniel",
          "Povey"
        ],
        [
          "Sanjeev",
          "Khudanpur"
        ]
      ],
      "title": "Efficient MDI Adaptation for n-Gram Language Models",
      "original": "2909",
      "page_count": 5,
      "order": 1000,
      "p1": "4916",
      "pn": "4920",
      "abstract": [
        "This paper presents an efficient algorithm for n-gram language model\nadaptation under the minimum discrimination information (MDI) principle,\nwhere an out-of-domain language model is adapted to satisfy the constraints\nof marginal probabilities of the in-domain data. The challenge for\nMDI language model adaptation is its computational complexity. By taking\nadvantage of the backoff structure of n-gram model and the idea of\nhierarchical training method, originally proposed for maximum entropy\n(ME) language models [1], we show that MDI adaptation can be computed\nin linear-time complexity to the inputs in each iteration. The complexity\nremains the same as ME models, although MDI is more general than ME.\nThis makes MDI adaptation practical for large corpus and vocabulary.\nExperimental results confirm the scalability of our algorithm on large\ndatasets, while MDI adaptation gets slightly worse perplexity but better\nword error rates compared to simple linear interpolation.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2909",
      "author_area_id": "9",
      "author_area_label": "Speech Recognition: Architecture, Search, and Linguistic Components"
    },
    "peyser20_interspeech": {
      "authors": [
        [
          "Cal",
          "Peyser"
        ],
        [
          "Sepand",
          "Mavandadi"
        ],
        [
          "Tara N.",
          "Sainath"
        ],
        [
          "James",
          "Apfel"
        ],
        [
          "Ruoming",
          "Pang"
        ],
        [
          "Shankar",
          "Kumar"
        ]
      ],
      "title": "Improving Tail Performance of a Deliberation E2E ASR Model Using a Large Text Corpus",
      "original": "1465",
      "page_count": 5,
      "order": 1001,
      "p1": "4921",
      "pn": "4925",
      "abstract": [
        "End-to-end (E2E) automatic speech recognition (ASR) systems lack the\ndistinct language model (LM) component that characterizes traditional\nspeech systems. While this simplifies the model architecture, it complicates\nthe task of incorporating text-only data into training, which is important\nto the recognition of tail words that do not occur often in audio-text\npairs. While shallow fusion has been proposed as a method for incorporating\na pre-trained LM into an E2E model at inference time, it has not yet\nbeen explored for very large text corpora, and it has been shown to\nbe very sensitive to hyperparameter settings in the beam search. In\nthis work, we apply shallow fusion to incorporate a very large text\ncorpus into a state-of-the-art E2E ASR model. We explore the impact\nof model size and show that intelligent pruning of the training set\ncan be more effective than increasing the parameter count. Additionally,\nwe show that incorporating the LM in minimum word error rate (MWER)\nfine tuning makes shallow fusion far less dependent on optimal hyperparameter\nsettings, reducing the difficulty of that tuning problem.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1465",
      "author_area_id": "9",
      "author_area_label": "Speech Recognition: Architecture, Search, and Linguistic Components"
    },
    "ogawa20_interspeech": {
      "authors": [
        [
          "Atsunori",
          "Ogawa"
        ],
        [
          "Naohiro",
          "Tawara"
        ],
        [
          "Marc",
          "Delcroix"
        ]
      ],
      "title": "Language Model Data Augmentation Based on Text Domain Transfer",
      "original": "1524",
      "page_count": 5,
      "order": 1002,
      "p1": "4926",
      "pn": "4930",
      "abstract": [
        "To improve the performance of automatic speech recognition (ASR) for\na specific domain, it is essential to train a language model (LM) using\ntext data of the target domain. In this study, we propose a method\nto transfer the domain of a large amount of source data to the target\ndomain and augment the data to train a target domain-specific LM. The\nproposed method consists of two steps, which use a bidirectional long\nshort-term memory (BLSTM)-based word replacing model and a target domain-adapted\nLSTMLM, respectively. Based on the learned domain-specific wordings,\nthe word replacing model converts a given source domain sentence to\na confusion network (CN) that includes a variety of target domain candidate\nword sequences. Then, the LSTMLM selects a target domain sentence from\nthe CN by evaluating its grammatical correctness based on decoding\nscores. In experiments using lecture and conversational speech corpora\nas the source and target domain data sets, we confirmed that the proposed\nLM data augmentation method improves the target conversational speech\nrecognition performance of a hybrid ASR system using an n-gram LM and\nthe performance of N-best rescoring using an LSTMLM.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1524",
      "author_area_id": "9",
      "author_area_label": "Speech Recognition: Architecture, Search, and Linguistic Components"
    },
    "wok20_interspeech": {
      "authors": [
        [
          "Krzysztof",
          "Wo\u0142k"
        ]
      ],
      "title": "Contemporary Polish Language Model (Version 2) Using Big Data and Sub-Word Approach",
      "original": "1207",
      "page_count": 5,
      "order": 1003,
      "p1": "4931",
      "pn": "4935",
      "abstract": [
        "Language and vocabulary continue to evolve in this era of big data,\nmaking language modelling an important language processing task that\nbenefits from the enormous data in different languages provided by\nweb-based corpora. In this paper, we present a set of 6-gram language\nmodels based on a big-data training of the contemporary Polish language,\nusing the Common Crawl corpus (a compilation of over 3.25 billion webpages)\nand other resources. The corpus is provided in different combinations\nof POS-tagged, grammatical groups-tagged, and sub-word-divided versions\nof raw corpora and trained models. The dictionary of contemporary Polish\nwas updated and presented, and we used the KENLM toolkit to train big-data\nlanguage models in ARPA format. Additionally, we have provided pre-trained\nvector models. The language model was trained, and the advances in\nBLEU score were obtained in MT systems along with the perplexity values,\nutilizing our models. The superiority of our model over Google&#8217;s\nWEB1T n-gram counts and the first version of our model was demonstrated\nthrough experiments, and the results illustrated that it guarantees\nimproved quality in perplexity and machine translation. Our models\ncan be applied in several natural language processing tasks and several\nscientific interdisciplinary fields.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1207",
      "author_area_id": "9",
      "author_area_label": "Speech Recognition: Architecture, Search, and Linguistic Components"
    },
    "pandey20b_interspeech": {
      "authors": [
        [
          "Prabhat",
          "Pandey"
        ],
        [
          "Volker",
          "Leutnant"
        ],
        [
          "Simon",
          "Wiesler"
        ],
        [
          "Jahn",
          "Heymann"
        ],
        [
          "Daniel",
          "Willett"
        ]
      ],
      "title": "Improving Speech Recognition of Compound-Rich Languages",
      "original": "2514",
      "page_count": 5,
      "order": 1004,
      "p1": "4936",
      "pn": "4940",
      "abstract": [
        "Traditional hybrid speech recognition systems use a fixed vocabulary\nfor recognition, which is a challenge for agglutinative and compounding\nlanguages due to the presence of large number of rare words. This causes\nhigh out-of-vocabulary rate and leads to poor probability estimates\nfor rare words. It is also important to keep the vocabulary size in\ncheck for a low-latency WFST-based speech recognition system. Previous\nworks have addressed this problem by utilizing subword units in the\nlanguage model training and merging them back to reconstruct words\nin the post-processing step. In this paper, we extend such open vocabulary\napproaches by focusing on compounding aspect. We present a data-driven\nunsupervised method to identify compound words in the vocabulary and\nlearn rules to segment them. We show that compound modeling can achieve\n3% to 8% relative reduction in word error rate and up to 9% reduction\nin the vocabulary size compared to word-based models. We also show\nthe importance of consistency between the lexicon employed during decoding\nand acoustic model training for subword-based systems.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2514",
      "author_area_id": "9",
      "author_area_label": "Speech Recognition: Architecture, Search, and Linguistic Components"
    },
    "wills20_interspeech": {
      "authors": [
        [
          "Simone",
          "Wills"
        ],
        [
          "Pieter",
          "Uys"
        ],
        [
          "Charl van",
          "Heerden"
        ],
        [
          "Etienne",
          "Barnard"
        ]
      ],
      "title": "Language Modeling for Speech Analytics in Under-Resourced Languages",
      "original": "1586",
      "page_count": 5,
      "order": 1005,
      "p1": "4941",
      "pn": "4945",
      "abstract": [
        "Different language modeling approaches are evaluated on two under-resourced,\nagglutinative, South African languages; Sesotho and isiZulu. The two\nlanguages present different challenges to language modeling based on\ntheir respective orthographies; isiZulu is conjunctively written whereas\nSotho is disjunctively written. Two subword modeling approaches are\nevaluated and shown to be useful to reduce the OOV rate for isiZulu,\nand for Sesotho, a multi-word approach is evaluated for improving ASR\naccuracy, with limited success. RNNs are also evaluated and shown to\nslightly improve ASR accuracy, despite relatively small text corpora.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1586",
      "author_area_id": "9",
      "author_area_label": "Speech Recognition: Architecture, Search, and Linguistic Components"
    },
    "han20c_interspeech": {
      "authors": [
        [
          "Jing",
          "Han"
        ],
        [
          "Kun",
          "Qian"
        ],
        [
          "Meishu",
          "Song"
        ],
        [
          "Zijiang",
          "Yang"
        ],
        [
          "Zhao",
          "Ren"
        ],
        [
          "Shuo",
          "Liu"
        ],
        [
          "Juan",
          "Liu"
        ],
        [
          "Huaiyuan",
          "Zheng"
        ],
        [
          "Wei",
          "Ji"
        ],
        [
          "Tomoya",
          "Koike"
        ],
        [
          "Xiao",
          "Li"
        ],
        [
          "Zixing",
          "Zhang"
        ],
        [
          "Yoshiharu",
          "Yamamoto"
        ],
        [
          "Bj\u00f6rn W.",
          "Schuller"
        ]
      ],
      "title": "An Early Study on Intelligent Analysis of Speech Under COVID-19: Severity, Sleep Quality, Fatigue, and Anxiety",
      "original": "2223",
      "page_count": 5,
      "order": 1006,
      "p1": "4946",
      "pn": "4950",
      "abstract": [
        "The COVID-19 outbreak was announced as a global pandemic by the World\nHealth Organisation in March 2020 and has affected a growing number\nof people in the past few weeks. In this context, advanced artificial\nintelligence techniques are brought to the fore in responding to fight\nagainst and reduce the impact of this global health crisis. In this\nstudy, we focus on developing some potential use-cases of intelligent\nspeech analysis for COVID-19 diagnosed patients. In particular, by\nanalysing speech recordings from these patients, we construct audio-only-based\nmodels to automatically categorise the health state of patients from\nfour aspects, including the severity of illness, sleep quality, fatigue,\nand anxiety. For this purpose, two established acoustic feature sets\nand support vector machines are utilised. Our experiments show that\nan average accuracy of .69 obtained estimating the severity of illness,\nwhich is derived from the number of days in hospitalisation. We hope\nthat this study can foster an extremely fast, low-cost, and convenient\nway to automatically detect the COVID-19 disease.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2223",
      "author_area_id": "3",
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "baird20_interspeech": {
      "authors": [
        [
          "Alice",
          "Baird"
        ],
        [
          "Nicholas",
          "Cummins"
        ],
        [
          "Sebastian",
          "Schnieder"
        ],
        [
          "Jarek",
          "Krajewski"
        ],
        [
          "Bj\u00f6rn W.",
          "Schuller"
        ]
      ],
      "title": "An Evaluation of the Effect of Anxiety on Speech &#8212; Computational Prediction of Anxiety from Sustained Vowels",
      "original": "1801",
      "page_count": 5,
      "order": 1007,
      "p1": "4951",
      "pn": "4955",
      "abstract": [
        "The current level of global uncertainty is having an implicit effect\non those with a diagnosed anxiety disorder. Anxiety can impact vocal\nqualities, particularly as physical symptoms of anxiety include muscle\ntension and shortness of breath. To this end, in this study, we explore\nthe effect of anxiety on speech &#8212; focusing on four classes of\nsustained vowels ( sad, smiling, comfortable, and  powerful) &#8212;\nvia feature analysis and a series of regression experiments. We extract\nthree well-known acoustic feature sets and evaluate the efficacy of\nmachine learning for prediction of anxiety based on the Beck Anxiety\nInventory (BAI) score. Of note, utilising a support vector regressor,\nwe find that the effects of anxiety in speech appear to be stronger\nat higher BAI levels. Significant differences (p &#60; 0.05) between\ntest predictions of  Low and  High-BAI groupings support this. Furthermore,\nwhen utilising a  High-BAI grouping for the prediction of standardised\nBAI, significantly higher results are obtained for  smiling sustained\nvowels, of up to 0.646 Spearman&#8217;s Correlation Coefficient (&#961;),\nand up to 0.592 &#961; with all sustained vowels. A significantly stronger\n(Cohens d of 1.718) result than all data combined without grouping,\nwhich achieves at best 0.234 &#961;.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1801",
      "author_area_id": "3",
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "zhao20h_interspeech": {
      "authors": [
        [
          "Ziping",
          "Zhao"
        ],
        [
          "Qifei",
          "Li"
        ],
        [
          "Nicholas",
          "Cummins"
        ],
        [
          "Bin",
          "Liu"
        ],
        [
          "Haishuai",
          "Wang"
        ],
        [
          "Jianhua",
          "Tao"
        ],
        [
          "Bj\u00f6rn W.",
          "Schuller"
        ]
      ],
      "title": "Hybrid Network Feature Extraction for Depression Assessment from Speech",
      "original": "2396",
      "page_count": 5,
      "order": 1008,
      "p1": "4956",
      "pn": "4960",
      "abstract": [
        "A fast-growing area of mental health research is the search for speech-based\nobjective markers for conditions such as depression. One vital challenge\nin the development of speech-based depression severity assessment systems\nis the extraction of depression-relevant features from speech signals.\nIn order to deliver more comprehensive feature representation, we herein\nexplore the benefits of a hybrid network that encodes depression-related\ncharacteristics in speech for the task of depression severity assessment.\nThe proposed network leverages self-attention networks (SAN) trained\non low-level acoustic features and deep convolutional neural networks\n(DCNN) trained on 3D Log-Mel spectrograms. The feature representations\nlearnt in the SAN and DCNN are concatenated and average pooling is\nexploited to aggregate complementary segment-level features. Finally,\nsupport vector regression is applied to predict a speaker&#8217;s Beck\nDepression Inventory-II score. Experiments based on a subset of the\nAudio-Visual Depressive Language Corpus, as used in the 2013 and 2014\nAudio/Visual Emotion Challenges, demonstrate the effectiveness of our\nproposed hybrid approach.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2396",
      "author_area_id": "3",
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "pan20d_interspeech": {
      "authors": [
        [
          "Yilin",
          "Pan"
        ],
        [
          "Bahman",
          "Mirheidari"
        ],
        [
          "Markus",
          "Reuber"
        ],
        [
          "Annalena",
          "Venneri"
        ],
        [
          "Daniel",
          "Blackburn"
        ],
        [
          "Heidi",
          "Christensen"
        ]
      ],
      "title": "Improving Detection of Alzheimer&#8217;s Disease Using Automatic Speech Recognition to Identify High-Quality Segments for More Robust Feature Extraction",
      "original": "2698",
      "page_count": 5,
      "order": 1009,
      "p1": "4961",
      "pn": "4965",
      "abstract": [
        "Speech and language based automatic dementia detection is of interest\ndue to it being non-invasive, low-cost and potentially able to aid\ndiagnosis accuracy. The collected data are mostly audio recordings\nof spoken language and these can be used directly for acoustic-based\nanalysis. To extract linguistic-based information, an automatic speech\nrecognition (ASR) system is used to generate transcriptions. However,\nthe extraction of reliable acoustic features is difficult when the\nacoustic quality of the data is poor as is the case with DementiaBank,\nthe largest opensource dataset for Alzheimer&#8217;s Disease classification.\nIn this paper, we explore how to improve the robustness of the acoustic\nfeature extraction by using time alignment information and confidence\nscores from the ASR system to identify audio segments of good quality.\nIn addition, we design rhythm-inspired features and combine them with\nacoustic features. By classifying the combined features with a bidirectional-LSTM\nattention network, the F-measure improves from 62.15% to 70.75% when\nonly the high-quality segments are used. Finally, we apply the same\napproach to our previously proposed hierarchical-based network using\nlinguistic-based features and show improvement from 74.37% to 77.25%.\nBy combining the acoustic and linguistic systems, a state-of-the-art\n78.34% F-measure is achieved on the DementiaBank task.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2698",
      "author_area_id": "3",
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "romana20_interspeech": {
      "authors": [
        [
          "Amrit",
          "Romana"
        ],
        [
          "John",
          "Bandon"
        ],
        [
          "Noelle",
          "Carlozzi"
        ],
        [
          "Angela",
          "Roberts"
        ],
        [
          "Emily Mower",
          "Provost"
        ]
      ],
      "title": "Classification of Manifest Huntington Disease Using Vowel Distortion Measures",
      "original": "2724",
      "page_count": 5,
      "order": 1010,
      "p1": "4966",
      "pn": "4970",
      "abstract": [
        "Huntington disease (HD) is a fatal autosomal dominant neurocognitive\ndisorder that causes cognitive disturbances, neuropsychiatric symptoms,\nand impaired motor abilities (e.g., gait, speech, voice). Due to its\nprogressive nature, HD treatment requires ongoing clinical monitoring\nof symptoms. Individuals with the Huntington gene mutation, which causes\nHD, may exhibit a range of speech symptoms as they progress from premanifest\nto manifest HD. Speech-based passive monitoring has the potential to\naugment clinical information by more continuously tracking manifestation\nsymptoms. Differentiating between premanifest and manifest HD is an\nimportant yet understudied problem, as this distinction marks the need\nfor increased treatment. In this work we present the first demonstration\nof how changes in speech can be measured to differentiate between premanifest\nand manifest HD. To do so, we focus on one speech symptom of HD: distorted\nvowels. We introduce a set of Filtered Vowel Distortion Measures (FVDM)\nwhich we extract from read speech. We show that FVDM, coupled with\nfeatures from existing literature, can differentiate between premanifest\nand manifest HD with 80% accuracy.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2724",
      "author_area_id": "3",
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "kadiri20_interspeech": {
      "authors": [
        [
          "Sudarsana Reddy",
          "Kadiri"
        ],
        [
          "Rashmi",
          "Kethireddy"
        ],
        [
          "Paavo",
          "Alku"
        ]
      ],
      "title": "Parkinson&#8217;s Disease Detection from Speech Using Single Frequency Filtering Cepstral Coefficients",
      "original": "3197",
      "page_count": 5,
      "order": 1011,
      "p1": "4971",
      "pn": "4975",
      "abstract": [
        "Parkinson&#8217;s disease (PD) is a progressive deterioration of the\nhuman central nervous system. Detection of PD (discriminating patients\nwith PD from healthy subjects) from speech is a useful approach due\nto its non-invasive nature. This study proposes to use novel cepstral\ncoefficients derived from the single frequency filtering (SFF) method,\ncalled as single frequency filtering cepstral coefficients (SFFCCs)\nfor the detection of PD. SFF has been shown to provide higher spectro-temporal\nresolution compared to the short-time Fourier transform. The current\nstudy uses the PC-GITA database, which consists of speech from speakers\nwith PD and healthy controls (50 males, 50 females). Our proposed detection\nsystem is based on the i-vectors derived from SFFCCs using SVM as a\nclassifier. In the detection of PD, better performance was achieved\nwhen the i-vectors were computed from the proposed SFFCCs compared\nto the popular conventional MFCCs. Furthermore, we investigated the\neffect of temporal variations by deriving the shifted delta cepstral\n(SDC) coefficients using SFFCCs. These experiments revealed that the\ni-vectors derived from the proposed SFFCCs+SDC features gave an absolute\nimprovement of 9% compared to the i-vectors derived from the baseline\nMFCCs+SDC features, indicating the importance of temporal variations\nin the detection of PD.\n"
      ],
      "doi": "10.21437/Interspeech.2020-3197",
      "author_area_id": "3",
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "quintas20_interspeech": {
      "authors": [
        [
          "Sebasti\u00e3o",
          "Quintas"
        ],
        [
          "Julie",
          "Mauclair"
        ],
        [
          "Virginie",
          "Woisard"
        ],
        [
          "Julien",
          "Pinquier"
        ]
      ],
      "title": "Automatic Prediction of Speech Intelligibility Based on X-Vectors in the Context of Head and Neck Cancer",
      "original": "1431",
      "page_count": 5,
      "order": 1012,
      "p1": "4976",
      "pn": "4980",
      "abstract": [
        "In the context of pathological speech, perceptual evaluation is still\nthe most widely used method for intelligibility estimation. Despite\nbeing considered a staple in clinical settings, it has a well-known\nsubjectivity associated with it, which results in greater variances\nand low reproducibility. On the other hand, due to the increasing computing\npower and latest research, automatic evaluation has become a growing\nalternative to perceptual assessments. In this paper we investigate\nan automatic prediction of speech intelligibility using the  x-vector\nparadigm, in the context of head and neck cancer. Experimental evaluation\nof the proposed model suggests a high correlation rate when applied\nto our corpus of HNC patients (p = 0.85). Our approach also displayed\nthe possibility of achieving very high correlation values (p = 0.95)\nwhen adapting the evaluation to each individual speaker, displaying\na significantly more accurate prediction whilst using smaller amounts\nof data. These results can also provide valuable insight to the redevelopment\nof test protocols, which typically tend to be substantial and effort-intensive\nfor patients.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1431",
      "author_area_id": "3",
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "abraham20_interspeech": {
      "authors": [
        [
          "Ajish K.",
          "Abraham"
        ],
        [
          "M.",
          "Pushpavathi"
        ],
        [
          "N.",
          "Sreedevi"
        ],
        [
          "A.",
          "Navya"
        ],
        [
          "C.M.",
          "Vikram"
        ],
        [
          "S.R. Mahadeva",
          "Prasanna"
        ]
      ],
      "title": "Spectral Moment and Duration of Burst of Plosives in Speech of Children with Hearing Impairment and Typically Developing Children &#8212; A Comparative Study",
      "original": "1805",
      "page_count": 5,
      "order": 1013,
      "p1": "4981",
      "pn": "4985",
      "abstract": [
        "Speech development in children with hearing impairment (CHI) is hampered\nby inadequate auditory input. Speech of CHI has reduced intelligibility\ncompared to typically developing children (TDC), mainly because of\narticulatory errors. Speech language pathologists (SLPs) assess these\nerrors through perceptual evaluation and accordingly device the protocol\nto correct them through several sessions of speech therapy. Automatic\nmethods need to be developed to reduce the time and enhance the accuracy\nof assessment. Acoustic measures of plosives may be utilized as valuable\ncues for automatic assessment.<br style=\"mso-data-placement:same-cell;\"\n/><br style=\"mso-data-placement:same-cell;\" />\n  The current study\nwas aimed to investigate the burst duration and spectral moment (centroid,\nskewness and kurtosis) of plosives in CHI in comparison with TDC. 24\nchildren in the age range of 5 to 8 years, divided into group I (13\nTDC) and group II (11 CHI) participated. Six words in Hindi embedded\nwith plosives (/p/, /b/, /&#x288;/, /&#x256;/, /k/, /&#609;/) in the\ninitial position were used as speech material.<br style=\"mso-data-placement:same-cell;\"\n/><br style=\"mso-data-placement:same-cell;\" />\n  Burst duration, spectral\ncentroid and skewness were found to be significantly different across\nthe groups for most of the plosives, whereas kurtosis was not. Results\nindicate that these measures except kurtosis are potential cues for\nautomatic assessment of articulatory errors.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1805",
      "author_area_id": "3",
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "perez20_interspeech": {
      "authors": [
        [
          "Matthew",
          "Perez"
        ],
        [
          "Zakaria",
          "Aldeneh"
        ],
        [
          "Emily Mower",
          "Provost"
        ]
      ],
      "title": "Aphasic Speech Recognition Using a Mixture of Speech Intelligibility Experts",
      "original": "2049",
      "page_count": 5,
      "order": 1014,
      "p1": "4986",
      "pn": "4990",
      "abstract": [
        "Robust speech recognition is a key prerequisite for semantic feature\nextraction in automatic aphasic speech analysis. However, standard\none-size-fits-all automatic speech recognition models perform poorly\nwhen applied to aphasic speech. One reason for this is the wide range\nof speech intelligibility due to different levels of severity (i.e.,\nhigher severity lends itself to less intelligible speech). To address\nthis, we propose a novel acoustic model based on a mixture of experts\n(MoE), which handles the varying intelligibility stages present in\naphasic speech by explicitly defining severity-based experts. At test\ntime, the contribution of each expert is decided by estimating speech\nintelligibility with a speech intelligibility detector (SID). We show\nthat our proposed approach significantly reduces phone error rates\nacross all severity stages in aphasic speech compared to a baseline\napproach that does not incorporate severity information into the modeling\nprocess.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2049",
      "author_area_id": "3",
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "kodrasi20_interspeech": {
      "authors": [
        [
          "Ina",
          "Kodrasi"
        ],
        [
          "Michaela",
          "Pernon"
        ],
        [
          "Marina",
          "Laganaro"
        ],
        [
          "Herv\u00e9",
          "Bourlard"
        ]
      ],
      "title": "Automatic Discrimination of Apraxia of Speech and Dysarthria Using a Minimalistic Set of Handcrafted Features",
      "original": "2253",
      "page_count": 5,
      "order": 1015,
      "p1": "4991",
      "pn": "4995",
      "abstract": [
        "To assist clinicians in the differential diagnosis and treatment of\nmotor speech disorders, it is imperative to establish objective tools\nwhich can reliably characterize different subtypes of disorders such\nas apraxia of speech (AoS) and dysarthria. Objective tools in the context\nof speech disorders typically rely on thousands of acoustic features,\nwhich raises the risk of difficulties in the interpretation of the\nunderlying mechanisms, over-adaptation to training data, and weak generalization\ncapabilities to test data. Seeking to use a small number of acoustic\nfeatures and motivated by the clinical-perceptual signs used for the\ndifferential diagnosis of AoS and dysarthria, we propose to characterize\ndifferences between AoS and dysarthria using only six handcrafted acoustic\nfeatures, with three features reflecting segmental distortions, two\nfeatures reflecting loudness and hypernasality, and one feature reflecting\nsyllabification. These three different sets of features are used to\nseparately train three classifiers. At test time, the decisions of\nthe three classifiers are combined through a simple majority voting\nscheme. Preliminary results show that the proposed approach achieves\na discrimination accuracy of 90%, outperforming using state-of-the-art\nfeatures such as openSMILE which yield a discrimination accuracy of\n65%.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2253",
      "author_area_id": "3",
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "shi20i_interspeech": {
      "authors": [
        [
          "Yangyang",
          "Shi"
        ],
        [
          "Yongqiang",
          "Wang"
        ],
        [
          "Chunyang",
          "Wu"
        ],
        [
          "Christian",
          "Fuegen"
        ],
        [
          "Frank",
          "Zhang"
        ],
        [
          "Duc",
          "Le"
        ],
        [
          "Ching-Feng",
          "Yeh"
        ],
        [
          "Michael L.",
          "Seltzer"
        ]
      ],
      "title": "Weak-Attention Suppression for Transformer Based Speech Recognition",
      "original": "1363",
      "page_count": 5,
      "order": 1016,
      "p1": "4996",
      "pn": "5000",
      "abstract": [
        "Transformers, originally proposed for natural language processing (NLP)\ntasks, have recently achieved great success in automatic speech recognition\n(ASR). However, adjacent acoustic units (i.e., frames) are highly correlated,\nand long-distance dependencies between them are weak, unlike text units.\nIt suggests that ASR will likely benefit from sparse and localized\nattention. In this paper, we propose Weak-Attention Suppression (WAS),\na method that dynamically induces sparsity in attention probabilities.\nWe demonstrate that WAS leads to consistent Word Error Rate (WER) improvement\nover strong transformer baselines. On the widely used LibriSpeech benchmark,\nour proposed method reduced WER by 10% on  test-clean and 5% on  test-other\nfor streamable transformers, resulting in a new state-of-the-art among\nstreaming models. Further analysis shows that WAS learns to suppress\nattention of non-critical and redundant continuous acoustic frames,\nand is more likely to suppress past frames rather than future ones.\nIt indicates the importance of lookahead in attention-based ASR models.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1363",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "huang20k_interspeech": {
      "authors": [
        [
          "Wenyong",
          "Huang"
        ],
        [
          "Wenchao",
          "Hu"
        ],
        [
          "Yu Ting",
          "Yeung"
        ],
        [
          "Xiao",
          "Chen"
        ]
      ],
      "title": "Conv-Transformer Transducer: Low Latency, Low Frame Rate, Streamable End-to-End Speech Recognition",
      "original": "2361",
      "page_count": 5,
      "order": 1017,
      "p1": "5001",
      "pn": "5005",
      "abstract": [
        "Transformer has achieved competitive performance against state-of-the-art\nend-to-end models in automatic speech recognition (ASR), and requires\nsignificantly less training time than RNN-based models. The original\nTransformer, with encoder-decoder architecture, is only suitable for\noffline ASR. It relies on an attention mechanism to learn alignments,\nand encodes input audio bidirectionally. The high computation cost\nof Transformer decoding also limits its use in production streaming\nsystems. To make Transformer suitable for streaming ASR, we explore\nTransducer framework as a streamable way to learn alignments. For audio\nencoding, we apply unidirectional Transformer with interleaved convolution\nlayers. The interleaved convolution layers are used for modeling future\ncontext which is important to performance. To reduce computation cost,\nwe gradually downsample acoustic input, also with the interleaved convolution\nlayers. Moreover, we limit the length of history context in self-attention\nto maintain constant computation cost for each decoding step. We show\nthat this architecture, named Conv-Transformer Transducer, achieves\ncompetitive performance on LibriSpeech dataset (3.6% WER on test-clean)\nwithout external language models. The performance is comparable to\npreviously published streamable Transformer Transducer and strong hybrid\nstreaming ASR systems, and is achieved with smaller look-ahead window\n(140 ms), fewer parameters and lower frame rate.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2361",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "li20na_interspeech": {
      "authors": [
        [
          "Song",
          "Li"
        ],
        [
          "Lin",
          "Li"
        ],
        [
          "Qingyang",
          "Hong"
        ],
        [
          "Lingling",
          "Liu"
        ]
      ],
      "title": "Improving Transformer-Based Speech Recognition with Unsupervised Pre-Training and Multi-Task Semantic Knowledge Learning",
      "original": "2007",
      "page_count": 5,
      "order": 1018,
      "p1": "5006",
      "pn": "5010",
      "abstract": [
        "Recently, the Transformer-based end-to-end speech recognition system\nhas become a state-of-the-art technology. However, one prominent problem\nwith current end-to-end speech recognition systems is that an extensive\namount of paired data are required to achieve better recognition performance.\nIn order to grapple with such an issue, we propose two unsupervised\npre-training strategies for the encoder and the decoder of Transformer\nrespectively, which make full use of unpaired data for training. In\naddition, we propose a new semi-supervised fine-tuning method named\nmulti-task semantic knowledge learning to strengthen the Transformer&#8217;s\nability to learn about semantic knowledge, thereby improving the system\nperformance. We achieve the best CER with our proposed methods on AISHELL-1\ntest set: 5.9%, which exceeds the best end-to-end model by 10.6% relative\nCER. Moreover, relative CER reduction of 20.3% and 17.8% are obtained\nfor low-resource Mandarin and English data sets, respectively.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2007",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "hori20_interspeech": {
      "authors": [
        [
          "Takaaki",
          "Hori"
        ],
        [
          "Niko",
          "Moritz"
        ],
        [
          "Chiori",
          "Hori"
        ],
        [
          "Jonathan Le",
          "Roux"
        ]
      ],
      "title": "Transformer-Based Long-Context End-to-End Speech Recognition",
      "original": "2928",
      "page_count": 5,
      "order": 1019,
      "p1": "5011",
      "pn": "5015",
      "abstract": [
        "This paper presents an approach to long-context end-to-end automatic\nspeech recognition (ASR) using Transformers, aiming at improving ASR\naccuracy for long audio recordings such as lecture and conversational\nspeeches. Most end-to-end ASR systems are basically designed to recognize\nindependent utterances, but contextual information (e.g., speaker or\ntopic) over multiple utterances is known to be useful for ASR. There\nare some prior studies on RNN-based models that utilize such contextual\ninformation, but very few on Transformers, which are becoming more\npopular in end-to-end ASR. In this paper, we propose a Transformer-based\narchitecture that accepts multiple consecutive utterances at the same\ntime and predicts an output sequence for the last utterance. This is\nrepeated in a sliding-window fashion with one-utterance shifts to recognize\nthe entire recording. Based on this framework, we also investigate\nhow to design the context window and train the model effectively in\nmonologue (one speaker) and dialogue (two speakers) scenarios. We demonstrate\nthe effectiveness of our approach using monologue benchmarks on CSJ\nand TED-LIUM3 and dialogue benchmarks on SWITCHBOARD and HKUST, showing\nsignificant error reduction from single-utterance ASR baselines with\nor without speaker i-vectors.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2928",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "zhou20i_interspeech": {
      "authors": [
        [
          "Xinyuan",
          "Zhou"
        ],
        [
          "Grandee",
          "Lee"
        ],
        [
          "Emre",
          "Y\u0131lmaz"
        ],
        [
          "Yanhua",
          "Long"
        ],
        [
          "Jiaen",
          "Liang"
        ],
        [
          "Haizhou",
          "Li"
        ]
      ],
      "title": "Self-and-Mixed Attention Decoder with Deep Acoustic Structure for Transformer-Based LVCSR",
      "original": "2556",
      "page_count": 5,
      "order": 1020,
      "p1": "5016",
      "pn": "5020",
      "abstract": [
        "Transformer has shown impressive performance in automatic speech recognition.\nIt uses an encoder-decoder structure with self-attention to learn the\nrelationship between high-level representation of source inputs and\nembedding of target outputs. In this paper, we propose a novel decoder\nstructure that features a self-and-mixed attention decoder (SMAD) with\na deep acoustic structure (DAS) to improve the acoustic representation\nof Transformer-based LVCSR. Specifically, we introduce a self-attention\nmechanism to learn a multi-layer deep acoustic structure for multiple\nlevels of acoustic abstraction. We also design a mixed attention mechanism\nthat learns the alignment between different levels of acoustic abstraction\nand its corresponding linguistic information simultaneously in a shared\nembedding space. The ASR experiments on Aishell-1 show that the proposed\nstructure achieves CERs of 4.8% on the dev set and 5.1% on the test\nset, which are the best reported results on this task to the best of\nour knowledge.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2556",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "zhao20i_interspeech": {
      "authors": [
        [
          "Yingzhu",
          "Zhao"
        ],
        [
          "Chongjia",
          "Ni"
        ],
        [
          "Cheung-Chi",
          "Leung"
        ],
        [
          "Shafiq",
          "Joty"
        ],
        [
          "Eng Siong",
          "Chng"
        ],
        [
          "Bin",
          "Ma"
        ]
      ],
      "title": "Universal Speech Transformer",
      "original": "1716",
      "page_count": 5,
      "order": 1021,
      "p1": "5021",
      "pn": "5025",
      "abstract": [
        "Transformer model has made great progress in speech recognition. However,\ncompared with models with iterative computation, transformer model\nhas fixed encoder and decoder depth, thus losing the recurrent inductive\nbias. Besides, finding the optimal number of layers involves trial-and-error\nattempts. In this paper, the universal speech transformer is proposed,\nwhich to the best of our knowledge, is the first work to use universal\ntransformer for speech recognition. It generalizes the speech transformer\nwith dynamic numbers of encoder/decoder layers, which can relieve the\nburden of tuning depth related hyperparameters. Universal transformer\nadds the depth and positional embeddings repeatedly for each layer,\nwhich dilutes the acoustic information carried by hidden representation,\nand it also performs a partial update of hidden vectors between layers,\nwhich is less efficient especially on the very deep models. For better\nuse of universal transformer, we modify its processing framework by\nremoving the depth embedding and only adding the positional embedding\nonce at transformer encoder frontend. Furthermore, to update the hidden\nvectors efficiently, especially on the very deep models, we adopt a\nfull update. Experiments on LibriSpeech, Switchboard and AISHELL-1\ndatasets show that our model outperforms a baseline by 3.88%&#8211;13.7%,\nand surpasses other model with less computation cost.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1716",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "tian20c_interspeech": {
      "authors": [
        [
          "Zhengkun",
          "Tian"
        ],
        [
          "Jiangyan",
          "Yi"
        ],
        [
          "Jianhua",
          "Tao"
        ],
        [
          "Ye",
          "Bai"
        ],
        [
          "Shuai",
          "Zhang"
        ],
        [
          "Zhengqi",
          "Wen"
        ]
      ],
      "title": "Spike-Triggered Non-Autoregressive Transformer for End-to-End Speech Recognition",
      "original": "2086",
      "page_count": 5,
      "order": 1022,
      "p1": "5026",
      "pn": "5030",
      "abstract": [
        "Non-autoregressive transformer models have achieved extremely fast\ninference speed and comparable performance with autoregressive sequence-to-sequence\nmodels in neural machine translation. Most of the non-autoregressive\ntransformers decode the target sequence from a predefined-length mask\nsequence. If the predefined length is too long, it will cause a lot\nof redundant calculations. If the predefined length is shorter than\nthe length of the target sequence, it will hurt the performance of\nthe model. To address this problem and improve the inference speed,\nwe propose a spike-triggered non-autoregressive transformer model for\nend-to-end speech recognition, which introduces a CTC module to predict\nthe length of the target sequence and accelerate the convergence. All\nthe experiments are conducted on a public Chinese mandarin dataset\nAISHELL-1. The results show that the proposed model can accurately\npredict the length of the target sequence and achieve a competitive\nperformance with the advanced transformers. What&#8217;s more, the\nmodel even achieves a real-time factor of 0.0056, which exceeds all\nmainstream speech recognition models.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2086",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "zhao20j_interspeech": {
      "authors": [
        [
          "Yingzhu",
          "Zhao"
        ],
        [
          "Chongjia",
          "Ni"
        ],
        [
          "Cheung-Chi",
          "Leung"
        ],
        [
          "Shafiq",
          "Joty"
        ],
        [
          "Eng Siong",
          "Chng"
        ],
        [
          "Bin",
          "Ma"
        ]
      ],
      "title": "Cross Attention with Monotonic Alignment for Speech Transformer",
      "original": "1198",
      "page_count": 5,
      "order": 1023,
      "p1": "5031",
      "pn": "5035",
      "abstract": [
        "Transformer, a state-of-the-art neural network architecture, has been\nused successfully for different sequence-to-sequence transformation\ntasks. This model architecture disperses the attention distribution\nover entire input to learn long-term dependencies, which is important\nfor some sequence-to-sequence tasks, such as neural machine translation\nand text summarization. However, automatic speech recognition (ASR)\nhas a characteristic to have monotonic alignment between text output\nand speech input. Techniques like Connectionist Temporal Classification\n(CTC), RNN Transducer (RNN-T) and Recurrent Neural Aligner (RNA) build\non top of this monotonic alignment and use local encoded speech representations\nfor corresponding token prediction. In this paper, we present an effective\ncross attention biasing technique in transformer that takes monotonic\nalignment between text output and speech input into consideration by\nmaking use of cross attention weights. Specifically, a Gaussian mask\nis applied on cross attention weights to limit the input speech context\nrange locally given alignment information. We further introduce a regularizer\nfor alignment regularization. Experiments on LibriSpeech dataset find\nthat our proposed model can obtain improved output-input alignment\nfor ASR, and yields 14.5%&#8211;25.0% relative word error rate (WER)\nreductions.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1198",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "gulati20_interspeech": {
      "authors": [
        [
          "Anmol",
          "Gulati"
        ],
        [
          "James",
          "Qin"
        ],
        [
          "Chung-Cheng",
          "Chiu"
        ],
        [
          "Niki",
          "Parmar"
        ],
        [
          "Yu",
          "Zhang"
        ],
        [
          "Jiahui",
          "Yu"
        ],
        [
          "Wei",
          "Han"
        ],
        [
          "Shibo",
          "Wang"
        ],
        [
          "Zhengdong",
          "Zhang"
        ],
        [
          "Yonghui",
          "Wu"
        ],
        [
          "Ruoming",
          "Pang"
        ]
      ],
      "title": "Conformer: Convolution-augmented Transformer for Speech Recognition",
      "original": "3015",
      "page_count": 5,
      "order": 1024,
      "p1": "5036",
      "pn": "5040",
      "abstract": [
        "Recently Transformer and Convolution neural network (CNN) based models\nhave shown promising results in Automatic Speech Recognition (ASR),\noutperforming Recurrent neural networks (RNNs). Transformer models\nare good at capturing content-based global interactions, while CNNs\nexploit local features effectively. In this work, we achieve the best\nof both worlds by studying how to combine convolution neural networks\nand transformers to model both local and global dependencies of an\naudio sequence in a parameter-efficient way. To this regard, we propose\nthe convolution-augmented transformer for speech recognition, named\n Conformer.  Conformer significantly outperforms the previous Transformer\nand CNN based models achieving state-of-the-art accuracies. On the\nwidely used LibriSpeech benchmark, our model achieves WER of 2.1%/4.3%\nwithout using a language model and 1.9%/3.9% with an external language\nmodel on test/testother. We also observe competitive performance of\n2.7%/6.3% with a small model of only 10M parameters.\n"
      ],
      "doi": "10.21437/Interspeech.2020-3015",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "lu20g_interspeech": {
      "authors": [
        [
          "Liang",
          "Lu"
        ],
        [
          "Changliang",
          "Liu"
        ],
        [
          "Jinyu",
          "Li"
        ],
        [
          "Yifan",
          "Gong"
        ]
      ],
      "title": "Exploring Transformers for Large-Scale Speech Recognition",
      "original": "2638",
      "page_count": 5,
      "order": 1025,
      "p1": "5041",
      "pn": "5045",
      "abstract": [
        "While recurrent neural networks still largely define state-of-the-art\nspeech recognition systems, the Transformer network has been proven\nto be a competitive alternative, especially in the offline condition.\nMost studies with Transformers have been constrained in a relatively\nsmall scale setting, and some forms of data argumentation approaches\nare usually applied to combat the data sparsity issue. In this paper,\nwe aim at understanding the behaviors of Transformers in the large-scale\nspeech recognition setting, where we have used around 65,000 hours\nof training data. We investigated various aspects on scaling up Transformers,\nincluding model initialization, warmup training as well as different\nLayer Normalization strategies. In the streaming condition, we compared\nthe widely used attention mask based future context lookahead approach\nto the Transformer-XL network. From our experiments, we show that Transformers\ncan achieve around 6% relative word error rate (WER) reduction compared\nto the BLSTM baseline in the offline fashion, while in the streaming\nfashion, Transformer-XL is comparable to LC-BLSTM with 800 millisecond\nlatency constraint.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2638",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "togami20_interspeech": {
      "authors": [
        [
          "Masahito",
          "Togami"
        ],
        [
          "Robin",
          "Scheibler"
        ]
      ],
      "title": "Sparseness-Aware DOA Estimation with Majorization Minimization",
      "original": "1168",
      "page_count": 5,
      "order": 1026,
      "p1": "5046",
      "pn": "5050",
      "abstract": [
        "We propose a direction-of-arrival (DOA) estimation technique which\nassumes that speech sources are sufficiently sparse and there is only\none active speech source at each time-frequency (T-F) point. The proposed\nmethod estimates the DOA of the active speech source at each T-F point.\nA typical way for DOA estimation is based on grid-searching for all\npossible directions. However, computational cost of grid-searching\nis proportional to the resolution of search area. Instead of accurate\ngrid-searching, the proposed method adopts rough grid-searching followed\nby an iterative parameter optimization based on Majorization-Minimization\n(MM) algorithm. We propose a parameter optimization method which guarantees\na monotonical increase of the objective function. Experimental results\nshow that the proposed method estimates DOAs of speech sources more\naccurately than conventional DOA estimation methods when computational\ncost of each method is almost the same.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1168",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "zhong20c_interspeech": {
      "authors": [
        [
          "Xiaoli",
          "Zhong"
        ],
        [
          "Hao",
          "Song"
        ],
        [
          "Xuejie",
          "Liu"
        ]
      ],
      "title": "Spatial Resolution of Early Reflection for Speech and White Noise",
      "original": "1220",
      "page_count": 5,
      "order": 1027,
      "p1": "5051",
      "pn": "5055",
      "abstract": [
        "In virtual auditory display, the accurate simulation of early reflection\nis helpful to guarantee audio fidelity and enhance immersion. However,\nthe early reflection may not be easily distinguished from the direct\nsound due to the masking effect. This work investigated the spatial\nresolution of early reflection for speech and white noise under different\nconditions, in which three-down-one-up adaptive strategy with three-interval-three-alternative\nforced-choice (3I-3AFC) was employed. Results show that, for both speech\nand white noise, the spatial resolution of early reflection decreases\nwith the increasing deviation of reflection orientation relative to\nthe direct sound, and has no relationship with the time delay; Moreover,\nthe spatial resolution of early reflection for speech is always lower\nthan that for white noise under the same condition.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1220",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "raikar20_interspeech": {
      "authors": [
        [
          "Aditya",
          "Raikar"
        ],
        [
          "Karan",
          "Nathwani"
        ],
        [
          "Ashish",
          "Panda"
        ],
        [
          "Sunil Kumar",
          "Kopparapu"
        ]
      ],
      "title": "Effect of Microphone Position Measurement Error on RIR and its Impact on Speech Intelligibility and Quality",
      "original": "1578",
      "page_count": 5,
      "order": 1028,
      "p1": "5056",
      "pn": "5060",
      "abstract": [
        "Room Impulse Response (RIR) measurement is done using a microphone\nand loudspeaker pair and is prone to error due to error in measurement\nof the microphone position. In literature, the adverse impact of ambient\nnoise on RIR measurement is mostly explored. However, the impact of\nmicrophone position error on full, early and late RIR measurements\nhave never been explored. In this paper, we investigate the error in\nRIR introduced due to error in measurement of the microphone position.\nWe also study the impact of this on the quality and intelligibility\nof speech. Our analysis shows that the impact of error in microphone\nposition measurement on RIR is as adverse as that of the ambient noise.\n"
      ],
      "doi": "10.21437/Interspeech.2020-1578",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "deng20c_interspeech": {
      "authors": [
        [
          "Shuwen",
          "Deng"
        ],
        [
          "Wolfgang",
          "Mack"
        ],
        [
          "Emanu\u00ebl A.P.",
          "Habets"
        ]
      ],
      "title": "Online Blind Reverberation Time Estimation Using CRNNs",
      "original": "2156",
      "page_count": 5,
      "order": 1029,
      "p1": "5061",
      "pn": "5065",
      "abstract": [
        "The reverberation time, T<SUB>60</SUB>, is an important acoustic parameter\nin speech and acoustic signal processing. Often, the T<SUB>60</SUB>\nis unknown and blind estimation from a single-channel measurement is\nrequired. State-of-the-art T<SUB>60</SUB> estimation is achieved by\na convolutional neural network (CNN) which maps a feature representation\nof the speech to the T<SUB>60</SUB>. The temporal input length of the\nCNN is fixed. Time-varying scenarios, e.g., robot audition, require\ncontinuous T<SUB>60</SUB> estimation in an online fashion, which is\ncomputationally heavy using the CNN. We propose to use a convolutional\nrecurrent neural network (CRNN) for blind T<SUB>60</SUB> estimation\nas it combines the parametric efficiency of CNNs with the online estimation\nof recurrent neural networks and, in contrast to CNNs, can process\ntime-sequences of variable length. We evaluated the proposed CRNN on\nthe  Acoustic Characterization of Environments Challenge dataset for\ndifferent input lengths. Our proposed method outperforms the state-of-the-art\nCNN approach even for shorter inputs at the cost of more trainable\nparameters.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2156",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "mack20_interspeech": {
      "authors": [
        [
          "Wolfgang",
          "Mack"
        ],
        [
          "Shuwen",
          "Deng"
        ],
        [
          "Emanu\u00ebl A.P.",
          "Habets"
        ]
      ],
      "title": "Single-Channel Blind Direct-to-Reverberation Ratio Estimation Using Masking",
      "original": "2171",
      "page_count": 5,
      "order": 1030,
      "p1": "5066",
      "pn": "5070",
      "abstract": [
        "Acoustic parameters, like the direct-to-reverberation ratio (DRR),\ncan be used in audio processing algorithms to perform, e.g., dereverberation\nor in audio augmented reality. Often, the DRR is not available and\nhas to be estimated blindly from recorded audio signals. State-of-the-art\nDRR estimation is achieved by deep neural networks (DNNs), which directly\nmap a feature representation of the acquired signals to the DRR. Motivated\nby the equality of the signal-to-reverberation ratio and the (channel-based)\nDRR under certain conditions, we formulate single-channel DRR estimation\nas an extraction task of two signal components from the recorded audio.\nThe DRR can be obtained by inserting the estimated signals in the definition\nof the DRR. The extraction is performed using time-frequency masks.\nThe masks are estimated by a DNN trained end-to-end to minimize the\nmean-squared error between the estimated and the oracle DRR. We conduct\nexperiments with different pre-processing and mask estimation schemes.\nThe proposed method outperforms state-of-the-art single- and multi-channel\nmethods on the ACE challenge data corpus.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2171",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "beiton20_interspeech": {
      "authors": [
        [
          "Hanan",
          "Beit-On"
        ],
        [
          "Vladimir",
          "Tourbabin"
        ],
        [
          "Boaz",
          "Rafaely"
        ]
      ],
      "title": "The Importance of Time-Frequency Averaging for Binaural Speaker Localization in Reverberant Environments",
      "original": "2256",
      "page_count": 5,
      "order": 1031,
      "p1": "5071",
      "pn": "5075",
      "abstract": [
        "A common approach to overcoming the effect of reverberation in speaker\nlocalization is to identify the time-frequency (TF) bins in which the\ndirect path is dominant, and then to use only these bins for estimation.\nVarious direct-path dominance (DPD) tests have been proposed for identifying\nthe direct-path bins. However, for a two-microphone binaural array,\ntests that do not employ averaging over TF bins seem to fail. In this\npaper, this anomaly is studied by comparing two DPD tests, in which\nonly one has been designed to employ averaging over TF bins. An analysis\nof these tests shows that, in the binaural case, a TF bin that is dominated\nby multiple reflections may be similar to a bin with a single source.\nThis insight can explain the high false alarm rate encountered with\ntests that do not employ averaging. Also, it is shown that incorporating\naveraging over TF bins can reduce the false alarm rate. A simulation\nstudy is presented that verifies the importance of TF averaging for\na reliable selection of direct-path bins in the binaural case.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2256",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "hu20j_interspeech": {
      "authors": [
        [
          "Yonggang",
          "Hu"
        ],
        [
          "Prasanga N.",
          "Samarasinghe"
        ],
        [
          "Thushara D.",
          "Abhayapala"
        ]
      ],
      "title": "Acoustic Signal Enhancement Using Relative Harmonic Coefficients: Spherical Harmonics Domain Approach",
      "original": "2316",
      "page_count": 5,
      "order": 1032,
      "p1": "5076",
      "pn": "5080",
      "abstract": [
        "Over recent years, spatial acoustic signal processing using higher\norder microphone arrays in the spherical harmonics domain has been\na popular research topic. This paper uses a recently introduced source\nfeature called the  relative harmonic coefficients to develop an acoustic\nsignal enhancement approach in noisy environments. This proposed method\nenables to extract the clean spherical harmonic coefficients from noisy\nhigher order microphone recordings. Hence, this technique can be used\nas a pre-processing tool for noise-free measurements required by many\nspatial audio applications. We finally present a simulation study analyzing\nthe performance of this approach in far field noisy environments.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2316",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "murthy20_interspeech": {
      "authors": [
        [
          "B.H.V.S. Narayana",
          "Murthy"
        ],
        [
          "J.V.",
          "Satyanarayana"
        ],
        [
          "Nivedita",
          "Chennupati"
        ],
        [
          "B.",
          "Yegnanarayana"
        ]
      ],
      "title": "Instantaneous Time Delay Estimation of Broadband Signals",
      "original": "2462",
      "page_count": 5,
      "order": 1033,
      "p1": "5081",
      "pn": "5085",
      "abstract": [
        "This paper presents a method of obtaining the instantaneous time delay\nof broadband signals collected at two spatially separated microphones\nin a live room. The method is based on using the complex signals at\nthe output of single frequency filtering (SFF) of the microphone signals.\nWe show that the complex SFF spectrum at each instant can be used to\nobtain the instantaneous time delay (TD). By using only the phase of\nthe SFF spectrum, it is possible to get a better estimate of the TD,\nas in the case of the standard GCC-PHAT method. We show the effectiveness\nof the proposed method for real microphone signals collected in a live\nroom. Robustness of the method is tested for additive babble noise\nat 0 dB for the live microphone data. Since we get the TD at every\nsampling instant, it may be possible to exploit this feature for two-channel\nmulti-speaker separation and for tracking a moving speaker.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2462",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "wang20ja_interspeech": {
      "authors": [
        [
          "Hao",
          "Wang"
        ],
        [
          "Kai",
          "Chen"
        ],
        [
          "Jing",
          "Lu"
        ]
      ],
      "title": "U-Net Based Direct-Path Dominance Test for Robust Direction-of-Arrival Estimation",
      "original": "2493",
      "page_count": 5,
      "order": 1034,
      "p1": "5086",
      "pn": "5090",
      "abstract": [
        "It has been noted that the identification of the time-frequency bins\ndominated by the contribution from the direct propagation of the target\nspeaker can significantly improve the robustness of the direction-of-arrival\nestimation. However, the correct extraction of the direct-path sound\nis challenging especially in adverse environments. In this paper, a\nU-net based direct-path dominance test method is proposed. Exploiting\nthe efficient segmentation capability of the U-net architecture, the\ndirect-path information can be effectively retrieved from a dedicated\nmulti-task neural network. Moreover, the training and inference of\nthe neural network only need the input of a single microphone, circumventing\nthe problem of array-structure dependence faced by common end-to-end\ndeep learning based methods. Simulations demonstrate that significantly\nhigher estimation accuracy can be achieved in high reverberant and\nlow signal-to-noise ratio environments.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2493",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "xue20b_interspeech": {
      "authors": [
        [
          "Wei",
          "Xue"
        ],
        [
          "Ying",
          "Tong"
        ],
        [
          "Chao",
          "Zhang"
        ],
        [
          "Guohong",
          "Ding"
        ],
        [
          "Xiaodong",
          "He"
        ],
        [
          "Bowen",
          "Zhou"
        ]
      ],
      "title": "Sound Event Localization and Detection Based on Multiple DOA Beamforming and Multi-Task Learning",
      "original": "2759",
      "page_count": 5,
      "order": 1035,
      "p1": "5091",
      "pn": "5095",
      "abstract": [
        "The performance of sound event localization and detection (SELD) degrades\nin source-overlapping cases since features of different sources collapse\nwith each other, and the network tends to fail to learn to separate\nthese features effectively. In this paper, by leveraging the conventional\nmicrophone array signal processing to generate comprehensive representations\nfor SELD, we propose a new SELD method based on multiple direction\nof arrival (DOA) beamforming and multi-task learning. By using multiple\nbeamformers to extract the signals from different DOAs, the sound field\nis more diversely described, and specialised representations of target\nsource and noises can be obtained. With labelled training data, the\nsteering vector is estimated based on the cross-power spectra (CPS)\nand the signal presence probability (SPP), which eliminates the need\nof knowing the array geometry. We design two networks for sound event\nlocalization (SED) and sound source localization (SSL) and use a multi-task\nlearning scheme for SED, in which the SSL-related task act as a regularization.\nExperimental results using the database of DCASE2019 SELD task show\nthat the proposed method achieves the state-of-art performance.\n"
      ],
      "doi": "10.21437/Interspeech.2020-2759",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    }
  },
  "sessions": [
    {
      "title": "Keynote 1",
      "papers": [
        "pierrehumbert20_interspeech"
      ]
    },
    {
      "title": "ASR Neural Network Architectures I",
      "papers": [
        "li20_interspeech",
        "gao20_interspeech",
        "jain20_interspeech",
        "pan20_interspeech",
        "kadetotad20_interspeech",
        "lohrenz20_interspeech",
        "pham20_interspeech",
        "kanda20_interspeech",
        "fukuda20_interspeech",
        "park20_interspeech"
      ]
    },
    {
      "title": "Multi-Channel Speech Enhancement",
      "papers": [
        "li20b_interspeech",
        "xu20_interspeech",
        "li20c_interspeech",
        "yu20_interspeech",
        "huang20_interspeech",
        "qi20_interspeech",
        "wu20_interspeech",
        "nakagome20_interspeech",
        "nakatani20_interspeech",
        "tu20_interspeech"
      ]
    },
    {
      "title": "Speech Processing in the Brain",
      "papers": [
        "youssef20_interspeech",
        "zhou20_interspeech",
        "lian20_interspeech",
        "fu20_interspeech",
        "wang20_interspeech",
        "zhao20_interspeech",
        "bosch20_interspeech",
        "talkar20_interspeech"
      ]
    },
    {
      "title": "Speech Signal Representation",
      "papers": [
        "shor20_interspeech",
        "rajan20_interspeech",
        "dai20_interspeech",
        "hu20_interspeech",
        "gresse20_interspeech",
        "yegnanarayana20_interspeech",
        "gump20_interspeech",
        "tran20_interspeech",
        "setlur20_interspeech",
        "takeuchi20_interspeech"
      ]
    },
    {
      "title": "Speech Synthesis: Neural Waveform Generation I",
      "papers": [
        "ai20_interspeech",
        "tian20_interspeech",
        "yang20_interspeech",
        "kanagawa20_interspeech",
        "hsu20_interspeech",
        "stephenson20_interspeech",
        "popov20_interspeech",
        "song20_interspeech",
        "maguer20_interspeech",
        "paul20_interspeech",
        "liu20_interspeech"
      ]
    },
    {
      "title": "Automatic Speech Recognition for Non-Native Children&#8217;s Speech",
      "papers": [
        "gretter20_interspeech",
        "lo20_interspeech",
        "knill20_interspeech",
        "kathania20_interspeech",
        "shahin20_interspeech"
      ]
    },
    {
      "title": "Speaker Diarization",
      "papers": [
        "horiguchi20_interspeech",
        "medennikov20_interspeech",
        "aronowitz20_interspeech",
        "lin20_interspeech",
        "wang20b_interspeech",
        "singh20_interspeech",
        "chung20_interspeech"
      ]
    },
    {
      "title": "Noise Robust and Distant Speech Recognition",
      "papers": [
        "zhang20_interspeech",
        "du20_interspeech",
        "bruguier20_interspeech",
        "andrusenko20_interspeech",
        "zhang20b_interspeech",
        "qiu20_interspeech",
        "chen20_interspeech",
        "wang20c_interspeech",
        "horiguchi20b_interspeech",
        "deadman20_interspeech"
      ]
    },
    {
      "title": "Speech in Multimodality",
      "papers": [
        "botelho20_interspeech",
        "zhang20c_interspeech",
        "pan20b_interspeech",
        "shen20_interspeech",
        "chen20b_interspeech",
        "liu20b_interspeech",
        "khare20_interspeech",
        "li20d_interspeech",
        "lian20b_interspeech"
      ]
    },
    {
      "title": "Speech, Language, and Multimodal Resources",
      "papers": [
        "yang20b_interspeech",
        "gutkin20_interspeech",
        "ha20_interspeech",
        "wang20d_interspeech",
        "ramanarayanan20_interspeech",
        "ng20_interspeech",
        "leino20_interspeech",
        "segbroeck20_interspeech",
        "wang20e_interspeech",
        "kirkedal20_interspeech"
      ]
    },
    {
      "title": "Language Recognition",
      "papers": [
        "duroselle20_interspeech",
        "li20e_interspeech",
        "li20f_interspeech",
        "chowdhury20_interspeech",
        "lindgren20_interspeech",
        "alvarez20_interspeech",
        "abdullah20_interspeech"
      ]
    },
    {
      "title": "Speech Processing and Analysis",
      "papers": [
        "tits20_interspeech",
        "hu20b_interspeech",
        "kelly20_interspeech",
        "kelly20b_interspeech",
        "kulebi20_interspeech",
        "ramanarayanan20b_interspeech",
        "lin20b_interspeech"
      ]
    },
    {
      "title": "Speech Emotion Recognition I",
      "papers": [
        "ren20_interspeech",
        "feng20_interspeech",
        "su20_interspeech",
        "mallolragolta20_interspeech",
        "sridhar20_interspeech",
        "latif20_interspeech",
        "dissanayake20_interspeech",
        "mao20_interspeech",
        "yeh20_interspeech"
      ]
    },
    {
      "title": "ASR Neural Network Architectures and Training I",
      "papers": [
        "kumar20_interspeech",
        "moriya20_interspeech",
        "tuske20_interspeech",
        "chen20c_interspeech",
        "shao20_interspeech",
        "an20_interspeech",
        "inaguma20_interspeech",
        "houston20_interspeech",
        "song20b_interspeech"
      ]
    },
    {
      "title": "Evaluation of Speech Technology Systems and Methods for Resource Construction and Annotation",
      "papers": [
        "stan20_interspeech",
        "shangguan20_interspeech",
        "liu20c_interspeech",
        "ramakrishna20_interspeech",
        "fan20_interspeech",
        "woodward20_interspeech",
        "ali20_interspeech",
        "ludusan20_interspeech",
        "martin20_interspeech"
      ]
    },
    {
      "title": "Phonetics and Phonology",
      "papers": [
        "zellou20_interspeech",
        "lorin20_interspeech",
        "luo20_interspeech",
        "yue20_interspeech",
        "wang20f_interspeech",
        "zellers20_interspeech",
        "huang20b_interspeech",
        "li20g_interspeech",
        "wilkins20_interspeech"
      ]
    },
    {
      "title": "Topics in ASR I",
      "papers": [
        "li20h_interspeech",
        "lu20_interspeech",
        "caseiro20_interspeech",
        "mao20b_interspeech",
        "geng20_interspeech",
        "wei20_interspeech",
        "barbera20_interspeech",
        "liu20d_interspeech",
        "lin20c_interspeech",
        "yi20_interspeech"
      ]
    },
    {
      "title": "Large-Scale Evaluation of Short-Duration Speaker Verification",
      "papers": [
        "chen20d_interspeech",
        "zeinali20_interspeech",
        "jiang20_interspeech",
        "mun20_interspeech",
        "alumae20_interspeech",
        "shen20b_interspeech",
        "thienpondt20_interspeech",
        "lozanodiez20_interspeech",
        "ravi20_interspeech"
      ]
    },
    {
      "title": "Voice Conversion and Adaptation I",
      "papers": [
        "zhang20d_interspeech",
        "ding20_interspeech",
        "li20i_interspeech",
        "polyak20_interspeech",
        "zhang20e_interspeech",
        "wang20g_interspeech",
        "polyak20b_interspeech",
        "ishihara20_interspeech",
        "cong20_interspeech"
      ]
    },
    {
      "title": "Acoustic Event Detection",
      "papers": [
        "hong20_interspeech",
        "wang20h_interspeech",
        "wang20i_interspeech",
        "pankajakshan20_interspeech",
        "kim20_interspeech",
        "zheng20_interspeech",
        "kao20_interspeech",
        "chang20_interspeech",
        "park20b_interspeech",
        "jindal20_interspeech"
      ]
    },
    {
      "title": "Spoken Language Understanding I",
      "papers": [
        "radfar20_interspeech",
        "liu20e_interspeech",
        "rao20_interspeech",
        "denisov20_interspeech",
        "chetupalli20_interspeech",
        "tian20b_interspeech",
        "cho20_interspeech",
        "ruan20_interspeech",
        "kuo20_interspeech",
        "gopalakrishnan20_interspeech"
      ]
    },
    {
      "title": "DNN Architectures for Speaker Recognition",
      "papers": [
        "ding20b_interspeech",
        "yu20b_interspeech",
        "zheng20b_interspeech",
        "jung20_interspeech",
        "wu20b_interspeech",
        "safari20_interspeech",
        "zhang20f_interspeech",
        "zhang20g_interspeech",
        "li20j_interspeech",
        "qu20_interspeech"
      ]
    },
    {
      "title": "ASR Model Training and Strategies",
      "papers": [
        "weng20_interspeech",
        "wang20j_interspeech",
        "zhang20h_interspeech",
        "dimitriadis20_interspeech",
        "sheikh20_interspeech",
        "gao20b_interspeech",
        "ding20c_interspeech",
        "loweimi20_interspeech",
        "xu20b_interspeech"
      ]
    },
    {
      "title": "Speech Annotation and Speech Assessment",
      "papers": [
        "kawamura20_interspeech",
        "choi20_interspeech",
        "xie20_interspeech",
        "udayakumar20_interspeech",
        "shi20_interspeech",
        "saeki20_interspeech",
        "feng20b_interspeech",
        "kimura20_interspeech"
      ]
    },
    {
      "title": "Cross/Multi-Lingual and Code-Switched Speech Recognition",
      "papers": [
        "li20k_interspeech",
        "tachbelie20_interspeech",
        "hou20_interspeech",
        "zhou20b_interspeech",
        "abate20_interspeech",
        "hu20c_interspeech",
        "li20l_interspeech",
        "hu20d_interspeech",
        "li20m_interspeech",
        "wang20k_interspeech"
      ]
    },
    {
      "title": "Anti-Spoofing and Liveness Detection",
      "papers": [
        "platen20_interspeech",
        "akimoto20_interspeech",
        "wang20l_interspeech",
        "shim20_interspeech",
        "g20_interspeech",
        "wu20c_interspeech",
        "tak20_interspeech",
        "parasu20_interspeech",
        "lei20_interspeech"
      ]
    },
    {
      "title": "Noise Reduction and Intelligibility",
      "papers": [
        "schroter20_interspeech",
        "tagliasacchi20_interspeech",
        "chuang20_interspeech",
        "bergler20_interspeech",
        "zhang20i_interspeech",
        "dinh20_interspeech",
        "pedersen20_interspeech",
        "arai20_interspeech",
        "abavisani20_interspeech",
        "trinh20_interspeech"
      ]
    },
    {
      "title": "Acoustic Scene Classification",
      "papers": [
        "li20n_interspeech",
        "jung20b_interspeech",
        "zhang20j_interspeech",
        "sharma20_interspeech",
        "wang20m_interspeech",
        "hu20e_interspeech",
        "hu20f_interspeech",
        "devalraju20_interspeech",
        "tzirakis20_interspeech",
        "kwiatkowska20_interspeech"
      ]
    },
    {
      "title": "Singing Voice Computing and Processing in Music",
      "papers": [
        "angelini20_interspeech",
        "wu20d_interspeech",
        "zhang20k_interspeech",
        "hou20b_interspeech",
        "liu20f_interspeech"
      ]
    },
    {
      "title": "Acoustic Model Adaptation for ASR",
      "papers": [
        "sadhu20_interspeech",
        "wan20_interspeech",
        "huang20c_interspeech",
        "zhao20b_interspeech",
        "ding20d_interspeech",
        "mathur20_interspeech",
        "winata20_interspeech",
        "khandelwal20_interspeech",
        "turan20_interspeech",
        "takeda20_interspeech"
      ]
    },
    {
      "title": "Singing and Multimodal Synthesis",
      "papers": [
        "wu20e_interspeech",
        "lu20b_interspeech",
        "lu20c_interspeech",
        "yadav20_interspeech",
        "wu20f_interspeech",
        "goto20_interspeech",
        "wang20n_interspeech"
      ]
    },
    {
      "title": "Intelligibility-Enhancing Speech Modification",
      "papers": [
        "schadler20_interspeech",
        "li20o_interspeech",
        "rennies20_interspeech",
        "simantiraki20_interspeech",
        "bederna20_interspeech",
        "chermaz20_interspeech",
        "paul20b_interspeech"
      ]
    },
    {
      "title": "Human Speech Production I",
      "papers": [
        "arai20b_interspeech",
        "fang20_interspeech",
        "illa20_interspeech",
        "liu20g_interspeech",
        "santos20_interspeech",
        "zhang20l_interspeech",
        "mannem20_interspeech",
        "purohit20_interspeech"
      ]
    },
    {
      "title": "Targeted Source Separation",
      "papers": [
        "ge20_interspeech",
        "li20p_interspeech",
        "qu20b_interspeech",
        "zhang20m_interspeech",
        "li20q_interspeech",
        "hao20_interspeech",
        "zhao20c_interspeech",
        "ochiai20_interspeech",
        "yasuda20_interspeech",
        "xu20c_interspeech"
      ]
    },
    {
      "title": "Keynote 2",
      "papers": [
        "shinncunningham20_interspeech"
      ]
    },
    {
      "title": "Speech Translation and Multilingual/Multimodal Learning",
      "papers": [
        "wang20o_interspeech",
        "elbayad20_interspeech",
        "nguyen20_interspeech",
        "gaido20_interspeech",
        "pino20_interspeech",
        "federico20_interspeech",
        "ohishi20_interspeech",
        "wu20g_interspeech"
      ]
    },
    {
      "title": "Speaker Recognition I",
      "papers": [
        "jung20c_interspeech",
        "jung20d_interspeech",
        "gu20_interspeech",
        "prieto20_interspeech",
        "nicolson20_interspeech",
        "kim20b_interspeech",
        "rozenberg20_interspeech",
        "shi20b_interspeech",
        "lavrentyeva20_interspeech",
        "li20r_interspeech"
      ]
    },
    {
      "title": "Spoken Language Understanding II",
      "papers": [
        "pal20_interspeech",
        "wang20p_interspeech",
        "jia20_interspeech",
        "gaspers20_interspeech",
        "wang20q_interspeech",
        "cao20_interspeech",
        "orihashi20_interspeech",
        "sar20_interspeech",
        "whang20_interspeech",
        "caubriere20_interspeech"
      ]
    },
    {
      "title": "Human Speech Processing",
      "papers": [
        "mcguire20_interspeech",
        "ngoc20_interspeech",
        "ngoc20b_interspeech",
        "cordero20_interspeech",
        "michelas20_interspeech",
        "liu20h_interspeech",
        "zeng20_interspeech",
        "feng20c_interspeech",
        "zellou20b_interspeech",
        "kavaki20_interspeech"
      ]
    },
    {
      "title": "Feature Extraction and Distant ASR",
      "papers": [
        "loweimi20b_interspeech",
        "agrawal20_interspeech",
        "oglic20_interspeech",
        "kurzinger20_interspeech",
        "ghahramani20_interspeech",
        "dutta20_interspeech",
        "joy20_interspeech",
        "parcollet20_interspeech",
        "kumar20b_interspeech",
        "purushothaman20_interspeech"
      ]
    },
    {
      "title": "Voice Privacy Challenge",
      "papers": [
        "tomashenko20_interspeech",
        "nautsch20_interspeech",
        "mawalim20_interspeech",
        "maouche20_interspeech",
        "srivastava20_interspeech",
        "noe20_interspeech"
      ]
    },
    {
      "title": "Speech Synthesis: Text Processing, Data and Evaluation",
      "papers": [
        "park20c_interspeech",
        "zhang20n_interspeech",
        "cohn20_interspeech",
        "taylor20_interspeech",
        "choi20b_interspeech",
        "mittag20_interspeech",
        "zhang20o_interspeech",
        "gallegos20_interspeech",
        "das20_interspeech"
      ]
    },
    {
      "title": "Search for Speech Recognition",
      "papers": [
        "zhou20c_interspeech",
        "chen20e_interspeech",
        "wang20r_interspeech",
        "wong20_interspeech",
        "kim20c_interspeech",
        "garg20_interspeech",
        "beck20_interspeech",
        "chen20f_interspeech"
      ]
    },
    {
      "title": "Computational Paralinguistics I",
      "papers": [
        "stappen20_interspeech",
        "cohn20b_interspeech",
        "cohn20c_interspeech",
        "martinezlucas20_interspeech",
        "tao20_interspeech",
        "kim20d_interspeech",
        "lepp20_interspeech",
        "neitsch20_interspeech",
        "chen20g_interspeech",
        "jayawardena20_interspeech"
      ]
    },
    {
      "title": "Acoustic Phonetics and Prosody",
      "papers": [
        "hughes20_interspeech",
        "neitsch20b_interspeech",
        "defina20_interspeech",
        "mizoguchi20_interspeech",
        "lee20b_interspeech",
        "zarka20_interspeech",
        "mumtaz20_interspeech",
        "riad20_interspeech",
        "dentel20_interspeech"
      ]
    },
    {
      "title": "Keynote 3",
      "papers": [
        "lee20_interspeech"
      ]
    },
    {
      "title": "Tonal Aspects of Acoustic Phonetics and Prosody",
      "papers": [
        "lalhminghlui20_interspeech",
        "wu20h_interspeech",
        "gao20c_interspeech",
        "lai20_interspeech",
        "zhang20p_interspeech",
        "cui20_interspeech",
        "rose20_interspeech",
        "tang20_interspeech"
      ]
    },
    {
      "title": "Speech Classification",
      "papers": [
        "loukina20_interspeech",
        "mhiri20_interspeech",
        "wang20s_interspeech",
        "vuong20_interspeech",
        "chang20b_interspeech",
        "wang20t_interspeech",
        "kumar20c_interspeech",
        "koizumi20_interspeech",
        "mo20_interspeech",
        "li20s_interspeech"
      ]
    },
    {
      "title": "Speech Synthesis Paradigms and Methods I",
      "papers": [
        "wang20u_interspeech",
        "liu20i_interspeech",
        "nakashika20_interspeech",
        "choi20c_interspeech",
        "ihm20_interspeech",
        "kaneko20_interspeech",
        "ellinas20_interspeech",
        "yu20c_interspeech",
        "mitsui20_interspeech",
        "m20_interspeech"
      ]
    },
    {
      "title": "The INTERSPEECH 2020 Computational Paralinguistics ChallengE (ComParE)",
      "papers": [
        "schuller20_interspeech",
        "koike20_interspeech",
        "illium20_interspeech",
        "klumpp20_interspeech",
        "montacie20_interspeech",
        "juliao20_interspeech",
        "markitantov20_interspeech",
        "mendonca20_interspeech",
        "macintyre20_interspeech",
        "szep20_interspeech",
        "yang20c_interspeech",
        "sogancoglu20_interspeech",
        "ristea20_interspeech"
      ]
    },
    {
      "title": "Streaming ASR",
      "papers": [
        "kumar20d_interspeech",
        "wang20v_interspeech",
        "kurata20_interspeech",
        "li20t_interspeech",
        "baqueroarnal20_interspeech",
        "wu20i_interspeech",
        "inaguma20b_interspeech",
        "zhang20q_interspeech",
        "nguyen20b_interspeech",
        "joshi20_interspeech"
      ]
    },
    {
      "title": "Alzheimer&#8217;s Dementia Recognition Through Spontaneous Speech",
      "papers": [
        "martinc20_interspeech",
        "yuan20_interspeech",
        "balagopalan20_interspeech",
        "luz20_interspeech",
        "pappagari20_interspeech",
        "cummins20_interspeech",
        "rohanian20_interspeech",
        "searle20_interspeech",
        "edwards20_interspeech",
        "pompili20_interspeech",
        "farzana20_interspeech",
        "sarawgi20_interspeech",
        "koo20_interspeech",
        "syed20_interspeech"
      ]
    },
    {
      "title": "Speaker Recognition Challenges and Applications",
      "papers": [
        "lee20c_interspeech",
        "li20u_interspeech",
        "antipov20_interspeech",
        "tao20b_interspeech",
        "shon20_interspeech",
        "chen20h_interspeech",
        "wang20w_interspeech",
        "sang20_interspeech",
        "chowdhury20b_interspeech",
        "li20v_interspeech"
      ]
    },
    {
      "title": "Applications of ASR",
      "papers": [
        "rybakov20_interspeech",
        "liu20j_interspeech",
        "kong20_interspeech",
        "wang20x_interspeech",
        "zuluagagomez20_interspeech",
        "gudepu20_interspeech",
        "sawhney20_interspeech",
        "chen20i_interspeech",
        "hout20_interspeech"
      ]
    },
    {
      "title": "Speech Emotion Recognition II",
      "papers": [
        "lin20d_interspeech",
        "latif20b_interspeech",
        "fujioka20_interspeech",
        "liu20k_interspeech",
        "zhu20_interspeech",
        "lian20c_interspeech",
        "mao20c_interspeech",
        "mao20d_interspeech"
      ]
    },
    {
      "title": "Bi- and Multilinguality",
      "papers": [
        "perezramon20_interspeech",
        "liu20l_interspeech",
        "li20w_interspeech",
        "spinu20_interspeech",
        "chowdhury20c_interspeech",
        "johnson20_interspeech",
        "zhang20r_interspeech",
        "du20b_interspeech",
        "afouras20_interspeech",
        "rhee20_interspeech"
      ]
    },
    {
      "title": "Single-Channel Speech Enhancement I",
      "papers": [
        "shi20c_interspeech",
        "lu20d_interspeech",
        "li20x_interspeech",
        "yu20d_interspeech",
        "lee20d_interspeech",
        "bando20_interspeech",
        "bulut20_interspeech",
        "tran20b_interspeech",
        "li20y_interspeech",
        "deng20_interspeech"
      ]
    },
    {
      "title": "Deep Noise Suppression Challenge",
      "papers": [
        "li20z_interspeech",
        "strake20_interspeech",
        "hu20g_interspeech",
        "westhausen20_interspeech",
        "valin20_interspeech",
        "isik20_interspeech",
        "reddy20_interspeech"
      ]
    },
    {
      "title": "Voice and Hearing Disorders",
      "papers": [
        "akbarzadeh20_interspeech",
        "wan20b_interspeech",
        "irino20_interspeech",
        "zhang20s_interspeech",
        "zhang20t_interspeech",
        "abderrazek20_interspeech",
        "mirheidari20_interspeech",
        "moore20_interspeech",
        "barche20_interspeech",
        "shivkumar20_interspeech"
      ]
    },
    {
      "title": "Spoken Term Detection",
      "papers": [
        "xu20d_interspeech",
        "bluche20_interspeech",
        "ylmaz20_interspeech",
        "wu20j_interspeech",
        "zhang20u_interspeech",
        "zhang20v_interspeech",
        "yang20d_interspeech",
        "chen20j_interspeech",
        "zhao20d_interspeech",
        "higuchi20_interspeech"
      ]
    },
    {
      "title": "The Fearless Steps Challenge Phase-02",
      "papers": [
        "heitkaemper20_interspeech",
        "zhang20w_interspeech",
        "lin20e_interspeech",
        "gorin20_interspeech",
        "joglekar20_interspeech"
      ]
    },
    {
      "title": "Monaural Source Separation",
      "papers": [
        "luo20b_interspeech",
        "chen20k_interspeech",
        "wang20y_interspeech",
        "pariente20_interspeech",
        "chen20l_interspeech",
        "deng20b_interspeech",
        "kinoshita20_interspeech",
        "narayanaswamy20_interspeech"
      ]
    },
    {
      "title": "Single-Channel Speech Enhancement II",
      "papers": [
        "qiu20b_interspeech",
        "xiang20_interspeech",
        "zhang20x_interspeech",
        "wang20z_interspeech",
        "shi20d_interspeech",
        "hao20b_interspeech",
        "roy20_interspeech",
        "yu20e_interspeech",
        "li20aa_interspeech",
        "shi20e_interspeech"
      ]
    },
    {
      "title": "Topics in ASR II",
      "papers": [
        "nortje20_interspeech",
        "lee20e_interspeech",
        "csapo20_interspeech",
        "csapo20b_interspeech",
        "feng20d_interspeech",
        "matsuura20_interspeech",
        "tsunematsu20_interspeech",
        "milde20_interspeech",
        "papadimitriou20_interspeech",
        "pratap20_interspeech"
      ]
    },
    {
      "title": "Neural Signals for Spoken Communication",
      "papers": [
        "parmonangan20_interspeech",
        "sharon20_interspeech",
        "cai20_interspeech",
        "angrick20_interspeech",
        "dash20_interspeech"
      ]
    },
    {
      "title": "Training Strategies for ASR",
      "papers": [
        "chen20m_interspeech",
        "tulsiani20_interspeech",
        "kanda20b_interspeech",
        "weninger20_interspeech",
        "guo20_interspeech",
        "zeyer20_interspeech",
        "park20d_interspeech",
        "masumura20_interspeech",
        "gowda20_interspeech",
        "wang20aa_interspeech"
      ]
    },
    {
      "title": "Speech Transmission &amp; Coding",
      "papers": [
        "das20b_interspeech",
        "broucke20_interspeech",
        "skoglund20_interspeech",
        "manocha20_interspeech",
        "masztalski20_interspeech",
        "naderi20_interspeech",
        "mittag20b_interspeech",
        "moller20_interspeech"
      ]
    },
    {
      "title": "Bioacoustics and Articulation",
      "papers": [
        "shahrebabaki20_interspeech",
        "shahrebabaki20b_interspeech",
        "gatto20_interspeech",
        "mannem20b_interspeech",
        "hernandez20_interspeech",
        "ma20_interspeech",
        "singh20b_interspeech",
        "yang20e_interspeech",
        "lenain20_interspeech",
        "naini20_interspeech"
      ]
    },
    {
      "title": "Speech Synthesis: Multilingual and Cross-Lingual Approaches",
      "papers": [
        "zhao20e_interspeech",
        "liu20m_interspeech",
        "fu20b_interspeech",
        "staib20_interspeech",
        "xin20_interspeech",
        "liu20n_interspeech",
        "bansal20_interspeech",
        "prakash20_interspeech",
        "korte20_interspeech",
        "nekvinda20_interspeech"
      ]
    },
    {
      "title": "Learning Techniques for Speaker Recognition I",
      "papers": [
        "chung20b_interspeech",
        "kye20_interspeech",
        "li20ba_interspeech",
        "shi20f_interspeech",
        "montalvo20_interspeech",
        "khan20_interspeech",
        "liu20o_interspeech",
        "zheng20c_interspeech",
        "chen20n_interspeech"
      ]
    },
    {
      "title": "Pronunciation",
      "papers": [
        "lin20f_interspeech",
        "lo20b_interspeech",
        "yan20_interspeech",
        "duan20_interspeech",
        "yang20f_interspeech",
        "cheng20_interspeech",
        "kyriakopoulos20_interspeech",
        "shi20g_interspeech",
        "chu20_interspeech"
      ]
    },
    {
      "title": "Diarization",
      "papers": [
        "gimeno20_interspeech",
        "lavechin20_interspeech",
        "peng20_interspeech",
        "lin20g_interspeech",
        "liu20p_interspeech",
        "zhu20b_interspeech",
        "neumann20_interspeech",
        "upadhyay20_interspeech",
        "cornell20_interspeech",
        "moritz20_interspeech"
      ]
    },
    {
      "title": "Computational Paralinguistics II",
      "papers": [
        "diener20_interspeech",
        "zhong20_interspeech",
        "gosztolya20_interspeech",
        "mori20_interspeech",
        "afshan20_interspeech",
        "sabu20_interspeech",
        "xue20_interspeech",
        "lin20h_interspeech",
        "anjos20_interspeech"
      ]
    },
    {
      "title": "Speech Synthesis Paradigms and Methods II",
      "papers": [
        "zhang20y_interspeech",
        "palkama20_interspeech",
        "yang20g_interspeech",
        "katsurada20_interspeech",
        "liang20_interspeech",
        "mohan20_interspeech",
        "tu20b_interspeech",
        "saha20_interspeech",
        "yamashita20_interspeech",
        "webber20_interspeech"
      ]
    },
    {
      "title": "Speaker Embedding",
      "papers": [
        "lin20i_interspeech",
        "pham20b_interspeech",
        "liu20q_interspeech",
        "xia20_interspeech",
        "kwon20_interspeech",
        "georges20_interspeech",
        "kreyssig20_interspeech",
        "peng20b_interspeech",
        "li20ca_interspeech",
        "cho20b_interspeech"
      ]
    },
    {
      "title": "Single-Channel Speech Enhancement III",
      "papers": [
        "zhao20f_interspeech",
        "zhang20z_interspeech",
        "du20c_interspeech",
        "kegler20_interspeech",
        "shankar20_interspeech",
        "lin20j_interspeech",
        "defossez20_interspeech",
        "romaniuk20_interspeech"
      ]
    },
    {
      "title": "Multi-Channel Audio and Emotion Recognition",
      "papers": [
        "chiba20_interspeech",
        "li20da_interspeech",
        "hiroe20_interspeech",
        "golokolenko20_interspeech",
        "fan20b_interspeech",
        "scheibler20_interspeech",
        "zhong20b_interspeech",
        "cai20b_interspeech",
        "grondin20_interspeech"
      ]
    },
    {
      "title": "Computational Resource Constrained Speech Recognition",
      "papers": [
        "jose20_interspeech",
        "adya20_interspeech",
        "majumdar20_interspeech",
        "mehrotra20_interspeech",
        "nguyen20c_interspeech",
        "garg20b_interspeech",
        "pratap20b_interspeech",
        "bai20_interspeech",
        "strimel20_interspeech"
      ]
    },
    {
      "title": "Speech Synthesis: Prosody and Emotion",
      "papers": [
        "shankar20b_interspeech",
        "shankar20c_interspeech",
        "tits20b_interspeech",
        "cao20b_interspeech",
        "sorin20_interspeech",
        "zhou20d_interspeech",
        "matsumoto20_interspeech",
        "zhang20aa_interspeech",
        "kishida20_interspeech",
        "yang20h_interspeech",
        "hono20_interspeech",
        "eskimez20_interspeech",
        "dhamyal20_interspeech"
      ]
    },
    {
      "title": "The Interspeech 2020 Far Field Speaker Verification Challenge",
      "papers": [
        "qin20_interspeech",
        "zhang20ba_interspeech",
        "gusev20_interspeech",
        "zhang20ca_interspeech",
        "tong20_interspeech"
      ]
    },
    {
      "title": "Multimodal Speech Processing",
      "papers": [
        "chung20c_interspeech",
        "chung20d_interspeech",
        "wand20_interspeech",
        "yu20f_interspeech",
        "li20ea_interspeech",
        "sterpu20_interspeech",
        "koumparoulis20_interspeech",
        "mortazavi20_interspeech",
        "liu20r_interspeech",
        "konda20_interspeech"
      ]
    },
    {
      "title": "Keynote 4",
      "papers": [
        "mevawalla20_interspeech"
      ]
    },
    {
      "title": "Speech Synthesis: Neural Waveform Generation II",
      "papers": [
        "michelsanti20_interspeech",
        "wu20k_interspeech",
        "wu20l_interspeech",
        "yoon20_interspeech",
        "sharma20b_interspeech",
        "cui20b_interspeech",
        "ai20b_interspeech",
        "vipperla20_interspeech",
        "song20c_interspeech",
        "vainer20_interspeech"
      ]
    },
    {
      "title": "ASR Neural Network Architectures and Training II",
      "papers": [
        "zhang20da_interspeech",
        "sapru20_interspeech",
        "li20fa_interspeech",
        "chang20c_interspeech",
        "lakomkin20_interspeech",
        "michel20_interspeech",
        "han20_interspeech",
        "sainath20_interspeech",
        "liu20s_interspeech"
      ]
    },
    {
      "title": "Neural Networks for Language Modeling",
      "papers": [
        "li20ga_interspeech",
        "jain20b_interspeech",
        "futami20_interspeech",
        "chien20_interspeech",
        "huo20_interspeech",
        "liu20t_interspeech",
        "higuchi20b_interspeech",
        "fujita20_interspeech"
      ]
    },
    {
      "title": "Phonetic Event Detection and Segmentation",
      "papers": [
        "chen20o_interspeech",
        "lee20f_interspeech",
        "xu20e_interspeech",
        "kumar20e_interspeech",
        "lavechin20b_interspeech",
        "agarwal20_interspeech",
        "zheng20d_interspeech",
        "kreuk20_interspeech",
        "zelasko20_interspeech",
        "limonard20_interspeech"
      ]
    },
    {
      "title": "Human Speech Production II",
      "papers": [
        "rasilo20_interspeech",
        "csapo20c_interspeech",
        "bozorg20_interspeech",
        "douros20_interspeech",
        "csapo20d_interspeech",
        "parrot20_interspeech",
        "diener20b_interspeech",
        "penney20_interspeech"
      ]
    },
    {
      "title": "New Trends in Self-Supervised Speech Processing",
      "papers": [
        "siriwardhana20_interspeech",
        "chung20e_interspeech",
        "song20d_interspeech",
        "singh20c_interspeech",
        "kumatani20_interspeech",
        "wu20m_interspeech",
        "yang20i_interspeech",
        "khurana20_interspeech",
        "abulimiti20_interspeech"
      ]
    },
    {
      "title": "Learning Techniques for Speaker Recognition II",
      "papers": [
        "zhou20e_interspeech",
        "rybicka20_interspeech",
        "mingote20_interspeech",
        "sarfjoo20_interspeech",
        "wei20b_interspeech",
        "kang20_interspeech",
        "desplanques20_interspeech",
        "chen20p_interspeech"
      ]
    },
    {
      "title": "Spoken Language Evaluatiosn",
      "papers": [
        "lu20e_interspeech",
        "papi20_interspeech",
        "wang20ba_interspeech",
        "raina20_interspeech",
        "wu20n_interspeech",
        "lin20k_interspeech",
        "bai20b_interspeech",
        "woszczyk20_interspeech",
        "hidaka20_interspeech"
      ]
    },
    {
      "title": "Spoken Dialogue System",
      "papers": [
        "chien20b_interspeech",
        "jeong20_interspeech",
        "luo20c_interspeech",
        "hong20b_interspeech",
        "he20_interspeech",
        "dang20_interspeech",
        "qian20_interspeech",
        "xu20f_interspeech"
      ]
    },
    {
      "title": "Dereverberation and Echo Cancellation",
      "papers": [
        "wang20ca_interspeech",
        "yang20j_interspeech",
        "kothapally20_interspeech",
        "zhang20ea_interspeech",
        "zhang20fa_interspeech",
        "pfeifenberger20_interspeech",
        "gao20d_interspeech",
        "chen20q_interspeech",
        "fan20c_interspeech",
        "kim20e_interspeech"
      ]
    },
    {
      "title": "Speech Synthesis: Toward End-to-End Synthesis",
      "papers": [
        "cai20c_interspeech",
        "cooper20_interspeech",
        "wang20da_interspeech",
        "wang20ea_interspeech",
        "peirolilja20_interspeech",
        "li20ha_interspeech",
        "lim20_interspeech",
        "aso20_interspeech",
        "dou20_interspeech",
        "fong20_interspeech",
        "chen20r_interspeech"
      ]
    },
    {
      "title": "Speech Enhancement, Bandwidth Extension and Hearing Aids",
      "papers": [
        "papadopoulos20_interspeech",
        "ji20_interspeech",
        "seki20_interspeech",
        "weisman20_interspeech",
        "ho20_interspeech",
        "fedorov20_interspeech",
        "hikosaka20_interspeech",
        "hou20c_interspeech",
        "hou20d_interspeech",
        "hu20h_interspeech"
      ]
    },
    {
      "title": "Speech Emotion Recognition III",
      "papers": [
        "huang20d_interspeech",
        "jalal20_interspeech",
        "fan20d_interspeech",
        "zhou20f_interspeech",
        "zhou20g_interspeech",
        "li20ia_interspeech",
        "chou20_interspeech",
        "jalal20b_interspeech"
      ]
    },
    {
      "title": "Accoustic Phonetics of L1-L2 and Other Interactions",
      "papers": [
        "gessinger20_interspeech",
        "gu20b_interspeech",
        "kaminskaia20_interspeech",
        "manghat20_interspeech",
        "hutin20_interspeech",
        "hope20_interspeech",
        "menshikova20_interspeech",
        "xu20g_interspeech",
        "li20ja_interspeech",
        "yang20k_interspeech"
      ]
    },
    {
      "title": "Conversational Systems",
      "papers": [
        "chuang20b_interspeech",
        "kuo20b_interspeech",
        "huang20e_interspeech",
        "zhang20ga_interspeech",
        "yamamoto20_interspeech",
        "shi20h_interspeech",
        "atamna20_interspeech",
        "fuscone20_interspeech",
        "hu20i_interspeech"
      ]
    },
    {
      "title": "The Attacker&#8217;s Perpective on Automatic Speaker Verification",
      "papers": [
        "das20c_interspeech",
        "sholokhov20_interspeech",
        "jiang20b_interspeech",
        "wang20fa_interspeech",
        "villalba20_interspeech",
        "zhang20ha_interspeech"
      ]
    },
    {
      "title": "Summarization, Semantic Analysis and Classification",
      "papers": [
        "n20_interspeech",
        "manakul20_interspeech",
        "zhang20ia_interspeech",
        "wang20ga_interspeech",
        "nguyen20d_interspeech",
        "yadav20b_interspeech",
        "mckenna20_interspeech",
        "tran20c_interspeech",
        "mittal20_interspeech",
        "agarwal20b_interspeech"
      ]
    },
    {
      "title": "Speaker Recognition II",
      "papers": [
        "liu20u_interspeech",
        "yi20b_interspeech",
        "han20b_interspeech",
        "lin20l_interspeech",
        "hautamaki20_interspeech",
        "afshan20b_interspeech",
        "seurin20_interspeech",
        "granqvist20_interspeech",
        "ramoji20_interspeech"
      ]
    },
    {
      "title": "General Topics in Speech Recognition",
      "papers": [
        "opatka20_interspeech",
        "hard20_interspeech",
        "huang20f_interspeech",
        "borgholt20_interspeech",
        "kumar20f_interspeech",
        "wu20o_interspeech",
        "zhu20c_interspeech",
        "novitasari20_interspeech",
        "raissi20_interspeech",
        "shahnawazuddin20_interspeech"
      ]
    },
    {
      "title": "Speech Synthesis: Prosody Modeling",
      "papers": [
        "karlapati20_interspeech",
        "lin20m_interspeech",
        "kulkarni20_interspeech",
        "bae20_interspeech",
        "tyagi20_interspeech",
        "kenter20_interspeech",
        "zhao20g_interspeech",
        "zeng20b_interspeech",
        "shirahata20_interspeech",
        "raitio20_interspeech",
        "morrison20_interspeech",
        "whitehill20_interspeech",
        "gao20e_interspeech"
      ]
    },
    {
      "title": "Language Learning",
      "papers": [
        "hirschi20_interspeech",
        "niekerk20_interspeech",
        "krishnamohan20_interspeech",
        "lan20_interspeech",
        "tsukada20_interspeech",
        "ng20b_interspeech",
        "ding20e_interspeech",
        "zhou20h_interspeech",
        "li20ka_interspeech"
      ]
    },
    {
      "title": "Speech Enhancement",
      "papers": [
        "cheng20b_interspeech",
        "wang20ha_interspeech",
        "su20b_interspeech",
        "pandey20_interspeech",
        "richter20_interspeech",
        "gogate20_interspeech",
        "sivaraman20_interspeech",
        "kishore20_interspeech",
        "fan20e_interspeech",
        "fontaine20_interspeech"
      ]
    },
    {
      "title": "Speech in Health II",
      "papers": [
        "albes20_interspeech",
        "seneviratne20_interspeech",
        "xezonaki20_interspeech",
        "huang20g_interspeech",
        "gosztolya20b_interspeech",
        "huckvale20_interspeech",
        "teplansky20_interspeech",
        "yue20b_interspeech",
        "mallela20_interspeech",
        "pompili20b_interspeech"
      ]
    },
    {
      "title": "Speech and Audio Quality Assessment",
      "papers": [
        "zhang20ja_interspeech",
        "falkowskigilski20_interspeech",
        "chiu20_interspeech",
        "huang20h_interspeech",
        "ragano20_interspeech",
        "algayres20_interspeech",
        "li20la_interspeech",
        "dong20_interspeech",
        "brueggeman20_interspeech",
        "kosmider20_interspeech"
      ]
    },
    {
      "title": "Privacy and Security in Speech Communication",
      "papers": [
        "oconnor20_interspeech",
        "leschanowsky20_interspeech",
        "kreuk20b_interspeech",
        "daubener20_interspeech",
        "adelani20_interspeech",
        "jayashankar20_interspeech"
      ]
    },
    {
      "title": "Voice Conversion and Adaptation II",
      "papers": [
        "huang20i_interspeech",
        "suda20_interspeech",
        "chen20s_interspeech",
        "wu20p_interspeech",
        "park20e_interspeech",
        "fu20c_interspeech",
        "lian20d_interspeech",
        "nercessian20_interspeech",
        "chen20t_interspeech",
        "liu20v_interspeech",
        "albadawy20_interspeech"
      ]
    },
    {
      "title": "Multilingual and Code-Switched ASR",
      "papers": [
        "wang20ia_interspeech",
        "thomas20_interspeech",
        "zhu20d_interspeech",
        "madikeri20_interspeech",
        "pratap20c_interspeech",
        "sailor20_interspeech",
        "chandu20_interspeech",
        "lu20f_interspeech",
        "sharma20c_interspeech",
        "qiu20c_interspeech"
      ]
    },
    {
      "title": "Speech and Voice Disorders",
      "papers": [
        "dinh20b_interspeech",
        "tong20b_interspeech",
        "lin20n_interspeech",
        "takashima20_interspeech",
        "degala20_interspeech",
        "pan20c_interspeech",
        "sharma20d_interspeech",
        "rowe20_interspeech",
        "alhinti20_interspeech",
        "halpern20_interspeech"
      ]
    },
    {
      "title": "The Zero Resource Speech Challenge 2020",
      "papers": [
        "dunbar20_interspeech",
        "niekerk20b_interspeech",
        "ds20_interspeech",
        "gundogdu20_interspeech",
        "tjandra20_interspeech",
        "morita20_interspeech",
        "tobing20_interspeech",
        "chen20u_interspeech",
        "rasanen20_interspeech",
        "bhati20_interspeech",
        "millet20_interspeech",
        "clayton20_interspeech",
        "m20b_interspeech",
        "li20ma_interspeech"
      ]
    },
    {
      "title": "LM Adaptation, Lexical Units and Punctuation",
      "papers": [
        "effendi20_interspeech",
        "augustyniak20_interspeech",
        "sunkara20_interspeech",
        "huang20j_interspeech",
        "peyser20_interspeech",
        "ogawa20_interspeech",
        "wok20_interspeech",
        "pandey20b_interspeech",
        "wills20_interspeech"
      ]
    },
    {
      "title": "Speech in Health I",
      "papers": [
        "han20c_interspeech",
        "baird20_interspeech",
        "zhao20h_interspeech",
        "pan20d_interspeech",
        "romana20_interspeech",
        "kadiri20_interspeech",
        "quintas20_interspeech",
        "abraham20_interspeech",
        "perez20_interspeech",
        "kodrasi20_interspeech"
      ]
    },
    {
      "title": "ASR Neural Network Architectures II &#8212; Transformers",
      "papers": [
        "shi20i_interspeech",
        "huang20k_interspeech",
        "li20na_interspeech",
        "hori20_interspeech",
        "zhou20i_interspeech",
        "zhao20i_interspeech",
        "tian20c_interspeech",
        "zhao20j_interspeech",
        "gulati20_interspeech",
        "lu20g_interspeech"
      ]
    },
    {
      "title": "Spatial Audio",
      "papers": [
        "togami20_interspeech",
        "zhong20c_interspeech",
        "raikar20_interspeech",
        "deng20c_interspeech",
        "mack20_interspeech",
        "beiton20_interspeech",
        "hu20j_interspeech",
        "murthy20_interspeech",
        "wang20ja_interspeech",
        "xue20b_interspeech"
      ]
    }
  ],
  "doi": "10.21437/Interspeech.2020"
}