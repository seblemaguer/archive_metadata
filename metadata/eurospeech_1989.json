{
 "title": "First European Conference on Speech Communication and Technology (Eurospeech 1989)",
 "location": "Paris, France",
 "startDate": "27/9/1989",
 "endDate": "29/9/1989",
 "conf": "Eurospeech",
 "year": "1989",
 "name": "eurospeech_1989",
 "series": "Eurospeech",
 "SIG": "",
 "title1": "First European Conference on Speech Communication and Technology",
 "title2": "(Eurospeech 1989)",
 "date": "27-29 September 1989",
 "papers": {
  "rolling89_eurospeech": {
   "authors": [
    [
     "Loll",
     "Rolling"
    ]
   ],
   "title": "Speech ninety-two -new horizons for the european community",
   "original": "e89_1001",
   "page_count": 2,
   "order": 1,
   "p1": "1001",
   "pn": "1002",
   "abstract": [
    "The open market beyond 1992 will confront the Community and its Member States with a number of major technological, economic and political challenges.\n",
    ""
   ]
  },
  "fant89_eurospeech": {
   "authors": [
    [
     "Gunnar",
     "Fant"
    ]
   ],
   "title": "Speech research in perspective",
   "original": "e89_1003",
   "page_count": 2,
   "order": 2,
   "p1": "1003",
   "pn": "1004",
   "abstract": [
    "The purpose of my talk will be to contribute to a perspective of speech research - to relate our present activities to the past and to the future. Much could be said on this topic but I will be neither complete in my presentation nor balanced in my judgements. I shall try to communicate feelings rather than facts and I will rely heavily on my own experience which spans almost half a century of speech research.\n",
    ""
   ]
  },
  "hwang89_eurospeech": {
   "authors": [
    [
     "Mei-Yuh",
     "Hwang"
    ],
    [
     "Hsiao-Wuen",
     "Hon"
    ],
    [
     "Kai-Fu",
     "Lee"
    ]
   ],
   "title": "Modeling between-word coarticulation in continuous speech recognition",
   "original": "e89_1005",
   "page_count": 4,
   "order": 3,
   "p1": "1005",
   "pn": "1008",
   "abstract": [
    "This paper describes the addition of between-word coarticulation modeling into sphinx, an accurate large-vocabulary speaker-independent continuous speech recognition system. Between-word coarticulation is a major source of phonetic variability in continuous speech. By detailed modeling of between-word triphones and utilizing the generalized triphone technique, we obtain an error rate reduction of 16% to 29% for different test sets and grammars on the DARPA Resource Management (RM) Task.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-1"
  },
  "torkkola89_eurospeech": {
   "authors": [
    [
     "Kari",
     "Torkkola"
    ],
    [
     "Kimmo",
     "Raivio"
    ]
   ],
   "title": "Comparison of symbolic and connectionist approaches to eliminate coarticulation effects in phonemic speech recognition",
   "original": "e89_1009",
   "page_count": 4,
   "order": 4,
   "p1": "1009",
   "pn": "1012",
   "abstract": [
    "Two methods to correct phonemic transcriptions produced by the acoustic processor of a speech recognition system are described and compared. The first method that was invented by Prof. Teuvo Kohonen and named the Dynamically Expanding Context (DEC), involves a large set of error-correcting rules automatically constructed from examples. This symbolic approach is compared with a connectionist one, which employs a multi-layered feed-forward network trained with back propagation. Our experiments demonstrate that the latter paradigm is far from optimal when context-dependent mapping (e.g. correction) from one set of symbol strings to another is desired. The DEC-method is shown to have better correction capabilities. Beside this, the training time required by DEC is a fraction of that required by back propagation.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-2"
  },
  "falaschi89_eurospeech": {
   "authors": [
    [
     "Alessandro",
     "Falaschi"
    ]
   ],
   "title": "A functional based phonetic units definition for statistical speech recognizers",
   "original": "e89_1013",
   "page_count": 4,
   "order": 5,
   "p1": "1013",
   "pn": "1016",
   "abstract": [
    "A functional approach to the description of speech in terms of phonemic units is exposed. It is based on a structural analysis of phonetically transcribed and syllabified texts, which takes into account the word stress and the syllabic structure of the language. As a consequence, the defined units utilize syllabic microprosodic cues and are present in a limited number of contexts. The method is easily applicable to continuous speech, and substantial improvements in recognizers performance can be expected. Finally, the methodology can be advantageously applied to other speech research areas.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-3"
  },
  "weigel89_eurospeech": {
   "authors": [
    [
     "Walter",
     "Weigel"
    ],
    [
     "Günther",
     "Ruske"
    ]
   ],
   "title": "Continuous speech recognition using syllabic segmentation and demisyllable hidden Markov models",
   "original": "e89_1017",
   "page_count": 4,
   "order": 6,
   "p1": "1017",
   "pn": "1020",
   "abstract": [
    "A main problem in continuous speech recognition consists of the coarticulation effects which make the recognition of phoneme-sized segments difficult. This problem can be reduced if larger units are used which contain the main coarticulation effects. An approach is presented which starts from an explicit localization of the syllabic nuclei, demisyllables are chosen as basic recognition units. The demisyllables are represented by Hidden Markov Models (HMM). Three different types of HMM's are exploited: discrete HMM's (vector quantization) , histogram HMM's and mixture Gaussian HMM's. The syllable boundaries are implicitly determined during the classification of the demisyllables. These methods constitute the acoustic-phonetic decoding module in a complete speech recognition system.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-4"
  },
  "vigier89_eurospeech": {
   "authors": [
    [
     "Alain J.",
     "Vigier"
    ],
    [
     "Harvey F.",
     "Silverman"
    ]
   ],
   "title": "Disambiguation of the e-set for connected-alphadigit recognition",
   "original": "e89_1021",
   "page_count": 4,
   "order": 7,
   "p1": "1021",
   "pn": "1024",
   "abstract": [
    "A real-time, talker-dependent, connected-speech recognizer has been operational at the Laboratory for Engineering Man-Machine Systems (LEMS) for 3 years. This recognizer analyses strings of connected digits or alphadigits, using dynamic programming (DP) techniques and an expert for final decision. DP often misclassifies within difficult subgroups of the vocabulary such as the E-set (letters e, p, t, b, d, g, v, z, c). In this paper, we present a feedback mechanism for disambiguation of words classified to be in the E-set by the first pass of the recognizer. This mechanism reanalyses the speech data within an appropriate context and uses the analysis as input to a hidden Markov model (HMM) which uses acoustic, phonetic and linguistic knowledge about the elements of the E-set. Experiments have been run using speech from 5 male talkers. A different model was computed for each talker. Each model was trained with 12 replications of each word and tested with 72 utterances. A recognition rate of 95% was achived.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-5"
  },
  "young89_eurospeech": {
   "authors": [
    [
     "S. R.",
     "Young"
    ]
   ],
   "title": "Use of dialogue, pragmatics and semantics to enhance speech recognition: applicability and limitations of dynamically reducing perplexity",
   "original": "e89_1025",
   "page_count": 1,
   "order": 8,
   "p1": "1025",
   "pn": "",
   "abstract": [
    "Current state-of-the-art speaker-independent continuous speech recognizers are able to achieve word recognition rates in excess of 94 percent with lexicons of 1000 words of less using grammars or language models with perplexity 60 or less. Performance of these systems decreases rapidly as the perplexity of the grammar increases. As we allow users more flexibility in interacting with speech recognition and understanding systems, the lexicon size and perplexity increases more than an order of magnitude. Allowing spontaneous speech further increases perplexity and introduces many additional sources of variability current speech systems are unaccustomed to dealing with. However, natural language knowledge sources can be used to dynamically decrease perplexity and allow more natural interaction given the current state of the technology.\n",
    ""
   ]
  },
  "niedermair89_eurospeech": {
   "authors": [
    [
     "G. Th.",
     "Niedermair"
    ]
   ],
   "title": "The use of a semantic network in speech dialogue",
   "original": "e89_1026",
   "page_count": 4,
   "order": 9,
   "p1": "1026",
   "pn": "1029",
   "abstract": [
    "The paper discusses the construction principles and use of a semantic network within the dialogue system for continuous speech input, called SPICOS II. One of the main problems in continuous speech recognition is the very close coupling of syntactic and semantic constraints with the recognition process while maintaining their descriptive independence. The paper will show how the evaluation of the semantic network, that actually models the behaviour of words and their semantically acceptable relations, can be integrated into the linguistic search process and help to reduce combinatorial complexity. The same network is used to support the resolution of anaphoric expressions in various ways.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-6"
  },
  "tropf89_eurospeech": {
   "authors": [
    [
     "Herbert S.",
     "Tropf"
    ]
   ],
   "title": "Syntax in the spoken dialogue system spicos-II",
   "original": "e89_1030",
   "page_count": 4,
   "order": 10,
   "p1": "1030",
   "pn": "1033",
   "abstract": [
    "The syntactic component of a speech understanding system is described. The interface between acoustic recognition and syntax is sketched, and the set-up of the lexicon is outlined. The main aspects of the paper concern the rule format of the APSG (augmented phrase structure grammar), the syntactic concept for handling the sentence topology of German, and the treatment of discontinuous structures. Finally, the covered syntactic phenomena are enumerated.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-7"
  },
  "biebow89_eurospeech": {
   "authors": [
    [
     "Brigitte",
     "Biebow"
    ],
    [
     "Pascal",
     "Coupey"
    ],
    [
     "Sylvie",
     "Szulman"
    ]
   ],
   "title": "Using exceptions in a semantic network for a natural language application",
   "original": "e89_1034",
   "page_count": 4,
   "order": 11,
   "p1": "1034",
   "pn": "1037",
   "abstract": [
    "This paper presents the advantages of using a semantic network with default and exception abilities for a specific problem in language processing. More precisely, we show that the features of this semantic network provide an adequate representation of texts expressing a general rule and its exceptions. General statements are usually expressed in natural language by the presence of some specific words, and they occur frequently in our application domain. Our research is effectively integrated in a whole project which is an interactive user-aided tool for the description of requirements including the use of graphics and textual comments. The general cases of treatments are represented by semantic rules which are not strict and admit exceptions. The inferences supported by the network are justified by an interpretation in default logic.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-8"
  },
  "streit89_eurospeech": {
   "authors": [
    [
     "Michael",
     "Streit"
    ]
   ],
   "title": "Presuppositions and anaphora in a question answering speech system",
   "original": "e89_1175",
   "page_count": 4,
   "order": 12,
   "p1": "1175",
   "pn": "1178",
   "abstract": [
    "The article deals with problems of dialogue handling in a natural speech dialog system. The central aspect of the article is the handling of presuppositions. A concept of presuppositions is defined that fits to questions. The handling of presuppositions by means of cooperative answering is worked out. Also the relation between presuppositions and anaphora will be discussed.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-9"
  },
  "price89_eurospeech": {
   "authors": [
    [
     "Patti",
     "Price"
    ],
    [
     "Robert",
     "Moore"
    ],
    [
     "Hy",
     "Murveit"
    ],
    [
     "Fernando",
     "Pereira"
    ],
    [
     "Jared",
     "Bernstein"
    ],
    [
     "Mary",
     "Dalrymple"
    ]
   ],
   "title": "The integration of speech and natural language in interactive spoken language systems",
   "original": "e89_1179",
   "page_count": 4,
   "order": 13,
   "p1": "1179",
   "pn": "1182",
   "abstract": [
    "Spoken language human-machine interfaces combine the advantages of speech recognition and natural language understanding to increase the potential of both. Spoken language is a natural, familiar and effective mode of cooperative problem solving. Many issues need to be addressed before such systems become a reality, including those inherited from speech recognition and natural language understanding. In this paper we address issues that arise when the two are combined.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-10"
  },
  "mousel89_eurospeech": {
   "authors": [
    [
     "P.",
     "Mousel"
    ],
    [
     "Jean-Marie",
     "Pierrel"
    ],
    [
     "A.",
     "Roussanaly"
    ]
   ],
   "title": "Cooperation and representation of syntactic-semantic and pragmatic knowledge in a natural language task oriented spoken dialogue system",
   "original": "e89_1183",
   "page_count": 4,
   "order": 14,
   "p1": "1183",
   "pn": "1186",
   "abstract": [
    "Natural language task oriented spoken dialogue systems require highly integrated processing of various knowledge sources: syntax, semantics and pragmatics. This paper presents the DIAL system being developed at present in Nancy. The system's architecture will be unfold as will be the linguistic models used by the system's components.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-11"
  },
  "matrouf89_eurospeech": {
   "authors": [
    [
     "K.",
     "Matrouf"
    ],
    [
     "Francoise",
     "Néel"
    ],
    [
     "Jean-Luc",
     "Gauvain"
    ],
    [
     "Joseph",
     "Mariani"
    ]
   ],
   "title": "Adaptive syntax representation in an oral task-oriented dialogue for air-traffic controller training",
   "original": "e89_1187",
   "page_count": 4,
   "order": 15,
   "p1": "1187",
   "pn": "1190",
   "abstract": [
    "We present here an oral dialogue system designed for air-traffic controller training with an air-traffic simulator. The system has been developed with two main objectives in mind: we try to make it both habitable and robust. The dialogue manager is designed to be able to cope with actual operational conditions and to allow users flexibility and freedom in particularly in the choice of their formulations and their pronunciation modes. Knowledge sources comprising the vocabulary, the phraseology, the universe of the task and the history of dialogue are represented in the form of hierarchized frames. The speech recognition system which is being used is AMADEUS developed at the LIMSI. It is a DTW-based continuous speech recognizer. The language model used is represented by binary rules with frequential weights obtained from automatic training on corpora. Weights are dynamically modified during the dialogue according to the universe state of the task, the dialogue history and even to the user's message. Those weights make some messages allowed or forbidden.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-12"
  },
  "hunnicutt89_eurospeech": {
   "authors": [
    [
     "Sheri",
     "Hunnicutt"
    ]
   ],
   "title": "Using syntactic and semantic information in a word prediction aid",
   "original": "e89_1191",
   "page_count": 3,
   "order": 16,
   "p1": "1191",
   "pn": "1193",
   "abstract": [
    "An initial attempt has been made to include syntactic and semantic information in programs for word prediction. A study has begun in which 1500 words in a. Swedish lexicon have been marked with semantic categories. Graphical semantic overviews and sketches of four texts have been composed using this classification. These graphical descriptions appear to be faithful to the subject matter. Small changes in prediction priority depending on semantic and syntactic information have not as yet resulted in keystroke savings. However, these are only the first of a number of planned uses of this information.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-13"
  },
  "inoue89_eurospeech": {
   "authors": [
    [
     "Naomi",
     "Inoue"
    ],
    [
     "Tsuyoshi",
     "Morimoto"
    ],
    [
     "Kentarou",
     "Ogura"
    ]
   ],
   "title": "A linguistic knowledge base for applying semantic information to a speech understanding system",
   "original": "e89_1194",
   "page_count": 4,
   "order": 17,
   "p1": "1194",
   "pn": "1197",
   "abstract": [
    "We have developed a knowledge base which describes the semantic co-occurrence between words and the inference mechanism on it. Using this knowledge base, it is possible to predict the next words likely to be uttered in the context. Also, this knowledge base is automatically created from ATR's linguistic database and can easily be constructed for a large vocabulary in a specific domain. This paper describes the structure of the knowledge base and the inference method.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-14"
  },
  "kitano89_eurospeech": {
   "authors": [
    [
     "Hiroaki",
     "Kitano"
    ],
    [
     "Hideto",
     "Tomabechi"
    ],
    [
     "Teruko",
     "Mitamura"
    ],
    [
     "Hitoshi",
     "Iida"
    ]
   ],
   "title": "A massively parallel model of speech-to-speech dialog translation: a step toward interpreting telephony",
   "original": "e89_1198",
   "page_count": 4,
   "order": 18,
   "p1": "1198",
   "pn": "1201",
   "abstract": [
    "This paper describes the overall picture of #DmDialog. #DmDialog is a real-time Japanese-English speech-to-speech dialog translation system that accepts speaker- independent continuous speech inputs. The scientific focus of the project is to model the cognitive process of simultaneous interpreters. As a result, the architecture of the system is very different from machine translation systems. Our model assumes hybridized parallelism as a basic computation mechanism, and the process of translation is highly interactive due to the dynamic participation of knowledge from morphophonetic-level to discourse-level. An almost concurrent parsing and generation scheme provides a simultaneous interpretation capability which is essential to interpreting telephony. #DmDialog has been publicly demonstrated at the Center for Machine Translation at Carnegie Mellon University.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-15"
  },
  "collier89_eurospeech": {
   "authors": [
    [
     "René",
     "Collier"
    ]
   ],
   "title": "Intonation analysis: the perception of speech melody in relation to acoustics and production",
   "original": "e89_1038",
   "page_count": 7,
   "order": 19,
   "p1": "1038",
   "pn": "1044",
   "abstract": [
    "For approximately one century, the study of intonation has attracted a great deal of scholarly attention, despite the notorious difficulty to analyse by ear this evasive property of spoken language. Probably, the chief impetus to this long-standing effort stems from the conviction that speech melody has a particular communicative value. Indeed, individual speech sounds do not carry an intrinsic meaning of their own, whereas intonation and other prosodic features seemingly add something to the content of a message that is not already expressed in the semantics of its individual words, nor in their syntactic relations. Therefore, most intonation studies were to be found originally in traditional linguistics. However, even if intonation did not serve some communicative purpose, it would still be necessary to study in detail the melodic structure of utterances if one wants to design rules for the automatic control of fundamental frequency in the electronic synthesis of speech. Therefore, the study of intonation is also relevant for present-day speech technology.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-16"
  },
  "avesani89_eurospeech": {
   "authors": [
    [
     "C.",
     "Avesani"
    ]
   ],
   "title": "Towards a model of Italian intonation",
   "original": "e89_1045",
   "page_count": 1,
   "order": 20,
   "p1": "1045",
   "pn": "",
   "abstract": [
    "This paper presents a model of Italian intonation included in a speech synthesis system developed at CSELT (Torino). The model is based on two theoretical assumptions. The first concerns the structure of the intonational system: following Pierrehumbert (1980) and Liberman & Pierrehumbert (1984), we think that intonation is the result of the interaction of principled choices in four different systems: phrasing, tune, pitch accent placement and pitch range. The second concerns the actual shape of F0 contours: we think is determined by categorical choices made from a small set of discrete phonological elements (Ladd, 1987).\n",
    ""
   ]
  },
  "mertens89_eurospeech": {
   "authors": [
    [
     "P.",
     "Mertens"
    ]
   ],
   "title": "Automatic recognition of intonation in French and dutch",
   "original": "e89_1046",
   "page_count": 5,
   "order": 21,
   "p1": "1046",
   "pn": "1050",
   "abstract": [
    "A computer program is described which converts the speech signal into a string of intonation units, connected to the syllable chain. The main steps in this system are [1] the segmentation into syllabic units, [2] the perceptual integration of prosodic parameters within each syllabic nucleus, [3] the assignment of prosodic features and [4] the parsing of the resulting chain of feature bundles according to a language-specific intonation grammar.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-17"
  },
  "hertrich89_eurospeech": {
   "authors": [
    [
     "Ingo",
     "Hertrich"
    ],
    [
     "R. D.",
     "Gartenberg"
    ]
   ],
   "title": "A new method in intonation research using partly controlled, simulated dialogues",
   "original": "e89_1051",
   "page_count": 4,
   "order": 22,
   "p1": "1051",
   "pn": "1054",
   "abstract": [
    "Adequate speech recordings for intonation research should meet the following conditions: (1) Naturalness, (2) Control of semantic, linguistic and pragmatic content, (3) Standardization of the recording situation. A method is described here using simulated dialogues comprising two alternating roles: Role one serves to set the desired situational framework for subsequent responses and is prerecorded on tape. Role two consists of fixed responses presented on a monitor screen at the appropriate moment in the dialogue. A subject is requested to play the 'life' dialogue partner listening to role one and using the presented responses. His/her spoken utterances are recorded on tape for subsequent analysis. Some possible applications are discussed.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-18"
  },
  "vaissiere89_eurospeech": {
   "authors": [
    [
     "Jacqueline",
     "Vaissière"
    ]
   ],
   "title": "On automatic extraction of prosodic information for automatic speech recognition system",
   "original": "e89_1202",
   "page_count": 4,
   "order": 23,
   "p1": "1202",
   "pn": "1205",
   "abstract": [
    "This paper is concerned with three types of causes leading to errors in a system using strictly speaker-independent rules for automatic extraction of linguistic information from measured prosodic parameters (PP) in read isolated sentences, in French: erroneous measurements of PP, duration and fun- damental frequency (type-1 errors); differences between speakers who do not fit into the same prosodic moult (type-2 problems) and certain combination of segmental influences on duration, which cannot be factored out in a strictly bottom-up system (type-3 problems). It suggests that neither further tuning of the existing rules, nor statistical learning are complete solutions. Type-1 errors are extrinsic to the prosodic module and can be hardly improved. An effective way of reducing incertainties due to type-2 problems is a partial tuning of the set of rules to the particular habits of the speaker: adaptation is feasible because there is a remarkable intra-speaker consistency in prosodic patterning, at least in serially read isolated sentences. Type-3 errors leads to multiple solutions in certain cases. It is necessary therefore to model to a certain extent the inter-speaker variability.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-19"
  },
  "howard89_eurospeech": {
   "authors": [
    [
     "I. S.",
     "Howard"
    ],
    [
     "J. R.",
     "Walliker"
    ]
   ],
   "title": "The implementation of a portable real-time multilayer-perceptron speech fundamental period estimator",
   "original": "e89_1206",
   "page_count": 4,
   "order": 24,
   "p1": "1206",
   "pn": "1209",
   "abstract": [
    "This paper describes the real-time implementation of a speech fundamental period estimation algorithm known as MLP-TX. The algorithm uses a multi-layer perceptron (MLP) classifier to determine the location of the points in time of vocal fold closure in a noisy speech signal. The real-time implementation was carried out in assembly language using a TMS320C25 signal processor. The system will be used in the next generation of SiVo hearing aids developed by the EPI group at UCL. These aids provide voice frequency information as a guide to Lipreading and voice control for the profoundly deaf.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-20"
  },
  "batliner89_eurospeech": {
   "authors": [
    [
     "Anton",
     "Batliner"
    ],
    [
     "Elmar",
     "Nöth"
    ]
   ],
   "title": "The prediction of focus",
   "original": "e89_1210",
   "page_count": 4,
   "order": 25,
   "p1": "1210",
   "pn": "1213",
   "abstract": [
    "We present results on how focus is marked intonationalty in German. Several speakers produced a large corpus of sentences. The corpus was constructed in a way that sentence modality and place of focus could only be differentiated by intonational means. Acoustic features representing the intonational parameters pitch, duration, and intensity, were extracted manually or automatically. The relevance of these features and the effect of several transformations were tested with statistical methods. Perceptual experiments where the listeners had to judge the naturalness and categories of the utterances were performed as well By calculating average values for the (appropriately transformed) relevant features we found \"normal\", prototypical cases. We will show that by looking at utterances where all listeners agreed on the naturalness and (intended) categories we arrived at coinciding results. At the same time we found \"unusual\" but regular productions.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-21"
  },
  "quene89_eurospeech": {
   "authors": [
    [
     "Hugo",
     "Quené"
    ],
    [
     "René",
     "Kager"
    ]
   ],
   "title": "Automatic accentuation and prosodic phrasing for dutch text-to-speech conversion",
   "original": "e89_1214",
   "page_count": 4,
   "order": 26,
   "p1": "1214",
   "pn": "1218",
   "abstract": [
    "Correct accentuation and phrasing improves the quality of synthetic speech. This paper discusses an algorithm which assigns both sentence accents and phrase boundaries on the basis of the prosodic sentence structure. Although this latter structure is theoretically derived from the syntactic structure, the present algorithm determines the prosodic structure by means of (linguistically and statistically motivated) rules of thumb, i.e., without exhaustive syntactic analysis. Subsequently, both prase boundaries and sentence accents are assigned on the basis of this prosodic sentence structure. In this component, several factors affecting accentuation are imitated by rules which do not directly refer to the syntactic or thematic structure.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-22"
  },
  "bezooijen89_eurospeech": {
   "authors": [
    [
     "Renée van",
     "Bezooijen"
    ],
    [
     "Louis C. W.",
     "Pols"
    ]
   ],
   "title": "Evaluation of a sentence accentuation algorithm for a dutch text-to-speech system",
   "original": "e89_1218",
   "page_count": 4,
   "order": 27,
   "p1": "1218",
   "pn": "1221",
   "abstract": [
    "In this contribution an algorithm for the automatic assignment of sentence accents in written Dutch, developed by Kager and Quené, is evaluated experimentally. Results show that the output of the algorithm is judged significantly more adequate than accents randomly distributed over content words but significantly less adequate than the accents produced by a trained news broadcaster and those realized by the majority of the subjects themselves. Insight was sought into the basis of the adequacy judgments and suggestions are made for the improvement of the algorithm.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-23"
  },
  "martin89_eurospeech": {
   "authors": [
    [
     "Philippe",
     "Martin"
    ]
   ],
   "title": "Automatic assignment of lexical stress in Italian",
   "original": "e89_1222",
   "page_count": 4,
   "order": 28,
   "p1": "1222",
   "pn": "1225",
   "abstract": [
    "An automatic method to assign lexical stress in Italian from a written text is presented here. This method is based on specific morphological properties of the Italian morphemes, which can only be stressed on the last or the penultimate syllable. Using a morphological analyzer and a morpheme database, the assignment program attempts to analyze each entry into a morpheme followed by one or more suffixes and flexions, which can contain a stressable syllable. The resulting stressed syllable is determined by applying a simple final stress rule to these elements.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-24"
  },
  "hieronymus89_eurospeech": {
   "authors": [
    [
     "James L.",
     "Hieronymus"
    ]
   ],
   "title": "Automatic sentential vowel stress labelling",
   "original": "e89_1226",
   "page_count": 4,
   "order": 29,
   "p1": "1226",
   "pn": "1229",
   "abstract": [
    "There is general agreement that sentential syllable vowel stress (called prominence by some authors) in American English is marked by pitch rise-falls, energy, and duration. None of these cues by themselves is sufficient, instead combinations of these cues are used by talkers to signal stress in continuous speech. After studying the stress marking strategies of 15 talkers of American English, an algorithm was devised which labels vowels with three levels of stress. The algorithm is based on combinations of pitch rise falls, relative energy and duration. The pitch is determined automatically in all voiced regions in the sentence. Then the regions are characterised as having rising pitch, falling pitch or steady pitch. Sequences of three regions are examined to find the pitch rise fall patterns which signal stress. The energy in the band 0-2500 Hz is determined throughout the utterance. All the energy measurements are made relative to the maximum energy in the sentence. If the energy of the vowel is within 11 db of the maximum it is considered energy stressed. The duration is determined from hand labels in the present implementation. Duration is corrected for prepausal effects. If two out of three cues are present, then the vowel is labelled stressed. If the vowel has the highest energy, longest duration, and highest pitch then it is labeled as highly stressed. If the vowel has very low energy relative to the loudest sound in the sentence, then it is labeled unstressed no matter what the other two cues indicate. The algorithm was tested on 125 sentences of American English and found to perform very well. The pitch stress was the most difficult. Detailed analysis of the results show that approximately 85 % of the syllables are correctly stress labelled.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-25"
  },
  "peeters89_eurospeech": {
   "authors": [
    [
     "Willem J. M.",
     "Peeters"
    ],
    [
     "William J.",
     "Barry"
    ]
   ],
   "title": "Diphthong dynamics: production and perception in southern british English",
   "original": "e89_1055",
   "page_count": 4,
   "order": 30,
   "p1": "1055",
   "pn": "1058",
   "abstract": [
    "The preferred temporal structure of the isolated diphthongs /ai/ and /au/ in Southern English is examined by a means of paired comparison test using a comprehensive range of dynamic trajectory variants. This is compaired with the temporal structure of those diphthongs produced by 10 speakers of Southern English. Results indicate a clear language-specific trajectory, differing from those found in tests with German and Dutch listeners. The English production data are more complex in their manifestations, but many tokens approximate the same language-specific dynamic structure.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-26"
  },
  "datscheweit89_eurospeech": {
   "authors": [
    [
     "W.",
     "Datscheweit"
    ]
   ],
   "title": "Quantitative measurement of the influence of acoustic cues on the perception of voiced plosives",
   "original": "e89_1059",
   "page_count": 4,
   "order": 31,
   "p1": "1059",
   "pn": "1062",
   "abstract": [
    "A series of experiments was conducted to investigate the role of formant transitions on the perception of the voiced plosives /b/, /d/ and /g/. In order to take into account the contextual influence of the surrounding vowels, vowel-consonant-vowel-units with the vowels /a:/,/u:/ and /i:/ were used as speech stimuli. Based on naturally spoken samples the stimuli were systematically manipulated with regard to different acoustic cues. The loci of the second and third formants (according to Delattre's locus-theory) were varied. Subjects had to estimate their perception of the different plosives. The results show that formant transitions affect the perception very much, but F2-locus alone does not discriminate the different voiced plosives. A combination of F2-and F3-loci may be efficient cues to distinguish between /b/, /d/ and /g/.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-27"
  },
  "schwartz89_eurospeech": {
   "authors": [
    [
     "Jean-Luc",
     "Schwartz"
    ],
    [
     "Louis Jean",
     "Boe"
    ],
    [
     "Pascal",
     "Perrier"
    ],
    [
     "Bernard",
     "Guérin"
    ],
    [
     "Pierre",
     "Escudier"
    ]
   ],
   "title": "Perceptual contract and stability in vowel systems: a 3-d simulation study",
   "original": "e89_1063",
   "page_count": 4,
   "order": 32,
   "p1": "1063",
   "pn": "1066",
   "abstract": [
    "Since the beginning of the 70s, several attempts have been made to explain the phonetic structure of vowel systems by introducing extralinguistic principles, listener-orientated (perceptual contrast and stability) or speaker-orientated (articulatory contrast and economy). The best predictions have been realized with the perceptual contrast theory (PCT), but two main problems remain: the too great number of high non-peripheral vowels and the impossibility to predict the [i,y,u] series within the high vowels set. We try to get rid of these difficulties while staying within the field of listener-orientated principles. First, we study he PCT in the F1-F2-F3 space, in order to better account for the role of higher formants in the perception of front vowels. In this space, we show that the problem of high non-peripheral vowels can be solved with an increased weight of Fl, but the case of [y] can only be understood by reinforcing the stability of the [i]-[y] pair. This is done by means of a \"focalization\" principle, according to which vowels with strong formant convergence - [i] characterized by a strong F3-F4 convergence and [y] characterized by a strong F2-F3 convergence - would be perceptually prefered.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-28"
  },
  "watson89_eurospeech": {
   "authors": [
    [
     "Ian M. C.",
     "Watson"
    ],
    [
     "Marianne",
     "McCormick"
    ],
    [
     "Franz",
     "Seitz"
    ],
    [
     "Anthony",
     "Bladon"
    ],
    [
     "Rosalind",
     "Temple"
    ]
   ],
   "title": "The use of perceptually scaled spectra in across-talker algorithmic classification of british English stop consonants",
   "original": "e89_1067",
   "page_count": 4,
   "order": 33,
   "p1": "1067",
   "pn": "1070",
   "abstract": [
    "A large-scale study of British English stop consonants investigated the usefulness of perceptually motivated spectral scaling (Bark scale, phones, broad band integration) in the algorithmic discrimination of place of articulation. 2947 tokens of syllable-initial stop consonants were processed to give representations scaled in dB/Hz., phon/Bark with a 1 Bark frequency smearing filter and phon/Bark with a 3 Bark frequency smearing filter. Each representation was then examined in a search for features robustly associated with place of articulation distinctions. Two types of features were sought, dynamic (patterns of spectral change) and static (features of the burst spectrum). No reliable dynamic features were established, but a static feature based on the frequency distribution of the two main spectral peaks in the burst was effective, especially in the phon/Bark/1 Bark frequency-smeared representations (77.4% correct classification). The addition of a second feature based on the amplitude difference between these two main peaks further improved classification to 81.3% for the phon/Bark/1 Bark frequency-smeared representations.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-29"
  },
  "broeders89_eurospeech": {
   "authors": [
    [
     "Ton",
     "Broeders"
    ],
    [
     "Toni",
     "Rietveld"
    ]
   ],
   "title": "Segmental marking as a cue in auditory voice identification of telephone speech",
   "original": "e89_1071",
   "page_count": 4,
   "order": 34,
   "p1": "1071",
   "pn": "1074",
   "abstract": [
    "The present investigation seeks to assess to what extent some Dutch phonemes can serve as cues in auditory voice identification of telephone speech. Four Dutch consonants, /x/, /r/, /s/ and /p/, occurring in three different contexts in 24 monosyllabic words, were selected on the basis of their different auditory interspeaker variability. The words were pronounced by 16 speakers in two guises, ordinary chest voice and falsetto. Although none of the consonants appeared to perform significantly better, an interaction was found between type of consonant and position in the word. It also appears that the confidence ratings assigned to the responses vary significantly as a function of the type of consonant and its position in the word.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-30"
  },
  "fesseler89_eurospeech": {
   "authors": [
    [
     "Peter",
     "Fesseler"
    ],
    [
     "Heidi",
     "Hackbarth"
    ],
    [
     "Marianne",
     "Kugler"
    ],
    [
     "Arnd",
     "Boehm"
    ]
   ],
   "title": "Automatic vocabulary extension for a speaker-adaptive speech recognition system based on CVC units",
   "original": "e89_1075",
   "page_count": 4,
   "order": 35,
   "p1": "1075",
   "pn": "1078",
   "abstract": [
    "For speech recognition with large vocabularies, a user should not be burdened with having to train several thousand words explicitly. Therefore, it proves extremely useful to provide a means for easy vocabulary generation and enlargement from written text input. Applying a set of appropriately defined rules, the orthography of a lexicon item is first transcribed into the phonetic symbols of the standard pronunciation and the most common alternatives thereof. From these, the multiple sequence of specific subword units of a lexicon entry is produced. The tool introduced here is part of a comprehensive speech processing system for subword-unit based, speaker-adaptive recognition of continuous speech.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-31"
  },
  "scagliola89_eurospeech": {
   "authors": [
    [
     "Carlo",
     "Scagliola"
    ],
    [
     "Cesare",
     "Vicenzi"
    ],
    [
     "Angelo",
     "Carossino"
    ],
    [
     "Donatella",
     "Sciarra"
    ]
   ],
   "title": "Iterative optimization of sub-word templates for speech recognition",
   "original": "e89_1079",
   "page_count": 4,
   "order": 36,
   "p1": "1079",
   "pn": "1082",
   "abstract": [
    "An iterative optimization procedure to derive reliable sub-word templates from training speech material is described. At each step sub-word occurrences are located and new templates computed in such a way that the overall distance of speech material from the language model approaches an absolute minimum. Different options are discussed and experimental results presented.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-32"
  },
  "monnet89_eurospeech": {
   "authors": [
    [
     "F.",
     "Monnet"
    ],
    [
     "S.",
     "Jousset"
    ],
    [
     "A.",
     "Demour"
    ],
    [
     "P.",
     "Richard"
    ]
   ],
   "title": "The IKAROS continuous speech understanding system: first demonstrator",
   "original": "e89_1083",
   "page_count": 1,
   "order": 37,
   "p1": "1083",
   "pn": "",
   "abstract": [
    "The aim of the IKAROS (Esprit project 954) is to evaluate the interest of Artificial Intelligence techniques in speech understanding. This is achieved by designing and implementing a system whose features include continuous speech, multispeaker, vocabulary of up to one thousand words, natural-type spoken language, multilingualism and railway timetable enquiry as application. A set of knowledge sources (KSs) for various levels (acoustic-phonetic, phonological, syntactic semantic and pragmatic) and an architecture that permits the closest co-operation between these KSs have been developed.\n",
    ""
   ]
  },
  "fournol89_eurospeech": {
   "authors": [
    [
     "D.",
     "Fournol"
    ],
    [
     "C.",
     "Godin"
    ],
    [
     "Y.",
     "Guidon"
    ],
    [
     "P.",
     "Richard"
    ]
   ],
   "title": "The spin continuous-speech decoding system",
   "original": "e89_1084",
   "page_count": 4,
   "order": 38,
   "p1": "1084",
   "pn": "1087",
   "abstract": [
    "The SPIN speaker independent, continuous speech decoding system has been developed to compare several training methods as well as several decoding algorithms within a whole integrated system. This system runs on a limited application, in French. Demisyllables are used as basic sub-word units. They are extracted from the application corpus semi-automatically. Speech units are modelled by different types of HMM-based techniques. A set of DTW/Viterbi-based decoding methods is available to perform recognition of sentences. The output of the decoding process can be either a lattice of syllabic units or a single string of decoded words. The lattice output is devoted to provide an AI speech understanding system with a suitable phonetic input. An evaluation module has been designed to evaluate the potentiality of the decoded lattices. The system is fully integrated in a software graphic interface which simplifies the management of the different modules and the understanding of the various outputs.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-33"
  },
  "dermatas89_eurospeech": {
   "authors": [
    [
     "E.",
     "Dermatas"
    ],
    [
     "Nikos",
     "Fanotakis"
    ],
    [
     "George",
     "Kokkinakis"
    ]
   ],
   "title": "Improved speaker independent IWRS for small vocabularies",
   "original": "e89_1088",
   "page_count": 2,
   "order": 39,
   "p1": "1088",
   "pn": "1090",
   "abstract": [
    "A speaker-independent isolated word recognition system (IWRS) using pattern matching is presented. The novelty of this new system lies in its structure and in the introduction of new and improved algorithms in several stages. A short description of the overall system is as follows.\n",
    ""
   ]
  },
  "buttafava89_eurospeech": {
   "authors": [
    [
     "P.",
     "Buttafava"
    ],
    [
     "Roberto",
     "Billi"
    ],
    [
     "W.",
     "Digiampietro"
    ],
    [
     "G.",
     "Massia"
    ],
    [
     "V.",
     "Vittorelli"
    ]
   ],
   "title": "Architecture and implementation of the olivetti PC-based very large vocabulary isolated word speech recognition system",
   "original": "e89_1091",
   "page_count": 3,
   "order": 40,
   "p1": "1091",
   "pn": "1093",
   "abstract": [
    "Referring to the associated paper \"A PC-Based Large Vocabulary Isolated Word Speech Recognition System\" from Roberto Billi et al, [1], this paper describes the structural choices and the software and hardware implementation of the system.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-34"
  },
  "addadecker89_eurospeech": {
   "authors": [
    [
     "Martine",
     "Adda-Decker"
    ]
   ],
   "title": "Continuous speech recognition using phone-based anchor point detection and diphone-based dp-matching",
   "original": "e89_1094",
   "page_count": 4,
   "order": 41,
   "p1": "1094",
   "pn": "1097",
   "abstract": [
    "This paper deals with acoustic-phonetic decoding for CSR. There are two different processing modules depending on the steady or transient nature of the speech input. First the steady state speech processing module, called phone-based anchor point detection, performs some preprocessing allowing the selection of only a subset of the vocabulary under consideration. Secondly a general processing module, based on diphone-like units, performs the actual decoding task. The work focuses on the combined use of preprocessing issued information and lexical knowledge to guide the main decoding stage. Speaker-dependent experiments were run on 50 CVCV sentences. A 275 word size lexicon is used to evaluate word recognition performances. No syntactical information being included, the perplexity of the language is about 140 due to the CVCV constraint imposed on word sequences. Results will be detailed for content and function words. The global word recognition rate is about 85 %.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-35"
  },
  "bundgaard89_eurospeech": {
   "authors": [
    [
     "Michael",
     "Bundgaard"
    ]
   ],
   "title": "Statistical analysis of large-scale lexical corpuses in the context of continuous speech recognition systems (CSR systems)",
   "original": "e89_1098",
   "page_count": 4,
   "order": 42,
   "p1": "1098",
   "pn": "1101",
   "abstract": [
    "A number of statistical properties of allophonic and prosodic distributions of the 8000 most frequent words of Danish were investigated. With CSR in mind the distributions of the words were examined with different degrees of partial description of the allophones and considering also to some extent incorrect segmentation. The effect of disregarding unstressed syllables as well as entire allophone groups in the lexicon lookup was examined. The metric characteristics of the initial syllable at word level was uncovered in the efforts to find a way of detecting syllable boundaries.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-36"
  },
  "marino89_eurospeech": {
   "authors": [
    [
     "Jose B.",
     "Marino"
    ],
    [
     "Climent",
     "Nadeu"
    ],
    [
     "Asunción",
     "Moreno"
    ],
    [
     "Eduardo",
     "Lleida"
    ],
    [
     "Enric",
     "Monte"
    ]
   ],
   "title": "Recognition of numbers and strings of numbers by using demisyllables: one speaker experiment",
   "original": "e89_1102",
   "page_count": 4,
   "order": 43,
   "p1": "1102",
   "pn": "1105",
   "abstract": [
    "This communication reports the use of demisyllables for continuous speech recognition in a specific application: the recognition of Spanish numbers. After a brief outline of the recognition system, a description of demisyllable syntactic constraints and one-speaker reference generation is provided. Finally, the recognition performance is assessed by means of two experiments: the recognition of integer numbers from zero to one thousand and telephone numbers uttered in a Spanish way (strings of integers from zero to ninety nine), in both applications the results that the system yielded were excellent.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-37"
  },
  "colangeli89_eurospeech": {
   "authors": [
    [
     "Giulio",
     "Colangeli"
    ],
    [
     "Filippo",
     "Ardito"
    ]
   ],
   "title": "A transputer based system for parallel dynamic time warping",
   "original": "e89_1106",
   "page_count": 4,
   "order": 44,
   "p1": "1106",
   "pn": "1109",
   "abstract": [
    "In this paper the use of the Transputer, as a basic processing element, to implement a parallel approach for Dynamic Time Warping algorithm will be described. A practical system, based on two levels hierarchic architecture, for parallel algorithms development in speech recognition field will be also introduced.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-38"
  },
  "noyes89_eurospeech": {
   "authors": [
    [
     "J. M.",
     "Noyes"
    ],
    [
     "Clive R.",
     "Frankish"
    ]
   ],
   "title": "Gender differences in speech recognition performance",
   "original": "e89_1110",
   "page_count": 3,
   "order": 45,
   "p1": "1110",
   "pn": "1112",
   "abstract": [
    "The achievement of good recognition performance is an important factor in determining the viability of present speech recognisers. A person's ability to maintain consistent speech patterns is a prerequisite for attaining acceptable recognition rates. This consistency in pronunciation varies between individuals and gender is thought to be a contributing variable. Two experiments primarily concerned with feedback in speech recognition systems are outlined with particular reference to gender differences. Findings include: (i) a significant overall difference between male and female recognition rates; (ii) identification of a number of users, all female, for whom recognition was exceptionally poor. Remedial actions are suggested in order to help improve the recognition performance of such users.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-39"
  },
  "carlson89_eurospeech": {
   "authors": [
    [
     "Rolf",
     "Carlson"
    ],
    [
     "Björn",
     "Granström"
    ],
    [
     "Anders",
     "Lindstrom"
    ]
   ],
   "title": "Predicting name pronunciation for a reverse directory service",
   "original": "e89_1113",
   "page_count": 4,
   "order": 46,
   "p1": "1113",
   "pn": "1116",
   "abstract": [
    "Text-to-speech systems are normally not suited for name pronunciation in, for example, an automatized reverse directory service. A new project has been started to alleviate these problems. First names, surnames and street names corpora from the Greater Stockholm telephone directory has been studied. The surnames corpus has been tagged as to language origin. Both rule based methods and an artificial neural net classifier has been tried to predict this information. Specialized rule components are currently being written to predict the pronunciation for names of different origin.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-40"
  },
  "spiegel89_eurospeech": {
   "authors": [
    [
     "Murray F.",
     "Spiegel"
    ],
    [
     "Marian J.",
     "Macchi"
    ],
    [
     "Kurt D.",
     "Gollhardt"
    ]
   ],
   "title": "Synthesis of names by a demisyllable-based speech synthesizer (SPOKESMAN)",
   "original": "e89_1117",
   "page_count": 4,
   "order": 47,
   "p1": "1117",
   "pn": "1120",
   "abstract": [
    "Many applications for text-to-speech synthesis involve the translation of names and/or addresses. However, most commercially available synthesizers are particularly poor at synthesizing names, since the focus of their development is typically synthesizing words. Names in the United States, for example, are often pronounced in accordance with rules different from the rules of English words. The proper pronunciation of names is particularly difficult, since there are so many (America has over 1.5 million different family names) and they derive from dozens of languages.\n",
    "Bellcore is developing an LPC demisyllable-based synthesizer called \"SPOKESMAN,\" which has been tailored for the synthesis of names and addresses. This paper describes the need for good name pronunciation capabilities and high segmental intelligibility in key applications and provides an overview of SPOKESMAN'S programs. Evaluation tests find that SPOKESMAN has higher segmental intelligibility than commercially available synthesizers and scores higher in preference tests.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-41"
  },
  "king89_eurospeech": {
   "authors": [
    [
     "Robin W.",
     "King"
    ]
   ],
   "title": "Layout processing, user control and prosody insertion in an on-line synthetic speech system",
   "original": "e89_1121",
   "page_count": 4,
   "order": 48,
   "p1": "1121",
   "pn": "1124",
   "abstract": [
    "For synthesised speech to be an effective substitute for a visual display it is necessary to divide the displayed text into a set of meaningful utterable elements, and provide the user with controls to select these elements in a meaningful sequence. The textual content of visually conceived data-bases may well have value for blind users, but their page layouts are frequently complex and would give rise to ambiguities and errors in a simple display-to-speech process which utters each line of the display in turn.\n",
    "The development of software-based layout processing to produce an ordered sequence of meaningful utterable elements linked to the user controls is discussed in the context of implementing a PC-based synthetic speech system for on-line interaction with videotex. Computationally low-cost techniques for generating stress, rhythm and intonation patterns for the synthesised output are also outlined, and consideration is given to generating prosodic enhancement of the synthesised speech output from significant features of the display layout. Further application of these techniques to other synthetic speech systems is addressed briefly.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-42"
  },
  "ainsworth89_eurospeech": {
   "authors": [
    [
     "William A.",
     "Ainsworth"
    ],
    [
     "B.",
     "Pell"
    ]
   ],
   "title": "Connectionist architectures for a text-to-speech system",
   "original": "e89_1125",
   "page_count": 4,
   "order": 49,
   "p1": "1125",
   "pn": "1128",
   "abstract": [
    "Previous work has demonstrated that multilayer perceptrons may be used to transcribe orthographic text into a phonemic representation. It is shown that this technique may be applied to a large dictionary of over 70,000 words without reduction in performance. The dictionary was divided into sub-dictionaries containing regularly and irregularly pronounced words using a rule-based system. The multilayer perceptron performed better with the regularly pronounced words. However, combining two separately trained networks resulted in little better performance than a single network trained on the whole dictionary.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-43"
  },
  "bosch89_eurospeech": {
   "authors": [
    [
     "L. F. M. ten",
     "Bosch"
    ],
    [
     "René",
     "Collier"
    ],
    [
     "Louis",
     "Boves"
    ]
   ],
   "title": "From diphones to allophones: from data to rules",
   "original": "e89_1129",
   "page_count": 3,
   "order": 50,
   "p1": "1129",
   "pn": "1131",
   "abstract": [
    "A research project is presented in which we aim to design a speech synthesis model based on both the diphone and the allophone concepts, i.e. the data-driven and rule-driven approach for speech synthesis, respectively. At present, diphone concatenation for Dutch leads to more intelligible speech than when a rule-based allophone synthesis is applied, although the latter synthesis has the theoretical advantage of optimal parametric freedom. However, in rule-based speech synthesis the iterative adjustment of the relevant speech parameters appears to be rather laborious. Therefore, an attempt is made to extract information about rules for allophone synthesis from the data-driven diphone synthesis (data-to-rule conversion) by means of a semi-automatic algorithm.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-44"
  },
  "loman89_eurospeech": {
   "authors": [
    [
     "H.",
     "Loman"
    ],
    [
     "Renée van",
     "Bezooijen"
    ],
    [
     "J.",
     "Kerkhoff"
    ],
    [
     "Louis",
     "Boves"
    ]
   ],
   "title": "A working environment and procedure for the development of speech synthesis rules",
   "original": "e89_1132",
   "page_count": 4,
   "order": 51,
   "p1": "1132",
   "pn": "1135",
   "abstract": [
    "For the development of phonetic rules in an allophone based text-to-speech system a strategy has been adopted which consists of three major elements, viz. (l) a tool that allows to test new rules without having to go through the time consuming process of compiling and linking, (2) efficient tools for obtaining acoustic data to build rules on, and (3) a schedule for a series of formal intelligibility tests. It is explained how these three elements make up a procedure for speech synthesis rule development: An interactive parameter adjustment module enables the rule developer to test the relevancy of speech data obtained by means of efficient signal processing techniques and intelligibility tests, before they are converted into a (set of) rule(s). The intelligibility tests carried out so far indicate that the adopted approach is promising.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-45"
  },
  "bailly89_eurospeech": {
   "authors": [
    [
     "Gérard",
     "Bailly"
    ],
    [
     "A.",
     "Tran"
    ]
   ],
   "title": "Compost: a rule-compiler for speech synthesis",
   "original": "e89_1136",
   "page_count": 4,
   "order": 52,
   "p1": "1136",
   "pn": "1139",
   "abstract": [
    "COMPOST is the result of our first reflections on the computing requirements for synthetic speech generation. It combines the basic facilities of a rule-oriented software with object-oriented design. COMPOST is a tree compiler: it manipulates and transforms not only lists of atoms but also trees and subtrees. Basic atoms involved into the tree inherit of properties of user defined classes . Features and numerical cues are associated with each atom. COMPOST has then numerical capabilities to do synthesis-by- rule. COMPOST has been developped to build a formant-based rule system but may generate any king of parametric trajectories and then could be used in rule-based articulatory synthesis.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-46"
  },
  "hirokawa89_eurospeech": {
   "authors": [
    [
     "Tomohisa",
     "Hirokawa"
    ]
   ],
   "title": "Speech synthesis using a waveform dictionary",
   "original": "e89_1140",
   "page_count": 4,
   "order": 53,
   "p1": "1140",
   "pn": "1143",
   "abstract": [
    "A new method for speech synthesis by concatenating waveforms selected from a dictionary is described. An adult male recorded a two-hour speech with acoustic phonetic labels. This data was used to construct the dictionary. The dictionary contains 35,000 waveforms which are identified by their duration, average pitch, pitch contour and average energy. The number of the phonetic labels is thirty-five. In the speech synthesis phase, given a phoneme string and prosody information, the optimum waveforms are selected by matching their attributes with the given phonetic and prosodic information. The matching score is defined as a function of phonetic coincidence and prosodic attribute differences. Selected waveforms are then concatenated to produce speech. The speech has high intelligibility and naturalness.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-47"
  },
  "backstrom89_eurospeech": {
   "authors": [
    [
     "Marina",
     "Bäckström"
    ],
    [
     "Ken",
     "Ceder"
    ],
    [
     "Bertil",
     "Lyberg"
    ]
   ],
   "title": "PROPHON - an interactive environment for text-to-speech conversion",
   "original": "e89_1144",
   "page_count": 4,
   "order": 54,
   "p1": "1144",
   "pn": "1147",
   "abstract": [
    "Prophon is an interactive environment for developing applications and conducting research in text-to-speech conversion. The environment, which is written in the programming language Prolog, can be considered as a shell for developing knowledge based systems. It is created specifically for the linguistic domain and encourages an incremental development of text-to-speech systems. Prophon offers a rich set of tools for creating rules at different levels.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-48"
  },
  "lee89_eurospeech": {
   "authors": [
    [
     "Kai-Fu",
     "Lee"
    ]
   ],
   "title": "Hidden Markov models: past, present, and future",
   "original": "e89_1148",
   "page_count": 8,
   "order": 55,
   "p1": "1148",
   "pn": "1155",
   "abstract": [
    "Hidden Markov models (HMMs) have recently become the predominant approach to speech recognition. In this paper, we will elucidate the success of hidden Markov models, using Sphinx as an example of an HMM-based system. We will also outline avenues for future research in hidden Markov models.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-49"
  },
  "bahl89_eurospeech": {
   "authors": [
    [
     "L. R.",
     "Bahl"
    ],
    [
     "S. V. De",
     "Gennaro"
    ],
    [
     "P. S.",
     "Gopalakrishnan"
    ],
    [
     "R. L.",
     "Mercer"
    ]
   ],
   "title": "A fast approximate acoustic match for large vocabulary speech recognition",
   "original": "e89_1156",
   "page_count": 3,
   "order": 56,
   "p1": "1156",
   "pn": "1158",
   "abstract": [
    "In a large vocabulary speech recognition system, a great deal of computer time is spent performing detailed acoustic matches of words in the vocabulary. In order to run in real time on a modest amount of hardware, it is important that these detailed matches be performed only on words which have a reasonable possibility of being correct. We describe a scheme for rapidly obtaining an approximate acoustic match of all of the words in the vocabulary in such a way as to ensure that the correct word is, with high probability, one of a small number of words examined in detail. We give experimental results showing the effectiveness of the fast match that we describe for a number of talkers. In particular, we give the average rank of the correct word, the average number of words suggested, and the number of times that the correct word is not among the words suggested.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-50"
  },
  "serralheiro89_eurospeech": {
   "authors": [
    [
     "A. J.",
     "Serralheiro"
    ],
    [
     "Y.",
     "Ephraim"
    ],
    [
     "Lawrence R.",
     "Rabiner"
    ]
   ],
   "title": "On nonstationary hidden Markov modeling of speech signals",
   "original": "e89_1159",
   "page_count": 4,
   "order": 57,
   "p1": "1159",
   "pn": "1162",
   "abstract": [
    "We propose an exact maximum likelihood (ML) approach for hidden Markov modeling of speech signals using models with mixtures of Gaussian autoregressive (AR) output probability distributions. This approach differs from the commonly used approach in two aspects. First, the parameters of the AR models are calculated using the exact, rather than the asymptotic, form of the likelihood function. Second, the gain of each AR model as well as its shape is estimated and used during the recognition phase. Since the asymptotic likelihood is appropriate only for sources which are stationary in some sense, the ML approach taken here can be considered as an approach for nonstationary modeling. The proposed approach was tested on the task of recognizing isolated versions of the English alphabet spoken by four different speakers by a system which was simultaneously trained for the four talkers (multi-speaker recognizer). This approach results in a recognition accuracy which is comparable to that obtained by the asymptotic ML approach.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-51"
  },
  "huang89_eurospeech": {
   "authors": [
    [
     "X. D.",
     "Huang"
    ],
    [
     "Hsiao-Wuen",
     "Hon"
    ],
    [
     "Kai-Fu",
     "Lee"
    ]
   ],
   "title": "Large-vocabulary speaker-independent continuous speech recognition with semi-continuous hidden Markov models",
   "original": "e89_1163",
   "page_count": 4,
   "order": 58,
   "p1": "1163",
   "pn": "1166",
   "abstract": [
    "A semi-continuous hidden Markov model based on the multiple vector quantization codebooks is used here for large-vocabulary speaker-independent continuous speech recognition. In the techniques employed here, the semi-continuous output probability density function for each codebook is represented by a combination of the corresponding discrete output probabilities of the hidden Markov model and the continuous Gaussian density functions of each individual codebook. Parameters of vector quantization codebook and hidden Markov model are mutually optimized to achieve an optimal model/codebook combination under a unified probabilistic framework. Another advantages of this approach is the enhanced robustness of the semicontinuous output probability by the combination of multiple codewords and multiple codebooks. For a 1000-word speaker-independent continuous speech recognition using a word-pair grammar, the recognition error rate of the semi-continuous hidden Markov model was reduced by more than 29% and 41% in comparison to the discrete and continuous mixture hidden Markov model respectively.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-52"
  },
  "varga89_eurospeech": {
   "authors": [
    [
     "Andrew",
     "Varga"
    ],
    [
     "Keith",
     "Ponting"
    ]
   ],
   "title": "Control experiments on noise compensation in hidden Markov model based continuous word recognition",
   "original": "e89_1167",
   "page_count": 4,
   "order": 59,
   "p1": "1167",
   "pn": "1170",
   "abstract": [
    "Noise compensation in speech recognition is a technique in which noise contamination is dealt with in the recognition phase rather than by some pre-processing system. In a previous paper, [1], we developed three noise compensation techniques for use in hidden Markov model based speech recognition. To date, however, no comprehensive examination and comparison of their performance has been carried out. This paper reports on such an examination and on the related topic of the use of a noise tracking \"silence\" model to \"recognise\" periods when there is no speech.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-53"
  },
  "imamura89_eurospeech": {
   "authors": [
    [
     "Akihiro",
     "Imamura"
    ],
    [
     "Hiroshi",
     "Hamada"
    ],
    [
     "Ryohei",
     "Nakatsu"
    ]
   ],
   "title": "Speaker-independent word recognition through telephone networks using hidden Markov models",
   "original": "e89_1171",
   "page_count": 4,
   "order": 60,
   "p1": "1171",
   "pn": "1174",
   "abstract": [
    "In this paper, we describe an experimental telephone based system that recognizes speaker-independent isolated words. The recognition method is based on discrete HMMs. We apply the following new techniques to the conventional discrete HMM method; interpolation of observation probabilities using Fuzzy Vector Quantization, multiple model construction, model training using expanded speech end-points, and state duration control using Gaussian windows. Experiments are carried out on Japanese digits spoken by 269 speakers (238 for training, 31 for evaluation). An improvement of about 4.5% in recognition accuracy is obtained with the new techniques.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-54"
  },
  "hon89_eurospeech": {
   "authors": [
    [
     "Hsiao-Wuen",
     "Hon"
    ],
    [
     "Kai-Fu",
     "Lee"
    ],
    [
     "Robert",
     "Weide"
    ]
   ],
   "title": "Towards speech recognition without vocabulary-specific training",
   "original": "e89_1481",
   "page_count": 4,
   "order": 61,
   "p1": "1481",
   "pn": "1484",
   "abstract": [
    "With the emergence of high-performance speaker-independent systems, a great barrier to man-machine interface has been overcome. This work describes our next step to improve the usability of speech recognizers - the use of vocabulary-independent (VI) models. If successful, VI models are trained once and for all. They will completely eliminate task-specific training, and will enable rapid configuration of speech recognizers for new vocabularies. Our initial results using generalized triphones as VI models show that with more training data and more detailed modeling, the error rate of VI models can be reduced substantially. For example, the error rates for VI models with 5,000, 10,000 and 15,000 training sentences are 23.9%, 15.2% and 13.3% respectively. Moreover, if task-specific training data were available, we can interpolate them with VI models. Our prelimenary results show that this interpolation can lead to an 18% error rate reduction over task-specific models.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-55"
  },
  "davies89_eurospeech": {
   "authors": [
    [
     "Peter",
     "Davies"
    ]
   ],
   "title": "Hidden Markov modelling of modern standard Chinese tones in connected speech",
   "original": "e89_1485",
   "page_count": 1,
   "order": 62,
   "p1": "1485",
   "pn": "",
   "abstract": [
    "This paper explores the possibilities of using Hidden Markov Models to represent, and to recognise, the tones in connected Modern Standard Chinese speech. The paper is based on data consisting of some 400 (currently being extended to 1000) syllables of connected speech by a single speaker. It has recently been shown (Yang et al. 1988) that HMM techniques can be used successfully to model the tones in isolated syllables, but the problem with the tones in connected speech is that they are subject to a good deal of variation. Syllable stress is one important conditioning factor involved in this variation.\n",
    ""
   ]
  },
  "wright89_eurospeech": {
   "authors": [
    [
     "J. H.",
     "Wright"
    ],
    [
     "E. N.",
     "Wrigley"
    ],
    [
     "M. J.",
     "Carey"
    ]
   ],
   "title": "Probabilistic multilevel language analysis for speech recognition",
   "original": "e89_1486",
   "page_count": 4,
   "order": 63,
   "p1": "1486",
   "pn": "1489",
   "abstract": [
    "Two adaptation methods for probabilistic context-free grammars are considered. In the first, an LR parser is enhanced with an error-recovery procedure which synthesises production rules in order to allow strings which are not in the language to be accepted. In the second, a hierarchy of probabilistic context-free grammars with progressively weaker structure has the same effect. Both methods are intended to promote flexibility and introduce a learning capability into linguistic modelling for speech recognition.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-56"
  },
  "lee89b_eurospeech": {
   "authors": [
    [
     "Kai-Fu",
     "Lee"
    ],
    [
     "Sanjoy",
     "Mahajan"
    ]
   ],
   "title": "Corrective and reinforcement learning for speaker-independent continuous speech recognition",
   "original": "e89_1490",
   "page_count": 4,
   "order": 64,
   "p1": "1490",
   "pn": "1493",
   "abstract": [
    "This paper addresses the issue of learning hidden Markov model (HMM) parameters for speaker-independent continuous speech recognition. Bahl et al. [1] introduced the corrective training algorithm for speaker-dependent isolated word recognition. Their algorithm attempted to improve the recognition accuracy on the training data. In this work, we extend this algorithm to speaker-independent continuous speech recognition. We use cross-validation to increase the effective training size. We also introduce a near-miss sentence hypothesization algorithm for continuous speech training. The combination of these two approaches resulted in over 20% error reductions both with and without grammar.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-57"
  },
  "rigoll89_eurospeech": {
   "authors": [
    [
     "Gerhard",
     "Rigoll"
    ]
   ],
   "title": "An information theory approach to speaker adaptation",
   "original": "e89_1494",
   "page_count": 4,
   "order": 65,
   "p1": "1494",
   "pn": "1497",
   "abstract": [
    "This paper describes a novel approach to speaker adaptation. The work was carried out by the author while he was a visiting scientist at the IBM Thomas Watson Research Center in Yorktown Heights/USA. The purpose of the research was to train the IBM speech recognition system with only five minutes of speech and to obtain at least a 95% recognition rate after adaptation for a 5000 word vocabulary recognition task. The adaptation algorithm is based on an Information Theory approach used for estimating the label stream of the new speaker by using a stochastic model describing the spectral differences between the new and a reference speaker. During an evaluation where twelve speakers were tested in ordinary 20 minutes speaker-dependent training mode the average recognition rate for a 5000 word vocabulary task was 96.4%. When the speakers were tested in 5 minutes adaptation mode the recognition rate dropped to 95.2%. A very important point is that the average decoding time increased by a factor of 1.35 while this factor is often 3-5 if other adaptation algorithms are used.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-58"
  },
  "darwin89_eurospeech": {
   "authors": [
    [
     "C. J.",
     "Darwin"
    ]
   ],
   "title": "Speech perception seen through the ear",
   "original": "e89_1230",
   "page_count": 5,
   "order": 66,
   "p1": "1230",
   "pn": "1234",
   "abstract": [
    "Speech is normally heard against a background of other sounds. This paper reviews recent work on listeners' ability to separate speech from other sounds. Evidence is presented that both low-level grouping mechanisms and knowledge specific to speech are deployed in solving this difficult problem.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-59"
  },
  "wu89_eurospeech": {
   "authors": [
    [
     "Zong Liang",
     "Wu"
    ],
    [
     "Jean Luc",
     "Schwartz"
    ],
    [
     "Pierre",
     "Escudier"
    ]
   ],
   "title": "A theoretical study of neural mechanisms specialized in the detection of articulatory-acoustic events",
   "original": "e89_1235",
   "page_count": 4,
   "order": 67,
   "p1": "1235",
   "pn": "1238",
   "abstract": [
    "A model for possible neural mechanisms specialized in the detection of articulatory- acoustic events is described. The model comprises two types of channels respectively specialized for onset and offset events. All channels are organized in parallel, and further process the spatio-temporal discharge pattern of the auditory nerve fibers which is simulated by a model incorporating the cochlear filtering in general and neural adaptation in particular. Channels for onset events are composed of \"ON\" mechanisms followed by large scale spatial integration, while channels for offset events are characterized by differentiation coding and temporal integration. These channels are very powerful in the detection of the concerned events. They are also used as a tool to explain some related psycho- acoustic phenomena.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-60"
  },
  "pont89_eurospeech": {
   "authors": [
    [
     "M. J.",
     "Pont"
    ],
    [
     "Robert I.",
     "Damper"
    ]
   ],
   "title": "A possible neural basis for the categorical perception of the English voiced/voiceless contrast",
   "original": "e89_1239",
   "page_count": 4,
   "order": 68,
   "p1": "1239",
   "pn": "1242",
   "abstract": [
    "We describe the representation of synthetic stop-consonants in a computational model of the mammalian dorsal cochlear nucleus. The speech stimuli have different values of voice-onset time (VOT) and are labelled by adult listeners as either /ga/ or /ka/, with a phonetic boundary at 44 ms VOT. The responses of the model's Type IV units to these stimuli also fall into two clear categories with a boundary at 45 ms VOT. These results provide evidence that the categorical perception of voicing in initial English stops - observed in behavioural experiments using human subjects (infant and adult) and chinchillas - may arise as a consequence of the representation of these sounds in the mammalian auditory nervous system at the level of the dorsal acoustic stria.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-61"
  },
  "hukin89_eurospeech": {
   "authors": [
    [
     "R. W.",
     "Hukin"
    ],
    [
     "Robert I.",
     "Damper"
    ]
   ],
   "title": "Testing an auditory model by resynthesis",
   "original": "e89_1243",
   "page_count": 4,
   "order": 69,
   "p1": "1243",
   "pn": "1246",
   "abstract": [
    "This paper describes the use of a resynthesis strategy in testing an auditory model, specifically a version of the DOMIN model. The steps by which the original speech is processed to produce a reduced, auditory (DOMIN) spectrum are described; subsequently, the reduced representation is used to produce resynthesised speech having the same auditory spectrum. In light of this equivalence, the extent to which the original and resynthesised speech are perceptually equivalent is argued to be a good test of the model. We show that a spectral representation in which approximately two-thirds of the FFT frequency components are discarded, but the DOMIN representation is unchanged, can produce resynthesised speech of high intelligibility. We conclude that the DOMIN model retains important information pertaining to the identity of both vowels and consonants. Further, we present evidence showing that testing by resynthesis is superior to the alternative techniques for assessing auditory models.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-62"
  },
  "berthommier89_eurospeech": {
   "authors": [
    [
     "F.",
     "Berthommier"
    ],
    [
     "Jean-Luc",
     "Schwartz"
    ],
    [
     "Pierre",
     "Escudier"
    ]
   ],
   "title": "Auditory processing in a post-cochlear neural network vowel spectrum processing based on spike synchrony",
   "original": "e89_1247",
   "page_count": 4,
   "order": 70,
   "p1": "1247",
   "pn": "1250",
   "abstract": [
    "The present work is concerned with the processing of spectral information by stochastic coding, at the output of the cochlea. We study the collective behaviour of a large number of cells based on a measure of the correlation of sequences of spikes between neighbour units. Our first results show that we can obtain significant discrimination of spectral components of vowels without any kind of cabled lateral inhibition or second filter device. This collective behaviour is made possible by the synchronization of inputs of adjacent units. This could provide an interesting and rather new way of making profit of both geographical coding (tonotopy) and temporal coding (synchronization) particularly useful for formant detection.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-63"
  },
  "keurs89_eurospeech": {
   "authors": [
    [
     "Mariken ter",
     "Keurs"
    ],
    [
     "Reinier",
     "Plomp"
    ],
    [
     "Joost M.",
     "Festen"
    ]
   ],
   "title": "Effects of spectral smearing on speech reception",
   "original": "e89_1251",
   "page_count": 3,
   "order": 71,
   "p1": "1251",
   "pn": "1253",
   "abstract": [
    "The effect of reduced frequency resolution on the Speech-Reception Threshold (SRT) for sentences in noise was investigated for eight, normal-hearing subjects. Signal processing was performed by short-time Fast Fourier Transforms (FFT) and overlapping additions to reconstruct a continuous signal. Spectral energy in the frequency region from 100 to 8000 Hz was smeared over fixed relative bandwidths of 1/8, 1/4, 1/2, 1, 2, 4, and 8 octaves. The order of conditions was varied according to a Latin-square design. Results show that the SRT increases progressively as the spectral energy is smeared over bandwidths exceeding the ear's critical bandwidth.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-64"
  },
  "fujihashi89_eurospeech": {
   "authors": [
    [
     "Y.",
     "Fujihashi"
    ],
    [
     "A.",
     "Fukui"
    ]
   ],
   "title": "Telephone speech recognition system with high noise immunity",
   "original": "e89_1254",
   "page_count": 4,
   "order": 72,
   "p1": "1254",
   "pn": "1257",
   "abstract": [
    "The purpose of this paper is to suggest the use of the word spotting method for isolated word recognition in order to realize a telephone speech recognition system with high noise immunity. Our recent simulation test reveals that the word spotting method is highly immune to burst noise which causes the decrease of the recognition rate of the conventional system that needed to detect voiced periods by voice power before DP matching. Based on the algorithm using the word spotting method ,we have developed a new telephone speech recognition system. Mounted on one card, it consists of a 16-bit CPU (uPD70216), a DP matching processor (uPD7764) and a speech analysis processor (uPD77C25). In its maximum configuration, this system is capable of recognizing 400 words. In our evaluation test, this system showed a recognition rate of over 98% under noiseless conditions, and over 95% under noisy conditions where human voices and telephone bell sounds were present. The results of this test indicate that this telephone speech recognition system has a very high immunity to noise.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-65"
  },
  "yong89_eurospeech": {
   "authors": [
    [
     "Gu",
     "Yong"
    ],
    [
     "John S.",
     "Mason"
    ]
   ],
   "title": "Speaker normalization via a linear transformation on a perceptual feature space and its benefits in ASR adaptation",
   "original": "e89_1258",
   "page_count": 4,
   "order": 73,
   "p1": "1258",
   "pn": "1261",
   "abstract": [
    "This paper examines inter-speaker variability of perceptually weighted features known as PLP. The motivation is to find a successful transformation between speakers for use in adaptation in speech recognition. Weighted cepstral distance measures are examined, including a combination of the unweighted d-CEP and the root-power-sum slope distortion measure d-RPS This is shown to be most effective in speaker-independent ASR. It is found that differences between two speakers are exhibited relatively clearly and consistently on the PLP/RPS domain. The attenuation of these differences by a linear transformation forms the basis of the proposed adaptation method for speech recognition. Recognition experiments indicate clearly the effectiveness of the method.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-66"
  },
  "ruehl89_eurospeech": {
   "authors": [
    [
     "Hans-Wilhelm",
     "Ruehl"
    ],
    [
     "S.",
     "Dobler"
    ],
    [
     "J.",
     "Weith"
    ],
    [
     "Peter",
     "Meyer"
    ],
    [
     "A.",
     "Noll"
    ],
    [
     "H. H.",
     "Hamer"
    ],
    [
     "H.",
     "Piotrowski"
    ]
   ],
   "title": "Speech recognition in the noisy car environment",
   "original": "e89_1262",
   "page_count": 4,
   "order": 74,
   "p1": "1262",
   "pn": "1265",
   "abstract": [
    "Adaptation and use of an algorithm for recognition of connected words in an application for mobile radio telephony is described. Two data bases were collected in a compact car running about 120 km/h containing speech uttered via handset and in hands-free mode for each 10 speakers. In the first phase, a connected-words recognition algorithm was improved using the handset data base. Starting with a recognition mismatch rate of less than i% for undisturbed speech, 14.5% errors were measured for noisy speech and an unmodified algorithm. After several modifications and introduction of new modules for speech signal preprocessing, the error rate decreased to 3% for handset data and 13.2 % for hands-free data. Work on recognition in hands-free mode is still in progress.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-67"
  },
  "carey89_eurospeech": {
   "authors": [
    [
     "Michael",
     "Carey"
    ],
    [
     "Amanda",
     "Howe"
    ],
    [
     "Roger",
     "Tucker"
    ]
   ],
   "title": "On the recognition of key words in unconstrained conversation",
   "original": "e89_1266",
   "page_count": 1,
   "order": 75,
   "p1": "1266",
   "pn": "",
   "abstract": [
    "For many applications it is not necessary or even desirable to recognise every word in an utterance. A more satisfactory approach may be to try to identify the occurrence of a number of key words within the utterance string. In this paper we describe a system which identifies key words occurring in unconstrained conversation.\n",
    ""
   ]
  },
  "mason89_eurospeech": {
   "authors": [
    [
     "John S.",
     "Mason"
    ],
    [
     "J.",
     "Oglesby"
    ],
    [
     "L.",
     "Xu"
    ]
   ],
   "title": "Codebooks to optimise speaker recognition",
   "original": "e89_1267",
   "page_count": 4,
   "order": 76,
   "p1": "1267",
   "pn": "1270",
   "abstract": [
    "A recent approach to speaker identification is based on personalised codebooks. The algorithm compares incoming test features with a set of N codebooks, one for each valid member of the user population, and the codebook which gives rise to the smallest accumulated distance for the full test feature sequence is assumed to identify the speaker. Results from this inherently text-independent approach have highlighted the performance variations for different test utterances: the spoken digit 'nine' is good, while 'six' is bad. This observation has lead to the idea of classifying speech, via a text and speaker-independent codebook, according to empirical discriminating properties in the recognition task. Such a classifier is developed here, and experimental results show that 10% or more of speech acts as little more than noise, interfering in the task of speaker recognition.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-68"
  },
  "xu89_eurospeech": {
   "authors": [
    [
     "L.",
     "Xu"
    ],
    [
     "John S.",
     "Mason"
    ]
   ],
   "title": "Instantaneous and transitional perceptually-based features in speaker identification",
   "original": "e89_1271",
   "page_count": 4,
   "order": 77,
   "p1": "1271",
   "pn": "1274",
   "abstract": [
    "A recent comparison of features and distance measures [3] shows the perceptually based linear prediction, PLP, together with the appropriate distance measure to be consistently better than other widely used standard combinations. This paper investigates the PLP-derived cepstra representing the instantaneous spectral information and the time slope of the cepstra representing the transitional spectral information in automatic speaker identification (ASI). The root-power-sum (RPS) distance and the inverse variance (INV) weighted distance are discussed. The experiments relate to a vector quantization (VQ) based digit-independent ASI. The study shows the first 8 coefficients of the PLP features are the most important in distinguishing inter-speaker differences. The RPS distance and the INV weighted distance perform similarly well, and significantly better than the unweighted cepstral distance. Also, The overall advantage of PLP features over the LPC features are demonstrated.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-69"
  },
  "rocchi89_eurospeech": {
   "authors": [
    [
     "Claudio",
     "Rocchi"
    ],
    [
     "Enzo",
     "Mumolo"
    ]
   ],
   "title": "A new method for performing weighted distances for speaker authentication",
   "original": "e89_1275",
   "page_count": 4,
   "order": 78,
   "p1": "1275",
   "pn": "1278",
   "abstract": [
    "In the paper, a new method for computing weighted distances for Dynamic Time Warping based speaker verification systems is described. Weighted distances use coefficients determined usually globally and this, of course, does not consider the phonetic content of the vocal pattern. The goal of local weighting is to connect the computation of the weights to the phonetic events occurring in the patterns in order to compute a weighting matrix for each phonetic event. This is approximately achieved, in the work described in this paper, by using the DTW optimum path obtained during the comparison of two reference patterns. The method has been simulated on a VAX computer, and an accuracy improvement with respect to the global case has been observed.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-70"
  },
  "federico89_eurospeech": {
   "authors": [
    [
     "A.",
     "Federico"
    ],
    [
     "G.",
     "Ibba"
    ],
    [
     "A.",
     "Paoloni"
    ],
    [
     "N. De",
     "Sario"
    ],
    [
     "B.",
     "Saverione"
    ]
   ],
   "title": "Comparison between automatic methods and human listeners in speaker recognition tasks",
   "original": "e89_1279",
   "page_count": 4,
   "order": 79,
   "p1": "1279",
   "pn": "1282",
   "abstract": [
    "In this paper we describe an experimental comparison between the performance of listeners and the performance of an automatic method in an experiment of speaker identification. The database used in both, automatic and subjective experiments, was made up of 4 different speakers. Their voices were selected as belonging to a homogeneous group of subjects whose dialects and social habits were very similar. The speech material consisted in five phrases that have been recorded in three different sessions: the second session follows the first after about three months and the third session follows the first after about ten years. Each recording session uses the same telephone channel and the same recording apparatus. The subjective experiments consist in a discrimination test, each test item consisting in two speech samples. The listener decides whether the two samples were produced by the same or different speakers. The parametric method is a traditional automatic approach based on the evaluation of suitable acoustic parameters. The recognition test is performed with that parameter utilized in usual hypothesis test. The comparison of the test results obtained by automatic and subjective methods are discussed.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-71"
  },
  "giannini89_eurospeech": {
   "authors": [
    [
     "Antonella",
     "Giannini"
    ],
    [
     "Massimo",
     "Pettorino"
    ],
    [
     "Umberto",
     "Cinque"
    ]
   ],
   "title": "Speaker's identification by voice",
   "original": "e89_1283",
   "page_count": 4,
   "order": 80,
   "p1": "1283",
   "pn": "1286",
   "abstract": [
    "Since the early 70's many experimental studies have pointed out the existence of a close relationship between the acoustic characteristics of voice and the physical features of the speaker. The purpose of this study is to determine, on one hand, whether listeners are capable of identifying age, weight and height of speakers; on the other hand, to verify on the basis of perceptive tests, whether it is possible to distinguish twin's voices with better than chance-guessing accuracy. The results give rise to various considerations. Suggestions for future research are discussed.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-72"
  },
  "zalewski89_eurospeech": {
   "authors": [
    [
     "Janusz",
     "Zalewski"
    ]
   ],
   "title": "Text dependent speaker recognition in noise",
   "original": "e89_1287",
   "page_count": 3,
   "order": 81,
   "p1": "1287",
   "pn": "1289",
   "abstract": [
    "The author has developed a method of speaker identification,based on representation of speakers by some LPC-coded vowels.The minimum cumulated spectral difference between corresponding test and reference samples was the decision criterion in the recognition task.The experiments reported here investigated the ability of the modified method to identify speakers under noise conditions.The modification of the method consisted in use a noise canceller.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-73"
  },
  "hirata89_eurospeech": {
   "authors": [
    [
     "Yoshimitsu",
     "Hirata"
    ],
    [
     "Seiich",
     "Nakagawa"
    ]
   ],
   "title": "A lOObit/s speech coding using a speech recognition technique",
   "original": "e89_1290",
   "page_count": 4,
   "order": 82,
   "p1": "1290",
   "pn": "1293",
   "abstract": [
    "In this paper, we describe a phonetic vocoder based on syllable-units which represents speech waves by extremely low rate (100 bits/s) using a speech recognition tequnique. We take syllables into consideration as the unit of recognition / synthesis. Speech waves are transformed into a sequence of frames, each of which consists of LPC cepstrum, PARCOR coefficients, pitch and power. After the 0(n)DP matching with reference patterns, the input speech is transformed into a sequence of Japanese syllables. The information of recognized syllable contains the category of syllables, duration, power and pitch, and is represented by 16 bits. Using this vocoder, speech can be represented by only 100 bits/sec.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-74"
  },
  "lefevre89_eurospeech": {
   "authors": [
    [
     "Jean-Paul",
     "Lefevre"
    ],
    [
     "Roberto",
     "Viola"
    ]
   ],
   "title": "Real-time multirate speech codec for manned spacecraft communications",
   "original": "e89_1294",
   "page_count": 4,
   "order": 83,
   "p1": "1294",
   "pn": "1297",
   "abstract": [
    "This paper describes a real-time implementation of a multirate (2.4, 4.8, 8 and 16 Kbit/sec) speech codec designed to fulfil the requirements of the telecommunication sub-system for HERMES spacecraft. Considering these requirements, our choice turned to the Multi-Pulse Excited Linear Predictive Coding (MPLPC) approach (except for the 2.4 Kbit/sec emergency rate which is based on the standard LPC-10 algorithm). In the first part of the paper, emphasis is put on coding aspects, in order to achieve effectively the expected bit rates. In particular, the encoding of the pulses is carefully studied. Then, details are given about hardware and software developments. Lastly, system complexity is discussed.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-75"
  },
  "soheili89_eurospeech": {
   "authors": [
    [
     "R.",
     "Soheili"
    ],
    [
     "A. M.",
     "Kondoz"
    ],
    [
     "B. G.",
     "Evans"
    ]
   ],
   "title": "New innovations in multi-pulse speech coding for bit rates below 8 kb/s",
   "original": "e89_1298",
   "page_count": 4,
   "order": 84,
   "p1": "1298",
   "pn": "1301",
   "abstract": [
    "Recently, MP-LPC [1] and other similar analysis-by-synthesis (A-by-S) coding schemes have proved that it is possible to achieve very high quality coded speech down to around 8 kb/s. In this paper we report on simulation results of a multi-pulse coder with pitch prediction at bit rates of 8 kb/s and below. The effects of various long term prediction configurations have been evaluated both subjectively and objectively. In order to reduce the bit rate and maintain the output quality, efficient quantisation of the MP-LPC parameters are required. Since in a multi-pulse coding scheme most of the bits are used to code the pulses, we have investigated various methods of adaptive pulse amplitude quantisation. Finally we report on a new joint quantisation scheme of the pulse amplitudes, which resulted in reductions of up to 1000 bits/sec with no impairment in the speech quality.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-76"
  },
  "boyd89_eurospeech": {
   "authors": [
    [
     "I.",
     "Boyd"
    ],
    [
     "C. B.",
     "Southcott"
    ],
    [
     "P. J.",
     "Bolingbroke"
    ]
   ],
   "title": "A speech coder for aeronautical telecommunications",
   "original": "e89_1302",
   "page_count": 4,
   "order": 85,
   "p1": "1302",
   "pn": "1305",
   "abstract": [
    "British Telecom International (BTI) has introduced an aeronautical telephone service known as the Skyphone. This service uses 9.6 kbit/s speech codecs because of the limited satellite power available. Because of the need to achieve operation of aeronautical telephones between any aircraft and any ground station in any country, the technical specifications have been agreed internationally. After extensive testing of the various speech codecs proposed for the service by companies from several different countries, the codec designed at British Telecom Research Laboratories (BTRL) was found to be the best and is to be used both for international telephony and for air traffic control. This paper gives a brief description of the Skyphone and the factors which influence the speech codec design and then describes the codec designed by BTRL for the service.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-77"
  },
  "ozawa89_eurospeech": {
   "authors": [
    [
     "Kazunori",
     "Ozawa"
    ]
   ],
   "title": "A 4.8 kb/s high-quality speech coding using various types of excitation signals",
   "original": "e89_1306",
   "page_count": 4,
   "order": 86,
   "p1": "1306",
   "pn": "1309",
   "abstract": [
    "A high-qulaity speech coding method (SPMEX) at 4.8 kb/s is proposed. The SPMEX selects a suitable excitation signal, based on the decision from acoustic features of speech signal in a frame. Improved pitch interpolation multi-pulse (PMPC) excitation is selected for vowel-like speech. In PMPC, multi-pulse during only one pitch period is calculated in the frame. Further, gain and phase adjusting coefficients are calculated for each of the other pitch periods in the same frame, in order to produce high-quality synthetic speech. Multi-pulse excitation (MPC) is selected for stop and transition-like speech. Stochastic codebook (SC) excitation is selected for Meat ion-like speech. Subjective evaluation results show that 4.8 kb/s SPMEX reconstructs high-quality synthetic speech, which is equivalent to 48 kb/s /n-law PCM.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-78"
  },
  "delprat89_eurospeech": {
   "authors": [
    [
     "M.",
     "Delprat"
    ],
    [
     "M.",
     "Lever"
    ],
    [
     "C.",
     "Gruet"
    ]
   ],
   "title": "Efficient excitation model and fast selection in CELP coding of speech",
   "original": "e89_1310",
   "page_count": 4,
   "order": 87,
   "p1": "1310",
   "pn": "1313",
   "abstract": [
    "The paper discusses several new approaches for efficiently modeling and selecting the excitation in CELP coding of speech. Modified error criteria and structured codebooks lead to a wide range of complexity reduction methods, that are evaluated in terms of quality and computational requirements. A very low complexity, though high quality, Regular Pulse (RP) CELP technique is then derived. Finally we address the design of a robust 6 kbps RPCELP coder for mobile radio communications.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-79"
  },
  "atungsiri89_eurospeech": {
   "authors": [
    [
     "S. A.",
     "Atungsiri"
    ],
    [
     "A. M.",
     "Kondoz"
    ],
    [
     "B. G.",
     "Evans"
    ]
   ],
   "title": "A low bit rate speech coder optimized for forward error control",
   "original": "e89_1314",
   "page_count": 4,
   "order": 88,
   "p1": "1314",
   "pn": "1317",
   "abstract": [
    "As digital speech transmission technology becomes more widespread, the trend is to find robust and low complexity bandwidth-efficient algorithms for speech coding. As forerunners in this area, the LPC based schemes like MPLPC [11 RPE [2], CELP [3], etc and recently, CELP-BB [4], have been extensively reported. With its reasonable complexity and high quality speech at low bit rates (2.4 to 4.8Kb/s)» CELP-BB is a good contender for 4.8Kb/s and below. Although all these schemes have good operation in ideal conditions, they cannot function properly under real channel conditions because of the less than ideal nature of most communication channels. In the past, the trend has been to find appropriate FEC schemes to protect the bits transmitted over given channels. However, for adequate protection large redundancy has to be used and so the original aim, speech transmission at low bit rates, is compromised. In an ideal system, the speech coder should have some error control capability while minimising delay, complexity and transmitted bit rate. In this paper we report on adaptation work done to give the CELP-BB coder just such a capability.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-80"
  },
  "satoh89_eurospeech": {
   "authors": [
    [
     "Kazumi",
     "Satoh"
    ],
    [
     "Hideaki",
     "Kurihara"
    ],
    [
     "Shigeyuki",
     "Unagami"
    ],
    [
     "Masanori",
     "Kajihara"
    ],
    [
     "Yoshihiro",
     "Tomita"
    ]
   ],
   "title": "8- and 16-kb/s APC-AB voice codec using a single chip DSP",
   "original": "e89_1318",
   "page_count": 4,
   "order": 89,
   "p1": "1318",
   "pn": "1321",
   "abstract": [
    "This paper describes the implementation of an APC-AB codec using a single-chip DSP. To handle the simultaneous 8- or 16-kb/s encoding and decoding with a single-chip DSP, we precisely evaluated arithmetic bit accuracy, simplifying the DSP algorithm and sharing subroutines. We designed a dedicated DSP with a 16E8 floating-point data format and a novel DMA control circuit. The 100 X 68 mm codec module contains 2 K words of RAM, a 12-ms echo canceler, and two interface LSI chips. An SNR of 39 dB for 16-kb/s coding and 36 dB for 8-kb/s coding were obtained using method 2 of CCITT recommendation G.712. The module consumes less than 1.0 watt.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-81"
  },
  "moreau89_eurospeech": {
   "authors": [
    [
     "N.",
     "Moreau"
    ],
    [
     "P.",
     "Dymarski"
    ]
   ],
   "title": "Mixed excitation CELP coder",
   "original": "e89_1322",
   "page_count": 4,
   "order": 90,
   "p1": "1322",
   "pn": "1325",
   "abstract": [
    "A new approach to the excitation problem for CELP coder is presented. A generalized codebook of excitation vectors is proposed, consisting of pulses (as in MP coders), stochastic sequences (as in classical CELP coders) and past excitation sequences (as in SEV). The resulting excitation is a linear combination of a small number of these vectors. No constraints are imposed on the type of selected vectors, the only criterion is a distance between the original and synthetic speech signals (both passed through the perceptual filter). Two algorithms for selection of codebook vectors and computation of gains are described. Some results of simulations are reported and problems of complexity and implementation are discussed.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-82"
  },
  "wery89_eurospeech": {
   "authors": [
    [
     "B. R.",
     "Wery"
    ],
    [
     "A.",
     "Leroux"
    ],
    [
     "H. Ph.",
     "Delbrouck"
    ],
    [
     "J.",
     "Leclerc"
    ]
   ],
   "title": "A new parametric speech analysis and synthesis technique in the frequency domain",
   "original": "e89_1326",
   "page_count": 3,
   "order": 91,
   "p1": "1326",
   "pn": "1328",
   "abstract": [
    "A new method for speech parametric coding at medium output rate is presented. The availability of new electronic components changes the hypotheses assumed in previous studies. A parametric coding scheme with medium output rate now seems appropriate for rule-based synthesis. The weak spot in most parametric synthesizers lies in the poor model used to represent the excitation signal. Our approach consists in coding, in the frequency domain, the residual signal of a linear prediction analysis. The residual signal is analysed as a function of frequency through a sub-band splitting. Particularly, voicing is analysed in each band and the decision is progressive from voiced to unvoiced. This parametric, frame based, representation of the residual signal, associated with the linear prediction coefficients, is used for transmission or storage purposes. The synthesizer builds the synthetic residue by generating spectra from these parameters. These spectra are combined through the overlapp-add algorithm to obtain a time domain signal. Our system thus allows complex filtering and phase control and provides a robust representation and a good quality while preserving the flexibility inherent to parametric coding schemes.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-83"
  },
  "blanchet89_eurospeech": {
   "authors": [
    [
     "Pascal",
     "Blanchet"
    ]
   ],
   "title": "Multilayer perceptron architectures for data compression tasks",
   "original": "e89_1329",
   "page_count": 4,
   "order": 92,
   "p1": "1329",
   "pn": "1332",
   "abstract": [
    "Different kinds of Multilayer Perceptrons, using a back-propagation learning algorithm, have been used to perform data compression tasks. Depending upon the architecture and the type of problem learned to solve (classification or auto-association), the networks provide different kinds of dimensionality reduction preserving different properties of the data space. Some experiments show that using the non-linearities of the MLP units may improve performances of classical linear dimensionality reduction. All the experiments reported here have been carried out on speech data.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-84"
  },
  "renals89_eurospeech": {
   "authors": [
    [
     "Steve",
     "Renals"
    ],
    [
     "Jonathan",
     "Dalby"
    ]
   ],
   "title": "Analysis of a neural network model for speech recognition",
   "original": "e89_1333",
   "page_count": 4,
   "order": 93,
   "p1": "1333",
   "pn": "1336",
   "abstract": [
    "An analysis of a feed-forward neural network model performing a vowel recognition task is reported. Two experiments analysed in detail involved training on speech processed using linear prediction and the discrete FFT. A phonetic analysis of the resultant weight matrices is offered, additionally we analyse the network structure using pairwise correlations and principal components analysis. We suggest methods of network initialisation obtained by inverting these analyses.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-85"
  },
  "patarnello89_eurospeech": {
   "authors": [
    [
     "Stefano",
     "Patarnello"
    ],
    [
     "Stefano",
     "Scarei"
    ]
   ],
   "title": "Self-organizing boolean networks for speech recognition",
   "original": "e89_1337",
   "page_count": 4,
   "order": 94,
   "p1": "1337",
   "pn": "1340",
   "abstract": [
    "We show the application of a self-organizing Boolean network to speech recognition. The model consists of a set of two-input Boolean gates which has to implement a n-to-1 Boolean mapping through a leaming-by-example procedure. The training scheme is based on an optimization process (Simulated Annealing). This approach is applied to a simple phoneme recognition task, achieving high accuracy.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-86"
  },
  "monte89_eurospeech": {
   "authors": [
    [
     "Enric",
     "Monte"
    ],
    [
     "Eduardo",
     "Lleida"
    ],
    [
     "Jose B.",
     "Marino"
    ]
   ],
   "title": "New backpropagation algorithm using quadratic potential functions, and an experiment on isolated word recognition",
   "original": "e89_1341",
   "page_count": 4,
   "order": 95,
   "p1": "1341",
   "pn": "1344",
   "abstract": [
    "This paper presents a new algorithm to train multilayered perceptrons, using quadratical potential functions. This new algorithm is compared in an isolated word recognition task, with the back propagation algorithm that uses linear combinations of the inputs. Some pattern recognition techniques are also used to reduce the dimensinality of the input pattern in order to reduce the computational burden of the training and the recognition. The algorithm that uses quadratic potential functions yields better results in the recognition task.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-87"
  },
  "kangas89_eurospeech": {
   "authors": [
    [
     "Jari",
     "Kangas"
    ],
    [
     "Teuvo",
     "Kohonen"
    ]
   ],
   "title": "Transient map method in stop consonant discrimination",
   "original": "e89_1345",
   "page_count": 4,
   "order": 96,
   "p1": "1345",
   "pn": "1348",
   "abstract": [
    "Discrimination between the voiceless stop consonants /k,p,t/ is a subproblem in phoneme-based speech recognition systems. Lack of energy during the pronunciation and the fast transient effects at the end of the phoneme make the recognition difficult. A method of so called Phonotopic Maps [2] was studied in order to develop simple and effective solutions for discrimination. In the following studies the method of Transient Maps, a derivative of Phonotopic Maps, was found to be an easy-to-implement and powerful algorithm for real-time speech recognition systems. It contains an automatic learning algorithm that tunes the discrimination elements to detect the differences between the spectra at the end of the stop consonant. Using Transient Maps it is possible to classify correctly 80 to 90 percent of all voiceless stop consonants in our speech recognition system. Thus the recognition accuracy of voiceless stop consonants is comparable to that of the other phonemes.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-88"
  },
  "caharel89_eurospeech": {
   "authors": [
    [
     "M.-H.",
     "Caharel"
    ],
    [
     "Laurent",
     "Miclet"
    ]
   ],
   "title": "Filtering a phonetic lattice with a connexionnist network",
   "original": "e89_2529",
   "page_count": 4,
   "order": 97,
   "p1": "2529",
   "pn": "2532",
   "abstract": [
    "In the Keal System [MER 89], as in most analytical speech recognition systems, the speech signal is processed in a bottom-up sequence of processing units. In Keal, these units are designed as follows: acoustic-phonetic decoding, lexical analysis, syntactic and semantic analysis. The purpose of this paper is to simplify the result of the first unit, namely the phonetic lattice, by removing its internal redundancy. This is achieved by using a connexionist network as a \"filter\" in order to tranform the lattice into a string. The task of the lexical decoder will henceforth be easier.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-89"
  },
  "bulot89_eurospeech": {
   "authors": [
    [
     "R.",
     "Bulot"
    ],
    [
     "P.",
     "Nocera"
    ]
   ],
   "title": "Explicit knowledge and neural networks for speech recognition",
   "original": "e89_2533",
   "page_count": 4,
   "order": 98,
   "p1": "2533",
   "pn": "2536",
   "abstract": [
    "In this paper we present a simple speech recognition application in which an explicit knowledge base system is coupled to a connectionist machine. A set of Prolog rules describes the structural aspects of the words in the vocabulary, in terms of acoustico-phonetic events. These events are jointly evaluated by a multi-layer network in order to identify the corresponding word. The system provides good results, notably with unknown speakers.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-90"
  },
  "bottou89_eurospeech": {
   "authors": [
    [
     "L.",
     "Bottou"
    ],
    [
     "F.",
     "Fogelman Soulie"
    ],
    [
     "Pascal",
     "Blanchet"
    ],
    [
     "Jean-Sylvain",
     "Lienard"
    ]
   ],
   "title": "Experiments with time delay networks and dynamic time warping for speaker independent isolated digits recognition",
   "original": "e89_2537",
   "page_count": 4,
   "order": 99,
   "p1": "2537",
   "pn": "2540",
   "abstract": [
    "We describe in this paper a speaker independent, global word recognition task using time delay networks. We first describe these networks as a way for learning feature extractors by constrained back-propagation. Such a time-delay network is shown to be capable of dealing with a test task: French digit recognition. The results are discussed and compared, on the same data sets, with those obtained with a classical time warping system. Both connectionist and classical systems achieved no more than 1% errors on the test set.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-91"
  },
  "dalsgaard89_eurospeech": {
   "authors": [
    [
     "Paul",
     "Dalsgaard"
    ]
   ],
   "title": "Semi-automatic phonemic labelling of speech data using a self-organising neural network",
   "original": "e89_2541",
   "page_count": 4,
   "order": 100,
   "p1": "2541",
   "pn": "2544",
   "abstract": [
    "In the perspective of assessing existing and new speech input and output devices, the development of methods for computerised semi-automatic labelling of speech data is becoming increasingly important. This paper describes preliminary work to achieving this goal by utilising Neural Network technique to perform phonemic classification. The speech data used for Neural Network learning are taken from the SAM-EUROM.O database, which has been manually labelled by expert phoneticians as a reference. Separate data from 1 male speaker are used to test the performance of the semi-automatic phonemic labelling system. The preliminary results show an average classification rate of 65 % on the vocalic and 72% on the consonantal Danish archiphonemes.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-92"
  },
  "baekgaard89_eurospeech": {
   "authors": [
    [
     "Anders",
     "Baekgaard"
    ],
    [
     "Paul",
     "Dalsgaard"
    ]
   ],
   "title": "Recognition of continuous speech using neural nets and expert system",
   "original": "e89_2545",
   "page_count": 4,
   "order": 101,
   "p1": "2545",
   "pn": "2548",
   "abstract": [
    "A system for recognising continuously spoken sentences is presented. The system has a vocabulary of approx. 35 words and a grammar specifying a few thousand sentences. The system operates in three stages. In the first stage, cepstrum vectors are computed in real time and used as input to a self organised neural network. The output of the network is mapped to a continuous valued acoustic phonetic distinctive feature vector for each frame of the speech signal. These vectors are in the second stage processed by a multi layer perceptron which is trained to estimate segment boundaries. The output from this stage is a discrete valued acoustic phonetic distinctive feature for each segment of the speech signal (allophones). The third stage contains an expert system, which processes the allophones using a lexicon and a parsing system.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-93"
  },
  "komori89_eurospeech": {
   "authors": [
    [
     "Yasuhiro",
     "Komori"
    ],
    [
     "Kaichiro",
     "Hatazaki"
    ],
    [
     "Takaharu",
     "Tanaka"
    ],
    [
     "Takeshi",
     "Kawabata"
    ],
    [
     "Kiyohiro",
     "Shikano"
    ]
   ],
   "title": "Phoneme recognition expert system using spectrogram reading knowledge and neural networks",
   "original": "e89_2549",
   "page_count": 4,
   "order": 102,
   "p1": "2549",
   "pn": "2552",
   "abstract": [
    "We present a method for phoneme recognition using an expert system combining spectrogram reading knowledge and neural networks, and we report its performance. The proposed expert system consists of two parts: (1) phoneme segmentation based on spectrogram reading knowledge used by human experts, and (2) phoneme identification using neural networks applied to the phoneme boundaries determined in phoneme segmentation. Highly accurate phoneme segmentation can be achieved by using human-like contextual spectrogram reading knowledge. Moreover, high performance phoneme identification can be achieved by applying neural networks to the accurate phoneme segmentation result. The system was tested on Japanese consonants, with 90.8% of the phonemes correctly segmented and 92.4% of the phonemes correctly identified within the correct segment. 83.9% of the phonemes were correctly recognized both in segmentation and identification.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-94"
  },
  "haffner89_eurospeech": {
   "authors": [
    [
     "P.",
     "Haffner"
    ],
    [
     "Alex",
     "Waibel"
    ],
    [
     "H.",
     "Sawai"
    ],
    [
     "Kiyohiro",
     "Shikano"
    ]
   ],
   "title": "Fast back-propagation learning methods for large phonemic neural networks",
   "original": "e89_2553",
   "page_count": 4,
   "order": 103,
   "p1": "2553",
   "pn": "2556",
   "abstract": [
    "Several improvements in the Back-Propagation procedure are proposed to increase training speed, and we discuss their limitations with respect to generalization performance. The error surface is modeled to avoid local minima and flat areas. The synaptic weights are updated as often as possible. Both the step size and the momentum are dynamically scaled to the largest possible values that do not result in overshooting. Training for the speaker-dependent recognition of the phonemes /b/, /d/ and /g/ has been reduced from 2 days to 1 minute on an Alliant parallel computer, delivering the same 98.6% recognition performance. With a 55000-connection TDNN, the same algorithm needs 1 hour and 5000 training tokens to recognize the 18 Japanese consonants with 96.7% correct.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-95"
  },
  "rohwer89_eurospeech": {
   "authors": [
    [
     "Richard",
     "Rohwer"
    ],
    [
     "David",
     "Cressy"
    ]
   ],
   "title": "Phoneme classification by boolean networks",
   "original": "e89_2557",
   "page_count": 4,
   "order": 104,
   "p1": "2557",
   "pn": "2560",
   "abstract": [
    "The most popular neural network models for use in speech recognition experiments are employ model neurons which apply a nonlinear function to a weighted sum of their inputs. These networks are trained by adjusting the weights in the weighted sums. There is another class of models called Boolean networks, in which the model neurons output logical functions of their inputs. The training process adjusts the truth-tables which specify the logical functions. Although less well-known than conventional models, Boolean networks have been studied since 1960's. They have been sufficiently successful to form the basis of a commercial product for classification of images. This is a report on the application of a Boolean network to phoneme classification.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-96"
  },
  "kokkonen89_eurospeech": {
   "authors": [
    [
     "Mikko",
     "Kokkonen"
    ],
    [
     "Kari",
     "Torkkola"
    ]
   ],
   "title": "Using self-organizing maps and multi-layered feed-forward nets to obtain phonemic transcriptions of spoken utterances",
   "original": "e89_2561",
   "page_count": 4,
   "order": 105,
   "p1": "2561",
   "pn": "2564",
   "abstract": [
    "Two schemes to obtain phonemic transcriptions of spoken utterances are described and compared. Both schemes utilize the so called Self-Organizing Kohonen Maps first to vector quantize speech into a sequence of phoneme labels centisecond apart. In the original scheme, this quasiphoneme sequence is converted into a phoneme string with simple durational transformation rules. In the scheme introduced in this paper, the conversion is carried out by using a multi-layered feed-forward network trained with error back propagation. The achieved phonemic recognition error rate is about 2.5 per cent units better with the multi-layered network approach (19.2% opposed to 21.7%). However, the back propagation algorithm requires a vast amount of training compared to the rule-based method.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-97"
  },
  "huckvale89_eurospeech": {
   "authors": [
    [
     "Mark A.",
     "Huckvale"
    ],
    [
     "I. S.",
     "Howard"
    ],
    [
     "William J.",
     "Barry"
    ]
   ],
   "title": "Automatic phonetic feature labelling of continuous speech",
   "original": "e89_2565",
   "page_count": 4,
   "order": 106,
   "p1": "2565",
   "pn": "2568",
   "abstract": [
    "This paper updates our previous work on the automatic phonetic feature analysis of speech. Previously we have described how a bank of feature-detectors can be used as a front-end to traditional speech recognition pattern matching algorithms, with increased performance in speaker-in-dependent isolated word recognition over purely acoustic front-ends. In this paper we extend the feature analysis to continuous speech: describing the labelling methodology and the additional classification performance of neural network classifiers over the Bayes normal classifier.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-98"
  },
  "karlsson89_eurospeech": {
   "authors": [
    [
     "Inger",
     "Karlsson"
    ]
   ],
   "title": "A female voice for a text-to-speech system",
   "original": "e89_1349",
   "page_count": 4,
   "order": 107,
   "p1": "1349",
   "pn": "1352",
   "abstract": [
    "There is a great need for better voice quality in text-to-speech systems. Today, only mechanically sounding male voices can be produced. The lack of success in producing a better sounding voice quality has been due mainly to a lack of knowledge of the voice source. We have also needed a good voice source model and an implementation of such a source in a text-to-speech system. The LF-model for the voice source has given us a tool for a description of the voice source dynamics in speech. The implementation of this source model in our text-to-speech system raises opportunities for synthesis with better voice quality and with different voices. In this paper the work on a female voice is described. The voice source dynamics in sentences and in different stress environments are studied. Acoustic parameters for a female reference speaker are compared to the male synthetic voice. These data are compiled into rules for synthesis and the results of these rules will be played at the conference.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-99"
  },
  "barry89_eurospeech": {
   "authors": [
    [
     "William J.",
     "Barry"
    ],
    [
     "Martine",
     "Grice"
    ],
    [
     "Valerie",
     "Hazan"
    ],
    [
     "Adrian J.",
     "Fourcin"
    ]
   ],
   "title": "Excitation distributions for synthesised speech",
   "original": "e89_1353",
   "page_count": 4,
   "order": 108,
   "p1": "1353",
   "pn": "1356",
   "abstract": [
    "A comparison is made between the excitation frequency distribution (Dx) plots and other excitation characteristics obtained from readings of short passages of text in six different languages and the excitation properties for the same passages produced on a number of synthesisers. Though frequency range is often a control variable in a synthesiser, some were found to have default settings inappropriate to the language. A number of other distribution properties are discussed including inter-period temporal scatter in natural and synthetic examples. In addition, pause and voicing relations, and basic pitch movement statistics are considered.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-100"
  },
  "terken89_eurospeech": {
   "authors": [
    [
     "Jacques M. B.",
     "Terken"
    ],
    [
     "René",
     "Collier"
    ]
   ],
   "title": "Automatic synthesis of natural-sounding intonation for text-to-speech conversion in dutch",
   "original": "e89_1357",
   "page_count": 3,
   "order": 109,
   "p1": "1357",
   "pn": "1359",
   "abstract": [
    "A set of rules is proposed for the automatic synthesis of natural-sounding intonation as part of speech synthesis in Dutch from unrestricted text. Results of a formal perceptual evaluation show that the synthetic intonation is judged to be as natural as human intonation for isolated utterances; for texts, additional provisions are required to model contributions of text structure. It is suggested that the regularities expressed by the rules may apply to other languages as well.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-101"
  },
  "moreno89_eurospeech": {
   "authors": [
    [
     "P. J.",
     "Moreno"
    ],
    [
     "M.",
     "Martinez"
    ],
    [
     "José M.",
     "Pardo"
    ],
    [
     "J. A.",
     "Vallejo"
    ]
   ],
   "title": "Improving naturalness in a text-to-speech system with a new fundamental frequency algorithm",
   "original": "e89_1360",
   "page_count": 4,
   "order": 110,
   "p1": "1360",
   "pn": "1363",
   "abstract": [
    "In this paper we present an strategy to improve the naturalness of a text to speech system for Spanish based on the development of a new pitch generator. The new model works with text previously parsed with an heuristic breath group parser. The results show that the new pitch generator is preferred by listeners compared to our previous model. It is also shown that errors of the heuristic parser can have a very adverse influence on the naturalness of the speech output.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-102"
  },
  "brovchenko89_eurospeech": {
   "authors": [
    [
     "T.",
     "Brovchenko"
    ],
    [
     "V.",
     "Voloshin"
    ]
   ],
   "title": "Discourse intonation and expressive synthetic speech",
   "original": "e89_1364",
   "page_count": 2,
   "order": 111,
   "p1": "1364",
   "pn": "1365",
   "abstract": [
    "Marked results have been achieved in recent years in text-to-speech synthesis. But though intonation of discourse has receivedo considerable attention in the past decade, prosody being one of the main factors, determining the quality of synthetic speech, surprisingly little research has been carried out into synthetic discourse intonation.\n",
    ""
   ]
  },
  "fant89b_eurospeech": {
   "authors": [
    [
     "Gunnar",
     "Fant"
    ],
    [
     "Anita",
     "Kruckenberg"
    ],
    [
     "Lennart",
     "Nord"
    ]
   ],
   "title": "Rhythmical structures in text reading - a language contrasting study",
   "original": "e89_1498",
   "page_count": 4,
   "order": 112,
   "p1": "1498",
   "pn": "1501",
   "abstract": [
    "Our earlier studies of stress group statistics from text reading in Swedish have been followed up by analyses of readings of texts, faithfully translated into English and French. English and Swedish showed apparent similarities in the increase of foot durations with the number of phonemes or syllables involved and also in terms of compensatory stress adjustments within a sentence and in the trend of a rhythmical continuity of stress pulses across pauses. Foot statistics for French became comparable to that of English and Swedish only when defining stress intervals as ending with a stressed syllable instead of, as in Swedish and English, beginning with a stressed syllable.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-103"
  },
  "monaghan89_eurospeech": {
   "authors": [
    [
     "Alex I. C.",
     "Monaghan"
    ]
   ],
   "title": "Phonological domains for intonation in speech synthesis",
   "original": "e89_1502",
   "page_count": 4,
   "order": 113,
   "p1": "1502",
   "pn": "1505",
   "abstract": [
    "This paper presents work done on the syntax-intonation interface used in the Edinburgh University Centre for Speech Technology Research (CSTR) text-to-speech system, a linguistically sophisticated morph-hased speech-output system for processing unrestricted English text [1]. The first part of the paper discusses the need to identify phonological domains for intonation synthesis and the usefulness of hierarchical information, and the second part shows how an approximation to hierarchical metrical structure can he constructed from a very basic syntactic analysis without recourse to expensive tree-building algorithms.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-104"
  },
  "quazza89_eurospeech": {
   "authors": [
    [
     "S.",
     "Quazza"
    ],
    [
     "G.",
     "Varese"
    ],
    [
     "E.",
     "Vivalda"
    ]
   ],
   "title": "Syntactic pre-processing for high quality text-to-speech",
   "original": "e89_1506",
   "page_count": 4,
   "order": 114,
   "p1": "1506",
   "pn": "1509",
   "abstract": [
    "To the end of improving the naturalness of the intonation generated by a text-to-speech system for Italian, a syntactico-prosodic pre-processor has been devised, inserting prosodic markers at syntactic boundaries in the input text. The relations between syntax and prosody, to be exploited by the system, are being investigated on a speech data base obtained by digital recording of a carefully designed corpus of sentences read by a professional speaker. First results concern the duration and position of breathing pauses, correlated to sentence syntactic structure, A prototype of the syntactico-prosodic parser has been implemented and tested on written texts.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-105"
  },
  "larreur89_eurospeech": {
   "authors": [
    [
     "Danielle",
     "Larreur"
    ],
    [
     "Francoise",
     "Emerard"
    ],
    [
     "F.",
     "Marty"
    ]
   ],
   "title": "Linguistic and prosodic processing for a text-to-speech synthesis system",
   "original": "e89_1510",
   "page_count": 4,
   "order": 115,
   "p1": "1510",
   "pn": "1513",
   "abstract": [
    "The CNET's commercially available text-to-speech system performs an automatic prosodic parsing, based on the detection of a small number of grammatical words. However, the lack of any other syntactic information is a serious source of errors at the supra-segmental level. In this paper, we present a new prosodic parsing algorithm which overcomes that drawback. First, a recursive left-to-right morphosyntactic analysis assigns a grammatical value to each word in the text and transcribes it phonetically. Second, one hundred and forty hierarchized parsing rules divide the message into a sequence of prosodic groups. Finally, prosodic patterns are automatically assigned to each word by queries to a data base of prosodic events. Preliminary tests indicate that such an approach, combining a linguistic processor and a data base of real prosodic features, yields synthetic speech with a high degree of naturalness.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-106"
  },
  "youd89_eurospeech": {
   "authors": [
    [
     "N. J.",
     "Youd"
    ],
    [
     "Frank",
     "Fallside"
    ]
   ],
   "title": "Driving a speech synthesizer from conceptual input in the context of a voice dialogue system",
   "original": "e89_1514",
   "page_count": 4,
   "order": 116,
   "p1": "1514",
   "pn": "1517",
   "abstract": [
    "This paper gives an overview of the message generation component of a voice dialogue system. This takes a conceptual representation of the message, and generates a syntactically labelled surface structure using domain independent linguistic rules. At the syllable level, features representing prosodic focus are used to constrain sentence accent placement. The output is used to drive a speech synthesizer at phoneme level.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-107"
  },
  "blokland89_eurospeech": {
   "authors": [
    [
     "Art",
     "Blokland"
    ],
    [
     "Henry S.",
     "Thompson"
    ]
   ],
   "title": "A parser for feature-based speech recognition",
   "original": "e89_1366",
   "page_count": 3,
   "order": 117,
   "p1": "1366",
   "pn": "1368",
   "abstract": [
    "Many phonemic units can be readily described as a configuration of segments of acoustic features. Certain stop allophones, for example, can be constructed out of the features silence, burst release and aspiration. It seems natural to specify this as a rewrite rule that can be interpreted by a parser: stop <- silence + burst + aspiration. Conventional parsers construct grammatical phrases out of words. The words and phrases are discrete and follow each other in a well-defined order. Such parsers are not appropriate for acoustic segments because these are variable in length, and overlap each other or have gaps separating them. This paper describes the capabilities of a parser that has been modified to handle this kind of input.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-108"
  },
  "nguyentrong89_eurospeech": {
   "authors": [
    [
     "Noel",
     "Nguyen-Trong"
    ]
   ],
   "title": "A recent advance in factorial analysis, related to phonetic feature extraction",
   "original": "e89_1369",
   "page_count": 1,
   "order": 118,
   "p1": "1369",
   "pn": "",
   "abstract": [
    "Among all the tools one can find in descriptive statistics, principal component analysis (PCA) is likely to be of much importance to phonetics for it provides an automatic process to extract features. If we represent a set of speech sounds as a cloud of points in a multi-dimensional space, CPA determines what are, for this cloud, the axes according to which distances between points are maximized. Moreover, such axes permit us to consider the weighted cloud only through its shape, while its absolute position in the space is left out. A formal definition of its structure can be based in this sense upon them.\n",
    ""
   ]
  },
  "shirai89_eurospeech": {
   "authors": [
    [
     "Katsuhiko",
     "Shirai"
    ],
    [
     "Noriyuki",
     "Aoki"
    ],
    [
     "Naoki",
     "Hosaka"
    ]
   ],
   "title": "Phoneme recognition in continuous speech using feature selection based on mutual information",
   "original": "e89_1370",
   "page_count": 4,
   "order": 119,
   "p1": "1370",
   "pn": "1373",
   "abstract": [
    "This paper describes an optimal statistical method to recognize phonemes in continuous speech. The novelty of this method is to search the most effective acoustic features in each acoustic level using the criterion of mutual information between acoustic feature vectors and phoneme labels assigned to the speech wave. In the proposed method for phoneme recognition using multiple acoustic features, input speech is first classified based on acoustic similarity, and possible phoneme is selected using variable acoustic features hierarchically. On each level of acoustic features including power and its variational pattern, LPC Mel-cepstrum and its pattern of temporal change are precisely evaluated. Multi-level clustering is suitable to discriminate phonemes by detecting the most reliable features in that context and by using the effective combination of various acoustic characteristics.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-109"
  },
  "poirier89_eurospeech": {
   "authors": [
    [
     "Franck",
     "Poirier"
    ]
   ],
   "title": "Automatic labelling of continuous speech based on hierarchical representation of the energy",
   "original": "e89_1374",
   "page_count": 4,
   "order": 120,
   "p1": "1374",
   "pn": "1377",
   "abstract": [
    "Automatic labelling of continuous speech is an essential process in speech research to amount of time-aligned acoustic data. An automatic labelling method based on hierarchical representation of the energy function is presented in this paper. This method attemps to find a descending sequence of break points and does not proceed from the left to the right. First, the algorithm tries to label the consonants at the beginning of the sylables, secondly the vowels are labelled and finally the other phonemes are associated to the remaining signal The main idea of this method is to use the structure of the energy function to order the evolution of the labelling process.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-110"
  },
  "thompson89_eurospeech": {
   "authors": [
    [
     "Henry S.",
     "Thompson"
    ]
   ],
   "title": "A chart parsing realisation of dynamic programming, with best-first enumeration of paths in a lattice",
   "original": "e89_1378",
   "page_count": 4,
   "order": 121,
   "p1": "1378",
   "pn": "1381",
   "abstract": [
    "Active chart parsing offers a clean and flexible way of implementing dynamic programming to find the best path through a-cyclic directed lattices. This paper describes how this comes about and what the general form of a chart parsing realisation of dynamic programming takes. Advantage is taken of the resulting flexibility to produce a system which not only finds the best path, but enumerates paths in order. Finally we exemplify the process for finding paths through a word lattice given bi-class probabilities, and gives a few results from some experiments.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-111"
  },
  "dermatas89b_eurospeech": {
   "authors": [
    [
     "E.",
     "Dermatas"
    ],
    [
     "George",
     "Kokkinakis"
    ]
   ],
   "title": "A system for automatic text labelling",
   "original": "e89_1382",
   "page_count": 4,
   "order": 122,
   "p1": "1382",
   "pn": "1385",
   "abstract": [
    "This paper presents a system for automatic labelling of natural language texts according to a more or less detailed system of linguistic categories (grammatical, syntactical, etc.). A Markovian model is used to predict the label of each word of the unknown text. Several assumptions and restrictions improve the computational efficiency with a small decrease of the performance of the system. This has been measured by labelling 120.000 words of Greek newspaper texts with grammatical labels and proved to be satisfactory.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-112"
  },
  "cericola89_eurospeech": {
   "authors": [
    [
     "D.",
     "Cericola"
    ],
    [
     "M.",
     "Danieli"
    ],
    [
     "M. J.",
     "Mollo"
    ],
    [
     "D.",
     "Voltolini"
    ]
   ],
   "title": "Morpho-syntactic tools for speech processing",
   "original": "e89_1386",
   "page_count": 4,
   "order": 123,
   "p1": "1386",
   "pn": "1389",
   "abstract": [
    "We describe a morpho-syntactic analyzer for the Italian language. This system integrates a lexical data-base handling about 2,000,000 forms and a probabilistic syntactic parser. It is intended to provide linguistic information for the tasks of speech recognition and text-to-speech synthesis. A description of the two components, experimental results and performances of the whole system are given.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-113"
  },
  "mastrolonardo89_eurospeech": {
   "authors": [
    [
     "A.",
     "Mastrolonardo"
    ],
    [
     "M.",
     "Refice"
    ]
   ],
   "title": "Measuring the power of self-organized linguistic models",
   "original": "e89_1390",
   "page_count": 4,
   "order": 124,
   "p1": "1390",
   "pn": "1393",
   "abstract": [
    "In this paper some preliminary results are presented concerning a study performed upon self-organized linguistic models for a multilingual speech processing system based on large vocabularies as part of the Esprit project 860 \"Linguistic Analysis of the European languages\". The model considered in this case consists of P.O.S. transition matrices, automatically derived from learning texts, for seven European languages, namely Dutch, English, French, German, Greek, Italian and Spanish. Several parameters have been considered and experimented for evaluating the power of the different linguistic models which can be obtained using different PCS sets.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-114"
  },
  "fouquere89_eurospeech": {
   "authors": [
    [
     "Christophe",
     "Fouquere"
    ]
   ],
   "title": "Is nonmonotonic grammar a solution to natural language processing?",
   "original": "e89_1394",
   "page_count": 4,
   "order": 125,
   "p1": "1394",
   "pn": "1397",
   "abstract": [
    "Natural Language Processing requires flexibility. This statement has considerably influenced Natural Language systems during the last years. Recent studies (3), (5), (15) have suggested that nonmonotonic logic would be an attractive frame. Associated with a bottom-up parse of sentences, it could help to solve ambiguity, to reduce parsing choices, and find corrections when the parse fails. This paper presents a methodology for describing Natural Language grammars in a nonmonotonic first-order logic theory. The general idea is to specify first the most general rules defining the grammatical symbols, then all the exceptional cases. We will expose how this method may help us to solve difficulties, as competence errors. For that purpose, we will use TMS default logic (2) in our examples. Finally, we will argue that this methodology shifts the problems to the definition of the grammar nonmonotonicity allows us to express additional informations that are needed in several cases, even if at the same time other questions are raising.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-115"
  },
  "reynier89_eurospeech": {
   "authors": [
    [
     "Emmanuel",
     "Reynier"
    ],
    [
     "Jean",
     "Caelen"
    ]
   ],
   "title": "ATN compiler and parser for an ASR system",
   "original": "e89_1398",
   "page_count": 4,
   "order": 126,
   "p1": "1398",
   "pn": "1401",
   "abstract": [
    "This paper describes some aspects of the linguistic expert system in continuous speech recognition, which makes part of the DIRA (Integrated Dialogue and Automatic Recognition) project, being developed in our laboratory. This system consists in a rule compiler and an analyzer. The compiler accepts context free or context sensitive rules and transform rules. It produces a transition network (ATN) which contains the nodes corresponding to the syntactic, lexical or phonetic categories and the transition arcs, whose traversal is conditioned by rules and predicates. These conditions arise both from semantic attributes for syntactic semantic analysis, and from phonologic rules describing the phonetic lexical network. In fact, it is possible to produce one multi-layer network in which interactions between syntactic-semantic level and lexical level are strong. For example, one chooses the input level of the grammar (axiom) and the description level of the terminal vocabulary. One can define also phonologic rules constrained by syntax or syntactic constraints from lexical informations, etc...\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-116"
  },
  "elenius89_eurospeech": {
   "authors": [
    [
     "Kjell",
     "Elenius"
    ],
    [
     "Rolf",
     "Carlson"
    ]
   ],
   "title": "Assigning parts-of-speech to words from their orthography using a connectionist model",
   "original": "e89_1534",
   "page_count": 4,
   "order": 127,
   "p1": "1534",
   "pn": "1537",
   "abstract": [
    "The orthographic surface structure of Swedish words has been used for predicting parts-of-speech information using a connectionist approach. This technique can be used to aid syntactic processing within a text-to-speech system. The error back-propagation technique has been used for the connectionist learning. A corpus of the 10 000 most frequent Swedish words have been used for training and testing the system. The results indicate that around 80% of the words can be correctly classified by using the last part of each word. The system is compared to a rule based system that makes the same sort of predictions from word endings. Both systems give comparable results for the lexicon used.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-117"
  },
  "mcallister89_eurospeech": {
   "authors": [
    [
     "Mike",
     "McAllister"
    ]
   ],
   "title": "The problems of punctuation ambiguity in fully automatic text-to-speech conversion",
   "original": "e89_1538",
   "page_count": 4,
   "order": 128,
   "p1": "1538",
   "pn": "1541",
   "abstract": [
    "Fully automatic text-to-speech systems must accept as input any texts in whatever form they might be stored on a computer. As such, the role of punctuation characters in marking sentences, phrases and other textual constructs has to be exploited to produce natural sounding synthetic speech. Some characters not in the alpha-numeric set can, however, act both as text and as punctuation in different situations. A pre-processing module has therefore been implemented which is sensitive to these different roles and attempts to use them in preparing texts for text-to-speech conversion.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-118"
  },
  "refice89_eurospeech": {
   "authors": [
    [
     "M.",
     "Refice"
    ],
    [
     "M.",
     "Savino"
    ]
   ],
   "title": "Word endings analysis of european languages",
   "original": "e89_1542",
   "page_count": 4,
   "order": 129,
   "p1": "1542",
   "pn": "1545",
   "abstract": [
    "This paper reports the preliminary results of a study conducted upon six European languages and started within the framework of the Esprit Project 860 \"Linguistic Analysis of the European Languages\". A software tool allows to dynamically define the length of endings according to some specified constraint with the purpose of identifying POS cohorts. Some of the statistical results obtained from extensive runs performed upon the given corpora are reported and briefly discussed. The performances of the identified POS cohorts with the respect to a labelling task, have been also assessed and the main outcomes are presented and discussed.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-119"
  },
  "prakash89_eurospeech": {
   "authors": [
    [
     "M.",
     "Prakash"
    ],
    [
     "G. V. Ramana",
     "Rao"
    ],
    [
     "C. Chandra",
     "Sekhar"
    ],
    [
     "B.",
     "Yegnanarayana"
    ]
   ],
   "title": "Parsing spoken utterances in an inflectional language",
   "original": "e89_1546",
   "page_count": 4,
   "order": 130,
   "p1": "1546",
   "pn": "1549",
   "abstract": [
    "This paper proposes a new approach to parse spoken utterances suitable for inflectional languages such as Hindi. This approach exploits a lot of redundant syntactic and semantic information present in the inflections. In this approach, parsing is done at three levels: phrase level, clause level and sentence level. This is because each level requires a different parsing strategy. Phrase level parsing uses both the case frame approach and the pattern matching approach. Therefore it has the directional freedom and the ability to focus upon the case markers as in the case frame approach and the robustness of the pattern matcher in the neighbourhood of a case marker. Clause level parsing allows the free occurrence of certain phrases within a clause. Sentence level parsing uses a simple pattern matching technique. The spurious and missing case markers are treated specially using metaknowledge.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-120"
  },
  "berthelin89_eurospeech": {
   "authors": [
    [
     "J. B.",
     "Berthelin"
    ],
    [
     "J. P.",
     "Fournier"
    ],
    [
     "B.",
     "Grau"
    ]
   ],
   "title": "Processing non-expected language",
   "original": "e89_1550",
   "page_count": 3,
   "order": 131,
   "p1": "1550",
   "pn": "1552",
   "abstract": [
    "We define non-expected language as the set of words and phrases which do not match exactly the syntactic, lexical and conversational expectations of a natural language processing system. We consider three different kinds of mistakes and mismatches: - lexical mistakes: misspellt or unknown words, - syntactic mistakes: phrases which are either misconstructed or not described by the grammar, - semantic or pragmatic mistakes: sentences which are either difficultly understood or in contradiction with the system's knowledge. This processing of non-expected language will be applied to intelligent word processing and conversational access to knowledge base.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-121"
  },
  "carter89_eurospeech": {
   "authors": [
    [
     "K. E. P.",
     "Carter"
    ],
    [
     "S.",
     "Gookson"
    ],
    [
     "A. F.",
     "Newell"
    ],
    [
     "J. L.",
     "Arnott"
    ],
    [
     "R.",
     "Dye"
    ]
   ],
   "title": "The effect of feedback on oomposition rate using a simulated listening typewriter",
   "original": "e89_1402",
   "page_count": 3,
   "order": 132,
   "p1": "1402",
   "pn": "1404",
   "abstract": [
    "Simulation Experiments have been conducted on text composition by voice input. Two experiments were conducted to investigate whether composition rate varied with different feedback strategies. No significant variation in composition rate could be attributed to the various feedback strategies used.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-122"
  },
  "chigier89_eurospeech": {
   "authors": [
    [
     "Benjamin",
     "Chigier"
    ],
    [
     "Erik",
     "Urdang"
    ],
    [
     "Judith",
     "Spitz"
    ]
   ],
   "title": "Analysis of two algorithms for telephone speech recognition",
   "original": "e89_1405",
   "page_count": 3,
   "order": 133,
   "p1": "1405",
   "pn": "1407",
   "abstract": [
    "The telephone network presents speech recognition devices with a band-limited, noisy, and, in some cases, distorted speech signal. A series of experiments were performed to quantify the effects of these transformations on two current recognition algorithms: a) an acoustic segmentation algorithm and b) an acoustic classification algorithm. The data used in these experiments are a subset of the TIMIT speech database and a telephone network version of the identical TIMIT utterances (N-TIMIT). In this paper, we present insertion and deletion results for the segmenter (for both conditions, compared to hand transcriptions) as well as patterns observed in segmentation errors as a function of data set. Also presented will be the results of the classification algorithm for both databases.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-123"
  },
  "thomas89_eurospeech": {
   "authors": [
    [
     "T.",
     "Thomas"
    ],
    [
     "J.",
     "Peckham"
    ],
    [
     "E.",
     "Frangoulis"
    ],
    [
     "J.",
     "Cove"
    ]
   ],
   "title": "The sensitivity of speech recognisers to speaker variability and speaker variation",
   "original": "e89_1408",
   "page_count": 4,
   "order": 134,
   "p1": "1408",
   "pn": "1411",
   "abstract": [
    "This paper is a description of an experiment conducted during the Alvey Speech Technology Assessment project to determine whether speaker variabilities could be expressed in simple mathematical terms and whether there was any relation between these speaker variabilities so defined and a speech recogniser's performance. The reason for doing the experiment lies in the realisation that there are basic difficulties in performing reliable speech recogniser performance evaluations, the current predominant technique of performing the assessment using a randomly collected database. We shall describe some of the difficulties associated with using such a database paying particular respect to the changes in the speech recogniser performance with respect to speaker variability.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-124"
  },
  "vu89_eurospeech": {
   "authors": [
    [
     "V. V.",
     "Vu"
    ],
    [
     "R. A.",
     "King"
    ]
   ],
   "title": "Automatic diagnostic and assessment procedures for the comparison and optimisation of time encoded speech (TES) DVI systems",
   "original": "e89_1412",
   "page_count": 5,
   "order": 135,
   "p1": "1412",
   "pn": "1416",
   "abstract": [
    "The development of simple automatic diagnostic and assessment procedures for the comparison and optimisation of TBS-based DVI systems is presented. The use of a \"Diagnostic Matrix\" [1] to assess the variability of input acoustic events purporting to be the same word and to indicate the degree of orthogonality between the acoustic events which form the vocabulary under examination, is discussed. The process of adapting the diagnostic procedures, so that system performance may be evaluated against incremental system parameter changes is described. An initial set of \"Assessment procedures\" used to predict likely system performance under varying noise conditions is presented. Experimental evidence is offered to support the discussion.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-125"
  },
  "hughes89_eurospeech": {
   "authors": [
    [
     "R. D.",
     "Hughes"
    ],
    [
     "R. A.",
     "King"
    ]
   ],
   "title": "A comparison of the performance of \"normal\" and \"whispered\" speech with simple time encoded digital speech (TES) direct voice input (DVI) systems in a tactical military environment",
   "original": "e89_1417",
   "page_count": 4,
   "order": 136,
   "p1": "1417",
   "pn": "1420",
   "abstract": [
    "A preliminary investigation into the performance of a simple Time Encoded Speech (TES) isolated word recognition (IWR) direct voice input (DVI) system, using both normal and whispered (unvoiced) speech is described. Experimental conditions include evaluations with untrained military speakers in severe acoustic background noise (c. 70-90 dB SPL) with handheld omnidirectional microphones. System performance in both normal and whispered speech domains is presented.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-126"
  },
  "hamada89_eurospeech": {
   "authors": [
    [
     "Hiroshi",
     "Hamada"
    ],
    [
     "Satoshi",
     "Miki"
    ],
    [
     "Ryohei",
     "Nakatsu"
    ]
   ],
   "title": "Automatic evaluation of English pronunciation based on speech recognition techniques",
   "original": "e89_1421",
   "page_count": 4,
   "order": 137,
   "p1": "1421",
   "pn": "1424",
   "abstract": [
    "A new method is proposed for automatically evaluating the English pronunciation quality of Japanese speakers. It is assumed that this can be determined based on three criteria: the static characteristics of phonetic spectra, the dynamic structure of spectrum sequences, and the prosodic characteristics of utterances. Evaluation is carried out by comparing English words pronounced by a Japanese with those pronounced by a native speaker using speech recognition techniques. Preliminary experiments show the evaluation results obtained using the proposed method to correspond well with human judgement of pronunciation quality.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-127"
  },
  "bakkum89_eurospeech": {
   "authors": [
    [
     "Mark J.",
     "Bakkum"
    ],
    [
     "Reinier",
     "Plomp"
    ],
    [
     "Louis C. W.",
     "Pols"
    ]
   ],
   "title": "Objective evaluation of word pronunciation by filter-band analysis",
   "original": "e89_1425",
   "page_count": 4,
   "order": 138,
   "p1": "1425",
   "pn": "1428",
   "abstract": [
    "Usually, pronunciation is evaluated subjectively by listening. The aim of this research project is to obtain an objective measurement of the quality of pronunciation. For this objective evaluation a real-time spectral analyzing method is developed on a digital signal-processor (16 bandfilters according to the critical-band model). For every word this method leads to a different trace in a 16-dimensional spectral space. Besides level and speaker normalisation also time normalisation will be applied by determining an average spectrum for each phoneme and transition. CVC-words were analyzed spoken according to the Dutch pronunciation rules by male adults (6 native Dutch and 9 foreigners). The feasibility of the objective evaluation has been investigated by considering whether the spectral information as expressed in various distancemeasures gives an adequate description of subjective judgements of the phonemes.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-128"
  },
  "leiser89_eurospeech": {
   "authors": [
    [
     "R. G.",
     "Leiser"
    ],
    [
     "S. E.",
     "Avons"
    ]
   ],
   "title": "Paralanguage and human-computer dialogue",
   "original": "e89_1429",
   "page_count": 4,
   "order": 139,
   "p1": "1429",
   "pn": "1432",
   "abstract": [
    "Paralanguage is that part of speech which is lost in transcription to formal text. This definition includes vocal segregates, the short non-lexical utterances such as \"mm-hm\" and \"oh!\" which regulate human dialogue. The paper focuses on the role of vocal segregates in human dialogue and presents the argument that the characteristics which make them such effective features of human dialogue might also make them useful in human-computer dialogue. Two studies are described which demonstrate subjects' ability to understand these sounds in both recorded and synthesised form, out of linguistic context. Three testbed applications of vocal segregates in human-computer dialogue are also described, it is concluded that vocal segregates, if used appropriately, can streamline speech input/output dialogues and provide support for users of conventional screen-based systems. Guidelines for the use of vocal segregates in human-computer dialogue are provided. It is recommended that further studies should be conducted to investigate the short-term and long-term benefits of using vocal segregates in human-computer interfaces.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-129"
  },
  "fellbaum89_eurospeech": {
   "authors": [
    [
     "Klaus",
     "Fellbaum"
    ],
    [
     "Rainer",
     "Heinstein"
    ],
    [
     "Helmut",
     "Loebner"
    ]
   ],
   "title": "Speech dialogue systems - state of the art and selected applications",
   "original": "e89_1433",
   "page_count": 4,
   "order": 140,
   "p1": "1433",
   "pn": "1436",
   "abstract": [
    "The paper deals with principles of electronic speech processing and their application in dialogue systems. Firstly the components of speech processing, namely speech recognition, voice store and forward and speech synthesis are discussed. Restrictions are summarized which are mainly caused by speech recognition and the quality of speech synthesis. Then general comments to dialogue systems, examples of application and experiences with existing dialogue systems are given. Finally, one system is presented and its hardware, user surface and the practical work with it are described.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-130"
  },
  "ciaramella89_eurospeech": {
   "authors": [
    [
     "Alberto",
     "Ciaramella"
    ],
    [
     "Davide",
     "Clementino"
    ],
    [
     "Roberto",
     "Pacifici"
    ]
   ],
   "title": "Characterization of a large vocabulary isolated words and continuous speech recognizer",
   "original": "e89_1437",
   "page_count": 4,
   "order": 141,
   "p1": "1437",
   "pn": "1440",
   "abstract": [
    "We describe the architecture of a multiDSP system for large vocabulary speech recognition, called RICO, and we characterize this system both in accuracy and in throughput for 3 subcases (one step isolated words, two steps isolated words,one step continuous speech) on a 1008 words Italian test vocabulary. Then we describe some architectural improvement performed in the following and we characterize the corresponding throughput improvement measured for the most demanding case, i.e. one step continuous speech recognition.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-131"
  },
  "schalk89_eurospeech": {
   "authors": [
    [
     "Thomas B.",
     "Schalk"
    ]
   ],
   "title": "Automating operator-assisted calls using speech recognition",
   "original": "e89_1441",
   "page_count": 4,
   "order": 142,
   "p1": "1441",
   "pn": "1444",
   "abstract": [
    "A speech processing system has been developed that is capable of automating a number of operator services over the telephone network. This system, called the VoiceGateway System (VG), is equipped with the following capabilities: speech recognition, speech output, speech storage, DTMF (touchtone) tone detection, remote database access, and telephone switching system communication. The specific application addressed in this paper is the automation of operator-assisted telephone calls. Operator-assisted calls are those that require telephone operator intervention to complete. These include collect calls, calls billed to a third number, creditcard calls from nontouchtone phones, and person-to-person calls. A detailed description of the way in which the VG interacts with humans to process these calls will be presented with an emphasis on the speech recognition requirements for this application.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-132"
  },
  "murillo89_eurospeech": {
   "authors": [
    [
     "G.",
     "Murillo"
    ],
    [
     "G.",
     "Benbassat"
    ],
    [
     "Y.",
     "Masse"
    ]
   ],
   "title": "The texas-instruments PC-based speech vocabulary development system makes it even easier to put speech into applications",
   "original": "e89_1445",
   "page_count": 1,
   "order": 143,
   "p1": "1445",
   "pn": "",
   "abstract": [
    "As synthetic speech applications demanding a limited vocabulary grow and synthesizer interfacing becomes easier, it is more and more important for the user to have a very performant, easy to use system allowing for the development of synthetic speech vocabulary. Texas Instruments has filled this need with the PC-SDS, a speech development system (composed of two plug-in boards and a dedicated software) working on any IBM compatible personal computer allowing to record a speaker, analyze, LPC synthesize and eventually correcting or modifying this synthesized speech before putting it into the final application. Nevertheless, as products dedicated to generating synthetic become lower priced and as the range of synthetic speech applications widens, vocabulary development can not be restricted to speech specialists and a fast, complete, performant, easy to use system needs to be provided to any user wishing to develop a synthetic speech application. The system presented here (Texas Instruments PC SDS) has been reviewed with this objective in view. The station provides now system guided window menus allowing the setting up for different synthesizers, output bit-rates, speaker characteristics, etc. It also permits simpler manipulation by increasing the number of available options within its \"record-analyze-synthesize\" module and finally it presents a more powerful and interactive memory data generation module.\n",
    ""
   ]
  },
  "kobayashi89_eurospeech": {
   "authors": [
    [
     "Yutaka",
     "Kobayashi"
    ],
    [
     "Yasuhisa",
     "Niimi"
    ]
   ],
   "title": "An efficient VQ code search algorithm using signal continuity",
   "original": "e89_1446",
   "page_count": 4,
   "order": 144,
   "p1": "1446",
   "pn": "1449",
   "abstract": [
    "The authors have developed a method to reduce the code search efforts in Vector Quantization by using continuity of physical signals and their geometiric relation, while keeping the minimal distortion condition. Suppose a codebook designed by the LBG algorithm is given. The algorithm proposed here is based on an assumption that there is some, hopefully good, estimate of the current code, for example, the previous code in the case of speech signal- The distance between the input vector and the previous code becomes a upper bound of the distortion. Distances among code vectors are tabulated in the increasing order for each of them, and used in order to decrease the number of code vectors examined in the quantization phase. The experimental results on speech and picture data proved that the calculation efforts were reduced by several times without increase of distortions comparing with the original full search algorithm.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-133"
  },
  "karjalainen89_eurospeech": {
   "authors": [
    [
     "Matti",
     "Karjalainen"
    ],
    [
     "Toomas",
     "Altosaar"
    ],
    [
     "Paavo",
     "Alku"
    ],
    [
     "Lauri",
     "Lehtinen"
    ],
    [
     "Seppo",
     "Helle"
    ]
   ],
   "title": "Speech processing in the object-oriented DSP environment quicksig",
   "original": "e89_1450",
   "page_count": 4,
   "order": 145,
   "p1": "1450",
   "pn": "1453",
   "abstract": [
    "Several new software techniques are available that can be used to enhance the productivity and flexibility of speech signal processing especially in research and exploratory programming of new algorithms. This paper describes an object-oriented signal processing environment QuickSig and how it is applied in various speech processing tasks. The notion of signals as objects and operations as generic functions is presented. Other features to be described are event-based analysis of signals, representation of symbolic structures, graphic programming and block diagram description of algorithms as well as code generation for DSP processors from high-level specifications. Examples are given to characterize the use of the system in speech synthesis, analysis and recognition.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-134"
  },
  "beroule89_eurospeech": {
   "authors": [
    [
     "Dominique G.",
     "Beroule"
    ]
   ],
   "title": "Management of time distortions through rough coincidence detection",
   "original": "e89_1454",
   "page_count": 4,
   "order": 146,
   "p1": "1454",
   "pn": "1457",
   "abstract": [
    "This paper presents a parallel processing principle implemented in a network of processing units, which requires a specific spectral representation of speech. This so-called topographic representation can be obtained through non-linear transformations that are described. Robustness to time variations of the resulting speech recognition system is shown to result from the duration of the internal signals that are synchronized and summed by the processing units.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-135"
  },
  "carlson89b_eurospeech": {
   "authors": [
    [
     "Rolf",
     "Carlson"
    ],
    [
     "Lennart",
     "Nord"
    ]
   ],
   "title": "Positional variants of Swedish sonorants in an analysis-synthesis scheme",
   "original": "e89_1458",
   "page_count": 4,
   "order": 147,
   "p1": "1458",
   "pn": "1461",
   "abstract": [
    "Analysis and synthesis of positional variants of the Swedish consonants /r,j,l,v/ are reported, and strategies for synthesis work on sonoraiits are discussed. Specifically, the analysis of /r/ shows a complex picture with a set of allophones, using different acoustic cues. Based on analysis of fluent text material, sonorant rules affecting both source and resonator features were formulated and tested. Special efforts were made to handle the realization of consonant clusters.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-136"
  },
  "chan89_eurospeech": {
   "authors": [
    [
     "Chorkin",
     "Chan"
    ],
    [
     "Jun",
     "Bao"
    ],
    [
     "Jian-xiong",
     "Wu"
    ]
   ],
   "title": "A preliminary study on the static representation of short-timed speech dynamics",
   "original": "e89_1462",
   "page_count": 4,
   "order": 148,
   "p1": "1462",
   "pn": "1465",
   "abstract": [
    "The distribution of feature vectors derived from short speech segments is considered a mixture of Gaussian densities. Each density corresponds to a phonetic state in the speech production process and is trained by the observed feature vectors derived from waveform segments corresponding to a specific phoneme. We propose in this paper that a short utterance of speech like a syllable, can be statically represented by a matrix of state transition probabilities considering the utterance as a chain of discrete acoustic events. With these static models, we experienced very competitive performance in terms of recognition rates and speeds when compared with that using Dynamic Programming and Hidden Markov Models for recognition. We are convinced that the proposed static model is more resilient against the lack of huge amount of training data and characterizes the dynamics of short utterances sufficiently for recognition purposes.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-137"
  },
  "moulsley89_eurospeech": {
   "authors": [
    [
     "T. J.",
     "Moulsley"
    ],
    [
     "P. R.",
     "Holmes"
    ]
   ],
   "title": "An adaptive voiced/unvoiced speech classifier",
   "original": "e89_1466",
   "page_count": 4,
   "order": 149,
   "p1": "1466",
   "pn": "1469",
   "abstract": [
    "This paper discusses pattern recognition techniques for classification of speech into voiced and unvoiced sounds. An algorithm is described which is capable of adapting to changing speaker characteristics, spectral distortion and background noise. Robust speech parameters are used in a computationally efficient multi-dimensional classifier which requires no training data base.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-138"
  },
  "stamenkovic89_eurospeech": {
   "authors": [
    [
     "H.",
     "Stamenkovic"
    ],
    [
     "J.",
     "Bakran"
    ]
   ],
   "title": "An intelligent pitch tracker based on formal language theory and phonetic knowledge",
   "original": "e89_1470",
   "page_count": 4,
   "order": 150,
   "p1": "1470",
   "pn": "1473",
   "abstract": [
    "This work presents a technique \"for construction of an accurate and efficient pitch tracker. The proposed model is based on formal languages theory and phonetic knowledge. The phonetic knowledge consists of acoustic description of speech segments and defined properties of speech signal.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-139"
  },
  "giustiniani89_eurospeech": {
   "authors": [
    [
     "Massimo",
     "Giustiniani"
    ]
   ],
   "title": "A new algorithm for fundamental frequency estimation",
   "original": "e89_1474",
   "page_count": 2,
   "order": 151,
   "p1": "1474",
   "pn": "1475",
   "abstract": [
    "Object of the work is a new algorythm to detect pitch pulses position. The algorithm, based on the properties of linear prediction residual, locates the time position of the glottal pulse; it can be succesfully used as a preprocessing for pitch syncronous analisys. The special case of 200 Hz high pass filtered speech signal is used as a stress test of the algorithm.\n",
    ""
   ]
  },
  "reetz89_eurospeech": {
   "authors": [
    [
     "Henning",
     "Reetz"
    ]
   ],
   "title": "A fast expert program for pitch extraction",
   "original": "e89_1476",
   "page_count": 4,
   "order": 152,
   "p1": "1476",
   "pn": "1479",
   "abstract": [
    "A simple and fast algorithm is presented, which extracts international contours over long speech segments with laryngograph-like quality. The algorithm operates in the time domain, is resistant against (non-periodic) noise, and detects pitch reliable in a range between 50 Hz and 1000 Hz without parameter adjustments or voice/voiceless pre-segmenting. The algorithm is divided into four major steps. First, a speech signal is reduced to positive and negative peaks that represent (possible) glottal pulses. Second, the amplitude and frequency variation of the peak train is reduced. Third, a way with minimal frequency variation through the remaining peaks is searched. The distances between the peaks of this way give the duration of the pitch periods. Finally, segments that are too short, and segments with a pitch higher or lower than a given range are eliminated. The algorithm has proven to give reliable results in all applications. The computing time of the algorithm is less than realtime on a VAX 750.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-140"
  },
  "hirst89_eurospeech": {
   "authors": [
    [
     "Daniel",
     "Hirst"
    ],
    [
     "Robert",
     "Espesser"
    ]
   ],
   "title": "Automatic modelling of fundamental frequency curves",
   "original": "e89_1480",
   "page_count": 1,
   "order": 153,
   "p1": "1480",
   "pn": "",
   "abstract": [
    "Fundamental frequency (F0) curves can be factored into two components: a microsegmental profile and a macrosegmental profile. The microsegmental profile reflects deviations from an idealised continuous and smooth macrosegmental profile which can be closely modelled as a quadratic spine function interpolating between a sequence of target points (ms;hz).\n",
    ""
   ]
  },
  "nasri89_eurospeech": {
   "authors": [
    [
     "M. K.",
     "Nasri"
    ],
    [
     "Geneviève",
     "Caelen-Haumont"
    ],
    [
     "Jean",
     "Caelen"
    ]
   ],
   "title": "Comparative study between uniform and variable coding used for inferring prosodic rules in automatic speech recognition expert systems",
   "original": "e89_1518",
   "page_count": 4,
   "order": 154,
   "p1": "1518",
   "pn": "1521",
   "abstract": [
    "This paper describes the use of prosodic rules for segmenting speech signal in sentences, words (lexical and grammatical) and putting milestones on their boundaries. The strategy in an automatic speech recognition system uses these milestones for filtering lexical and syntactical hypotheses in order to limit the depth of the lexical and syntactical search. Prosodic parameters (pitch, energy, duration) are estimated over vocalic nuclei and normalized using two types of coding: a) uniform coding, b) variable coding.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-141"
  },
  "carbonell89_eurospeech": {
   "authors": [
    [
     "Noelle",
     "Carbonell"
    ]
   ],
   "title": "On the use of prosodic knowledge for continuous speech recognition and understanding",
   "original": "e89_1522",
   "page_count": 4,
   "order": 155,
   "p1": "1522",
   "pn": "1525",
   "abstract": [
    "We are currently studying how prosodic cues could facilitate continuous speech recognition and understanding. In this paper, results are discussed concerning the detection of syntagm boundaries from the analysis of fundamental frequency and rhythm variations. Three corpus have been tested corresponding to differents speech production conditions: reading, memorized sentences (short-term memory), quasi-spontaneous information dialogues. Our present rates for the detection of word or syntagm boundaries are roughly: 90% correct markers.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-142"
  },
  "bundgaard89b_eurospeech": {
   "authors": [
    [
     "Michael",
     "Bundgaard"
    ]
   ],
   "title": "An algorithm for recognition of stress in danish and its application in an ASR system",
   "original": "e89_1526",
   "page_count": 1,
   "order": 156,
   "p1": "1526",
   "pn": "",
   "abstract": [
    "A three step algorithm for automatic detection of stressed syllables in Danish and its application in an Automatic Speech Recognition system is described in this paper.\n",
    ""
   ]
  },
  "carlo89_eurospeech": {
   "authors": [
    [
     "A. Di",
     "Carlo"
    ],
    [
     "A.",
     "Paoloni"
    ]
   ],
   "title": "An experiment in word hypothesization performed in the context of a continuous speech recognition system",
   "original": "e89_1527",
   "page_count": 3,
   "order": 157,
   "p1": "1527",
   "pn": "1529",
   "abstract": [
    "The experiment consists in testing an hypothesizer that assembles, in different degrees of corruption of the input phonetic sequence, data on the processing time, on the complexity of those underlattices which develop \"around\" a single lexical hypothesis as well on performances, in numerical terms, of the separate hypotheses of the words present in the symbolic input sequence. An Italian sentence, in graphemic form, is phonetically transcribed and it undergoes a phase of corruption in which every space is suppressed and errors of insertion, deletion and substitution are introduced in order to reach an determined error rate. The corrupting program makes use of statistical knowledge on the performance of acoustical-phonetic decoder and on experiments about human perception. The sequence so corrupted is presented tothe lexical hypothesizer.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-143"
  },
  "kaspar89_eurospeech": {
   "authors": [
    [
     "Bernhard",
     "Kaspar"
    ],
    [
     "Bernd",
     "Lochschmidt"
    ]
   ],
   "title": "SPEECHLEX - phonological word modelling component of an experimental speech recognition system",
   "original": "e89_1530",
   "page_count": 4,
   "order": 158,
   "p1": "1530",
   "pn": "1533",
   "abstract": [
    "SpeechLex is a module of an experimental speech recognition system. The main features of this system axe the rule-based approach for the acoustic-phonetic detection, the incorporation of Fuzzy Logic to handle uncertain decisions, and an overall top-down recognition strategy. The task of SpeechLex is to set up a phonotactical network describing the words of a given vocabulary with common variations in pronunciation and to drive the acoustic-phonetic rules of another module. Processing is, in principle, carried out left-right, i.e. one syllable after the other, but inside-out within a syllable. Since the whole system is of an experimental nature, emphasis was laid on providing it with graphical aids to trace the processing steps and to support its development.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-144"
  },
  "moore89_eurospeech": {
   "authors": [
    [
     "T. A.",
     "Moore"
    ],
    [
     "R. A.",
     "King"
    ]
   ],
   "title": "A voice to data convertor for use in a hostile tactical military environment",
   "original": "e89_1553",
   "page_count": 4,
   "order": 159,
   "p1": "1553",
   "pn": "1556",
   "abstract": [
    "This paper describes a portable Time Encoded Speech (TES) based Isolated Word Recognition (IWR) Direct Voice Input (DVI) system designed as a demonstrator for investigating very-low-bit-rate recognition/synthesis communications for possible use in a hostile tactical military arena. To cope with the special demands of the military environment, the system has been designed to be operated almost entirely by voice. Dedicated speaker \"archetypes\" (templates) are stored on plug-in creditcard size memory cards, retained by the individual user. The equipment has no visual display, all feedback being presented aurally via a speech synthesiser. Provision is made for a display as an aid for training, demonstration and diagnostic purposes only.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-145"
  },
  "boillon89_eurospeech": {
   "authors": [
    [
     "D.",
     "Boillon"
    ],
    [
     "R.",
     "Breitschaedel"
    ],
    [
     "Y.",
     "Corvellec"
    ],
    [
     "D.",
     "Bergmann"
    ]
   ],
   "title": "Speech interface for an experimental office system",
   "original": "e89_1557",
   "page_count": 4,
   "order": 160,
   "p1": "1557",
   "pn": "1560",
   "abstract": [
    "This paper aims at presenting the hardware and software design of an experimental system allowing the inclusion of speech capabilities at the man-machine interface level. That experimental system is made of a workstation and of a SPeech INterface (SPIN) connected to the workstation via a serial link and acting as a complete interface for applications in which speech is involved; the SPeech INterface includes indeed all the elements of speech processing, speaker verification, speech coding and text-to-speech synthesis.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-146"
  },
  "riccio89_eurospeech": {
   "authors": [
    [
     "A.",
     "Riccio"
    ],
    [
     "F.",
     "Carraro"
    ],
    [
     "E.",
     "Mumolo"
    ]
   ],
   "title": "Voice based remote data base access",
   "original": "e89_1561",
   "page_count": 4,
   "order": 161,
   "p1": "1561",
   "pn": "1564",
   "abstract": [
    "The paper decribes an application of Speech Processing technologies in a real environment and the results of a preliminary field trial with real users. The aim was to integrate various Speech Processing capabilities, provided by a dedicated system, with a small data base and to make it possible an enquiry via a standard telephone; both public and private network have been used to test the performances. Since the speech interface was the only way to access the data base, the experiment gave the opportunity to verify on field, both technical and ergonomics aspects.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-147"
  },
  "cecinati89_eurospeech": {
   "authors": [
    [
     "Riccardo",
     "Cecinati"
    ],
    [
     "Alberto",
     "Ciaramella"
    ],
    [
     "Luigi",
     "Licciardi"
    ],
    [
     "Giovanni",
     "Venuti"
    ]
   ],
   "title": "Implementation of a dynamic time warp integrated circuit for large vocabulary isolated and connected speech recognition",
   "original": "e89_1565",
   "page_count": 4,
   "order": 162,
   "p1": "1565",
   "pn": "1568",
   "abstract": [
    "A new custom VLSI circuit for dynamic time warping is described; it can be used for both isolated and connected speech recognition, with both templates and discrete hidden Markov models (D.H.M.M.). The chip has an Harvard architecture, with an internal program ROM horizontally microcoded and an external data memory up to 16 Mbytes: an internal cache relaxes speed requirements for the external data RAM. A separate address generation block and an arithmetic unit specialized to the dynamic programming task complete the internal chip structure. With a clock of 16 MHz. the microinstruction cycle is of 125 nsec.; we have evaluated a task speed up of about 5 in comparison to a DSP implementation. The chip is implemented in a 3 micron 2 metal levels CMOS technology, with a complexity of 70K equivalent transistors and a die size of 6 mm. by 5.5 mm.; it is housed in 68 pins LCC package.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-148"
  },
  "gagnoulet89_eurospeech": {
   "authors": [
    [
     "C.",
     "Gagnoulet"
    ],
    [
     "J.",
     "Damay"
    ]
   ],
   "title": "MAIRIEVOX: a speech-activated voice information system",
   "original": "e89_1569",
   "page_count": 4,
   "order": 163,
   "p1": "1569",
   "pn": "1572",
   "abstract": [
    "In this paper, we describe an application of speaker independent speech recognition over telephone lines. The MAIRIEVOX system is a speech-activated interactive voice response system that provides informations of local interest via the ordinary telephone network. The system has been installed in the town hall of Lannion in France for more than one year. After presenting the main features of this system, we mainly point out the importance of human factors and related studies for the success of such an application. We also mention future improvements of the system and current industrial developments.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-149"
  },
  "haberbeck89_eurospeech": {
   "authors": [
    [
     "Rolf",
     "Haberbeck"
    ]
   ],
   "title": "The communication interface - a management system for advanced user interfaces",
   "original": "e89_1573",
   "page_count": 4,
   "order": 164,
   "p1": "1573",
   "pn": "1576",
   "abstract": [
    "This paper describes a multi modal/multi media communication interface (CIF). The CIF is a basic management system for multi media/multi modal user interfaces. The CIF will support a user friendly man machine dialogue. This concept of interface design will open advanced man machine communication to a wide group of users. A classification of user interfaces is given on the basis of communication theory. The design of the CIF is based on the demands of the architecture and functionality of advanced user interfaces.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-150"
  },
  "buther89_eurospeech": {
   "authors": [
    [
     "H.",
     "Büther"
    ],
    [
     "Rolf",
     "Haberbeck"
    ],
    [
     "C.",
     "Volmary"
    ]
   ],
   "title": "Strategies for the use of multi-media and multi-modal input facilities at office workstations",
   "original": "e89_1577",
   "page_count": 4,
   "order": 165,
   "p1": "1577",
   "pn": "1580",
   "abstract": [
    "In this paper we present strategies for the use of multi-modal/multi-media input facilities at office workstations. Multi-modal/multi-media input facilities refer to the integrated and parallel use of different media. We have been mainly engaged with the integration of the medium speech into office workstations (speech workstations) under consideration of ergonomical criteria (human factors). In this connection we have developed techniques for testing and evaluating multi-media workstations. The test environment, results and resulting strategies are described.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-151"
  },
  "lecomte89_eurospeech": {
   "authors": [
    [
     "I.",
     "Lecomte"
    ],
    [
     "J.",
     "Boudy"
    ],
    [
     "C.",
     "Baillargeat"
    ],
    [
     "M.",
     "Lever"
    ]
   ],
   "title": "Speech processing in car environment",
   "original": "e89_1581",
   "page_count": 1,
   "order": 166,
   "p1": "1581",
   "pn": "",
   "abstract": [
    "In the last twenty years, much has been done on speech enhancement but in most cases, methods have been tested with additive white Gaussian noise and sometimes are only derived in this case.\n",
    ""
   ]
  },
  "ventura89_eurospeech": {
   "authors": [
    [
     "J. C.",
     "Ventura"
    ]
   ],
   "title": "Multiband digital gain controller",
   "original": "e89_1582",
   "page_count": 4,
   "order": 167,
   "p1": "1582",
   "pn": "1585",
   "abstract": [
    "The restoring of correct loudness functions for hearing impaired requires inputloutput functions which depend on both, amplitude and frequency. The described gain controller does so by splitting the input spectrum in three bands and correcting the loudness functions in each separately. The inputloutput static characteristics are fully programmable and the gain is piloted by an envelope detection based on speech properties.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-152"
  },
  "aktas89_eurospeech": {
   "authors": [
    [
     "Abdulmesih",
     "Aktas"
    ],
    [
     "Harald",
     "Höge"
    ]
   ],
   "title": "Multi-DSP and VQ-ASIC based acoustic front-end for real-time speech processing tasks",
   "original": "e89_1586",
   "page_count": 4,
   "order": 168,
   "p1": "1586",
   "pn": "1589",
   "abstract": [
    "This paper describes the architecture of a multi-DSP based acoustic front-end (AkuFE) developed within the speaker adaptive continuous speech understanding and dialogue system SPICOS. The AkuFE is designed as a configurable high performance signal processing VMEbus system employing up to five Texas Instruments TMS320C25 signal processors and an ASIC for vector quantization (VQ) developed within the project. The system is realized on three boards and achieves a total computational power of more than 100 MIPs. In this case two VQ processors can work in parallel. A 68020 based workstation serves as host computer. The AkuFE is employed for the real-time acoustic-phonetic decoding task in the SPICOS system. Due to its flexibility, it can be used for a wide range of real-time speech processing tasks.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-153"
  },
  "coile89_eurospeech": {
   "authors": [
    [
     "Bert Van",
     "Coile"
    ],
    [
     "J. P.",
     "Martens"
    ]
   ],
   "title": "Dutch text-to-speech aids for the vocally handicapped",
   "original": "e89_1590",
   "page_count": 4,
   "order": 169,
   "p1": "1590",
   "pn": "1593",
   "abstract": [
    "This paper describes two Dutch talking communication aids. They offer full text-to-speech capabilities and produce a highly intelligible and naturally sounding output. We focus on the speech synthesis strategy, and its implementation in hard- and software. Much attention is paid to special development tools which have enabled us to perform the software development independently of the hardware.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-154"
  },
  "chen89_eurospeech": {
   "authors": [
    [
     "Xixian",
     "Chen"
    ],
    [
     "Changnian",
     "Cai"
    ]
   ],
   "title": "A novel approach for degraded speech recognition",
   "original": "e89_1594",
   "page_count": 2,
   "order": 170,
   "p1": "1594",
   "pn": "1595",
   "abstract": [
    "Noisy speech recognition is an involved and complicated research area that lacks both sound theory and experiences. Early experiments showed that word recognition performance dropped significantly to noise ratios (SNR) less than 24dB (1) and accuracy reduced by 34% for a SNR of 11 dB (2). Although a number of speech enhancement algorithms have been proposed for noise reduction (2,3), none of them seems to be an effective solution, especially when the SNR is relatively low.\n",
    ""
   ]
  },
  "jovicic89_eurospeech": {
   "authors": [
    [
     "S.T.",
     "Jovicic"
    ],
    [
     "P.",
     "Randelovid"
    ]
   ],
   "title": "An algorithm for time-scaling of speech signal",
   "original": "e89_1596",
   "page_count": 4,
   "order": 171,
   "p1": "1596",
   "pn": "1599",
   "abstract": [
    "This paper presents the investigation results in the field of time scaling of speech signal. Firstly, the perceptual evidences for speeded up and slowed down speech were analysed, both in time and spectral domain. Based on this results a new algorithm for time-scaling of speech was proposed. The algorithm is not pitch synhronous and, because of that, its application is convinient for multitalker environment. Also, flexible choise of processing parameters allows application of proposed algorithm for non speech signals. The algorithm can be easily implemented in real time using a single chip digital signal processor.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-155"
  },
  "varley89_eurospeech": {
   "authors": [
    [
     "M. R.",
     "Varley"
    ],
    [
     "R. J.",
     "Simpson"
    ],
    [
     "T. J.",
     "Terrell"
    ]
   ],
   "title": "Pitch determination algorithms for speech and their implementation using a high performance single chip digital signal processor",
   "original": "e89_1600",
   "page_count": 4,
   "order": 172,
   "p1": "1600",
   "pn": "1603",
   "abstract": [
    "This paper describes two algorithms for pitch determination of speech signals. One is an extensively modified Tucker and Bates algorithm which extracts features of the signal in the time domain and uses these to estimate the pitch period, and the other is a modified SIFT algorithm involving short-term autocorrelation of the LPC residual signal. For a real-time implementation, the modified Tucker-Bates algorithm was written in assembly language for a single chip DSP device, the Motorola DSP56001. Details of the algorithm and its implementation are explained and its performance discussed.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-156"
  },
  "yegnanarayana89_eurospeech": {
   "authors": [
    [
     "B.",
     "Yegnanarayana"
    ],
    [
     "K. V. Madhu",
     "Murthy"
    ]
   ],
   "title": "Analysis of short time speech segments based on linear prediction",
   "original": "e89_1604",
   "page_count": 1,
   "order": 173,
   "p1": "1604",
   "pn": "",
   "abstract": [
    "We present a new method of analysis of short data records that will enable us to process nonstationary signals like speech. The method uses standard autocorrelation-based linear prediction (LP) analysis in two phases. In the first phase a low order LP analysis is used to pick up the gross features of the spectral envelope. The LP residual from the first phase is used to modify the LP spectrum in the second phase to capture the details over a short segment of analysis frame. By this method we try to reduce the finite window effects for short time segment analysis. We make use of the fact that the correlation within the samples of the LP residual is less compared to that of the actual signal. Hence effects of short window size are smaller in the LP residual than in the case of the actual signal.\n",
    ""
   ]
  },
  "owens89_eurospeech": {
   "authors": [
    [
     "F. J.",
     "Owens"
    ],
    [
     "M. S.",
     "Murphy"
    ]
   ],
   "title": "Non-uniform RFT filterbank design for speech processing",
   "original": "e89_1605",
   "page_count": 4,
   "order": 174,
   "p1": "1605",
   "pn": "1608",
   "abstract": [
    "This paper describes a technique for designing uniform and non-uniform filterbanks based on the Running Fourier Transform (RFT). The RFT is implemented by convolving the input signal with one of a family of windows, h(nT) = (nT)ke-anT, where k and a may be chosen to specify the order and bandwidth, respectively, of each analysing filter. An expression for the equivalent composite impulse response has been derived which can be used to optimise its composite amplitude and phase responses. Non-uniform filterbanks can be designed by splitting the frequency band of interest into a number of uniform sections and then optimising the equivalent composite impulse response of each section. Finally, a modified cepstral smoothing technique for non-uniform spectra is presented and shown to be superior to conventional bi-pass filtering.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-157"
  },
  "psutka89_eurospeech": {
   "authors": [
    [
     "Josef",
     "Psutka"
    ]
   ],
   "title": "The use of the LPC residual error autocorrelation to pitch period extraction",
   "original": "e89_1609",
   "page_count": 4,
   "order": 175,
   "p1": "1609",
   "pn": "1612",
   "abstract": [
    "The determination of the fundamental frequency or pitch period of voiced human speech plays an important role in the analysis and synthesis of a speech signal. The present . paper compares some procedures of the fundamental frequency measuring based on modifications of the autocorrelation function (AGP) and on the ACP of the LPC residual error signal.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-158"
  },
  "fohr89_eurospeech": {
   "authors": [
    [
     "Dominique",
     "Fohr"
    ],
    [
     "Yves",
     "Laprie"
    ]
   ],
   "title": "Snorri: an interactive tool for speech analysis",
   "original": "e89_1613",
   "page_count": 4,
   "order": 176,
   "p1": "1613",
   "pn": "1616",
   "abstract": [
    "The Snorri software has been developed at CRIN. It provides a means of editing and analysing a speech signal. The aim of Snorri is to increase the efficiency of our research in acoustic-phonetic decoding. With Snorri the user can record and play back sentences, compute and display spectrograms as well as label speech utterances. It also enables global investigation of speech labelled corpus and extraction of all the occurrences of a given phonetic pattern. More-over, coarticulation phenomena can be studied with other tools like formant tracking displayed in the F1-F2 plane. Snorri is hence an efficient and an easy-to-use tool for speech recognition.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-159"
  },
  "fikri89_eurospeech": {
   "authors": [
    [
     "M.",
     "Fikri"
    ],
    [
     "M. F.",
     "Aou-El-Yazid"
    ],
    [
     "M. R.",
     "El-Ghonemy"
    ]
   ],
   "title": "Improving formant bandwidth estimation by selective lag windowing",
   "original": "e89_1617",
   "page_count": 4,
   "order": 177,
   "p1": "1617",
   "pn": "1620",
   "abstract": [
    "Formant bandwidths are severely underestimated in LPC analysis of voiced segments with low F1/F0 ratio. Parameter quantization errors aggravate the situation and cause steady nasal sounds and high vowels to have amplitude booms. Spectral smoothing using a lag window was used, as a possible remedy. It is found, however. that indiscriminate smoothing reduces the crispness of the synthesized speech. Lag windowing is introduced, only for frames with low F1/F0. Detection of such frames is made using a pattern classification approach after a preliminary reduced order LPC analysis stage. Estimates of F1 and F2 are found by a closed form solution of the 4th order inverse filter polynomial. A Fisher classifier is used to detect the condition of low F1/F0, If a frame is classified as having low F1/F0 the autocorrelation vector is multiplied by a binomial window. Results show improved quality over both indiscriminate spectral smoothing and the standard autocorrelation methods.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-160"
  },
  "alphen89_eurospeech": {
   "authors": [
    [
     "Paul van",
     "Alphen"
    ],
    [
     "Louis C. W.",
     "Pols"
    ]
   ],
   "title": "A real-time FIR-based felterbank, as the acoustic front end of a speech recognizer",
   "original": "e89_1621",
   "page_count": 4,
   "order": 178,
   "p1": "1621",
   "pn": "1624",
   "abstract": [
    "This paper describes the design of a digital filterbank, that runs in real-time on a single TMS 320 processor. The designed filterbank uses Finite Impulse Response (FIR) filters, which enables exact time alignment for all frequencies. Furthermore, the output channels of the filterbank have a high frequency selectivity. This high selectivity has certain advantages, but also potential disadvantages, for instance with respect to low harmonics positioned inside or outside a specific filter. To ensure robustness of the filterbank to changes in f0, an algorithm is presented that eliminates most of these effects. Results for vowel identification are presented.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-161"
  },
  "estola89_eurospeech": {
   "authors": [
    [
     "Kari-Pekka",
     "Estola"
    ]
   ],
   "title": "Multirate Gaussian scale-space filtering",
   "original": "e89_1625",
   "page_count": 4,
   "order": 179,
   "p1": "1625",
   "pn": "1628",
   "abstract": [
    "This paper proposes multirate signal processing methods for realizing Gaussian scale-space filtering. We introduce new computationally efficient interpolated Gaussian scale-space filters. Also, the use of decimators together with interpolated Gaussian filters is considered. The proposed filtering methods require dramatically less computation than conventional Gaussian scale-space filtering especially in cases where the scale changes with an integer factor. The new filters are extremely efficient when the change in scale is an integer power of two. However, also more complex scaling factors such as y/2 can be efficiently realized.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-162"
  },
  "farassopoulos89_eurospeech": {
   "authors": [
    [
     "A.",
     "Farassopoulos"
    ],
    [
     "A.",
     "Farassopoulos-Gerber"
    ]
   ],
   "title": "Speech enhancement for hearing aids in noisy environment",
   "original": "e89_1629",
   "page_count": 4,
   "order": 180,
   "p1": "1629",
   "pn": "1632",
   "abstract": [
    "Hearing impaired persons lose their ability to discriminate speech in ambient noise. Actual Hearing Aids (H.A.), us us ally amplify equally speech and noise, thus they do not provide any speech enhancement. Two techniques are proposed to be used on HA, based on real time adaptive beamforming to reduce the ambient noise thus enhancing the main speech signal. These techniques are evaluated in real time using a specially developed board plugged in a Personal Computer. This evaluation concerns the directivity patterns, the signal to noise ratio and intelligibility improvement in realistic noisy situations. The results are presented.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-163"
  },
  "nakata89_eurospeech": {
   "authors": [
    [
     "Kazuo",
     "Nakata"
    ],
    [
     "Akihiko",
     "Sugiura"
    ]
   ],
   "title": "Noise reduction of speech by neural networks and vector quantization",
   "original": "e89_1633",
   "page_count": 4,
   "order": 181,
   "p1": "1633",
   "pn": "1636",
   "abstract": [
    "In order to reduce the effects of noise mixed with a speech, a method of noise reduction is proposed. The method is composed of a combination of neural networks and vector quantization. A multi-stage neural networks is proposed. Each stage has a small number of output categories, and its performances are evaluated. The scheme is effective for noise reduction in the range of SNR better than 0 dB» and developed to be practicable by the introduction of reject and re-try process.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-164"
  },
  "scaife89_eurospeech": {
   "authors": [
    [
     "Ronan",
     "Scaife"
    ]
   ],
   "title": "Vocal tract area estimation - extending the wakita inverse filter",
   "original": "e89_2648",
   "page_count": 4,
   "order": 182,
   "p1": "2648",
   "pn": "2651",
   "abstract": [
    "This paper describes an algorithm for identifying the parameters of a Vocal Tract model from a record of the lip-pressure/glottal-flow impulse response. The basis of the identification algorithm is a formal inverse to the reference Vocal Tract network. The Vocal Tract model is in the form of a digital network, topologically equivalent to the normal lossless acoustic tube model, but with frequency-dependent terminations. The topological equivalence suggests an extension of the Burg [1] algorithm for identification of a lattice network production model. This extended algorithm is described. To assess the performance of the algorithm, it was tested on impulse response records generated from the six Russian vowels of Fant[2]. The results are compared with those of Strube[3].\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-165"
  },
  "hirsch89_eurospeech": {
   "authors": [
    [
     "Hans-Günter",
     "Hirsch"
    ],
    [
     "Hans-Wilhelm",
     "Rühl"
    ]
   ],
   "title": "Automatic speech recognition in a noisy environment",
   "original": "e89_2652",
   "page_count": 4,
   "order": 183,
   "p1": "2652",
   "pn": "2655",
   "abstract": [
    "The practical use of speech recognition systems is often restricted by the influence of the acoustical environment. Usually speech recognizers are developed in laboratories which do not reflect the practical working conditions such as additive back-ground noise as e.g. inside a car. Various studies (1),(2) have already shown that noise signals have a considerable influence on the recognition rate. In this contribution firstly the influence of noise on a speaker independent isolated word recognizer will be described. Then several possibilities to improve the recognition by the use of a noise suppression method are shown. The noise suppression is based on a spectral subtraction technique.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-166"
  },
  "gehrenbeck89_eurospeech": {
   "authors": [
    [
     "P.",
     "Gehrenbeck"
    ]
   ],
   "title": "A computationally inexpensive algorithm for enhancing speech disturbed by coloured noise",
   "original": "e89_2656",
   "page_count": 1,
   "order": 184,
   "p1": "2656",
   "pn": "",
   "abstract": [
    "The common problem of enhancing speech disturbed by background noise has been examined, under the assumption that no a priori knowledge about the noise is available. That is why we have built an algorithm trying to retain the part of the signal that follows a speech production model.\n",
    ""
   ]
  },
  "compernolle89_eurospeech": {
   "authors": [
    [
     "Dirk Van",
     "Compernolle"
    ],
    [
     "Weiye",
     "Ma"
    ],
    [
     "Marc Van",
     "Diest"
    ]
   ],
   "title": "Speech recognition in noisy environments with the aid of microphone arrays",
   "original": "e89_2657",
   "page_count": 4,
   "order": 185,
   "p1": "2657",
   "pn": "2660",
   "abstract": [
    "In this paper we describe a speech enhancement system which is based on an adaptive beamformer and which can significantly enhance cocktail party speech. This beamformer is effective in suppressing both stationary and non-stationary interferences and is therefore suited as preprocessor for a much wider range of speech recognition systems than any single channel noise suppression scheme. The underlying structure is a steered Griffiths-Jim beamformer, with an added switch for speech detection. Experiments were performed in a reverberant room with a 4 microphone array. With a talker, one to several meters away from the array, typical SNR improvements range from 4 to 14 dB. This is enough to extend the operating range of current speech recognition into many every day circumstances.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-167"
  },
  "schlang89_eurospeech": {
   "authors": [
    [
     "Martin F.",
     "Schlang"
    ]
   ],
   "title": "An auditory based approach for echo compensation with modulation filtering",
   "original": "e89_2661",
   "page_count": 4,
   "order": 186,
   "p1": "2661",
   "pn": "2664",
   "abstract": [
    "The paper presents a system which reduces reverberative and stationary noise in a real room. A corrupted speech signal is analyzed by an audio transform, i.e. the spectral transformation and their characteristic parameters are adapted to the human auditory system. The concept of this transformation is described in detail. The transformed speech signals are filtered with the inverse modulation transfer function of the room causing the reverberation. This function has to be determined only once and is constant for all positions in the room. After filtering a sinusoidal representation of the input signal is extracted. The dereverberated speech is resynthesized on the basis of this part-tone time pattern.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-168"
  },
  "lokenkim89_eurospeech": {
   "authors": [
    [
     "K. H.",
     "Loken-Kim"
    ],
    [
     "Y.",
     "Nara"
    ]
   ],
   "title": "A postprocessor for a large vocabulary Japanese speech recognition system",
   "original": "e89_2001",
   "page_count": 4,
   "order": 187,
   "p1": "2001",
   "pn": "2004",
   "abstract": [
    "In this paper, we explain the design philosophy of a postprocessor that supports a large vocabulary Japanese speech recognition system (Fujitsu Model 2361A). The postprocessor improves performance of the speech recognition system by selecting syntactically plausible sentences from the sentence candidate list. Selection of the sentences is achieved by 1) searching through a bunsetsu lattice (group of short Japanese phrases) which is grouped based on distance scores, 2) use of a bottom-up parser, and 3) case checking based on lexical functional grammar. The result of the performance evaluation was that the postprocessor recovered 21 sentences out of 26 resulting in an 80% recovery rate.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-169"
  },
  "emam89_eurospeech": {
   "authors": [
    [
     "O. S.",
     "Emam"
    ],
    [
     "M. A.",
     "Hashish"
    ]
   ],
   "title": "Large-vocabulary isolated Arabic word recognition system: preliminary results",
   "original": "e89_2005",
   "page_count": 4,
   "order": 188,
   "p1": "2005",
   "pn": "2008",
   "abstract": [
    "Large-vocabulary speech recognition systems require the definition of recognition units. These units should be trainable, well-defined, and insensitive to context. In this paper a set of basic phonetic units, for the Arabic language, has been designed and each individual phone is represented by a Hidden Markov Model. The proposed set of models were tested on a speaker-trained, speech recognition task with a vocabulary of 500 basic Arabic words. The analysis of the errors made by the recognizer resulted in the addition of some context-dependent to the initial phonetic set.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-170"
  },
  "wothke89_eurospeech": {
   "authors": [
    [
     "K.",
     "Wothke"
    ],
    [
     "U.",
     "Bandara"
    ],
    [
     "J.",
     "Kempf"
    ],
    [
     "E.",
     "Keppel"
    ],
    [
     "K.",
     "Mohr"
    ],
    [
     "G.",
     "Walch"
    ]
   ],
   "title": "The SPRING speech recognition system for German",
   "original": "e89_2009",
   "page_count": 4,
   "order": 189,
   "p1": "2009",
   "pn": "2012",
   "abstract": [
    "An experimental speech recognition system was developed for German using an already existing technology reported elsewhere (1). The system recognizes complete sentences when the words are spoken with a small pause in between. The user has to train the system in advance by uttering 110 short sentences. The size of the system's vocabulary is presently limited to about 1300 words, with a coverage being 58% of the running words in a newspaper text from the commercial discourse domain. The system uses 60 allophones and a statistical trigram model made out of a text corpus of 14 million words. The recognition accuracy is over 95%.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-171"
  },
  "charpentier89_eurospeech": {
   "authors": [
    [
     "Francis",
     "Charpentier"
    ],
    [
     "Eric",
     "Moulines"
    ]
   ],
   "title": "Pitch-synchronous waveform processing techniques for text-to-speech synthesis using diphones",
   "original": "e89_2013",
   "page_count": 7,
   "order": 190,
   "p1": "2013",
   "pn": "2019",
   "abstract": [
    "We review in a common framework several algorithms that have been proposed recently, in order to improve the voice quality of speech synthesis using diphones [1-3]. These algorithms are based on a pitch-synchronous overlap-add (PSOLA) approach for modifying the speech prosody and concatenating diphone waveforms. The modifications of the speech signal are performed either in the frequency domain (FD-PSOLA), using the Fast Fourier Transform, or directly in the time domain (TD-PSOLA), depending on the length of the window used in the synthesis process. The frequency domain approach is capable of a great flexibility in modifying the spectral characteristics of the speech signal, while the time domain approach provides very efficient solutions for the real time implementation of synthesis systems. We also discuss the different kinds of distortions involved in these different algorithms.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-172"
  },
  "heike89_eurospeech": {
   "authors": [
    [
     "Georg",
     "Heike"
    ],
    [
     "Reinhold",
     "Greisbach"
    ],
    [
     "Stefan",
     "Hilger"
    ],
    [
     "Bernd",
     "Kröger"
    ]
   ],
   "title": "Speech synthesis by acoustic control",
   "original": "e89_2020",
   "page_count": 3,
   "order": 191,
   "p1": "2020",
   "pn": "2022",
   "abstract": [
    "For performing text-to-speech conversion or synthesis by rule with an articulation based synthesis model data on articulatory dynamics have to be collected. Speech synthesis by acoustic control can be used as an aid for acquiring these articulatory data.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-173"
  },
  "badin89_eurospeech": {
   "authors": [
    [
     "Pierre",
     "Badin"
    ],
    [
     "Gunnar",
     "Fant"
    ]
   ],
   "title": "Fricative production modelling: aerodynamic and acoustic data",
   "original": "e89_2023",
   "page_count": 4,
   "order": 192,
   "p1": "2023",
   "pn": "2026",
   "abstract": [
    "The aerodynamic and acoustic phenomena involved in the production of fricative consonants are far from being completely understood. The overall level and spectral characteristics of the radiated sound pressure are known to primarily depend on the aerodynamic state of the vocal tract, i.e. the pseudo-static pressure drop across the main oral constriction and the cross-sectional area of the constriction. The first part of this paper describes an attempt to establish some quantitative relationships, based upon experimental measurements carried out on a human subject, both for sustained and dynamic voiceless fricatives. The second part of the paper focuses on the acoustic modelling of the vocal tract in the frequency domain , it describes our attempts to match some measured spectra with transfer functions computed from simplified area functions, and the influence of different parameters on these spectra. We show, among other things, that the obstacle effect can be explained in terms of filter characteristics.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-174"
  },
  "mrayati89_eurospeech": {
   "authors": [
    [
     "Mohamed",
     "Mrayati"
    ],
    [
     "René",
     "Carré"
    ]
   ],
   "title": "Speech synthesis based on vocal tract region theory",
   "original": "e89_2172",
   "page_count": 4,
   "order": 193,
   "p1": "2172",
   "pn": "2175",
   "abstract": [
    "A new concept in speech production based on distinctive spatial regions along the vocal tract and distinctive modes, is used in speech synthesis with new insights. It is a step ahead towards articulatory speech synthesis. A 9-parameter model for speech synthesis is described (Region Synthesizer). These parameters are tightly correlated with articulatory parameters. This model has the merit of making complex Acoustic-Articulatory-Phonetic relations easier. The parameters of the model are pseudo-orthogonal. The model is capable of synthesizing vowels and consonants. A new command strategy refers to movements of eight regions vertically around the neutral axis of the vocal tract. It uses extensively synergy and compensatory movements with respect to the mid-point of the vocal tract. We will present in this article results on vowel, vowel-vowel synthesis and examples of coarticulation in VCV cases.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-175"
  },
  "owens89b_eurospeech": {
   "authors": [
    [
     "F. J.",
     "Owens"
    ],
    [
     "G. T. H.",
     "Wright"
    ],
    [
     "N. W.",
     "Ramsey"
    ]
   ],
   "title": "A time-domain articulatory speech synthesiser",
   "original": "e89_2176",
   "page_count": 4,
   "order": 194,
   "p1": "2176",
   "pn": "2179",
   "abstract": [
    "This paper describes a new time-domain articulatory speech synthesis model. The system consists of a model of the speech articulators, a time-domain simulation of a lossy acoustic tube model of the vocal tract and a two-mass model of the vocal cords. The vocal tract area function is expressed as a transformation of ten articulatory variables which provide a two-dimensional geometrical description of the vocal tract in the mid-sagittal plane. The acoustic tube model is based on the Kelly-Lochbaum structure which uses the method of reflection coefficients to compute the foward and backward sound pressure waves in a lossless digital transmission line analogue of the vocal tract. The basic Kelly-Lochbaum structure has been extended to permit the modelling of viscous friction losses, yielding wall losses, radiation from the mouth, variable vocal tract length, aspiration and frication.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-176"
  },
  "lehtinen89_eurospeech": {
   "authors": [
    [
     "Lauri",
     "Lehtinen"
    ],
    [
     "Matti",
     "Karjalainen"
    ]
   ],
   "title": "Individual sounding speech synthesis by rule using the microphonemic method",
   "original": "e89_2180",
   "page_count": 3,
   "order": 195,
   "p1": "2180",
   "pn": "2182",
   "abstract": [
    "This paper describes the microphonemic speech synthesis method and its implementation in the form of a text-to-speech synthesizer. The synthesis is based on combining waveforms that are collected from real speech, thus retaining the most part of the individual features of the speaker. The rule-based text-to-speech synthesis is based on object-oriented representations of the hierarchical structure of the units in speech. Syllable classes play an important role as a unit for the prosodic structure.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-177"
  },
  "bimbot89_eurospeech": {
   "authors": [
    [
     "Frédéric",
     "Bimbot"
    ],
    [
     "Gérard",
     "Chollet"
    ],
    [
     "P.",
     "Deleglise"
    ]
   ],
   "title": "Speech synthesis by structured segments, using temporal decomposition and a glottal excitation",
   "original": "e89_2183",
   "page_count": 4,
   "order": 196,
   "p1": "2183",
   "pn": "2186",
   "abstract": [
    "Classical speech synthesis systems either concatenate diphone-like tabulated patterns or reconstruct speech parameters according to pre-defined rales. Both techniques show drawbacks: the former lacks flexibility while the latter is highly time-consuming to built.\n",
    "We propose an intermediate technique using structured segments: segmental units are still resorted to, but they are automatically analysed in terms of a set of spectral targets, a temporal decomposition pattern and a parametric glottal excitation. Structured segments can then be handled by rules. They also supply a valuable material which can be referred to, for building gradually a reconstructive synthesis system.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-178"
  },
  "falaschi89b_eurospeech": {
   "authors": [
    [
     "Alessandro",
     "Falaschi"
    ],
    [
     "Massimo",
     "Giustiniani"
    ],
    [
     "Massimo",
     "Verola"
    ]
   ],
   "title": "A hidden Markov model approach to speech synthesis",
   "original": "e89_2187",
   "page_count": 4,
   "order": 197,
   "p1": "2187",
   "pn": "2190",
   "abstract": [
    "This paper describes a novel technique for producing smooth speech parametric representation evolution by means of an application of traditional hidden Markov modeling techniques. It is based on the consideration that HMM training is able to locate the main acoustic events which occur during the speech process; thus the obtained decomposition can be used to reproduce the linguistic units utilized for training. This is accomplished by interpolating the state-related features values with some weighting functions, as it is done for temporal decomposition technique [1] [15]. Results will be given for isolated word synthesis.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-179"
  },
  "posmyk89_eurospeech": {
   "authors": [
    [
     "Reinhard",
     "Posmyk"
    ]
   ],
   "title": "Time-domain synthesizer for preserving microprosody",
   "original": "e89_2191",
   "page_count": 4,
   "order": 198,
   "p1": "2191",
   "pn": "2194",
   "abstract": [
    "In this paper a new technique for coding speech signals for the use in high quality text-to-speech environment is presented. A prosodic control of the acoustic output is implemented merely by modifying the speech waveform during the resynthesis phase. Any degradation in sound quality is avoided, since no parametric representation of the speech signal is used for the storage of speech. This algorithm preserves the intrinsic features of the original speech signal (microprosody) while allowing for a prosodic control with only few values per time. The algorithm requires very small computational equipment for the synthesis.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-180"
  },
  "takeda89_eurospeech": {
   "authors": [
    [
     "Kazuya",
     "Takeda"
    ],
    [
     "Katsuo",
     "Abe"
    ],
    [
     "Yoshinori",
     "Sagisaka"
    ],
    [
     "Hisao",
     "Kuwabara"
    ]
   ],
   "title": "Adaptive manipulation of non-uniform synthesis units using multi-level unit transcription",
   "original": "e89_2195",
   "page_count": 4,
   "order": 199,
   "p1": "2195",
   "pn": "2198",
   "abstract": [
    "A synthesis-by-rule system based on the selective use of non-uniform synthesis units has been developed. This system uses a natural speech database and an algorithm which searches the database for the optimal speech segment to be used as the synthesis unit. Because of flexible use of synthesis units, this scheme has great advantages, especially in expressing many coarticulatory variations. However, in the system, because of its great numbers of units, precise manipulation of speech segments is difficult. In this paper, we will discuss precise manipulation using multiple acoustic-phonetic labels of the unit database, consists of the following five levels: phonemic symbol, acoustic event, allophonic variation, inseparable phenomena and vowel center level. Based on the transcription, speech segments of the database can be utilized for synthesis units by adapting their acoustic-phonetic characteristics.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-181"
  },
  "lobo89_eurospeech": {
   "authors": [
    [
     "A. P.",
     "Lobo"
    ],
    [
     "William A.",
     "Ainsworth"
    ]
   ],
   "title": "Evaluation of a glottal ARMA modelling scheme",
   "original": "e89_2027",
   "page_count": 4,
   "order": 200,
   "p1": "2027",
   "pn": "2030",
   "abstract": [
    "It is well known that glottal pulse shapes differ from person to person and also for the same person for utterances in different contexts. Since our objective was to determine the effect of the glottal waveshape on synthetic speech we have implemented an analysis oy synthesis system whose characteristics can be controlled through a set of parameters to realize any desired voice source characteristics. A natural continuous voiced utterance was analyzed and it is shown that the source and the vocal tract parameters are well estimated by combining a 6 parameter source model with ARMA analysis. The better performance of this system over two other methods i.e. closed phase LPC covariance analysis[5] and robust LPC analysis [2] (for the case of non gaussian excitations) is demonstrated in terms of formant tracking ability and efficiency of resynthesis.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-182"
  },
  "alku89_eurospeech": {
   "authors": [
    [
     "Paavo",
     "Alku"
    ],
    [
     "Unto K.",
     "Laine"
    ]
   ],
   "title": "A new glottal LPC method of low complexity for speech analysis and coding",
   "original": "e89_2031",
   "page_count": 4,
   "order": 201,
   "p1": "2031",
   "pn": "2034",
   "abstract": [
    "A new straightforward method to compute glottal pulses from speech samples is presented. In the method the human speech production mechanism is modelled with three functional blocks: the source, the tract and the lip radiation. After cancelling the effect of the vocal tract and the lip radiation glottal pulses close to the natural shape are obtained. The method can be effectively applied to speech coding. Improvements of this new method as compared to conventional LPC-coders are discussed. Coding results of long vowels and short words consisting of vowels are presented.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-183"
  },
  "gautherot89_eurospeech": {
   "authors": [
    [
     "O.",
     "Gautherot"
    ],
    [
     "John S.",
     "Mason"
    ],
    [
     "P.",
     "Corney"
    ]
   ],
   "title": "LPC residual phase investigation",
   "original": "e89_2035",
   "page_count": 4,
   "order": 202,
   "p1": "2035",
   "pn": "2038",
   "abstract": [
    "This paper examines the information content of the residual error associated with the LPC approach to speech analysis. This is done in the Fourier transform domain. In the ideal case, the magnitude of the residual spectrum is flat, suggesting that the major information component is in the phase. We present results on the relative importance of phase and magnitude by various non-linear processing schemes applied to the residual prior to re-construction in a conventional full RELP system. Included in the experiments are LPC vector quantisation and forward and backward pitch prediction. We show that most of the important information within the residual signal is contained in the phase spectrum. Furthermore, the important components are within a relatively narrow frequency band (0 to 600Hz). It is found that coarse quantisation of these components is sufficient to convey the information critical to the speech re-construction. This leads to a coding scheme based on pitch prediction, an LPC codebook, residual phase and gain with a combined rate of 1520 bits/sec.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-184"
  },
  "francesco89_eurospeech": {
   "authors": [
    [
     "R. Di",
     "Francesco"
    ],
    [
     "E.",
     "Moulines"
    ]
   ],
   "title": "Detection of the glottal closure by jumps in the statistical properties of the signal",
   "original": "e89_2039",
   "page_count": 4,
   "order": 203,
   "p1": "2039",
   "pn": "2042",
   "abstract": [
    "Two new methods for the detection of the instant of glottal closure directly from the speech waveform are presented here. Both detect the abrupt changes in the short-term spectral characteristics of the speech signal within a pitch period caused by glottal events. The same statistical approach of sequential detection of events by hypothesis testing is used for this purpose. The first method is based on the maximization of a likelihood ratio, the second one uses a divergence convexity test. Experiments on real speech demonstrate the improved robustness of these methods.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-185"
  },
  "veth89_eurospeech": {
   "authors": [
    [
     "J. de",
     "Veth"
    ],
    [
     "W. van Golstein",
     "Brouwers"
    ],
    [
     "Louis",
     "Boves"
    ]
   ],
   "title": "Robust ARMA analysis for accurate determination of system parameters of the voice source and vocal tract",
   "original": "e89_2043",
   "page_count": 4,
   "order": 204,
   "p1": "2043",
   "pn": "2046",
   "abstract": [
    "When inverse filtering speech signals, many theoretical approximations and practical difficulties are involved. Therefore, care must be taken when interpreting inverse filter results. In this paper it is studied what constraints are needed to obtain reliable separation of source and filter parameters. Inverse filter results are presented based on a Robust ARMA analysis method running in a pitch synchronous mode. It is shown that these results are highly accurate for closed vowels and voiced consonants.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-186"
  },
  "chan89b_eurospeech": {
   "authors": [
    [
     "D. S. F.",
     "Chan"
    ],
    [
     "D. M.",
     "Brookes"
    ]
   ],
   "title": "Variability of excitation parameters derived from robust closed phase glottal inverse filtering",
   "original": "e89_2199",
   "page_count": 4,
   "order": 205,
   "p1": "2199",
   "pn": "2202",
   "abstract": [
    "This paper presents a robust procedure for implementing glottal inverse filtering that requires no manual intervention. Speech from a number of individuals, male and female, has been analysed using this procedure and a glottal model has been fitted to the inverse filtered waveforms. Using the excitation waveform defined by the model, the vocal tract filter has been re-estimated using whole-cycle larynx synchronous analysis. The paper discusses the resultant formant tracks and also the intra-speaker and inter-speaker variability of the glottal model parameters for three selected vowels.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-187"
  },
  "marques89_eurospeech": {
   "authors": [
    [
     "Jorge S.",
     "Marques"
    ],
    [
     "Luis B.",
     "Almeida"
    ]
   ],
   "title": "Sinusoidal modeling of voiced and unvoiced speech",
   "original": "e89_2203",
   "page_count": 4,
   "order": 206,
   "p1": "2203",
   "pn": "2206",
   "abstract": [
    "Speech modeling is an active research area which has received considerable attention in recent years. Application areas include speech coding, recognition and synthesis. Several models have been proposed both in the time and frequency domain contexts. Early frequency domain models approximated the voiced segments by locally periodic signals. The generalized harmonic model was able to deal with departures from periodicity resulting from vocal tract variation, but had difficulty in handling fast pitch variations. Sinusoidal models were developed to overcome these limitations. In this paper, we briefly review the basic concepts related to sinusoidal modelling of speech and present the results of an extensive experimental comparison between several sinusoidal modeling variants.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-188"
  },
  "itahashi89_eurospeech": {
   "authors": [
    [
     "Shuichi",
     "Itahashi"
    ],
    [
     "Kazuyuki",
     "Takagi"
    ]
   ],
   "title": "Automatic formant frequency extraction by moment calculation of speech spectrum",
   "original": "e89_2207",
   "page_count": 4,
   "order": 207,
   "p1": "2207",
   "pn": "2210",
   "abstract": [
    "A revised algorithm of the spectrum moment method for formant frequency estimation is proposed. Second-order and third-order spectrum moments, which reflect the variance and skewness of a spectrum pattern, respectively, are utilized to adjust the frequency regions of centroid calculation. This adjustment improves estimation precision. Experiments using model spectra with an all-pole model gave an estimation error of 1.43 %(male) and 1.07 %(female). Speech material of nineteen kinds of utterances - five Japanese vowels, three kinds of connected vowels and eleven short interrogative sentences - spoken by five male and five female speakers were analyzed. The results gave a mean estimation error of 4.45 % for the male speakers and 7. 30 % for the female speakers.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-189"
  },
  "dalessandro89_eurospeech": {
   "authors": [
    [
     "Christophe",
     "d'Alessandro"
    ]
   ],
   "title": "Time-frequency modifications using an elementary waveform speech model",
   "original": "e89_2211",
   "page_count": 4,
   "order": 208,
   "p1": "2211",
   "pn": "2214",
   "abstract": [
    "An elementary waveform speech model (EWSM) is defined and some capabilities are demonstrated for the modification of localized time-frequency events. The elementary waveforms allow for modelling the local spectro-temporal maxima of energy inside the speech signal by simple mathematical functions. EWSM parameters axe estimated using a frame by frame processing: spectral modelling and segmentation using short-time Fourier transform and LPC spectrum, Fourier filtering according to this segmentation, waveforms spotting in each channel waveform modelling with simple functions. The EWSM parameters are relevant according to the classical theory of speech production, and their modifications yield well-localized time-frequency transformations, including frequency compression/expansion, pitch, formant, noise modifications.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-190"
  },
  "nadeu89_eurospeech": {
   "authors": [
    [
     "Climent",
     "Nadeu"
    ],
    [
     "Eduardo",
     "Lleida"
    ],
    [
     "F. Javier",
     "Hernando"
    ]
   ],
   "title": "Modeling of the analytic spectrum for speech recognition",
   "original": "e89_2215",
   "page_count": 4,
   "order": 209,
   "p1": "2215",
   "pn": "2218",
   "abstract": [
    "In this paper, a new spectral representation is introduced and applied to speech recognition. As the widely used LPC autocorrelation technique, it arises from an optimization approach that starts from a set of M+1 autocorrelations estimated from the signal samples. This new technique models the analytic spectrum (Fourier's transform of the causal autocorrelation sequence) by assuming that its cepstral coefficients are zero beyond M, and uses an extremely simple algorithm to compute the non-zero coefficients. In speech recognition, the same Euclidean cepstral distance measure that is the object of the optimization is also used to calculate the spectral dissimilarity. Preliminary recognition tests with this technique are presented.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-191"
  },
  "gong89_eurospeech": {
   "authors": [
    [
     "Yifan",
     "Gong"
    ],
    [
     "Anne",
     "Boyer"
    ],
    [
     "Jean-Paul",
     "Haton"
    ]
   ],
   "title": "Parallel construction of syntactic structure for continuous speech recognition",
   "original": "e89_2047",
   "page_count": 4,
   "order": 210,
   "p1": "2047",
   "pn": "2050",
   "abstract": [
    "Because of the uncertainty involved in terminal recognition, the objective of a syntactic analyzer under uncertainty is no longer to find one structure fitting the speech, but the best one. Two problems must be solved: (1) define and apply an appropriate quality measure of the resulting structures, in order to find the best one, (2) construct and maintain in parallel a set of plausible partial structures, in order to be able to compare them. We present a syntax analyzer which uses the notion of confidence islands realizing: parallel construction of the syntactic structures of a speech signal using beam-search strategy based on a specific quality measure we introduced, mixed bottom-up and top-down extension of parsing trees, and unification of islands relating to different parsing trees. Applied to continuous speech understanding, for a 200 sentence single speaker test, the system obtained a correct rate of 90% at sentence level.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-192"
  },
  "steinbiss89_eurospeech": {
   "authors": [
    [
     "Volker",
     "Steinbiss"
    ]
   ],
   "title": "Sentence-hypotheses generation in a continuous-speech recognition system",
   "original": "e89_2051",
   "page_count": 4,
   "order": 211,
   "p1": "2051",
   "pn": "2054",
   "abstract": [
    "In this paper, the dynamic-programming algorithm for continuous-speech recognition is modified in order to obtain a top-N sentence-hypotheses list instead of the usual one sentence only. The theoretical basis of this extension is a generalization of Bellman's principle of optimality. Due to the computational complexity of the new algorithm, a sub-optimal variant is proposed, and experimental results within the SPICOS system are presented.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-193"
  },
  "mckelvie89_eurospeech": {
   "authors": [
    [
     "David",
     "McKelvie"
    ],
    [
     "Fergus R.",
     "McInnes"
    ]
   ],
   "title": "Using entropy as a measure of phoneme lattice quality and to evaluate lexical access mechanisms",
   "original": "e89_2055",
   "page_count": 4,
   "order": 212,
   "p1": "2055",
   "pn": "2058",
   "abstract": [
    "Entropy is proposed as a robust statistical measure of the performance of the lexical access component of a continuous speech recognition system, which has a number of uses. Entropy can be used to measure the amount of information added by grammatical and semantic constraints and hence to evaluate different language models. It can also be used as a system internal measure of the confidence that the system has in its answers, which is strongly correlated to the degree of accuracy achieved by the recogniser. These uses are illustrated by data from the ISTD speech recogniser.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-194"
  },
  "thompson89b_eurospeech": {
   "authors": [
    [
     "Henry S.",
     "Thompson"
    ],
    [
     "David",
     "McKelvie"
    ],
    [
     "Fergus R.",
     "McInnes"
    ]
   ],
   "title": "Robust lexical access for continuous speech using dynamic time warping and finite-state transducers",
   "original": "e89_2059",
   "page_count": 4,
   "order": 213,
   "p1": "2059",
   "pn": "2062",
   "abstract": [
    "This paper presents an approach to lexical access in continuous speech recognition systems based on dynamic time warping at the phoneme level, implemented using finite-state transducers. Robustness is thereby achieved in the face of both contingent and necessary imperfections in the phoneme lattice from which lexical access starts. Issues of training, evaluation and context-sensitivity are discussed.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-195"
  },
  "senders89_eurospeech": {
   "authors": [
    [
     "Walther",
     "Senders"
    ],
    [
     "Marianne",
     "Kugler"
    ],
    [
     "Louis",
     "Boves"
    ]
   ],
   "title": "Simultaneous optimisation of several variables in a probabilistic language model",
   "original": "e89_2063",
   "page_count": 4,
   "order": 214,
   "p1": "2063",
   "pn": "2066",
   "abstract": [
    "When we build a probabilistic language model based on the transition probabilities of syntactic classes instead of on the transition probabilities of word forms, then the system of syntactic classes used is extremely important. In this paper we will report on experiments done for seven languages in which two different syntactic coding systems per language were tested against three other variables: order of probabilistic knowledge, EEC office text versus Newspaper text and 'known' versus 'unknown' text-material.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-196"
  },
  "kurematsu89_eurospeech": {
   "authors": [
    [
     "A.",
     "Kurematsu"
    ]
   ],
   "title": "Relationship between speech processing and language processing - speech translation from Japanese to English",
   "original": "e89_2219",
   "page_count": 1,
   "order": 215,
   "p1": "2219",
   "pn": "",
   "abstract": [
    "Integrating speech and language processing is an important problem to be solved to achieve continuous large vocabulary speech recognition which will be aimed at a speech translation system or a human interface of man-machine system. Since conversational speech is normally continuous, with most words running together, recognition of phrases, at least, of continuous speech is necessary.\n",
    ""
   ]
  },
  "henrich89_eurospeech": {
   "authors": [
    [
     "Peter",
     "Henrich"
    ]
   ],
   "title": "Language identification for the automatic grapheme-to-phoneme conversion of foreign words in a German text-to-speech system",
   "original": "e89_2220",
   "page_count": 4,
   "order": 216,
   "p1": "2220",
   "pn": "2223",
   "abstract": [
    "The German language is interspersed with words, mostly foreign words, which cannot be converted correctly by German grapheme-to-phoneme rules. Almost all of these words belong to the English or French language. In order to convert the whole text correctly, it is necessary to identify the language of each word and then to use the corresponding set of grapheme-to-phoneme rules. The problem of language identification is centrally important for almost every application of automatic text output when its source is a multilingual text database.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-197"
  },
  "nakagawa89_eurospeech": {
   "authors": [
    [
     "Seiichi",
     "Nakagawa"
    ],
    [
     "Yoshihisa",
     "Ohguro"
    ],
    [
     "Yasuhide",
     "Hashimoto"
    ]
   ],
   "title": "The syntax-oriented speech understanding system - SPOJUS-SYNO",
   "original": "e89_2224",
   "page_count": 4,
   "order": 217,
   "p1": "2224",
   "pn": "2227",
   "abstract": [
    "This paper describes a syntax oriented spoken Japanese understanding system named \"SPOJUS-SYNO\". At first, this system makes word HMMs automatically by concatenating syllable-based (trained) HMMs. Then a word lattice is hypothesized by using a word spotting algorithm and word-based HMMs. Finally, the time- synchronous left~to-right parsing algorithm is executed to find the best word sequence from the word lattice according to syntactic knowledge represented by CFG. This system was implemented in the \"UNIX-QA\" task with the vocabulary size of 521 words. Experimental result shows that the sentence understanding rate was about 80% for six male speakers.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-198"
  },
  "iida89_eurospeech": {
   "authors": [
    [
     "Hitoshi",
     "Iida"
    ],
    [
     "Kiyoshi",
     "Kogure"
    ],
    [
     "Kei",
     "Yoshimoto"
    ],
    [
     "Teruaki",
     "Aizawa"
    ]
   ],
   "title": "An experimental spoken natural dialogue translation system using a lexicon-driven grammar",
   "original": "e89_2228",
   "page_count": 4,
   "order": 218,
   "p1": "2228",
   "pn": "2231",
   "abstract": [
    "A new method for spoken natural dialogue translation which analyzes linguistic intention expressions in utterances is presented. An experimental Japanese to English translation system using the proposed method, called the Dialogue Translation Method, has been implemented on a workstation. After analyzing an utterance, two different semantic expressions , propositional contents and intentional contents presented by surface speech act types are separately handled in the transfer process. The method makes transfer processes and rules clearer because it is not necessary to apply transfer rules to handling complicated combinations made from Japanese auxiliary verbs and particles. In a goal-oriented dialogue corpus, 'inquiries and explanations regarding an international conference', the experimental system shows the possibility of an interlingual strategy for utterance intentions.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-199"
  },
  "larsen89_eurospeech": {
   "authors": [
    [
     "Lars Bo",
     "Larsen"
    ],
    [
     "Anders",
     "Baekgaard"
    ],
    [
     "Michael",
     "Bundgaard"
    ]
   ],
   "title": "Constructing a large size lexicon for a continuous speech recognition system",
   "original": "e89_2232",
   "page_count": 4,
   "order": 219,
   "p1": "2232",
   "pn": "2235",
   "abstract": [
    "This paper describes the principles and the implementation of a large size lexicon to be used within a continuous speech recognition system, the HEAD system. The lexicon contains the system application vocabulary and it is a major design criterion that a new vocabulary is entered into the HEAD system at lexicon level only. As the lower levels of the system remains unaltered, no additional training of the system is required when the vocabulary is changed. The lexicon is based on partial allophone information realized as a set of distinctive phonetic features. A lookup consists of a search- and a verification- phase. The search phase is based a number of predefined key archiphones, whereas the verification phase utilizes all available information.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-200"
  },
  "delomier89_eurospeech": {
   "authors": [
    [
     "Dominique",
     "Delomier"
    ],
    [
     "Andre",
     "Meunier"
    ],
    [
     "Mary-Annick",
     "Morel"
    ]
   ],
   "title": "Linguistic features of human-machine oral dialogue",
   "original": "e89_2236",
   "page_count": 4,
   "order": 220,
   "p1": "2236",
   "pn": "2239",
   "abstract": [
    "A thorough analysis of two corpuses made of oral dialogues gathered in authentic situations and comprising two phases of a dialogue with a machine has allowed the identification of three structuration levels in the structure of a dialogue: the informational one, the interactional one and the syntactic one (concerning the global syntax as well as the local one). These three structuration levels can be described in terms of specific features of pragmatic and, above all, linguistic nature, which appears as absolutely essential to the existence of the dialogue as such.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-201"
  },
  "tsopanoglou89_eurospeech": {
   "authors": [
    [
     "A.",
     "Tsopanoglou"
    ],
    [
     "J.",
     "Mourjopoulos"
    ],
    [
     "George",
     "Kokkinakis"
    ]
   ],
   "title": "Continuous speech phoneme segmentation method based on the instantaneous frequency",
   "original": "e89_2067",
   "page_count": 4,
   "order": 221,
   "p1": "2067",
   "pn": "2070",
   "abstract": [
    "A novel continuous speech segmentation method is described based on a measure of the Instantaneous Frequency function of the speech waveform. This function is employed for the estimation of a distance measure which in conjunction to established energy and duration threshold criteria gives the boundaries of speech phonemes, identified from vowels and non vowels as well as vowel and non vowel strings- This segmentation technique is more efficient computationally than established segmentation methods and its accuracy and performance is comparable to the best results obtained by the other methods. Results of application of 4 other methods are also given, when they were applied on a common Greek and English Language, continuous speech data base.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-202"
  },
  "mierlo89_eurospeech": {
   "authors": [
    [
     "E. J. M. van",
     "Mierlo"
    ],
    [
     "E.",
     "Blaauw"
    ],
    [
     "Gerrit",
     "Bloothooft"
    ]
   ],
   "title": "Phoneme segmentation of speech, based on temporal decomposition using band filter spectra and phonetic rules",
   "original": "e89_2071",
   "page_count": 4,
   "order": 222,
   "p1": "2071",
   "pn": "2074",
   "abstract": [
    "Temporal Decomposition (TD) is a technique that can be used to segment speech. Although the technique is most often used with log-area coefficients (or ratios) as input parameters, in relation to an articulatory interpretation of the results, this is not a necessary condition. In this study, the output of a filter bank is used as input for TD. Our aim was not an articulatory interpretation, but simply a type of data reduction which may be helpful for dividing the speech signal into phonetically relevant segments. It is known that TD generally produces more segments than may be expected, on the basis of phonetic expert judgements, while only in a few cases too few segments are found. TD on the basis of band filter energies shows this same behaviour, but in addition the spectra which are assigned to each segment are better interpretable because intensity information is preserved. This is not the case when log-area coefficients (or ratios) are used. The intensity information is probably useful for an improved phoneme segmentation and classification.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-203"
  },
  "moreno89b_eurospeech": {
   "authors": [
    [
     "Asunción",
     "Moreno"
    ],
    [
     "P.",
     "Armas"
    ],
    [
     "Jose B.",
     "Marino"
    ],
    [
     "E.",
     "Masgrau"
    ]
   ],
   "title": "Automatic segmentation of Spanish speech into syllables",
   "original": "e89_2075",
   "page_count": 4,
   "order": 223,
   "p1": "2075",
   "pn": "2078",
   "abstract": [
    "This paper presents an algorithm that provides a syllabic segmentation of speech following the syllabification rules of Spanish language. The implemented algorithm is divided into two parts. First, an initial segmentation is made based on energy contour, sonority and duration. Second, a fine adjustement of syllable boundaries and final segmentation is made by applying syllabic rules.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-204"
  },
  "sorensen89_eurospeech": {
   "authors": [
    [
     "Helge B. D.",
     "Sorensen"
    ],
    [
     "Paul",
     "Dalsgaard"
    ]
   ],
   "title": "Multi-level segmentation of natural continuous speech using different auditory front-ends",
   "original": "e89_2079",
   "page_count": 4,
   "order": 224,
   "p1": "2079",
   "pn": "2082",
   "abstract": [
    "The research reported in this paper is aiming at acoustic-phonetic segmentation of speech signals to be used in a continuous speech recognition system. The goal of segmentation is to transform the continuous speech signal into a discrete set of segments each describing an acoustic event which corresponds to a homogeneous sound element. Recent years of research into multi-level segmentation of continuous speech has used either a neurophysiological auditory model or a Fourier Transform as front-end processing. This paper describes and presents results obtained from a system configuration consisting of a phychoacoustic auditory model and a multi-level segmentation algorithm. Furthermore this alternative system is modified and compared to multi-level segmentation using an original/modified neurophysiological auditory model. All results are based on analysis of a large database of naturally spoken continuous speech.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-205"
  },
  "dours89_eurospeech": {
   "authors": [
    [
     "C.",
     "Dours"
    ],
    [
     "M. de",
     "Calmes"
    ],
    [
     "H.",
     "Kabre"
    ],
    [
     "J. M.",
     "Pecarte"
    ],
    [
     "Guy",
     "Perennou"
    ],
    [
     "Nadine",
     "Vigouroux"
    ]
   ],
   "title": "A multi-level automatic segmentation system: SAPHO and VERIPHONE",
   "original": "e89_2083",
   "page_count": 4,
   "order": 225,
   "p1": "2083",
   "pn": "2086",
   "abstract": [
    "After having described problems involved in automatic labelling and having shown current application in the domain of automatic speech processing, we expose our automatic labelling system. This system comprises two sub-systems: the sub-system for labelling and segmenting to phonetic events, the VERIPHONE system, which aligns strings of phonetic events with strings of phonetic notation. Results obtained on the EUROM-0 corpus are discussed.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-206"
  },
  "perennou89_eurospeech": {
   "authors": [
    [
     "Guy",
     "Perennou"
    ],
    [
     "Nadine",
     "Vigouroux"
    ],
    [
     "Louis-Jean",
     "Boe"
    ],
    [
     "Geneviève",
     "Caelen-Haumont"
    ],
    [
     "Denis",
     "Autesserre"
    ],
    [
     "Dominique",
     "Fohr"
    ]
   ],
   "title": "Comparison of temporal and frequential methods of speech data base labelling",
   "original": "e89_2087",
   "page_count": 1,
   "order": 226,
   "p1": "2087",
   "pn": "",
   "abstract": [
    "French laboratories of GRECO-PRC \"Dialogue Homme-Machine\" are involved in \"Speech Input and Assessment Methodology\", the Esprit Project SAM nr 2589.\n",
    ""
   ]
  },
  "erp89_eurospeech": {
   "authors": [
    [
     "A. van",
     "Erp"
    ],
    [
     "C. G. J.",
     "Houben"
    ],
    [
     "William J.",
     "Barry"
    ],
    [
     "Martine",
     "Grice"
    ],
    [
     "Louis-Jean",
     "Boe"
    ],
    [
     "G.",
     "Braun"
    ],
    [
     "Piero",
     "Cosi"
    ],
    [
     "N.",
     "Dyhr"
    ],
    [
     "Guy",
     "Perennou"
    ],
    [
     "Nadine",
     "Vigouroux"
    ],
    [
     "Denis",
     "Autesserre"
    ]
   ],
   "title": "A unified approach to the labelling of speech: first multilingual results",
   "original": "e89_2088",
   "page_count": 4,
   "order": 227,
   "p1": "2088",
   "pn": "2091",
   "abstract": [
    "In ESPRIT-project nx, 2589 'Multilingual Speech Input and Output Assessment Methodology and Standardisation' (SAM), eight European languages (viz. Danish, Dutch, English, French, German, Italian, Norwegian and Swedish) form the subject of investigation. As a basic support tool for the development of assessment methods a standard approach to the (semi-automatic) labelling of speech is needed, which will serve, at the same time, the European speech community at large. In the present contribution some of the work going on in the ESPRIT-SAM project are displayed. Some of the labelling criteria used are presented, as well as some data on the agreements between labellers. To conclude, it is presented how the standard approach to semi-automatic labelling is envisaged.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-207"
  },
  "thompson89c_eurospeech": {
   "authors": [
    [
     "Henry S.",
     "Thompson"
    ]
   ],
   "title": "Hill climbing to improve the performance of rule-based segmentation and labelling",
   "original": "e89_2092",
   "page_count": 4,
   "order": 228,
   "p1": "2092",
   "pn": "2095",
   "abstract": [
    "Results are reported of a pilot experiment in the use of hill climbing to derive the numerical parameters used in a rule-based approach to segmentation and labelling of continuous speech. Starting from two sorts of symbolically expressed rules, segment rules and feature rules, the user may nominate any numerical field for hill climbing, together with desired mean rate of change. Given a suitable corpus of positive and negative examples, the system then optimises the values of these fields with respect to overall labelling performance. An open test of context-specialised versions of an existing rule for identifying nasals showed a roughly 50% reduction in error rate.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-208"
  },
  "seidl89_eurospeech": {
   "authors": [
    [
     "S.",
     "Seidl"
    ],
    [
     "R.",
     "Poirier"
    ]
   ],
   "title": "An approach for automatic determination of break points in the speech waveform",
   "original": "e89_2096",
   "page_count": 4,
   "order": 229,
   "p1": "2096",
   "pn": "2099",
   "abstract": [
    "Very often in signal analysis it is necessary to determine certain points in the signal waveform as anchor points for the processing. This paper proposes a strategy to find automatically the voice onset for initial vowels, the burst point for voiceless and voiced stops and the release point for nasals. The presented approach exploits only the information inherent in the waveform of the signal by imitating the human skill of abstraction. The procedure is divided into the following steps 1. data compression by a) coding the original signal sampled at 16 kHz by its local extrema and their position in time b) eliminating the silent parts of the signal c) signal smoothing 2. classification of the syllables by their type 3. special treatment for each type of syllable\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-209"
  },
  "datta89_eurospeech": {
   "authors": [
    [
     "A. K.",
     "Datta"
    ]
   ],
   "title": "Manner-based labelling of speech signal using total energy profile",
   "original": "e89_2100",
   "page_count": 4,
   "order": 230,
   "p1": "2100",
   "pn": "2103",
   "abstract": [
    "The paper presents a system for manner-based labelling of the speech signal for isolated words from total energy profile. The manner-based classes selected are sibilant, vowel, nasal murmur, semi vowel and lateral, trill, unvoiced unaspirated interrupt, unvoiced aspirated interrupt, voiced unaspirated interrupt , voiced aspirated interrupt and consonant clusters. The results of labelling for 100 words uttered by one male and one female informants are presented. The relavance of this approach in very large vocabulary phoneme-based spoken word recognition system is discussed.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-210"
  },
  "houben89_eurospeech": {
   "authors": [
    [
     "C. G. J.",
     "Houben"
    ]
   ],
   "title": "Automatic labelling of speech using an acoustic-phonetic knowledge base",
   "original": "e89_2104",
   "page_count": 4,
   "order": 231,
   "p1": "2104",
   "pn": "2107",
   "abstract": [
    "At our laboratories we are presently developing an automatic labelling method. In our approach, knowledge of the behaviour of signal parameters has been formalised in the form of context-sensitive rules and stored in a knowledge base. Prior to the labelling, the rules take a given phonetic transcription as an input and produce a suggestion of what the actual labelling procedure should be looking for in the speech signal. In the present contribution an outline of our knowledge based approach will be given.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-211"
  },
  "collier89b_eurospeech": {
   "authors": [
    [
     "René",
     "Collier"
    ],
    [
     "A. de",
     "Zitter"
    ],
    [
     "Jacques M. B.",
     "Terken"
    ]
   ],
   "title": "On the perceptual salience of melodical variations and its consequences for intonation synthesis",
   "original": "e89_2108",
   "page_count": 4,
   "order": 232,
   "p1": "2108",
   "pn": "2111",
   "abstract": [
    "This paper describes an experiment in which listeners had to evaluate the amount of perceptual difference between pitch contours derived from the same intonation pattern. The melodical difference was felt to be largest if the contours differed in overall shape, irrespective of additional differences in the size of the pitch movements. On the speakers' side, the choice between different contour shapes appears to be controlled by syntactic factors. Listeners however, have no outspoken preference for one shape over the other.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-212"
  },
  "gartenberg89_eurospeech": {
   "authors": [
    [
     "R. D.",
     "Gartenberg"
    ],
    [
     "Ingo",
     "Hertrich"
    ]
   ],
   "title": "Speaker responses to F0 manipulations in partly controlled simulated dialogues",
   "original": "e89_2112",
   "page_count": 1,
   "order": 233,
   "p1": "2112",
   "pn": "",
   "abstract": [
    "Accents can be conveyed by F0 peaks timed either 'early', 'medially' or 'late' with respect to the onset of the stressed vowel. Perception tests involving single-peaked utterances have shown that listeners can differentiate these variants and attribute them distinctive intonational meanings. Production tests comprising simulated interviews have confirmed that speakers use intonation to convey such differences in meaning in single-peaked utterances when their linguistic means of expression are limited through the test situation.\n",
    ""
   ]
  },
  "koopmansvanbeinum89_eurospeech": {
   "authors": [
    [
     "Florien J.",
     "Koopmans-van Beinum"
    ],
    [
     "Dick R. van",
     "Bergem"
    ]
   ],
   "title": "The role of 'given' and 'new in the production and perception of vowel contrasts in read text and in spontaneous speech",
   "original": "e89_2113",
   "page_count": 4,
   "order": 234,
   "p1": "2113",
   "pn": "2116",
   "abstract": [
    "Studies on the role of 'given' and 'new' information in speech communication normally concentrate mainly on prosodic aspects like duration and fundamental frequency. Except for one study, no significant differences between 'given' and 'new' words are found. In our study we included, unlike the other studies, spectral measurements together with duration and F0 for vowels occurring in (the same) 'given' and 'new' words, in read sentences and in conversation. A significant difference in distance to the centroid and in F0 could be demonstrated between 'given' and 'new' in conversation, but for duration no significant effects could be observed. Whether the spectral differences play a role in perception too, is subject of further research.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-213"
  },
  "potapova89_eurospeech": {
   "authors": [
    [
     "Rodmonga",
     "Potapova"
    ]
   ],
   "title": "Some aspects of intonation in modern standard Russian (analysis-synthesis-analysis)",
   "original": "e89_2117",
   "page_count": 4,
   "order": 235,
   "p1": "2117",
   "pn": "2120",
   "abstract": [
    "In modem standard Russian, speech intonation is interpreted in three types of utterances: terminal declarative, nonterminal declarative, and interrogative (variable). This paper focuses on the acoustical characteristics (fundamental frequency-F0 duration-t, intensity-I) of this types of the utterances and concern with the system of intonation invariants of the Russian standard language.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-214"
  },
  "nushikyan89_eurospeech": {
   "authors": [
    [
     "E.",
     "Nushikyan"
    ]
   ],
   "title": "Analysis, synthesis and perception of emotional speech",
   "original": "e89_2121",
   "page_count": 2,
   "order": 236,
   "p1": "2121",
   "pn": "2122",
   "abstract": [
    "Interest in the role of emotion in communication has continued from ancient time to the present. Considerable research has been conducted on the verbal and non-verbal types of emotional expression. This investigation of fiction texts in English, Russian and Ukranian (languages with different structures) shows that information about emotions comes over multiple linguistic channels by grammatical. structures, lexical cues and acoustic indicators.\n",
    ""
   ]
  },
  "pascal89_eurospeech": {
   "authors": [
    [
     "D.",
     "Pascal"
    ],
    [
     "C.",
     "Thill"
    ],
    [
     "M.",
     "Boyer"
    ]
   ],
   "title": "Perceptive similarity of male voices: correlation with acoustic measures",
   "original": "e89_2123",
   "page_count": 4,
   "order": 237,
   "p1": "2123",
   "pn": "2126",
   "abstract": [
    "A study of the relations between the perceptive similarity of voices and physical measures of the speech signal has been carried out. The goal of this study was to identify a set of acoustic parameters which contribute most to the voice discrimination process. We worked on a corpus of logatoms CVCV. Ten male speakers uttering nine items were considered and a single sample of each item was used. We collected dissimilarity judgments (direct estimation) for every distinct pair of speakers out of sixteen listeners. The experimental procedure was designed to study the effects of item and intrapair order as well as listener effect. In addition, we measured thirteen parameters on the speech signal: fundamental and formant frequencies of the vocalic segments of each item, along with the durations of each different segment. The correlation and regression analyses of the three INDSCAL dimensions - which explained 81.5 % of the perceptive data variance - based on the acoustic variables reveal that the first perceptive discriminant axis is associated with the three formant frequencies of speech vocalic segments as well as with durational parameters. The second perceptive axis is associated with the fundamental frequencies at the center of each vowel. Finally, a third stage in the perceptive discrimination involves duration measures again.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-215"
  },
  "tielen89_eurospeech": {
   "authors": [
    [
     "Mirjam T. J.",
     "Tielen"
    ]
   ],
   "title": "Intelligibility of male and female voices under a few noise conditions",
   "original": "e89_2127",
   "page_count": 4,
   "order": 238,
   "p1": "2127",
   "pn": "2130",
   "abstract": [
    "The aim of the research described in this paper was to ascertain whether or not there are differences in intelligibility between male and female voices under noise conditions which are spectrally in agreement with everyday speech maskers. An experiment was carried out in which the CVC and phoneme intelligibility of ten male and female voices was measured. The listeners also gave their opinion on a number of scales per speaker, including intelligibility. The results of the CVC intelligibility test indicate that no differences exist between male and female speakers as a group, whereas clear differences are found within the groups. The results of the subjectively scored intelligibility correlate fairly well with the results of the CVC intelligibility test.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-216"
  },
  "shevchenko89_eurospeech": {
   "authors": [
    [
     "T.",
     "Shevchenko"
    ]
   ],
   "title": "What's in a voice: a system of regional and social acoustic characteristics based on the analysis of 100 british English voices",
   "original": "e89_2131",
   "page_count": 4,
   "order": 239,
   "p1": "2131",
   "pn": "2134",
   "abstract": [
    "The present paper is an attempt at a systematic description of regional and social acoustic characteristics of the English Voice within the framework of one the ory and method. In a step-by-step analysis a series of experiments with natural speech were conducted with regards to the following acoustic characteristics: fundamental frequency, in tensity duration, long-term average spectrum. The result give evidence to consistent correlation between regional and social acoustic characteristics in an individual voice which may be interpreted as an interplay of two major tendencies in British English intonation, rhythm, temporal organization and voice quality variation: oral folk speech tradition and cultural formal speech tradition*The assumption is supported by the data which demonstrate F0/intensity dominance as pitch and stress indicators, different tendencies in rhythm, tempo and voice quality variation.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-217"
  },
  "cave89_eurospeech": {
   "authors": [
    [
     "Christian",
     "Cave"
    ],
    [
     "Thami",
     "Benkirane"
    ]
   ],
   "title": "A perceptual study of vowel-duration as a cue to word-boundaries in moroccan Arabic",
   "original": "e89_2135",
   "page_count": 1,
   "order": 240,
   "p1": "2135",
   "pn": "",
   "abstract": [
    "In Moroccan Arabic, word final vowels are lengthened when they are followed by a consonant. Compared to vowels in syllable-final position, this lengthening may attain 50%. The difference in duration cannot be put down either to intrinsic or cointrinsic effects, nor to final lengthening. Since this temporal difference occurs at word boundaries it is consequently a potential cue for the identification of word bondaries, as can be shown from minimal pairs such as: /za bfasu/ \"He came with his pickaxe #/zab fasu/ \"He brought his pickaxe\" where the only difference is in the duration of the first vowel.\n",
    ""
   ]
  },
  "grassegger89_eurospeech": {
   "authors": [
    [
     "Hans",
     "Grassegger"
    ]
   ],
   "title": "Perceptual investigations on consonantal segments of a German text-to-speech system",
   "original": "e89_2136",
   "page_count": 3,
   "order": 241,
   "p1": "2136",
   "pn": "2138",
   "abstract": [
    "A German text-to-speech system, called FLEX-DEUTSCH, which has been developed at the Institute of Linguistics (Phonetic Laboratory) of the Hungarian Academy of Sciences, Budapest, was used to generate CV- and VCV-stimuli. These stimuli were administered to 20 German speaking subjects in order to evaluate the system's efficiency concerning consonantal segments. The results of this perception test will be the basis for further modification of the acoustic parameters to improve the segmental intelligibility of the system.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-218"
  },
  "harmegnies89_eurospeech": {
   "authors": [
    [
     "Bernard",
     "Harmegnies"
    ],
    [
     "John",
     "Esling"
    ],
    [
     "Veronique",
     "Delplancq"
    ]
   ],
   "title": "Quantitative study of the effects of settings changes on the LTAS",
   "original": "e89_2139",
   "page_count": 4,
   "order": 242,
   "p1": "2139",
   "pn": "2142",
   "abstract": [
    "Several experiments have suggested that deliberate changes of the speaker's voice quality result in modifying the LTAS of his speech productions. The few studies in this field have mainly dealt with speaker recognition tasks under various conditions, where the speakers were simply asked to modify their voices. However, Nolan (19$3) had 2 trained subjects utter 30 times an invariant text, using a given 'setting' in each utterance. He computed a LTAS for each utterance and mainly performed a qualitative study of the spectral traces. The purpose of our experiment is to combine Nolan's approach, allowing to control the introduced vocal variability, with a quantitative technique allowing to measure the resulting spectral variability. One trained subject has produced a French balanced text with 31 different qualities. Each utterance has been analysed by means of a 2033 Bruel Kjaer FFT analyser. The so obtained LTAS were compared by means of the SDDD dissimilarity index. The results show what groups of settings deliver similar spectral results.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-219"
  },
  "gunzburger89_eurospeech": {
   "authors": [
    [
     "Deborah",
     "Günzburger"
    ],
    [
     "Marianne de",
     "Vries"
    ]
   ],
   "title": "How do minor acoustical cues affect male and female voice quality?",
   "original": "e89_2143",
   "page_count": 3,
   "order": 243,
   "p1": "2143",
   "pn": "2145",
   "abstract": [
    "Speakers' sex identification scores were elicited in order to test the possible perceptual function of minor differentiating cues. The variable cues under investigation, F0 range and F3 location, were manipulated and presented to listeners in a pairwise comparison task. Minor F3 location variation yielded results in the expected direction, whereas F0 range proved to be too small to be perceptually distinctive. Sex-dependent listening behaviour was established and compared with earlier findings.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-220"
  },
  "pardo89_eurospeech": {
   "authors": [
    [
     "José M.",
     "Pardo"
    ],
    [
     "H.",
     "Hasan"
    ]
   ],
   "title": "Large vocabulary speaker-independent isolated-word speech recognition using hidden Markov models: status report and planned research",
   "original": "e89_2146",
   "page_count": 4,
   "order": 244,
   "p1": "2146",
   "pn": "2149",
   "abstract": [
    "In this paper, the motivation for the development of a 1000 isolated-word, speaker-independent speech recognition for Spanish is first introduced. Then the database collection is explained with some details. An approach to build an speech recognition system for the previous task is described later, which is essentially similar to other published approaches used for continuous speech. Finally, some preliminary experiments towards the development of the system are also presented.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-221"
  },
  "cerfdanon89_eurospeech": {
   "authors": [
    [
     "H.",
     "Cerf-Danon"
    ],
    [
     "A.-M.",
     "Derouault"
    ],
    [
     "M.",
     "Elbeze"
    ],
    [
     "B.",
     "Merialdo"
    ]
   ],
   "title": "Speech recognition in French with a very large dictionary",
   "original": "e89_2150",
   "page_count": 4,
   "order": 245,
   "p1": "2150",
   "pn": "2153",
   "abstract": [
    "A key factor in the development of Listening Typewriters is the ability to support Very Large Size Dictionaries (VLSD, containing several hundred of thousands words), because any restriction on the vocabulary is a restriction on potential users. This is even more important for inflected languages, such as French. This paper reports some experiments in the development of a speech recognition system using a Very Large Size French dictionary and gives an overview of our decoding strategy and our prototype. The particularity of our system is that it is based on syllable recognition. In the first phase of our research, the speaker is asked to speak with short pauses between syllables. Our next goal will be Continuous Speech Dictation.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-222"
  },
  "ferretti89_eurospeech": {
   "authors": [
    [
     "Marco",
     "Ferretti"
    ],
    [
     "Stefano",
     "Scarci"
    ]
   ],
   "title": "Large-vocabulary speech recognition with speaker-adapted codebook and HMM parameters",
   "original": "e89_2154",
   "page_count": 3,
   "order": 246,
   "p1": "2154",
   "pn": "2156",
   "abstract": [
    "In large-vocabulary speech recognition previous knowledge about the voice of the speaker is extremely useful for achieving high accuracy. For systems based on VQ and HMMs, a training sample is collected from the new speaker to build the VQ codebook and HMM parameters. We describe some techniques aimed at achieving a fast, adaptation of speaker-independent codebook and HMM parameters, based on a small speech sample provided by the new speaker. Results are presented concerning the 20000-word vocabulary, isolated-utterance, real-time IBM recognizer for the Italian language.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-223"
  },
  "billi89_eurospeech": {
   "authors": [
    [
     "Roberto",
     "Billi"
    ],
    [
     "G.",
     "Arman"
    ],
    [
     "D.",
     "Cericola"
    ],
    [
     "G.",
     "Massia"
    ],
    [
     "M. J.",
     "Mollo"
    ],
    [
     "F.",
     "Tafini"
    ],
    [
     "G.",
     "Varese"
    ],
    [
     "V.",
     "Vittorelli"
    ]
   ],
   "title": "A pc-based very large vocabulary isolated word speech recognition system",
   "original": "e89_2157",
   "page_count": 4,
   "order": 247,
   "p1": "2157",
   "pn": "2160",
   "abstract": [
    "The Speech Recognition Group at Olivetti has designed an isolated word recognition system for Italian which runs on a MS-DOS personal computer and can accept a very large vocabulary in excess of 60,000 words. The system is speaker adapted rather than fully trained: a new speaker is requested to pronounce only 40 words. The system accepts natural language without any restriction and can be used to dictate documents of generic content. The results of a real dictation experiment with an active vocabulary of 30,000 words are presented.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-224"
  },
  "baker89_eurospeech": {
   "authors": [
    [
     "Janet M.",
     "Baker"
    ]
   ],
   "title": "Dragondictate (TM)-30k: natural language speech recognition with 30,000 words",
   "original": "e89_2161",
   "page_count": 3,
   "order": 248,
   "p1": "2161",
   "pn": "2163",
   "abstract": [
    "The DragonDictate(TM)-30K automatic transcription system is a large vocabulary, discrete utterance, natural language speech recognition system. First-time users are not required to provide any initial training or enrollment speech data before using the system. Speech models for 16,000 frequent words / phrases of English are built-in. Powerful adaptation techniques dynamically build and refine up to 30,000 speech models on-line during system use. With its integration of a 80,000 word dictionary and easy introduction of new terminology, DragonDictate(TM) offers a large, open vocabulary which can be used to generate free text immediately in any discipline, however specialized. Although slower than a skilled typist, interactive users of the DragonDictate(TM) system typically create printed text more quickly by speaking than they can generate handwritten text alone. Compatible with most popular and custom software for word-processing, databases, spreadsheets, etc., DragonDictate(TM)-30K presently consists of 1 8-bit peripheral card and software, running near real-time on an MS-DOS 386-based (AT-bus), personal computer with 6 MB memory. Professionals, business executives, and other people with limited typing skills, are now independently creating their documents and reports simply by speaking them. This paper describes the features of DragonDictate(TM) from both technical and user perspectives.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-225"
  },
  "sciarra89_eurospeech": {
   "authors": [
    [
     "Donatella",
     "Sciarra"
    ],
    [
     "Carlo",
     "Scagliola"
    ]
   ],
   "title": "Two-step recognition of large vocabulary isolated words based on diphone spotting",
   "original": "e89_2164",
   "page_count": 4,
   "order": 249,
   "p1": "2164",
   "pn": "2167",
   "abstract": [
    "The paper describes a method for recognizing large vocabulary isolated words in two steps: a preclassification step in which cohorts are scored, and a fine matching step in which the words belonging to a number of best scoring cohorts are compared with the input. The two steps use essentially the same algorithm with a different representation of acoustic knowledges cohorts are represented in terms of gross phonetic classes, while words in terms of diphones. Distance measures from gross classes are simply obtained by taking the minimum of distance measures from the diphones included in the gross classes. Recognition tests on words belonging to a 10,000 word vocabulary indicate that a reduction of computational load to slightly over 20% of that needed to compare the entire vocabulary is obtained, with a very small loss in recognition accuracy.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-226"
  },
  "boves89_eurospeech": {
   "authors": [
    [
     "Louis",
     "Boves"
    ]
   ],
   "title": "A multi-lingual language model for large vocabulary speech recognition",
   "original": "e89_2168",
   "page_count": 4,
   "order": 250,
   "p1": "2168",
   "pn": "2171",
   "abstract": [
    "In this paper the language model for use in multilingual speech technology systems is described that was developed in ESPRIT project 860 'Linguistic Analysis of European Languages'. The model can serve in a variety of applications. In the present contribution attention is focussed on its role in a large vocabulary isolated words speech recognition system. Problems in training and testing a class of models comprising a number of co-operating probabilistic and deterministic knowledge sources are discussed. Results obtained from tests of some of the knowledge sources will be given.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-227"
  },
  "rouat89_eurospeech": {
   "authors": [
    [
     "J.",
     "Rouat"
    ],
    [
     "Renato De",
     "Mori"
    ],
    [
     "J. P.",
     "Adoul"
    ]
   ],
   "title": "Speaker and mother tongue independent analysis and recognition of some nasals, liquids and fricatives for integration in an automatic speech recognition system",
   "original": "e89_2240",
   "page_count": 4,
   "order": 251,
   "p1": "2240",
   "pn": "2243",
   "abstract": [
    "Analysis and recognition experiments of nasals, liquids and fricatives for a 36 word English vocabulary, based on the alphabet and digits are reported. Speaker independent and mother tongue independent recognition tests have been conducted on 40 speakers, different from the other 40 speakers whose speech has been used for system training. Half of the speakers (for training and recognition) are non English speaking people, are not bilingual and come from different countries. Specific and simple analyses were designed for fricative and nasal/liquid recognition, yielding a 98.2 % recognition rate on fricatives and 90.5 % on nasals/liquids. It shows that it is possible to obtain good performance on difficult tasks with simple and specific analyses. These have been integrated in an automatic speech recognition system described in previous publications.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-228"
  },
  "dalby89_eurospeech": {
   "authors": [
    [
     "Jonathan",
     "Dalby"
    ],
    [
     "Alan",
     "Crowe"
    ],
    [
     "Andrew M.",
     "Sutherland"
    ]
   ],
   "title": "Formantbased vowel classification in continuous speech",
   "original": "e89_2244",
   "page_count": 4,
   "order": 252,
   "p1": "2244",
   "pn": "2247",
   "abstract": [
    "This paper presents the results of a series of experiments in acoustic phonetic feature-based recognition of phonemic segments from continuous speech. The work is directed toward the development of a computationally efficient framework which combines acoustic phonetic knowledge and statistical classification techniques. The statistical techniques selected are multivariate Gaussian classifiers which are applied to the problem of identifying the 20 vocalic phonemes (12 vowels and 8 diphthongs) of British Received Pronunciation. Current results of classification of hand segmented data show cumulative correct-in-rank scores of 71%, 88%, and 94% in the top 3 ranks for one male talker and 66%, 85%, and 90% for a second male speaker on test sets of 98 sentence-length utterances containing a total of approximately 750 vowels.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-229"
  },
  "diest89_eurospeech": {
   "authors": [
    [
     "Marc Van",
     "Diest"
    ],
    [
     "Dirk Van",
     "Compernolle"
    ],
    [
     "Andre",
     "Oosterlinck"
    ]
   ],
   "title": "A rule based system for speech verification",
   "original": "e89_2248",
   "page_count": 4,
   "order": 253,
   "p1": "2248",
   "pn": "2251",
   "abstract": [
    "A rule based system for speech verification is proposed as a feedback mechanism for speech recognition. The system provides an explicit way of selecting verification parameters depending on the item to be recognized and its context. An extended continuous logic is used to combine the evidence from different parameters and an automatic way to convert parameter values to logic evidence values is given. From the parameter evaluation and through the combination of logical values 9 an information weight is propagated to account for parameter extraction errors and poorly trained data.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-230"
  },
  "williams89_eurospeech": {
   "authors": [
    [
     "B.",
     "Williams"
    ],
    [
     "S. M.",
     "Hiller"
    ],
    [
     "Fergus R.",
     "McInnes"
    ],
    [
     "Jonathan",
     "Dalby"
    ]
   ],
   "title": "A knowledge-based nasal classifier for use in continuous speech recognition",
   "original": "e89_2252",
   "page_count": 4,
   "order": 254,
   "p1": "2252",
   "pn": "2255",
   "abstract": [
    "In a phoneme-based speaker-adaptive automatic recognition system for continuous English speech, a segmentation algorithm for nasals uses automatically derived thresholds on spectral energy measures. A Gaussian classifier using formant information, with duration, two compound measures, and a 'spectral contrast' measure, is applied to the hypothesised nasal segments, which are classified phonemically. Tests were carried out over the training data and over a second reading for each of two speakers. Results were comparable with earlier work as regards nasal/non-nasal hit rate, and better as regards the ratio of imposters to nasals. This method also goes further and achieves some success at the phonemic level.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-231"
  },
  "tattegrain89_eurospeech": {
   "authors": [
    [
     "Helene",
     "Tattegrain"
    ],
    [
     "Jean",
     "Caelen"
    ]
   ],
   "title": "Phonetic unit localization in a multi-expert recognition system",
   "original": "e89_2256",
   "page_count": 4,
   "order": 255,
   "p1": "2256",
   "pn": "2259",
   "abstract": [
    "This paper describes an acoustic-to-phonetic decoder (APD) (based on a mixed strategy: a) bottom-up which hypothesizes the most robust information about the speech signal, b) top-down which makes some verifications about the acoustic features or about the macro-class localization on the speech signal. In this paper, only the bottom-up strategy is described. In our system, a phoneme is described as a phonetic network whose nodes are mapped onto the acoustic signal. The coarse phonetic description then uses five phonetic networks whose nodes correspond to the acoustic phases of the analyzed sound in the speech signal. These phases are extracted by automatic segmentation using different parameters (energy, pitch, formant frequencies, acoustic cues from an ear model). The bottom-up APD is divided into three steps: a) the first step localizes pseudo-phonetic segments (called acoustic phases) on the signal and defines phoneme boundaries according to a macro-class description (stop consonants, fricatives, other consonants, vowels and pauses); b) context-sensitive rules are then applied in order to filter out the most improbable solutions; c) the third step labels the most significant phase of each phoneme by acoustic features (using Bayesian methods). In these paper, the performance is measured by the comparison between labels generated automatically and labels generated normally: for example, detection of plosive burst rates 97% while detection of occlusive phonetic network rates 94.3%. This strategy is written in Prolog II.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-232"
  },
  "nishinuma89_eurospeech": {
   "authors": [
    [
     "Yukihiro",
     "Nishinuma"
    ],
    [
     "Danielle",
     "Duez"
    ],
    [
     "Chantal",
     "Paboudjian"
    ]
   ],
   "title": "Duration of consonant clusters in French: automatic detection rules",
   "original": "e89_2260",
   "page_count": 4,
   "order": 256,
   "p1": "2260",
   "pn": "2263",
   "abstract": [
    "This study describes how to automatically detect clusters of 2 consonants in a spoken French word recognition system. A set of rules was deduced from the statistic analyses carried out on the 4 corpora recorded by 10 subjects. Five relevant parameters were extracted, namely: 1) voice feature and 2) mode of articulation on the first half of the consonant segment, 3) duration ratio between vowel and consonant segment, 4) duration of consonant segment, and 5) position (prevocalic, intervocalic, postvocalic). This phonetic approach was examined on the GRECO-BDSONS, public test corpus. Our classification was 90% correct using the standard values of the duration parameters provided by our data base.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-233"
  },
  "elsheikh89_eurospeech": {
   "authors": [
    [
     "T. S.",
     "El-Sheikh"
    ],
    [
     "M. R.",
     "El-Ghonemy"
    ],
    [
     "O. M.",
     "Mansour"
    ]
   ],
   "title": "Toward a phoneme-based word recognition system",
   "original": "e89_2264",
   "page_count": 4,
   "order": 257,
   "p1": "2264",
   "pn": "2267",
   "abstract": [
    "The main objective of this paper is to recognize isolated Arabic words using segmentation. In order to achieve this objective, a hierarchical classification system has been designed to classify Arabic phonemes. The first stage of this system is to discriminate between vowels, voiced consonants and unvoiced consonants with a classification rate of 94 %. Then, the voiced consonants are classified into voiced fricatives, voiced plosives or vowel-like phonemes with a classification rate of 85 %. The set of unvoiced consonants is also divided into unvoiced fricatives and unvoiced plosives with a rate of 87 %. The labeled frames are further classified into different phonemes. Contextual smoothing is used to correct some misclassified frames. Thus, instead of storing the features of each word, the features of each phoneme are stored using much smaller storage area.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-234"
  },
  "djoudi89_eurospeech": {
   "authors": [
    [
     "M.",
     "Djoudi"
    ],
    [
     "Dominique",
     "Fohr"
    ],
    [
     "Jean-Paul",
     "Haton"
    ]
   ],
   "title": "Phonetic study for automatic recognition of Arabic",
   "original": "e89_2268",
   "page_count": 4,
   "order": 258,
   "p1": "2268",
   "pn": "2271",
   "abstract": [
    "We propose in this paper a phonetic study of standard Arabic based essentielly on the spectrographic visions of 50 sentences of the DJOUMA corpus we have constituted. This study allowed us to determine the pertinent parameters of continuous speech recognition. For the recognition part, we present the algorithms developed and the results obtained for three important questions: The segmentation of speech into gross phonetic classes. This process is based* on methodes adapted from the APHODEX expert system developed in our group [Foh 86], [Car 86]. The recognition of vowels in continuous speech for multispeakers. The detection of parameters necessary for identification of plosives and fricatives. Results obtained on speech pronounced by 5 different speakers are presented and discussed, together with preliminary results for recognition of Arabic consonants.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-235"
  },
  "caraty89_eurospeech": {
   "authors": [
    [
     "M. J.",
     "Caraty"
    ],
    [
     "J. C.",
     "Richard"
    ],
    [
     "Xavier",
     "Rodet"
    ]
   ],
   "title": "Vowel recognition in a data base of continuous speech: experiments with local and global identification principles",
   "original": "e89_2272",
   "page_count": 1,
   "order": 259,
   "p1": "2272",
   "pn": "",
   "abstract": [
    "In continuous speech, a vocalic segment presents an evolution resulting mainly of coarticulation effects. The idea of a unique point in the vocalic space, interpretable in terms of speech production, is attractive for the identification method. However, assumption of existence and unicity of such a point in vocalic segments is contradicted by many observations. Consequently, an improvement of recognition rate is expected if the evolution within the segment is taken into account in the identification process. Therefore, we propose a vowel recognition experiment by pattern matching (ie location and identification) on a BDSON (1) data base, including about 1500 vowels, in order to compare the advantages of a global principle of identification (ie on the whole segment) with the advantages of its local counterpart (ie on selected frame(s)).\n",
    ""
   ]
  },
  "glassman89_eurospeech": {
   "authors": [
    [
     "Martin S.",
     "Glassman"
    ],
    [
     "Mary Beth",
     "Starkey"
    ]
   ],
   "title": "Minimal consonant pair discrimination for speech therapy using an expanded feature set and pattern element selection in time and frequency",
   "original": "e89_2273",
   "page_count": 4,
   "order": 260,
   "p1": "2273",
   "pn": "2276",
   "abstract": [
    "Systems for speech training have concentrated almost exclusively on presenting analysis of stationary speech parameters (formants, vocal tract shape, pitch, etc.) in visual form, or giving feedback based on vowel or utterance recognition [1]. Consonants are, however, usually given more attention during therapy. This is primarily due to their greater complexity (increased articulator agility is required to produce consonants) and their importance for intelligibility. We have developed a system based on dynamic selection from a variety of feature generation options, and a detailed selection of regions over time and frequency using an error rate criterion which allows the system to focus on aspects of speech patterns that are most effective for phoneme pair discrimination. Tests with a set of 14 minimal consonant pairs resulted in discrimination error rates of 0.4% for this system vs. 2.7% for a knn classifier and 5.7% for a neural network trained with backpropagation. Prototype application with a version of this algorithm utilizing only the expanded feature set indicates clinically useful performance with head injury and stroke patients.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-236"
  },
  "kobayashi89b_eurospeech": {
   "authors": [
    [
     "Tetsunori",
     "Kobayashi"
    ],
    [
     "Toshiyuki",
     "Matsuda"
    ],
    [
     "Kazuhiro",
     "Watanabe"
    ]
   ],
   "title": "Contextual factor analysis of vowel distribution",
   "original": "e89_2277",
   "page_count": 4,
   "order": 261,
   "p1": "2277",
   "pn": "2280",
   "abstract": [
    "A new categorical factor analysis method is proposed and it is applied to investigate the influence of phonemic context on the distribution of vowel spectra. Conventional categorical factor analysis methods, such as quantification theory, assume that the influence of multiple categories are expressed by the linear combination of that of single category. In our method, the theory is modified to deal with nonlinear factor. Using this method, the multiple correlation coefficients are improved by 3.6 - 7.8 % as compared with the conventional linear method and some nonlinear categorical factors which affect the vowel distribution became apparent.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-237"
  },
  "boe89_eurospeech": {
   "authors": [
    [
     "Louis-Jean",
     "Boe"
    ],
    [
     "Pascal",
     "Perrier"
    ],
    [
     "Bernard",
     "Guérin"
    ],
    [
     "Jean-Luc",
     "Schwartz"
    ]
   ],
   "title": "Maximal vowel space",
   "original": "e89_2281",
   "page_count": 4,
   "order": 262,
   "p1": "2281",
   "pn": "2284",
   "abstract": [
    "A vowel sound can be characterized acoustically in terms of its formant pattern, and generally by the first three formants. However, all combinations of F1, F2, F3 are not produced by human speakers. Vowel space is contained in what we call the Maximal Vowel Space (MVS), which is well known and even analytically approximated in the F1-F2 plane. The aim of this work is to determine acoustic and articulatory respective contributions to characteristics related to the MVS. From a 4-tube model, we precise intrinsic acoustic phenomena which limit the F1-F2 realization domain ; then, in order to bring out the specific effects of articulatory constraints, we explored the MVS with the help of two models which integrate, in their way, certain fundamental knowledge of speech production. Differences between F1-F2 maximal spaces generated in these three cases are acoustically discussed and explained in relation to articulatory constraints and knowledge of vowel production. Finally, a tridimensional description of the MVS and a parameterization in the F1-F2 plane as a function of F3? is proposed with the help of a model that best reflects articulatory reality.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-238"
  },
  "bergem89_eurospeech": {
   "authors": [
    [
     "Dick R. van",
     "Bergem"
    ],
    [
     "Florien J.",
     "Koopmans-van Beinum"
    ]
   ],
   "title": "Vowel reduction in natural speech",
   "original": "e89_2285",
   "page_count": 4,
   "order": 263,
   "p1": "2285",
   "pn": "2288",
   "abstract": [
    "In this paper a model of vowel reduction is introduced that emphasizes the role of the listener in a natural speech situation. A listener has such powerful strategies at his disposal at phonological, grammatical, semantic and pragmatic levels to restore missing acoustic Information, that a talker is not obliged to articulate every phoneme at its target position. This will particularly be the case in a spontaneous speech situation; in a more formal speech style the talker will increase his articulatory effort which will result in less reduced vowels. Since the listener has more need for content words than for function words, which do not contribute much to the meaning of the message, a talker will consequently put more articulatory effort in content words resulting in less reduced vowels. These views were confirmed by the present investigation in which speech characteristics of free conversation were compared with those of the same sentences but now read. The clear relationship between vowel duration and spectral undershoot suggested by Lindblom [152] is not supported in this investigation.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-239"
  },
  "iivonen89_eurospeech": {
   "authors": [
    [
     "Antti",
     "Iivonen"
    ],
    [
     "Raima",
     "Toivonen"
    ]
   ],
   "title": "Simulation of the psycho-acoustical vowel space for linguistic applications",
   "original": "e89_2289",
   "page_count": 4,
   "order": 264,
   "p1": "2289",
   "pn": "2292",
   "abstract": [
    "Despite of the the remarks concerning the perceptual formant integration and some other factors, it is assumed that an F1/F2-plot based on Bark-scale, in which the vowel points can be plotted according to a continuous scale, is capable to show in the most cases the psycho-acoustical vowel differences concerning the features front/hack, close/open, and rounded/ unrounded (with some consideration also tense/lax). About 5 0 vowel symbols have been used for the description of the vowel systems of the world. About the same number of 1 Bark sized circles have room on the physiologically possible F1/F2-space, This number seems to be near the general F1/F2 vowel resolution. Different scales of the F1/F2-plot for the psycho-acoustical simulation, exemplified by German vowel material, are compared with each other: full logarithmic (A), linear up to 510 Hz, logarithmic above 510 Hz (B); technical mel-scale (C), F1/F2- plot (D).\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-240"
  },
  "gosy89_eurospeech": {
   "authors": [
    [
     "Maria",
     "Gosy"
    ]
   ],
   "title": "Identification of synthesized Hungarian vowels with two vs. five formants",
   "original": "e89_2293",
   "page_count": 3,
   "order": 265,
   "p1": "2293",
   "pn": "2295",
   "abstract": [
    "Although the first two formants are assumed to define vowel quality, the exact number of formants necessary for identification in the case of children and adults seem to be debated. The experiments reported here compare perceptual judgments on two- vs. five-formant vowels. Four- and six-year old children, and adults took part in the listening experiments. The results support the claim that the property detectors work in terms of vowel quality, according to the phoneme inventory concerned. The hypothesis about the different operations of the property detectors during and after speech acquisition has also been confirmed.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-241"
  },
  "ganguli89_eurospeech": {
   "authors": [
    [
     "N. R.",
     "Ganguli"
    ]
   ],
   "title": "Acoustic phonetic study of bengali nasals",
   "original": "e89_2296",
   "page_count": 4,
   "order": 266,
   "p1": "2296",
   "pn": "2299",
   "abstract": [
    "This paper presents the results of the investigations on the formant frequencies and durations of nasal murmurs in different vowel context. Spectrographic analysis of 150 multisyllabic Bengali words spoken by three male and three female informants constitute the data base for this study.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-242"
  },
  "watson89b_eurospeech": {
   "authors": [
    [
     "Gordon S.",
     "Watson"
    ]
   ],
   "title": "APS: an environment for acoustic phonetic research",
   "original": "e89_2300",
   "page_count": 4,
   "order": 267,
   "p1": "2300",
   "pn": "2303",
   "abstract": [
    "Investigation into the relationship between phonetic segments and their acoustic values demands complex data analysis. This paper describes a software environment called aps designed to aid this task, aps provides a means of indexing segments of continuous speech in a general way. These segments may be obtained from a hand-produced phonemic transcription of the speech according to a phonemic context specified by the user. Alternatively, segments may be generated on the basis of acoustic values produced by a variety of signal processing operations. Functions are available that measure the acoustic values within segments. Such operations are commonly performed on thousands of speech segments simultaneously, aps has therefore been integrated with S, a comprehensive data analysis environment.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-243"
  },
  "chafcouloff89_eurospeech": {
   "authors": [
    [
     "Michel",
     "Chafcouloff"
    ],
    [
     "Alain",
     "Marchal"
    ],
    [
     "Thami",
     "Benkirane"
    ]
   ],
   "title": "Coarticulatory patterns in stop sequences",
   "original": "e89_2304",
   "page_count": 4,
   "order": 268,
   "p1": "2304",
   "pn": "2307",
   "abstract": [
    "An acoustic Investigation was undertaken to specify the coarticulatory influence of vowels across consonant clusters. The results show that: 1/ Vowel formants are subject to noticeable variations as a function of the vowel's quality and consonant's place of articulation. 2/ Transition frequency onsets and shapes incur important modifications in relation to the consonant's distinctive articulatory region. A tentative explanation based on an acoustical-physiological mapping is proposed.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-244"
  },
  "fourcin89_eurospeech": {
   "authors": [
    [
     "Adrian J.",
     "Fourcin"
    ]
   ],
   "title": "Progress overview for the SAM project",
   "original": "e89_2308",
   "page_count": 1,
   "order": 269,
   "p1": "2308",
   "pn": "",
   "abstract": [
    "The multi-lingual ESPRIT Speech Assessment Methodology Project (2589) started in March 1989. Eight countries collaborate in the work; six from the European community, two from EFTA.\n",
    "This contribution is concerned to provide a first overview of the practical achievements arising from the European collaboration made possible by the coherent adoption of standard sets of protocols, methods, hardware and software in the field of speech assessment methodology. Detailed reports of particular aspects of the work are presented at the conference by members of the SAM consortium.\n",
    ""
   ]
  },
  "garofolo89_eurospeech": {
   "authors": [
    [
     "John S.",
     "Garofolo"
    ],
    [
     "David S.",
     "Pallett"
    ]
   ],
   "title": "Use of CD-ROM for speech database storage and exchange",
   "original": "e89_2309",
   "page_count": 4,
   "order": 270,
   "p1": "2309",
   "pn": "2312",
   "abstract": [
    "Speech databases have become important resources for the speech research community. Experience with the use of magnetic media for exchange of these databases has proven to be unsatisfactory. The use of CD-ROM media for speech database storage and exchange is attractive for a number of reasons. At the National Institute of Standards and Technology (NIST), we have produced a prototype CD-ROM version of the DARPA TIMIT Acoustic- Phonetic Speech Database. This paper discusses issues involved in producing this disc and future CD-ROMs, including CD-ROM standards and portability, directory and filename organization, speech file headers, and speech data formats.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-245"
  },
  "kingham89_eurospeech": {
   "authors": [
    [
     "L. O.",
     "Kingham"
    ]
   ],
   "title": "Criteria in the design of experiments for speech recognition performance assessment",
   "original": "e89_2313",
   "page_count": 3,
   "order": 271,
   "p1": "2313",
   "pn": "2315",
   "abstract": [
    "All manufacturers of speech recognition equipment claim near perfect recognition performance for their products, of 98% correct recognition or better. Few manufacturers put their performance rates into context, by describing the circumstances in which these measurements were made. In practise, performance rates in excess of 95% are achieved with very small vocabularies of dissimilar words, in ideal environments with consistent speakers.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-246"
  },
  "steeneken89_eurospeech": {
   "authors": [
    [
     "Herman J. M.",
     "Steeneken"
    ],
    [
     "Jeroen. G. van",
     "Velden"
    ]
   ],
   "title": "RAMOS - recognizer assessment by means of manipulation of speech",
   "original": "e89_2316",
   "page_count": 4,
   "order": 272,
   "p1": "2316",
   "pn": "2319",
   "abstract": [
    "A method is proposed to specify the performance of a recognizer as a function of the variation of specific speech parameters and environmental conditions. The method uses a small test vocabulary with minimal difference wordsets of CVC-type words. Three wordsets are available including all Dutch initial consonants, vowels, and final consonants, respectively. This allows all possible confusions. By means of an analysis-resynthesis method, the testwords will be physically manipulated according to the changes of human speech in defined conditions. In an earlier paper (6), the results of a pilot study based on recognition experiments with this method were presented. In this paper the evaluation of the test vocabulary and the analysis of representative speech tokens are considered. The speech tokens include: male and female speech, inter- and intra-speaker variation and speech recorded just after a physical stress.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-247"
  },
  "crowe89_eurospeech": {
   "authors": [
    [
     "D. P.",
     "Crowe"
    ]
   ],
   "title": "Selection of voice codec for the aeronautical satellite service",
   "original": "e89_2320",
   "page_count": 4,
   "order": 273,
   "p1": "2320",
   "pn": "2323",
   "abstract": [
    "The Airlines Electronic Engineering Committee (AEEC), of the USA, was formed several years ago to promote the standardisation of communication equipment for the airlines industry. Recently the committee selected a 9.6 kbit/s coding standard for the Aeronautical Satellite Service (ASS). British Telecom Research Laboratories (BTRL), on behalf of the AEEC, acted as a processing laboratory with responsibility for the organisation and control of an international subjective test designed to select one of four candidate codecs. The test results show the subjective quality, in terms of Mean Opinion Score (MOS), attainable from Low-Rate-Encoders (LREs) operating at a bit-rate of 9.6 kbit/s [2].\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-248"
  },
  "bourjot89_eurospeech": {
   "authors": [
    [
     "C.",
     "Bourjot"
    ],
    [
     "A.",
     "Boyer"
    ],
    [
     "D.",
     "Fohr"
    ]
   ],
   "title": "Phonetic decoder assessment",
   "original": "e89_2457",
   "page_count": 4,
   "order": 274,
   "p1": "2457",
   "pn": "2460",
   "abstract": [
    "This paper describes a methodology to perform automatically a standard assessment of acoustic phonetic decoders. Results such as confusion matrix, deletion or insertion matrices are computed to provide a global overview of the system assessment. This method relies on the dynamic programming algorithm. It matches the outputs of the decoder with a standard transcription of the spoken sentence and its most common utterances derived with phonological rules. The errors made by the assesser are computed as errors of the decoder. In order to minimize this drawback, we guide the matching process by running an adaptation of the assessment algorithm to the decoder. The assessment of APHODEX, the acoustic phonetic decoder developed in CRIN, is performed and results are discussed.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-249"
  },
  "mcinnes89_eurospeech": {
   "authors": [
    [
     "Fergus R.",
     "McInnes"
    ],
    [
     "Y.",
     "Ariki"
    ],
    [
     "Alan A.",
     "Wrench"
    ]
   ],
   "title": "Enhancement and optimisation of a speech recognition front end based on hidden Markov models",
   "original": "e89_2461",
   "page_count": 4,
   "order": 275,
   "p1": "2461",
   "pn": "2464",
   "abstract": [
    "A method for performance evaluation of the acoustic-phonetic front end of a continuous speech recognition system, using the entropy of its output, is described. Results are given for a front end based on phonemic hidden Markov models, with various optional enhancements which have been optimised using the entropy criterion.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-250"
  },
  "marcus89_eurospeech": {
   "authors": [
    [
     "Jeffrey N.",
     "Marcus"
    ]
   ],
   "title": "Significance tests for comparing speech recognizer performance using small test sets",
   "original": "e89_2465",
   "page_count": 4,
   "order": 276,
   "p1": "2465",
   "pn": "2468",
   "abstract": [
    "Speech recognition researchers must often compare the performance of two recognizers or classifiers. In particular, they must assess the statistical significance of any differences exhibited in the performance of two algorithms. Gillick and Cox [1] have addressed this issue a-nd proposed using McNemar's test to compare recognizer performance. This test is based on the classification decisions made by each recognizer. In this paper, we propose several significance tests based on the confidence level the recognizer as well. In particular, we consider paired versions of the sign test, the signed-rank test and the t-test using as statistics the log probability assigned to the correct class and the difference in log probabilities between the correct class and the highest-scoring class besides the correct class. We show that for a phoneme recognition task involving over 2000 fricatives excised from continuous speech, the retention of this information yields a larger probability of obtaining statistically signficant results than does McNemar's test for a test set of a given size.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-251"
  },
  "vernooij89_eurospeech": {
   "authors": [
    [
     "G. J.",
     "Vernooij"
    ],
    [
     "Gerrit",
     "Bloothooft"
    ],
    [
     "Y. van",
     "Holsteijn"
    ]
   ],
   "title": "Simulation of isolated word recognition on the basis of a hierarchy of phonetic classes",
   "original": "e89_2469",
   "page_count": 4,
   "order": 277,
   "p1": "2469",
   "pn": "2472",
   "abstract": [
    "We have investigated the benefits of an interaction between lexical knowledge and acoustic analysis in an isolated word recognizer. By using lexical knowledge, the need for detailed acoustic analysis can be reduced and higher recognition rates can be obtained. In our proposal the lexical knowledge we use is determined by hierarchies of phonetic classes. These hierarchies have the form of binary decision trees and are used to guide the recognition process. A description of the proposed recognition process is given.\n",
    "By means of a simulation of this recognition process we are able to predict word recognition and confusion rates for the recognition system. Results of this simulation for different Dutch lexica are given.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-252"
  },
  "pitrelli89_eurospeech": {
   "authors": [
    [
     "John F.",
     "Pitrelli"
    ],
    [
     "Victor W.",
     "Zue"
    ]
   ],
   "title": "A hierarchical model for phoneme duration in american English",
   "original": "e89_2324",
   "page_count": 4,
   "order": 278,
   "p1": "2324",
   "pn": "2327",
   "abstract": [
    "We are developing a model which predicts phoneme duration as a function of segmental and suprasegmental factors, with the objective of using it for speech recognition. Our goal is to account for the many duration effects, ranging from local phonetic to sentence-level, and to determine how accurately we can model segment durations for sentences drawn from a large database spoken by many speakers. Our approach is to develop a hierarchical structure of categorical distinctions based on discrete-valued variables representing attributes of a phoneme and its context. We choose this technique over additive or multiplicative models because duration effects often interact in a complex manner. In our procedure, two descendents of a parent node can be split using different variables, thus allowing us to model non-uniform interactions among factors. When tested on 630 sentences from 126 speakers not used for training, our models explain 60% of vowel duration variance and 55% of the consonant duration variance within manner classes, yielding a root-mean-square prediction error of approximately 31 ms for vowels and 26 ms for consonants.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-253"
  },
  "carlson89c_eurospeech": {
   "authors": [
    [
     "Rolf",
     "Carlson"
    ],
    [
     "Björn",
     "Granström"
    ]
   ],
   "title": "Modelling duration for different text materials",
   "original": "e89_2328",
   "page_count": 4,
   "order": 279,
   "p1": "2328",
   "pn": "2332",
   "abstract": [
    "Rules for segmental duration has been studied in the context of a speech database that is under development in our department. The database search procedures include the same kind of context sensitive rules that are used in our speech synthesis project. This gives us the possibility to make a direct comparison to the database durations when developing durational rules for a text-to-speech system. Different kinds of speech material have been studied, including a novel and read sentences. Some different descriptive frameworks have been tried. A modified version of a rule structure suggested by Klatt has proven to be especially useful.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-254"
  },
  "farnetani89_eurospeech": {
   "authors": [
    [
     "Edda",
     "Farnetani"
    ]
   ],
   "title": "Acoustic correlates of linguistic boundaries in Italian: a study on duration and fundamental frequency",
   "original": "e89_2332",
   "page_count": 4,
   "order": 280,
   "p1": "2332",
   "pn": "2335",
   "abstract": [
    "The paper concerns the acoustic realization of some selected boundary types in Italian. Scope of the paper is to investigate whether the concept of boundary magnitude is applicable to Italian and to study the relationship between duration and F0 as boundary signals. The data on three subjects indicate that within-phrase boundaries are usually not signalled, while for between-phrase and between-clause boundaries both the temporal pattern (the duration of the temporal interval between onset of the domain-final syllable and onset of the domain-initial syllable) and the type of the F0 contour are clear indexes of the degree of cohesion/separation between the units. The present data seem to suggest a priority of the function of F0 as a boundary signal over its function as a stress correlate.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-255"
  },
  "alanani89_eurospeech": {
   "authors": [
    [
     "M.I.",
     "Al-Anani"
    ]
   ],
   "title": "Durational differences between emphatic and non emphatic syllables",
   "original": "e89_2336",
   "page_count": 4,
   "order": 281,
   "p1": "2336",
   "pn": "2339",
   "abstract": [
    "The primary objective of this study was to investigate the degree of influence of Arabic duration on the production of English when English is learned as a second language. The specific questions raised were: I. What is the average Arabic syllable duration in \"emphatic\" and \"non-emphatic\" contexts. Is length variation a function of the adjecent consonant. II. Is duration of English syllables as produced by Jordanian students rule-governed i.e. having the same acoustic features as those of L1.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-256"
  },
  "zarkadis89_eurospeech": {
   "authors": [
    [
     "D. J.",
     "Zarkadis"
    ],
    [
     "B. G.",
     "Evans"
    ]
   ],
   "title": "Coding of the LPC spectral parameters using vector predictive quantization",
   "original": "e89_2340",
   "page_count": 4,
   "order": 282,
   "p1": "2340",
   "pn": "2343",
   "abstract": [
    "The LPC-type speech coders require efficient quantization and coding of the LPC spectral parameters in order to operate at low bit-rates. We present here results from an investigation to the Vector Predictive Quantization (VPQ) approach for the compression of the LPC spectral information. Reduction of the computation requirements of the VPQ scheme by means of Split-Vector Quantization is considered. The distortion-rate characteristic of the coding scheme incorporating the Split-Vector Quantizer is estimated, based on log-spectral distortion measures. Our results indicate that this coding scheme can operate at 20 to 22 bits/frame without any perceptually significant distortions in the synthesized speech.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-257"
  },
  "wong89_eurospeech": {
   "authors": [
    [
     "W. T. K.",
     "Wong"
    ],
    [
     "I.",
     "Boyd"
    ]
   ],
   "title": "Optimal quantization performance of LPC parameters for speech coding",
   "original": "e89_2344",
   "page_count": 4,
   "order": 283,
   "p1": "2344",
   "pn": "2347",
   "abstract": [
    "For most LPC-based low bit rate speech coders, the LPC coefficients must be transmitted. To minimize the distortion in encoding the LPC coefficients, both reflection coefficients and line spectral pair (LSP) coefficients have been used. It has been claimed that, for the same degree of distortion, encoding the LSP coefficients requires about 25% less bits than the encoding of the reflection coefficients. However, these comparisons were not carried out when both these coefficient sets were optimally quantized. It is intended that this paper provides such a comparison, with the quantizers and bit allocations both optimally derived using the same performance criterion. Our comparison shows that optimal scalar quantization of reflection and LSP coefficients produces approximately the same distortion. This paper also introduces a differential LSP quantization scheme which out-performs the more conventional LPC quantization schemes.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-258"
  },
  "hunt89_eurospeech": {
   "authors": [
    [
     "Melvyn J.",
     "Hunt"
    ],
    [
     "Dariusz A.",
     "Zwierzyriski"
    ],
    [
     "Raymond C.",
     "Can"
    ]
   ],
   "title": "Issues in high quality LPC analysis and synthesis",
   "original": "e89_2348",
   "page_count": 4,
   "order": 284,
   "p1": "2348",
   "pn": "2351",
   "abstract": [
    "This paper deals with careful non-real-time LPC analysis. A baseline system is first described. It uses a pitch-synchronous covariance-method analysis with a laryngograph signal providing the pitch synchrony. Work to improve the voicing decision and F0 determination and to find a better voiced excitation waveform is described. Setting a lower limit on the value of B-, is found to be useful. Buzziness in the synthesis of voiced fricatives can be reduced by adding white noise to the excitation, and regions where this should be done can be automatically detected using three parameters: total power, the ratio of high-frequency power to total power, and B^. The location of the analysis frame for covariance-method analysis is found to be unimportant. A modified autocorrelation method with a carefully placed, Manning-windowed, pitch-synchronous analysis frame is found to give results that are as good as or -better than the covariance method. Finally, a method of resynthesizing speech using a modified LPC residual is described. The method allows prosody and formant parameters to be manipulated with minima! degradation.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-259"
  },
  "omologo89_eurospeech": {
   "authors": [
    [
     "Maurizio",
     "Omologo"
    ]
   ],
   "title": "The computation and some spectral considerations on line spectrum pairs (LSP)",
   "original": "e89_2352",
   "page_count": 4,
   "order": 285,
   "p1": "2352",
   "pn": "2355",
   "abstract": [
    "In this paper two aspects of Line Spectrum Pair (LSP) representation of speech are investigated: the method of converting Linear Predictive Coding (LPC) coefficients to LSP frequencies and the relationships between LPC spectrum and LSP frequencies. A new approach to LSP extraction is proposed which exploits the interframe redundancy of LSP frequencies in order to ensure, on the average, a low computational complexity. Some spectral properties of the LSP frequencies are then described. Starting from these properties, an LSP-based distance measure is introduced which approximates the rms log spectral measure and can be easily evaluated in parallel with the extraction procedure, in order to obtain a preliminary segmentation of speech.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-260"
  },
  "liu89_eurospeech": {
   "authors": [
    [
     "T. M.",
     "Liu"
    ],
    [
     "Harald",
     "Hoege"
    ]
   ],
   "title": "Phonetically-based LPC vector quantization of high quality speech",
   "original": "e89_2356",
   "page_count": 4,
   "order": 286,
   "p1": "2356",
   "pn": "2359",
   "abstract": [
    "In this paper, we present a phonetically-based LPC vector quantization (VQ) method for high quality speech. Our objective is to quantize the LPC parameters at a lowest possible bit rate with no noticeable difference in listening tests when synthetic speech is obtained by exciting the quantized LPC synthesis filter with the unquantized excitation signal, derived from inverse filtering. The proposed scheme uses speaker independent speech classification to adapt corresponding VQ's. The classifier divides the speech into 7 phonetical categories. The LPC parameters of the corresponding categories are quantized by dedicated VQ's. In our experiments, listening tests are made to determine the just-unnoticeable-difference bit number for each category. Since the results show that some categories do not require so many bits as others, we further classify the 7 categories into 4 groups. The conclusion is that the synthetic speech is negligibly different from the original when the bits per frame are 14, and that a total of 18 bits per frame can be sufficient without introducing any perceptible quantization noise.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-261"
  },
  "rao89_eurospeech": {
   "authors": [
    [
     "G. V. Ramana",
     "Rao"
    ],
    [
     "M.",
     "Prakash"
    ],
    [
     "B.",
     "Yegnanarayana"
    ]
   ],
   "title": "Word boundary hypothesisation in hindi speech",
   "original": "e89_2360",
   "page_count": 4,
   "order": 287,
   "p1": "2360",
   "pn": "2363",
   "abstract": [
    "In this paper we report the work done on the hypothesisation of word boundaries in Hindi speech. Firstly, the language related clues useful in word boundary hypothesisation are identified. These clues are then grouped into two classes (i) clues describing conditions to be satisfied at word boundaries, called lexical clues and (ii) clues that consist of frequently occurring patterns in text, called pattern clues. Two measures, frequency and correctness, are used to evaluate these clues. The results led to the organization of these clues into hypothesisation and verification clues. Experiments with the word boundary hypothesiser on a 300 sentence text yielded two results: (i) upto 50% of word boundaries in the text are hypothesised and (ii) considerable speed improvement in lexical search is achieved due to the hypothesisation of these boundaries.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-262"
  },
  "perennou89b_eurospeech": {
   "authors": [
    [
     "Guy",
     "Perennou"
    ],
    [
     "M. De",
     "Calmes"
    ],
    [
     "I.",
     "Ferrane"
    ],
    [
     "J.",
     "Tihoni"
    ]
   ],
   "title": "Automated phonotypical transcription through the GEPH phonology expert-system",
   "original": "e89_2364",
   "page_count": 4,
   "order": 288,
   "p1": "2364",
   "pn": "2367",
   "abstract": [
    "Within the scope of the Man-Machine Communication PRC (Programme de Recherches Concertees) Speech topic, we have developed the GEPH System, which is meant to enhance BDLEX, the lexical database of both spoken and written French. GEPH is supposed to derive the permissible pronunciations of a given statement, from its underlying representation, as this is secured from both analysing the statement for syntax and consulting BDLEX for a phonological representation of words. Within the scope of the ESPRIT SAM Project (Speech Assesment Methodologies), the semi-automation of the annotation tasks, applied to speech corpora, involves as a sub-task an automatic phonotypical transcription. In the context of an expert system in phonology, this makes the problem easier for only one typical pronunciation is supposed to be derived. Furthermore, this pronunciation is to remain abstract and will not require applying low-level rules. In our paper, we intend to give the configuration of the automatic transcription system worked out from GEPH and made to operate on compatible IBM-PC -the system is written in C language.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-263"
  },
  "williams89b_eurospeech": {
   "authors": [
    [
     "B.",
     "Williams"
    ],
    [
     "H.",
     "Thompson"
    ]
   ],
   "title": "Modelling phonological processes in continuous speech recognition",
   "original": "e89_2368",
   "page_count": 4,
   "order": 289,
   "p1": "2368",
   "pn": "2371",
   "abstract": [
    "The assimilation and deletion of consonants at word boundaries poses a problem for the continuous recognition of English speech. This is because the location of word boundaries in the input is not known, Two methods for accounting for these phonological processes are discussed. The offline approach increases the size and complexity of the lexicon, while the online approach generates possible but non-occurring words. A third type of algorithm, Finite State Transducers (FST's), is shown to combine the advantages of these two approaches while avoiding their drawbacks. The results of implementing and testing FST's are given. Consideration is given to the use of FST's in accounting for similar consonantal phonological processes within words.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-264"
  },
  "cutler89_eurospeech": {
   "authors": [
    [
     "Anne",
     "Cutler"
    ],
    [
     "Sally",
     "Butterfield"
    ]
   ],
   "title": "Natural speech cues to word segmentation under difficult listening conditions",
   "original": "e89_2372",
   "page_count": 4,
   "order": 290,
   "p1": "2372",
   "pn": "2375",
   "abstract": [
    "One of a listener's major tasks in understanding continuous speech is segmenting the speech signal into separate words. When listening conditions are difficult, speakers can help listeners by deliberately speaking more clearly. In three experiments, we examined how word boundaries are produced in deliberately clear speech. We found that speakers do indeed attempt to mark word boundaries; moreover, they differentiate between word boundaries in a way which suggests they are sensitive to listener needs. Application of heuristic segmentation strategies makes word boundaries before strong syllables easiest for listeners to perceive; but under difficult listening conditions speakers pay more attention to marking word boundaries before weak syllables, i.e. they mark those boundaries which are otherwise particularly hard to perceive.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-265"
  },
  "lacheretdujour89_eurospeech": {
   "authors": [
    [
     "Anne",
     "Lacheret-Dujour"
    ]
   ],
   "title": "Automatic generation of phonological variations",
   "original": "e89_2376",
   "page_count": 4,
   "order": 291,
   "p1": "2376",
   "pn": "2379",
   "abstract": [
    "A recognition system must include structures which account for the different aspects of possible phonological variability in the incoming speech signal. To aid in dealing with variability at the phonological level, a grapheme-to-several-phoneme strings module, VARION0, has been developed at LIMSI. Tests of this system showed the necessity of creating a hierarchical structure to order phonological variations, according to a fixed set of criteria, eliminating infrequent phenomena, thus bringing the number of possible strings down to a reasonable size. The resulting new module, VARION1, was tested on transciptions of recorded speech (BDSONS-30 speakers and a variable speech rate database).\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-266"
  },
  "chen89b_eurospeech": {
   "authors": [
    [
     "Xixian",
     "Chen"
    ],
    [
     "Changnian",
     "Cai"
    ]
   ],
   "title": "A continuous VQ clustering algorithm for realtime speech recognition",
   "original": "e89_2380",
   "page_count": 4,
   "order": 292,
   "p1": "2380",
   "pn": "2383",
   "abstract": [
    "This paper presents a continuous VQ clustering (CVQC) algorithm for realtime speech recognition, which incorporates the temporal information of speech into both training and recognition processes. In comparison with the conventional DTW and VQ methods, this new algorithm delivers faster training and recognition speed and smaller codebook size while still retains merits of both. Realtime implementation is emphasized in the design of sophisticated algorithms. And a custom available voice controlled computer command input system based on CVQC is also introduced.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-267"
  },
  "bowles89_eurospeech": {
   "authors": [
    [
     "R. L.",
     "Bowles"
    ],
    [
     "Robert I.",
     "Damper"
    ]
   ],
   "title": "Application of the dempster-shafer theory of evidence to improved-accuracy isolated-word recognition",
   "original": "e89_2384",
   "page_count": 4,
   "order": 293,
   "p1": "2384",
   "pn": "2388",
   "abstract": [
    "This paper describes experiments in which the outputs of three isolated-word recognition algorithms were combined to yield a lower average error rate than that achieved by any individual algorithm. The input tokens were simulated, fixed-length spectral speech patterns subjected to additive noise and either a \"high\" or \"low\" degree of time distortion. The three recognition techniques used were dynamic time warping, hidden Markov modelling and a multilayer perceptron. Two combination techniques were employed: the formula derived from the Dempster-Shafer (D-S) theory of evidence and simple majority voting (MV). D-S performed significantly better than MV under both time-distortion conditions. Evidence is also presented that the assumption of independent word scores, which is necessary for D-S theory to be strictly applicable, is questionable.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-268"
  },
  "shillcock89_eurospeech": {
   "authors": [
    [
     "R. C.",
     "Shillcock"
    ]
   ],
   "title": "Competitor effects in auditory word recognition: implications for interactive-activation models of word recognition",
   "original": "e89_2388",
   "page_count": 4,
   "order": 294,
   "p1": "2388",
   "pn": "2391",
   "abstract": [
    "The TRACE model of word recognition predicts the activation of \"words within words\" (tar in guitar). Simulations with TRACE are reported which demonstrate that the level of this activation is affected both by the size of the set of competing word hypotheses (a \"prefixing effect\") and by the presence of a high-frequency competitor. A cross-modal priming study is described which supports and extends these predictions.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-269"
  },
  "maire89_eurospeech": {
   "authors": [
    [
     "V. Le",
     "Maire"
    ],
    [
     "Régine",
     "Andre-Obrecht"
    ],
    [
     "D.",
     "Jouvet"
    ]
   ],
   "title": "An acoustic-phonetic decoder an automatic segmentation algorithm",
   "original": "e89_2392",
   "page_count": 4,
   "order": 295,
   "p1": "2392",
   "pn": "2395",
   "abstract": [
    "This paper describes the introduction of an automatic segmentation algorithm as a pre-processing of a speaker-independent digit recognition system based on hidden Markov models ; so an acoustic analysis is performed over each segment. The observation sequence is composed of the sequence of the provided vectors of acoustic coefficients. The used Markov modelling reposes on a description in two levels each digit is translated in terms of basic units (allophone, phoneme, pseudo-diphone), and each basic unit is developped in terms of acoustic segments. The recognition performances obtained for the allophone based model with different acoustic analysis lead to prove that the segmental analysis doesn't lose essential information in respect with the sliding-block analysis; the comparision between the articulatory interpretation of the segmentation and the best path found by the Viterbi algorithm in the pseudo-diphone based model validates our qualitative analysis of the acoustic segments.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-270"
  },
  "hamada89b_eurospeech": {
   "authors": [
    [
     "Hiroshi",
     "Hamada"
    ],
    [
     "Tatsuya",
     "Hirahara"
    ],
    [
     "Akihiro",
     "Imamura"
    ],
    [
     "Tatsuo",
     "Matsuoka"
    ],
    [
     "Ryohei",
     "Nakatsu"
    ]
   ],
   "title": "Auditory-based filter-bank analysis as a front-end processor for speech recognition",
   "original": "e89_2396",
   "page_count": 4,
   "order": 296,
   "p1": "2396",
   "pn": "2399",
   "abstract": [
    "A comparison of speech analysis based on human auditory processing and conventional LPC analysis is described. A comparison was made of the capabilities of these two types of parameters to recognize fourteen consonants extracted from Japanese consonant-vowel (CV) syllables spoken in isolation. Tree types of recognition algorithms were used: Dynamic time-warping with multiple template sets, hidden Markov models, and neural networks. The auditory system consisted of 35 channels, spanning from 100 to 5400 Hz, each of which consisted of a critical bandpass filtering process, a rectification process, an integration process, and a transformation into logarithmic form. A lateral inhibition process was also included in order to more closely simulate human auditory processing. The recognition experiments showed that parameters based on the features of human auditory processing are excellent for use in various types of speech recognition methods.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-271"
  },
  "komatsu89_eurospeech": {
   "authors": [
    [
     "Akio",
     "Komatsu"
    ],
    [
     "Eiji",
     "Oohira"
    ],
    [
     "Akira",
     "Ichikawa"
    ]
   ],
   "title": "Prosodical sentence structure inference for natural conversational speech understanding",
   "original": "e89_2400",
   "page_count": 4,
   "order": 297,
   "p1": "2400",
   "pn": "2403",
   "abstract": [
    "In order to develop a system capable of understanding natural conversational speech, along with the current developments in technology for phonetic information processing, a technology must be developed that will utilize prosodic information of natural speech.\n",
    "Me propose here an algorithm for generating a parsing tree that represents for the semantical relationships between phrases, based on analysis of fundamental frequency contour patterns. The feasibility and validity of our algorithm are confirmed by computer simulation experiments. The capability of prosodical sentence structure inference is confirmed using natural Japanese speech. Also, it is confirmed that our algorithm is speaker independent and task independent. Furthermore, it is confirmed that the algorithm is applicable to another language, by experiments using English speech.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-272"
  },
  "ye89_eurospeech": {
   "authors": [
    [
     "H.",
     "Ye"
    ],
    [
     "M. J.",
     "Caraty"
    ],
    [
     "Louis-Jean",
     "Boe"
    ],
    [
     "D.",
     "Tuffelli"
    ]
   ],
   "title": "Structural (phonetic) evaluation of dissimilarities functions used in speech recognition",
   "original": "e89_2404",
   "page_count": 4,
   "order": 298,
   "p1": "2404",
   "pn": "2407",
   "abstract": [
    "We have evaluated 17 variants of 6 dissimilarities: PLOMP, Log Likelihood Ratio, Cepstrum, Mel Frequency Cepstrum Coefficients, Weighted Slope Metric, and Spectral Peaks Adjustment derived from FFT and/or LPC analysis with two types of integration (KLATT and ZWICKER). We used as \"references\" synthetic and natural vocalic stimuli for which we have a phonetic structural representation.\n",
    "The intervocalic dissimilarities were used as input for a multidimensional analysis (KRUSKAL) to obtain an output space that we compared with the acoustic one from the data. The appraisal of the 2 spaces - the first one corresponding to F1-F2, derived from acoustic analysis and the second one rebuilt from dissimilarities as input of the Multidimensional Scaling KRUSKAL - allows us to compare dissimilarities and to make an extrinsic (phonetic) judgment on their behavior. We have used 5 criteria based on the capability of these processings to deliver vocalic dissimilarities that could be interpreted in terms of phonetic description (acoustic representation). This comparative evaluation of dissimilarities can guide better choices regarding their application to automatic recognition and also in the domain of phonetic analysis, including perceptual simulation.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-273"
  },
  "marino89b_eurospeech": {
   "authors": [
    [
     "Jose B.",
     "Marino"
    ],
    [
     "Enric",
     "Monte"
    ]
   ],
   "title": "Generation of multiple hypothesis in connected phonetic-unit recognition by a modified one-stage dynamic programming algorithm",
   "original": "e89_2408",
   "page_count": 4,
   "order": 299,
   "p1": "2408",
   "pn": "2411",
   "abstract": [
    "One of the most popular algorithms for connected word (or subword phonetic unit) recognition is the one-stage dynamic programming algorithm. In its available formulation, this algorithm is not designed to provide multiple hypothesis; such a limitation is currently becoming a drawback, since the need of alternative recognitions in the systems presently under research is being acknowledged. This paper introduces a modified version of the one-stage dynamic programming algorithm tailored to afford multiple hypothesis.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-274"
  },
  "castelli89_eurospeech": {
   "authors": [
    [
     "Eric",
     "Castelli"
    ],
    [
     "Pascal",
     "Perrier"
    ],
    [
     "Pierre",
     "Badin"
    ]
   ],
   "title": "Acoustic considerations upon the low nasal formant based on nasopharyngeal tract transfer function measurements",
   "original": "e89_2412",
   "page_count": 4,
   "order": 300,
   "p1": "2412",
   "pn": "2415",
   "abstract": [
    "To understand acoustic phenomena involved in the production of nasal vowels, we have built an experimental setting to obtain vocal tract transfer functions. The vocal tract is excited through the skin of the glottis by a white noise.\n",
    "We have particulary studied the first nasal formant Fn1 situated around 250 Hz. It seems to agree with FENG'S predictions: Fn1 is a HELMHOLTZ resonance. But, because the wall vibration effects, the resonator can not be the simple nasopharyngeal tract. Namely the volume of this cavity should be greater than 800 cm3 ! Could an important role of the cranial cavity in the production of the nasal vowels and the nasal velar consonnant be then considered?\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-275"
  },
  "delattre89_eurospeech": {
   "authors": [
    [
     "C.",
     "Delattre"
    ],
    [
     "M.",
     "Jomaa"
    ],
    [
     "C.",
     "Worley"
    ],
    [
     "Christian",
     "Abry"
    ]
   ],
   "title": "The phasing of the jaw in consonant and vowel lengthening Arabic and French patterns",
   "original": "e89_2416",
   "page_count": 4,
   "order": 301,
   "p1": "2416",
   "pn": "2419",
   "abstract": [
    "The main aim of this study is to examine cross-linguistically, on the acoustic and articulatory levels, across two rate conditions (conversational and fast), the control of vowel-consonant timing in two different lengthening tasks: the so-called vocalic length and consonantal gemination.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-276"
  },
  "recasens89_eurospeech": {
   "authors": [
    [
     "Daniel",
     "Recasens"
    ]
   ],
   "title": "Control mechanisms of tongue dorsum activity in speech production",
   "original": "e89_2420",
   "page_count": 4,
   "order": 302,
   "p1": "2420",
   "pn": "2423",
   "abstract": [
    "This paper shows that relevant information about the mechanisms of tongue dorsum control may be obtained through a measure of acoustic and articulatory variability. The data reported here indicate that coartlculatory effects, for tongue dorsum activity decrease with an increase in the degree of mediopalatal constriction, an increase In the number of vocal tract constrictions, and an increase in the duration of the temporal lag between two successive gestures in time.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-277"
  },
  "elhalees89_eurospeech": {
   "authors": [
    [
     "Yousef",
     "El-Halees"
    ]
   ],
   "title": "A study of subglottal pressure for emphatic and non-emphatic sounds in Arabic",
   "original": "e89_2424",
   "page_count": 1,
   "order": 303,
   "p1": "2424",
   "pn": "",
   "abstract": [
    "The Phonetic feature of \"emphasis\" in Arabic involves (1) a narrowing in the back cavity caused by retracting and lowering the rest of the tongue and the epiglottis and an inward movement of the lateral walls of the pharynx, (2) a widening in the front cavity caused by lowering the body of the tongue in the mouth. Acoustically this results in F1 raising and F2 lowering; F0 has been found to be lower for the emphatic items than for the non-emphatic counterparts.\n",
    ""
   ]
  },
  "strik89_eurospeech": {
   "authors": [
    [
     "Helmer",
     "Strik"
    ],
    [
     "Louis",
     "Boves"
    ]
   ],
   "title": "The fundamental frequency - subglottal pressure ratio",
   "original": "e89_2425",
   "page_count": 4,
   "order": 304,
   "p1": "2425",
   "pn": "2428",
   "abstract": [
    "It is known that subglottal pressure (Psb) is a major factor in the control of fundamental frequency (F0) in speech. Yet, the details of this relation remain unclear. Estimates of the F0 to Psb ratio (FPR) from speech and special phonation tasks yield values between 5 and 15 Hz/cmH2O [1,2,3,4]. In another type of experiments pressure variations are induced externally, either subglottally or supraglottally. The FPR's measured in these experiments tend towards values of 2-5 Hz/cmH2O [5,6,7,8]. There seems to be no a priori reason for the FPR to be different in both kinds of experiments. After all, the voice source is the same and why should it behave differently during both kinds of phonation tasks? Therefore we carried out experiments that aimed at resolving this discrepancy.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-278"
  },
  "farnetani89b_eurospeech": {
   "authors": [
    [
     "Edda",
     "Farnetani"
    ],
    [
     "William",
     "Hardcastle"
    ],
    [
     "Alain",
     "Marchal"
    ]
   ],
   "title": "Cross-language investigation of lingual coarticulatory processes using EPQ",
   "original": "e89_2429",
   "page_count": 4,
   "order": 305,
   "p1": "2429",
   "pn": "2432",
   "abstract": [
    "Lingual coarticulatory effects on intervocalic stop consonants in French, Italian and English were studied with EPG. Nonsense words of the type /bV'CV/ (where C = /t,k/ and /V/ =/i,a/ were repeated five times by three native speakers in carrier sentences, and the coarticulatory influences of the vowel on C were studied at the onset of closure and at the release. A numerical index of lingual coarticulation was proposed and the results showed some similarities and differences between the languages, e.g. English showed more tendency towards carryover coarticulation than either French or Italian.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-279"
  },
  "autesserre89_eurospeech": {
   "authors": [
    [
     "Denis",
     "Autesserre"
    ],
    [
     "Yukihiro",
     "Nishinuma"
    ],
    [
     "Isabelle",
     "Guaitella"
    ]
   ],
   "title": "Breathing, pausing, and speaking in dialogue",
   "original": "e89_2433",
   "page_count": 4,
   "order": 306,
   "p1": "2433",
   "pn": "2436",
   "abstract": [
    "The respiratory aspects of the speech of two conversing subjects were studied. The data showed that (1) the breath regulating mechanism is independent of speech (phonation and pausing); (2) total dialogue duration can be broken down into: 25% breathing in, 25% phonation, and 50% pausing, although the absolute durations vary by subject; (3) breathing in takes less than one second, while expiration lasts four times as long; (4) \"silent\" pausing covers several physiological events: final expiration, breathing in, and prephonation pauses; (5) interlocutors generally speak when their partner is pausing and listener/speaker speech overlapping is minimal.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-280"
  },
  "autesserre89b_eurospeech": {
   "authors": [
    [
     "Denis",
     "Autesserre"
    ],
    [
     "Benoit",
     "Galindo"
    ],
    [
     "Bernard",
     "Teston"
    ],
    [
     "Nadine",
     "Vigouroux"
    ]
   ],
   "title": "Movement of the lips and velum in speech: variations in aerodynamic parameters",
   "original": "e89_2437",
   "page_count": 4,
   "order": 307,
   "p1": "2437",
   "pn": "2440",
   "abstract": [
    "This comparative study of the coordination of lip and velum movements during the articulation of the nasal consonant (m) in a symmetric vocalic context in French, and of the aerodynamic parameters associated with these movements, has provided evidence of the functioning modes of the velum and their dependence upon the nature of the vowel an its location with respect to the nasal consonant,. During the articulation of the vowel preceding the nasal consonant, the lowering of the velum is first accompanied by a decrease in the volume of air in the nasal passages. The subsequent increase in nasal air flow only occurs by the end of the preceding vowel and during the emission of the nasal consonant itself. However, the raising of the velum for the next vowel occurs in two stages: an initial fast rise during the first phase of the vowel, then a much slower rise during the final portion. This research sheds new light on our understanding of the relationships between the trajectories of lip and velum movements and the acoustic events that are associated with them.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-281"
  },
  "maeda89_eurospeech": {
   "authors": [
    [
     "Shinji",
     "Maeda"
    ]
   ],
   "title": "Compensatory articulation in speech: analysis of x-ray data with an articulatory model",
   "original": "e89_2441",
   "page_count": 4,
   "order": 308,
   "p1": "2441",
   "pn": "2445",
   "abstract": [
    "Roughly 1000 frames of cineradiographic and labiofilm data on the vocal tract corresponding to 10 French sentences uttered by two speakers have been analyzed statistically. The analysis resulted in an articulatory model consists of a limited number of linear components. With this model the temporal variations of the vocal-tract shapes are described by the frame-to-frame samples of the articulatory parameters. We observe that \"target\" parameter values for the same vowel vary significantly pre- sumably due to different phonetic contexts. An acoustic calculation with the model predicts that a particular pair of articulators can compensate acoustically each other. For example, by an appropriate adjustment of the tongue-dorsal position, the model is capable of producing the same F1-F2 pattern for different jaw position, or vice-versa. The compensation between the jaw and the dorsal positions, however, is possible only for unrounded vowels. In the case of rounded vowels, the jaw position can be compensated by the lip aperture. The measured \"target\" values of the paired parameters indicated a linear relationship, suggesting that the speakers actually exploit the inter-articulator compensation in the speech production. This explains the observed large \"target\" value variability. The comparison of parameter trajectories for the same sentences uttered by the two speakers indicates more similitude than difference, suggesting mat the manner of the production involving the compensatory articulation could be relatively invariant.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-282"
  },
  "zerling89_eurospeech": {
   "authors": [
    [
     "Jean-Pierre",
     "Zerling"
    ]
   ],
   "title": "The three degrees of labialisation of the French steady-state vowels a study for 105 speakers",
   "original": "e89_2445",
   "page_count": 4,
   "order": 309,
   "p1": "2445",
   "pn": "2448",
   "abstract": [
    "We study frontal lip-opening shape for the 14 steady-state French vowels pronounced by 105 native speakers: A few wellknown common facts are first confirmed. Interestingly, data reveal that although French vocalic labiality is described in terms of a binary phonological feature (+/-round), articulatory processes show 3 degrees of labialization, i.e. non-rounded [-lab], rounded [+lab] and hyperrounded [++lab], Splitting of the traditional labialized category does not seem to have been considered before. Observation of the shape-factor variations (K2=A/B) shows that either a flat or a rounded frontal lip opening may be indifferently used for any class. Further, relating labial data to x-ray sagittal view of the vocal-tract leads to interesting remarks about compensatory behaviour in vowel production.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-283"
  },
  "barney89_eurospeech": {
   "authors": [
    [
     "Anna M.",
     "Barney"
    ],
    [
     "Christine H.",
     "Shadle"
    ],
    [
     "David W.",
     "Thomas"
    ]
   ],
   "title": "An investigation of air flow through the larynx by computer and mechanical modelling",
   "original": "e89_2449",
   "page_count": 4,
   "order": 310,
   "p1": "2449",
   "pn": "2452",
   "abstract": [
    "A computer model of the larynx has been developed, based on the two-mass model [1] but including some of the more recently observed flow phenomena in the glottis. In order to test the validity of the output of the computer model and the assumptions on which it is based, a mechanical model has been constructed in which the vocal folds are modelled by vibrating shutters. Measurements have shown that the pressure-flow relations for the model, when the shutters are stationary, are comparable with published data for other static models.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-284"
  },
  "caldognetto89_eurospeech": {
   "authors": [
    [
     "E. Magno",
     "Caldognetto"
    ],
    [
     "K.",
     "Vagges"
    ],
    [
     "N. A.",
     "Borghese"
    ],
    [
     "G.",
     "Ferrigno"
    ]
   ],
   "title": "Automatic analysis of lips and jaw kinematics in VCV sequences",
   "original": "e89_2453",
   "page_count": 4,
   "order": 311,
   "p1": "2453",
   "pn": "2456",
   "abstract": [
    "A new system for automatic jaw and lips movement 3D analysis in speech production is presented. The system (ELITE) uses small, non obtrusive, passive markers of .5mm of diameter attached on the speaking subject in six repere points. Two TV cameras survey the scene and the markers are detected by means of a cross-correlation algorithm implemented by a real-time dedicated hardware at a sampling rate of 50Hz. The 3D coordinates of the markers are automatically computed with a standard deviation error of .lmm, by a generalized stereophotogrammetric algorithm which allows a free camera positioning and requires a short time for set-up. The results of preliminary studies on lips rounding movement in coarticulation are presented.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-285"
  },
  "ferretti89b_eurospeech": {
   "authors": [
    [
     "Marco",
     "Ferretti"
    ],
    [
     "Giulio",
     "Maltese"
    ],
    [
     "Stefano",
     "Scarci"
    ]
   ],
   "title": "Measures of language model and acoustic model information in probabilistic speech recognition",
   "original": "e89_2473",
   "page_count": 4,
   "order": 312,
   "p1": "2473",
   "pn": "2476",
   "abstract": [
    "To predict the performance of a probabilistic speech recognizer it is often desirable to estimate the contribution of the language model and that of the acoustic model. We describe an approach to this problem which tries to take into account the interaction between the two sources of information. Some results are presented concerning the 20000-word vocabulary, real-time IBM recognizer of the Italian language.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-286"
  },
  "huber89_eurospeech": {
   "authors": [
    [
     "Dieter",
     "Huber"
    ]
   ],
   "title": "Voice characteristics of female speech and their representation in computer speech synthesis and recognition",
   "original": "e89_2477",
   "page_count": 4,
   "order": 313,
   "p1": "2477",
   "pn": "2480",
   "abstract": [
    "This paper presents a comparative study of female versus male voice characteristics in read speech, both in a global (intonation) and local (laryngealization) perspective. Swedish newspaper texts comprising a total of 2610 running words were read by two female and two male speakers of Standard Swedish. The two women produced consistently more intonation units per text, with on the average higher F0 onsets and offsets, shorter durations, steeper falls, a larger proportion of rising versus falling declination lines, and a markedly stronger tendency to time-align their intonation units with features of syntactic structure in the subsentence domain. Also, both female speakers made significantly more extensive and more varied use of laryngealization as a boundary marker than their male counterparts. The various voice parameters are described in quantitative terms with respect to their acoustical characteristics. Correlations and distributional properties are established in probabilistic terms for use in computer speech applications.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-287"
  },
  "schoentgen89_eurospeech": {
   "authors": [
    [
     "Jean",
     "Schoentgen"
    ]
   ],
   "title": "The spectral dynamics of a non-linear model of the glottal waveform",
   "original": "e89_2481",
   "page_count": 4,
   "order": 314,
   "p1": "2481",
   "pn": "2484",
   "abstract": [
    "The spectral dynamics of a non-linear model of the voice source signal was examined in two experiments. Controlled changes in the output spectrum were brought about by varying the amplitude of the model's excitation signal. A first experiment illustrated the model's ability to output continuously evolving spectra during an increase or decrease in time of the excitation function amplitude. In a second experiment, a comparison was carried out between the harmonic values of glottis cycles obtained by inverse filtering on the one hand, and the harmonics of synthesized glottis signals on the other; the identification of the parameters of the model was carried out on the basis of the same experimentally obtained cycles. Results show a very good agreement between real and synthetic voice source signals. In a second stage, we tested the ability of the model of a glottis cycle to approach the spectrum of other glottis signals produced by a same speaker under different conditions; here, the only control parameter was the amplitude of the model's excitation function. Results show that synthesized spectra manage to approximate the overall spectral trend, though without being able to reproduce individual spectral components exactly.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-288"
  },
  "howard89b_eurospeech": {
   "authors": [
    [
     "David M.",
     "Howard"
    ],
    [
     "Graham F.",
     "Welch"
    ]
   ],
   "title": "Visual feedback applied to the learning of conscious pitch control in singing",
   "original": "e89_2485",
   "page_count": 4,
   "order": 315,
   "p1": "2485",
   "pn": "2488",
   "abstract": [
    "Recent research suggests that singing ability can be characterised by certain stages along a continuum of ability. Development of singing ability along this continuum can be readily assisted by the use of appropriate visual feedback. This paper describes a system working on a BBC microcomputer which enables note pitching accuracy to be developed and assessed. It incorporates a specially developed hardware interface which estimates voice fundamental period in real-time without output smoothing. Whilst it is intended that the system be used with any age group, a recent pilot study has concentrated on classes of primary school children and the results obtained support its use for the development of conscious pitch control. The findings have implications for other areas of voice development in which vocal pitch is significant.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-289"
  },
  "harmegnies89b_eurospeech": {
   "authors": [
    [
     "Bernard",
     "Harmegnies"
    ],
    [
     "Marielle",
     "Bruyninckx"
    ],
    [
     "Joaquim",
     "Llisteri"
    ],
    [
     "Dolors",
     "Poch"
    ]
   ],
   "title": "Effects of language change on voice quality. an experimental contribution to the study of the catalan-castilian case",
   "original": "e89_2489",
   "page_count": 4,
   "order": 316,
   "p1": "2489",
   "pn": "2492",
   "abstract": [
    "Various experiments have dealt with the idea that voice quality could be influenced by the speaker's language. Most authors in this field have tried to objectivate voice quality by means of long term average spectra (LTAS). It has been shown [1] that their results - although apparently contradictory - reveal that part of the LTAS variability can be explained by language variations. Recently, a powerful method allowing an accurate evaluation of those effects has been developed [2] and applied to Dutch and French. The purpose of the present work is to replicate this study with a new pair of languages: Catalan and Spanish (Castilian). 10 bilingual speakers have produced, each, 4 readings of 2 (1 Castilian and 1 Catalan) balanced texts. A 400-line LTAS was computed for each utterance. The LTAS are compared by means of the SDDD dissimilarity index. The results confirm previous findings, i.e., the inter-language variability is greater than the intra-language variabilities.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-290"
  },
  "nord89_eurospeech": {
   "authors": [
    [
     "Lennart",
     "Nord"
    ],
    [
     "Britta",
     "Hammarberg"
    ]
   ],
   "title": "Analysis of laryngectomee speech - a progress report",
   "original": "e89_2493",
   "page_count": 4,
   "order": 317,
   "p1": "2493",
   "pn": "2496",
   "abstract": [
    "This paper is a progress report, discussing methods and preliminary results of acoustic analysis of laryngectomee speech. We have investigated three types of alaryngeal voices: esophageal speech, tracheo-esophageal speech and electrolaryngeal speech and made comparisons with normal laryngeal speech. Our aim is to obtain an acoustic objective evaluation of the different speaking methods and to relate perceptual dimensions to acoustic measurements. Prosodic features, such as pitch, intensity, duration, as well as voice quality features, including long time average spectral shape and aspects of spectral energy distributions were measured. Among the results it was found that the alaryngeal voices were characterized by weak level of the fundamental compared to normal laryngeal voices. Other, more detailed voice source characteristics, such as inverse filtered flow registrations were analysed and revealed strong irregularities for the alaryngeal voices.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-291"
  },
  "trancoso89_eurospeech": {
   "authors": [
    [
     "Isabel M.",
     "Trancoso"
    ],
    [
     "C. M.",
     "Ribeiro"
    ]
   ],
   "title": "Adaptive and stochastic search procedures in CELP based coders",
   "original": "e89_2497",
   "page_count": 4,
   "order": 318,
   "p1": "2497",
   "pn": "2500",
   "abstract": [
    "The excitation to the LPC filter in CELP based coders includes two components: the contribution of the stochastic codebook and the contribution of the pitch or long-term predictor, which can be derived from an adaptive codebook in which the codewords are delayed versions of the past excitation signal. This paper is devoted to a systematic study of codebook related issues in CELP based coders: open and closed-loop schemes for determining the optimum adaptive and stochastic codewords, dimension and overlap between analysis frames, codebook population, etc. We shall cover some of the most well known search procedures and introduce two new variants that significantly reduce the computational complexity of the stochastic search procedure, without quality degradation: the truncated autocorrelation approach and the unity-magnitude approach.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-292"
  },
  "lee89c_eurospeech": {
   "authors": [
    [
     "K. Y.",
     "Lee"
    ],
    [
     "B. G.",
     "Evans"
    ]
   ],
   "title": "Combined optimization of excitation and filter parameters in analysis-by-synthesis coders",
   "original": "e89_2501",
   "page_count": 4,
   "order": 319,
   "p1": "2501",
   "pn": "2504",
   "abstract": [
    "Analysis-by-Synthesis LPC (ABS-LPC) based speech cod- ing schemes have been very successful at producing good quality speech below 8Kbit/s. In this paper we report on work to further enhance on the capabilities of ABS-LPC schemes, particularly CELP. Specifically, our attention has been focused on reoptimising the time-varying filter of ABS-LPC schemes by introducing a sequential iterative estimation procedure which extracts the best excitation and filter combination. Also, by adopting a pole-zero filter in place of the all-pole LPC filter a more general approach is introduced which gives better coverage of the speech variations. This new coder, ARMA-CELP, was found to give good objective and subjective results when compared with a standard CELP coder.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-293"
  },
  "masgraugomez89_eurospeech": {
   "authors": [
    [
     "E.",
     "Masgrau-Gomez"
    ],
    [
     "J. A.",
     "Rodriguez-Fonollosa"
    ],
    [
     "A.",
     "Moreno-Bilbao"
    ]
   ],
   "title": "Including zeros in the backward adaptive predictor of AVPC coders",
   "original": "e89_2505",
   "page_count": 4,
   "order": 320,
   "p1": "2505",
   "pn": "2508",
   "abstract": [
    "The inclusion of a pole-zero vector predictor in the backward loop of an AVPC coder[1] is approached . We hope to reach the same profit shown by the pole-zero predictor in the ADPCM-CCITT standard. By using a VLMS transversal algorithm the obtained behaviour does not outperform the all-pole scheme. We argue it is due to the dependence on the eigenvalues spread of the LMS perfomance. By using an orthogonalized-lattice structure this problem can be overcame. The zeros term shows an aditional advantage consisting in the removal of the artificial correlation introduced by the pole term of the predictor. This leads to a better profit of the VQ ability and a good coding perfomance of the whole system.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-294"
  },
  "marques89b_eurospeech": {
   "authors": [
    [
     "J. S.",
     "Marques"
    ],
    [
     "Jose M.",
     "Tribolet"
    ],
    [
     "Isabel M.",
     "Trancoso"
    ],
    [
     "Luis B.",
     "Almeida"
    ]
   ],
   "title": "Pitch prediction with fractional delays in CELP coding",
   "original": "e89_2509",
   "page_count": 4,
   "order": 321,
   "p1": "2509",
   "pn": "2512",
   "abstract": [
    "Time-domain coders use long-term predictors to exploit the quasi-periodic structure of voiced speech. However, one major drawback with classical predictors is the restriction of the delay to integer values, reducing the performance especially when the pitch period is short. This paper presents an extension of the prediction technique to encompass non-integer delays enabling more accurate representation of quasi-periodic segments and an increased flexibility in the design of long-term predictors. The use of long-term predictors with fractional delays in CELP coding produces an enhancement of the harmonic structure in high frequencies and an improvement of quality for female speakers.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-295"
  },
  "holmes89_eurospeech": {
   "authors": [
    [
     "Wendy J.",
     "Holmes"
    ]
   ],
   "title": "Copy synthesis of female speech using the JSRU parallel formant synthesiser",
   "original": "e89_2513",
   "page_count": 4,
   "order": 322,
   "p1": "2513",
   "pn": "2516",
   "abstract": [
    "This paper presents work using a new method of copy synthesis for the JSRU parallel formant synthesiser, to obtain good quality synthesis of both female and male speech. Natural speech was analysed to derive values for each of the synthesiser control parameters at regular (10 ms) intervals. The formant frequencies were obtained using excitation synchronous formant analysis and labelling techniques previously reported [10,11]. The amplitude control parameters were derived by a two-stage process: first the spectral amplitudes at the formant frequencies were measured from FFT analyses, and then the measured amplitudes were transformed using a table of \"amplitude correction values\" (ACVs) to obtain amplitudes which are appropriate as control parameters. Some modifications made to the synthesiser to improve the production of female speech are also described. The results have demonstrated that it is possible to obtain good quality female speech from the JSRU synthesiser.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-296"
  },
  "barbe89_eurospeech": {
   "authors": [
    [
     "S.",
     "Barbe"
    ],
    [
     "Rolf",
     "Carlson"
    ],
    [
     "Piero",
     "Cosi"
    ],
    [
     "Maria Gabriella Di",
     "Benedetto"
    ],
    [
     "Björn",
     "Granström"
    ],
    [
     "K.",
     "Vagges"
    ]
   ],
   "title": "A rule-based Italian text-to-speech system",
   "original": "e89_2517",
   "page_count": 4,
   "order": 323,
   "p1": "2517",
   "pn": "2520",
   "abstract": [
    "An Italian text-to-speech system based on the INFOVOX architecture will be described- With respect to earlier versions of the system, the complete set of Italian vowels and diphthongs was taken into consideration. A set of specific rules for the assignment of word stress, entirely based on statistical considerations, as well as a complete set of grapheme-to-phoneme rules were implemented. Appropriate parameter definitions for the realisation of Italian phonemes were formulated and, on the basis of preliminary results on the study of Italian prosodic structure, appropriate phonetic changes were introduced. Finally, the results of perceptual experiments which were carried out in order to evaluate the system will be described.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-297"
  },
  "shi89_eurospeech": {
   "authors": [
    [
     "Bo",
     "Shi"
    ]
   ],
   "title": "A Chinese text-to-speech system",
   "original": "e89_2521",
   "page_count": 4,
   "order": 324,
   "p1": "2521",
   "pn": "2524",
   "abstract": [
    "This paper presents a Chinese text-to-speech system which aims to produce highly intelligible standard Chinese speech with natural sounding intonation from Pinyin text, using the LSI parallel formant speech synthesizer. Two specific features of the system are: the use of a demisyllable dictionary which permits an effective and reliable way of carrying out text to phonetic element conversion and the implementation of a flexible intonation framework which enables a very wide variety of natural sounding fundamental frequency (F0) contours to be generated automatically. Some evaluation data obtained from a syllable intelligibility test are provided.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-298"
  },
  "olaszy89_eurospeech": {
   "authors": [
    [
     "Gabor",
     "Olaszy"
    ]
   ],
   "title": "MULTIVOX - a flexible text-to-speech system for Hungarian, Finnish, German, esperanto, Italian and other languages for IBM-PC",
   "original": "e89_2525",
   "page_count": 4,
   "order": 325,
   "p1": "2525",
   "pn": "2528",
   "abstract": [
    "In the last decade intensive phonetic research has been going on in Hungary in the field of speech acoustics at the Phonetics Laboratory of Linguistics Institute of the Academy of Science. The succesful results from 1979-1981 formed the basis for the development of text-to-speech (TTS) synthesis techniques (1,2). A general system philosophy has been developed and used as a basis of text to speech conversion on many languages. The MULTIVOX system can be easily adopted to different languages. The conversion programs, data and rules for one language need a maximum of 32 kbytes. The adoptation process -depending on the new language- takes about 1-3 months. Besides the languages mentioned, the adoptation of Spanish and Portuguese is in progress. MULTIVOX is designed to use the Philips PCF 8200 formant synthesizer.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-299"
  },
  "romary89_eurospeech": {
   "authors": [
    [
     "Laurent",
     "Romary"
    ],
    [
     "Jean-Marie",
     "Pierrel"
    ]
   ],
   "title": "Should an oral dialogue system be modular?",
   "original": "e89_2569",
   "page_count": 4,
   "order": 326,
   "p1": "2569",
   "pn": "2572",
   "abstract": [
    "We discuss in this paper the problems bound to modular architectures, which lie at the root of most current speech understanding systems. We see why the use of different linguistic models brings about communication problems inside such systems and therefore, we detail the possible steps that could lead to an integration of this different types of knowledge. Finally, we propose a structure for a new architecture, illustrating its feasability through the study of temporal information in man-machine dialogues.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-300"
  },
  "bard89_eurospeech": {
   "authors": [
    [
     "Ellen G.",
     "Bard"
    ],
    [
     "A. J.",
     "Lowe"
    ],
    [
     "G. T. M.",
     "Altmann"
    ]
   ],
   "title": "The effect of repetition on words in recorded dictations",
   "original": "e89_2573",
   "page_count": 4,
   "order": 327,
   "p1": "2573",
   "pn": "2576",
   "abstract": [
    "Two experiments on words isolated from recorded dictations show that the effect of repetition on intelligibility depends on the discourse roles of the tokens being compared. When the later token adds no new information to the discourse, as in self-corrections and other coreferential repetitions, intelligibility falls with repetition. When the later token introduces a new discourse entity, intelligibility may rise with repetition. The results are discussed with reference to the effects of the speaker's experience in machine dictation and to the consequences f0 automatic speech recognition.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-301"
  },
  "luzzati89_eurospeech": {
   "authors": [
    [
     "D.",
     "Luzzati"
    ]
   ],
   "title": "A dynamic dialog model for human machine communication",
   "original": "e89_2577",
   "page_count": 4,
   "order": 328,
   "p1": "2577",
   "pn": "2580",
   "abstract": [
    "With vocal or written input, the real problem of human machine dialog deals with the dialog function itself. Taking into account analysis and recognition performance, we consider that the task of a dialog module is to avoid breaking off of communications as much as possible through the use of a dialog strategy. To this purpose, we describe a model using two axes and a set of variables to build a graph allowing any dialog state to be situated in a metric space at any time. Due to the model, the system may control all dialog phases to avoid some critical zones or, as a last resort, be award of their proximities.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-302"
  },
  "mcqueen89_eurospeech": {
   "authors": [
    [
     "James M.",
     "McQueen"
    ]
   ],
   "title": "The use of lexical knowledge in phonetic categorisation",
   "original": "e89_2581",
   "page_count": 4,
   "order": 329,
   "p1": "2581",
   "pn": "2584",
   "abstract": [
    "Lexical effects on phonetic categorisation have been taken as evidence that the listener's word knowledge influences phonetic processing during normal speech perception. The present study examined word-nonword effects in the categorisation of word-initial and word-final stop consonants. Natural speech was edited to produce bilabial, alveolar and velar voicing continue. The data revealed a significant word-nonword effect, such that subjects were more likely to categorise an ambiguous consonant as voiced if the voiced endpoint of the continuum was a word, but as unvoiced if the unvoiced endpoint of the continuum was a word. But it was found that within some blocks there was no evidence of this lexical shift. Subsequent experimental manipulations provided additional evidence that this shift is highly variable. The fact that the effect is nonmandatory provides support for the view that phonetic processing can occur independently of lexical processing.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-303"
  },
  "miles89_eurospeech": {
   "authors": [
    [
     "Christopher",
     "Miles"
    ],
    [
     "Dylan M.",
     "Jones"
    ],
    [
     "Annette",
     "Simpson"
    ]
   ],
   "title": "Human factors in speech synthesis: factors affecting friendliness and efficiency",
   "original": "e89_2585",
   "page_count": 4,
   "order": 330,
   "p1": "2585",
   "pn": "2588",
   "abstract": [
    "This study is concerned with the introduction of synthesized speech into the human - computer interface. A simulated hotel booking system is described which asks for information via either the screen or a DECtalk speech synthesizer. The degree of redundancy within the computer's response set is varied. The perceived 'friendliness' of the system increased with the use of synthesized speech incorporating redundancy. In addition, this type of output influenced the quality of the users' responses, such that they 'modelled' the computer's speech.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-304"
  },
  "frankish89_eurospeech": {
   "authors": [
    [
     "Clive R.",
     "Frankish"
    ]
   ],
   "title": "Conversations with computers: problems of feedback and error correction",
   "original": "e89_2589",
   "page_count": 4,
   "order": 331,
   "p1": "2589",
   "pn": "2592",
   "abstract": [
    "In a simple data entry task, feedback and error correction routines will allow users to detect misrecognitions and to re-enter incorrect data. Even when these resources are available, a small proportion of data strings are still entered incorrectly. Examination of these cases indicates that the source of these errors depends on the modality of feedback used. With visual feedback, users sometimes fail to monitor incorrect feedback. With spoken feedback, the problem is more likely to involve confusions within the task dialogue. The implications of these findings for the design of feedback and error correction procedures are discussed.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-305"
  },
  "kuijpers89_eurospeech": {
   "authors": [
    [
     "C. Th. L.",
     "Kuijpers"
    ]
   ],
   "title": "Strategies in developmental dialogue system",
   "original": "e89_2593",
   "page_count": 4,
   "order": 332,
   "p1": "2593",
   "pn": "2596",
   "abstract": [
    "This paper describes some aspects of the speech communication system in young children (2-3 years old). The imitative mechanism in adult-child interaction is considered to be one of the traceable processes in speech communication. Within this framework a methodology is developed for utterance classification that allows for describing the transitional stage from imitation into topic continuation. Within this transitional stage, theoretical implications of the function of imitation are related to several acoustic analyses. Some tentative conclusions for a strategy in language and speech acquisition can be deduced.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-306"
  },
  "hitzenberger89_eurospeech": {
   "authors": [
    [
     "Ludwig",
     "Hitzenberger"
    ],
    [
     "Huberta",
     "Kritzenberger"
    ]
   ],
   "title": "Simulation experiments and prototyping of user interfaces in a multimedial environment of an information system",
   "original": "e89_2597",
   "page_count": 4,
   "order": 333,
   "p1": "2597",
   "pn": "2600",
   "abstract": [
    "This paper describes some experiments on multimedial user interfaces within the domain of information systems. The factors tested are the input mode (spoken and written) and system quality (cooperativity, language restrictions) and the influence of the consciousness about the interlocutor's identity. The results show that there are some significant differences on the word level (type, token) and that the users reactions on machine performance are more obvious than on the interlocutor's identity.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-307"
  },
  "luzzati89b_eurospeech": {
   "authors": [
    [
     "D.",
     "Luzzati"
    ],
    [
     "Francoise",
     "Néel"
    ]
   ],
   "title": "Dialogue behaviour induced by the machine",
   "original": "e89_2601",
   "page_count": 4,
   "order": 334,
   "p1": "2601",
   "pn": "2604",
   "abstract": [
    "In order to design human-machine dialogue systems using speech input as well as written intput, it is necessary to study the influence of the machine not only on the user's lexicon and syntax, but also on his dialogue behaviour. Dialogue is not handled between a man and a machine in the same way as an interpersonal dialogue. This paper is based on the linguistic study of a corpus obtained by simulating a machine in an actual human-machine dialogue situation: it deals with a task-oriented dialogue recorded by a telephone information service of the French National Railway Company (SNCF), for train timetables (schedules). In the paper, we focus on the basic elements of dialogue behaviour induced by the machine, namely the respect of \"turns\" (the way one participant gives or takes control of the communication channel to or from the other): in the experiment carried out at the SNCF, naive users spontaneously accepted this constraint. We believe that such behaviour reflects a. hierarchically-structured dialogue which we formalize with a set of defined and reduced symbols.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-308"
  },
  "robert89_eurospeech": {
   "authors": [
    [
     "Jean-Marc",
     "Robert"
    ],
    [
     "Jean-Yves",
     "Fiset"
    ],
    [
     "Gilles",
     "Bergeron"
    ]
   ],
   "title": "Impact of five task-related factors on the choice of a vocal or a manual input modality",
   "original": "e89_2605",
   "page_count": 4,
   "order": 335,
   "p1": "2605",
   "pn": "2608",
   "abstract": [
    "167 engineering students evaluated through a questionnaire the usefulness of a vocal and of a manual input modality for computerized systems. Five factors, grouped in triplets, were systematically manipulated in a set of scenarios: the type of task, the mode of presentation of information to the users, the task difficulty, the task duration, and the task frequency. Each triplet included the first two factors with one of the last three factors. The main results show that in \"benign\" conditions where the task is either short, not frequent, or easy, the users perceive either input modality as equally useful. In \"stressful\" conditions where the task is either long, frequent, or difficult, two general reactions are observed: the users prefer a manual input modality in spatial tasks where the information is presented visually, and slightly prefer a vocal input modality for all the other situations.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-309"
  },
  "fohr89b_eurospeech": {
   "authors": [
    [
     "Dominique",
     "Fohr"
    ],
    [
     "Noelle",
     "Carbonell"
    ],
    [
     "Jean-Paul",
     "Haton"
    ]
   ],
   "title": "Phonetic decoding of continuous speech with the APHODEX expert system",
   "original": "e89_2609",
   "page_count": 4,
   "order": 336,
   "p1": "2609",
   "pn": "2613",
   "abstract": [
    "In order to increase the accuracy of continuous speech acoustic-phonetic decoding, we started the APHODEX project some years ago. Our aim is to develop an expert system that implements the knowledge of an expert spectrogram reader, the phonetician F. Lonchamp. In the present version of the system, procedural and declarative approaches have been used jointly for the representation of phonetic expertise relating to: the segmentation of speech into phonemelike units, the classification of segments into phonetic categories (coarse labeling) and subsequent interpretation in terms of phones (fine labeling). APHODEX is capable of processing hypotheses in parallel, using contextual analysis, fuzzy and uncertain reasoning, especially for the interpretation of cues and features extracted from the speech signal. Decoding results are represented in the form of a phone-lattice; the description of alternative segmentations and labelings is possible. A prototype version of APHODEX has been implemented, using the SNORRI speech processing environment. Results concerning a multi-speaker corpus of continuous speech are presented and discussed.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-310"
  },
  "altosaar89_eurospeech": {
   "authors": [
    [
     "Toomas",
     "Altosaar"
    ],
    [
     "Matti",
     "Karjalainen"
    ]
   ],
   "title": "A knowledge-based approach to unlimited vocabulary speech recognition for the Finnish language",
   "original": "e89_2613",
   "page_count": 4,
   "order": 337,
   "p1": "2613",
   "pn": "2616",
   "abstract": [
    "This paper describes a strategy for the development of a speech recognition system for Finnish that is based on several different knowledge sources which include auditory, phonetic, and linguistic details. Auditory knowledge is derived from the application of computational models which simulate the human peripheral hearing system. Phonetic knowledge is represented by rule-based analysis, parsing and classification of phonetically relevant units and structures from the output of the auditory model. Finally, linguistic knowledge is used to filter the word forms generated by the phonetic level by accepting or rejecting hypotheses through the use of morphological analysis. The main components of the system including the structure of the rules are presented.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-311"
  },
  "martelli89_eurospeech": {
   "authors": [
    [
     "T.",
     "Martelli"
    ]
   ],
   "title": "Modelling speech knowledges in a distributed complex object oriented architecture: REMORA",
   "original": "e89_2617",
   "page_count": 4,
   "order": 338,
   "p1": "2617",
   "pn": "2620",
   "abstract": [
    "This article presents the formal bases of our complex object oriented language, together with our distributed architecture, originally developed for a speech recognition application, generalized as a tool that can be used regardless its field of application to conceive evolutive multi-expert bases of complex knowledge.\n",
    "The originality in this system lies in the definition of a complex object oriented language CODEX (Complex Object Dynamic EXpert language). We have defined a complex object to represent a complex knowledge, that can be dynamically modified, is composed of parts and that eventually has temporal characteristics. This representation grants standardization, so that within the same system declarative and procedural knowledge derived from different experts can be brought into collaboration. Our system takes into account temporal aspects of reasoning and of knowledge representation. Standardization of knowledge and distributed control forecast an extension to our knowledge base system of Distributed Artificial Intelligence.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-312"
  },
  "blomberg89_eurospeech": {
   "authors": [
    [
     "Mats",
     "Blomberg"
    ]
   ],
   "title": "Voice source adaptation of synthetic phoneme spectra in speech recognition",
   "original": "e89_2621",
   "page_count": 4,
   "order": 339,
   "p1": "2621",
   "pn": "2624",
   "abstract": [
    "A recognition system based on a reference library of synthetic phoneme prototypes is described. The phoneme templates are specified in terms of formant synthesis parameters. The vocabulary and grammar is described in a finite-state network of phonemes. Each phoneme is divided into a number of new states representing transitions and steady-state regions. The parameters of the transition states are interpolated from the steady-state parameters. At each state, a 16-channel filter bank section is computed from the synthesis parameters. Dynamic adaptation to each speaker's individual voice source spectrum is performed during recognition. Without adaptation, the average recognition for ten male speakers was 88% on an isolated-word task using a 26-word vocabulary. Adding voice source adaptation raised the performance to 96%. On a vocabulary of 3 connected digits, the adaptation technique improved the recognition rate for six male speakers from 87.7% to 92.8%. The improvement was largest for subjects with low initial recognition rate, indicating the usefulness of the voice source adaptation technique for certain voices.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-313"
  },
  "meloni89_eurospeech": {
   "authors": [
    [
     "Henri",
     "Meloni"
    ],
    [
     "A.",
     "Betari"
    ],
    [
     "P.",
     "Gilles"
    ]
   ],
   "title": "A knowledge-based system for speaker-independent recognition of letters",
   "original": "e89_2625",
   "page_count": 4,
   "order": 340,
   "p1": "2625",
   "pn": "2628",
   "abstract": [
    "The major difficulties for speaker-independent recognition of letters in French, result from the acoustic likness of some words (\"M\" and \"N\", \"B\" and \"D\", \"F\" and \"S\", \"J\" and \"G\", etc.) and from the lack of informations to solve ambiguities. This particular vocabulary requires the encoding of the main informations taken into account for the acoustic and phonetic decoding of continuous speech. The techniques the more frequently used to build up such systems are based on pattern matching. There are various pattern-coding and pattern-matching methods (Vector Quantization, Hidden Markow Modelling, Neural Network, Dynamic Time Warping, etc.) [Aldelfeld 80], [Burton 85], [Jelinek 85], [Huang 88], [Bulot 89], but most of them do not use explicitly coded knowledge. We are proposing a system based on a set of acoustic, phonetic, phonologic and lexical knowledge, represented by rules in Prolog II [Meloni 86]. The informations represented are various and they allow the localization and the identification of acoustic and phonetic phenomena (patterns, grouping of patterns, events, properties, cues, features, phonemes, syllables, etc.). A score is associated to each phenomenon described when the latter is identified. Rules and control use these scores in many ways to assign valuations to complex phenomena and to sort the most probable hypotheses of recognition.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-314"
  },
  "hall89_eurospeech": {
   "authors": [
    [
     "M. C.",
     "Hall"
    ]
   ],
   "title": "Objective quality evaluation of parallel-formant synthesised speech",
   "original": "e89_2629",
   "page_count": 4,
   "order": 341,
   "p1": "2629",
   "pn": "2632",
   "abstract": [
    "High quality synthesised speech can be produced from the Holmes Parallel-Formant Synthesiser if the control parameters are obtained from analysis of natural speech. However, the quality of the modelling of individual voice characteristics can be very variable. This paper investigates the variability in speech encoded for a parallel-formant synthesiser through the application of objective distortion criteria developed for speech coders. Results are presented for the distortion calculated using the LPC cepstral distance measure.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-315"
  },
  "benoit89_eurospeech": {
   "authors": [
    [
     "Christian",
     "Benoît"
    ],
    [
     "A. van",
     "Erp"
    ],
    [
     "Martine",
     "Grice"
    ],
    [
     "Valerie",
     "Hazan"
    ],
    [
     "Ute",
     "Jekosch"
    ]
   ],
   "title": "Multilingual synthesiser assessment using semantically unpredictable sentences",
   "original": "e89_2633",
   "page_count": 4,
   "order": 342,
   "p1": "2633",
   "pn": "2636",
   "abstract": [
    "The use of semantically unpredictable sentences was investigated as part of a standard test battery for the evaluation of synthetic speech within the ESPRIT SAM project. Such sentences were randomly generated using five syntactic structures and word lists for each syntactic category. Initial results of experiments run in English, French and German are presented.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-316"
  },
  "pickering89_eurospeech": {
   "authors": [
    [
     "J. Brian",
     "Pickering"
    ]
   ],
   "title": "The effects of voice type and quality on the intelligibility of a text-to-speech system",
   "original": "e89_2637",
   "page_count": 3,
   "order": 343,
   "p1": "2637",
   "pn": "2639",
   "abstract": [
    "In developing a text-to-speech system, it is important to consider not only how intelligible the final output may be, but also how acceptable the synthetic voice is to the users of such a system. We might even go so far as to consider offering a range of different types of voice, some male, some female, to allow the user the flexibility they may require. It is not clear, however, just how different voices and different voice qualities may affect the intelligibility of the text-to-speech system. Nor indeed, is it clear whether generating different voice types might involve simple modification of the basic synthetic voice offered, or require the extraction of a whole new parameter set. This paper addresses three basic issues: first, can we generate a number of different voices by modifying a single voice, second, how acceptable do naive listeners judge those voices to be; and finally, what, if any, is the effect of changing the voice type on the intelligibility of the synthetic output.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-317"
  },
  "robert89b_eurospeech": {
   "authors": [
    [
     "Jean-Marc",
     "Robert"
    ],
    [
     "Andre",
     "Choiniere"
    ],
    [
     "Raymond",
     "Descout"
    ]
   ],
   "title": "Subjective evaluation of the naturalness and acceptability of three text-to-speech systems in French",
   "original": "e89_2640",
   "page_count": 4,
   "order": 344,
   "p1": "2640",
   "pn": "2643",
   "abstract": [
    "Fifty-three subjects evaluated the naturalness and acceptability of three text-to-speech systems in French which are Loquax, Televox, and Multivox. Their evaluation bore upon 25 5-point perceptual scales. The main results indicate that the performance profiles of the three systems are very similar to each other. They also reveal that the average evaluations of the three systems over the 25 scales are negative (Multivox: -0.14; Televox: -0.19, Loquax: -0.38) and do not appear to differ significantly from each other. An interesting result is that the oldest system, Loquax, received the largest number of -2 ratings from the subjects, whereas Multivox obtained the largest number of +2 ratings. Some research directions are proposed for the future.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-318"
  },
  "pavlovic89_eurospeech": {
   "authors": [
    [
     "Chaslav V.",
     "Pavlovic"
    ],
    [
     "Mario",
     "Rossi"
    ],
    [
     "Robert",
     "Espesser"
    ]
   ],
   "title": "Direct scaling of the performance of text-to-speech synthesis systems",
   "original": "e89_2644",
   "page_count": 4,
   "order": 345,
   "p1": "2644",
   "pn": "2647",
   "abstract": [
    "As text-to-speech systems develop it becomes necessary to compare various solutions and to evaluate whether a change in the synthesis procedure has an effect on the listener's attitude to the system. Because there are no physical measurements that result in indices that quantify perceptual attributes of synthesized speech, psychophysical tests need to be used. The present study assesses the effectiveness of measuring listeners' impressions of synthesized speech using a magnitude estimation task. In particular, this study focuses on acceptability (i.e. the overall users' satisfaction with the communication situation), intelligibility (i.e. how identifiable is the linguistic message), and naturalness (i.g. how much the system sounds like a normal human talker). The study consists of three experiments which are described further in the text. Depending on the experiment, four or seven synthesizers were evaluated in one or two types of external distortion (noise), as well as in quiet.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-319"
  },
  "son89_eurospeech": {
   "authors": [
    [
     "Rob J. J. H. van",
     "Son"
    ],
    [
     "Louis C. W.",
     "Pols"
    ]
   ],
   "title": "Comparing formant movements in fast and normal rate speech",
   "original": "e89_2665",
   "page_count": 4,
   "order": 346,
   "p1": "2665",
   "pn": "2668",
   "abstract": [
    "Standard theories about articulation predict an increase in coarticulation and reduction at increased speaking rates due to physiological constraints. We show that under near natural conditions, a professional speaker will produce fast spoken vowels that are only negligibly different from his normal rate spoken vowels. The correlation between vowel formant frequency and vowel duration is minimal in all parts of the vowel. We conclude that coarticuiation and reduction are not the result of physiological limitations of the speaker but are language governed features of speech.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-320"
  },
  "laprie89_eurospeech": {
   "authors": [
    [
     "Yves",
     "Laprie"
    ]
   ],
   "title": "Formant tracking adapted to acoustic-phonetic decoding",
   "original": "e89_2669",
   "page_count": 4,
   "order": 347,
   "p1": "2669",
   "pn": "2672",
   "abstract": [
    "We describe a novel algorithm for formant ex- traction and tracking to be used in the context of acoustic-phonetic decoding. The construction of formant tracks involves both image processing techniques and geometric reasoning, which constitutes the originality and efficiency of the method. The algorithm which works either on LPC or on cepstral data, comprises two distinct steps: - first, formant segments are extracted from the signal by means of local processing, - secondly geometric reasoning involving acoustic-phonetic knowledge is applied, in order to connect meaningful elementary tracks consistently and eliminate irrelevant segments.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-321"
  },
  "lin89_eurospeech": {
   "authors": [
    [
     "Qiguang",
     "Lin"
    ],
    [
     "Gunnar",
     "Fant"
    ]
   ],
   "title": "Vocal-tract area-function parameters from formant frequencies",
   "original": "e89_2673",
   "page_count": 4,
   "order": 348,
   "p1": "2673",
   "pn": "2676",
   "abstract": [
    "The basic three-parameter vocal tract area function model of Fant [2] has been extended to allow for asymmetry and variable length of the tongue hump section as well as variations of the VT overall length. An important class of articulations are those that conform with this model, mostly vowel sounds and consonants without apical modification or a secondary posterior articulation. Staring out from a theory of small perturbations [1,4], we calculate the shift in the VT parameter accompanying a certain set of shifts in F1, F2, and F3. This is done from a set of three linear differential equations expressing the shifts in each of F1, F2, and F3 as the sum of the contributions from the unknown shifts in each of the VT parameters. These are solved at each small step of a pathway from a reference starting point to the target formant-pattern. The overall length is optimized by reference to F4 and some available measured data. A resynthesis of a time varying F-pattern from a set of articulatory targets has been attempted with a possible application in articulatory synthesis.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-322"
  },
  "fink89_eurospeech": {
   "authors": [
    [
     "F. K.",
     "Fink"
    ],
    [
     "Paul",
     "Dalsgaard"
    ]
   ],
   "title": "Estimation of formants in noise corrupted speech using auditory models",
   "original": "e89_2677",
   "page_count": 4,
   "order": 349,
   "p1": "2677",
   "pn": "2680",
   "abstract": [
    "In many practical applications involving speech recognition it is of great importance to be able to handle noise suppression in the preprocessing stage. In this paper we describe different front-end processing systems, one based on speech production modelling and two based on auditory modelling, and present results on their formant estimation abilities when being excitated by speech signals contaminated by noise. Three noise types - car, cocktailparty and open-plan office noise - are added to speech signals at signal-to-noise ratios varying between 20 and -10 dB. The results show that preprocessing using auditory modelling is much more robust to noise than speech production modelling, and that formants can still be reliably estimated at SNR = -5 dB for speech signal contaminated by car noise.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-323"
  },
  "grenie89_eurospeech": {
   "authors": [
    [
     "M.",
     "Grenie"
    ],
    [
     "A.-S. Del",
     "Negro"
    ]
   ],
   "title": "Acoustic-phonetic analysis of speech produced under noise and various auditory feedback",
   "original": "e89_2681",
   "page_count": 1,
   "order": 350,
   "p1": "2681",
   "pn": "",
   "abstract": [
    "The aim of this study was to analyze some of the major acoustic-phonetic changes that occur in French when speech is produced under high levels of white noise. Acoustical analyses were made on a nineteen word vocabulary spoken by two male and one female speakers in a anechoic chamber with 75,85 and 95 dbA of white noise played at the level of the ears and with three levels of auditory feedback (loud, normal and high).\n",
    ""
   ]
  },
  "hawkins89_eurospeech": {
   "authors": [
    [
     "Sarah",
     "Hawkins"
    ]
   ],
   "title": "Reconciling trading relations and acoustic invariance",
   "original": "e89_2682",
   "page_count": 4,
   "order": 351,
   "p1": "2682",
   "pn": "2685",
   "abstract": [
    "An approach to modelling speech perception is proposed that integrates acoustic-phonetic and psycholinguists knowledge. The approach acknowledges the value of models of acoustic invariance and trading relations, usually considered to be in opposition to each other, by suggesting that both types of perception occur, but operate in different circumstances. It acknowledges the versatility of the human listener, and the robustness of the speech signal.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-324"
  },
  "mcallister89b_eurospeech": {
   "authors": [
    [
     "J. M.",
     "McAllister"
    ]
   ],
   "title": "The processing of stressed syllables in connected speech",
   "original": "e89_2686",
   "page_count": 4,
   "order": 352,
   "p1": "2686",
   "pn": "2689",
   "abstract": [
    "Various studies have shown that stressed syllables are more easily processed by human hearers than unstressed syllables. However, the superior intelligibility of stressed syllables may be attributable either to phonetic characteristics such as increased amplitude and duration, or to informational (phonological) factors such as the wider range of vowel types they permit. This paper presents results of a gating experiment which suggest that the latter explanation accounts for stressed syllables' greater intelligibility.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-325"
  },
  "nord89b_eurospeech": {
   "authors": [
    [
     "Lennart",
     "Nord"
    ],
    [
     "Anita",
     "Kruckenberg"
    ],
    [
     "Gunnar",
     "Fant"
    ]
   ],
   "title": "Some timing studies of prose, poetry and music",
   "original": "e89_2690",
   "page_count": 4,
   "order": 353,
   "p1": "2690",
   "pn": "2693",
   "abstract": [
    "Prosodic relations in prose, poetry and music are discussed with an emphasis on durational properties. In order to gain a deeper understanding of speech prosody, we are presently engaged in a comparison of the timing relations in such activities as the reading of poetry and music performance, where there usually is a strong and obvious rhythmic patterning of the produced sound sequences. Also there are interesting parallels to be drawn by comparing the formal notations of prose, poetry and music. Generally, there are no simple relations between abstract notations and performance, and moreover, notations have varied with tradition and particular needs. However, it is a challenge to tie descriptive systems closer to common human constraints in production and perception.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-326"
  },
  "nishinuma89b_eurospeech": {
   "authors": [
    [
     "Yukihiro",
     "Nishinuma"
    ],
    [
     "Danielle",
     "Duez"
    ]
   ],
   "title": "Perceptual optimization of syllable duration in short French sentences",
   "original": "e89_2694",
   "page_count": 4,
   "order": 354,
   "p1": "2694",
   "pn": "2697",
   "abstract": [
    "Two experiments were conducted in order to study the perceptual discrimination of syllable duration within sentences. Two natural French sentences each consisting of 8 syllables were re-synthesized with 15% increases and decreases in syllable duration up to ±60%. The modified stimuli were presented to subjects whose task was to judge how natural they sounded. The same stimuli were then used to determine whether the modifications in duration were perceptible. The results provide evidence of the fact that judgment of the rhythmic quality of sentences (a linguistic task) and temporal discrimination (a psycho-acoustic task) both have the same functional basis. The detection threshold for unstressed syllables was found to be lower than that of stressed syllables due to linguistic familiarity.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-327"
  },
  "campbell89_eurospeech": {
   "authors": [
    [
     "W. Nick",
     "Campbell"
    ]
   ],
   "title": "Syllable-level duration determination",
   "original": "e89_2698",
   "page_count": 4,
   "order": 355,
   "p1": "2698",
   "pn": "2701",
   "abstract": [
    "Accurate prediction of duration in a text-to-speech system is essential to natural-sounding intonation. Klatt [1] proposed a set of phoneme-based rules to perform this task, but an adaptation of the rule-set to British English [2] accounted for only 68% of the variance in the duration observed in a 4000-syllable test text. Modification of these rules to incorporate foot-level effects [3,4] improved the prediction slightly to account for 71% of the variance. A similar degree of prediction can be attained, with minimum reference to segment specifics, by modelling duration at the level of the syllable, with sensitivity to stress, position in phrase and foot, and number of segments in onset, peak and coda. This supposes that micro-durational features such as shortening of segments in clusters, and lengthening of vowels to cue voicing, operate at a phonetic level, within the constraints of a syllable frame, and that higher-level features determine factors of lengthening or compression for the framework into which they are to fit. In support of this view, a connectionist implementation, of eight input features, one layer of hidden units and one analog output unit, that accounts for an equivalent 70% of the variance in the duration is described.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-328"
  },
  "piroth89_eurospeech": {
   "authors": [
    [
     "Hans Georg",
     "Piroth"
    ],
    [
     "Thomas",
     "Arnhold"
    ]
   ],
   "title": "Psychophysiological measurements during tactile speech identification and discrimination tests",
   "original": "e89_2702",
   "page_count": 4,
   "order": 356,
   "p1": "2702",
   "pn": "2705",
   "abstract": [
    "A large number of investigations have been under-taken to develop technical systems in order to enable a tactile substitution of speech for the deaf. These aids have been evaluated with various experimental procedures (identification and discrimination tests, learning experiments, tracking procedures and other overt behaviour methods). In order to find additional evidence for the processing and perception of such stimuli, in the present experiment psychophysiological responses during identification and discrimination tests using tactile speech stimuli were recorded. A 3-factorial ANOVA-design was used to evaluate test condition, task condition and habituation during the presentation of tactile speech stimuli. Besides the Ss' identification and discrimination answers, eye blink rate (BR) was registered and electrodermal activity (EDA) measured on both hands concomitantly. The results show that both psychophysiological measures (EDA and BR) very sensitively differentiate various task conditions. Blink rate clearly reflects a difference in the processing of identification and discrimination.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-329"
  },
  "summers89_eurospeech": {
   "authors": [
    [
     "Ian R.",
     "Summers"
    ],
    [
     "Judith",
     "Farr"
    ]
   ],
   "title": "Development of a single-channel tactile aid for the profoundly deaf",
   "original": "e89_2706",
   "page_count": 4,
   "order": 357,
   "p1": "2706",
   "pn": "2709",
   "abstract": [
    "In this study of lipreading with supplementary tactile input, normally hearing subjects with noise masking were presented with tactile information relating to the voiced/unvoiced contrast. Considerable training was required before subjects were able to make use of this information so as to gain higher scores for lipreading with tactile supplement than for lipreading alone. These results are in contrast with those from earlier experiments which showed that relatively little training was necessary for successful tactile perception of voice stress.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-330"
  },
  "cottle89_eurospeech": {
   "authors": [
    [
     "M.",
     "Cottle"
    ],
    [
     "C.",
     "Gaunt"
    ],
    [
     "R. M.",
     "Stephens"
    ]
   ],
   "title": "The use of speech recognition as an aid for speech therapists",
   "original": "e89_2710",
   "page_count": 4,
   "order": 358,
   "p1": "2710",
   "pn": "2714",
   "abstract": [
    "Speech recognisers are normally trained and used by people who can speak words in a predefined vocabulary consistently. People with speech disorders but who make reproducible sounds or utterances can also use a recogniser. However children who experience language and phonological delays, associated with learning difficulties, must be trained to speak clearly and consistently, and the production of templates representing spoken words can be extremely difficult. This paper discusses how templates representing single words can be extracted and edited from a continuous string of utterances made in less than ideal conditions. The edited templates are then used as references for subsequent utterances.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-331"
  },
  "oster89_eurospeech": {
   "authors": [
    [
     "Anne-Marie",
     "Öster"
    ]
   ],
   "title": "Applications and experiences of computer-based speech training",
   "original": "e89_2714",
   "page_count": 4,
   "order": 359,
   "p1": "2714",
   "pn": "2717",
   "abstract": [
    "A microcomputer-based aid for speech training has been introduced on a trial basis in the Swedish schools for the deaf. The prototype has been used for some years to evaluate its applicability in the speech clinic of a bilingual school for prelingually deaf children. Results are reported from two training experiments and some general comments are given on this new type of visual technical aids.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-332"
  },
  "faulkner89_eurospeech": {
   "authors": [
    [
     "Andrew",
     "Faulkner"
    ],
    [
     "Adrian J.",
     "Fourcin"
    ]
   ],
   "title": "Speech-pattern presentation to the deaf: speech perception and production",
   "original": "e89_2718",
   "page_count": 4,
   "order": 360,
   "p1": "2718",
   "pn": "2721",
   "abstract": [
    "The optimal use of the impaired auditory system in the essential task of lipreading depends upon the transmission of invisible speech factors such as voice fundamental frequency, voicing, and manner of articulation. The same factors are also important for voice control. The corresponding speech features are imperfectly perceived by the deaf, but their acoustic correlates can now be relatively efficiently extracted in real-time (eg, IS Howard and JR Walliker: this meeting) and presented as an ensemble of simplified speech pattern elements matched to the patient's auditory abilities. Our recent receptive results show that combined pattern elements of voice fundamental frequency and voiceless friction can give additional benefit over voice fundamental frequency alone, and can also provide more benefit than whole speech presentation in an hitherto neglected sector of the deaf population. Matched speech pattern element presentation can also greatly assist speech production. The paper also examines the use of overall amplitude information in addition to voice fundamental frequency and voiceless friction.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1989-333"
  }
 },
 "sessions": [
  {
   "title": "Plenary",
   "papers": [
    "rolling89_eurospeech",
    "fant89_eurospeech"
   ]
  },
  {
   "title": "Speech Recognition: Subword Units in ASR",
   "papers": [
    "hwang89_eurospeech",
    "torkkola89_eurospeech",
    "falaschi89_eurospeech",
    "weigel89_eurospeech",
    "vigier89_eurospeech"
   ]
  },
  {
   "title": "Language Processing: Semantic Oriented Language Analysis",
   "papers": [
    "young89_eurospeech",
    "niedermair89_eurospeech",
    "tropf89_eurospeech",
    "biebow89_eurospeech",
    "streit89_eurospeech",
    "price89_eurospeech",
    "mousel89_eurospeech",
    "matrouf89_eurospeech",
    "hunnicutt89_eurospeech",
    "inoue89_eurospeech",
    "kitano89_eurospeech"
   ]
  },
  {
   "title": "Speech Analysis: Prosody",
   "papers": [
    "collier89_eurospeech",
    "avesani89_eurospeech",
    "mertens89_eurospeech",
    "hertrich89_eurospeech",
    "vaissiere89_eurospeech",
    "howard89_eurospeech",
    "batliner89_eurospeech",
    "quene89_eurospeech",
    "bezooijen89_eurospeech",
    "martin89_eurospeech",
    "hieronymus89_eurospeech"
   ]
  },
  {
   "title": "Perception: Segmental",
   "papers": [
    "peeters89_eurospeech",
    "datscheweit89_eurospeech",
    "schwartz89_eurospeech",
    "watson89_eurospeech",
    "broeders89_eurospeech"
   ]
  },
  {
   "title": "Speech Recognition: Systems",
   "papers": [
    "fesseler89_eurospeech",
    "scagliola89_eurospeech",
    "monnet89_eurospeech",
    "fournol89_eurospeech",
    "dermatas89_eurospeech",
    "buttafava89_eurospeech",
    "addadecker89_eurospeech",
    "bundgaard89_eurospeech",
    "marino89_eurospeech",
    "colangeli89_eurospeech",
    "noyes89_eurospeech"
   ]
  },
  {
   "title": "Speech Synthesis: Techniques and Applications",
   "papers": [
    "carlson89_eurospeech",
    "spiegel89_eurospeech",
    "king89_eurospeech",
    "ainsworth89_eurospeech",
    "bosch89_eurospeech",
    "loman89_eurospeech",
    "bailly89_eurospeech",
    "hirokawa89_eurospeech",
    "backstrom89_eurospeech"
   ]
  },
  {
   "title": "Speech Recognition: HMM",
   "papers": [
    "lee89_eurospeech",
    "bahl89_eurospeech",
    "serralheiro89_eurospeech",
    "huang89_eurospeech",
    "varga89_eurospeech",
    "imamura89_eurospeech",
    "hon89_eurospeech",
    "davies89_eurospeech",
    "wright89_eurospeech",
    "lee89b_eurospeech",
    "rigoll89_eurospeech"
   ]
  },
  {
   "title": "Perception: Auditory Processing",
   "papers": [
    "darwin89_eurospeech",
    "wu89_eurospeech",
    "pont89_eurospeech",
    "hukin89_eurospeech",
    "berthommier89_eurospeech",
    "keurs89_eurospeech"
   ]
  },
  {
   "title": "Speech Recognition: Robust Recognition and Speaker Recognition",
   "papers": [
    "fujihashi89_eurospeech",
    "yong89_eurospeech",
    "ruehl89_eurospeech",
    "carey89_eurospeech",
    "mason89_eurospeech",
    "xu89_eurospeech",
    "rocchi89_eurospeech",
    "federico89_eurospeech",
    "giannini89_eurospeech",
    "zalewski89_eurospeech"
   ]
  },
  {
   "title": "Speech Coding: Coders Design and Evaluation",
   "papers": [
    "hirata89_eurospeech",
    "lefevre89_eurospeech",
    "soheili89_eurospeech",
    "boyd89_eurospeech",
    "ozawa89_eurospeech",
    "delprat89_eurospeech",
    "atungsiri89_eurospeech",
    "satoh89_eurospeech",
    "moreau89_eurospeech",
    "wery89_eurospeech"
   ]
  },
  {
   "title": "Speech Recognition: Connectionist Network",
   "papers": [
    "blanchet89_eurospeech",
    "renals89_eurospeech",
    "patarnello89_eurospeech",
    "monte89_eurospeech",
    "kangas89_eurospeech",
    "caharel89_eurospeech",
    "bulot89_eurospeech",
    "bottou89_eurospeech",
    "dalsgaard89_eurospeech",
    "baekgaard89_eurospeech",
    "komori89_eurospeech",
    "haffner89_eurospeech",
    "rohwer89_eurospeech",
    "kokkonen89_eurospeech",
    "huckvale89_eurospeech"
   ]
  },
  {
   "title": "Speech Synthesis: Voice Source and Prosody",
   "papers": [
    "karlsson89_eurospeech",
    "barry89_eurospeech",
    "terken89_eurospeech",
    "moreno89_eurospeech",
    "brovchenko89_eurospeech",
    "fant89b_eurospeech",
    "monaghan89_eurospeech",
    "quazza89_eurospeech",
    "larreur89_eurospeech",
    "youd89_eurospeech"
   ]
  },
  {
   "title": "Speech Recognition: Tools for Feature-Based Phonetic Recognition",
   "papers": [
    "blokland89_eurospeech",
    "nguyentrong89_eurospeech",
    "shirai89_eurospeech",
    "poirier89_eurospeech",
    "thompson89_eurospeech"
   ]
  },
  {
   "title": "Language Processing: Written Language Analysis",
   "papers": [
    "dermatas89b_eurospeech",
    "cericola89_eurospeech",
    "mastrolonardo89_eurospeech",
    "fouquere89_eurospeech",
    "reynier89_eurospeech",
    "elenius89_eurospeech",
    "mcallister89_eurospeech",
    "refice89_eurospeech",
    "prakash89_eurospeech",
    "berthelin89_eurospeech"
   ]
  },
  {
   "title": "Applications: Designing an Application",
   "papers": [
    "carter89_eurospeech",
    "chigier89_eurospeech",
    "thomas89_eurospeech",
    "vu89_eurospeech",
    "hughes89_eurospeech",
    "hamada89_eurospeech",
    "bakkum89_eurospeech",
    "leiser89_eurospeech",
    "fellbaum89_eurospeech",
    "ciaramella89_eurospeech",
    "schalk89_eurospeech",
    "murillo89_eurospeech",
    "kobayashi89_eurospeech"
   ]
  },
  {
   "title": "Speech Analysis: Miscellaneous",
   "papers": [
    "karjalainen89_eurospeech",
    "beroule89_eurospeech",
    "carlson89b_eurospeech",
    "chan89_eurospeech",
    "moulsley89_eurospeech",
    "stamenkovic89_eurospeech",
    "giustiniani89_eurospeech",
    "reetz89_eurospeech",
    "hirst89_eurospeech"
   ]
  },
  {
   "title": "Speech Recognition: Prosody and Lexical Access in Knowledge-Based Recognition",
   "papers": [
    "nasri89_eurospeech",
    "carbonell89_eurospeech",
    "bundgaard89b_eurospeech",
    "carlo89_eurospeech",
    "kaspar89_eurospeech"
   ]
  },
  {
   "title": "Applications: Applications and Specialized Devices",
   "papers": [
    "moore89_eurospeech",
    "boillon89_eurospeech",
    "riccio89_eurospeech",
    "cecinati89_eurospeech",
    "gagnoulet89_eurospeech",
    "haberbeck89_eurospeech",
    "buther89_eurospeech",
    "lecomte89_eurospeech",
    "ventura89_eurospeech",
    "aktas89_eurospeech",
    "coile89_eurospeech"
   ]
  },
  {
   "title": "Signal Processing: Acoustics, Noise, and Enhancement",
   "papers": [
    "chen89_eurospeech",
    "jovicic89_eurospeech",
    "varley89_eurospeech",
    "yegnanarayana89_eurospeech",
    "owens89_eurospeech",
    "psutka89_eurospeech",
    "fohr89_eurospeech",
    "fikri89_eurospeech",
    "alphen89_eurospeech",
    "estola89_eurospeech",
    "farassopoulos89_eurospeech",
    "nakata89_eurospeech",
    "scaife89_eurospeech",
    "hirsch89_eurospeech",
    "gehrenbeck89_eurospeech",
    "compernolle89_eurospeech",
    "schlang89_eurospeech"
   ]
  },
  {
   "title": "Speech Recognition: Large Vocabulary and Multilingual Aspects",
   "papers": [
    "lokenkim89_eurospeech",
    "emam89_eurospeech",
    "wothke89_eurospeech"
   ]
  },
  {
   "title": "Speech Synthesis: Methods",
   "papers": [
    "charpentier89_eurospeech",
    "heike89_eurospeech",
    "badin89_eurospeech",
    "mrayati89_eurospeech",
    "owens89b_eurospeech",
    "lehtinen89_eurospeech",
    "bimbot89_eurospeech",
    "falaschi89b_eurospeech",
    "posmyk89_eurospeech",
    "takeda89_eurospeech"
   ]
  },
  {
   "title": "Signal Processing: Miscellaneous",
   "papers": [
    "lobo89_eurospeech",
    "alku89_eurospeech",
    "gautherot89_eurospeech",
    "francesco89_eurospeech",
    "veth89_eurospeech",
    "chan89b_eurospeech",
    "marques89_eurospeech",
    "itahashi89_eurospeech",
    "dalessandro89_eurospeech",
    "nadeu89_eurospeech"
   ]
  },
  {
   "title": "Language Processing: Speech-Oriented Language Analysis",
   "papers": [
    "gong89_eurospeech",
    "steinbiss89_eurospeech",
    "mckelvie89_eurospeech",
    "thompson89b_eurospeech",
    "senders89_eurospeech",
    "kurematsu89_eurospeech",
    "henrich89_eurospeech",
    "nakagawa89_eurospeech",
    "iida89_eurospeech",
    "larsen89_eurospeech",
    "delomier89_eurospeech"
   ]
  },
  {
   "title": "Speech Analysis: Segmentation and Labelling",
   "papers": [
    "tsopanoglou89_eurospeech",
    "mierlo89_eurospeech",
    "moreno89b_eurospeech",
    "sorensen89_eurospeech",
    "dours89_eurospeech",
    "perennou89_eurospeech",
    "erp89_eurospeech",
    "thompson89c_eurospeech",
    "seidl89_eurospeech",
    "datta89_eurospeech",
    "houben89_eurospeech"
   ]
  },
  {
   "title": "Perceptual Aspects of Prosody and Voice Quality",
   "papers": [
    "collier89b_eurospeech",
    "gartenberg89_eurospeech",
    "koopmansvanbeinum89_eurospeech",
    "potapova89_eurospeech",
    "nushikyan89_eurospeech",
    "pascal89_eurospeech",
    "tielen89_eurospeech",
    "shevchenko89_eurospeech",
    "cave89_eurospeech",
    "grassegger89_eurospeech",
    "harmegnies89_eurospeech",
    "gunzburger89_eurospeech"
   ]
  },
  {
   "title": "Speech Recognition: Miscellaneous",
   "papers": [
    "pardo89_eurospeech",
    "cerfdanon89_eurospeech",
    "ferretti89_eurospeech",
    "billi89_eurospeech",
    "baker89_eurospeech",
    "sciarra89_eurospeech",
    "boves89_eurospeech"
   ]
  },
  {
   "title": "Speech Recognition0: Recognition of Phonetic Units",
   "papers": [
    "rouat89_eurospeech",
    "dalby89_eurospeech",
    "diest89_eurospeech",
    "williams89_eurospeech",
    "tattegrain89_eurospeech",
    "nishinuma89_eurospeech",
    "elsheikh89_eurospeech",
    "djoudi89_eurospeech",
    "caraty89_eurospeech",
    "glassman89_eurospeech"
   ]
  },
  {
   "title": "Phonetics",
   "papers": [
    "kobayashi89b_eurospeech",
    "boe89_eurospeech",
    "bergem89_eurospeech",
    "iivonen89_eurospeech",
    "gosy89_eurospeech",
    "ganguli89_eurospeech",
    "watson89b_eurospeech",
    "chafcouloff89_eurospeech"
   ]
  },
  {
   "title": "Assessment: Speech Recognition",
   "papers": [
    "fourcin89_eurospeech",
    "garofolo89_eurospeech",
    "kingham89_eurospeech",
    "steeneken89_eurospeech",
    "crowe89_eurospeech",
    "bourjot89_eurospeech",
    "mcinnes89_eurospeech",
    "marcus89_eurospeech",
    "vernooij89_eurospeech"
   ]
  },
  {
   "title": "Speech Analysis: Duration",
   "papers": [
    "pitrelli89_eurospeech",
    "carlson89c_eurospeech",
    "farnetani89_eurospeech",
    "alanani89_eurospeech"
   ]
  },
  {
   "title": "Speech Coding: LPC Parameters Evaluation",
   "papers": [
    "zarkadis89_eurospeech",
    "wong89_eurospeech",
    "hunt89_eurospeech",
    "omologo89_eurospeech",
    "liu89_eurospeech"
   ]
  },
  {
   "title": "Phonological Variants",
   "papers": [
    "rao89_eurospeech",
    "perennou89b_eurospeech",
    "williams89b_eurospeech",
    "cutler89_eurospeech",
    "lacheretdujour89_eurospeech"
   ]
  },
  {
   "title": "Speech Recognition: Algorithms and Models",
   "papers": [
    "chen89b_eurospeech",
    "bowles89_eurospeech",
    "shillcock89_eurospeech",
    "maire89_eurospeech",
    "hamada89b_eurospeech",
    "komatsu89_eurospeech",
    "ye89_eurospeech",
    "marino89b_eurospeech"
   ]
  },
  {
   "title": "Speech Production",
   "papers": [
    "castelli89_eurospeech",
    "delattre89_eurospeech",
    "recasens89_eurospeech",
    "elhalees89_eurospeech",
    "strik89_eurospeech",
    "farnetani89b_eurospeech",
    "autesserre89_eurospeech",
    "autesserre89b_eurospeech",
    "maeda89_eurospeech",
    "zerling89_eurospeech",
    "barney89_eurospeech",
    "caldognetto89_eurospeech"
   ]
  },
  {
   "title": "Speech Analysis: Voice Quality",
   "papers": [
    "ferretti89b_eurospeech",
    "huber89_eurospeech",
    "schoentgen89_eurospeech",
    "howard89b_eurospeech",
    "harmegnies89b_eurospeech",
    "nord89_eurospeech"
   ]
  },
  {
   "title": "Speech Coding: Techniques for Narrowband Coding",
   "papers": [
    "trancoso89_eurospeech",
    "lee89c_eurospeech",
    "masgraugomez89_eurospeech",
    "marques89b_eurospeech"
   ]
  },
  {
   "title": "Speech Synthesis: Systems",
   "papers": [
    "holmes89_eurospeech",
    "barbe89_eurospeech",
    "shi89_eurospeech",
    "olaszy89_eurospeech"
   ]
  },
  {
   "title": "Language Processing: Human Factors, Psychology, and Human-Machine Dialogue",
   "papers": [
    "romary89_eurospeech",
    "bard89_eurospeech",
    "luzzati89_eurospeech",
    "mcqueen89_eurospeech",
    "miles89_eurospeech",
    "frankish89_eurospeech",
    "kuijpers89_eurospeech",
    "hitzenberger89_eurospeech",
    "luzzati89b_eurospeech",
    "robert89_eurospeech"
   ]
  },
  {
   "title": "Speech Recognition: Knowledge-Based Phonetic Recognition Systems",
   "papers": [
    "fohr89b_eurospeech",
    "altosaar89_eurospeech",
    "martelli89_eurospeech",
    "blomberg89_eurospeech",
    "meloni89_eurospeech"
   ]
  },
  {
   "title": "Assessment: Speech Synthesis",
   "papers": [
    "hall89_eurospeech",
    "benoit89_eurospeech",
    "pickering89_eurospeech",
    "robert89b_eurospeech",
    "pavlovic89_eurospeech"
   ]
  },
  {
   "title": "Speech Analysis: Formants",
   "papers": [
    "son89_eurospeech",
    "laprie89_eurospeech",
    "lin89_eurospeech",
    "fink89_eurospeech",
    "grenie89_eurospeech"
   ]
  },
  {
   "title": "Perception: Syllable and Timing",
   "papers": [
    "hawkins89_eurospeech",
    "mcallister89b_eurospeech",
    "nord89b_eurospeech",
    "nishinuma89b_eurospeech",
    "campbell89_eurospeech"
   ]
  },
  {
   "title": "Applications: Aids to the Handicapped",
   "papers": [
    "piroth89_eurospeech",
    "summers89_eurospeech",
    "cottle89_eurospeech",
    "oster89_eurospeech",
    "faulkner89_eurospeech"
   ]
  }
 ],
 "doi": "10.21437/Eurospeech.1989"
}