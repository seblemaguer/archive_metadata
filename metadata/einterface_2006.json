{
 "title": "Summer Workshop on Multimodal Interfaces (eINTERFACE 2006)",
 "location": "Dubrovnik, Croatia",
 "startDate": "17/7/2006",
 "endDate": "11/8/2006",
 "conf": "eINTERFACE",
 "year": "2006",
 "name": "einterface_2006",
 "series": "eINTERFACE",
 "SIG": "",
 "title1": "Summer Workshop on Multimodal Interfaces",
 "title2": "(eINTERFACE 2006)",
 "date": "17 July - 11 August 2006",
 "papers": {
  "huang06_einterface": {
   "authors": [
    [
     "Hung-Hsuan",
     "Huang"
    ],
    [
     "Aleksandra",
     "Cerekovic"
    ],
    [
     "Kateryna",
     "Tarasenko"
    ],
    [
     "Vjekoslav",
     "Levacic"
    ],
    [
     "Goranka",
     "Zoric"
    ],
    [
     "Margus",
     "Treumuth"
    ],
    [
     "Igor S.",
     "Pandzic"
    ],
    [
     "Yukiko",
     "Nakano"
    ],
    [
     "Toyoaki",
     "Nishida"
    ]
   ],
   "title": "An agent based multicultural user interface in a customer service application",
   "original": "ei06_001",
   "page_count": 10,
   "order": 1,
   "p1": "1",
   "pn": "10",
   "abstract": [
    "The advancement of traffic and computer networks makes the world more and more internationalized and increases the frequency of communications between people using different languages and expressing different nonverbal behaviors. To improve communication of embodied conversational agent (ECA) systems with their human users, the importance of their capability to cover the cultural differences emerged. Various excellent ECA systems are developed and proposed previously, however, the cross-culture communication issues are seldom addressed by researchers. This project aims to explore the possibility of rapidly building multicultural and multimodal ECA inter-faces for customer service applications with a generic framework connecting their functional blocks.\n",
    ""
   ]
  },
  "moustakas06_einterface": {
   "authors": [
    [
     "Konstantinos",
     "Moustakas"
    ],
    [
     "Georgios",
     "Nikolakis"
    ],
    [
     "Dimitrios",
     "Tzovaras"
    ],
    [
     "Benoit",
     "Deville"
    ],
    [
     "Ioannis",
     "Marras"
    ],
    [
     "Jakov",
     "Pavlek"
    ]
   ],
   "title": "Multimodal tools and interfaces for the intercommunication between visually impaired and \"deaf and mute\" people",
   "original": "ei06_011",
   "page_count": 12,
   "order": 2,
   "p1": "11",
   "pn": "22",
   "abstract": [
    "The present paper presents the framework and the results of Project 2: “Multimodal tools and interfaces for the intercommunication between visually impaired and “deaf and mute” people”, which has been developed during the eNTERFACE-2006 summer workshop in the context of the SIMILAR NoE. The developed system aims to provide alternative tools and interfaces to blind and deaf-and-mute persons so as to enable their intercommunication as well as their interaction with the computer. All the involved technologies are integrated into a treasure hunting game application that is jointly played by the blind and deaf-and-mute user. The reason for choosing to integrate the multimodal interfaces into a game application is that it serves both as an entertainment and as a pleasant education tool to its users. The proposed application integrates haptics, audio, visual output as well as computer vision, sign language analysis and synthesis, speech recognition and synthesis, in order to provide an interactive environment where the blind and deaf and mute users can collaborate in order to play the treasure hunting game.\n",
    "",
    "",
    "Index Terms: Multimodal interfaces, Rehabilitation technologies, Virtual Reality.\n",
    ""
   ]
  },
  "aran06_einterface": {
   "authors": [
    [
     "Oya",
     "Aran"
    ],
    [
     "Ismail",
     "Ari"
    ],
    [
     "Alexandre",
     "Benoit"
    ],
    [
     "Ana Huerta",
     "Carrillo"
    ],
    [
     "François-Xavier",
     "Fanard"
    ],
    [
     "Pavel",
     "Campr"
    ],
    [
     "Lale",
     "Akarun"
    ],
    [
     "Alice",
     "Caplier"
    ],
    [
     "Michele",
     "Rombaut"
    ],
    [
     "Bülent",
     "Sankur"
    ]
   ],
   "title": "Sign language tutoring tool",
   "original": "ei06_023",
   "page_count": 11,
   "order": 3,
   "p1": "23",
   "pn": "33",
   "abstract": [
    "In this project, we have developed a sign language tutor that lets users learn isolated signs by watching recorded videos and by trying the same signs. The system records the user's video and analyses it. If the sign is recognized, both verbal and animated feedback is given to the user. The system is able to recognize complex signs that involve both hand gestures and head movements and expressions. Our performance tests yield a 99% recognition rate on signs involving only manual gestures and 85% recognition rate on signs that involve both manual and non manual components, such as head movement and facial expressions.\n",
    "",
    "",
    "Index Terms: Gesture recognition, sign language recognition, head movement analysis, human body animation\n",
    ""
   ]
  },
  "dutoit06_einterface": {
   "authors": [
    [
     "Thierry",
     "Dutoit"
    ],
    [
     "A.",
     "Holzapfel"
    ],
    [
     "M.",
     "Jottrand"
    ],
    [
     "F.",
     "Marqués"
    ],
    [
     "A.",
     "Moinet"
    ],
    [
     "F.",
     "Ofli"
    ],
    [
     "J.",
     "Pérez"
    ],
    [
     "Yannis",
     "Stylianou"
    ]
   ],
   "title": "Multimodal speaker conversion - his master's voice ... and face",
   "original": "ei06_034",
   "page_count": 12,
   "order": 4,
   "p1": "34",
   "pn": "45",
   "abstract": [
    "The goal of this project is to convert a given speaker's speech (the Source speaker) into another identified voice (the Target speaker) as well as analysing the face animation of the source to animate a 3D avatar imitating the source facial movements. We assume we have at our disposal a large amount of speech samples from the source and target voices with a reasonable amount of parallel data. Speech and video are processed separately and recombined at the end.\n",
    "Voice conversion is obtained in two steps: a voice mapping step followed by a speech synthesis step. In the speech synthesis step, we specifically propose to select speech frames directly from the large target speech corpus, in a way that recall the unit-selection principle used in state-of-the-art text-to-speech systems.\n",
    "The output of this four weeks work can be summarized as: a tailored source database, a set of open-source MATLAB and C files and finally audio and video files obtained by our conversion method. Experimental results show that we cannot aim to reach the target with our LPC synthesis method; further work is required to enhance the quality of the speech.\n",
    "",
    "",
    "Index Terms-voice conversion, speech-to-speech conversion, speaker mapping, face tracking, cloning, morphing, avatar control.\n",
    ""
   ]
  },
  "mosmondor06_einterface": {
   "authors": [
    [
     "Miran",
     "Mosmondor"
    ],
    [
     "Ognjen",
     "Dobrijevic"
    ],
    [
     "Ivan",
     "Piskovic"
    ],
    [
     "Mirko",
     "Suznjevic"
    ],
    [
     "Maja",
     "Matijasevic"
    ],
    [
     "Sasa",
     "Desic"
    ]
   ],
   "title": "Introducing network-awareness for networked multimedia and multi-modal applications",
   "original": "ei06_046",
   "page_count": 13,
   "order": 5,
   "p1": "46",
   "pn": "58",
   "abstract": [
    "Due to increased user/service requirements in terms of network quality of service (QoS) parameters, and heterogeneity of end-user access network options and terminal capabilities, introducing “network-awareness” into rich multimedia and multimodal networked applications could provide a critical advantage. An idea behind network-awareness is to let the applications indicate their requirements and to adapt to changing conditions in the network, as well as to let the network “know” of the applications' resource demands. This approach is based on signaling, as a means to request special treatment for traffic in the network and to receive indications from the network of different conditions. Another important issue for the proposed solution is the simplicity of use. Providing developers with a reusable solution that, to much extent, removes the need for understanding a specific signaling protocol eases and quickens development of the network-aware applications. The project objective was to identify generic signaling functionality, and to create an application programming interface (API) which will enable application developers to create advanced multimodal networked services. The developed API was applied in a case study using a prototype application.\n",
    "",
    "",
    "Index Terms: Application Programming Interface, Dynamic service adaptation, End-to-end Quality of Service signaling, Multimedia and multimodal networked applications, IP Multimedia Subsystem\n",
    ""
   ]
  },
  "brouse06_einterface": {
   "authors": [
    [
     "Andrew",
     "Brouse"
    ],
    [
     "Jean-Julien",
     "Filatriau"
    ],
    [
     "Kosta",
     "Gaitanis"
    ],
    [
     "Rémy",
     "Lehembre"
    ],
    [
     "Benoît",
     "Macq"
    ],
    [
     "Eduardo",
     "Miranda"
    ],
    [
     "Alexandre",
     "Zénon"
    ]
   ],
   "title": "An instrument of sound and visual creation driven by biological signals",
   "original": "ei06_059",
   "page_count": 10,
   "order": 6,
   "p1": "59",
   "pn": "68",
   "abstract": [
    "Recent advances in new technologies offer a large range of innovative instruments for designing and processing sounds. This paper reports on the results of a project that took place during the eNTERFACE06 summer workshop in Dubrovnik, Croatia. During four weeks, researchers from the fields of brain-computer interfaces and sound synthesis worked together to explore multiple ways of mapping analysed physiological signals to sound and image synthesis parameters in order to build biologically-driven musical instruments. A reusable flexible framework for bio-musical applications has been developed and validated using three experimental prototypes, from whence emerged some worthwhile perspectives on future research.\n",
    "",
    "",
    "Index Terms: EEG, EMG, brain-computer interfaces, digital musical instruments, mapping\n",
    ""
   ]
  },
  "savran06_einterface": {
   "authors": [
    [
     "Arman",
     "Savran"
    ],
    [
     "Koray",
     "Ciftci"
    ],
    [
     "Guillame",
     "Chanel"
    ],
    [
     "Javier",
     "Cruz Mota"
    ],
    [
     "Luong Hong",
     "Viet"
    ],
    [
     "Bülent",
     "Sankur"
    ],
    [
     "Lale",
     "Akarun"
    ],
    [
     "Alice",
     "Caplier"
    ],
    [
     "Michele",
     "Rombaut"
    ]
   ],
   "title": "Emotion detection in the loop from brain signals and facial images",
   "original": "ei06_069",
   "page_count": 12,
   "order": 7,
   "p1": "69",
   "pn": "80",
   "abstract": [
    "In this project, we intended to develop techniques for multimodal emotion detection, one modality being brain signals via fNIRS, the second modality being face video and the third modality being the scalp EEG signals. EEG and fNIRS provided us with an “internal” look at the emotion generation processes, while video sequence gave us an “external” look on the “same” phenomenon.\n",
    "Fusions of fNIRS with video and of EEG with fNIRS were considered. Fusion of all three modalities was not considered due to the extensive noise on the EEG signals caused by facial muscle movements, which are required for emotion detection from video sequences.\n",
    "Besides the techniques mentioned above, peripheral signals, namely, respiration, cardiac rate, and galvanic skin resistance were also measured from the subjects during “fNIRS + EEG” recordings. These signals provided us with extra information about the emotional state of the subjects.\n",
    "The critical point in the success of this project was to be able to build a “good” database. Good data acquisition means synchronous data and requires the definition of some specific experimental protocols for emotions elicitation. Thus, we devoted much of our time to data acquisition throughout the workshop, which resulted in a large enough database for making the first analyses. Results presented in this report should be considered as preliminary. However, they are promising enough to extend the scope of the research.\n",
    "",
    "",
    "Index Terms: Emotion detection, EEG, video, near-infrared spectroscopy\n",
    ""
   ]
  },
  "dalessandro06_einterface": {
   "authors": [
    [
     "N.",
     "D'Alessandro"
    ],
    [
     "Boris",
     "Doval"
    ],
    [
     "S. Le",
     "Beux"
    ],
    [
     "P.",
     "Woodruff"
    ],
    [
     "Y.",
     "Fabre"
    ]
   ],
   "title": "RAMCESS: realtime and accurate musical control of expression in singing synthesis",
   "original": "ei06_081",
   "page_count": 10,
   "order": 8,
   "p1": "81",
   "pn": "90",
   "abstract": [
    "The main purpose of this project is to develop a full computer-based musical instrument allowing realtime synthesis of expressive singing voice. The expression will result from the continuous action of an interpreter through a gestural control interface. That gestural parameters will influence the voice caracteristics thanks to particular mapping strategies.\n",
    "",
    "",
    "Index Terms: Singing voice, voice synthesis, voice quality, glottal flow models, gestural control, interfaces.\n",
    ""
   ]
  },
  "benoit06_einterface": {
   "authors": [
    [
     "Alexandre",
     "Benoit"
    ],
    [
     "L.",
     "Bonnaud"
    ],
    [
     "Alice",
     "Caplier"
    ],
    [
     "Y.",
     "Damousis"
    ],
    [
     "D.",
     "Tzovaras"
    ],
    [
     "F.",
     "Jourde"
    ],
    [
     "L.",
     "Nigay"
    ],
    [
     "M.",
     "Serrano"
    ],
    [
     "L.",
     "Lawson"
    ]
   ],
   "title": "Multimodal signal processing and interaction for a driving simulator: component-based architecture",
   "original": "ei06_091",
   "page_count": 11,
   "order": 9,
   "p1": "91",
   "pn": "101",
   "abstract": [
    "After a first workshop at eNTERFACE 2005 focusing on developing video-based modalities for an augmented driving simulator, this project aims at designing and developing a multimodal driving simulator that is based on both multimodal driver's focus of attention detection as well as driver's fatigue state detection and prediction. Capturing and interpreting the driver's focus of attention and fatigue state will be based on video data (e.g., facial expression, head movement, eye tracking). While the input multimodal interface relies on passive modalities only (also called attentive user interface), the output multimodal user interface includes several active output modalities for presenting alert messages including graphics and text on a mini-screen and in the windshield, sounds, speech and vibration (vibration wheel). Active input modalities are added in the meta-User Interface to let the user dynamically select the output modalities. The driving simulator is used as a case study for studying software architecture for multimodal signal processing and multimodal interaction using two software component-based platforms, OpenInterface and ICARE.\n",
    "",
    "",
    "Index Terms— Attention level, Component, Driving simulator, Facial movement analysis, ICARE, Interaction modality, OpenInterface, Software architecture, Multimodal interaction.\n",
    ""
   ]
  }
 },
 "sessions": [
  {
   "title": "Papers",
   "papers": [
    "huang06_einterface",
    "moustakas06_einterface",
    "aran06_einterface",
    "dutoit06_einterface",
    "mosmondor06_einterface",
    "brouse06_einterface",
    "savran06_einterface",
    "dalessandro06_einterface",
    "benoit06_einterface"
   ]
  }
 ]
}