{
 "location": "Terrigal, Sydney, NSW, Australia",
 "startDate": "4/12/1998",
 "endDate": "6/12/1998",
 "conf": "AVSP",
 "year": "1998",
 "name": "avsp_1998",
 "series": "AVSP",
 "SIG": "AVISA",
 "title": "Auditory-Visual Speech Processing",
 "title1": "Auditory-Visual Speech Processing",
 "date": "4-6 December 1998",
 "papers": {
  "burnham98_avsp": {
   "authors": [
    [
     "Denis",
     "Burnham"
    ]
   ],
   "title": "Harry McGurk and the McGurk Effect",
   "original": "av98_001",
   "page_count": 2,
   "order": 1,
   "p1": "1",
   "pn": "2",
   "abstract": [
    "The auditory-visual illusion now called the McGurk effect is a powerful demonstration of the integration of auditory and visual information in speech perception. This paper describes the theoretical Zeitgeist at the time of the discovery of this effect as a prelude to the following paper, in which McGurk discusses his work on infant development and auditory-visual speech perception.\n",
    ""
   ]
  },
  "mcgurk98_avsp": {
   "authors": [
    [
     "Harry",
     "McGurk"
    ]
   ],
   "title": "Developmental Psychology and the Vision of Speech (McGurk's Inaugural Lecture in 1988)",
   "original": "av98_003",
   "page_count": 18,
   "order": 2,
   "p1": "3",
   "pn": "20",
   "abstract": [
    "This is the unpublished inaugural professorial lecture by Professor Harry McGurk at the University of Surrey in March 1988. It is preceded by an introductory comment by Denis Burnham. In this paper McGurk charts his studies in infant perceptual development, and describes his discovery of the McGurk effect.\n",
    ""
   ]
  },
  "massaro98_avsp": {
   "authors": [
    [
     "Dominic W.",
     "Massaro"
    ]
   ],
   "title": "Illusions and Issues In Bimodal Speech Perception",
   "original": "av98_021",
   "page_count": 6,
   "order": 3,
   "p1": "21",
   "pn": "26",
   "abstract": [
    "As witnessed by this conference and many other sources of evidence, the study of bimodal speech perception has attained the status of a cottage industry. The addition of just one more modality has made transparent several new phenomena, new theoretical endeavors, and a closer link between research and application. The goal of this paper is to review a series of relevant issues in our search for an understanding of speech perception by ear and eye. The issues include a discussion of viable explanations of the McGurk effect, the time course of auditory/visual processing, neural processing, the role of dynamic information, the information in visual speech, the fusion of written language and auditory speech, and the issue of generalizing from studies of syllables to words and larger segments.\n",
    ""
   ]
  },
  "fixmer98_avsp": {
   "authors": [
    [
     "Eric",
     "Fixmer"
    ],
    [
     "Sarah",
     "Hawkins"
    ]
   ],
   "title": "The Influence of Quality of Information on the McGurk Effect",
   "original": "av98_027",
   "page_count": 6,
   "order": 4,
   "p1": "27",
   "pn": "32",
   "abstract": [
    "Quality of information in the McGurk effect was manipulated by using auditory stimuli that spanned a wide range of intelligibility, and by adding auditory and/or visual noise. Questions were whether responses are affected by subjects' awareness of the variation in stimulus quality, and whether quality of information affects type as well as number of McGurk responses. Expt. 1 established the auditory intelligibility of 8 tokens of /aga/ from 4 speakers. In expt. 2 each /aga/ was presented simultaneously with a videod face saying /aba/ with and without visual and/or auditory noise. There were fewer McGurk responses with more visual noise, and more McGurk responses with more auditory noise, including more responses of /b/ alone rather than e.g. /bg/. Less intelligible auditory stimuli increased the number of McGurk responses when the second syllable was stressed. The data suggest that what the subject perceives depends on both the physical and relative quality of the stimuli, and the perceiver's judgement of their quality.\n",
    ""
   ]
  },
  "sekiyama98_avsp": {
   "authors": [
    [
     "Kaoru",
     "Sekiyama"
    ]
   ],
   "title": "Face Or Voice? Determinant of Compellingness To The McGurk Effect",
   "original": "av98_033",
   "page_count": 4,
   "order": 5,
   "p1": "33",
   "pn": "36",
   "abstract": [
    "This study examined sources of talker differences in the McGurk effect, by questioning which of auditory speech or visual speech is more determining the size of the McGurk effect. Cross-talker dubbing was done between faces and voices of utterances (/ba/ and /ga/) pronounced by two talkers: one compelling talker (CT) to the McGurk effect, and one less compelling talker (LT), as measured in our previous studies. The two talkers and 18 subjects were native speakers of Japanese. There were three presentation conditions: Audio-only, video-only, and audiovisual. The results of unimodal conditions showed that CT was easier to speechread than LT, but that CT was more difficult to listen to than LT. The results of audiovisual condition showed that, although both the visual and auditory talker affect the size of the McGurk effect, the audio component is more responsible than the video component for the talker differences in the McGurk effect.\n",
    ""
   ]
  },
  "burnham98b_avsp": {
   "authors": [
    [
     "Denis",
     "Burnham"
    ],
    [
     "Susanna",
     "Lau"
    ]
   ],
   "title": "The Effect of Tonal Information on Auditory Reliance in the McGurk Effect",
   "original": "av98_037",
   "page_count": 6,
   "order": 6,
   "p1": "37",
   "pn": "42",
   "abstract": [
    "The McGurk effect occurs when conflicting auditory and visual speech information result in an emergent percept. The incidence of the McGurk effect is greater for speakers of English than Japanese, and in turn for speakers of Japanese than Cantonese. Sekiyama postulates that this is because speakers of tonal languages rely more upon auditory than visual information in speech perception. Here this hypothesis is tested by presenting both tonal (Cantonese) and non-tonal (English) language speakers with McGurk stimuli in which the tone on syllables either varied or remained constant across trials. Cantonese perceivers relied more upon auditory information than did Australian perceivers, but over and above this, tone variation affected the relative salience of auditory and visual information. This effect was different for the two syllables [ba] and [ga]. In auditory-visual conflict trials subjects' responses appear to depend on the effect of the tone variation on the specific auditory and visual syllables. The results are discussed in terms of the auditory and visual correlates of tone.\n",
    ""
   ]
  },
  "amano98_avsp": {
   "authors": [
    [
     "Jun",
     "Amano"
    ],
    [
     "Kaoru",
     "Sekiyama"
    ]
   ],
   "title": "The McGurk Effect Is Influenced By The Stimulus Set Size",
   "original": "av98_043",
   "page_count": 6,
   "order": 7,
   "p1": "43",
   "pn": "48",
   "abstract": [
    "This study examined if the size of the McGurk effect depends on the size of stimulus set presented in a block. The auditory syllables used in the present experiment were eight Japanese monosyllables, /pa/, /ta/, /ma/, / na/, /ba/, /da/, /ga/, and /ka/. Each auditory syllable was dubbed with either a compatible visible syllable or a discrepant visible syllable about place of articulation, resulting in 16 audio-visual stimuli. In the small set condition, two auditory consonants, /pa/ and /ta/ in one case and /ma/ and /na/ in another case, appeared in a block. In the medium set condition, four appeared /pa/, /ta/, /ma/, and /na/. In the large set condition, eight appeared /pa/, /ta/, /ma/, /na/, /ka/, / ba/, /da/, and /ga/. We examined if the size of the McGurk effect for /pa/, /ta/, /ma/, and /na/ varies depending on stimulus set-size. Participants identified consonant in three presentation conditions: audio-visual, audio-only, video-only. Except the video-only condition, auditory white noise was added (S/N=0dB). There was also a clear audio-visual condition in which no auditory noise was added. The results for bimodal discrepant pairs showed that auditory labials differ from auditory nonlabials with respect to the effect of the set size: although the auditory nonlabials (/ta/, / na/) did not show the effect of the set size, the size of the McGurk effect for auditory labials (/pa/, /ma/) depended on the stimulus set size, being larger when a consonant appeared in smaller sets. on the other hand, unimodal identifications were not affected by the set size. The observed effect of the set size on the McGurk effect was argued in terms of the number of dimensions in auditory and visual information.\n",
    ""
   ]
  },
  "braida98_avsp": {
   "authors": [
    [
     "Louis D.",
     "Braida"
    ],
    [
     "Kaoru",
     "Sekiyama"
    ],
    [
     "Ann K.",
     "Dix"
    ]
   ],
   "title": "Integration of Audiovisually Compatible and Incompatible Consonants In Identification Experiments",
   "original": "av98_049",
   "page_count": 6,
   "order": 8,
   "p1": "49",
   "pn": "54",
   "abstract": [
    "A mathematical model that has successfully accounted for audiovisual integration of natural consonants with compatible audio and visual components is applied to artificial stimuli with incompatible components. Twenty eight Japanese subjects identified stimuli derived from the eight consonants /b,d,g,p,t,k,m,n/ under auditory, visual, and audiovisual conditions. The acoustic stimulus was degraded by lowpass filtering and additive noise. A single set of model parameters, derived from confusion matrices obtained in the unimodal experiments, was found to make fairly good predictions of the results obtained in the audiovisual experiment for both the compatible and incompatible stimuli. Limitations of the approach and implications of the results are discussed.\n",
    ""
   ]
  },
  "colin98_avsp": {
   "authors": [
    [
     "Cecile",
     "Colin"
    ],
    [
     "Monique",
     "Radeau"
    ],
    [
     "Paul",
     "Deltenre"
    ]
   ],
   "title": "Intermodal Interactions In Speech: A French Study",
   "original": "av98_055",
   "page_count": 6,
   "order": 9,
   "p1": "55",
   "pn": "60",
   "abstract": [
    "The aim of the present work was to determine the best conditions that elicit illusions of the McGurk type in French. We manipulated the item length (monosyllables vs bisyllables), the speaking rate of the bisyllables (slow vs normal vs fast), the vocalic environment (/a/ vs /i/), the voiced-voiceless characteristic of some stop consonants (/b/ vs /p/ and /g/ vs /k/) and the sound intensity (70 dB in the first experiment and 40dB in the second one). In both experiments, the effects were not affected by item length but, for the bisyllables, they increased at a slow speaking rate. The vowel /i/ gave rise to more illusions than /a/. Voiceless consonants produced more combinations than voiced ones, but in the case of fusions there was a tendency toward the reverse pattern. The percentage of illusion increased from the first to the second experiment: from 41% to 49% in the case of combinations and from 3% to 18% in the case of fusions. The McGurk effect was thus strongly affected by listening conditions.\n",
    ""
   ]
  },
  "hayashi98_avsp": {
   "authors": [
    [
     "Yasuko",
     "Hayashi"
    ],
    [
     "Kaoru",
     "Sekiyama"
    ]
   ],
   "title": "Native-Foreign Langage Effect In The McGurk Effect : A Test With Chinese and Japanese",
   "original": "av98_061",
   "page_count": 6,
   "order": 10,
   "p1": "61",
   "pn": "66",
   "abstract": [
    "This study examined the manners of audiovisual speech perception, using the 'McGurk effect', when the speakers were foreigners. The McGurk effect demonstrates that visual (lip movement) information is used during speech perception even when it is discrepant with auditory information. Subjects, 17 Chinese and 23 Japanese reported what they heard while looking at and listening to the speakers[HEX 146] face on the monitor. There were 4 speakers, 2 Chinese and 2 Japanese. All the stimuli were made of one syllable utterance. Half of them were audio-visually compatible stimuli, but half of them were audio-visually incompatible stimuli. The results indicate that the Japanese subjects used more visual information on speech perception when the speakers were foreigners. But the Chinese subjects did not show such language asymmetry.\n",
    ""
   ]
  },
  "foucher98_avsp": {
   "authors": [
    [
     "Elodie",
     "Foucher"
    ],
    [
     "Laurent",
     "Girin"
    ],
    [
     "Gang",
     "Feng"
    ]
   ],
   "title": "Audiovisual Speech Coder : Using Vector Quantization To Exploit The Audio/Video Correlation",
   "original": "av98_067",
   "page_count": 6,
   "order": 11,
   "p1": "67",
   "pn": "72",
   "abstract": [
    "Visual information can help listeners to better understand what is said. In the speech coding domain, it will be shown that it allows to reduce the transmission rate of a classic vocoder (1,9 kbit/s instead of 2,4 kbit/s) by estimating audio parameters from video ones. In addition, vector quantization seems to be a good method to reduce the redundancy between some audio and visual coefficients. With the vector quantization, we can reduce again the bit rate while decreasing the quantization error.\n",
    ""
   ]
  },
  "matthews98_avsp": {
   "authors": [
    [
     "Iain",
     "Matthews"
    ],
    [
     "Tim",
     "Cootes"
    ],
    [
     "Stephen",
     "Cox"
    ],
    [
     "Richard",
     "Harvey"
    ],
    [
     "J. andrew",
     "Bangham"
    ]
   ],
   "title": "Lipreading Using Shape, Shading and Scale",
   "original": "av98_073",
   "page_count": 6,
   "order": 12,
   "p1": "73",
   "pn": "78",
   "abstract": [
    "This paper compares three methods of lipreading for visual and audio-visual speech recognition. Lip shape information is obtained using an Active Shape Model (ASM) lip tracker but is not as effective as modelling the combined shape and enclosed greylevel surface using an Active Appearance Model (AAM). A non-tracked alternative is a nonlinear transform of the image using a multiscale spatial analysis (MSA). This performs almost identically to AAM's in both visual and audio-visual recognition tasks on a multi-talker database of isolated letters.\n",
    ""
   ]
  },
  "yang98_avsp": {
   "authors": [
    [
     "Jie",
     "Yang"
    ],
    [
     "Rainer",
     "Stiefelhagen"
    ],
    [
     "Uwe",
     "Meier"
    ],
    [
     "Alex",
     "Waibel"
    ]
   ],
   "title": "Real-Time Face and Facial Feature Tracking and Applications",
   "original": "av98_079",
   "page_count": 6,
   "order": 13,
   "p1": "79",
   "pn": "84",
   "abstract": [
    "A human face provides a variety of different communicative functions. In this paper, we present approaches for real-time face/facial feature tracking and their applications. First, we present techniques of tracking human faces. It is revealed that human skin-color can be used as a major feature for tracking human faces. An adaptive stochastic model has been developed to characterize the skin-color distributions. Based on the maximum likelihood method, the model parameters can be adapted for different people and different lighting conditions. The feasibility of the model has been demonstrated by the development of a real-time face tracker. We then present a top-down approach for tracking facial features such as eyes, nostrils, and lip corners. These real-time tracking techniques have been successfully applied to many applications such as eye-gaze monitoring, head pose tracking, and lip-reading.\n",
    ""
   ]
  },
  "hallgren98_avsp": {
   "authors": [
    [
     "Asa",
     "Hallgren"
    ],
    [
     "Bertil",
     "Lyberg"
    ]
   ],
   "title": "Lip Movements In Non-Focal and Focal Position for Visual Speech Synthesis",
   "original": "av98_085",
   "page_count": 4,
   "order": 14,
   "p1": "85",
   "pn": "88",
   "abstract": [
    "In acoustic and visual synthesis based on concatenation of speech units such as demisyllables, the recording of these units is normally taken from nonsense utterances where the demisyllable in question is pronounced in a non-focal position. In the present investigation, the relation between the lip movements in focal and non-focal position is studied and a computational model is hypothesised.\n",
    ""
   ]
  },
  "sams98_avsp": {
   "authors": [
    [
     "Mikko",
     "Sams"
    ],
    [
     "Sari",
     "Rusanen"
    ]
   ],
   "title": "Integration of Dichotically and Visually Presented Speech Stimuli",
   "original": "av98_089",
   "page_count": 4,
   "order": 15,
   "p1": "89",
   "pn": "92",
   "abstract": [
    "In dichotic listening, two competing messages are delivered to the left and right ear, respectively. Right-handed subjects tend to report hearing more frequently the message input to the right ear. This is called Right Ear Advantage (REA). When intensities and other properties of the messages are properly adjusted, subjects may have a single perception localized to the center of the head. Frequently the two stimuli 'fuse' and the resultant perception corresponds to neither of the presented stimuli. In a similar vein, in audiovisual speech perception discordant auditory and visual components of the stimulus may fuse (the McGurk effect). In the present study, we investigated the relationship between dichotic listening and audiovisual speech perception. Our results demonstrated REA and dominance of acoustical /ta/ stimulus. The influence of visual speech on the perception was clearly stronger than REA. When visual information was concordant with the auditory input to one ear, the perception of that syllable increased strongly, irrespective of the ear of stimulation. Interestingly, REA appeared even when visible speech modified perception.\n",
    ""
   ]
  },
  "gelder98_avsp": {
   "authors": [
    [
     "Beatrice de",
     "Gelder"
    ],
    [
     "Jean",
     "Vroomen"
    ],
    [
     "Paul",
     "Bertelson"
    ]
   ],
   "title": "Cross-modal Bias of Voice Tone on Facial Expression: Upper versus Lower Halves of a Face",
   "original": "av98_093",
   "page_count": 4,
   "order": 16,
   "p1": "93",
   "pn": "96",
   "abstract": [
    "Emotional states are communicated by seeing the face as well as by listening to affective prosody. The two inputs can also be present at the same time and are then processed concurrently. The present experiment examines the role of the voice on the upper versus lower halves of a face. Previous research using an angry-fear facial expression continuum showed that recognition of the lower half of a face was nearly at chance level. Our experiment asked whether in these circumstances the impact of the voice would be the same for both face halves. The results showed that the cross-modal effect of the voice was the same for the two face conditions.\n",
    ""
   ]
  },
  "radeau98_avsp": {
   "authors": [
    [
     "Monique",
     "Radeau"
    ]
   ],
   "title": "Auditory-Visual Interactions In Spatial Scene Analysis: Development and Neural Bases",
   "original": "av98_097",
   "page_count": 6,
   "order": 17,
   "p1": "97",
   "pn": "102",
   "abstract": [
    "The interactions observed in the ventriloquism situation suggest that visual and auditory signals are not processed as independently as the notion of separate senses would imply. A review of the conditions for pairing, the hypothetical mechanism underlying these interactions, argue for cognitive impenetrability and computational autonomy, the pairing rules being the Gestalt principles of common fate and proximity. There is much evidence in support of the view that auditory-visual integration is present early in life. Data from studies of the perinatal period, such as those on neonatal synesthesia, sensory deprivation, and sensory sur-stimulation as well as neuroanatomical evidence for transitory intersensory connections in the brain support the view that sensory modalities are bound together at birth and differentiate later, consistent with experience-expectant development. The discovery in the superior colliculus of different species of bimodal neurons governed by spatial and temporal rules similar to those underlying ventriloquism suggests a possible neural substrate. Differences between ventriloquism and speechreading are discussed.\n",
    ""
   ]
  },
  "barker98_avsp": {
   "authors": [
    [
     "Jon P.",
     "Barker"
    ],
    [
     "Frederic",
     "Berthommier"
    ],
    [
     "Jean-Luc",
     "Schwartz"
    ]
   ],
   "title": "Is Primitive AV Coherence An Aid To Segment The Scene?",
   "original": "av98_103",
   "page_count": 6,
   "order": 18,
   "p1": "103",
   "pn": "108",
   "abstract": [
    "In this paper we propose the existence of an audio-visual scene analysis (AVSA) module which is able to integrate primitive information from both auditory and visual modalities. This module forms correspondences between the auditory and visual streams based on primitive properties of either representation. Through these correspondences visual information is employed to aid the segregation of acoustic sources. This enhanced segregation may be partly responsible for the increased intelligibility of audio-visual speech. The paper presents the initial results of a planned series of audio-visual speech experiments designed to test this account. Specifically, the experiments reported here address the question of whether visible movement of the speech articulators may protect speech from the effects of masking by noise. It is shown that a reduction in temporal uncertainty due to visual information may reduce the detection threshold for CVs in noise. The same performance increase was not observed in a parallel experiment testing consonant identification.\n",
    ""
   ]
  },
  "bernstein98_avsp": {
   "authors": [
    [
     "Lynne E.",
     "Bernstein"
    ],
    [
     "Edward T. Jr.",
     "Auer"
    ],
    [
     "Paula E.",
     "Tucker"
    ]
   ],
   "title": "Does Training Enhance Visual Speech Perception",
   "original": "av98_109",
   "page_count": 6,
   "order": 19,
   "p1": "109",
   "pn": "114",
   "abstract": [
    "This study investigated whether visual speech perception can be improved with short-term training. This paper first reviews the literature on lipreading training. Then a study is reported that involved 6-7 hours of lipreading training within at most three weeks. In Exp. I, subjects were adults with impaired hearing (IH) or with normal hearing (NH). They alternated training and testing on sets of prerecorded isolated sentences. Exp. II replicated Exp. I, except that subjects also received vibrotactile speech signals during training. It was hypothesized, based on previous studies, that this might promote learning. Evidence was obtained for learning in both experiments during initial test periods, primarily for the NH groups in both experiments. However, the IH group was overall more accurate. The training effects and the differential performance of IH versus NH groups imply the need to take perceptual learning and experience into account in evaluating practical applications involving visible speech.\n",
    ""
   ]
  },
  "cathiard98_avsp": {
   "authors": [
    [
     "Marie-Agnes",
     "Cathiard"
    ],
    [
     "Christian",
     "Abry"
    ],
    [
     "Jean-Luc",
     "Schwartz"
    ]
   ],
   "title": "Visual Perception of Glides Versus Vowels: The Effect of Dynamic Expectancy",
   "original": "av98_115",
   "page_count": 6,
   "order": 20,
   "p1": "115",
   "pn": "120",
   "abstract": [
    "Does the visual perception of glides require a dynamic representation? In our previous experiments on the visual perception of rounding, we argued against the mandatory status of dynamic representations for visual vowels. In this paper, we focus on a specifically temporal contrast between the French vowel [y] and the corresponding [y] glide. Results are twofold. First: for subjects who use this contrast, we demonstrate that the duration of the static phase of rounding provides a basic correlate of the vowel vs. glide identification. Second: a gating experiment indicates that the intrinsically dynamic nature of the glide is not exploited until subject expectancy is oriented towards motion processing. Again we cannot support an exclusive visual dynamic representation neither for vowels nor glides.\n",
    ""
   ]
  },
  "davis98_avsp": {
   "authors": [
    [
     "Chris",
     "Davis"
    ],
    [
     "Jeesun",
     "Kim"
    ]
   ],
   "title": "Repeating and Remembering Foreign Language Words: Does Seeing Help?",
   "original": "av98_121",
   "page_count": 5,
   "order": 21,
   "p1": "121",
   "pn": "126",
   "abstract": [
    "Normal hearing people use lip-reading when listening conditions are not good [1]. Yet even when the listening environment is ideal, lip reading can help with a difficult signal such a listening to a foreign language [2]. These authors demonstrated that non-native French student's shadowing performance of French was improved by seeing the lips and mandible of the speaker. This observation suggests that learning the sounds of a foreign language may be aided by audio-visual presentation compared with audio alone. One recent example of such an application of the audio-visual approach has been in teaching a foreign alphabet by children [3]. The current experiment extended this approach by examining whether the mode of presentation affected the accuracy of repetitions of short phrases of a language participants had not heard before (Korean). Participants either heard a (five syllable) Korean phrase while watching the top part of face (no lips or jaw) or heard the phrase while watching the lips and jaw of the speaker. Three native speakers (blind to the presentation status of the participant) judged the accuracy of the participants' subsequent rendition of the phrase.  The experiment also examined whether presentation mode affected performance on a subsequent old/new recognition task of the experimental phrases.  The results are discussed in relation to the relative contribution of auditory and visual information in L2 acquisition in the immediate and longer term.\n",
    ""
   ]
  },
  "lyxell98_avsp": {
   "authors": [
    [
     "Bjorn",
     "Lyxell"
    ],
    [
     "Jerker",
     "Ronnberg"
    ],
    [
     "Ulf",
     "Andersson"
    ],
    [
     "Jan",
     "Andersson"
    ],
    [
     "Stefan",
     "Samuelsson"
    ]
   ],
   "title": "Working Memory and Visual Speech Communication",
   "original": "av98_127",
   "page_count": 4,
   "order": 22,
   "p1": "127",
   "pn": "130",
   "abstract": [
    "This paper gives an overview of the research that we have carried out at Linkoping University on cognitive aspects of visual speech processing.  First, a cognitive, individual difference perspective on speechreading is introduced, with a specific emphasis on the cognitive architecture of speechreading skill. Second, common cognitive characteristics of exceptionally skilled speechreaders is described. In the final part, a working memory model for visual speechreading is outlined. The model includes three parts; one with amodal cognitive constraints, one with poorly specified language input, and a bottleneck between the modal and amodal parts that is constituted by a semi-abstract phonological processor.\n",
    ""
   ]
  },
  "vroomen98_avsp": {
   "authors": [
    [
     "Jean",
     "Vroomen"
    ],
    [
     "Paul",
     "Bertelson"
    ],
    [
     "Beatrice de",
     "Gelder"
    ]
   ],
   "title": "A Visual Influence in the Discrimination of Auditory Location",
   "original": "av98_131",
   "page_count": 4,
   "order": 23,
   "p1": "131",
   "pn": "134",
   "abstract": [
    "The compellingness of the interaction between vision and auditory localization (the ventriloquist effect) was investigated using a discrimination task. A tone sequence was presented either from the same location or from two locations that alternated along the horizontal plane. In synchrony with the tones, lights were presented either at the same or at alternating locations. Subjects had to decide whether the tones alternated or not, thereby ignoring the lights. The main result was that presenting non-alternating tones together with alternating lights increased the number of [HEX 145]alternating'-judgements, even though subjects were told to ignore the lights, and even though they received corrective feed-back after each trial.\n",
    ""
   ]
  },
  "caldognetto98_avsp": {
   "authors": [
    [
     "Emanuela Magno",
     "Caldognetto"
    ],
    [
     "Claudio",
     "Zmarich"
    ],
    [
     "Piero",
     "Cosi"
    ]
   ],
   "title": "Statistical Definition of Visual Information for Italian Vowels and Consonants",
   "original": "av98_135",
   "page_count": 6,
   "order": 24,
   "p1": "135",
   "pn": "140",
   "abstract": [
    "The aim of this research is to identify visual cues for Italian vocalic and consonantal visemes on a articulatory basis and to verify the results of visual intelligibility tests. These data will be useful when per-forming cross-linguistic comparisons, in defining some relevant visual parameters and variation ranges which could be used in the development of bimodal automatic speech synthesis and recognition systems.\n",
    ""
   ]
  },
  "cerrato98_avsp": {
   "authors": [
    [
     "Loredana",
     "Cerrato"
    ],
    [
     "Federico",
     "Albano Leoni"
    ],
    [
     "Mauro",
     "Falcone"
    ]
   ],
   "title": "Is it Possible to Evaluate the Contribution of Visual Information to the Process of Speech Comprehension?",
   "original": "av98_141",
   "page_count": 5,
   "order": 25,
   "p1": "141",
   "pn": "146",
   "abstract": [
    "We report the results of a series of comprehension tests run with the aim of investigating the contribution of visual information to the process of comprehension of conversational speech. Using as sample material a short conversation held by two male speakers, edited from an Italian TV soap opera, we run 3 comprehension tests: 1. submission of the multi-modal speech signal (auditory + visual); 2. submission of the sample only in the auditory modality (i.e. without the integration of visual cues); 3. submission of the sample only in the visual modality (without the integration of auditory cues). It is clear from our results that the visual cues help the subjects to understand the main topic of conversation and to remember some of the details of the conversation. Moreover they seem to play an important role for the interpretation of the emotional state of the speakers. In some cases visual cues appear to be misleading.\n",
    ""
   ]
  },
  "imaizumi98_avsp": {
   "authors": [
    [
     "Kazuya",
     "Imaizumi"
    ],
    [
     "Shizuo",
     "Hiki"
    ],
    [
     "Yumiko",
     "Fukuda"
    ]
   ],
   "title": "A Symbolic Descriptive System for Facial Expression Conveying Linguistic Information In Signing",
   "original": "av98_147",
   "page_count": 6,
   "order": 26,
   "p1": "147",
   "pn": "152",
   "abstract": [
    "The kinds of change in facial expression which can be produced by a sender's neural control of facial muscles, and, at the same time, can be visually perceived by a listener are described in discrete symbols.  The linguistic information supplemented by the facial expressions in signing is categorized into syllabic, lexical and syntactic components. In order to control the facial expressions for this linguistic information, the timing and duration of the symbols are specified, and, by applying a set of concatenation rules, the control commands are generated.  The adequacy of the symbolic descriptive system and its control rules have been examined through a computer synthesis of artificial facial expression, taking the example of transmitting syntactic information in traditional Japanese sign language. These facial expressions are combined with the symbolic descriptive system for hand shapes and arm actions and their control rules for the pictorial display of signing gestures.\n",
    ""
   ]
  },
  "burnham98c_avsp": {
   "authors": [
    [
     "Denis",
     "Burnham"
    ],
    [
     "Jordi",
     "Robert-Ribes"
    ],
    [
     "Ruth",
     "Ellison"
    ]
   ],
   "title": "Why Captions Have To Be on Time",
   "original": "av98_153",
   "page_count": 4,
   "order": 27,
   "p1": "153",
   "pn": "156",
   "abstract": [
    "Closed captioning dramatically improves deaf people[HEX 146]s enjoyment of television shows, and appears to augment the auditory signal for people with some degree of hearing impairment. However, reports from people with mild to severe hearing loss suggest that when there is a delay between the audio track and the caption, perceivers are confused unless they turn down the volume. These effects have not yet been investigated experimentally. This study provides a preliminary investigation of the importance of synchronisation of captions with auditory-visual material for hearing-impaired people[HEX 146]s enjoyment and comprehension of captioned television programs. Two participants were presented with audio-caption delays of 0, 1, 2, and 4 secs in an auditory-visual condition and an auditory-only condition. Both enjoyment and intelligibility diminished over lag times. In general enjoyment and intelligibility were higher in the auditory-visual than the auditory-only condition, however, for the more severely hearing impaired of the two participants, both enjoyment and intelligibility diminished at a faster rate over delay times for the auditory-visual than the auditory-only condition. Thus at long delays the presence of the visual signal appeared to be distracting. These results are discussed in terms of perceptual mechanisms and practical applications for captioning.\n",
    ""
   ]
  },
  "gelder98b_avsp": {
   "authors": [
    [
     "Beatrice de",
     "Gelder"
    ],
    [
     "Jean",
     "Vroomen"
    ],
    [
     "Bruno",
     "Laeng"
    ]
   ],
   "title": "Impaired Speechreading Related To Arrested Development of Face Processing",
   "original": "av98_157",
   "page_count": 4,
   "order": 28,
   "p1": "157",
   "pn": "160",
   "abstract": [
    "Childhood agnosia and prosopagnosia is are rare disorder of object and face recognition consisting in the loss of one of more aspects of normal visual processing for objects and faces as a consequence of a brain injury not present at birth. The paper reports on the case of patient, RP, a child prosopagnosic who sustained a closed head injury at age 7. Different aspects of speechreading ability are examined both with static and dynamic stimuli (recognition of spoken vowels and digits, serial recall). Moreover, two bimodal tasks were used in order to assess the combination of auditory speech with speechreading. The results show that RP is performing poorly on all of the visual tasks, while his auditory performance is entirely normal. This dissociation between auditory and visual speech skills replicates similar findings with adult prosopagnosics and underscores the importance of intact face processing for the development of normal speechreading.\n",
    ""
   ]
  },
  "duchnowski98_avsp": {
   "authors": [
    [
     "Paul",
     "Duchnowski"
    ],
    [
     "Louis D.",
     "Braida"
    ],
    [
     "David",
     "Lum"
    ],
    [
     "Matthew",
     "Sexton"
    ],
    [
     "Jean",
     "Krause"
    ],
    [
     "Smriti",
     "Banthia"
    ]
   ],
   "title": "Automatic Generation of Cued Speech for The Deaf: Status and Outlook",
   "original": "av98_161",
   "page_count": 6,
   "order": 29,
   "p1": "161",
   "pn": "166",
   "abstract": [
    "Manual Cued Speech is a system of hand gestures designed to help deaf speechreaders distinguish among ambiguous speech elements. We have developed a computerized cueing system that uses automatic speech recognition to determine and display cues to the cue receiver. Keyword scores of 66% in low-context sentences have been obtained with this system, almost double the speechreading-alone scores. We describe the design issues with the largest impact on the cuer's performance, concentrating on the characteristics of cue display and cue timing. Enhancing cue images with color is found to improve their discriminability and may lead to improved speech reception by cue receivers.\n",
    ""
   ]
  },
  "gagne98_avsp": {
   "authors": [
    [
     "Jean-Pierre",
     "Gagne"
    ],
    [
     "Kim Le",
     "Monday"
    ],
    [
     "Christine",
     "Desbiens"
    ],
    [
     "Marie",
     "Lapalme"
    ],
    [
     "Luc",
     "Ducas"
    ]
   ],
   "title": "Evaluation of A Visual-FM System To Enhance Speechreading",
   "original": "av98_167",
   "page_count": 4,
   "order": 30,
   "p1": "167",
   "pn": "170",
   "abstract": [
    "An experiment was conducted to investigate the potential benefits of a visual-FM system for speechreading. Two speakers took part in the study. They were recorded while they spoke 36 sentences at a distance of 1.83, 3.66, and 7.32 m from a stationary Hi8 video camera. Under each experimental condition, the signal available from the camera of a visual-FM system was recorded simultaneously. A speechreading test consisting of a randomization of 432 recorded sentences was administered to a group of 16 subjects with normal hearing and normal (or corrected normal) visual acuity. The results revealed that for the recordings obtained with the Hi8 mm camera speechreading performance decreased as a function of distance. Specifically, there were no differences between the recordings made at 1.83 and 3.66 m. However, there was a significant difference between those two recordings distances and the recordings made at 7.32 m. For the recordings obtained with the visual-FM camera, speechreading performance did not vary significantly as a function of distance. Those findings indicate that the visual-FM system constitutes an effective method of providing visual-speech cues in environmental conditions where the distance between the speaker and the speechreader is not optimal for live-speechreading.\n",
    ""
   ]
  },
  "lyxell98b_avsp": {
   "authors": [
    [
     "Bjorn",
     "Lyxell"
    ],
    [
     "Ulf",
     "Andersson"
    ]
   ],
   "title": "Phonological Capabilities and Speech Understanding",
   "original": "av98_171",
   "page_count": 4,
   "order": 31,
   "p1": "171",
   "pn": "174",
   "abstract": [
    "In the present article we will review results from a number of studies conducted in our laboratory where the purpose has been to examine phonological processing capabilities in deafened adults and individuals with a severe hearing-impairment, and to relate their phonological processing skills to visual and audio-visual speech understanding performance. The results show that deafened adults and individuals with a severe hearing-impairment perform at a significantly lower level than normal hearing controls on cognitive tasks that explicitly require phonological processing, whereas there are no differences between the groups in cognitive tasks where the requirements of such processing are less explicit. The characteristics of the individuals[HEX 180] phonological processing skills are further correlated with visual speechreading and speech understanding with cochlear implants. The results are discussed with respect to what factors that might cause for a deterioration in phonological processing skills, and the effects on visual and audio-visual speech understanding.\n",
    ""
   ]
  },
  "arslan98_avsp": {
   "authors": [
    [
     "Levent M.",
     "Arslan"
    ],
    [
     "David",
     "Talkin"
    ]
   ],
   "title": "3-D Face Point Trajectory Synthesis Using An Automatically Derived Visual Phoneme Similarity Matrix",
   "original": "av98_175",
   "page_count": 6,
   "order": 32,
   "p1": "175",
   "pn": "180",
   "abstract": [
    "This paper presents a novel algorithm which generates three-dimensional face point trajectories for a given speech file with or without its text. The proposed algorithm first employs an off-line training phase.  In this phase, recorded face point trajectories along with their speech data and phonetic labels are used to generate phonetic codebooks. These codebooks consist of both acoustic and visual features. Acoustics are represented by line spectral frequencies (LSF), and face points are represented with their principal components (PC). During the synthesis stage, speech input is rated in terms of its similarity to the codebook entries. Based on the similarity, each codebook entry is assigned a weighting coefficient. If the phonetic information about the test speech is available, this is utilized in restricting the codebook search to only several codebook entries which are visually closest to the current phoneme (a visual phoneme similarity matrix is generated for this purpose). Then these weights are used to synthesize the principal components of the face point trajectory. The performance of the algorithm is tested on held-out data, and the synthesized face point trajectories showed a correlation of 0.73 with true face point trajectories.\n",
    ""
   ]
  },
  "hallgren98b_avsp": {
   "authors": [
    [
     "Asa",
     "Hallgren"
    ],
    [
     "Bertil",
     "Lyberg"
    ]
   ],
   "title": "Visual Speech Synthesis With Concatenative Speech",
   "original": "av98_181",
   "page_count": 3,
   "order": 33,
   "p1": "181",
   "pn": "184",
   "abstract": [
    "Today synthetic speech is often based on concatenation of natural speech, i.e. units such as diphones or polyphones are taken from natural speech and are then put together to form any word or sentence. So far there have mainly been two ways of adding a visual modality to such a synthesis: Morphing between single images or concatenating video sequences. In this study, however, a new method is presented where recorded natural movements of points on the face are used to control an animated face.\n",
    ""
   ]
  },
  "kuratate98_avsp": {
   "authors": [
    [
     "Takaaki",
     "Kuratate"
    ],
    [
     "Hani",
     "Yehia"
    ],
    [
     "Eric",
     "Vatikiotis-Bateson"
    ]
   ],
   "title": "Kinematics-Based Synthesis of Realistic Talking Faces",
   "original": "av98_185",
   "page_count": 6,
   "order": 34,
   "p1": "185",
   "pn": "190",
   "abstract": [
    "A method is described for animating talking faces that approach both cosmetic and communicative realism. The animations can be driven directly from a small set of time-varying positions measured on the face. This method of animation provides distinct benefits for both industrial and behavioral research applications, because the kinematic control parameters are easily obtained and are highly correlated with the measurable acoustic and neuromuscular events associated with speech production.\n",
    ""
   ]
  },
  "galanes98_avsp": {
   "authors": [
    [
     "Francisco M.",
     "Galanes"
    ],
    [
     "Jack",
     "Unverferth"
    ],
    [
     "Levent M.",
     "Arslan"
    ],
    [
     "David",
     "Talkin"
    ]
   ],
   "title": "Generation of Lip-Synched Synthetic Faces From Phonetically Clustered Face Movement Data",
   "original": "av98_191",
   "page_count": 4,
   "order": 35,
   "p1": "191",
   "pn": "194",
   "abstract": [
    "In this paper we present a method for generating lip-synched synthetic faces using phonetically clustered data. This method allows us to train lip movement from a database of facial trajectories that have been recorded synchronously with speech data. The whole process is automatic and involves no hand processing of the data once the database has been collected. The main discussion will focus on the analysis of real-life data and the generation of a set of regression trees that will allow us to synthesize speech-related facial movements that can drive a three dimensional model of a face.\n",
    ""
   ]
  },
  "morishima98_avsp": {
   "authors": [
    [
     "Shigeo",
     "Morishima"
    ]
   ],
   "title": "Real-time Talking Head Driven by Voice and its Application to Communication and Entertainment",
   "original": "av98_195",
   "page_count": 5,
   "order": 36,
   "p1": "195",
   "pn": "200",
   "abstract": [
    "Recently computer can make cyberspace to walk through by an interactive virtual reality technique. An avatar in cyberspace can bring us a virtual face-to-face communication environment. In this paper, we realize an avatar which has a real face in cyberspace and construct a multi-user communication system by voice transmission through network. Voice from microphone is transmitted and analyzed, then mouth shape and facial expression of avatar are synchronously estimated and synthesized on real time. and also we introduce an entertainment application of a real-time voice driven synthetic face. This project is named \"Fifteen Seconds of Fame\" which is an example of interactive movie.\n",
    ""
   ]
  },
  "cohen98_avsp": {
   "authors": [
    [
     "Michael M.",
     "Cohen"
    ],
    [
     "Jonas",
     "Beskow"
    ],
    [
     "Dominic W.",
     "Massaro"
    ]
   ],
   "title": "Recent Developments In Facial Animation: An Inside View",
   "original": "av98_201",
   "page_count": 6,
   "order": 37,
   "p1": "201",
   "pn": "206",
   "abstract": [
    "We report on our recent facial animation work to improve the realism and accuracy of visual speech synthesis. The general approach is to use both static and dynamic observations of natural speech to guide the facial modeling. One current goal is to model the internal articulators of a highly realistic palate, teeth, and an improved tongue. Because our talking head can be made transparent, we can provide an anatomically valid and pedagogically useful display that can be used in speech training of children with hearing loss [1]. High-resolution models of palate and teeth [2] were reduced to a relatively small number of polygons for real-time animation [3]. For the improved tongue, we are using 3D ultrasound data and electropalatography (EPG) [4] with error minimization algorithms to educate our parametric B-spline based tongue model to simulate realistic speech. In addition, a high-speed algorithm has been developed for detection and correction of collisions, to prevent the tongue from protruding through the palate and teeth, and to enable the real-time display of synthetic EPG patterns.\n",
    ""
   ]
  },
  "reveret98_avsp": {
   "authors": [
    [
     "Lionel",
     "Reveret"
    ],
    [
     "Christian",
     "Benoit"
    ]
   ],
   "title": "A New 3D Lip Model for Analysis and Synthesis of Lip Motion In Speech Production",
   "original": "av98_207",
   "page_count": 6,
   "order": 38,
   "p1": "207",
   "pn": "212",
   "abstract": [
    "This work presents a methodology for 3D modeling of lip motion in speech production and its application to lip tracking and visual speech animation. Firstly, a geometric modeling allows to create a 3D lip model from 30 control points for any lip shape. Secondly, a statistical analysis, performed on a set of 10 key shapes, generates a lip gesture coding with three articulatory-oriented parameters, specific to one speaker. The choice of the key shapes is based on general phonetic observations. Finally, the application for lip tracking of the 3D model controlled by the three parameters is presented and evaluated.\n",
    ""
   ]
  },
  "brooke98_avsp": {
   "authors": [
    [
     "N. Michael",
     "Brooke"
    ],
    [
     "Simon D.",
     "Scott"
    ]
   ],
   "title": "Two- and Three-Dimensional Audio-Visual Speech Synthesis",
   "original": "av98_213",
   "page_count": 6,
   "order": 39,
   "p1": "213",
   "pn": "220",
   "abstract": [
    "An audio-visual speech synthesiser has been built that will generate animated computer-graphics displays of high-resolution, colour images of a speaker's mouth area. The visual displays can simulate the movements of the lower face of a talker for any spoken sentence of British English, given a text input. The synthesiser is based on a data-driven technique. It uses encoded video-recorded images and sounds of a real speaker to find optimal parameter values for, or 'train', hidden Markov models (or HMMs) that capture both the sounds and facial gestures for each of the speech sounds of British English. To synthesise an utterance, the trained HMMs associated with the speech sounds are invoked in sequence to produce outputs which can be decoded into an image and sound sequence. Whilst the basic image syntheses are two-dimensional, they can be pasted onto a three-dimensional wireframe model of the lower part of a head, which, when the jaw outline is adjusted, produces a plausible three-dimensional visual speech animation.\n",
    ""
   ]
  },
  "tamura98_avsp": {
   "authors": [
    [
     "Masatsune",
     "Tamura"
    ],
    [
     "Takashi",
     "Masuko"
    ],
    [
     "Takao",
     "Kobayashi"
    ],
    [
     "Keiichi",
     "Tokuda"
    ]
   ],
   "title": "Visual Speech Synthesis Based on Parameter Generation From HMM: Speech-Driven and Text-And-Speech-Driven Approaches",
   "original": "av98_221",
   "page_count": 6,
   "order": 40,
   "p1": "221",
   "pn": "224",
   "abstract": [
    "This paper describes a technique for synthesizing synchronized lip movements from auditory input speech signal. The technique is based on an algorithm for parameter generation from HMM with dynamic features, which has been successfully applied to text-to-speech synthesis. Audio-visual speech unit HMMs, namely, syllable HMMs are trained with parameter vector sequences that represent both auditory and visual speech features.  Input speech is recognized using the syllable HMMs and converted into a transcription and a state sequence. A sentence HMM is constructed by concatenating the syllable HMMs corresponding to the transcription for the input speech. Then an optimum visual speech parameter sequence is generated from the sentence HMM in ML sense. Since the generated parameter sequence reflects statistical information of both static and dynamic features of several phonemes before and after the current phonemes, synthetic lip motion becomes smooth and realistic. We show experimental results which demonstrate the effectiveness of the proposed technique.\n",
    ""
   ]
  },
  "yamamoto98_avsp": {
   "authors": [
    [
     "Eli",
     "Yamamoto"
    ],
    [
     "Satoshi",
     "Nakamura"
    ],
    [
     "Kiyohiro",
     "Shikano"
    ]
   ],
   "title": "Subjective Evaluation for HMM-Based Speech-To-Lip Movement Synthesis",
   "original": "av98_227",
   "page_count": 6,
   "order": 41,
   "p1": "227",
   "pn": "232",
   "abstract": [
    "An audio-visual intelligibility score is generally used as an evaluation measure in visual speech synthesis. Especially an intelligibility score of talking heads represents accuracy of facial models[1][2]. The facial models has two stages such as construction of real faces and realization of dynamical human-like motions. We focus on lip movement synthesis from input acoustic speech to realize dynamical motions. The goal of our researchis to synthesize lip movements natural enough to do lip-reading. In previous research, we have proposed a lip movement synthesis method using HMMs which can incorporate a forward coarticulation effect and confirmed its effectiveness through objective evaluation tests. In this paper, subjective evaluation tests are performed. Intelligibility test and acceptability test are conducted for subjective evaluation.\n",
    ""
   ]
  },
  "rubin98_avsp": {
   "authors": [
    [
     "Philip",
     "Rubin"
    ],
    [
     "Eric",
     "Vatikiotis-Bateson"
    ]
   ],
   "title": "Talking Heads",
   "original": "av98_233",
   "page_count": 6,
   "order": 42,
   "p1": "233",
   "pn": "236",
   "abstract": [
    "This paper describes an interactive presentation that introduces the Talking Heads website, which was originally proposed at the AVSP'97 meeting in Rhodes, Greece. Talking Heads is an effort to bring together information from a wide range of sources. The site provides interactive access to multimodal material in both its original form and as summarized by us. In addition, the authors have provided historical information, supporting essays and tutorials, interviews, etc., that try to contextualize and make coherent this rapidly developing area. Both the website and the interactive presentation are described.\n",
    ""
   ]
  }
 },
 "sessions": [
  {
   "title": "McGurk's McGurk",
   "papers": [
    "burnham98_avsp",
    "mcgurk98_avsp"
   ]
  },
  {
   "title": "McGurk Effect",
   "papers": [
    "massaro98_avsp",
    "fixmer98_avsp",
    "sekiyama98_avsp",
    "burnham98b_avsp",
    "amano98_avsp",
    "braida98_avsp",
    "colin98_avsp",
    "hayashi98_avsp"
   ]
  },
  {
   "title": "Automatic Analysis and Recognition",
   "papers": [
    "foucher98_avsp",
    "matthews98_avsp",
    "yang98_avsp",
    "hallgren98_avsp"
   ]
  },
  {
   "title": "Perception",
   "papers": [
    "sams98_avsp",
    "gelder98_avsp",
    "radeau98_avsp",
    "barker98_avsp",
    "bernstein98_avsp",
    "cathiard98_avsp",
    "davis98_avsp",
    "lyxell98_avsp",
    "vroomen98_avsp"
   ]
  },
  {
   "title": "Visual Linguistics",
   "papers": [
    "caldognetto98_avsp",
    "cerrato98_avsp",
    "imaizumi98_avsp"
   ]
  },
  {
   "title": "Speechreading, Cued Speech, and Hearing Impairment",
   "papers": [
    "burnham98c_avsp",
    "gelder98b_avsp",
    "duchnowski98_avsp",
    "gagne98_avsp",
    "lyxell98b_avsp"
   ]
  },
  {
   "title": "Synthesis",
   "papers": [
    "arslan98_avsp",
    "hallgren98b_avsp",
    "kuratate98_avsp",
    "galanes98_avsp",
    "morishima98_avsp",
    "cohen98_avsp",
    "reveret98_avsp",
    "brooke98_avsp",
    "tamura98_avsp",
    "yamamoto98_avsp",
    "rubin98_avsp"
   ]
  }
 ]
}