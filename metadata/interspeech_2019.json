{
  "title": "Interspeech 2019",
  "location": "Graz, Austria",
  "startDate": "15/9/2019",
  "endDate": "19/9/2019",
  "URL": "http://www.interspeech2019.org/",
  "chair": "Chairs: Gernot Kubin and Zdravko Ka\u010di\u010d",
  "conf": "Interspeech",
  "year": "2019",
  "name": "interspeech_2019",
  "series": "Interspeech",
  "SIG": "",
  "title1": "Interspeech 2019",
  "date": "15-19 September 2019",
  "booklet": "interspeech_2019.pdf",
  "papers": {
    "tokuda19_interspeech": {
      "authors": [
        [
          "Keiichi",
          "Tokuda"
        ]
      ],
      "title": "Statistical Approach to Speech Synthesis: Past, Present and Future",
      "original": "abs1",
      "page_count": 0,
      "order": 1,
      "p1": "0",
      "pn": "",
      "abstract": [
        "The basic problem of statistical speech synthesis is quite simple:\nwe have a speech database for training, i.e., a set of speech waveforms\nand corresponding texts; given a text not included in the training\ndata, what is the speech waveform corresponding to the text?  The whole\ntext-to-speech generation process is decomposed into feasible subproblems:\nusually, text analysis, acoustic modeling, and waveform generation,\ncombined as a statistical generative model.  Each submodule can be\nmodeled by a statistical machine learning technique: first, hidden\nMarkov models were applied to acoustic modeling module and then various\ntypes of deep neural networks (DNN) have been applied to not only acoustic\nmodeling module but also other modules.  I will give an overview of\nsuch statistical approaches to speech synthesis, looking back on the\nevolution in the last couple of decades.  Recent DNN-based approaches\ndrastically improved the speech quality, causing a paradigm shift from\nconcatenative speech synthesis approach to generative model-based statistical\napproach.  However, for realizing human-like talking machines, the\ngoal is not only to generate natural-sounding speech but also to flexibly\ncontrol variations in speech, such as speaker identities, speaking\nstyles, emotional expressions, etc.  This talk will also discuss such\nfuture challenges and the direction in speech synthesis Research.\n"
      ]
    },
    "wu19_interspeech": {
      "authors": [
        [
          "Fei",
          "Wu"
        ],
        [
          "Leibny Paola",
          "Garc\u00eda-Perera"
        ],
        [
          "Daniel",
          "Povey"
        ],
        [
          "Sanjeev",
          "Khudanpur"
        ]
      ],
      "title": "Advances in Automatic Speech Recognition for Child Speech Using Factored Time Delay Neural Network",
      "original": "2980",
      "page_count": 5,
      "order": 2,
      "p1": "1",
      "pn": "5",
      "abstract": [
        "Automatic speech recognition (ASR) has shown huge advances in adult\nspeech; however, when the models are tested on child speech, the performance\ndoes not achieve satisfactory word error rates (WER). This is mainly\ndue to the high variance in acoustic features of child speech and the\nlack of clean, labeled corpora. We apply the factored time delay neural\nnetwork (TDNN-F) to the child speech domain, finding that it yields\nbetter performance. To enable our models to handle the different noise\nconditions and extremely small corpora, we augment the original training\ndata by adding noise and reverberation. Compared with conventional\nGMM-HMM and TDNN systems, TDNN-F does better on two widely accessible\ncorpora: CMU Kids and CSLU Kids, and on the combination of these two.\nOur system achieves a 26% relative improvement in WER.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2980",
      "author_area_id": 13,
      "author_area_label": "Special Sessions"
    },
    "yeung19_interspeech": {
      "authors": [
        [
          "Gary",
          "Yeung"
        ],
        [
          "Abeer",
          "Alwan"
        ]
      ],
      "title": "A Frequency Normalization Technique for Kindergarten Speech Recognition Inspired by the Role of f<SUB>o</SUB> in Vowel Perception",
      "original": "1847",
      "page_count": 5,
      "order": 3,
      "p1": "6",
      "pn": "10",
      "abstract": [
        "Accurate automatic speech recognition (ASR) of kindergarten speech\nis particularly important as this age group may benefit the most from\nvoice-based educational tools. Due to the lack of young child speech\ndata, kindergarten ASR systems often are trained using older child\nor adult speech. This study proposes a fundamental frequency (f<SUB>o</SUB>)-based\nnormalization technique to reduce the spectral mismatch between kindergarten\nand older child speech. The technique is based on the tonotopic distances\nbetween formants and f<SUB>o</SUB> developed to model vowel perception.\nThis proposed procedure only relies on the computation of median f<SUB>o</SUB>\nacross an utterance. Tonotopic distances for vowel perception were\nreformulated as a linear relationship between formants and f<SUB>o</SUB>\nto provide an effective approach for frequency normalization. This\nreformulation was verified by examining the formants and f<SUB>o</SUB>\nof child vowel productions. A 208-word ASR experiment using older child\nspeech for training and kindergarten speech for testing was performed\nto examine the effectiveness of the proposed technique against piecewise\nvocal tract length, F3-based, and subglottal resonance normalization\ntechniques. Results suggest that the proposed technique either has\nperformance advantages or requires the computation of fewer parameters.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1847"
    },
    "gale19_interspeech": {
      "authors": [
        [
          "Robert",
          "Gale"
        ],
        [
          "Liu",
          "Chen"
        ],
        [
          "Jill",
          "Dolata"
        ],
        [
          "Jan van",
          "Santen"
        ],
        [
          "Meysam",
          "Asgari"
        ]
      ],
      "title": "Improving ASR Systems for Children with Autism and Language Impairment Using Domain-Focused DNN Transfer Techniques",
      "original": "3161",
      "page_count": 5,
      "order": 4,
      "p1": "11",
      "pn": "15",
      "abstract": [
        "This study explores building and improving an automatic speech recognition\n(ASR) system for children aged 6&#8211;9 years and diagnosed with autism\nspectrum disorder (ASD), language impairment (LI), or both. Working\nwith only 1.5 hours of target data in which children perform the Clinical\nEvaluation of Language Fundamentals Recalling Sentences task, we apply\ndeep neural network (DNN) weight transfer techniques to adapt a large\nDNN model trained on the LibriSpeech corpus of adult speech. To begin,\nwe aim to find the best proportional training rates of the DNN layers.\nOur best configuration yields a 29.38% word error rate (WER). Using\nthis configuration, we explore the effects of quantity and similarity\nof data augmentation in transfer learning. We augment our training\nwith portions of the OGI Kids&#8217; Corpus, adding 4.6 hours of typically\ndeveloping speakers aged kindergarten through 3<SUP>rd</SUP> grade.\nWe find that 2<SUP>nd</SUP> grade data alone &#8212; approximately\nthe mean age of the target data &#8212; outperforms other grades and\nall the sets combined. Doubling the data for 1<SUP>st</SUP>, 2<SUP>nd</SUP>,\nand 3<SUP>rd</SUP> grade, we again compare each grade as well as pairs\nof grades. We find the combination of 1<SUP>st</SUP> and 2<SUP>nd</SUP>\ngrade performs best at a 26.21% WER.\n"
      ],
      "doi": "10.21437/Interspeech.2019-3161",
      "author_area_id": 13,
      "author_area_label": "Special Sessions"
    },
    "ribeiro19_interspeech": {
      "authors": [
        [
          "Manuel Sam",
          "Ribeiro"
        ],
        [
          "Aciel",
          "Eshky"
        ],
        [
          "Korin",
          "Richmond"
        ],
        [
          "Steve",
          "Renals"
        ]
      ],
      "title": "Ultrasound Tongue Imaging for Diarization and Alignment of Child Speech Therapy Sessions",
      "original": "2612",
      "page_count": 5,
      "order": 5,
      "p1": "16",
      "pn": "20",
      "abstract": [
        "We investigate the automatic processing of child speech therapy sessions\nusing ultrasound visual biofeedback, with a specific focus on complementing\nacoustic features with ultrasound images of the tongue for the tasks\nof speaker diarization and time-alignment of target words. For speaker\ndiarization, we propose an ultrasound-based time-domain signal which\nwe call estimated tongue activity. For word-alignment, we augment an\nacoustic model with low-dimensional representations of ultrasound images\nof the tongue, learned by a convolutional neural network. We conduct\nour experiments using the Ultrasuite repository of ultrasound and speech\nrecordings for child speech therapy sessions. For both tasks, we observe\nthat systems augmented with ultrasound data outperform corresponding\nsystems using only the audio signal.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2612",
      "author_area_id": 13,
      "author_area_label": "Special Sessions"
    },
    "loukina19_interspeech": {
      "authors": [
        [
          "Anastassia",
          "Loukina"
        ],
        [
          "Beata Beigman",
          "Klebanov"
        ],
        [
          "Patrick",
          "Lange"
        ],
        [
          "Yao",
          "Qian"
        ],
        [
          "Binod",
          "Gyawali"
        ],
        [
          "Nitin",
          "Madnani"
        ],
        [
          "Abhinav",
          "Misra"
        ],
        [
          "Klaus",
          "Zechner"
        ],
        [
          "Zuowei",
          "Wang"
        ],
        [
          "John",
          "Sabatini"
        ]
      ],
      "title": "Automated Estimation of Oral Reading Fluency During Summer Camp e-Book Reading with MyTurnToRead",
      "original": "2889",
      "page_count": 5,
      "order": 6,
      "p1": "21",
      "pn": "25",
      "abstract": [
        "Use of speech technologies in the classroom is often limited by the\ninferior acoustic conditions as well as other factors that might affect\nthe quality of the recordings. We describe MyTurnToRead, an e-book-based\napp designed to support an interleaved listening and reading experience,\nwhere the child takes turns reading aloud with a virtual partner. The\nchild&#8217;s reading turns are recorded, and processed by an automated\nspeech analysis system in order to provide feedback or track improvement\nin reading skill. We describe the architecture of the speech processing\nback-end and evaluate system performance on the data collected in several\nsummer camps where children used the app on consumer-grade devices\nas part of the camp programming. We show that while the quality of\nthe audio recordings varies greatly, our estimates of student oral\nreading fluency are very good: for example, the correlation between\nASR-based and transcription-based estimates of reading fluency at the\nspeaker level is r=0.93. These are also highly correlated with an external\nmeasure of reading comprehension.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2889",
      "author_area_id": 13,
      "author_area_label": "Special Sessions"
    },
    "lopes19_interspeech": {
      "authors": [
        [
          "Vanessa",
          "Lopes"
        ],
        [
          "Jo\u00e3o",
          "Magalh\u00e3es"
        ],
        [
          "Sofia",
          "Cavaco"
        ]
      ],
      "title": "Sustained Vowel Game: A Computer Therapy Game for Children with Dysphonia",
      "original": "3017",
      "page_count": 5,
      "order": 7,
      "p1": "26",
      "pn": "30",
      "abstract": [
        "Problems in vocal quality are common in 4 to 12-year-old children,\nwhich may affect their health as well as their social interactions\nand development process. The sustained vowel exercise is widely used\nby speech and language pathologists for the child&#8217;s voice recovery\nand vocal re-education. Nonetheless, despite being an important voice\nexercise, it can be a monotonous and tedious activity for children.\nHere, we propose a computer therapy game that uses the sustained vowel\nexercise to motivate children on doing this exercise often. In addition,\nthe game gives visual feedback on the child&#8217;s performance, which\nhelps the child understand how to improve the voice production. The\ngame uses a vowel classification model learned with a support vector\nmachine and Mel frequency cepstral coefficients. A user test with 14\nchildren showed that when using the game, children achieve longer phonation\ntimes than without the game. Also, it shows that the visual feedback\nhelps and motivates children on improving their sustained vowel productions.\n"
      ],
      "doi": "10.21437/Interspeech.2019-3017",
      "author_area_id": 13,
      "author_area_label": "Special Sessions"
    },
    "esposito19_interspeech": {
      "authors": [
        [
          "Anna",
          "Esposito"
        ],
        [
          "Terry",
          "Amorese"
        ],
        [
          "Marialucia",
          "Cuciniello"
        ],
        [
          "Maria Teresa",
          "Riviello"
        ],
        [
          "Antonietta M.",
          "Esposito"
        ],
        [
          "Alda",
          "Troncone"
        ],
        [
          "Gennaro",
          "Cordasco"
        ]
      ],
      "title": "The Dependability of Voice on Elders&#8217; Acceptance of Humanoid Agents",
      "original": "1734",
      "page_count": 5,
      "order": 8,
      "p1": "31",
      "pn": "35",
      "abstract": [
        "The research on ambient assistive technology is concerned with features\nhumanoid agents should show in order to gain user acceptance. However,\ndifferently aged groups may have different requirements. This paper\nis particularly focused on agent&#8217;s voice preferences among elders,\nyoung adults, and adolescents.<br style=\"mso-data-placement:same-cell;\"\n/><br style=\"mso-data-placement:same-cell;\" />\n  To this aim 316 users\norganized in groups of 45/46 subjects of which 3 groups of elders (65+\nyears old), 2 of young adults (aged between 22&#8211;35 years), and\n2 of adolescents (aged between 14&#8211;16 years) were recruited and\nadministered the Virtual Agent Acceptance Questionnaire (VAAQ), after\nwatching video-clips of mute and speaking agents, in order to test\ntheir preferences in terms of willingness to interact, pragmatic and\nhedonic qualities, and attractiveness, of proposed speaking and mute\nagents. In addition, the elders were also tested on listening only\nthe agent&#8217;s. The results suggest that voice is primary for getting\nelder&#8217;s acceptance of virtual humanoid agents in contrast to\nyoung adults and adolescents which accept equally well either mute\nor speaking agents.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1734",
      "author_area_id": 13,
      "author_area_label": "Special Sessions"
    },
    "niebuhr19_interspeech": {
      "authors": [
        [
          "Oliver",
          "Niebuhr"
        ],
        [
          "Uffe",
          "Schjoedt"
        ]
      ],
      "title": "God as Interlocutor &#8212; Real or Imaginary? Prosodic Markers of Dialogue Speech and Expected Efficacy in Spoken Prayer",
      "original": "1193",
      "page_count": 5,
      "order": 9,
      "p1": "36",
      "pn": "40",
      "abstract": [
        "We analyze the phonetic correlates of petitionary prayer in 22 Christian\npractitioners. Our aim is to examine if praying is characterized by\nprosodic markers of dialogue speech and expected efficacy. Three similar\nconditions are compared; 1) requests to God, 2) requests to a human\nrecipient, 3) requests to an imaginary person. We find that making\nrequests to God is clearly distinguishable from making requests to\nboth human and imaginary interlocutors. Requests to God are, unlike\nrequests to an imaginary person, characterized by markers of dialogue\nspeech (as opposed to monologue speech), including, a higher f0 level,\na larger f0 range, and a slower speaking rate. In addition, requests\nto God differ from those made to both human and imaginary persons in\nmarkers of expected efficacy on the part of the speaker. These markers\nare related to a more careful speech production, including almost complete\nlack of hesitations, more pauses, and a much longer speaking time.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1193",
      "author_area_id": 13,
      "author_area_label": "Special Sessions"
    },
    "cohn19_interspeech": {
      "authors": [
        [
          "Michelle",
          "Cohn"
        ],
        [
          "Georgia",
          "Zellou"
        ]
      ],
      "title": "Expressiveness Influences Human Vocal Alignment Toward voice-AI",
      "original": "1368",
      "page_count": 5,
      "order": 10,
      "p1": "41",
      "pn": "45",
      "abstract": [
        "This study explores whether people align to expressive speech spoken\nby a voice-activated artificially intelligent device (voice-AI), specifically\nAmazon&#8217;s Alexa. Participants shadowed words produced by the Alexa\nvoice in two acoustically distinct conditions: &#8220;regular&#8221;\nand &#8220;expressive&#8221;, containing more exaggerated pitch contours\nand longer word durations. Another group of participants rated the\nshadowed items, in an AXB perceptual similarity task, as an assessment\nof overall degree of vocal alignment. Results show greater vocal alignment\ntoward expressive speech produced by the Alexa voice and, furthermore,\nsystematic variation based on speaker gender. Overall, these findings\nhave applications to the field of affective computing in understanding\nhuman responses to synthesized emotional expressiveness.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1368",
      "author_area_id": 13,
      "author_area_label": "Special Sessions"
    },
    "lai19_interspeech": {
      "authors": [
        [
          "Catherine",
          "Lai"
        ],
        [
          "Beatrice",
          "Alex"
        ],
        [
          "Johanna D.",
          "Moore"
        ],
        [
          "Leimin",
          "Tian"
        ],
        [
          "Tatsuro",
          "Hori"
        ],
        [
          "Gianpiero",
          "Francesca"
        ]
      ],
      "title": "Detecting Topic-Oriented Speaker Stance in Conversational Speech",
      "original": "2632",
      "page_count": 5,
      "order": 11,
      "p1": "46",
      "pn": "50",
      "abstract": [
        "Being able to detect topics and speaker stances in conversations is\na key requirement for developing spoken language understanding systems\nthat are personalized and adaptive. In this work, we explore how topic-oriented\nspeaker stance is expressed in conversational speech. To do this, we\npresent a new set of topic and stance annotations of the CallHome corpus\nof spontaneous dialogues. Specifically, we focus on six stances &#8212;\npositivity, certainty, surprise, amusement, interest, and comfort &#8212;\nwhich are useful for characterizing important aspects of a conversation,\nsuch as whether a conversation is going well or not. Based on this,\nwe investigate the use of neural network models for automatically detecting\nspeaker stance from speech in multi-turn, multi-speaker contexts. In\nparticular, we examine how performance changes depending on how input\nfeature representations are constructed and how this is related to\ndialogue structure. Our experiments show that incorporating both lexical\nand acoustic features is beneficial for stance detection. However,\nwe observe variation in whether using hierarchical models for encoding\nlexical and acoustic information improves performance, suggesting that\nsome aspects of speaker stance are expressed more locally than others.\nOverall, our findings highlight the importance of modelling interaction\ndynamics and non-lexical content for stance detection.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2632",
      "author_area_id": 13,
      "author_area_label": "Special Sessions"
    },
    "sebastian19_interspeech": {
      "authors": [
        [
          "Jilt",
          "Sebastian"
        ],
        [
          "Piero",
          "Pierucci"
        ]
      ],
      "title": "Fusion Techniques for Utterance-Level Emotion Recognition Combining Speech and Transcripts",
      "original": "3201",
      "page_count": 5,
      "order": 12,
      "p1": "51",
      "pn": "55",
      "abstract": [
        "In human perception and understanding, a number of different and complementary\ncues are adopted according to different modalities. Various emotional\nstates in communication between humans reflect this variety of cues\nacross modalities. Recent developments in multi-modal emotion recognition\nutilize deep-learning techniques to achieve remarkable performances,\nwith models based on different features suitable for text, audio and\nvision. This work focuses on cross-modal fusion techniques over deep\nlearning models for emotion detection from spoken audio and corresponding\ntranscripts.<br style=\"mso-data-placement:same-cell;\" /><br style=\"mso-data-placement:same-cell;\"\n/>\n  We investigate the use of long short-term memory (LSTM) recurrent\nneural network (RNN) with pre-trained word embedding for text-based\nemotion recognition and convolutional neural network (CNN) with utterance-level\ndescriptors for emotion recognition from speech. Various fusion strategies\nare adopted on these models to yield an overall score for each of the\nemotional categories. Intra-modality dynamics for each emotion is captured\nin the neural network designed for the specific modality. Fusion techniques\nare employed to obtain the inter-modality dynamics. Speaker and session-independent\nexperiments on IEMOCAP multi-modal emotion detection dataset show the\neffectiveness of the proposed approaches. This method yields state-of-the-art\nresults for utterance-level emotion recognition based on speech and\ntext.\n"
      ],
      "doi": "10.21437/Interspeech.2019-3201",
      "author_area_id": 13,
      "author_area_label": "Special Sessions"
    },
    "rajwadi19_interspeech": {
      "authors": [
        [
          "Marvin",
          "Rajwadi"
        ],
        [
          "Cornelius",
          "Glackin"
        ],
        [
          "Julie",
          "Wall"
        ],
        [
          "G\u00e9rard",
          "Chollet"
        ],
        [
          "Nigel",
          "Cannings"
        ]
      ],
      "title": "Explaining Sentiment Classification",
      "original": "2743",
      "page_count": 5,
      "order": 13,
      "p1": "56",
      "pn": "60",
      "abstract": [
        "This paper presents a novel 1-D sentiment classifier trained on the\nbenchmark IMDB dataset. The classifier is a 1-D convolutional neural\nnetwork with repeated convolution and max pooling layers. The main\ncontribution of this work is the demonstration of a deconvolution technique\nfor 1-D convolutional neural networks that is agnostic to specific\narchitecture types. This deconvolution technique enables text classification\nto be explained, a feature that is important for NLP-based decision\nsupport systems, as well as being an invaluable diagnostic tool.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2743",
      "author_area_id": 13,
      "author_area_label": "Special Sessions"
    },
    "kleinlein19_interspeech": {
      "authors": [
        [
          "Ricardo",
          "Kleinlein"
        ],
        [
          "Cristina Luna",
          "Jim\u00e9nez"
        ],
        [
          "Juan Manuel",
          "Montero"
        ],
        [
          "Zoraida",
          "Callejas"
        ],
        [
          "Fernando",
          "Fern\u00e1ndez-Mart\u00ednez"
        ]
      ],
      "title": "Predicting Group-Level Skin Attention to Short Movies from Audio-Based LSTM-Mixture of Experts Models",
      "original": "2799",
      "page_count": 5,
      "order": 14,
      "p1": "61",
      "pn": "65",
      "abstract": [
        "Electrodermal activity (EDA) is a psychophysiological indicator that\ncan be considered a somatic marker of the emotional and attentional\nreaction of subjects towards stimuli like audiovisual content. EDA\nmeasurements are not biased by the cognitive process of giving an opinion\nor a score to characterize the subjective perception, and group-level\nEDA recordings integrate the reaction of an audience, thus reducing\nthe signal noise. This paper contributes to the field of audience&#8217;s\nattention prediction to video content, extending previous novel work\non the use of EDA as ground truth for prediction algorithms. Videos\nare segmented into shorter clips attending to the audience&#8217;s\nincreasing or decreasing attention, and we process videos&#8217; audio\nwaveform to extract meaningful aural embeddings from a VGGish model\npretrained on the Audioset database. While previous similar work on\nattention level prediction using only audio accomplished 69.83% accuracy,\nwe propose a Mixture of Experts approach to train a binary classifier\nthat outperforms the main existing state-of-the-art approaches predicting\nincreasing and decreasing attention levels with 81.76% accuracy. These\nresults confirm the usefulness of providing acoustic features with\na semantic significance, and the convenience of considering experts\nover partitions of the dataset in order to predict group-level attention\nfrom audio.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2799",
      "author_area_id": 13,
      "author_area_label": "Special Sessions"
    },
    "schluter19_interspeech": {
      "authors": [
        [
          "Ralf",
          "Schl\u00fcter"
        ]
      ],
      "title": "Survey Talk: Modeling in Automatic Speech Recognition: Beyond Hidden Markov Models",
      "original": "abs4",
      "page_count": 0,
      "order": 15,
      "p1": "0",
      "pn": "",
      "abstract": [
        "The general architecture and modeling of the state-of-the-art statistical\napproach to automatic speech recognition (ASR) have not been challenged\nsignificantly for decades. The classical statistical approach to ASR\nis based on Bayes decision rule, a separation of acoustic and language\nmodeling, hidden Markov modeling (HMM), and a search organization based\non dynamic programming and hypothesis pruning methods. Even when artificial\nneural networks for acoustic modeling and language modeling started\nto considerably boost ASR performance, the general architecture of\nstate-of-the-art ASR systems was not altered considerably. The hybrid\ndeep neural network (DNN)/HMM approach, together with recurrent long\nshort-term memory (LSTM) neural network language modeling currently\nmarks the state-of-the-art on many tasks, covering a wide range of\ntraining set sizes. However, currently more and more alternative approaches\noccur, moving gradually towards so-called end-to-end approaches. Gradually,\nthese novel end-to-end approaches replace explicit time alignment modeling\nand dedicated search space organization by more implicit, integrated\nneural-network based representations, while also dropping the separation\nbetween acoustic and language modeling. Corresponding approaches show\npromising results, especially using large training sets. In this presentation,\nan overview of current modeling approaches to ASR will be given, including\nvariations of both HMM-based and end-to-end modeling.\n"
      ]
    },
    "pham19_interspeech": {
      "authors": [
        [
          "Ngoc-Quan",
          "Pham"
        ],
        [
          "Thai-Son",
          "Nguyen"
        ],
        [
          "Jan",
          "Niehues"
        ],
        [
          "Markus",
          "M\u00fcller"
        ],
        [
          "Alex",
          "Waibel"
        ]
      ],
      "title": "Very Deep Self-Attention Networks for End-to-End Speech Recognition",
      "original": "2702",
      "page_count": 5,
      "order": 16,
      "p1": "66",
      "pn": "70",
      "abstract": [
        "Recently, end-to-end sequence-to-sequence models for speech recognition\nhave gained significant interest in the research community. While previous\narchitecture choices revolve around time-delay neural networks (TDNN)\nand long short-term memory (LSTM) recurrent neural networks, we propose\nto use self-attention via the Transformer architecture as an alternative.\nOur analysis shows that deep Transformer networks with high learning\ncapacity are able to exceed performance from previous end-to-end approaches\nand even match the conventional hybrid systems. Moreover, we trained\nvery deep models with up to 48 Transformer layers for both encoder\nand decoders combined with stochastic residual connections, which greatly\nimprove generalizability and training efficiency. The resulting models\noutperform all previous end-to-end ASR approaches on the Switchboard\nbenchmark. An ensemble of these models achieve 9.9% and 17.7% WER on\nSwitchboard and CallHome test sets respectively. This finding brings\nour end-to-end models to competitive levels with previous hybrid systems.\nFurther, with model ensembling the Transformers can outperform certain\nhybrid systems, which are more complicated in terms of both structure\nand training procedure.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2702",
      "author_area_id": 8,
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "li19_interspeech": {
      "authors": [
        [
          "Jason",
          "Li"
        ],
        [
          "Vitaly",
          "Lavrukhin"
        ],
        [
          "Boris",
          "Ginsburg"
        ],
        [
          "Ryan",
          "Leary"
        ],
        [
          "Oleksii",
          "Kuchaiev"
        ],
        [
          "Jonathan M.",
          "Cohen"
        ],
        [
          "Huyen",
          "Nguyen"
        ],
        [
          "Ravi Teja",
          "Gadde"
        ]
      ],
      "title": "Jasper: An End-to-End Convolutional Neural Acoustic Model",
      "original": "1819",
      "page_count": 5,
      "order": 17,
      "p1": "71",
      "pn": "75",
      "abstract": [
        "In this paper we report state-of-the-art results on LibriSpeech among\nend-to-end speech recognition models without any external training\ndata. Our model, Jasper, uses only 1D convolutions, batch normalization,\nReLU, dropout, and residual connections. To improve training, we further\nintroduce a new layer-wise optimizer called NovoGrad. Through experiments,\nwe demonstrate that the proposed deep architecture performs as well\nor better than more complex choices. Our deepest Jasper variant uses\n54 convolutional layers. With this architecture, we achieve 2.95% WER\nusing a beam-search decoder with an external neural language model\nand 3.86% WER with a greedy decoder on LibriSpeech test-clean. We also\nreport competitive results on Wall Street Journal and the Hub5&#8217;00\nconversational evaluation datasets.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1819",
      "author_area_id": 8,
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "moritz19_interspeech": {
      "authors": [
        [
          "Niko",
          "Moritz"
        ],
        [
          "Takaaki",
          "Hori"
        ],
        [
          "Jonathan Le",
          "Roux"
        ]
      ],
      "title": "Unidirectional Neural Network Architectures for End-to-End Automatic Speech Recognition",
      "original": "2837",
      "page_count": 5,
      "order": 18,
      "p1": "76",
      "pn": "80",
      "abstract": [
        "In hybrid automatic speech recognition (ASR) systems, neural networks\nare used as acoustic models (AMs) to recognize phonemes that are composed\nto words and sentences using pronunciation dictionaries, hidden Markov\nmodels, and language models, which can be jointly represented by a\nweighted finite state transducer (WFST). The importance of capturing\ntemporal context by an AM has been studied and discussed in prior work.\nIn an end-to-end ASR system, however, all components are merged into\na single neural network, i.e., the breakdown into an AM and the different\nparts of the WFST model is no longer possible. This implies that end-to-end\nneural network architectures have even stronger requirements for processing\nlong contextual information. Bidirectional long short-term memory (BLSTM)\nneural networks have demonstrated state-of-the-art results in end-to-end\nASR but are unsuitable for streaming applications. Latency-controlled\nBLSTMs account for this by limiting the future context seen by the\nbackward directed recurrence using chunk-wise processing. In this paper,\nwe propose two new unidirectional neural network architectures, the\ntime-delay LSTM (TDLSTM) and the parallel time-delayed LSTM (PTDLSTM)\nstreams, which both limit the processing latency to a fixed size and\ndemonstrate significant improvements compared to prior art on a variety\nof ASR tasks.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2837",
      "author_area_id": 8,
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "belinkov19_interspeech": {
      "authors": [
        [
          "Yonatan",
          "Belinkov"
        ],
        [
          "Ahmed",
          "Ali"
        ],
        [
          "James",
          "Glass"
        ]
      ],
      "title": "Analyzing Phonetic and Graphemic Representations in End-to-End Automatic Speech Recognition",
      "original": "2599",
      "page_count": 5,
      "order": 19,
      "p1": "81",
      "pn": "85",
      "abstract": [
        "End-to-end neural network systems for automatic speech recognition\n(ASR) are trained from acoustic features to text transcriptions. In\ncontrast to modular ASR systems, which contain separately-trained components\nfor acoustic modeling, pronunciation lexicon, and language modeling,\nthe end-to-end paradigm is both conceptually simpler and has the potential\nbenefit of training the entire system on the end task. However, such\nneural network models are more opaque: it is not clear how to interpret\nthe role of different parts of the network and what information it\nlearns during training. In this paper, we analyze the learned internal\nrepresentations in an end-to-end ASR model. We evaluate the representation\nquality in terms of several classification tasks, comparing phonemes\nand graphemes, as well as different articulatory features. We study\ntwo languages (English and Arabic) and three datasets, finding remarkable\nconsistency in how different properties are represented in different\nlayers of the deep neural network.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2599",
      "author_area_id": 8,
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "tawara19_interspeech": {
      "authors": [
        [
          "Naohiro",
          "Tawara"
        ],
        [
          "Tetsunori",
          "Kobayashi"
        ],
        [
          "Tetsuji",
          "Ogawa"
        ]
      ],
      "title": "Multi-Channel Speech Enhancement Using Time-Domain Convolutional Denoising Autoencoder",
      "original": "3197",
      "page_count": 5,
      "order": 20,
      "p1": "86",
      "pn": "90",
      "abstract": [
        "This paper investigates the use of time-domain convolutional denoising\nautoencoders (TCDAEs) with multiple channels as a method of speech\nenhancement. In general, denoising autoencoders (DAEs), deep learning\nsystems that map noise-corrupted into clean waveforms, have been shown\nto generate high-quality signals while working in the time domain without\nthe intermediate stage of phase modeling. Convolutional DAEs are one\nof the popular structures which learns a mapping between noise-corrupted\nand clean waveforms with convolutional denoising autoencoder. Multi-channel\nsignals for TCDAEs are promising because the different times of arrival\nof a signal can be directly processed with their convolutional structure,\nUp to this time, TCDAEs have only been applied to single-channel signals.\nThis paper explorers the effectiveness of TCDAEs in a multi-channel\nconfiguration. A multi-channel TCDAEs are evaluated on multi-channel\nspeech enhancement experiments, yielding significant improvement over\nsingle-channel DAEs in terms of signal-to-distortion ratio, perceptual\nevaluation of speech quality (PESQ), and word error rate.\n"
      ],
      "doi": "10.21437/Interspeech.2019-3197",
      "author_area_id": 6,
      "author_area_label": "Speech Coding and Enhancement"
    },
    "tesch19_interspeech": {
      "authors": [
        [
          "Kristina",
          "Tesch"
        ],
        [
          "Robert",
          "Rehr"
        ],
        [
          "Timo",
          "Gerkmann"
        ]
      ],
      "title": "On Nonlinear Spatial Filtering in Multichannel Speech Enhancement",
      "original": "2751",
      "page_count": 5,
      "order": 21,
      "p1": "91",
      "pn": "95",
      "abstract": [
        "Using multiple microphones for speech enhancement allows for exploiting\nspatial information for improved performance. In most cases, the spatial\nfilter is selected to be a linear function of the input as, for example,\nthe minimum variance distortionless response (MVDR) beamformer. For\nnon-Gaussian distributed noise, however, the minimum mean square error\n(MMSE) optimal spatial filter may be nonlinear.<br style=\"mso-data-placement:same-cell;\"\n/><br style=\"mso-data-placement:same-cell;\" />\n  Potentially, such\nnonlinear functional relationships could be learned by deep neural\nnetworks. However, the performance would depend on many parameters\nand the architecture of the neural network. Therefore, in this paper,\nwe more generally analyze the potential benefit of nonlinear spatial\nfilters as a function of the multivariate kurtosis of the noise distribution.<br\nstyle=\"mso-data-placement:same-cell;\" /><br style=\"mso-data-placement:same-cell;\"\n/>\n  The results imply that using a nonlinear spatial filter is only\nworth the effort if the noise data follows a distribution with a multivariate\nkurtosis that is considerably higher than for a Gaussian. In this case,\nwe report a performance difference of up to 2.6 dB segmental signal-to-noise\nratio (SNR) improvement for artificial stationary noise. We observe\nan advantage of 1.2dB for the nonlinear spatial filter over the linear\none even for real-world noise data from the CHiME-3 dataset given oracle\ndata for parameter estimation.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2751",
      "author_area_id": 6,
      "author_area_label": "Speech Coding and Enhancement"
    },
    "martindonas19_interspeech": {
      "authors": [
        [
          "Juan M.",
          "Mart\u00edn-Do\u00f1as"
        ],
        [
          "Jens",
          "Heitkaemper"
        ],
        [
          "Reinhold",
          "Haeb-Umbach"
        ],
        [
          "Angel M.",
          "Gomez"
        ],
        [
          "Antonio M.",
          "Peinado"
        ]
      ],
      "title": "Multi-Channel Block-Online Source Extraction Based on Utterance Adaptation",
      "original": "2244",
      "page_count": 5,
      "order": 22,
      "p1": "96",
      "pn": "100",
      "abstract": [
        "This paper deals with multi-channel speech recognition in scenarios\nwith multiple speakers. Recently, the spectral characteristics of a\ntarget speaker, extracted from an adaptation utterance, have been used\nto guide a neural network mask estimator to focus on that speaker.\nIn this work we present two variants of speaker-aware neural networks,\nwhich exploit both spectral and spatial information to allow better\ndiscrimination between target and interfering speakers. Thus, we introduce\neither a spatial pre-processing prior to the mask estimation or a spatial\nplus spectral speaker characterization block whose output is directly\nfed into the neural mask estimator. The target speaker&#8217;s spectral\nand spatial signature is extracted from an adaptation utterance recorded\nat the beginning of a session. We further adapt the architecture for\nlow-latency processing by means of block-online beamforming that recursively\nupdates the signal statistics. Experimental results show that the additional\nspatial information clearly improves source extraction, in particular\nin the same-gender case, and that our proposal achieves state-of-the-art\nperformance in terms of distortion reduction and recognition accuracy.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2244",
      "author_area_id": 6,
      "author_area_label": "Speech Coding and Enhancement"
    },
    "bagheri19_interspeech": {
      "authors": [
        [
          "Saeed",
          "Bagheri"
        ],
        [
          "Daniele",
          "Giacobello"
        ]
      ],
      "title": "Exploiting Multi-Channel Speech Presence Probability in Parametric Multi-Channel Wiener Filter",
      "original": "2665",
      "page_count": 5,
      "order": 23,
      "p1": "101",
      "pn": "105",
      "abstract": [
        "In this paper, we present a practical implementation of the parametric\nmulti-channel Wiener filter (PMWF) noise reduction algorithm. In particular,\nwe extend on methods that incorporate the multi-channel speech presence\nprobability (MC-SPP) in the PMWF derivation and its output. The use\nof the MC-SPP brings several advantages. Firstly, the MC-SPP allows\nfor better estimates of noise and speech statistics, for which we derive\na direct update of the inverse of the noise power spectral density\n(PSD). Secondly, the MC-SPP is used to control the trade-off parameter\nin PMWF which, with proper tuning, outperforms the traditional approach\nwith a fixed trade-off parameter. Thirdly, the MC-SPP for each frequency-band\nis used to obtain the MMSE estimate of the desired speech signal at\nthe output, where we control the maximum amount of noise reduction\nbased on our application. Experimental results on a large number of\nsimulated scenarios show significant benefits of employing MC-SPP in\nterms of SNR improvements and speech distortion.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2665",
      "author_area_id": 6,
      "author_area_label": "Speech Coding and Enhancement"
    },
    "togami19_interspeech": {
      "authors": [
        [
          "Masahito",
          "Togami"
        ],
        [
          "Tatsuya",
          "Komatsu"
        ]
      ],
      "title": "Variational Bayesian Multi-Channel Speech Dereverberation Under Noisy Environments with Probabilistic Convolutive Transfer Function",
      "original": "1220",
      "page_count": 5,
      "order": 24,
      "p1": "106",
      "pn": "110",
      "abstract": [
        "In this paper, we propose a multi-channel speech dereverberation method\nwhich can reduce reverberation even when acoustic transfer functions\n(ATFs) are time varying under noisy environments. The microphone input\nsignal is modeled as a convolutive mixture in a time-frequency domain\nso as to incorporate late reverberation whose tap length is longer\nthan frame size of short term Fourier transform. To reduce reverberation\neffectively under the time-varying ATF conditions, the proposed method\nextends the deterministic convolutive transfer function (D-CTF) into\na probabilistic convolutive transfer function (P-CTF). A variational\nBayesian framework was applied to approximation of a joint posterior\nprobability density functions of a speech source signal and the ATFs.\nVariational posterior probability density functions and the other parameters\nare iteratively updated so as to maximize an evidence lower bound (ELBO).\nExperimental results when the ATFs are time-varying and there is background\nnoise showed that the proposed method can reduce reverberation more\naccurately than the Weighted Prediction error (WPE) and the Kalman-EM\nfor dereverberation (KEMD).\n"
      ],
      "doi": "10.21437/Interspeech.2019-1220",
      "author_area_id": 6,
      "author_area_label": "Speech Coding and Enhancement"
    },
    "nakatani19_interspeech": {
      "authors": [
        [
          "Tomohiro",
          "Nakatani"
        ],
        [
          "Keisuke",
          "Kinoshita"
        ]
      ],
      "title": "Simultaneous Denoising and Dereverberation for Low-Latency Applications Using Frame-by-Frame Online Unified Convolutional Beamformer",
      "original": "1286",
      "page_count": 5,
      "order": 25,
      "p1": "111",
      "pn": "115",
      "abstract": [
        "This article presents frame-by-frame online processing algorithms for\na Weighted Power minimization Distortionless response convolutional\nbeamformer (WPD). The WPD unifies widely-used multichannel dereverberation\nand denoising methods, namely a weighted prediction error based dereverberation\nmethod (WPE) and a minimum power distortionless response beamformer\n(MPDR) into a single convolutional beamformer, and achieves simultaneous\ndereverberation and denoising based on maximum likelihood estimation.\nWe derive two different online algorithms, one based on frame-by-frame\nrecursive updating of the spatio-temporal covariance matrix of the\ncaptured signal, and the other on recursive least square estimation\nof the convolutional beamformer. In addition, for both algorithms,\nthe desired signal&#8217;s relative transfer function (RTF) is estimated\nby online processing using a neural network based online mask estimation.\nExperiments using the REVERB challenge dataset show the effectiveness\nof both algorithms in terms of objective speech enhancement measures\nand automatic speech recognition (ASR) performance.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1286",
      "author_area_id": 6,
      "author_area_label": "Speech Coding and Enhancement"
    },
    "snyder19_interspeech": {
      "authors": [
        [
          "Cathryn",
          "Snyder"
        ],
        [
          "Michelle",
          "Cohn"
        ],
        [
          "Georgia",
          "Zellou"
        ]
      ],
      "title": "Individual Variation in Cognitive Processing Style Predicts Differences in Phonetic Imitation of Device and Human Voices",
      "original": "2669",
      "page_count": 5,
      "order": 26,
      "p1": "116",
      "pn": "120",
      "abstract": [
        "Phonetic imitation, or implicitly matching the acoustic-phonetic patterns\nof another speaker, has been empirically associated with natural tendencies\nto promote successful social communication, as well as individual differences\nin personality and cognitive processing style. The present study explores\nwhether individual differences in cognitive processing style, as indexed\nby self-reported scored from the Autism-Spectrum Quotient (AQ) questionnaire,\nare linked to the way people imitate the vocal productions by two digital\ndevice voices (i.e., Apple&#8217;s Siri) and two human voices. Subjects\nfirst performed a word shadowing task of human and device voices and\nthen completed the self-administered AQ. We assessed imitation of two\nacoustic properties: f0 and vowel duration. We find that the attention\nto detail and the imagination subscale scores on the AQ mediated degree\nof imitation of f0 and vowel duration, respectively. The findings yield\nnew insight to speech production and perception mechanisms and how\nit interacts with individual cognitive processing style differences.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2669",
      "author_area_id": 1,
      "author_area_label": "Speech Perception, Production and Acquisition"
    },
    "illa19_interspeech": {
      "authors": [
        [
          "Aravind",
          "Illa"
        ],
        [
          "Prasanta Kumar",
          "Ghosh"
        ]
      ],
      "title": "An Investigation on Speaker Specific Articulatory Synthesis with Speaker Independent Articulatory Inversion",
      "original": "2664",
      "page_count": 5,
      "order": 27,
      "p1": "121",
      "pn": "125",
      "abstract": [
        "Estimating speech representations from articulatory movements is known\nas articulatory-to-acoustic forward (AAF) mapping. Typically this mapping\nis learned using directly measured articulatory movement in a subject-specific\nmanner. Such AAF mapping has been shown to benefit the speech synthesis\napplications. In this work, we investigate the speaker similarity and\nnaturalness of utterances generated by AAF which is driven by the articulatory\nmovements from a subject (referred to as cross speaker) different from\nthe speaker (target speaker) used for training AAF mapping. Experiments\nare performed with directly measured articulatory data from 9 speakers\n(8 target speakers and 1 cross speaker), which are recorded using Electromagnetic\narticulograph AG501. Experiments are also performed with articulatory\nfeatures estimated using speaker independent acoustic-to-articulatory\ninversion (SI-AAI) model trained on 26 reference speakers. Objective\nevaluation on target speakers reveal that the articulatory features\nestimated from SI-AAI result in a lower Mel-cepstrum distortion compared\nto that using directly measured articulatory features. Further, listening\ntests reveal that the directly measured articulatory movements preserve\nthe speaker similarity better than estimated ones. Although, for naturalness,\narticulatory movements predicted by SI-AAI perform better than the\ndirect measurements.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2664",
      "author_area_id": 1,
      "author_area_label": "Speech Perception, Production and Acquisition"
    },
    "zhang19_interspeech": {
      "authors": [
        [
          "Xiaohan",
          "Zhang"
        ],
        [
          "Chongke",
          "Bi"
        ],
        [
          "Kiyoshi",
          "Honda"
        ],
        [
          "Wenhuan",
          "Lu"
        ],
        [
          "Jianguo",
          "Wei"
        ]
      ],
      "title": "Individual Difference of Relative Tongue Size and its Acoustic Effects",
      "original": "2452",
      "page_count": 5,
      "order": 28,
      "p1": "126",
      "pn": "130",
      "abstract": [
        "This study examines how the speaker&#8217;s tongue size contributes\nto generating dynamic characteristics of speaker individuality. The\nrelative tongue size (RTS) has been proposed as an index for the tongue\narea within the oropharyngeal cavity on the midsagittal magnetic resonance\nimaging (MRI). Our earlier studies have shown that the smaller the\nRTS, the faster the tongue movement. In this study, acoustic consequences\nof individual RTS values were analyzed by comparing tongue movement\nvelocity and formant transition rate. The materials used were cine-MRI\ndata and acoustic signals during production of a sentence and two words\nproduced by two female speakers with contrasting RTS values. The results\nindicate that the speaker with the small RTS value exhibited the faster\nchanges of tongue positions and formant transitions than the speakers\nwith the large RTS values. Since the tongue size is uncontrollable\nby a speaker&#8217;s intention, the RTS can be regarded as one of the\ncausal factors of dynamic individual characteristics in the lower frequency\nregion of speech signals.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2452",
      "author_area_id": 1,
      "author_area_label": "Speech Perception, Production and Acquisition"
    },
    "yoshinaga19_interspeech": {
      "authors": [
        [
          "Tsukasa",
          "Yoshinaga"
        ],
        [
          "Kazunori",
          "Nozaki"
        ],
        [
          "Shigeo",
          "Wada"
        ]
      ],
      "title": "Individual Differences of Airflow and Sound Generation in the Vocal Tract of Sibilant /s/",
      "original": "1376",
      "page_count": 5,
      "order": 29,
      "p1": "131",
      "pn": "135",
      "abstract": [
        "To clarify the individual differences of flow and sound characteristics\nof sibilant /s/, the large eddy simulation of compressible flow was\napplied to vocal tract geometries of five subjects pronouncing /s/.\nThe vocal tract geometry was extracted by separately collecting images\nof digital dental casts and the vocal tract of /s/. The computational\ngrids were constructed for each geometry, and flow and acoustic fields\nwere predicted by the simulation. Results of the simulation showed\nthat jet flow in the vocal tract was disturbed and fluctuated, and\nthe sound source of /s/ was generated in different place for each subject.\nWith an increment of the jet velocity, not only the overall sound amplitude\nbut also the spectral mean was increased, indicating that the increment\nof the jet velocity contributes to the increase of amplitudes in a\nhigher frequency range among different vocal tract geometries.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1376",
      "author_area_id": 1,
      "author_area_label": "Speech Perception, Production and Acquisition"
    },
    "uttam19_interspeech": {
      "authors": [
        [
          "Shashwat",
          "Uttam"
        ],
        [
          "Yaman",
          "Kumar"
        ],
        [
          "Dhruva",
          "Sahrawat"
        ],
        [
          "Mansi",
          "Aggarwal"
        ],
        [
          "Rajiv Ratn",
          "Shah"
        ],
        [
          "Debanjan",
          "Mahata"
        ],
        [
          "Amanda",
          "Stent"
        ]
      ],
      "title": "Hush-Hush Speak: Speech Reconstruction Using Silent Videos",
      "original": "3269",
      "page_count": 5,
      "order": 30,
      "p1": "136",
      "pn": "140",
      "abstract": [
        "Speech Reconstruction is the task of recreation of speech using silent\nvideos as input. In the literature, it is also referred to as  lipreading.\nIn this paper, we design an encoder-decoder architecture which takes\nsilent videos as input and outputs an audio spectrogram of the reconstructed\nspeech. The model, despite being a speaker-independent model, achieves\ncomparable results on speech reconstruction to the current state-of-the-art\n speaker-dependent model. We also perform user studies to infer speech\nintelligibility. Additionally, we test the usability of the trained\nmodel using bilingual speech.\n"
      ],
      "doi": "10.21437/Interspeech.2019-3269",
      "author_area_id": 1,
      "author_area_label": "Speech Perception, Production and Acquisition"
    },
    "saha19_interspeech": {
      "authors": [
        [
          "Pramit",
          "Saha"
        ],
        [
          "Muhammad",
          "Abdul-Mageed"
        ],
        [
          "Sidney",
          "Fels"
        ]
      ],
      "title": "SPEAK YOUR MIND! Towards Imagined Speech Recognition with Hierarchical Deep Learning",
      "original": "3041",
      "page_count": 5,
      "order": 31,
      "p1": "141",
      "pn": "145",
      "abstract": [
        "Speech-related Brain Computer Interface (BCI) technologies provide\neffective vocal communication strategies for controlling devices through\nspeech commands interpreted from brain signals. In order to infer imagined\nspeech from active thoughts, we propose a novel hierarchical deep learning\nBCI system for subject-independent classification of 11 speech tokens\nincluding phonemes and words. Our novel approach exploits predicted\narticulatory information of six phonological categories (e.g., nasal,\nbilabial) as an intermediate step for classifying the phonemes and\nwords, thereby finding discriminative signal responsible for natural\nspeech synthesis. The proposed network is composed of hierarchical\ncombination of spatial and temporal CNN cascaded with a deep autoencoder.\nOur best models on the KARA database achieve an average accuracy of\n83.42% across the six different binary phonological classification\ntasks, and 53.36% for the individual token identification task, significantly\noutperforming our baselines. Ultimately, our work suggests the possible\nexistence of a brain imagery footprint for the underlying articulatory\nmovement related to different sounds that can be used to aid imagined\nspeech decoding.\n"
      ],
      "doi": "10.21437/Interspeech.2019-3041",
      "author_area_id": 1,
      "author_area_label": "Speech Perception, Production and Acquisition"
    },
    "chung19_interspeech": {
      "authors": [
        [
          "Yu-An",
          "Chung"
        ],
        [
          "Wei-Ning",
          "Hsu"
        ],
        [
          "Hao",
          "Tang"
        ],
        [
          "James",
          "Glass"
        ]
      ],
      "title": "An Unsupervised Autoregressive Model for Speech Representation Learning",
      "original": "1473",
      "page_count": 5,
      "order": 32,
      "p1": "146",
      "pn": "150",
      "abstract": [
        "This paper proposes a novel unsupervised autoregressive neural model\nfor learning generic speech representations. In contrast to other speech\nrepresentation learning methods that aim to remove noise or speaker\nvariabilities, ours is designed to preserve information for a wide\nrange of downstream tasks. In addition, the proposed model does not\nrequire any phonetic or word boundary labels, allowing the model to\nbenefit from large quantities of unlabeled data. Speech representations\nlearned by our model significantly improve performance on both phone\nclassification and speaker verification over the surface features and\nother supervised and unsupervised approaches. Further analysis shows\nthat different levels of speech information are captured by our model\nat different layers. In particular, the lower layers tend to be more\ndiscriminative for speakers, while the upper layers provide more phonetic\ncontent.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1473",
      "author_area_id": 5,
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "huang19_interspeech": {
      "authors": [
        [
          "Feng",
          "Huang"
        ],
        [
          "Peter",
          "Balazs"
        ]
      ],
      "title": "Harmonic-Aligned Frame Mask Based on Non-Stationary Gabor Transform with Application to Content-Dependent Speaker Comparison",
      "original": "1327",
      "page_count": 5,
      "order": 33,
      "p1": "151",
      "pn": "155",
      "abstract": [
        "We propose harmonic-aligned frame mask for speech signals using non-stationary\nGabor transform (NSGT). A frame mask operates on the transfer coefficients\nof a signal and consequently converts the signal into a counterpart\nsignal. It depicts the difference between the two signals. In preceding\nstudies, frame masks based on regular Gabor transform were applied\nto single-note instrumental sound analysis. This study extends the\nframe mask approach to speech signals. For voiced speech, the fundamental\nfrequency is usually changing consecutively over time. We employ NSGT\nwith pitch-dependent and therefore time-varying frequency resolution\nto attain harmonic alignment in the transform domain and hence yield\nharmonic-aligned frame masks for speech signals. We propose to apply\nthe harmonic-aligned frame mask to content-dependent speaker comparison.\nFrame masks, computed from voiced signals of a same vowel but from\ndifferent speakers, were utilized as similarity measures to compare\nand distinguish the speaker identities (SID). Results obtained with\ndeep neural networks demonstrate that the proposed frame mask is valid\nin representing speaker characteristics and shows a potential for SID\napplications in limited data scenarios.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1327",
      "author_area_id": 5,
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "m19_interspeech": {
      "authors": [
        [
          "Gurunath Reddy",
          "M."
        ],
        [
          "K. Sreenivasa",
          "Rao"
        ],
        [
          "Partha Pratim",
          "Das"
        ]
      ],
      "title": "Glottal Closure Instants Detection from Speech Signal by Deep Features Extracted from Raw Speech and Linear Prediction Residual",
      "original": "1981",
      "page_count": 5,
      "order": 34,
      "p1": "156",
      "pn": "160",
      "abstract": [
        "Glottal closure instants (GCI) also called as instants of significant\nexcitation occur during abrupt closure of vocal folds is a well-studied\nproblem for its many potential applications in speech processing. Speech\nsignal or its transformed linear prediction residual (LPR) is the most\npopular signal representations for GCI detection. In this paper, we\npropose a supervised classification based GCI detection method, in\nwhich, we train multiple convolution neural networks to determine the\nsuitable feature representation for efficient GCI detection. Also,\nwe show that the combined model trained with joint acoustic-residual\ndeep features and the model trained with low pass filtered speech significantly\nincreases the detection accuracy. We have manually annotated the speech\nsignal for ground truth GCI using electroglottograph (EGG) as a reference\nsignal. The evaluation results showed that the proposed model trained\nwith very small and less diverse data performs significantly better\nthan the traditional signal processing and most recent data-driven\napproaches.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1981",
      "author_area_id": 5,
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "pascual19_interspeech": {
      "authors": [
        [
          "Santiago",
          "Pascual"
        ],
        [
          "Mirco",
          "Ravanelli"
        ],
        [
          "Joan",
          "Serr\u00e0"
        ],
        [
          "Antonio",
          "Bonafonte"
        ],
        [
          "Yoshua",
          "Bengio"
        ]
      ],
      "title": "Learning Problem-Agnostic Speech Representations from Multiple Self-Supervised Tasks",
      "original": "2605",
      "page_count": 5,
      "order": 35,
      "p1": "161",
      "pn": "165",
      "abstract": [
        "Learning good representations without supervision is still an open\nissue in machine learning, and is particularly challenging for speech\nsignals, which are often characterized by long sequences with a complex\nhierarchical structure. Some recent works, however, have shown that\nit is possible to derive useful speech representations by employing\na self-supervised encoder-discriminator approach. This paper proposes\nan improved self-supervised method, where a single neural encoder is\nfollowed by multiple workers that jointly solve different self-supervised\ntasks. The needed consensus across different tasks naturally imposes\nmeaningful constraints to the encoder, contributing to discover general\nrepresentations and to minimize the risk of learning superficial ones.\nExperiments show that the proposed approach can learn transferable,\nrobust, and problem-agnostic features that carry on relevant information\nfrom the speech signal, such as speaker identity, phonemes, and even\nhigher-level features such as emotional cues. In addition, a number\nof design choices make the encoder easily exportable, facilitating\nits direct usage or adaptation to different problems.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2605",
      "author_area_id": 5,
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "nellore19_interspeech": {
      "authors": [
        [
          "Bhanu Teja",
          "Nellore"
        ],
        [
          "Sri Harsha",
          "Dumpala"
        ],
        [
          "Karan",
          "Nathwani"
        ],
        [
          "Suryakanth V.",
          "Gangashetty"
        ]
      ],
      "title": "Excitation Source and Vocal Tract System Based Acoustic Features for Detection of Nasals in Continuous Speech",
      "original": "2785",
      "page_count": 5,
      "order": 36,
      "p1": "166",
      "pn": "170",
      "abstract": [
        "The aim of the current study is to propose acoustic features for detection\nof nasals in continuous speech. Acoustic features that represent certain\ncharacteristics of speech production are extracted. Features representing\nexcitation source characteristics are extracted using zero frequency\nfiltering method. Features representing vocal tract system characteristics\nare extracted using zero time windowing method.<br style=\"mso-data-placement:same-cell;\"\n/><br style=\"mso-data-placement:same-cell;\" />\n  Feature sets are formed\nby combining certain subsets of the features mentioned above. These\nfeature sets are evaluated for their representativeness of nasals in\ncontinuous speech in three different languages, namely, English, Hindi\nand Telugu. Results show that nasal detection is reliable and consistent\nacross all the languages mentioned above.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2785",
      "author_area_id": 5,
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "chatziagapi19_interspeech": {
      "authors": [
        [
          "Aggelina",
          "Chatziagapi"
        ],
        [
          "Georgios",
          "Paraskevopoulos"
        ],
        [
          "Dimitris",
          "Sgouropoulos"
        ],
        [
          "Georgios",
          "Pantazopoulos"
        ],
        [
          "Malvina",
          "Nikandrou"
        ],
        [
          "Theodoros",
          "Giannakopoulos"
        ],
        [
          "Athanasios",
          "Katsamanis"
        ],
        [
          "Alexandros",
          "Potamianos"
        ],
        [
          "Shrikanth",
          "Narayanan"
        ]
      ],
      "title": "Data Augmentation Using GANs for Speech Emotion Recognition",
      "original": "2561",
      "page_count": 5,
      "order": 37,
      "p1": "171",
      "pn": "175",
      "abstract": [
        "In this work, we address the problem of data imbalance for the task\nof Speech Emotion Recognition (SER). We investigate conditioned data\naugmentation using Generative Adversarial Networks (GANs), in order\nto generate samples for underrepresented emotions. We adapt and improve\na conditional GAN architecture to generate synthetic spectrograms for\nthe minority class. For comparison purposes, we implement a series\nof signal-based data augmentation methods. The proposed GAN-based approach\nis evaluated on two datasets, namely IEMOCAP and FEEL-25k, a large\nmulti-domain dataset. Results demonstrate a 10% relative performance\nimprovement in IEMOCAP and 5% in FEEL-25k, when augmenting the minority\nclasses.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2561",
      "author_area_id": 5,
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "kons19_interspeech": {
      "authors": [
        [
          "Zvi",
          "Kons"
        ],
        [
          "Slava",
          "Shechtman"
        ],
        [
          "Alex",
          "Sorin"
        ],
        [
          "Carmel",
          "Rabinovitz"
        ],
        [
          "Ron",
          "Hoory"
        ]
      ],
      "title": "High Quality, Lightweight and Adaptable TTS Using LPCNet",
      "original": "1705",
      "page_count": 5,
      "order": 38,
      "p1": "176",
      "pn": "180",
      "abstract": [
        "We present a lightweight adaptable neural TTS system with high quality\noutput. The system is composed of three separate neural network blocks:\nprosody prediction, acoustic feature prediction and Linear Prediction\nCoding Net as a neural vocoder. This system can synthesize speech with\nclose to natural quality while running 3 times faster than real-time\non a standard CPU.<br style=\"mso-data-placement:same-cell;\" /><br style=\"mso-data-placement:same-cell;\"\n/>\n  The modular setup of the system allows for simple adaptation to\nnew voices with a small amount of data.<br style=\"mso-data-placement:same-cell;\"\n/><br style=\"mso-data-placement:same-cell;\" />\n  We first demonstrate\nthe ability of the system to produce high quality speech when trained\non large, high quality datasets. Following that, we demonstrate its\nadaptability by mimicking unseen voices using 5 to 20 minutes long\ndatasets with lower recording quality. Large scale Mean Opinion Score\nquality and similarity tests are presented, showing that the system\ncan adapt to unseen voices with quality gap of 0.12 and similarity\ngap of 3% compared to natural speech for male voices and quality gap\nof 0.35 and similarity of gap of 9% for female voices.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1705",
      "author_area_id": 7,
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "lorenzotrueba19_interspeech": {
      "authors": [
        [
          "Jaime",
          "Lorenzo-Trueba"
        ],
        [
          "Thomas",
          "Drugman"
        ],
        [
          "Javier",
          "Latorre"
        ],
        [
          "Thomas",
          "Merritt"
        ],
        [
          "Bartosz",
          "Putrycz"
        ],
        [
          "Roberto",
          "Barra-Chicote"
        ],
        [
          "Alexis",
          "Moinet"
        ],
        [
          "Vatsal",
          "Aggarwal"
        ]
      ],
      "title": "Towards Achieving Robust Universal Neural Vocoding",
      "original": "1424",
      "page_count": 5,
      "order": 39,
      "p1": "181",
      "pn": "185",
      "abstract": [
        "This paper explores the potential universality of neural vocoders.\nWe train a WaveRNN-based vocoder on 74 speakers coming from 17 languages.\nThis vocoder is shown to be capable of generating speech of consistently\ngood quality (98% relative mean MUSHRA when compared to natural speech)\nregardless of whether the input spectrogram comes from a speaker or\nstyle seen during training or from an out-of-domain scenario when the\nrecording conditions are studio-quality. When the recordings show significant\nchanges in quality, or when moving towards non-speech vocalizations\nor singing, the vocoder still significantly outperforms speaker-dependent\nvocoders, but operates at a lower average relative MUSHRA of 75%. These\nresults are shown to be consistent across languages, regardless of\nthem being seen during training (e.g. English or Japanese) or unseen\n(e.g. Wolof, Swahili, Ahmaric).\n"
      ],
      "doi": "10.21437/Interspeech.2019-1424",
      "author_area_id": 7,
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "neekhara19_interspeech": {
      "authors": [
        [
          "Paarth",
          "Neekhara"
        ],
        [
          "Chris",
          "Donahue"
        ],
        [
          "Miller",
          "Puckette"
        ],
        [
          "Shlomo",
          "Dubnov"
        ],
        [
          "Julian",
          "McAuley"
        ]
      ],
      "title": "Expediting TTS Synthesis with Adversarial Vocoding",
      "original": "3099",
      "page_count": 5,
      "order": 40,
      "p1": "186",
      "pn": "190",
      "abstract": [
        "Recent approaches in text-to-speech (TTS) synthesis employ neural network\nstrategies to vocode perceptually-informed spectrogram representations\ndirectly into listenable waveforms. Such vocoding procedures create\na computational bottleneck in modern TTS pipelines. We propose an alternative\napproach which utilizes generative adversarial networks (GANs) to learn\nmappings from perceptually-informed spectrograms to simple magnitude\nspectrograms which can be heuristically vocoded. Through a user study,\nwe show that our approach significantly outperforms na&#239;ve vocoding\nstrategies while being hundreds of times faster than neural network\nvocoders used in state-of-the-art TTS systems. We also show that our\nmethod can be used to achieve state-of-the-art results in unsupervised\nsynthesis of individual words of speech.\n"
      ],
      "doi": "10.21437/Interspeech.2019-3099",
      "author_area_id": 7,
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "mustafa19_interspeech": {
      "authors": [
        [
          "Ahmed",
          "Mustafa"
        ],
        [
          "Arijit",
          "Biswas"
        ],
        [
          "Christian",
          "Bergler"
        ],
        [
          "Julia",
          "Schottenhamml"
        ],
        [
          "Andreas",
          "Maier"
        ]
      ],
      "title": "Analysis by Adversarial Synthesis &#8212; A Novel Approach for Speech Vocoding",
      "original": "1195",
      "page_count": 5,
      "order": 41,
      "p1": "191",
      "pn": "195",
      "abstract": [
        "Classical parametric speech coding techniques provide a compact representation\nfor speech signals. This affords a very low transmission rate but with\na reduced perceptual quality of the reconstructed signals. Recently,\nautoregressive deep generative models such as WaveNet and SampleRNN\nhave been used as speech vocoders to scale up the perceptual quality\nof the reconstructed signals without increasing the coding rate. However,\nsuch models suffer from a very slow signal generation mechanism due\nto their sample-by-sample modelling approach. In this work, we introduce\na new methodology for neural speech vocoding based on generative adversarial\nnetworks (GANs). A fake speech signal is generated from a very compressed\nrepresentation of the glottal excitation using conditional GANs as\na deep generative model. This fake speech is then refined using the\nLPC parameters of the original speech signal to obtain a natural reconstruction.\nThe reconstructed speech waveforms based on this approach show a higher\nperceptual quality than the classical vocoder counterparts according\nto subjective and objective evaluation scores for a dataset of 30 male\nand female speakers. Moreover, the usage of GANs enables to generate\nsignals in one-shot compared to autoregressive generative models. This\nmakes GANs promising for exploration to implement high-quality neural\nvocoders.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1195",
      "author_area_id": 7,
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "wu19b_interspeech": {
      "authors": [
        [
          "Yi-Chiao",
          "Wu"
        ],
        [
          "Tomoki",
          "Hayashi"
        ],
        [
          "Patrick Lumban",
          "Tobing"
        ],
        [
          "Kazuhiro",
          "Kobayashi"
        ],
        [
          "Tomoki",
          "Toda"
        ]
      ],
      "title": "Quasi-Periodic WaveNet Vocoder: A Pitch Dependent Dilated Convolution Model for Parametric Speech Generation",
      "original": "1232",
      "page_count": 5,
      "order": 42,
      "p1": "196",
      "pn": "200",
      "abstract": [
        "In this paper, we propose a quasi-periodic neural network (QPNet) vocoder\nwith a novel network architecture named pitch-dependent dilated convolution\n(PDCNN) to improve the pitch controllability of WaveNet (WN) vocoder.\nThe effectiveness of the WN vocoder to generate high-fidelity speech\nsamples from given acoustic features has been proved recently. However,\nbecause of the fixed dilated convolution and generic network architecture,\nthe WN vocoder hardly generates speech with given F<SUB>0</SUB> values\nwhich are outside the range observed in training data. Consequently,\nthe WN vocoder lacks the pitch controllability which is one of the\nessential capabilities of conventional vocoders. To address this limitation,\nwe propose the PDCNN component which has the time-variant adaptive\ndilation size related to the given F<SUB>0</SUB> values and a cascade\nnetwork structure of the QPNet vocoder to generate quasi-periodic signals\nsuch as speech. Both objective and subjective tests are conducted,\nand the experimental results demonstrate the better pitch controllability\nof the QPNet vocoder compared to the same and double sized WN vocoders\nwhile attaining comparable speech qualities.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1232",
      "author_area_id": 7,
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "tian19_interspeech": {
      "authors": [
        [
          "Xiaohai",
          "Tian"
        ],
        [
          "Eng Siong",
          "Chng"
        ],
        [
          "Haizhou",
          "Li"
        ]
      ],
      "title": "A Speaker-Dependent WaveNet for Voice Conversion with Non-Parallel Data",
      "original": "1514",
      "page_count": 5,
      "order": 43,
      "p1": "201",
      "pn": "205",
      "abstract": [
        "In a typical voice conversion system, vocoder is commonly used for\nspeech-to-features analysis and features-to-speech synthesis. However,\nvocoder can be a source of speech quality degradation. This paper presents\na novel approach to voice conversion using WaveNet for non-parallel\ntraining data. Instead of reconstructing speech with intermediate features,\nthe proposed approach utilizes the WaveNet to map the Phonetic PosteriorGrams\n(PPGs) to the waveform samples directly. In this way, we avoid the\nestimation errors arising from vocoding and feature conversion. Additionally,\nas PPG is assumed to be speaker independent, the proposed approach\nalso reduces the feature mismatch problem in WaveNet vocoder based\nsolutions. Experimental results conducted on the CMU-ARCTIC database\nshow that the proposed approach significantly outperforms the traditional\nvocoder and WaveNet Vocoder baselines in terms of speech quality.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1514",
      "author_area_id": 7,
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "han19_interspeech": {
      "authors": [
        [
          "Kyu J.",
          "Han"
        ],
        [
          "Ramon",
          "Prieto"
        ],
        [
          "Tao",
          "Ma"
        ]
      ],
      "title": "Survey Talk: When Attention Meets Speech Applications: Speech &amp; Speaker Recognition Perspective",
      "original": "abs5",
      "page_count": 0,
      "order": 44,
      "p1": "0",
      "pn": "",
      "abstract": [
        "Attention is to let neural layers pay more attention to what is relevant\nto a given task while giving less attention to what is less important,\nand since its introduction in 2015 for machine translation, has been\nsuccessfully applied to speech applications in a number of different\nforms. This survey presents how the attention mechanisms have been\napplied to speech and speaker recognition tasks. The attention mechanism\nwas firstly applied to sequence-to-sequence speech recognition and\nlater became the critical part of Google&#8217;s well-known Listen,\nAttend and Spell ASR system. In the framework of hybrid DNN/HMM approaches\nor CTC-based ASR systems, the attention mechanisms recently started\nto get more traction in the form of self-attention. In a speaker recognition\nperspective, the attention mechanisms have been utilized to improve\nthe capability of representing speaker characteristics in neural outputs,\nmostly in the form of attentive pooling. In this survey we detail the\nattentive strategies that have been successful in both speech and speaker\nrecognition tasks, and discuss challenging issues in practice.\n"
      ]
    },
    "zhao19_interspeech": {
      "authors": [
        [
          "Ziping",
          "Zhao"
        ],
        [
          "Zhongtian",
          "Bao"
        ],
        [
          "Zixing",
          "Zhang"
        ],
        [
          "Nicholas",
          "Cummins"
        ],
        [
          "Haishuai",
          "Wang"
        ],
        [
          "Bj\u00f6rn W.",
          "Schuller"
        ]
      ],
      "title": "Attention-Enhanced Connectionist Temporal Classification for Discrete Speech Emotion Recognition",
      "original": "1649",
      "page_count": 5,
      "order": 45,
      "p1": "206",
      "pn": "210",
      "abstract": [
        "Discrete  speech emotion recognition (SER), the assignment of a single\nemotion label to an entire speech utterance, is typically performed\nas a sequence-to-label task. This approach, however, is limited, in\nthat it can result in models that do not capture temporal changes in\nthe speech signal, including those indicative of a particular emotion.\nOne potential solution to overcome this limitation is to model SER\nas a sequence-to-sequence task instead. In this regard, we have developed\nan attention-based  bidirectional long short-term memory (BLSTM) neural\nnetwork in combination with a  connectionist temporal classification\n(CTC) objective function (Attention-BLSTM-CTC) for SER. We also assessed\nthe benefits of incorporating two contemporary attention mechanisms,\nnamely component attention and quantum attention, into the CTC framework.\nTo the best of the authors&#8217; knowledge, this is the first time\nthat such a hybrid architecture has been employed for SER.We demonstrated\nthe effectiveness of our approach on the Interactive Emotional Dyadic\nMotion Capture (IEMOCAP) and FAU-Aibo Emotion corpora. The experimental\nresults demonstrate that our proposed model outperforms current state-of-the-art\napproaches.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1649",
      "author_area_id": 3,
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "li19b_interspeech": {
      "authors": [
        [
          "Jeng-Lin",
          "Li"
        ],
        [
          "Chi-Chun",
          "Lee"
        ]
      ],
      "title": "Attentive to Individual: A Multimodal Emotion Recognition Network with Personalized Attention Profile",
      "original": "2044",
      "page_count": 5,
      "order": 46,
      "p1": "211",
      "pn": "215",
      "abstract": [
        "A growing number of human-centered applications benefit from continuous\nadvancements in the emotion recognition technology. Many emotion recognition\nalgorithms have been designed to model multimodal behavior cues to\nachieve high performances. However, most of them do not consider the\nmodulating factors of an individual&#8217;s personal attributes in\nhis/her expressive behaviors. In this work, we propose a Personalized\nAttributes-Aware Attention Network (PAaAN) with a novel personalized\nattention mechanism to perform emotion recognition using speech and\nlanguage cues. The attention profile is learned from embeddings of\nan individual&#8217;s profile, acoustic, and lexical behavior data.\nThe profile embedding is derived using linguistics inquiry word count\ncomputed between the target speaker and a large set of movie scripts.\nOur method achieves the state-of-the-art 70.3% unweighted accuracy\nin a four class emotion recognition task on the IEMOCAP. Further analysis\nreveals that affect-related semantic categories are emphasized differently\nfor each speaker in the corpus showing the effectiveness of our attention\nmechanism for personalization.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2044",
      "author_area_id": 3,
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "gallardoantolin19_interspeech": {
      "authors": [
        [
          "Ascensi\u00f3n",
          "Gallardo-Antol\u00edn"
        ],
        [
          "Juan Manuel",
          "Montero"
        ]
      ],
      "title": "A Saliency-Based Attention LSTM Model for Cognitive Load Classification from Speech",
      "original": "1603",
      "page_count": 5,
      "order": 47,
      "p1": "216",
      "pn": "220",
      "abstract": [
        "Cognitive Load (CL) refers to the amount of mental demand that a given\ntask imposes on an individual&#8217;s cognitive system and it can affect\nhis/her productivity in very high load situations. In this paper, we\npropose an automatic system capable of classifying the CL level of\na speaker by analyzing his/her voice. Our research on this topic goes\ninto two main directions. In the first one, we focus on the use of\nLong Short-Term Memory (LSTM) networks with different weighted pooling\nstrategies for CL level classification. In the second contribution,\nfor overcoming the need of a large amount of training data, we propose\na novel attention mechanism that uses the Kalinli&#8217;s auditory\nsaliency model. Experiments show that our proposal outperforms significantly\nboth, a baseline system based on Support Vector Machines (SVM) and\na LSTM-based system with logistic regression attention model.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1603",
      "author_area_id": 3,
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "mallolragolta19_interspeech": {
      "authors": [
        [
          "Adria",
          "Mallol-Ragolta"
        ],
        [
          "Ziping",
          "Zhao"
        ],
        [
          "Lukas",
          "Stappen"
        ],
        [
          "Nicholas",
          "Cummins"
        ],
        [
          "Bj\u00f6rn W.",
          "Schuller"
        ]
      ],
      "title": "A Hierarchical Attention Network-Based Approach for Depression Detection from Transcribed Clinical Interviews",
      "original": "2036",
      "page_count": 5,
      "order": 48,
      "p1": "221",
      "pn": "225",
      "abstract": [
        "The high prevalence of depression in society has given rise to a need\nfor new digital tools that can aid its early detection. Among other\neffects, depression impacts the use of language. Seeking to exploit\nthis, this work focuses on the detection of depressed and non-depressed\nindividuals through the analysis of linguistic information extracted\nfrom transcripts of clinical interviews with a virtual agent. Specifically,\nwe investigated the advantages of employing hierarchical attention-based\nnetworks for this task. Using Global Vectors (GloVe) pretrained word\nembedding models to extract low-level representations of the words,\nwe compared hierarchical local-global attention networks and hierarchical\ncontextual attention networks. We performed our experiments on the\nDistress Analysis Interview Corpus - Wizard of Oz (DAIC-WoZ) dataset,\nwhich contains audio, visual, and linguistic information acquired from\nparticipants during a clinical session. Our results using the DAIC-WoZ\ntest set indicate that hierarchical contextual attention networks are\nthe most suitable configuration to detect depression from transcripts.\nThe configuration achieves an Unweighted Average Recall (UAR) of .66\nusing the test set, surpassing our baseline, a Recurrent Neural Network\nthat does not use attention.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2036",
      "author_area_id": 3,
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "carmantini19_interspeech": {
      "authors": [
        [
          "Andrea",
          "Carmantini"
        ],
        [
          "Peter",
          "Bell"
        ],
        [
          "Steve",
          "Renals"
        ]
      ],
      "title": "Untranscribed Web Audio for Low Resource Speech Recognition",
      "original": "2623",
      "page_count": 5,
      "order": 49,
      "p1": "226",
      "pn": "230",
      "abstract": [
        "Speech recognition models are highly susceptible to mismatch in the\nacoustic and language domains between the training and the evaluation\ndata. For low resource languages, it is difficult to obtain transcribed\nspeech for target domains, while untranscribed data can be collected\nwith minimal effort. Recently, a method applying lattice-free maximum\nmutual information (LF-MMI) to untranscribed data has been found to\nbe effective for semi-supervised training. However, weaker initial\nmodels and domain mismatch can result in high deletion rates for the\nsemi-supervised model. Therefore, we propose a method to force the\nbase model to overgenerate possible transcriptions, relying on the\nability of LF-MMI to deal with uncertainty. On data from the IARPA\nMATERIAL programme, our new semi-supervised method outperforms the\nstandard semi-supervised method, yielding significant gains when adapting\nfor mismatched bandwidth and domain.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2623",
      "author_area_id": 8,
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "luscher19_interspeech": {
      "authors": [
        [
          "Christoph",
          "L\u00fcscher"
        ],
        [
          "Eugen",
          "Beck"
        ],
        [
          "Kazuki",
          "Irie"
        ],
        [
          "Markus",
          "Kitza"
        ],
        [
          "Wilfried",
          "Michel"
        ],
        [
          "Albert",
          "Zeyer"
        ],
        [
          "Ralf",
          "Schl\u00fcter"
        ],
        [
          "Hermann",
          "Ney"
        ]
      ],
      "title": "RWTH ASR Systems for LibriSpeech: Hybrid vs Attention",
      "original": "1780",
      "page_count": 5,
      "order": 50,
      "p1": "231",
      "pn": "235",
      "abstract": [
        "We present state-of-the-art automatic speech recognition (ASR) systems\nemploying a standard hybrid DNN/HMM architecture compared to an attention-based\nencoder-decoder design for the LibriSpeech task. Detailed descriptions\nof the system development, including model design, pretraining schemes,\ntraining schedules, and optimization approaches are provided for both\nsystem architectures. Both hybrid DNN/HMM and attention-based systems\nemploy bi-directional LSTMs for acoustic modeling/encoding. For language\nmodeling, we employ both LSTM and Transformer based architectures.\nAll our systems are built using RWTH&#8217;s open-source toolkits RASR\nand RETURNN. To the best knowledge of the authors, the results obtained\nwhen training on the full LibriSpeech training set, are the best published\ncurrently, both for the hybrid DNN/HMM and the attention-based systems.\nOur single hybrid system even outperforms previous results obtained\nfrom combining eight single systems. Our comparison shows that on the\nLibriSpeech 960h task, the hybrid DNN/HMM system outperforms the attention-based\nsystem by 15% relative on the clean and 40% relative on the other test\nsets in terms of word error rate. Moreover, experiments on a reduced\n100h-subset of the LibriSpeech training corpus even show a more pronounced\nmargin between the hybrid DNN/HMM and attention-based architectures.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1780"
    },
    "kanda19_interspeech": {
      "authors": [
        [
          "Naoyuki",
          "Kanda"
        ],
        [
          "Shota",
          "Horiguchi"
        ],
        [
          "Ryoichi",
          "Takashima"
        ],
        [
          "Yusuke",
          "Fujita"
        ],
        [
          "Kenji",
          "Nagamatsu"
        ],
        [
          "Shinji",
          "Watanabe"
        ]
      ],
      "title": "Auxiliary Interference Speaker Loss for Target-Speaker Speech Recognition",
      "original": "1126",
      "page_count": 5,
      "order": 51,
      "p1": "236",
      "pn": "240",
      "abstract": [
        "In this paper, we propose a novel auxiliary loss function for target-speaker\nautomatic speech recognition (ASR). Our method automatically extracts\nand transcribes target speaker&#8217;s utterances from a monaural mixture\nof multiple speakers speech given a short sample of the target speaker.\nThe proposed auxiliary loss function attempts to additionally maximize\ninterference speaker ASR accuracy during training. This will regularize\nthe network to achieve a better representation for speaker separation,\nthus achieving better accuracy on the target-speaker ASR. We evaluated\nour proposed method using two-speaker-mixed speech in various signal-to-interference-ratio\nconditions. We first built a strong target-speaker ASR baseline based\non the state-of-the-art lattice-free maximum mutual information. This\nbaseline achieved a word error rate (WER) of 18.06% on the test set\nwhile a normal ASR trained with clean data produced a completely corrupted\nresult (WER of 84.71%). Then, our proposed loss further reduced the\nWER by 6.6% relative to this strong baseline, achieving a WER of 16.87%.\nIn addition to the accuracy improvement, we also showed that the auxiliary\noutput branch for the proposed loss can even be used for a secondary\nASR for interference speakers&#8217; speech.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1126",
      "author_area_id": 8,
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "meng19_interspeech": {
      "authors": [
        [
          "Zhong",
          "Meng"
        ],
        [
          "Yashesh",
          "Gaur"
        ],
        [
          "Jinyu",
          "Li"
        ],
        [
          "Yifan",
          "Gong"
        ]
      ],
      "title": "Speaker Adaptation for Attention-Based End-to-End Speech Recognition",
      "original": "3135",
      "page_count": 5,
      "order": 52,
      "p1": "241",
      "pn": "245",
      "abstract": [
        "We propose three regularization-based speaker adaptation approaches\nto adapt the attention-based encoder-decoder (AED) model with very\nlimited adaptation data from target speakers for end-to-end automatic\nspeech recognition. The first method is Kullback-Leibler divergence\n(KLD) regularization, in which the output distribution of a speaker-dependent\n(SD) AED is forced to be close to that of the speaker-independent (SI)\nmodel by adding a KLD regularization to the adaptation criterion. To\ncompensate for the asymmetric deficiency in KLD regularization, an\nadversarial speaker adaptation (ASA) method is proposed to regularize\nthe deep-feature distribution of the SD AED through the adversarial\nlearning of an auxiliary discriminator and the SD AED. The third approach\nis the multi-task learning, in which an SD AED is trained to jointly\nperform the primary task of predicting a large number of output units\nand an auxiliary task of predicting a small number of output units\nto alleviate the target sparsity issue. Evaluated on a Microsoft short\nmessage dictation task, all three methods are highly effective in adapting\nthe AED model, achieving up to 12.2% and 3.0% word error rate improvement\nover an SI AED trained from 3400 hours data for supervised and unsupervised\nadaptation, respectively.\n"
      ],
      "doi": "10.21437/Interspeech.2019-3135",
      "author_area_id": 8,
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "wang19_interspeech": {
      "authors": [
        [
          "Peidong",
          "Wang"
        ],
        [
          "Jia",
          "Cui"
        ],
        [
          "Chao",
          "Weng"
        ],
        [
          "Dong",
          "Yu"
        ]
      ],
      "title": "Large Margin Training for Attention Based End-to-End Speech Recognition",
      "original": "1680",
      "page_count": 5,
      "order": 53,
      "p1": "246",
      "pn": "250",
      "abstract": [
        "End-to-end speech recognition systems are typically evaluated using\nthe maximum a posterior criterion. Since only one hypothesis is involved\nduring evaluation, the ideal number of hypotheses for training should\nalso be one. In this study, we propose a large margin training scheme\nfor attention based end-to-end speech recognition. Using only one training\nhypothesis, the large margin training strategy achieves the same performance\nas the minimum word error rate criterion using four hypotheses. The\ntheoretical derivation in this study is widely applicable to other\nsequence discriminative criteria such as maximum mutual information.\nIn addition, this paper provides a more succinct formulation of the\nlarge margin concept, paving the road towards a better combination\nof support vector machine and deep neural network. \n"
      ],
      "doi": "10.21437/Interspeech.2019-1680",
      "author_area_id": 8,
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "mac19_interspeech": {
      "authors": [
        [
          "Khoi-Nguyen C.",
          "Mac"
        ],
        [
          "Xiaodong",
          "Cui"
        ],
        [
          "Wei",
          "Zhang"
        ],
        [
          "Michael",
          "Picheny"
        ]
      ],
      "title": "Large-Scale Mixed-Bandwidth Deep Neural Network Acoustic Modeling for Automatic Speech Recognition",
      "original": "2641",
      "page_count": 5,
      "order": 54,
      "p1": "251",
      "pn": "255",
      "abstract": [
        "In automatic speech recognition (ASR), wideband (WB) and narrowband\n(NB) speech signals with different sampling rates typically use separate\nacoustic models. Therefore mixed-bandwidth (MB) acoustic modeling has\nimportant practical values for ASR system deployment. In this paper,\nwe extensively investigate large-scale MB deep neural network acoustic\nmodeling for ASR using 1,150 hours of WB data and 2,300 hours of NB\ndata. We study various MB strategies including downsampling, upsampling\nand bandwidth extension for MB acoustic modeling and evaluate their\nperformance on 8 diverse WB and NB test sets from various application\ndomains. To deal with the large amounts of training data, distributed\ntraining is carried out on multiple GPUs using synchronous data parallelism.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2641",
      "author_area_id": 8,
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "milde19_interspeech": {
      "authors": [
        [
          "Benjamin",
          "Milde"
        ],
        [
          "Chris",
          "Biemann"
        ]
      ],
      "title": "SparseSpeech: Unsupervised Acoustic Unit Discovery with Memory-Augmented Sequence Autoencoders",
      "original": "2938",
      "page_count": 5,
      "order": 55,
      "p1": "256",
      "pn": "260",
      "abstract": [
        "We propose a sparse sequence autoencoder model for unsupervised acoustic\nunit discovery, based on bidirectional LSTM encoders/decoders with\na sparsity-inducing bottleneck. The sparsity layer is based on memory-augmented\nneural networks, with a differentiable embedding memory bank addressed\nfrom the encoder. The decoder reconstructs the encoded input feature\nsequence from an utterance-level context embedding and the bottleneck\nrepresentation. At some time steps, the input to the decoder is randomly\nomitted by applying sequence dropout, forcing the decoder to learn\nabout the temporal structure of the sequence. We propose a bootstrapping\ntraining procedure, after which the network can be trained end-to-end\nwith standard back-propagation. Sparsity of the generated representation\ncan be controlled with a parameter in the proposed loss function. We\nevaluate the units with the ABX discriminability on minimal triphone\npairs and also on entire words. Forcing the network to favor highly\nsparse memory addressings in the memory component yields symbolic-like\nrepresentations of speech that are very compact and still offer better\nABX discriminability than MFCC.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2938",
      "author_area_id": 10,
      "author_area_label": "Speech Recognition: Technologies and Systems for New Applications"
    },
    "ondel19_interspeech": {
      "authors": [
        [
          "Lucas",
          "Ondel"
        ],
        [
          "Hari Krishna",
          "Vydana"
        ],
        [
          "Luk\u00e1\u0161",
          "Burget"
        ],
        [
          "Jan",
          "\u010cernock\u00fd"
        ]
      ],
      "title": "Bayesian Subspace Hidden Markov Model for Acoustic Unit Discovery",
      "original": "2224",
      "page_count": 5,
      "order": 56,
      "p1": "261",
      "pn": "265",
      "abstract": [
        "This work tackles the problem of learning a set of language specific\nacoustic units from unlabeled speech recordings given a set of labeled\nrecordings from other languages. Our approach may be described by the\nfollowing two steps procedure: first the model learns the notion of\nacoustic units from the labelled data and then the model uses its knowledge\nto find new acoustic units on the target language. We implement this\nprocess with the Bayesian Subspace Hidden Markov Model (SHMM), a model\nakin to the Subspace Gaussian Mixture Model (SGMM) where each low dimensional\nembedding represents an acoustic unit rather than just a HMM&#8217;s\nstate. The subspace is trained on 3 languages from the GlobalPhone\ncorpus (German, Polish and Spanish) and the AUs are discovered on the\nTIMIT corpus. Results, measured in equivalent Phone Error Rate, show\nthat this approach significantly outperforms previous HMM based acoustic\nunits discovery systems and compares favorably with the Variational\nAuto Encoder-HMM.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2224",
      "author_area_id": 10,
      "author_area_label": "Speech Recognition: Technologies and Systems for New Applications"
    },
    "higuchi19_interspeech": {
      "authors": [
        [
          "Yosuke",
          "Higuchi"
        ],
        [
          "Naohiro",
          "Tawara"
        ],
        [
          "Tetsunori",
          "Kobayashi"
        ],
        [
          "Tetsuji",
          "Ogawa"
        ]
      ],
      "title": "Speaker Adversarial Training of DPGMM-Based Feature Extractor for Zero-Resource Languages",
      "original": "2052",
      "page_count": 5,
      "order": 57,
      "p1": "266",
      "pn": "270",
      "abstract": [
        "We propose a novel framework for extracting speaker-invariant features\nfor zero-resource languages. A deep neural network (DNN)-based acoustic\nmodel is normalized against speakers via adversarial training: a multi-task\nlearning process trains a shared bottleneck feature to be discriminative\nto phonemes and independent of speakers. However, owing to the absence\nof phoneme labels, zero-resource languages cannot employ adversarial\nmulti-task (AMT) learning for speaker normalization. In this work,\nwe obtain a posteriorgram from a Dirichlet process Gaussian mixture\nmodel (DPGMM) and utilize the posterior vector for supervision of the\nphoneme estimation in the AMT training. The AMT network is designed\nso that the DPGMM posteriorgram itself is embedded in a speaker-invariant\nfeature space. The proposed network is expected to resolve the potential\nproblem that the posteriorgram may lack reliability as a phoneme representation\nif the DPGMM components are intermingled with phoneme and speaker information.\nBased on the Zero Resource Speech Challenges, we conduct phoneme discriminant\nexperiments on the extracted features. The results of the experiments\nshow that the proposed framework extracts discriminative features,\nsuppressing the variety in speakers.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2052",
      "author_area_id": 10,
      "author_area_label": "Speech Recognition: Technologies and Systems for New Applications"
    },
    "prasad19_interspeech": {
      "authors": [
        [
          "Manasa",
          "Prasad"
        ],
        [
          "Daan van",
          "Esch"
        ],
        [
          "Sandy",
          "Ritchie"
        ],
        [
          "Jonas Fromseier",
          "Mortensen"
        ]
      ],
      "title": "Building Large-Vocabulary ASR Systems for Languages Without Any Audio Training Data",
      "original": "1775",
      "page_count": 5,
      "order": 58,
      "p1": "271",
      "pn": "275",
      "abstract": [
        "When building automatic speech recognition (ASR) systems, typically\nsome amount of audio and text data in the target language is needed.\nWhile text data can be obtained relatively easily across many languages,\ntranscribed audio data is challenging to obtain. This presents a barrier\nto making voice technologies available in more languages of the world.\nIn this paper, we present a way to build an ASR system system for a\nlanguage even in the absence of any audio training data in that language\nat all. We do this by simply re-using an existing acoustic model from\na phonologically similar language, without any kind of modification\nor adaptation towards the target language. The basic insight is that,\nif two languages are sufficiently similar in terms of their phonological\nsystem, an acoustic model should hold up relatively well when used\nfor another language. We describe how we tailor our pronunciation models\nto enable such re-use, and show experimental results across a number\nof languages from various language families. We also provide a theoretical\nanalysis of situations in which this approach is likely to work. Our\nresults show that it is possible to achieve less than 20% word error\nrate (WER) using this method. \n"
      ],
      "doi": "10.21437/Interspeech.2019-1775",
      "author_area_id": 10,
      "author_area_label": "Speech Recognition: Technologies and Systems for New Applications"
    },
    "azuh19_interspeech": {
      "authors": [
        [
          "Emmanuel",
          "Azuh"
        ],
        [
          "David",
          "Harwath"
        ],
        [
          "James",
          "Glass"
        ]
      ],
      "title": "Towards Bilingual Lexicon Discovery From Visually Grounded Speech Audio",
      "original": "1718",
      "page_count": 5,
      "order": 59,
      "p1": "276",
      "pn": "280",
      "abstract": [
        "In this paper, we present a method for the discovery of word-like units\nand their approximate translations from visually grounded speech across\nmultiple languages. We first train a neural network model to map images\nand their spoken audio captions in both English and Hindi to a shared,\nmultimodal embedding space. Next, we use this model to segment and\ncluster regions of the spoken captions which approximately correspond\nto words. Finally, we exploit between-cluster similarities in the embedding\nspace to associate English pseudo-word clusters with Hindi pseudo-word\nclusters, and show that many of these cluster pairings capture semantic\ntranslations between English and Hindi words. We present quantitative\ncross-lingual clustering results, as well as qualitative results in\nthe form of a bilingual picture dictionary.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1718",
      "author_area_id": 10,
      "author_area_label": "Speech Recognition: Technologies and Systems for New Applications"
    },
    "feng19_interspeech": {
      "authors": [
        [
          "Siyuan",
          "Feng"
        ],
        [
          "Tan",
          "Lee"
        ]
      ],
      "title": "Improving Unsupervised Subword Modeling via Disentangled Speech Representation Learning and Transformation",
      "original": "1338",
      "page_count": 5,
      "order": 60,
      "p1": "281",
      "pn": "285",
      "abstract": [
        "This study tackles unsupervised subword modeling in the zero-resource\nscenario, learning frame-level speech representation that is phonetically\ndiscriminative and speaker-invariant, using only untranscribed speech\nfor target languages. Frame label acquisition is an essential step\nin solving this problem. High quality frame labels should be in good\nconsistency with golden transcriptions and robust to speaker variation.\nWe propose to improve frame label acquisition in our previously adopted\ndeep neural network-bottleneck feature (DNN-BNF) architecture by applying\nthe factorized hierarchical variational autoencoder (FHVAE). FHVAEs\nlearn to disentangle linguistic content and speaker identity information\nencoded in speech. By discarding or unifying speaker information, speaker-invariant\nfeatures are learned and fed as inputs to DPGMM frame clustering and\nDNN-BNF training. Experiments conducted on ZeroSpeech 2017 show that\nour proposed approaches achieve 2.4% and 0.6% absolute ABX error rate\nreductions in across- and within-speaker conditions, comparing to the\nbaseline DNN-BNF system without applying FHVAEs. Our proposed approaches\nsignificantly outperform vocal tract length normalization in improving\nframe labeling and subword modeling.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1338",
      "author_area_id": 10,
      "author_area_label": "Speech Recognition: Technologies and Systems for New Applications"
    },
    "nissen19_interspeech": {
      "authors": [
        [
          "Shawn",
          "Nissen"
        ],
        [
          "Sharalee",
          "Blunck"
        ],
        [
          "Anita",
          "Dromey"
        ],
        [
          "Christopher",
          "Dromey"
        ]
      ],
      "title": "Listeners&#8217; Ability to Identify the Gender of Preadolescent Children in Different Linguistic Contexts",
      "original": "1865",
      "page_count": 5,
      "order": 61,
      "p1": "286",
      "pn": "290",
      "abstract": [
        "This study evaluated listeners&#8217; ability to identify the gender\nof preadolescent children from speech samples of varying length and\nlinguistic context. The listeners were presented with a total of 190\nspeech samples in four different categories of linguistic context:\nsegments, words, sentences, and discourse. The listeners were instructed\nto evaluate each speech sample and decide whether the speaker was a\nmale or female and rate their level of confidence in their decision.\nResults showed listeners identified the gender of the speakers with\na high degree of accuracy, ranging from 86% to 95%. Significant differences\nin listener judgments were found across the four levels of linguistic\ncontext, with segments having the lowest accuracy (83%) and discourse\nthe highest accuracy (99%). At the segmental level, the listeners&#8217;\nidentification of each speaker&#8217;s gender was greater for vowels\nthan for fricatives, with both types of phoneme being identified at\na rate well above chance. Significant differences in identification\nwere found between the /s/ and /&#643;/ fricatives, but not between\nthe four corner vowels. The perception of gender is likely multifactorial,\nwith listeners possibly using phonetic, prosodic, or stylistic speech\ncues to determine a speaker&#8217;s gender.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1865",
      "author_area_id": 2,
      "author_area_label": "Phonetics, Phonology, and Prosody"
    },
    "ahlers19_interspeech": {
      "authors": [
        [
          "Wiebke",
          "Ahlers"
        ],
        [
          "Philipp",
          "Meer"
        ]
      ],
      "title": "Sibilant Variation in New Englishes: A Comparative Sociophonetic Study of Trinidadian and American English /s(tr)/-Retraction",
      "original": "1821",
      "page_count": 5,
      "order": 62,
      "p1": "291",
      "pn": "295",
      "abstract": [
        "The retraction of /s/, particularly in /str/ clusters, toward [&#643;]\nhas been investigated in British, Australian, and American English\nand shown to be conditioned phonetically and sociolinguistically. To\ndate, however, no research exists on the retraction of /s/ in New Englishes,\nthe nativized Englishes spoken in postcolonial territories like the\nCaribbean. We take up this research gap and present the results of\na large-scale comparative acoustic analysis of /s/-retraction in Trinidadian\nEnglish (TrinE) and American English (AmE), using Center of Gravity\nmeasurements of more than 23,500 sibilants produced by 181 speakers\nfrom two speech corpora.<br style=\"mso-data-placement:same-cell;\" /><br\nstyle=\"mso-data-placement:same-cell;\" />\n  The results show that, in\nTrinE, /str/ is considerably retracted toward [&#643;t&#633;], while\nall other /sC(r)/ clusters are non-retracted and acoustically close\nto singleton /s/; less retracted realizations of /str/ occur across\nword boundaries. Although a statistically significant contrast is overall\nmaintained between /&#643;/ and the sibilant in /str/, there is considerable\noverlap across many speakers. The comparison between TrinE and AmE\nindicates that, while sibilants in TrinE overall show acoustically\nlower values, both varieties have in common that retraction is limited\nto /str/ contexts and significantly larger in younger speakers. The\ndegree of /str/-retraction, however, is overall larger in TrinE than\nAmE.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1821",
      "author_area_id": 2,
      "author_area_label": "Phonetics, Phonology, and Prosody"
    },
    "gubian19_interspeech": {
      "authors": [
        [
          "Michele",
          "Gubian"
        ],
        [
          "Jonathan",
          "Harrington"
        ],
        [
          "Mary",
          "Stevens"
        ],
        [
          "Florian",
          "Schiel"
        ],
        [
          "Paul",
          "Warren"
        ]
      ],
      "title": "Tracking the New Zealand English NEAR/SQUARE Merger Using Functional Principal Components Analysis",
      "original": "2115",
      "page_count": 5,
      "order": 63,
      "p1": "296",
      "pn": "300",
      "abstract": [
        "The focus of the study is the application of functional principal components\nanalysis (FPCA) to a sound change in progress in which the  square\nand  near falling diphthongs are merging in New Zealand English. FPCA\napproximated the trajectory shapes of the first two formant frequencies\n(F1/F2) in a large acoustic database of read New Zealand English speech\nspanning three different age groups and two regions. The derived FPCA\nparameters showed a greater degree of centralisation and monophthongisation\nin  square than in  near. Compatibly with the evidence of an ongoing\nsound change in which  square is shifting towards  near, these shape\ndifferences were more marked for older than for younger/mid-age speakers.\nThere was no effect of region nor of place of articulation of the preceding\nconsonant; there was a trend for the merger to be more advanced in\nlow frequency words. The study underlines the benefits of FPCA for\nquantifying the many types of sound changes involving subtle shifts\nin speech dynamics. In particular, multi-dimensional trajectory shape\ndifferences can be quantified without the need for vowel targets nor\nfor determining the influence of the parameters &#8212; in this case\nof the first two formant frequencies &#8212; independently of each\nother.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2115",
      "author_area_id": 2,
      "author_area_label": "Phonetics, Phonology, and Prosody"
    },
    "gessinger19_interspeech": {
      "authors": [
        [
          "Iona",
          "Gessinger"
        ],
        [
          "Bernd",
          "M\u00f6bius"
        ],
        [
          "Bistra",
          "Andreeva"
        ],
        [
          "Eran",
          "Raveh"
        ],
        [
          "Ingmar",
          "Steiner"
        ]
      ],
      "title": "Phonetic Accommodation in a Wizard-of-Oz Experiment: Intonation and Segments",
      "original": "2445",
      "page_count": 5,
      "order": 64,
      "p1": "301",
      "pn": "305",
      "abstract": [
        "This paper discusses phonetic accommodation of 20 native German speakers\ninteracting with the simulated spoken dialogue system Mirabella in\na Wizard-of-Oz experiment. The study examines intonation of wh-questions\nand pronunciation of allophonic contrasts in German. In a question-and-answer\nexchange with the system, the users produce predominantly falling intonation\npatterns for wh-questions when the system does so as well. The number\nof rising patterns on the part of the users increases significantly\nwhen Mirabella produces questions with rising intonation. In a map\ntask, Mirabella provides information about hidden items while producing\nvariants of two allophonic contrasts which are dispreferred by the\nusers. For the [&#618;&#231;] vs. [&#618;k] contrast in the suffix\n&#x27e8;-ig&#x27e9;, the number of dispreferred variants on the part\nof the users increases significantly during the map task. For the [&#949;&#720;]\nvs. [e&#720;] contrast as a realization of stressed &#x27e8;-&#228;-&#x27e9;,\nsuch a convergence effect is not found on the group level, yet still\noccurs for some individual users. Almost every user converges to the\nsystem to a substantial degree for a subset of the examined features,\nbut we also find maintenance of preferred variants and even occasional\ndivergence. This individual variation is in line with previous findings\nin accommodation research.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2445",
      "author_area_id": 2,
      "author_area_label": "Phonetics, Phonology, and Prosody"
    },
    "niebuhr19b_interspeech": {
      "authors": [
        [
          "Oliver",
          "Niebuhr"
        ],
        [
          "Jan",
          "Michalsky"
        ]
      ],
      "title": "PASCAL and DPA: A Pilot Study on Using Prosodic Competence Scores to Predict Communicative Skills for Team Working and Public Speaking",
      "original": "3034",
      "page_count": 5,
      "order": 65,
      "p1": "306",
      "pn": "310",
      "abstract": [
        "Strong communication skills in public-speaking and team-working exercises\nare associated with specific acoustic-prosodic profiles and strategies.\nWe hypothesize that analyzing and assessing these profiles and strategies\nallows us to predict communicative skills. To that end, we used two\nanalysis methods, one for charismatic and persuasive public speaking\n(PASCAL), and one for cooperative communication (DPA). PASCAL and DPA\ncompetency scores are determined on an acoustic basis for speech recordings\nof 21 students whose task was to co-create, in 7 teams of 3 students,\na fully functioning weather station over 14 weeks in an Electrical\nEngineering project course &#8212; and to jointly write a development\nreport about it afterwards. Results show that the students&#8217; PASCAL\nscores are significantly correlated with both the grade in their final\noral project presentation and the grade of their written report as\nassessed by an independent lecturer group. The DPA scores correlate\nwith better time-management and team working as well as with the quality\nand functionality of the designed product. Explanations for the links\nbetween student performance and acoustic competence scores are discussed.\n"
      ],
      "doi": "10.21437/Interspeech.2019-3034",
      "author_area_id": 2,
      "author_area_label": "Phonetics, Phonology, and Prosody"
    },
    "michalsky19_interspeech": {
      "authors": [
        [
          "Jan",
          "Michalsky"
        ],
        [
          "Heike",
          "Schoormann"
        ],
        [
          "Thomas",
          "Schultze"
        ]
      ],
      "title": "Towards the Prosody of Persuasion in Competitive Negotiation. The Relationship Between f0 and Negotiation Success in Same Sex Sales Tasks",
      "original": "3031",
      "page_count": 5,
      "order": 66,
      "p1": "311",
      "pn": "315",
      "abstract": [
        "Prosodic features play a key role in a speaker&#8217;s persuasive power.\nHowever, previous studies on persuasion have been focused on public\nspeaking and the signaling of leadership, while acoustic studies on\nnegotiation have been primarily concerned with cooperative interactions.\nIn this study we are taking a first step into investigating the role\nof acoustic-prosodic cues in competitive negotiation, focusing on f0\nin same-sex negotiations. Specifically, we ask whether the prosodic\ncorrelates of persuasive speech are comparable for public speaking\nand negotiation. Sixty-two speakers (44f/18m) in 31 same-sex pairs\nparticipated in a competitive task to bargain over the selling price\nof a fictional company. We find a significant correlation between a\nspeaker&#8217;s f0 features and his/her interlocutor&#8217;s concession\nrange. In line with findings from public speaking, greater f0 excursions\nand higher f0 minima correlate with negotiation success. However, while\nthe female speakers also show an expected elevated f0 mean, the opposite\nis the case for male speakers. We propose that in competitive negotiation,\ndisplaying dominance may overrule showing passion in contrast to public\nspeaking, but only for male speakers.\n"
      ],
      "doi": "10.21437/Interspeech.2019-3031",
      "author_area_id": 2,
      "author_area_label": "Phonetics, Phonology, and Prosody"
    },
    "sager19_interspeech": {
      "authors": [
        [
          "Jacob",
          "Sager"
        ],
        [
          "Ravi",
          "Shankar"
        ],
        [
          "Jacob",
          "Reinhold"
        ],
        [
          "Archana",
          "Venkataraman"
        ]
      ],
      "title": "VESUS: A Crowd-Annotated Database to Study Emotion Production and Perception in Spoken English",
      "original": "1413",
      "page_count": 5,
      "order": 67,
      "p1": "316",
      "pn": "320",
      "abstract": [
        "We introduce the Varied Emotion in Syntactically Uniform Speech (VESUS)\nrepository as a new resource for the speech community. VESUS is a lexically\ncontrolled database, in which a semantically neutral script is portrayed\nwith different emotional inflections. In total, VESUS contains over\n250 distinct phrases, each read by ten actors in five emotional states.\nWe use crowd sourcing to obtain ten human ratings for the perceived\nemotional content of each utterance. Our unique database construction\nenables a multitude of scientific and technical explorations. To jumpstart\nthis effort, we provide benchmark performance on three distinct emotion\nrecognition tasks using VESUS: longitudinal speaker analysis, extrapolating\nacross syntactical complexity, and generalization to a new speaker.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1413",
      "author_area_id": 12,
      "author_area_label": "Spoken Language Processing: Translation, Information Retrieval, Summarization, Resources and Evaluation"
    },
    "koh19_interspeech": {
      "authors": [
        [
          "Jia Xin",
          "Koh"
        ],
        [
          "Aqilah",
          "Mislan"
        ],
        [
          "Kevin",
          "Khoo"
        ],
        [
          "Brian",
          "Ang"
        ],
        [
          "Wilson",
          "Ang"
        ],
        [
          "Charmaine",
          "Ng"
        ],
        [
          "Ying-Ying",
          "Tan"
        ]
      ],
      "title": "Building the Singapore English National Speech Corpus",
      "original": "1525",
      "page_count": 5,
      "order": 68,
      "p1": "321",
      "pn": "325",
      "abstract": [
        "The National Speech Corpus (NSC) is the first large-scale Singapore\nEnglish corpus spearheaded by the Info-communications and Media Development\nAuthority of Singapore. It aims to become an important source of open\nspeech data for automatic speech recognition (ASR) research and speech-related\napplications. The first release of the corpus features more than 2000\nhours of orthographically transcribed read speech data designed with\nthe inclusion of locally relevant words. It is available for public\nand commercial use upon request at &#8220;www.imda.gov.sg/nationalspeechcorpus&#8221;,\nunder the Singapore Open Data License. An accompanying lexicon is currently\nin the works and will be published soon. In addition, another 1000\nhours of conversational speech data will be made available in the near\nfuture under the second release of NSC. This paper reports on the development\nand collection process of the read speech and conversational speech\ncorpora.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1525",
      "author_area_id": 12,
      "author_area_label": "Spoken Language Processing: Translation, Information Retrieval, Summarization, Resources and Evaluation"
    },
    "picheny19_interspeech": {
      "authors": [
        [
          "Michael",
          "Picheny"
        ],
        [
          "Zolt\u00e1n",
          "T\u00fcske"
        ],
        [
          "Brian",
          "Kingsbury"
        ],
        [
          "Kartik",
          "Audhkhasi"
        ],
        [
          "Xiaodong",
          "Cui"
        ],
        [
          "George",
          "Saon"
        ]
      ],
      "title": "Challenging the Boundaries of Speech Recognition: The MALACH Corpus",
      "original": "1907",
      "page_count": 5,
      "order": 69,
      "p1": "326",
      "pn": "330",
      "abstract": [
        "There has been huge progress in speech recognition over the last several\nyears. Tasks once thought extremely difficult, such as SWITCHBOARD,\nnow approach levels of human performance. The MALACH corpus (LDC catalog\nLDC2012S05), a 375-Hour subset of a large archive of Holocaust testimonies\ncollected by the Survivors of the Shoah Visual History Foundation,\npresents significant challenges to the speech community. The collection\nconsists of unconstrained, natural speech filled with disfluencies,\nheavy accents, age-related coarticulations, un-cued speaker and language\nswitching, and emotional speech - all still open problems for speech\nrecognition systems. Transcription is challenging even for skilled\nhuman annotators. This paper proposes that the community place focus\non the MALACH corpus to develop speech recognition systems that are\nmore robust with respect to accents, disfluencies and emotional speech.\nTo reduce the barrier for entry, a lexicon and training and testing\nsetups have been created and baseline results using current deep learning\ntechnologies are presented. The metadata has just been released by\nLDC (LDC2019S11). It is hoped that this resource will enable the community\nto build on top of these baselines so that the extremely important\ninformation in these and related oral histories becomes accessible\nto a wider audience.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1907",
      "author_area_id": 12,
      "author_area_label": "Spoken Language Processing: Translation, Information Retrieval, Summarization, Resources and Evaluation"
    },
    "ramteke19_interspeech": {
      "authors": [
        [
          "Pravin Bhaskar",
          "Ramteke"
        ],
        [
          "Sujata",
          "Supanekar"
        ],
        [
          "Pradyoth",
          "Hegde"
        ],
        [
          "Hanna",
          "Nelson"
        ],
        [
          "Venkataraja",
          "Aithal"
        ],
        [
          "Shashidhar G.",
          "Koolagudi"
        ]
      ],
      "title": "NITK Kids&#8217; Speech Corpus",
      "original": "2061",
      "page_count": 5,
      "order": 70,
      "p1": "331",
      "pn": "335",
      "abstract": [
        "This paper introduces speech database for analyzing children&#8217;s\nspeech. The proposed database of children is recorded in Kannada language\n(one of the South Indian languages) from children between age 2.5 to\n6.5 years. The database is named as National Institute of Technology\nKarnataka Kids&#8217; Speech Corpus (NITK Kids&#8217; Speech Corpus).\nThe relevant design considerations for the database collection are\ndiscussed in detail. It is divided into four age groups with an interval\nof 1 year between each age group. The speech corpus includes nearly\n10 hours of speech recordings from 160 children. For each age range,\nthe data is recorded from 40 children (20 male and 20 female). Further,\nthe effect of developmental changes on the speech from 2.5 to 6.5 years\nare analyzed using pitch and formant analysis. Some of the potential\napplications, of the NITK Kids&#8217; Speech Corpus, such as, systematic\nstudy on the language learning ability of children, phonological process\nanalysis and children speech recognition are discussed.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2061",
      "author_area_id": 12,
      "author_area_label": "Spoken Language Processing: Translation, Information Retrieval, Summarization, Resources and Evaluation"
    },
    "ali19_interspeech": {
      "authors": [
        [
          "Ahmed",
          "Ali"
        ],
        [
          "Salam",
          "Khalifa"
        ],
        [
          "Nizar",
          "Habash"
        ]
      ],
      "title": "Towards Variability Resistant Dialectal Speech Evaluation",
      "original": "2692",
      "page_count": 5,
      "order": 71,
      "p1": "336",
      "pn": "340",
      "abstract": [
        "We study the problem of evaluating automatic speech recognition (ASR)\nsystems that target dialectal speech input. A major challenge in this\ncase is that the orthography of dialects is typically not standardized.\nFrom an ASR evaluation perspective, this means that there is no clear\ngold standard for the expected output, and several possible outputs\ncould be considered correct according to different human annotators,\nwhich makes standard word error rate (WER) inadequate as an evaluation\nmetric. Specifically targeting the case of Arabic dialects, which are\nalso morphologically rich and complex, we propose a number of alternative\nWER-based metrics that vary in terms of text representation, including\ndifferent degrees of morphological abstraction and spelling normalization.We\nevaluate the efficacy of these metrics by comparing their correlation\nwith human judgments on a validation set of 1,000 utterances. Our results\nshow that the use of morphological abstractions and spelling normalization\nproduces systems with higher correlation with human judgment. We released\nthe code and the datasets to the research community.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2692",
      "author_area_id": 12,
      "author_area_label": "Spoken Language Processing: Translation, Information Retrieval, Summarization, Resources and Evaluation"
    },
    "fallgren19_interspeech": {
      "authors": [
        [
          "Per",
          "Fallgren"
        ],
        [
          "Zofia",
          "Malisz"
        ],
        [
          "Jens",
          "Edlund"
        ]
      ],
      "title": "How to Annotate 100 Hours in 45 Minutes",
      "original": "1648",
      "page_count": 5,
      "order": 72,
      "p1": "341",
      "pn": "345",
      "abstract": [
        "Speech data found in the wild hold many advantages over artificially\nconstructed speech corpora in terms of ecological validity and cultural\nworth. Perhaps most importantly, there is a lot of it. However, the\ncombination of great quantity, noisiness and variation poses a challenge\nfor its access and processing. Generally speaking, automatic approaches\nto tackle the problem require good labels for training, while manual\napproaches require time. In this study, we provide further evidence\nfor a semi-supervised, human-in-the-loop framework that previously\nhas shown promising results for browsing and annotating large quantities\nof found audio data quickly. The findings of this study show that a\n100-hour long subset of the Fearless Steps corpus can be annotated\nfor speech activity in less than 45 minutes, a fraction of the time\nit would take traditional annotation methods, without a loss in performance.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1648",
      "author_area_id": 12,
      "author_area_label": "Spoken Language Processing: Translation, Information Retrieval, Summarization, Resources and Evaluation"
    },
    "diez19_interspeech": {
      "authors": [
        [
          "Mireia",
          "Diez"
        ],
        [
          "Luk\u00e1\u0161",
          "Burget"
        ],
        [
          "Shuai",
          "Wang"
        ],
        [
          "Johan",
          "Rohdin"
        ],
        [
          "Jan",
          "\u010cernock\u00fd"
        ]
      ],
      "title": "Bayesian HMM Based x-Vector Clustering for Speaker Diarization",
      "original": "2813",
      "page_count": 5,
      "order": 73,
      "p1": "346",
      "pn": "350",
      "abstract": [
        "This paper presents a simplified version of the previously proposed\ndiarization algorithm based on Bayesian Hidden Markov Models, which\nuses Variational Bayesian inference for very fast and robust clustering\nof x-vector (neural network based speaker embeddings). The presented\nresults show that this clustering algorithm provides significant improvements\nin diarization performance as compared to the previously used Agglomerative\nHierarchical Clustering. The output of this system can be further employed\nas an initialization for a second stage VB diarization system, using\nframe-wise MFCC features as input, to obtain optimal results.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2813",
      "author_area_id": 4,
      "author_area_label": "Speaker and Language Identification"
    },
    "vestman19_interspeech": {
      "authors": [
        [
          "Ville",
          "Vestman"
        ],
        [
          "Kong Aik",
          "Lee"
        ],
        [
          "Tomi H.",
          "Kinnunen"
        ],
        [
          "Takafumi",
          "Koshinaka"
        ]
      ],
      "title": "Unleashing the Unused Potential of i-Vectors Enabled by GPU Acceleration",
      "original": "1955",
      "page_count": 5,
      "order": 74,
      "p1": "351",
      "pn": "355",
      "abstract": [
        "Speaker embeddings are continuous-value vector representations that\nallow easy comparison between voices of speakers with simple geometric\noperations. Among others, i-vector and x-vector have emerged as the\nmainstream methods for speaker embedding. In this paper, we illustrate\nthe use of modern computation platform to harness the benefit of GPU\nacceleration for i-vector extraction. In particular, we achieve an\nacceleration of 3000 times in frame posterior computation compared\nto real time and 25 times in training the i-vector extractor compared\nto the CPU baseline from Kaldi toolkit. This significant speed-up allows\nthe exploration of ideas that were hitherto impossible. In particular,\nwe show that it is beneficial to update the universal background model\n(UBM) and re-compute frame alignments while training the i-vector extractor.\nAdditionally, we are able to study different variations of i-vector\nextractors more rigorously than before. In this process, we reveal\nsome undocumented details of Kaldi&#8217;s i-vector extractor and show\nthat it outperforms the standard formulation by a margin of 1 to 2%\nwhen tested with VoxCeleb speaker verification protocol. All of our\nfindings are asserted by ensemble averaging the results from multiple\nruns with random start.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1955",
      "author_area_id": 4,
      "author_area_label": "Speaker and Language Identification"
    },
    "shon19_interspeech": {
      "authors": [
        [
          "Suwon",
          "Shon"
        ],
        [
          "Najim",
          "Dehak"
        ],
        [
          "Douglas",
          "Reynolds"
        ],
        [
          "James",
          "Glass"
        ]
      ],
      "title": "MCE 2018: The 1st Multi-Target Speaker Detection and Identification Challenge Evaluation",
      "original": "1572",
      "page_count": 5,
      "order": 75,
      "p1": "356",
      "pn": "360",
      "abstract": [
        "The Multi-target Challenge aims to assess how well current speech technology\nis able to determine whether or not a recorded utterance was spoken\nby one of a large number of blacklisted speakers. It is a form of multi-target\nspeaker detection based on real-world telephone conversations. Data\nrecordings are generated from call center customer-agent conversations.\nThe task is to measure how accurately one can detect 1) whether a test\nrecording is spoken by a blacklisted speaker, and 2) which specific\nblacklisted speaker was talking. This paper outlines the challenge\nand provides its baselines, results, and discussions.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1572",
      "author_area_id": 4,
      "author_area_label": "Speaker and Language Identification"
    },
    "gao19_interspeech": {
      "authors": [
        [
          "Zhifu",
          "Gao"
        ],
        [
          "Yan",
          "Song"
        ],
        [
          "Ian",
          "McLoughlin"
        ],
        [
          "Pengcheng",
          "Li"
        ],
        [
          "Yiheng",
          "Jiang"
        ],
        [
          "Li-Rong",
          "Dai"
        ]
      ],
      "title": "Improving Aggregation and Loss Function for Better Embedding Learning in End-to-End Speaker Verification System",
      "original": "1489",
      "page_count": 5,
      "order": 76,
      "p1": "361",
      "pn": "365",
      "abstract": [
        "Deep embedding learning based speaker verification (SV) methods have\nrecently achieved significant performance improvement over traditional\ni-vector systems, especially for short duration utterances. Embedding\nlearning commonly consists of three components: frame-level feature\nprocessing, utterance-level embedding learning, and loss function to\ndiscriminate between speakers. For the learned embeddings, a back-end\nmodel (i.e., Linear Discriminant Analysis followed by Probabilistic\nLinear Discriminant Analysis (LDA-PLDA)) is generally applied as a\nsimilarity measure. In this paper, we propose to further improve the\neffectiveness of deep embedding learning methods in the following components:\n(1) A multi-stage aggregation strategy, exploited to hierarchically\nfuse time-frequency context information for effective frame-level feature\nprocessing. (2) A discriminant analysis loss is designed for end-to-end\ntraining, which aims to explicitly learn the discriminative embeddings,\ni.e. with small intra-speaker and large inter-speaker variances. To\nevaluate the effectiveness of the proposed improvements, we conduct\nextensive experiments on the VoxCeleb1 dataset. The results outperform\nstate-of-the-art systems by a significant margin. It is also worth\nnoting that the results are obtained using a simple cosine metric instead\nof the more complex LDA-PLDA backend scoring.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1489",
      "author_area_id": 4,
      "author_area_label": "Speaker and Language Identification"
    },
    "lin19_interspeech": {
      "authors": [
        [
          "Qingjian",
          "Lin"
        ],
        [
          "Ruiqing",
          "Yin"
        ],
        [
          "Ming",
          "Li"
        ],
        [
          "Herv\u00e9",
          "Bredin"
        ],
        [
          "Claude",
          "Barras"
        ]
      ],
      "title": "LSTM Based Similarity Measurement with Spectral Clustering for Speaker Diarization",
      "original": "1388",
      "page_count": 5,
      "order": 77,
      "p1": "366",
      "pn": "370",
      "abstract": [
        "More and more neural network approaches have achieved considerable\nimprovement upon submodules of speaker diarization system, including\nspeaker change detection and segment-wise speaker embedding extraction.\nStill, in the clustering stage, traditional algorithms like probabilistic\nlinear discriminant analysis (PLDA) are widely used for scoring the\nsimilarity between two speech segments. In this paper, we propose a\nsupervised method to measure the similarity matrix between all segments\nof an audio recording with sequential bidirectional long short-term\nmemory networks (Bi-LSTM). Spectral clustering is applied on top of\nthe similarity matrix to further improve the performance. Experimental\nresults show that our system significantly outperforms the state-of-the-art\nmethods and achieves a diarization error rate of 6.63% on the NIST\nSRE 2000 CALLHOME database.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1388"
    },
    "chung19b_interspeech": {
      "authors": [
        [
          "Joon Son",
          "Chung"
        ],
        [
          "Bong-Jin",
          "Lee"
        ],
        [
          "Icksang",
          "Han"
        ]
      ],
      "title": "Who Said That?: Audio-Visual Speaker Diarisation of Real-World Meetings",
      "original": "3116",
      "page_count": 5,
      "order": 78,
      "p1": "371",
      "pn": "375",
      "abstract": [
        "The goal of this work is to determine &#8216;who spoke when&#8217;\nin real-world meetings. The method takes surround-view video and single\nor multi-channel audio as inputs, and generates robust diarisation\noutputs.<br style=\"mso-data-placement:same-cell;\" /><br style=\"mso-data-placement:same-cell;\"\n/>\n  To achieve this, we propose a novel iterative approach that first\nenrolls speaker models using audio-visual correspondence, then uses\nthe enrolled models together with the visual information to determine\nthe active speaker.<br style=\"mso-data-placement:same-cell;\" /><br\nstyle=\"mso-data-placement:same-cell;\" />\n  We show strong quantitative\nand qualitative performance on a dataset of real-world meetings. The\nmethod is also evaluated on the public AMI meeting corpus, on which\nwe demonstrate results that exceed all comparable methods. We also\nshow that beamforming can be used together with the video to further\nimprove the performance when multi-channel audio is available.\n"
      ],
      "doi": "10.21437/Interspeech.2019-3116",
      "author_area_id": 4,
      "author_area_label": "Speaker and Language Identification"
    },
    "xie19_interspeech": {
      "authors": [
        [
          "Jiamin",
          "Xie"
        ],
        [
          "Leibny Paola",
          "Garc\u00eda-Perera"
        ],
        [
          "Daniel",
          "Povey"
        ],
        [
          "Sanjeev",
          "Khudanpur"
        ]
      ],
      "title": "Multi-PLDA Diarization on Children&#8217;s Speech",
      "original": "2961",
      "page_count": 5,
      "order": 79,
      "p1": "376",
      "pn": "380",
      "abstract": [
        "Children&#8217;s speech and other vocalizations pose challenges for\nspeaker diarization. The spontaneity of kids causes rapid or delayed\nphonetic variations in an utterance, which makes speaker&#8217;s information\ndifficult to extract. Fast speaker turns and long overlap in conversations\nbetween children and their guardians makes correct segmentation even\nharder compared to, say a business meeting. In this work, we explore\ndiarization of child-guardian interactions. We investigate the effectiveness\nof adding children&#8217;s speech to adult data in Probabilistic Linear\nDiscriminant Analysis (PLDA) training. We also train each of two PLDAs\nwith separate objective to a coarse or fine classification of speakers.\nA fusion of the two PLDAs is examined. By performing this fusion, we\nexpect to improve on children&#8217;s speech while preserving adult\nsegmentations. Our experimental results show that including children&#8217;s\nspeech helps reduce DER by 2.7%, achieving a best overall DER of 33.1%\nwith the x-vector system. A fusion system yields a reasonable 33.3%\nDER that validates our concept.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2961",
      "author_area_id": 4,
      "author_area_label": "Speaker and Language Identification"
    },
    "mccree19_interspeech": {
      "authors": [
        [
          "Alan",
          "McCree"
        ],
        [
          "Gregory",
          "Sell"
        ],
        [
          "Daniel",
          "Garcia-Romero"
        ]
      ],
      "title": "Speaker Diarization Using Leave-One-Out Gaussian PLDA Clustering of DNN Embeddings",
      "original": "2912",
      "page_count": 5,
      "order": 80,
      "p1": "381",
      "pn": "385",
      "abstract": [
        "Many modern systems for speaker diarization, such as the top-performing\nJHU system in the DIHARD 2018 challenge, rely on clustering of DNN\nspeaker embeddings followed by HMM resegmentation. Two problems with\nthis approach are that parameters need significant retuning for different\napplications, and that the DNN contributes only to the clustering task\nand not the resegmentation. This paper presents two contributions:\nan improved HMM segment assignment algorithm using leave-one-out Gaussian\nPLDA scoring, and an approach to training the DNN such that embeddings\ndirectly optimize performance of this scoring method with generatively\nupdated PLDA parameters. Initial experiments with this new system are\nvery promising, achieving state-of-the-art performance for two separate\ntasks (Callhome and DIHARD18) without any task-dependent parameter\ntuning.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2912",
      "author_area_id": 4,
      "author_area_label": "Speaker and Language Identification"
    },
    "ghahabi19_interspeech": {
      "authors": [
        [
          "Omid",
          "Ghahabi"
        ],
        [
          "Volker",
          "Fischer"
        ]
      ],
      "title": "Speaker-Corrupted Embeddings for Online Speaker Diarization",
      "original": "2756",
      "page_count": 5,
      "order": 81,
      "p1": "386",
      "pn": "390",
      "abstract": [
        "Speaker diarization is more challenging in presence of background noise\nor music, frequent speaker changes, and cross talks. In an online scenario,\nthe decision should be made at time, given only the current short segment\nand the speakers detected in the past, which makes the task even harder.\nIn this work, an online robust speaker diarization algorithm is proposed\nin which speech segments are represented by low dimensional vectors\nreferred to as speaker-corrupted embeddings. The proposed speaker embedding\nnetwork is a deep neural network which takes speaker-corrupted supervectors\nas input, uses variable ReLU (VReLU) as an activation function, and\ntries to discriminate the background speakers. Speaker corruption is\nperformed by adding supervectors built by 20 speech frames from other\nspeakers to the supervectors of a given speaker. It is shown that speaker\ncorruption, VReLU, and input dropout increase the generalization power\nof the proposed network. To increase the robustness, the proposed embeddings\nare concatenated with LDA transformed supervectors. Experimental results\non the Albayzin 2018 evaluation set show a competitive accuracy, more\nrobustness, and much lower computational cost compared to typical offline\nalgorithms.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2756",
      "author_area_id": 4,
      "author_area_label": "Speaker and Language Identification"
    },
    "park19_interspeech": {
      "authors": [
        [
          "Tae Jin",
          "Park"
        ],
        [
          "Kyu J.",
          "Han"
        ],
        [
          "Jing",
          "Huang"
        ],
        [
          "Xiaodong",
          "He"
        ],
        [
          "Bowen",
          "Zhou"
        ],
        [
          "Panayiotis",
          "Georgiou"
        ],
        [
          "Shrikanth",
          "Narayanan"
        ]
      ],
      "title": "Speaker Diarization with Lexical Information",
      "original": "1947",
      "page_count": 5,
      "order": 82,
      "p1": "391",
      "pn": "395",
      "abstract": [
        "This work presents a novel approach for speaker diarization to leverage\nlexical information provided by automatic speech recognition. We propose\na speaker diarization system that can incorporate word-level speaker\nturn probabilities with speaker embeddings into a speaker clustering\nprocess to improve the overall diarization accuracy. To integrate lexical\nand acoustic information in a comprehensive way during clustering,\nwe introduce an adjacency matrix integration for spectral clustering.\nSince words and word boundary information for word-level speaker turn\nprobability estimation are provided by a speech recognition system,\nour proposed method works without any human intervention for manual\ntranscriptions. We show that the proposed method improves diarization\nperformance on various evaluation datasets compared to the baseline\ndiarization system using acoustic information only in speaker embeddings.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1947",
      "author_area_id": 4,
      "author_area_label": "Speaker and Language Identification"
    },
    "shafey19_interspeech": {
      "authors": [
        [
          "Laurent El",
          "Shafey"
        ],
        [
          "Hagen",
          "Soltau"
        ],
        [
          "Izhak",
          "Shafran"
        ]
      ],
      "title": "Joint Speech Recognition and Speaker Diarization via Sequence Transduction",
      "original": "1943",
      "page_count": 5,
      "order": 83,
      "p1": "396",
      "pn": "400",
      "abstract": [
        "Speech applications dealing with conversations require not only recognizing\nthe spoken words, but also determining who spoke when. The task of\nassigning words to speakers is typically addressed by merging the outputs\nof two separate systems, namely, an automatic speech recognition (ASR)\nsystem and a speaker diarization (SD) system. The two systems are trained\nindependently with different objective functions. Often the SD systems\noperate directly on the acoustics and are not constrained to respect\nword boundaries and this deficiency is overcome in an  ad hoc manner.\nMotivated by recent advances in sequence to sequence learning, we propose\na novel approach to tackle the two tasks by a joint ASR and SD system\nusing a recurrent neural network transducer. Our approach utilizes\nboth linguistic and acoustic cues to infer speaker roles, as opposed\nto typical SD systems, which only use acoustic cues. We evaluated the\nperformance of our approach on a large corpus of medical conversations\nbetween physicians and patients. Compared to a competitive conventional\nbaseline, our approach improves word-level diarization error rate from\n15.8% to 2.2%.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1943",
      "author_area_id": 4,
      "author_area_label": "Speaker and Language Identification"
    },
    "cumani19_interspeech": {
      "authors": [
        [
          "Sandro",
          "Cumani"
        ]
      ],
      "title": "Normal Variance-Mean Mixtures for Unsupervised Score Calibration",
      "original": "1609",
      "page_count": 5,
      "order": 84,
      "p1": "401",
      "pn": "405",
      "abstract": [
        "Generative calibration models have shown to be an effective alternative\nto traditional discriminative score calibration techniques, such as\nLogistic Regression (LogReg). Provided that the score distribution\nassumptions are sufficiently accurate, generative approaches not only\nhave similar or better performance with respect to LogReg, but also\nallow for unsupervised or semi-supervised training.<br style=\"mso-data-placement:same-cell;\"\n/><br style=\"mso-data-placement:same-cell;\" />\n  Recently, we have\nproposed non-Gaussian linear calibration models able to overcome the\nlimitations of Gaussian approaches. Although these models allow for\nbetter characterization of score distributions, they still require\nthe target and non-target distributions to be reciprocally symmetric.<br\nstyle=\"mso-data-placement:same-cell;\" /><br style=\"mso-data-placement:same-cell;\"\n/>\n  In this work we further extend these models to cover asymmetric\nscore distributions, as to improve calibration for both supervised\nand unsupervised scenarios. The improvements have been assessed on\nNIST SRE 2010 telephone data.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1609",
      "author_area_id": 4,
      "author_area_label": "Speaker and Language Identification"
    },
    "yamamoto19_interspeech": {
      "authors": [
        [
          "Hitoshi",
          "Yamamoto"
        ],
        [
          "Kong Aik",
          "Lee"
        ],
        [
          "Koji",
          "Okabe"
        ],
        [
          "Takafumi",
          "Koshinaka"
        ]
      ],
      "title": "Speaker Augmentation and Bandwidth Extension for Deep Speaker Embedding",
      "original": "1508",
      "page_count": 5,
      "order": 85,
      "p1": "406",
      "pn": "410",
      "abstract": [
        "This paper investigates a novel data augmentation approach to train\ndeep neural networks (DNNs) used for speaker embedding, i.e. to extract\nrepresentation that allows easy comparison between speaker voices with\na simple geometric operation. Data augmentation is used to create new\nexamples from an existing training set, thereby increasing the quantity\nof training data improves the robustness of the model. We attempt to\nincrease the number of speakers in the training set by generating new\nspeakers via voice conversion. This speaker augmentation expands the\ncoverage of speakers in the embedding space in contrast to conventional\naudio augmentation methods which focus on within-speaker variability.\nWith an increased number of speakers in the training set, the DNN is\ntrained to produce a better speaker-discriminative embedding. We also\nadvocate using bandwidth extension to augment narrowband speech for\na wideband application. Text-independent speaker recognition experiments\nin Speakers in the Wild (SITW) demonstrate a 17.9% reduction in minimum\ndetection cost with speaker augmentation. The combined use of the two\ntechniques provides further improvement.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1508",
      "author_area_id": 4,
      "author_area_label": "Speaker and Language Identification"
    },
    "ylmaz19_interspeech": {
      "authors": [
        [
          "Emre",
          "Y\u0131lmaz"
        ],
        [
          "Adem",
          "Derinel"
        ],
        [
          "Kun",
          "Zhou"
        ],
        [
          "Henk van den",
          "Heuvel"
        ],
        [
          "Niko",
          "Brummer"
        ],
        [
          "Haizhou",
          "Li"
        ],
        [
          "David A. van",
          "Leeuwen"
        ]
      ],
      "title": "Large-Scale Speaker Diarization of Radio Broadcast Archives",
      "original": "1399",
      "page_count": 5,
      "order": 86,
      "p1": "411",
      "pn": "415",
      "abstract": [
        "This paper describes our initial efforts to build a large-scale speaker\ndiarization (SD) and identification system on a recently digitized\nradio broadcast archive from the Netherlands which has more than 6500\naudio tapes with 3000 hours of Frisian-Dutch speech recorded between\n1950&#8211;2016. The employed large-scale diarization scheme involves\ntwo stages: (1) tape-level speaker diarization providing pseudo-speaker\nidentities and (2) speaker linking to relate pseudo-speakers appearing\nin multiple tapes. Having access to the speaker models of several frequently\nappearing speakers from the previously collected FAME! speech corpus,\nwe further perform speaker identification by linking these known speakers\nto the pseudo-speakers identified at the first stage. In this work,\nwe present a recently created longitudinal and multilingual SD corpus\ndesigned for large-scale SD research and evaluate the performance of\na new speaker linking system using x-vectors with PLDA to quantify\ncross-tape speaker similarity on this corpus. The performance of this\nspeaker linking system is evaluated on a small subset of the archive\nwhich is manually annotated with speaker information. The speaker linking\nperformance reported on this subset (53 hours) and the whole archive\n(3000 hours) is compared to quantify the impact of scaling up in the\namount of speech data.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1399",
      "author_area_id": 4,
      "author_area_label": "Speaker and Language Identification"
    },
    "dubey19_interspeech": {
      "authors": [
        [
          "Harishchandra",
          "Dubey"
        ],
        [
          "Abhijeet",
          "Sangwan"
        ],
        [
          "John H.L.",
          "Hansen"
        ]
      ],
      "title": "Toeplitz Inverse Covariance Based Robust Speaker Clustering for Naturalistic Audio Streams",
      "original": "1102",
      "page_count": 5,
      "order": 87,
      "p1": "416",
      "pn": "420",
      "abstract": [
        "Speaker diarization determines  who spoke and when? in an audio stream.\nIn this study, we propose a model-based approach for robust speaker\nclustering using i-vectors. The i-vectors extracted from different\nsegments of same speaker are correlated. We model this correlation\nwith a Markov Random Field (MRF) network. Leveraging the advancements\nin MRF modeling, we used Toeplitz Inverse Covariance (TIC) matrix to\nrepresent the MRF correlation network for each speaker. This approaches\ncaptures the sequential structure of i-vectors (or equivalent speaker\nturns) belonging to same speaker in an audio stream. A variant of standard\nExpectation Maximization (EM) algorithm is adopted for deriving closed-form\nsolution using dynamic programming (DP) and the alternating direction\nmethod of multiplier (ADMM). Our diarization system has four steps:\n(1) ground-truth segmentation; (2) i-vector extraction; (3) post-processing\n(mean subtraction, principal component analysis, and length-normalization)\n; and (4) proposed speaker clustering. We employ cosine K-means and\nmovMF speaker clustering as baseline approaches. Our evaluation data\nis derived from: (i) CRSS-PLTL corpus, and (ii) two meetings subset\nof the AMI corpus. Relative reduction in diarization error rate (DER)\nfor CRSS-PLTL corpus is 43.22% using the proposed advancements as compared\nto baseline. For AMI meetings IS1000a and IS1003b, relative DER reduction\nis 29.37% and 9.21%, respectively.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1102",
      "author_area_id": 4,
      "author_area_label": "Speaker and Language Identification"
    },
    "kovacs19_interspeech": {
      "authors": [
        [
          "Gy\u00f6rgy",
          "Kov\u00e1cs"
        ],
        [
          "L\u00e1szl\u00f3",
          "T\u00f3th"
        ],
        [
          "Dirk Van",
          "Compernolle"
        ],
        [
          "Marcus",
          "Liwicki"
        ]
      ],
      "title": "Examining the Combination of Multi-Band Processing and Channel Dropout for Robust Speech Recognition",
      "original": "3215",
      "page_count": 5,
      "order": 88,
      "p1": "421",
      "pn": "425",
      "abstract": [
        "A pivotal question in Automatic Speech Recognition (ASR) is the robustness\nof the trained models. In this study, we investigate the combination\nof two methods commonly applied to increase the robustness of ASR systems.\nOn the one hand, inspired by auditory experiments and signal processing\nconsiderations, multi-band band processing has been used for decades\nto improve the noise robustness of speech recognition. On the other\nhand, dropout is a commonly used regularization technique to prevent\noverfitting by keeping the model from becoming over-reliant on a small\nset of neurons. We hypothesize that the careful combination of the\ntwo approaches would lead to increased robustness, by preventing the\nresulting model from over-rely on any given band.<br style=\"mso-data-placement:same-cell;\"\n/><br style=\"mso-data-placement:same-cell;\" />\n  To verify our hypothesis,\nwe investigate various approaches for the combination of the two methods\nusing the Aurora-4 corpus. The results obtained corroborate our initial\nassumption, and show that the proper combination of the two techniques\nleads to increased robustness, and to significantly lower word error\nrates (WERs). Furthermore, we find that the accuracy scores attained\nhere compare favourably to those reported recently on the clean training\nscenario of the Aurora-4 corpus.\n"
      ],
      "doi": "10.21437/Interspeech.2019-3215",
      "author_area_id": 8,
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "soni19_interspeech": {
      "authors": [
        [
          "Meet",
          "Soni"
        ],
        [
          "Ashish",
          "Panda"
        ]
      ],
      "title": "Label Driven Time-Frequency Masking for Robust Continuous Speech Recognition",
      "original": "2172",
      "page_count": 5,
      "order": 89,
      "p1": "426",
      "pn": "430",
      "abstract": [
        "The application of Time-Frequency (T-F) masking based approaches for\nAutomatic Speech Recognition has been shown to provide significant\ngains in system performance in the presence of additive noise. Such\napproaches give performance improvement when the T-F masking front-end\nis trained jointly with the acoustic model. However, such systems still\nrely on a pre-trained T-F masking enhancement block, trained using\npairs of clean and noisy speech signals. Pre-training is necessary\ndue to large number of parameters associated with the enhancement network.\nIn this paper, we propose a flat-start joint training of a network\nthat has both a T-F masking based enhancement block and a phoneme classification\nblock. In particular, we use fully convolutional network as an enhancement\nfront-end to reduce the number of parameters. We train the network\nby jointly updating the parameters of both these blocks using tied\nContext-Dependent phoneme states as targets. We observe that pretraining\nof the proposed enhancement block is not necessary for the convergence.\nIn fact, the proposed flat-start joint training converges faster than\nthe baseline multi-condition trained model. The experiments performed\non Aurora-4 database show 7.06% relative improvement over multi-conditioned\nbaseline. We get similar improvements for unseen test conditions as\nwell.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2172",
      "author_area_id": 8,
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "wu19c_interspeech": {
      "authors": [
        [
          "Long",
          "Wu"
        ],
        [
          "Hangting",
          "Chen"
        ],
        [
          "Li",
          "Wang"
        ],
        [
          "Pengyuan",
          "Zhang"
        ],
        [
          "Yonghong",
          "Yan"
        ]
      ],
      "title": "Speaker-Invariant Feature-Mapping for Distant Speech Recognition via Adversarial Teacher-Student Learning",
      "original": "2136",
      "page_count": 5,
      "order": 90,
      "p1": "431",
      "pn": "435",
      "abstract": [
        "Feature mapping (FM) jointly trained with acoustic model (AFM) is commonly\nused for single-channel speech enhancement. However, the performance\nis affected by the inter-speaker variability. In this paper, we propose\nspeaker-invariant AFM (SIAFM) aiming at curtailing the inter-talker\nvariability while achieving speech enhancement. In SIAFM, a feature-mapping\nnetwork, an acoustic model and a speaker classifier network are jointly\noptimized to minimize the feature-mapping loss and the senone classification\nloss, and simultaneously min-maximize the speaker classification loss.\nEvaluated on AMI dataset, the proposed SIAFM achieves 4.8% and 7.0%\nrelative word error rate (WER) reduction on the overlapped and non-overlapped\ncondition over the baseline acoustic model trained with single distant\nmicrophone (SDM) data. Additionally, the SIAFM obtains 3.0% relative\noverlapped WER and 4.2% relative non-overlapped WER decrease over the\nmulti-conditional (MCT) acoustic model. To further promote the performance\nof SIAFM, we employ teacher-student learning (TS), in which the posterior\nprobabilities generated by the individual headset microphone (IHM)\ndata can be used in lieu of labels to train the SIAFM model. The experiments\nshow that compared with MCT model, SIAFM with TS (SIAFM-TS) can reach\n4.2% relative overlapped WER and 6.3% relative non-overlapped WER decrease\nrespectively.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2136",
      "author_area_id": 8,
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "ming19_interspeech": {
      "authors": [
        [
          "Ji",
          "Ming"
        ],
        [
          "Danny",
          "Crookes"
        ]
      ],
      "title": "Full-Sentence Correlation: A Method to Handle Unpredictable Noise for Robust Speech Recognition",
      "original": "2127",
      "page_count": 5,
      "order": 91,
      "p1": "436",
      "pn": "440",
      "abstract": [
        "We describe the theory and implementation of full-sentence speech correlation\nfor speech recognition, and demonstrate its superior robustness to\nunseen/untrained noise. For the Aurora 2 data, trained with only clean\nspeech, the new method performs competitively against the state-of-the-art\nwith multicondition training and adaptation, and achieves the lowest\nword error rate in very low SNR (-5 dB). Further experiments with highly\nnonstationary noise (pop song, broadcast news, etc.) show the surprising\nability of the new method to handle unpredictable noise. The new method\nadds several novel developments to our previous research, including\nthe modeling of the speaker characteristics along with other acoustic\nand semantic features of speech for separating speech from noise, and\na novel Viterbi algorithm to implement full-sentence correlation for\nspeech recognition.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2127",
      "author_area_id": 8,
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "soni19b_interspeech": {
      "authors": [
        [
          "Meet",
          "Soni"
        ],
        [
          "Sonal",
          "Joshi"
        ],
        [
          "Ashish",
          "Panda"
        ]
      ],
      "title": "Generative Noise Modeling and Channel Simulation for Robust Speech Recognition in Unseen Conditions",
      "original": "2090",
      "page_count": 5,
      "order": 92,
      "p1": "441",
      "pn": "445",
      "abstract": [
        "Multi-conditioned training is a state-of-the-art approach to achieve\nrobustness in Automatic Speech Recognition (ASR) systems. This approach\nworks well in practice for seen degradation conditions. However, the\nperformance of such system is still an issue for unseen degradation\nconditions. In this work we consider distortions due to additive noise\nand channel mismatch. To achieve the robustness to additive noise,\nwe propose a parametric generative model for noise signals. By changing\nthe parameters of the proposed generative model, various noise signals\ncan be generated and used to develop a multi-conditioned dataset for\nASR system training. The generative model is designed to span the feature\nspace of Mel Filterbank Energies by using band-limited white noise\nsignals as basis. To simulate channel distortions, we propose to shift\nthe mean of log spectral magnitude using utterances with estimated\nchannel distortions. Experiments performed on the Aurora 4 noisy speech\ndatabase show that using noise types generated from the proposed generative\nmodel for multi-conditioned training provides significant performance\ngain for additive noise in unseen conditions. We compare our results\nwith those from multi-conditioning by various real noise databases\nincluding environmental and other real life noises.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2090",
      "author_area_id": 8,
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "kumar19_interspeech": {
      "authors": [
        [
          "Shashi",
          "Kumar"
        ],
        [
          "Shakti P.",
          "Rath"
        ]
      ],
      "title": "Far-Field Speech Enhancement Using Heteroscedastic Autoencoder for Improved Speech Recognition",
      "original": "2032",
      "page_count": 5,
      "order": 93,
      "p1": "446",
      "pn": "450",
      "abstract": [
        "Automatic speech recognition (ASR) systems trained on clean speech\ndo not perform well in far-field scenario. Degradation in word error\nrate (WER) can be as large as 40% in this mismatched scenario. Typically,\nspeech enhancement is applied to map speech from far-field condition\nto clean condition using a neural network, commonly known as denoising\nautoencoder (DA). Such speech enhancement technique has shown significant\nimprovement in ASR accuracy. It is a common practice to use mean-square\nerror (MSE) loss to train DA which is based on regression model with\nresidual noise modeled by zero-mean and constant co-variance Gaussian\ndistribution. However, both these assumptions are not optimal, especially\nin highly non-stationary noisy and far-field scenario. Here, we propose\na more generalized loss based on non-zero mean and heteroscedastic\nco-variance distribution for the residual variables. On the top, we\npresent several novel DA architectures that are more suitable for the\nheteroscedastic loss. It is shown that the proposed methods outperform\nthe conventional DA and MSE loss by a large margin. We observe relative\nimprovement of 7.31% in WER compared to conventional DA and overall,\na relative improvement of 14.4% compared to mismatched train and test\nscenario.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2032",
      "author_area_id": 8,
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "delcroix19_interspeech": {
      "authors": [
        [
          "Marc",
          "Delcroix"
        ],
        [
          "Shinji",
          "Watanabe"
        ],
        [
          "Tsubasa",
          "Ochiai"
        ],
        [
          "Keisuke",
          "Kinoshita"
        ],
        [
          "Shigeki",
          "Karita"
        ],
        [
          "Atsunori",
          "Ogawa"
        ],
        [
          "Tomohiro",
          "Nakatani"
        ]
      ],
      "title": "End-to-End SpeakerBeam for Single Channel Target Speech Recognition",
      "original": "1856",
      "page_count": 5,
      "order": 94,
      "p1": "451",
      "pn": "455",
      "abstract": [
        "End-to-end (E2E) automatic speech recognition (ASR) that directly maps\na sequence of speech features into a sequence of characters using a\nsingle neural network has received a lot of attention as it greatly\nsimplifies the training and decoding pipelines and enables optimizing\nthe whole system E2E. Recently, such systems have been extended to\nrecognize speech mixtures by inserting a speech separation mechanism\ninto the neural network, allowing to output recognition results for\neach speaker in the mixture. However, speech separation suffers from\na global permutation ambiguity issue, i.e. arbitrary mapping between\nsource speakers and outputs. We argue that this ambiguity would seriously\nlimit the practical use of E2E separation systems. SpeakerBeam has\nbeen proposed as an alternative to speech separation to mitigate the\nglobal permutation ambiguity. SpeakerBeam aims at extracting only a\ntarget speaker in a mixture based on his/her speech characteristics,\nthus avoiding the global permutation problem. In this paper, we combine\nSpeakerBeam and an E2E ASR system to allow E2E training of a target\nspeech recognition system. We show promising target speech recognition\nresults in mixtures of two speakers, and discuss interesting properties\nof the proposed system in terms of speech enhancement and diarization\nability.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1856",
      "author_area_id": 8,
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "hsu19_interspeech": {
      "authors": [
        [
          "I-Hung",
          "Hsu"
        ],
        [
          "Ayush",
          "Jaiswal"
        ],
        [
          "Premkumar",
          "Natarajan"
        ]
      ],
      "title": "NIESR: Nuisance Invariant End-to-End Speech Recognition",
      "original": "1836",
      "page_count": 5,
      "order": 95,
      "p1": "456",
      "pn": "460",
      "abstract": [
        "Deep neural network models for speech recognition have achieved great\nsuccess recently, but they can learn incorrect associations between\nthe target and nuisance factors of speech (e.g., speaker identities,\nbackground noise, etc.), which can lead to overfitting. While several\nmethods have been proposed to tackle this problem, existing methods\nincorporate additional information about nuisance factors during training\nto develop invariant models. However, enumeration of all possible nuisance\nfactors in speech data and the collection of their annotations is difficult\nand expensive. We present a robust training scheme for end-to-end speech\nrecognition that adopts an unsupervised adversarial invariance induction\nframework to separate out essential factors for speech-recognition\nfrom nuisances without using any supplementary labels besides the transcriptions.\nExperiments show that the speech recognition model trained with the\nproposed training scheme achieves relative improvements of 5.48% on\nWSJ0, 6.16% on CHiME3, and 6.61% on TIMIT dataset over the base model.\nAdditionally, the proposed method achieves a relative improvement of\n14.44% on the combined WSJ0+CHiME3 dataset.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1836",
      "author_area_id": 8,
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "suzuki19_interspeech": {
      "authors": [
        [
          "Takahito",
          "Suzuki"
        ],
        [
          "Jun",
          "Ogata"
        ],
        [
          "Takashi",
          "Tsunakawa"
        ],
        [
          "Masafumi",
          "Nishida"
        ],
        [
          "Masafumi",
          "Nishimura"
        ]
      ],
      "title": "Knowledge Distillation for Throat Microphone Speech Recognition",
      "original": "1597",
      "page_count": 5,
      "order": 96,
      "p1": "461",
      "pn": "465",
      "abstract": [
        "Throat microphones are robust against external noise because they receive\nvibrations directly from the skin, however, their available speech\ndata is limited. This work aims to improve the speech recognition accuracy\nof throat microphones, and we propose a knowledge distillation method\nof hybrid DNN-HMM acoustic model. This method distills the knowledge\nfrom acoustic model trained with a large amount of close-talk microphone\nspeech data (teacher model) to acoustic model for throat microphones\n(student model) using a small amount of parallel data of throat and\nclose-talk microphones. The frontend network of the student model contains\na feature mapping network from throat microphone acoustic features\nto close-talk microphone bottleneck features, and the back-end network\nis a phonetic discrimination network from close-talk microphone bottleneck\nfeatures. We attempted to improve recognition accuracy further by initializing\nstudent model parameters using pretrained front-end and back-end networks.\nExperimental results using Japanese read speech data showed that the\nproposed approach achieved 9.8% relative improvement of character error\nrate (14.3% &#8594; 12.9%) compared to the hybrid acoustic model trained\nonly with throat microphone speech data. Furthermore, under noise environments\nof approximately 70 dBA or higher, the throat microphone system with\nour approach outperformed the close-talk microphone system.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1597",
      "author_area_id": 8,
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "wu19d_interspeech": {
      "authors": [
        [
          "Jian",
          "Wu"
        ],
        [
          "Yong",
          "Xu"
        ],
        [
          "Shi-Xiong",
          "Zhang"
        ],
        [
          "Lianwu",
          "Chen"
        ],
        [
          "Meng",
          "Yu"
        ],
        [
          "Lei",
          "Xie"
        ],
        [
          "Dong",
          "Yu"
        ]
      ],
      "title": "Improved Speaker-Dependent Separation for CHiME-5 Challenge",
      "original": "1569",
      "page_count": 5,
      "order": 97,
      "p1": "466",
      "pn": "470",
      "abstract": [
        "This paper summarizes several contributions for improving the speaker-dependent\nseparation system for CHiME-5 challenge, which aims to solve the problem\nof multi-channel, highly-overlapped conversational speech recognition\nin a dinner party scenario with reverberations and non-stationary noises.\nSpecifically, we adopt a speaker-aware training method by using i-vector\nas the target speaker information for multi-talker speech separation.\nWith only one unified separation model for all speakers, we achieve\na 10% absolute improvement in terms of word error rate (WER) over the\nprevious baseline of 80.28% on the development set by leveraging our\nnewly proposed data processing techniques and beamforming approach.\nWith our improved back-end acoustic model, we further reduce WER to\n60.15% which surpasses the result of our submitted CHiME-5 challenge\nsystem without applying any fusion techniques.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1569",
      "author_area_id": 8,
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "wang19b_interspeech": {
      "authors": [
        [
          "Peidong",
          "Wang"
        ],
        [
          "Ke",
          "Tan"
        ],
        [
          "DeLiang",
          "Wang"
        ]
      ],
      "title": "Bridging the Gap Between Monaural Speech Enhancement and Recognition with Distortion-Independent Acoustic Modeling",
      "original": "1495",
      "page_count": 5,
      "order": 98,
      "p1": "471",
      "pn": "475",
      "abstract": [
        "Monaural speech enhancement has made dramatic advances in recent years.\nAlthough enhanced speech has been demonstrated to have better intelligibility\nand quality for human listeners, feeding it directly to automatic speech\nrecognition (ASR) systems trained with noisy speech has not produced\nexpected improvements in ASR performance. The lack of an enhancement\nbenefit on recognition, or the gap between monaural speech enhancement\nand recognition, is often attributed to speech distortions introduced\nin the enhancement process. In this study, we analyze the distortion\nproblem and propose a distortion-independent acoustic modeling scheme.\nExperimental results show that the distortion-independent acoustic\nmodel is able to overcome the distortion problem. Moreover, it can\nbe used with various speech enhancement models. Both the distortion-independent\nand a noise-dependent acoustic model perform better than the previous\nbest system on the CHiME-2 corpus. The noise-dependent acoustic model\nachieves a word error rate of 8.7%, outperforming the previous best\nresult by 6.5% relatively.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1495",
      "author_area_id": 8,
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "wang19c_interspeech": {
      "authors": [
        [
          "Peidong",
          "Wang"
        ],
        [
          "DeLiang",
          "Wang"
        ]
      ],
      "title": "Enhanced Spectral Features for Distortion-Independent Acoustic Modeling",
      "original": "1493",
      "page_count": 5,
      "order": 99,
      "p1": "476",
      "pn": "480",
      "abstract": [
        "It has recently been shown that a distortion-independent acoustic modeling\nmethod is able to overcome the distortion problem caused by speech\nenhancement. In this study, we improve the distortion-independent acoustic\nmodel by feeding it with enhanced spectral features. Using enhanced\nmagnitude spectra, the automatic speech recognition (ASR) system achieves\na word error rate of 7.8% on the CHiME-2 corpus, outperforming our\nprevious best system by more than 10% relatively. Compared with the\ncorresponding enhanced waveform signal based system, systems using\nenhanced spectral features obtain up to 24% relative improvement. These\ncomparisons show that speech enhancement is helpful for robust ASR\nand that enhanced spectral features are more suitable for ASR tasks\nthan enhanced waveform signals.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1493",
      "author_area_id": 8,
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "neekhara19b_interspeech": {
      "authors": [
        [
          "Paarth",
          "Neekhara"
        ],
        [
          "Shehzeen",
          "Hussain"
        ],
        [
          "Prakhar",
          "Pandey"
        ],
        [
          "Shlomo",
          "Dubnov"
        ],
        [
          "Julian",
          "McAuley"
        ],
        [
          "Farinaz",
          "Koushanfar"
        ]
      ],
      "title": "Universal Adversarial Perturbations for Speech Recognition Systems",
      "original": "1353",
      "page_count": 5,
      "order": 100,
      "p1": "481",
      "pn": "485",
      "abstract": [
        "In this work, we demonstrate the existence of universal adversarial\naudio perturbations that cause mis-transcription of audio signals by\nautomatic speech recognition (ASR) systems. We propose an algorithm\nto find a single quasi-imperceptible perturbation, which when added\nto any arbitrary speech signal, will most likely fool the victim speech\nrecognition model. Our experiments demonstrate the application of our\nproposed technique by crafting audio-agnostic universal perturbations\nfor the state-of-the-art ASR system &#8212; Mozilla DeepSpeech. Additionally,\nwe show that such perturbations generalize to a significant extent\nacross models that are not available during training, by performing\na transferability test on a WaveNet based ASR system.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1353",
      "author_area_id": 8,
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "fujimoto19_interspeech": {
      "authors": [
        [
          "Masakiyo",
          "Fujimoto"
        ],
        [
          "Hisashi",
          "Kawai"
        ]
      ],
      "title": "One-Pass Single-Channel Noisy Speech Recognition Using a Combination of Noisy and Enhanced Features",
      "original": "1270",
      "page_count": 5,
      "order": 101,
      "p1": "486",
      "pn": "490",
      "abstract": [
        "This paper introduces a method of noise-robust automatic speech recognition\n(ASR) that remains effective under one-pass single-channel processing.\nUnder these constraints, the use of single-channel speech enhancement\nseems to be a reasonable noise-robust approach to ASR, because complicated\ntechniques requiring multi-pass processing cannot be used. However,\nin many cases, single-channel speech enhancement seriously deteriorates\nthe accuracy of ASR because of speech distortion. In addition, the\nadvanced acoustic modeling framework (joint training) is relatively\nineffective in the case of single-channel processing. To overcome these\nproblems, we propose a noise-robust acoustic modeling framework based\non a feature-level combination of noisy speech and enhanced speech.\nTo obtain further improvements, we also adopt a sub-network-level combination\nof noisy and enhanced speech, and a gating mechanism that can dynamically\nselect appropriate speech features. Through comparative evaluations,\nwe confirm that the proposed method successfully improves the accuracy\nof ASR in noisy environments under strong constraints.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1270",
      "author_area_id": 8,
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "liu19_interspeech": {
      "authors": [
        [
          "Bin",
          "Liu"
        ],
        [
          "Shuai",
          "Nie"
        ],
        [
          "Shan",
          "Liang"
        ],
        [
          "Wenju",
          "Liu"
        ],
        [
          "Meng",
          "Yu"
        ],
        [
          "Lianwu",
          "Chen"
        ],
        [
          "Shouye",
          "Peng"
        ],
        [
          "Changliang",
          "Li"
        ]
      ],
      "title": "Jointly Adversarial Enhancement Training for Robust End-to-End Speech Recognition",
      "original": "1242",
      "page_count": 5,
      "order": 102,
      "p1": "491",
      "pn": "495",
      "abstract": [
        "Recently, the end-to-end system has made significant breakthroughs\nin the field of speech recognition. However, this single end-to-end\narchitecture is not especially robust to the input variations interfered\nof noises and reverberations, resulting in performance degradation\ndramatically in reality. To alleviate this issue, the mainstream approach\nis to use a well-designed speech enhancement module as the front-end\nof ASR. However, enhancement modules would result in speech distortions\nand mismatches to training, which sometimes degrades the ASR performance.\nIn this paper, we propose a jointly adversarial enhancement training\nto boost robustness of end-to-end systems. Specifically, we use a jointly\ncompositional scheme of mask-based enhancement network, attention-based\nencoder-decoder network and discriminant network during training. The\ndiscriminator is used to distinguish between the enhanced features\nfrom enhancement network and clean features, which could guide enhancement\nnetwork to output towards the realistic distribution. With the joint\noptimization of the recognition, enhancement and adversarial loss,\nthe compositional scheme is expected to learn more robust representations\nfor the recognition task automatically. Systematic experiments on AISHELL-1\nshow that the proposed method improves the noise robustness of end-to-end\nsystems and achieves the relative error rate reduction of 4.6% over\nthe multi-condition training.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1242",
      "author_area_id": 8,
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "yang19_interspeech": {
      "authors": [
        [
          "Zixiaofan",
          "Yang"
        ],
        [
          "Bingyan",
          "Hu"
        ],
        [
          "Julia",
          "Hirschberg"
        ]
      ],
      "title": "Predicting Humor by Learning from Time-Aligned Comments",
      "original": "3113",
      "page_count": 5,
      "order": 103,
      "p1": "496",
      "pn": "500",
      "abstract": [
        "In this paper, we describe a novel approach for generating unsupervised\nhumor labels using time-aligned user comments, and predicting humor\nusing audio information alone. We collected 241 videos of comedy movies\nand gameplay videos from one of the largest Chinese video-sharing websites.\nWe generate unsupervised humor labels from laughing comments, and find\nhigh agreement between these labels and human annotations. From these\nunsupervised labels, we build deep learning models using speech and\ntext features, which obtain an AUC of 0.751 in predicting humor on\na manually annotated test set. To our knowledge, this is the first\nstudy predicting perceived humor in large-scale audio data.\n"
      ],
      "doi": "10.21437/Interspeech.2019-3113",
      "author_area_id": 3,
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "dinkov19_interspeech": {
      "authors": [
        [
          "Yoan",
          "Dinkov"
        ],
        [
          "Ahmed",
          "Ali"
        ],
        [
          "Ivan",
          "Koychev"
        ],
        [
          "Preslav",
          "Nakov"
        ]
      ],
      "title": "Predicting the Leading Political Ideology of YouTube Channels Using Acoustic, Textual, and Metadata Information",
      "original": "2965",
      "page_count": 5,
      "order": 104,
      "p1": "501",
      "pn": "505",
      "abstract": [
        "We address the problem of predicting the leading political ideology,\ni.e., left-center-right bias, for YouTube channels of news media. Previous\nwork on the problem has focused exclusively on text and on analysis\nof the language used, topics discussed, sentiment, and the like. In\ncontrast, here we study videos, which yields an interesting multimodal\nsetup. Starting with gold annotations about the leading political ideology\nof major world news media from Media Bias/Fact Check, we searched on\nYouTube to find their corresponding channels, and we downloaded a recent\nsample of videos from each channel. We crawled more than 1,000 YouTube\nhours along with the corresponding subtitles and metadata, thus producing\na new multimodal dataset. We further developed a multimodal deep-learning\narchitecture for the task. Our analysis shows that the use of acoustic\nsignal helped to improve bias detection by more than 6% absolute over\nusing text and metadata only. We release the dataset to the research\ncommunity, hoping to help advance the field of multi-modal political\nbias detection.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2965",
      "author_area_id": 3,
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "an19_interspeech": {
      "authors": [
        [
          "Guozhen",
          "An"
        ],
        [
          "Rivka",
          "Levitan"
        ]
      ],
      "title": "Mitigating Gender and L1 Differences to Improve State and Trait Recognition",
      "original": "2868",
      "page_count": 4,
      "order": 105,
      "p1": "506",
      "pn": "509",
      "abstract": [
        "Automatic detection of speaker states and traits is made more difficult\nby intergroup differences in how they are distributed and expressed\nin speech and language. In this study, we explore various deep learning\narchitectures for incorporating demographic information into the classification\ntask. We find that early and late fusion of demographic information\nboth improve performance on the task of personality recognition, and\na multitask learning model, which performs best, also significantly\nimproves deception detection accuracy. Our findings establish a new\nstate-of-the-art for personality recognition and deception detection\non the CXD corpus, and suggest new best practices for mitigating intergroup\ndifferences to improve speaker state and trait recognition.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2868",
      "author_area_id": 3,
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "weninger19_interspeech": {
      "authors": [
        [
          "Felix",
          "Weninger"
        ],
        [
          "Yang",
          "Sun"
        ],
        [
          "Junho",
          "Park"
        ],
        [
          "Daniel",
          "Willett"
        ],
        [
          "Puming",
          "Zhan"
        ]
      ],
      "title": "Deep Learning Based Mandarin Accent Identification for Accent Robust ASR",
      "original": "2737",
      "page_count": 5,
      "order": 106,
      "p1": "510",
      "pn": "514",
      "abstract": [
        "In this paper, we present an in-depth study on the classification of\nregional accents in Mandarin speech. Experiments are carried out on\nMandarin speech data systematically collected from 15 different geographical\nregions in China for broad coverage. We explore bidirectional Long\nShort-Term Memory (bLSTM) networks and i-vectors to model longer-term\nacoustic context. Starting from the classification of the collected\ndata into the 15 regional accents, we derive a three-class grouping\nvia non-metric dimensional scaling (NMDS), for which 68.4% average\nrecall can be obtained. Furthermore, we evaluate a state-of-the-art\nASR system on the accented data and demonstrate that the character\nerror rate (CER) strongly varies among these accent groups, even if\ni-vector speaker adaptation is used. Finally, we show that model selection\nbased on the prediction of our bLSTM accent classifier can yield up\nto 7.6% CER reduction for accented speech.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2737",
      "author_area_id": 3,
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "gosztolya19_interspeech": {
      "authors": [
        [
          "G\u00e1bor",
          "Gosztolya"
        ],
        [
          "L\u00e1szl\u00f3",
          "T\u00f3th"
        ]
      ],
      "title": "Calibrating DNN Posterior Probability Estimates of HMM/DNN Models to Improve Social Signal Detection from Audio Data",
      "original": "2552",
      "page_count": 5,
      "order": 107,
      "p1": "515",
      "pn": "519",
      "abstract": [
        "To detect social signals such as laughter or filler events from audio\ndata, a straightforward choice is to apply a Hidden Markov Model (HMM)\nin combination with a Deep Neural Network (DNN) that supplies the local\nclass posterior estimates ( HMM/DNN hybrid model). However, the posterior\nestimates of the DNN may be suboptimal due to a mismatch between the\ncost function used during training (e.g. frame-level cross-entropy)\nand the actual evaluation metric (e.g. segment-level F<SUB>1</SUB>\nscore). In this study, we show experimentally that by employing a simple\nposterior probability calibration technique on the DNN outputs, the\nperformance of the HMM/DNN workflow can be significantly improved.\nSpecifically, we apply a linear transformation on the activations of\nthe output layer right before using the softmax function, and fine-tune\nthe parameters of this transformation. Out of the calibration approaches\ntested, we got the best F<SUB>1</SUB> scores when the posterior calibration\nprocess was adjusted so as to maximize the actual HMM-based evaluation\nmetric.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2552",
      "author_area_id": 3,
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "mori19_interspeech": {
      "authors": [
        [
          "Hiroki",
          "Mori"
        ],
        [
          "Tomohiro",
          "Nagata"
        ],
        [
          "Yoshiko",
          "Arimoto"
        ]
      ],
      "title": "Conversational and Social Laughter Synthesis with WaveNet",
      "original": "2131",
      "page_count": 4,
      "order": 108,
      "p1": "520",
      "pn": "523",
      "abstract": [
        "The studies of laughter synthesis are relatively few, and they are\nstill in a preliminary stage. We explored the possibility of applying\nWaveNet to laughter synthesis. WaveNet is potentially more suitable\nto model laughter waveforms that do not have a well-established theory\nof production like speech signals. Conversational laughter was modelled\nwith a spontaneous dialogue speech corpus based on WaveNet. To obtain\nmore stable laughter generation, conditioning WaveNet by power contour\nwas proposed. Experimental results showed that the synthesized laughter\nby WaveNet was perceived as closer to natural laughter than HMM-based\nsynthesized laughter.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2131",
      "author_area_id": 3,
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "ludusan19_interspeech": {
      "authors": [
        [
          "Bogdan",
          "Ludusan"
        ],
        [
          "Petra",
          "Wagner"
        ]
      ],
      "title": "Laughter Dynamics in Dyadic Conversations",
      "original": "1733",
      "page_count": 5,
      "order": 109,
      "p1": "524",
      "pn": "528",
      "abstract": [
        "Human verbal communication is a complex phenomenon involving dynamics\nthat normally result in the alignment of participants on several modalities,\nand across various linguistic domains. We examined here whether such\ndynamics occur also for paralinguistic events, in particular, in the\ncase of laughter. Using a conversational corpus containing dyadic interactions\nin three languages (French, German and Mandarin Chinese), we investigated\nthree measures of alignment: convergence, synchrony and agreement.\nSupport for convergence and synchrony was found in all three languages,\nalthough the level of support varied with the language, while the agreement\nin laughter type was found to be significant for the German data. The\nimplications of these findings towards a better understanding of the\nrole of laughter in human communication are discussed.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1733",
      "author_area_id": 3,
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "truong19_interspeech": {
      "authors": [
        [
          "Khiet P.",
          "Truong"
        ],
        [
          "J\u00fcrgen",
          "Trouvain"
        ],
        [
          "Michel-Pierre",
          "Jansen"
        ]
      ],
      "title": "Towards an Annotation Scheme for Complex Laughter in Speech Corpora",
      "original": "1557",
      "page_count": 5,
      "order": 110,
      "p1": "529",
      "pn": "533",
      "abstract": [
        "Although laughter research has gained quite some interest over the\npast few years, a shared description of how to annotate laughter and\nits sub-units is still missing. We present a first attempt towards\nan annotation scheme that contributes to improving the homogeneity\nand transparency with which laughter is annotated. This includes the\nintegration of respiratory noises as well as stretches of speech-laughs,\nand to a limited extend to smiled speech and short silent intervals.\nInter-annotator agreement is assessed while applying the scheme to\ndifferent corpora where laughter is evoked through different methods\nand varying settings. Annotating laughter becomes more complex when\nthe situation in which laughter occurs becomes more spontaneous and\nsocial. There is a substantial disagreement among the annotators with\nrespect to temporal alignment (when does a unit start and when does\nit end) and unit classification, particularly the determination of\nstarts/ends of laughter episodes. In summary, this detailed laughter\nannotation study reflects the need for better investigations of the\nvarious components of laughter.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1557",
      "author_area_id": 3,
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "baird19_interspeech": {
      "authors": [
        [
          "Alice",
          "Baird"
        ],
        [
          "Shahin",
          "Amiriparian"
        ],
        [
          "Nicholas",
          "Cummins"
        ],
        [
          "Sarah",
          "Sturmbauer"
        ],
        [
          "Johanna",
          "Janson"
        ],
        [
          "Eva-Maria",
          "Messner"
        ],
        [
          "Harald",
          "Baumeister"
        ],
        [
          "Nicolas",
          "Rohleder"
        ],
        [
          "Bj\u00f6rn W.",
          "Schuller"
        ]
      ],
      "title": "Using Speech to Predict Sequentially Measured Cortisol Levels During a Trier Social Stress Test",
      "original": "1352",
      "page_count": 5,
      "order": 111,
      "p1": "534",
      "pn": "538",
      "abstract": [
        "The effect of stress on the human body is substantial, potentially\nresulting in serious health implications. Furthermore, with modern\nstressors seemingly on the increase, there is an abundance of contributing\nfactors which lead to a diagnosis of acute stress. However, observing\nbiological stress reactions usually includes costly and time consuming\nsequential fluid-based samples to determine the degree of biological\nstress. On the contrary, a speech monitoring approach would allow for\na non-invasive indication of stress. To evaluate the efficacy of the\nspeech signal as a marker of stress, we explored, for the first time,\nthe relationship between sequential cortisol samples and speech-based\nfeatures. Utilising a novel corpus of 43 individuals undergoing a standardised\nTrier Social Stress Test (TSST), we extract a variety of feature sets\nand observe a correlation between speech and sequential cortisol measurements.\nFor prediction of mean cortisol levels from speech, results show that\nfor the entire TSST oral presentation, handcrafted COMPARE features\nachieve best results of 0.244 root mean square error [0 ;1] for the\nsample 20 minutes after the TSST. Correlation also increases at minute\n20, with a Spearman&#8217;s correlation coefficient of 0.421, and Cohen&#8217;s\nd of 0.883 between the baseline and minute 20 cortisol predictions.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1352",
      "author_area_id": 3,
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "baird19b_interspeech": {
      "authors": [
        [
          "Alice",
          "Baird"
        ],
        [
          "Eduardo",
          "Coutinho"
        ],
        [
          "Julia",
          "Hirschberg"
        ],
        [
          "Bj\u00f6rn W.",
          "Schuller"
        ]
      ],
      "title": "Sincerity in Acted Speech: Presenting the Sincere Apology Corpus and Results",
      "original": "1349",
      "page_count": 5,
      "order": 112,
      "p1": "539",
      "pn": "543",
      "abstract": [
        "The ability to discern an individual&#8217;s level of sincerity varies\nfrom person to person and across cultures. Sincerity is typically a\nkey indication of personality traits such as trustworthiness, and portraying\nsincerity can be integral to an abundance of scenarios, e. g. , when\napologising. Speech signals are one important factor when discerning\nsincerity and, with more modern interactions occurring remotely, automatic\napproaches for the recognition of sincerity from speech are beneficial\nduring both interpersonal and professional scenarios. In this study\nwe present details of the Sincere Apology Corpus ( Sina-C). Annotated\nby 22 individuals for their perception of sincerity,  Sina-C is an\nEnglish acted-speech corpus of 32 speakers, apologising in multiple\nways. To provide an updated baseline for the corpus, various machine\nlearning experiments are conducted. Finding that extracting deep data-representations\n(utilising the  Deep Spectrum toolkit) from the speech signals is best\nsuited. Classification results on the binary (sincere / not sincere)\ntask are at best 79.2% Unweighted Average Recall and for regression,\nin regards to the degree of sincerity, a Root Mean Square Error of\n0.395 from the standardised range [-1.51; 1.72] is obtained.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1349",
      "author_area_id": 3,
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "niebuhr19c_interspeech": {
      "authors": [
        [
          "Oliver",
          "Niebuhr"
        ],
        [
          "Kerstin",
          "Fischer"
        ]
      ],
      "title": "Do not Hesitate! &#8212; Unless You Do it Shortly or Nasally: How the Phonetics of Filled Pauses Determine Their Subjective Frequency and Perceived Speaker Performance",
      "original": "1194",
      "page_count": 5,
      "order": 113,
      "p1": "544",
      "pn": "548",
      "abstract": [
        "In this paper, we test whether the perception of filled-pause (FP)\nfrequency and public-speaking performance are mediated by the phonetic\ncharacteristics of FPs. In particular, total duration, vowel-formant\npattern (if present), and nasal-segment proportion of FPs were correlated\nwith perceptual data of 29 German listeners who rated excerpts of business\npresentations given by 68 German-speaking managers. Results show strong\ninter-speaker differences in how and how often FPs are realized. Moreover,\ndifferences in FP duration and nasal proportion are significantly correlated\nwith estimated (i.e. subjective) FP frequency and perceived speaker\nperformance. The shorter and more nasal a speaker&#8217;s FPs are,\nthe more do listeners underestimate the speaker&#8217;s actual FP frequency\nand the higher they rate the speaker&#8217;s public-speaking performance.\nThe results are discussed in terms of their implications for FP saliency\nand rhetorical training.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1194",
      "author_area_id": 3,
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "vasquezcorrea19_interspeech": {
      "authors": [
        [
          "J.C.",
          "V\u00e1squez-Correa"
        ],
        [
          "Philipp",
          "Klumpp"
        ],
        [
          "Juan Rafael",
          "Orozco-Arroyave"
        ],
        [
          "Elmar",
          "N\u00f6th"
        ]
      ],
      "title": "Phonet: A Tool Based on Gated Recurrent Neural Networks to Extract Phonological Posteriors from Speech",
      "original": "1405",
      "page_count": 5,
      "order": 114,
      "p1": "549",
      "pn": "553",
      "abstract": [
        "There are a lot of features that can be extracted from speech signals\nfor different applications such as automatic speech recognition or\nspeaker verification. However, for pathological speech processing there\nis a need to extract features about the presence of the disease or\nthe state of the patients that are comprehensible for clinical experts.\nPhonological posteriors are a group of features that can be interpretable\nby the clinicians and at the same time carry suitable information about\nthe patient&#8217;s speech. This paper presents a tool to extract phonological\nposteriors directly from speech signals. The proposed method consists\nof a bank of parallel bidirectional recurrent neural networks to estimate\nthe posterior probabilities of the occurrence of different phonological\nclasses. The proposed models are able to detect the phonological classes\nwith accuracies over 90%. In addition, the trained models are available\nto be used by the research community interested in the topic.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1405",
      "author_area_id": 3,
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "chang19_interspeech": {
      "authors": [
        [
          "Ching-Ting",
          "Chang"
        ],
        [
          "Shun-Po",
          "Chuang"
        ],
        [
          "Hung-Yi",
          "Lee"
        ]
      ],
      "title": "Code-Switching Sentence Generation by Generative Adversarial Networks and its Application to Data Augmentation",
      "original": "3214",
      "page_count": 5,
      "order": 115,
      "p1": "554",
      "pn": "558",
      "abstract": [
        "Code-switching is about dealing with alternative languages in speech\nor text. It is partially speaker-dependent and domain-related, so completely\nexplaining the phenomenon by linguistic rules is challenging. Compared\nto most monolingual tasks, insufficient data is an issue for code-switching.\nTo mitigate the issue without expensive human annotation, we proposed\nan unsupervised method for code-switching data augmentation. By utilizing\na generative adversarial network, we can generate intra-sentential\ncode-switching sentences from monolingual sentences. We applied the\nproposed method on two corpora, and the result shows that the generated\ncode-switching sentences improve the performance of code-switching\nlanguage models.\n"
      ],
      "doi": "10.21437/Interspeech.2019-3214",
      "author_area_id": 12,
      "author_area_label": "Spoken Language Processing: Translation, Information Retrieval, Summarization, Resources and Evaluation"
    },
    "meier19_interspeech": {
      "authors": [
        [
          "Moritz",
          "Meier"
        ],
        [
          "Celeste",
          "Mason"
        ],
        [
          "Felix",
          "Putze"
        ],
        [
          "Tanja",
          "Schultz"
        ]
      ],
      "title": "Comparative Analysis of Think-Aloud Methods for Everyday Activities in the Context of Cognitive Robotics",
      "original": "3072",
      "page_count": 5,
      "order": 116,
      "p1": "559",
      "pn": "563",
      "abstract": [
        "We describe our efforts to compare data collection methods using two\nthink-aloud protocols in preparation to be used as a basis for automatic\nstructuring and labeling of a large database of high-dimensional human\nactivities data into a valuable resource for research in cognitive\nrobotics. The envisioned dataset, currently in development, will contain\nsynchronously recorded multimodal data, including audio, video, and\nbiosignals (eye-tracking, motion-tracking, muscle and brain activity)\nfrom about 100 participants performing everyday activities while describing\ntheir task through use of think-aloud protocols. This paper provides\ndetails of our pilot recordings in the well-established and scalable\n&#8220;table setting scenario,&#8221; describes the concurrent and\nretrospective think-aloud protocols used, the methods used to analyze\nthem, and compares their potential impact on the data collected as\nwell as the automatic data segmentation and structuring process.\n"
      ],
      "doi": "10.21437/Interspeech.2019-3072",
      "author_area_id": 12,
      "author_area_label": "Spoken Language Processing: Translation, Information Retrieval, Summarization, Resources and Evaluation"
    },
    "beeferman19_interspeech": {
      "authors": [
        [
          "Doug",
          "Beeferman"
        ],
        [
          "William",
          "Brannon"
        ],
        [
          "Deb",
          "Roy"
        ]
      ],
      "title": "RadioTalk: A Large-Scale Corpus of Talk Radio Transcripts",
      "original": "2714",
      "page_count": 5,
      "order": 117,
      "p1": "564",
      "pn": "568",
      "abstract": [
        "We introduce RadioTalk, a corpus of speech recognition transcripts\nsampled from talk radio broadcasts in the United States between October\nof 2018 and March of 2019. The corpus is intended for use by researchers\nin the fields of natural language processing, conversational analysis,\nand the social sciences. The corpus encompasses approximately 2.8 billion\nwords of automatically transcribed speech from 284,000 hours of radio,\ntogether with metadata about the speech, such as geographical location,\nspeaker turn boundaries, gender, and radio program information. In\nthis paper we summarize why and how we prepared the corpus, give some\ndescriptive statistics on stations, shows and speakers, and carry out\na few high-level analyses.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2714",
      "author_area_id": 12,
      "author_area_label": "Spoken Language Processing: Translation, Information Retrieval, Summarization, Resources and Evaluation"
    },
    "mdhaffar19_interspeech": {
      "authors": [
        [
          "Salima",
          "Mdhaffar"
        ],
        [
          "Yannick",
          "Est\u00e8ve"
        ],
        [
          "Nicolas",
          "Hernandez"
        ],
        [
          "Antoine",
          "Laurent"
        ],
        [
          "Richard",
          "Dufour"
        ],
        [
          "Solen",
          "Quiniou"
        ]
      ],
      "title": "Qualitative Evaluation of ASR Adaptation in a Lecture Context: Application to the PASTEL Corpus",
      "original": "2661",
      "page_count": 5,
      "order": 118,
      "p1": "569",
      "pn": "573",
      "abstract": [
        "Lectures are usually known to be highly specialised in that they deal\nwith multiple and domain specific topics. This context is challenging\nfor Automatic Speech Recognition (ASR) systems since they are sensitive\nto topic variability. Language Model (LM) adaptation is a commonly\nused technique to address the mismatch problem between training and\ntest data. In this paper, we are interested in a qualitative analysis\nin order to relevantly compare the accuracy of the LM adaptation. While\nword error rate is the most common metric used to evaluate ASR systems,\nwe consider that this metric cannot provide accurate information. Consequently,\nwe explore the use of other metrics based on individual word error\nrate, indexability, and capability of building relevant requests for\ninformation retrieval from the ASR outputs. Experiments are carried\nout on the PASTEL corpus, a new dataset in French language, composed\nof lecture recordings, manual chaptering, manual transcriptions, and\nslides. While an adapted LM allows us to reduce the global classical\nword error rate by 15.62% in relative, we show that this reduction\nreaches 44.2% when computed on relevant words only. These observations\nare confirmed with the high LM adaptation gains obtained with indexability\nand information retrieval metrics.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2661",
      "author_area_id": 12,
      "author_area_label": "Spoken Language Processing: Translation, Information Retrieval, Summarization, Resources and Evaluation"
    },
    "marinelli19_interspeech": {
      "authors": [
        [
          "Federico",
          "Marinelli"
        ],
        [
          "Alessandra",
          "Cervone"
        ],
        [
          "Giuliano",
          "Tortoreto"
        ],
        [
          "Evgeny A.",
          "Stepanov"
        ],
        [
          "Giuseppe Di",
          "Fabbrizio"
        ],
        [
          "Giuseppe",
          "Riccardi"
        ]
      ],
      "title": "Active Annotation: Bootstrapping Annotation Lexicon and Guidelines for Supervised NLU Learning",
      "original": "2537",
      "page_count": 5,
      "order": 119,
      "p1": "574",
      "pn": "578",
      "abstract": [
        "Natural Language Understanding (NLU) models are typically trained in\na supervised learning framework. In the case of intent classification,\nthe predicted labels are predefined and based on the designed annotation\nschema while the labeling process is based on a laborious task where\nannotators manually inspect each utterance and assign the corresponding\nlabel. We propose an Active Annotation (AA) approach where we combine\nan unsupervised learning method in the embedding space, a human-in-the-loop\nverification process, and linguistic insights to create lexicons that\ncan be open categories and adapted over time. In particular, annotators\ndefine the y-label space on-the-fly during the annotation using an\niterative process and without the need for prior knowledge about the\ninput data. We evaluate the proposed annotation paradigm in a real\nuse-case NLU scenario. Results show that our Active Annotation paradigm\nachieves accurate and higher quality training data, with an annotation\nspeed of an order of magnitude higher with respect to the traditional\nhuman-only driven baseline annotation methodology.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2537",
      "author_area_id": 12,
      "author_area_label": "Spoken Language Processing: Translation, Information Retrieval, Summarization, Resources and Evaluation"
    },
    "dabike19_interspeech": {
      "authors": [
        [
          "Gerardo Roa",
          "Dabike"
        ],
        [
          "Jon",
          "Barker"
        ]
      ],
      "title": "Automatic Lyric Transcription from Karaoke Vocal Tracks: Resources and a Baseline System",
      "original": "2378",
      "page_count": 5,
      "order": 120,
      "p1": "579",
      "pn": "583",
      "abstract": [
        "Automatic sung speech recognition is a relatively understudied topic\nthat has been held back by a lack of large and freely available datasets.\nThis has recently changed thanks to the release of the DAMP Sing! dataset,\na 1100 hour karaoke dataset originating from the social music-making\ncompany, Smule. This paper presents work undertaken to define an easily\nreplicable, automatic speech recognition benchmark for this data. In\nparticular, we describe how transcripts and alignments have been recovered\nfrom Karaoke prompts and timings; how suitable training, development\nand test sets have been defined with varying degrees of accent variability;\nand how language models have been developed using lyric data from the\nLyricWikia website. Initial recognition experiments have been performed\nusing factored-layer TDNN acoustic models with lattice-free MMI training\nusing Kaldi. The best WER is 19.60% &#8212; a new state-of-the-art\nfor this type of data. The paper concludes with a discussion of the\nmany challenging problems that remain to be solved. Dataset definitions\nand Kaldi scripts have been made available so that the benchmark is\neasily replicable.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2378",
      "author_area_id": 12,
      "author_area_label": "Spoken Language Processing: Translation, Information Retrieval, Summarization, Resources and Evaluation"
    },
    "huang19b_interspeech": {
      "authors": [
        [
          "Qiang",
          "Huang"
        ],
        [
          "Thomas",
          "Hain"
        ]
      ],
      "title": "Detecting Mismatch Between Speech and Transcription Using Cross-Modal Attention",
      "original": "2125",
      "page_count": 5,
      "order": 121,
      "p1": "584",
      "pn": "588",
      "abstract": [
        "In this paper, we propose to detect mismatches between speech and transcriptions\nusing deep neural networks. Although it is generally assumed there\nare no mismatches in some speech related applications, it is hard to\navoid the errors due to one reason or another. Moreover, the use of\nmismatched data probably leads to performance reduction when training\na model. In our work, instead of detecting the errors by computing\nthe distance between manual transcriptions and text strings obtained\nusing a speech recogniser, we view mismatch detection as a classification\ntask and merge speech and transcription features using deep neural\nnetworks. To enhance detection ability, we use cross-modal attention\nmechanism in our approach by learning the relevance between the features\nobtained from the two modalities. To evaluate the effectiveness of\nour approach, we test it on Factored WSJCAM0 by randomly setting three\nkinds of mismatch, word deletion, insertion or substitution. To test\nits robustness, we train our models using a small number of samples\nand detect mismatch with different number of words being removed, inserted,\nand substituted. In our experiments, the results show the use of our\napproach for mismatch detection is close to 80% on insertion and deletion\nand outperforms the baseline.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2125",
      "author_area_id": 12,
      "author_area_label": "Spoken Language Processing: Translation, Information Retrieval, Summarization, Resources and Evaluation"
    },
    "vidal19_interspeech": {
      "authors": [
        [
          "Jazm\u00edn",
          "Vidal"
        ],
        [
          "Luciana",
          "Ferrer"
        ],
        [
          "Leonardo",
          "Brambilla"
        ]
      ],
      "title": "EpaDB: A Database for Development of Pronunciation Assessment Systems",
      "original": "1839",
      "page_count": 5,
      "order": 122,
      "p1": "589",
      "pn": "593",
      "abstract": [
        "In this paper, we describe the methodology for collecting and annotating\na new database designed for conducting research and development on\npronunciation assessment. While a significant amount of research has\nbeen done in the area of pronunciation assessment, to our knowledge,\nno database is available for public use for research in the field.\nConsidering this need, we created EpaDB (English Pronunciation by Argentinians\nDatabase), which is composed of English phrases read by native Spanish\nspeakers with different levels of English proficiency. The recordings\nare annotated with ratings of pronunciation quality at phrase-level\nand detailed phonetic alignments and transcriptions indicating which\nphones were actually pronounced by the speakers. We present inter-rater\nagreement, the effect of each phone on overall perceived non-nativeness,\nand the frequency of specific pronunciation errors.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1839",
      "author_area_id": 12,
      "author_area_label": "Spoken Language Processing: Translation, Information Retrieval, Summarization, Resources and Evaluation"
    },
    "angerbauer19_interspeech": {
      "authors": [
        [
          "Katrin",
          "Angerbauer"
        ],
        [
          "Heike",
          "Adel"
        ],
        [
          "Ngoc Thang",
          "Vu"
        ]
      ],
      "title": "Automatic Compression of Subtitles with Neural Networks and its Effect on User Experience",
      "original": "1750",
      "page_count": 5,
      "order": 123,
      "p1": "594",
      "pn": "598",
      "abstract": [
        "Understanding spoken language can be impeded through factors like noisy\nenvironments, hearing impairments or lack of proficiency. Subtitles\ncan help in those cases. However, for fast speech or limited screen\nsize, it might be advantageous to compress the subtitles to their most\nrelevant content. Therefore, we address automatic sentence compression\nin this paper. We propose a neural network model based on an encoder-decoder\napproach with the possibility of integrating the desired compression\nratio. Using this model, we conduct a user study to investigate the\neffects of compressed subtitles on user experience. Our results show\nthat compressed subtitles can suffice for comprehension but may pose\nadditional cognitive load.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1750",
      "author_area_id": 12,
      "author_area_label": "Spoken Language Processing: Translation, Information Retrieval, Summarization, Resources and Evaluation"
    },
    "luo19_interspeech": {
      "authors": [
        [
          "Hongyin",
          "Luo"
        ],
        [
          "Mitra",
          "Mohtarami"
        ],
        [
          "James",
          "Glass"
        ],
        [
          "Karthik",
          "Krishnamurthy"
        ],
        [
          "Brigitte",
          "Richardson"
        ]
      ],
      "title": "Integrating Video Retrieval and Moment Detection in a Unified Corpus for Video Question Answering",
      "original": "1736",
      "page_count": 5,
      "order": 124,
      "p1": "599",
      "pn": "603",
      "abstract": [
        "Traditional video question answering models have been designed to retrieve\nvideos to answer input questions. A drawback of this scenario is that\nusers have to watch the entire video to find their desired answer.\nRecent work presented unsupervised neural models with attention mechanisms\nto find moments or segments from retrieved videos to provide accurate\nanswers to input questions. Although these two tasks look similar,\nthe latter is more challenging because the former task only needs to\njudge whether the question is answered in a video and returns the entire\nvideo, while the latter is expected to judge which moment within a\nvideo matches the question and accurately returns a segment of the\nvideo. Moreover, there is a lack of labeled data for training moment\ndetection models. In this paper, we focus on integrating video retrieval\nand moment detection in a unified corpus. We further develop two models\n&#8212; a self-attention convolutional network and a memory network\n&#8212; for the tasks. Experimental results on our corpus show that\nthe neural models can accurately detect and retrieve moments in supervised\nsettings.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1736",
      "author_area_id": 12,
      "author_area_label": "Spoken Language Processing: Translation, Information Retrieval, Summarization, Resources and Evaluation"
    },
    "gutz19_interspeech": {
      "authors": [
        [
          "Sarah E.",
          "Gutz"
        ],
        [
          "Jun",
          "Wang"
        ],
        [
          "Yana",
          "Yunusova"
        ],
        [
          "Jordan R.",
          "Green"
        ]
      ],
      "title": "Early Identification of Speech Changes Due to Amyotrophic Lateral Sclerosis Using Machine Classification",
      "original": "2967",
      "page_count": 5,
      "order": 125,
      "p1": "604",
      "pn": "608",
      "abstract": [
        "We used a machine learning (ML) approach to detect bulbar amyotrophic\nlateral sclerosis (ALS) prior to the onset of overt speech symptoms.\nThe dataset included speech samples from 123 participants who were\nstratified by sex and into three groups: healthy controls, ALS symptomatic,\nand ALS presymptomatic. We compared models trained on three group pairs\n(symptomatic-control, presymptomatic-control, and all ALS-control participants).\nUsing acoustic features obtained with the OpenSMILE ComParE13 configuration,\nwe tested several feature filtering techniques. ML classification was\nachieved using an SVM model and leave-one-out cross-validation. The\nmost successful model, which was trained on symptomatic-control data,\nyielded an AUC=0.99 for females and AUC=0.91 for males. Models trained\non all ALS-control participants had high diagnostic accuracy for classifying\nsymptomatic and presymptomatic ALS participants (females: AUC=0.85;\nmales: AUC=0.91). Additionally, probabilities from these models correlated\nwith speaking rate (females: Spearman coefficient=-0.60, p&#60;0.001;\nmales: Spearman coefficient=-0.43, p&#60;0.001) and intelligible speaking\nrate (females: Spearman coefficient=-0.65, p&#60;0.001; males: Spearman\ncoefficient=-0.40, p&#60;0.01), indicating their possible use as a\nseverity index of bulbar motor involvement in ALS. These results highlight\nthe importance of stratifying patients by speech severity when testing\ndiagnostic models and demonstrate the potential of ML classification\nin early detection and progress monitoring of ALS.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2967",
      "author_area_id": 5,
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "k19_interspeech": {
      "authors": [
        [
          "Mohamed Ismail Yasar Arafath",
          "K."
        ],
        [
          "Aurobinda",
          "Routray"
        ]
      ],
      "title": "Automatic Detection of Breath Using Voice Activity Detection and SVM Classifier with Application on News Reports",
      "original": "2434",
      "page_count": 5,
      "order": 126,
      "p1": "609",
      "pn": "613",
      "abstract": [
        "Breath detection during speech has broad applications ranging from\nemotion recognition to detection of diseases. Most of the breath detection\nequipment are contact based. In the proposed method, we use a voice\nactivity detector (VAD) to find the non-speech region and searches\nthe breath only in this region since breath is a non-speech activity.\nThis reduces the execution time. A support vector machine (SVM) classifier\nis used with radial basis function (RBF) kernel trained on the cepstrogram\nfeature to detect the breaths in the non-speech regions. The classifier\noutput is post-processed to join breathing segments which are closely\nspaced and remove small duration breaths. Speech breathing rate is\ncalculated as the ratio of the number of breaths to the time between\nthe first and last breath. The algorithm is tested on a student evaluation\ndatabase. The algorithm yields an F1 Score of 94% and root mean square\nerror (RMSE) of 7.08 breaths/min for the speech-breathing rate. The\noutput has been validated using thermal videos. The breaths have been\nclassified as full and partial detection based on the Intersection\nover Union (IOU). The algorithm is also tested on some news channel\nreports which gave a minimum F1 Score of 73%.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2434",
      "author_area_id": 5,
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "heo19_interspeech": {
      "authors": [
        [
          "Hee-Soo",
          "Heo"
        ],
        [
          "Jee-weon",
          "Jung"
        ],
        [
          "Hye-jin",
          "Shim"
        ],
        [
          "Ha-Jin",
          "Yu"
        ]
      ],
      "title": "Acoustic Scene Classification Using Teacher-Student Learning with Soft-Labels",
      "original": "1989",
      "page_count": 5,
      "order": 127,
      "p1": "614",
      "pn": "618",
      "abstract": [
        "Acoustic scene classification identifies an input segment into one\nof the pre-defined classes using spectral information. The spectral\ninformation of acoustic scenes may not be mutually exclusive due to\ncommon acoustic properties across different classes, such as babble\nnoises included in both airports and shopping malls. However, conventional\ntraining procedure based on one-hot labels does not consider the similarities\nbetween different acoustic scenes. We exploit teacher-student learning\nwith the purpose to derive soft-labels that consider common acoustic\nproperties among different acoustic scenes. In teacher-student learning,\nthe teacher network produces soft-labels, based on which the student\nnetwork is trained. We investigate various methods to extract soft-labels\nthat better represent similarities across different scenes. Such attempts\ninclude extracting soft-labels from multiple audio segments that are\ndefined as an identical acoustic scene. Experimental results demonstrate\nthe potential of our approach, showing a classification accuracy of\n77.36% on the DCASE 2018 task 1 validation set.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1989",
      "author_area_id": 5,
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "chen19_interspeech": {
      "authors": [
        [
          "Yanping",
          "Chen"
        ],
        [
          "Hongxia",
          "Jin"
        ]
      ],
      "title": "Rare Sound Event Detection Using Deep Learning and Data Augmentation",
      "original": "1985",
      "page_count": 5,
      "order": 128,
      "p1": "619",
      "pn": "623",
      "abstract": [
        "There is an increasing interest in smart environment and a growing\nadoption of smart devices. Smart assistants such as Google Home and\nAmazon Alexa, although focus on speech, could be extended to identify\ndomestic events in real-time to provide more and better smart functions.\nSound event detection aims to detect multiple target sound events that\nmay happen simultaneously. The task is challenging due to the overlapping\nof sound events, the highly imbalanced nature of target and non-target\ndata, and the complicated real-world background noise. In this paper,\nwe proposed a unified approach that takes advantages of both the deep\nlearning and data augmentation. A convolutional neural network (CNN)\nwas combined with a feed-forward neural network (FNN) to improve the\ndetection performance, and a dynamic time warping based data augmentation\n(DA) method was proposed to address the data imbalance problem. Experiments\non several datasets showed a more than 7% increase in accuracy compared\nto the state-of-the-art approaches.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1985",
      "author_area_id": 5,
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "sharma19_interspeech": {
      "authors": [
        [
          "Bidisha",
          "Sharma"
        ],
        [
          "Haizhou",
          "Li"
        ]
      ],
      "title": "A Combination of Model-Based and Feature-Based Strategy for Speech-to-Singing Alignment",
      "original": "1942",
      "page_count": 5,
      "order": 129,
      "p1": "624",
      "pn": "628",
      "abstract": [
        "Speech and singing are different in many ways. In this work, we propose\na novel method to align phonetically identical spoken lyric with a\nsinging vocal in a speech-singing parallel corpus, that is needed in\nspeech-to-singing conversion. We attempt to align speech to singing\nvocal using a combination of model-based forced alignment and feature-based\ndynamic time warping (DTW). We first obtain the word boundaries of\nspeech and singing vocals with forced alignment using speech and singing\nadapted acoustic models, respectively. We consider that speech acoustic\nmodels are more accurate than singing acoustic models, therefore, boundaries\nof spoken words are more accurate than sung words. By searching in\nthe neighborhood of the sung word boundaries in the singing vocal,\nwe hope to improve the alignment between spoken words and sung words.\nConsidering the word boundaries as landmark, we perform speech-to-singing\nalignment at frame-level using DTW. The proposed method is able to\nachieve a 47.5% reduction in terms of word boundary error over the\nbaseline, and subsequent improvement of singing quality in a speech-to-singing\nconversion system.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1942",
      "author_area_id": 5,
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "shrem19_interspeech": {
      "authors": [
        [
          "Yosi",
          "Shrem"
        ],
        [
          "Matthew",
          "Goldrick"
        ],
        [
          "Joseph",
          "Keshet"
        ]
      ],
      "title": "Dr.VOT: Measuring Positive and Negative Voice Onset Time in the Wild",
      "original": "1735",
      "page_count": 5,
      "order": 130,
      "p1": "629",
      "pn": "633",
      "abstract": [
        "Voice Onset Time (VOT), a key measurement of speech for basic research\nand applied medical studies, is the time between the onset of a stop\nburst and the onset of voicing. When the voicing onset precedes burst\nonset the VOT is negative; if voicing onset follows the burst, it is\npositive. In this work, we present a deep-learning model for accurate\nand reliable measurement of VOT in naturalistic speech. The proposed\nsystem addresses two critical issues: it can measure positive and negative\nVOT equally well, and it is trained to be robust to variation across\nannotations. Our approach is based on the structured prediction framework,\nwhere the feature functions are defined to be RNNs. These learn to\ncapture segmental variation in the signal. Results suggest that our\nmethod substantially improves over the current state-of-the-art. In\ncontrast to previous work, our Deep and Robust VOT annotator, Dr.VOT,\ncan successfully estimate negative VOTs while maintaining state-of-the-art\nperformance on positive VOTs. This high level of performance generalizes\nto new corpora without further retraining.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1735",
      "author_area_id": 5,
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "hui19_interspeech": {
      "authors": [
        [
          "J.",
          "Hui"
        ],
        [
          "Y.",
          "Wei"
        ],
        [
          "S.T.",
          "Chen"
        ],
        [
          "R.H.Y.",
          "So"
        ]
      ],
      "title": "Effects of Base-Frequency and Spectral Envelope on Deep-Learning Speech Separation and Recognition Models",
      "original": "1715",
      "page_count": 5,
      "order": 131,
      "p1": "634",
      "pn": "638",
      "abstract": [
        "Base-frequencies (F0) and spectral envelopes play an important role\nin speech separation and recognition by humans. Two experiments were\nconducted to study how trained networks for multi-speaker speech separation/recognition\nare affected by difference of F0 and spectral envelopes between source\nsignals. The first experiment examined the effects of natural F0/envelope\non the performance of speech separation. Results showed that when the\ntwo target signals differed in F0 by &#177;3 semitones or more or differed\nin the envelope by a scaling factor larger than 1.08 or less than 0.92,\nseparation performance improved significantly. This is consistent with\nhuman listeners and is the first finding for deep learning-network\n(DNN) models. The second experiment tested the effect of F0/envelope\ndifference on multi-speaker automatic speech recognition(ASR) system&#8217;s\nperformance. Results showed that multi-speaker recognition result also\nsignificantly rely on F0/envelope differences. The overall results\nindicated that the dependency of the existing automatic systems on\nmonaural cues is similar to that of human, while automatic systems\nstill perform inferior than human on same tasks.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1715",
      "author_area_id": 5,
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "shah19_interspeech": {
      "authors": [
        [
          "Nirmesh J.",
          "Shah"
        ],
        [
          "Hemant A.",
          "Patil"
        ]
      ],
      "title": "Phone Aware Nearest Neighbor Technique Using Spectral Transition Measure for Non-Parallel Voice Conversion",
      "original": "1504",
      "page_count": 5,
      "order": 132,
      "p1": "639",
      "pn": "643",
      "abstract": [
        "Nearest Neighbor (NN)-based alignment techniques are popular in non-parallel\nVoice Conversion (VC). The performance of NN-based alignment improves\nwith the information about phone boundary. However, estimating the\nexact phone boundary is a challenging task. If text corresponding to\nthe utterance is available, the Hidden Markov Model (HMM) can be used\nto identify the phone boundaries. However, it requires a large amount\nof training data that is difficult to collect in realistic VC scenarios.\nHence, we propose to exploit a Spectral Transition Measure (STM)-based\nalignment technique that does not require apriori training data. The\nidea behind STM is that neurons in the auditory or visual cortex respond\nstrongly to the  transitional stimuli compared to the steady-state\nstimuli. The phone boundaries estimated using the STM algorithm are\nthen applied to the NN technique to obtain the aligned spectral features\nof the source and target speakers. Proposed STM+NN alignment technique\nis giving on an average 13.67% relative improvement in phonetic accuracy\n(PA) compared to the NN-based alignment technique. The improvement\nin %PA after alignment has positively reflected in the better performance\nin terms of speech quality and speaker similarity (in particular, a\nrelative improvement of 13.63% and 13.26% , respectively) of the converted\nvoice.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1504",
      "author_area_id": 5,
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "shankar19_interspeech": {
      "authors": [
        [
          "Ravi",
          "Shankar"
        ],
        [
          "Archana",
          "Venkataraman"
        ]
      ],
      "title": "Weakly Supervised Syllable Segmentation by Vowel-Consonant Peak Classification",
      "original": "1450",
      "page_count": 5,
      "order": 133,
      "p1": "644",
      "pn": "648",
      "abstract": [
        "We present a novel approach for blind syllable segmentation that combines\nmodel-based feature selection with data-driven classification. In particular,\nwe learn a function that maps short-term energy peaks of a speech utterance\nonto either the vowel or consonant class. The features used for classification\ncapture spectral and energy signatures which are characteristic of\nthe phonetic properties of the English language. The identified vowel\npeaks subsequently act as the nucleus of our syllable segments. We\ndemonstrate the effectiveness of our proposed method using nested cross\nvalidation on 400 unique test utterances taken randomly from the TIMIT\ndataset containing over 5000 syllables in total. Our hybrid approach\nachieves lower insertion rate than the state-of-the-art segmentation\nmethods and a lower deletion rate than all the baseline comparisons.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1450",
      "author_area_id": 5,
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "mateju19_interspeech": {
      "authors": [
        [
          "Lukas",
          "Mateju"
        ],
        [
          "Petr",
          "Cerva"
        ],
        [
          "Jindrich",
          "Zdansky"
        ]
      ],
      "title": "An Approach to Online Speaker Change Point Detection Using DNNs and WFSTs",
      "original": "1407",
      "page_count": 5,
      "order": 134,
      "p1": "649",
      "pn": "653",
      "abstract": [
        "In this paper, a new approach to speaker change point (SCP) detection\nis presented. This method is suitable for online applications (e.g.,\nreal-time broadcast monitoring). It is designed in a series of consecutive\nexperiments, aiming at quality of detection as well as low latency.\nThe resulting scheme utilizes a convolution neural network (CNN), whose\noutput is smoothed by a decoder. The CNN is trained using data complemented\nby artificial examples to reduce different types of errors, and the\ndecoder is based on a weighted finite state transducer (WFST) with\nthe forced length of the transition model. Results obtained on data\ntaken from the COST278 database show that our online approach yields\nresults comparable with an offline multi-pass LIUM toolkit while operating\nonline with a low latency.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1407",
      "author_area_id": 5,
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "tang19_interspeech": {
      "authors": [
        [
          "Zhenyu",
          "Tang"
        ],
        [
          "John D.",
          "Kanu"
        ],
        [
          "Kevin",
          "Hogan"
        ],
        [
          "Dinesh",
          "Manocha"
        ]
      ],
      "title": "Regression and Classification for Direction-of-Arrival Estimation with Convolutional Recurrent Neural Networks",
      "original": "1111",
      "page_count": 5,
      "order": 135,
      "p1": "654",
      "pn": "658",
      "abstract": [
        "We present a novel learning-based approach to estimate the direction-of-arrival\n(DOA) of a sound source using a convolutional recurrent neural network\n(CRNN) trained via regression on synthetic data and Cartesian labels.\nWe also describe an improved method to generate synthetic data to train\nthe neural network using state-of-the-art sound propagation algorithms\nthat model specular as well as diffuse reflections of sound. We compare\nour model against three other CRNNs trained using different formulations\nof the same problem: classification on categorical labels, and regression\non spherical coordinate labels. In practice, our model achieves up\nto 43% decrease in angular error over prior methods. The use of diffuse\nreflection results in 34% and 41% reduction in angular prediction errors\non LOCATA and SOFA datasets, respectively, over prior methods based\non image-source methods. Our method results in an additional 3% error\nreduction over prior schemes that use classification networks, and\nwe use 36% fewer network parameters.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1111",
      "author_area_id": 5,
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "paul19_interspeech": {
      "authors": [
        [
          "Dipjyoti",
          "Paul"
        ],
        [
          "Yannis",
          "Pantazis"
        ],
        [
          "Yannis",
          "Stylianou"
        ]
      ],
      "title": "Non-Parallel Voice Conversion Using Weighted Generative Adversarial Networks",
      "original": "2869",
      "page_count": 5,
      "order": 136,
      "p1": "659",
      "pn": "663",
      "abstract": [
        "In this paper, we suggest a novel way to train Generative Adversarial\nNetwork (GAN) for the purpose of non-parallel, many-to-many voice conversion.\nThe goal of voice conversion (VC) is to transform speech from a source\nspeaker to that of a target speaker without changing the phonetic contents.\nBased on ideas from Game Theory, we suggest to multiply the gradient\nof the Generator with suitable weights. Weights are calculated so that\nthey increase the power of fake samples that fool the Discriminator\nresulting in a stronger Generator. Motivated by a recently presented\nGAN based approach for VC, StarGAN-VC, we suggest a variation to StarGAN,\nreferred to as Weighted StarGAN (WeStarGAN). The experiments are conducted\non standard CMU ARCTIC database. WeStarGAN-VC approach achieves significantly\nbetter relative performance and is clearly preferred over recently\nproposed StarGAN-VC method in terms of speech subjective quality and\nspeaker similarity with 75% and 65% preference scores, respectively.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2869",
      "author_area_id": 7,
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "chou19_interspeech": {
      "authors": [
        [
          "Ju-chieh",
          "Chou"
        ],
        [
          "Hung-Yi",
          "Lee"
        ]
      ],
      "title": "One-Shot Voice Conversion by Separating Speaker and Content Representations with Instance Normalization",
      "original": "2663",
      "page_count": 5,
      "order": 137,
      "p1": "664",
      "pn": "668",
      "abstract": [
        "Recently, voice conversion (VC) without parallel data has been successfully\nadapted to multi-target scenario in which a single model is trained\nto convert the input voice to many different speakers. However, such\nmodel suffers from the limitation that it can only convert the voice\nto the speakers in the training data, which narrows down the applicable\nscenario of VC. In this paper, we proposed a novel one-shot VC approach\nwhich is able to perform VC by only an example utterance from source\nand target speaker respectively, and the source and target speaker\ndo not even need to be seen during training. This is achieved by disentangling\nspeaker and content representations with instance normalization (IN).\nObjective and subjective evaluation shows that our model is able to\ngenerate the voice similar to target speaker. In addition to the performance\nmeasurement, we also demonstrate that this model is able to learn meaningful\nspeaker representations without any supervision.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2663",
      "author_area_id": 7,
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "lu19_interspeech": {
      "authors": [
        [
          "Hui",
          "Lu"
        ],
        [
          "Zhiyong",
          "Wu"
        ],
        [
          "Dongyang",
          "Dai"
        ],
        [
          "Runnan",
          "Li"
        ],
        [
          "Shiyin",
          "Kang"
        ],
        [
          "Jia",
          "Jia"
        ],
        [
          "Helen",
          "Meng"
        ]
      ],
      "title": "One-Shot Voice Conversion with Global Speaker Embeddings",
      "original": "2365",
      "page_count": 5,
      "order": 138,
      "p1": "669",
      "pn": "673",
      "abstract": [
        "Building a voice conversion (VC) system for a new target speaker typically\nrequires a large amount of speech data from the target speaker. This\npaper investigates a method to build a VC system for arbitrary target\nspeaker using one given utterance without any adaptation training process.\nInspired by global style tokens (GSTs), which recently has been shown\nto be effective in controlling the style of synthetic speech, we propose\nthe use of global speaker embeddings (GSEs) to control the conversion\ntarget of the VC system. Speaker-independent phonetic posteriorgrams\n(PPGs) are employed as the local condition input to a conditional WaveNet\nsynthesizer for waveform generation of the target speaker. Meanwhile,\nspectrograms are extracted from the given utterance and fed into a\nreference encoder, the generated reference embedding is then employed\nas attention query to the GSEs to produce the speaker embedding, which\nis employed as the global condition input to the WaveNet synthesizer\nto control the generated waveform&#8217;s speaker identity. In experiments,\nwhen compared with an adaptation training based any-to-any VC system,\nthe proposed GSEs based VC approach performs equally well or better\nin both speech naturalness and speaker similarity, with apparently\nhigher flexibility to the comparison.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2365",
      "author_area_id": 7,
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "tobing19_interspeech": {
      "authors": [
        [
          "Patrick Lumban",
          "Tobing"
        ],
        [
          "Yi-Chiao",
          "Wu"
        ],
        [
          "Tomoki",
          "Hayashi"
        ],
        [
          "Kazuhiro",
          "Kobayashi"
        ],
        [
          "Tomoki",
          "Toda"
        ]
      ],
      "title": "Non-Parallel Voice Conversion with Cyclic Variational Autoencoder",
      "original": "2307",
      "page_count": 5,
      "order": 139,
      "p1": "674",
      "pn": "678",
      "abstract": [
        "In this paper, we present a novel technique for a non-parallel voice\nconversion (VC) with the use of cyclic variational auto-encoder (CycleVAE)-based\nspectral modeling. In a variational autoencoder (VAE) framework, a\nlatent space, usually with a Gaussian prior, is used to encode a set\nof input features. In a VAE-based VC, the encoded latent features are\nfed into a decoder, along with speaker-coding features, to generate\nestimated spectra with either the original speaker identity (reconstructed)\nor another speaker identity (converted). Due to the non-parallel modeling\ncondition, the converted spectra can not be directly optimized, which\nheavily degrades the performance of a VAE-based VC. In this work, to\novercome this problem, we propose to use CycleVAE-based spectral model\nthat indirectly optimizes the conversion flow by recycling the converted\nfeatures back into the system to obtain corresponding cyclic reconstructed\nspectra that can be directly optimized. The cyclic flow can be continued\nby using the cyclic reconstructed features as input for the next cycle.\nThe experimental results demonstrate the effectiveness of the proposed\nCycleVAE-based VC, which yields higher accuracy of converted spectra,\ngenerates latent features with higher correlation degree, and significantly\nimproves the quality and conversion accuracy of the converted speech.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2307",
      "author_area_id": 7,
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "kaneko19_interspeech": {
      "authors": [
        [
          "Takuhiro",
          "Kaneko"
        ],
        [
          "Hirokazu",
          "Kameoka"
        ],
        [
          "Kou",
          "Tanaka"
        ],
        [
          "Nobukatsu",
          "Hojo"
        ]
      ],
      "title": "StarGAN-VC2: Rethinking Conditional Methods for StarGAN-Based Voice Conversion",
      "original": "2236",
      "page_count": 5,
      "order": 140,
      "p1": "679",
      "pn": "683",
      "abstract": [
        "Non-parallel multi-domain voice conversion (VC) is a technique for\nlearning mappings among multiple domains without relying on parallel\ndata. This is important but challenging owing to the requirement of\nlearning multiple mappings and the non-availability of explicit supervision.\nRecently, StarGAN-VC has garnered attention owing to its ability to\nsolve this problem only using a single generator. However, there is\nstill a gap between real and converted speech. To bridge this gap,\nwe rethink conditional methods of StarGAN-VC, which are key components\nfor achieving non-parallel multi-domain VC in a single model, and propose\nan improved variant called StarGAN-VC2. Particularly, we rethink conditional\nmethods in two aspects: training objectives and network architectures.\nFor the former, we propose a source-and-target conditional adversarial\nloss that allows all source domain data to be convertible to the target\ndomain data. For the latter, we introduce a modulation-based conditional\nmethod that can transform the modulation of the acoustic feature in\na domain-specific manner. We evaluated our methods on non-parallel\nmulti-speaker VC. An objective evaluation demonstrates that our proposed\nmethods improve speech quality in terms of both global and local structure\nmeasures. Furthermore, a subjective evaluation shows that StarGAN-VC2\noutperforms StarGAN-VC in terms of naturalness and speaker similarity.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2236",
      "author_area_id": 7,
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "kurita19_interspeech": {
      "authors": [
        [
          "Yusuke",
          "Kurita"
        ],
        [
          "Kazuhiro",
          "Kobayashi"
        ],
        [
          "Kazuya",
          "Takeda"
        ],
        [
          "Tomoki",
          "Toda"
        ]
      ],
      "title": "Robustness of Statistical Voice Conversion Based on Direct Waveform Modification Against Background Sounds",
      "original": "2206",
      "page_count": 5,
      "order": 141,
      "p1": "684",
      "pn": "688",
      "abstract": [
        "This paper presents an investigation of the robustness of statistical\nvoice conversion (VC) under noisy environments. To develop various\nVC applications, such as augmented vocal production and augmented speech\nproduction, it is necessary to handle noisy input speech because some\nbackground sounds, such as external noise and an accompanying sound,\nusually exist in a real environment. In this paper, we investigate\nan impact of the background sounds on the conversion performance in\nsinging voice conversion focusing on two main VC frameworks, 1) vocoder-based\nVC and 2) vocoder-free VC based on direct waveform modification. We\nconduct a subjective evaluation on the converted singing voice quality\nunder noisy conditions and reveal that the vocoder-free VC is more\nrobust against background sounds compared with the vocoder-based VC.\nWe also analyze the robustness of statistical VC and show that a kurtosis\nratio of power spectral components before and after conversion is useful\nas an objective metric to evaluate it without using any target reference\nsignals.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2206",
      "author_area_id": 7,
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "zhao19b_interspeech": {
      "authors": [
        [
          "Shengkui",
          "Zhao"
        ],
        [
          "Trung Hieu",
          "Nguyen"
        ],
        [
          "Hao",
          "Wang"
        ],
        [
          "Bin",
          "Ma"
        ]
      ],
      "title": "Fast Learning for Non-Parallel Many-to-Many Voice Conversion with Residual Star Generative Adversarial Networks",
      "original": "2067",
      "page_count": 5,
      "order": 142,
      "p1": "689",
      "pn": "693",
      "abstract": [
        "This paper proposes a fast learning framework for non-parallel many-to-many\nvoice conversion with residual Star Generative Adversarial Networks\n(StarGAN). In addition to the state-of-the-art StarGAN-VC approach\nthat learns an unreferenced mapping between a group of speakers&#8217;\nacoustic features for nonparallel many-to-many voice conversion, our\nmethod, which we call Res-StarGAN-VC, presents an enhancement by incorporating\na residual mapping. The idea is to leverage on the shared linguistic\ncontent between source and target features during conversion. The residual\nmapping is realized by using identity shortcut connections from the\ninput to the output of the generator in Res-StarGAN-VC. Such shortcut\nconnections accelerate the learning process of the network with no\nincrease of parameters and computational complexity. They also help\ngenerate high-quality fake samples at the very beginning of the adversarial\ntraining. Experiments and subjective evaluations show that the proposed\nmethod offers (1) significantly faster convergence in adversarial training\nand (2) clearer pronunciations and better speaker similarity of converted\nspeech, compared to the StarGAN-VC baseline on both mono-lingual and\ncross-lingual many-to-many voice conversion tasks.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2067",
      "author_area_id": 7,
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "juvela19_interspeech": {
      "authors": [
        [
          "Lauri",
          "Juvela"
        ],
        [
          "Bajibabu",
          "Bollepalli"
        ],
        [
          "Junichi",
          "Yamagishi"
        ],
        [
          "Paavo",
          "Alku"
        ]
      ],
      "title": "GELP: GAN-Excited Linear Prediction for Speech Synthesis from Mel-Spectrogram",
      "original": "2008",
      "page_count": 5,
      "order": 143,
      "p1": "694",
      "pn": "698",
      "abstract": [
        "Recent advances in neural network -based text-to-speech have reached\nhuman level naturalness in synthetic speech. The present sequence-to-sequence\nmodels can directly map text to mel-spectrogram acoustic features,\nwhich are convenient for modeling, but present additional challenges\nfor vocoding (i.e., waveform generation from the acoustic features).\nHigh-quality synthesis can be achieved with neural vocoders, such as\nWaveNet, but such autoregressive models suffer from slow sequential\ninference. Meanwhile, their existing parallel inference counterparts\nare difficult to train and require increasingly large model sizes.\nIn this paper, we propose an alternative training strategy for a parallel\nneural vocoder utilizing generative adversarial networks, and integrate\na linear predictive synthesis filter into the model. Results show that\nthe proposed model achieves significant improvement in inference speed,\nwhile outperforming a WaveNet in copy-synthesis quality.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2008"
    },
    "yamamoto19b_interspeech": {
      "authors": [
        [
          "Ryuichi",
          "Yamamoto"
        ],
        [
          "Eunwoo",
          "Song"
        ],
        [
          "Jae-Min",
          "Kim"
        ]
      ],
      "title": "Probability Density Distillation with Generative Adversarial Networks for High-Quality Parallel Waveform Generation",
      "original": "1965",
      "page_count": 5,
      "order": 144,
      "p1": "699",
      "pn": "703",
      "abstract": [
        "This paper proposes an effective probability density distillation (PDD)\nalgorithm for WaveNet-based parallel waveform generation (PWG) systems.\nRecently proposed teacher-student frameworks in the PWG system have\nsuccessfully achieved a real-time generation of speech signals. However,\nthe difficulties optimizing the PDD criteria without auxiliary losses\nresult in quality degradation of synthesized speech. To generate more\nnatural speech signals within the teacher-student framework, we propose\na novel optimization criterion based on generative adversarial networks\n(GANs). In the proposed method, the inverse autoregressive flow-based\nstudent model is incorporated as a generator in the GAN framework,\nand jointly optimized by the PDD mechanism with the proposed adversarial\nlearning method. As this process encourages the student to model the\ndistribution of realistic speech waveform, the perceptual quality of\nthe synthesized speech becomes much more natural. Our experimental\nresults verify that the PWG systems with the proposed method outperform\nboth those using conventional approaches, and also autoregressive generation\nsystems with a well-trained teacher WaveNet.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1965",
      "author_area_id": 7,
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "mohammadi19_interspeech": {
      "authors": [
        [
          "Seyed Hamidreza",
          "Mohammadi"
        ],
        [
          "Taehwan",
          "Kim"
        ]
      ],
      "title": "One-Shot Voice Conversion with Disentangled Representations by Leveraging Phonetic Posteriorgrams",
      "original": "1798",
      "page_count": 5,
      "order": 145,
      "p1": "704",
      "pn": "708",
      "abstract": [
        "We propose voice conversion model from arbitrary source speaker to\narbitrary target speaker with disentangled representations. Voice conversion\nis a task to convert the voice of spoken utterance of source speaker\nto that of target speaker. Most prior work require to know either source\nspeaker or target speaker or both in training, with either parallel\nor non-parallel corpus. Instead, we study the problem of voice conversion\nin nonparallel speech corpora and one-shot learning setting. We convert\nan arbitrary sentences of an arbitrary source speaker to target speakers\ngiven only one or few target speaker training utterances. To achieve\nthis, we propose to use disentangled representations of speaker identity\nand linguistic context. We use a recurrent neural network (RNN) encoder\nfor speaker embedding and phonetic posteriorgram as linguistic context\nencoding, along with a RNN decoder to generate converted utterances.\nOurs is a simpler model without adversarial training or hierarchical\nmodel design and thus more efficient. In the subjective tests, our\napproach achieved significantly better results compared to baseline\nregarding similarity.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1798"
    },
    "huang19c_interspeech": {
      "authors": [
        [
          "Wen-Chin",
          "Huang"
        ],
        [
          "Yi-Chiao",
          "Wu"
        ],
        [
          "Chen-Chou",
          "Lo"
        ],
        [
          "Patrick Lumban",
          "Tobing"
        ],
        [
          "Tomoki",
          "Hayashi"
        ],
        [
          "Kazuhiro",
          "Kobayashi"
        ],
        [
          "Tomoki",
          "Toda"
        ],
        [
          "Yu",
          "Tsao"
        ],
        [
          "Hsin-Min",
          "Wang"
        ]
      ],
      "title": "Investigation of F0 Conditioning and Fully Convolutional Networks in Variational Autoencoder Based Voice Conversion",
      "original": "1774",
      "page_count": 5,
      "order": 146,
      "p1": "709",
      "pn": "713",
      "abstract": [
        "In this work, we investigate the effectiveness of two techniques for\nimproving variational autoencoder (VAE) based voice conversion (VC).\nFirst, we reconsider the relationship between vocoder features extracted\nusing the high quality vocoders adopted in conventional VC systems,\nand hypothesize that the spectral features are in fact F0 dependent.\nSuch hypothesis implies that during the conversion phase, the latent\ncodes and the converted features in VAE based VC are in fact source\nF0 dependent. To this end, we propose to utilize the F0 as an additional\ninput of the decoder. The model can learn to disentangle the latent\ncode from the F0 and thus generates converted F0 dependent converted\nfeatures. Second, to better capture temporal dependencies of the spectral\nfeatures and the F0 pattern, we replace the frame wise conversion structure\nin the original VAE based VC framework with a fully convolutional network\nstructure. Our experiments demonstrate that the degree of disentanglement\nas well as the naturalness of the converted speech are indeed improved.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1774",
      "author_area_id": 7,
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "liu19b_interspeech": {
      "authors": [
        [
          "Songxiang",
          "Liu"
        ],
        [
          "Yuewen",
          "Cao"
        ],
        [
          "Xixin",
          "Wu"
        ],
        [
          "Lifa",
          "Sun"
        ],
        [
          "Xunying",
          "Liu"
        ],
        [
          "Helen",
          "Meng"
        ]
      ],
      "title": "Jointly Trained Conversion Model and WaveNet Vocoder for Non-Parallel Voice Conversion Using Mel-Spectrograms and Phonetic Posteriorgrams",
      "original": "1316",
      "page_count": 5,
      "order": 147,
      "p1": "714",
      "pn": "718",
      "abstract": [
        "The N10 system in the Voice Conversion Challenge 2018 (VCC 2018) has\nachieved high voice conversion (VC) performance in terms of speech\nnaturalness and speaker similarity. We believe that further improvements\ncan be gained from joint optimization (instead of separate optimization)\nof the conversion model and WaveNet vocoder, as well as leveraging\ninformation from the acoustic representation of the speech waveform,\ne.g. from Mel-spectrograms. In this paper, we propose a VC architecture\nto jointly train a conversion model that maps phonetic posteriorgrams\n(PPGs) to Mel-spectrograms and a WaveNet vocoder. The conversion model\nhas a bottle-neck layer, whose outputs are concatenated with PPGs before\nbeing fed into the WaveNet vocoder as local conditioning. A weighted\nsum of a Mel-spectrogram prediction loss and a WaveNet loss is used\nas the objective function to jointly optimize parameters of the conversion\nmodel and the WaveNet vocoder. Objective and subjective evaluation\nresults show that the proposed approach is capable of achieving significantly\nimproved quality in voice conversion in terms of speech naturalness\nand speaker similarity of the converted speech for both cross-gender\nand intra-gender conversions.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1316",
      "author_area_id": 7,
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "chen19b_interspeech": {
      "authors": [
        [
          "Li-Wei",
          "Chen"
        ],
        [
          "Hung-Yi",
          "Lee"
        ],
        [
          "Yu",
          "Tsao"
        ]
      ],
      "title": "Generative Adversarial Networks for Unpaired Voice Transformation on Impaired Speech",
      "original": "1265",
      "page_count": 5,
      "order": 148,
      "p1": "719",
      "pn": "723",
      "abstract": [
        "This paper focuses on using voice conversion (VC) to improve the speech\nintelligibility of surgical patients who have had parts of their articulators\nremoved. Due to the difficulty of data collection, VC without parallel\ndata is highly desired. Although techniques for unparallel VC &#8212;\nfor example, CycleGAN &#8212; have been developed, they usually focus\non transforming the speaker identity, and directly transforming the\nspeech of one speaker to that of another speaker and as such do not\naddress the task here. In this paper, we propose a new approach for\nunparallel VC. The proposed approach transforms impaired speech to\nnormal speech while preserving the linguistic content and speaker characteristics.\nTo our knowledge, this is the first end-to-end GAN-based unsupervised\nVC model applied to impaired speech. The experimental results show\nthat the proposed approach outperforms CycleGAN.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1265",
      "author_area_id": 7,
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "ding19_interspeech": {
      "authors": [
        [
          "Shaojin",
          "Ding"
        ],
        [
          "Ricardo",
          "Gutierrez-Osuna"
        ]
      ],
      "title": "Group Latent Embedding for Vector Quantized Variational Autoencoder in Non-Parallel Voice Conversion",
      "original": "1198",
      "page_count": 5,
      "order": 149,
      "p1": "724",
      "pn": "728",
      "abstract": [
        "This paper proposes a Group Latent Embedding for Vector Quantized Variational\nAutoencoders (VQ-VAE) used in nonparallel Voice Conversion (VC). Previous\nstudies have shown that VQ-VAE can generate high-quality VC syntheses\nwhen it is paired with a powerful decoder. However, in a conventional\nVQ-VAE, adjacent atoms in the embedding dictionary can represent entirely\ndifferent phonetic content. Therefore, the VC syntheses can have mispronunciations\nand distortions whenever the output of the encoder is quantized to\nan atom representing entirely different phonetic content. To address\nthis issue, we propose an approach that divides the embedding dictionary\ninto groups and uses the weighted average of atoms in the nearest group\nas the latent embedding. We conducted both objective and subjective\nexperiments on the non-parallel CSTR VCTK corpus. Results show that\nthe proposed approach significantly improves the acoustic quality of\nthe VC syntheses compared to the traditional VQ-VAE (13.7% relative\nimprovement) while retaining the voice identity of the target speaker.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1198",
      "author_area_id": 7,
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "stephenson19_interspeech": {
      "authors": [
        [
          "Cory",
          "Stephenson"
        ],
        [
          "Gokce",
          "Keskin"
        ],
        [
          "Anil",
          "Thomas"
        ],
        [
          "Oguz H.",
          "Elibol"
        ]
      ],
      "title": "Semi-Supervised Voice Conversion with Amortized Variational Inference",
      "original": "1840",
      "page_count": 5,
      "order": 150,
      "p1": "729",
      "pn": "733",
      "abstract": [
        "In this work we introduce a semi-supervised approach to the voice conversion\nproblem, in which speech from a source speaker is converted into speech\nof a target speaker. The proposed method makes use of both parallel\nand non-parallel utterances from the source and target simultaneously\nduring training. This approach can be used to extend existing parallel\ndata voice conversion systems such that they can be trained with semi-supervision.\nWe show that incorporating semi-supervision improves the voice conversion\nperformance compared to fully supervised training when the number of\nparallel utterances is limited as in many practical applications. Additionally,\nwe find that increasing the number non-parallel utterances used in\ntraining continues to improve performance when the amount of parallel\ntraining data is held constant.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1840",
      "author_area_id": 7,
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "dey19_interspeech": {
      "authors": [
        [
          "Subhadeep",
          "Dey"
        ],
        [
          "Petr",
          "Motlicek"
        ],
        [
          "Trung",
          "Bui"
        ],
        [
          "Franck",
          "Dernoncourt"
        ]
      ],
      "title": "Exploiting Semi-Supervised Training Through a Dropout Regularization in End-to-End Speech Recognition",
      "original": "3246",
      "page_count": 5,
      "order": 151,
      "p1": "734",
      "pn": "738",
      "abstract": [
        "In this paper, we explore various approaches for semi-supervised learning\nin an end-to-end automatic speech recognition (ASR) framework. The\nfirst step in our approach involves training a seed model on the limited\namount of labelled data. Additional unlabelled speech data is employed\nthrough a data-selection mechanism to obtain the best hypothesized\noutput, further used to retrain the seed model. However, uncertainties\nof the model may not be well captured with a single hypothesis. As\nopposed to this technique, we apply a dropout mechanism to capture\nthe uncertainty by obtaining multiple hypothesized text transcripts\nof an speech recording. We assume that the diversity of automatically\ngenerated transcripts for an utterance will implicitly increase the\nreliability of the model. Finally, the data-selection process is also\napplied on these hypothesized transcripts to reduce the uncertainty.\nExperiments on freely-available TEDLIUM corpus and proprietary Adobe&#8217;s\ninternal dataset show that the proposed approach significantly reduces\nASR errors, compared to the baseline model.\n"
      ],
      "doi": "10.21437/Interspeech.2019-3246",
      "author_area_id": 8,
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "kim19_interspeech": {
      "authors": [
        [
          "Chanwoo",
          "Kim"
        ],
        [
          "Minkyu",
          "Shin"
        ],
        [
          "Abhinav",
          "Garg"
        ],
        [
          "Dhananjaya",
          "Gowda"
        ]
      ],
      "title": "Improved Vocal Tract Length Perturbation for a State-of-the-Art End-to-End Speech Recognition System",
      "original": "3227",
      "page_count": 5,
      "order": 152,
      "p1": "739",
      "pn": "743",
      "abstract": [
        "In this paper, we present an improved vocal tract length perturbation\n(VTLP) algorithm as a data augmentation technique. VTLP is usually\naccomplished by adjusting the center frequencies of mel filterbank\nin [1]. Compared to the conventional approach, we re-synthesize waveforms\nfrom the frequency-warped spectra using overlap and addition (OLA).\nThis approach had two advantages: First, we can apply an &#8220;acoustic\nsimulator&#8221; [2, 3] after performing the VTLP-based frequency warping.\nSecond, we may use a different window length for frequency warping\nfrom that used in feature processing. We observe that the best performance\nwas obtained when the warping coefficient distribution is between 0.8\nand 1.2, and the window length is 50 ms. We obtained 3.66% WER and\n12.39% WER on the Librispeech test-clean and test-other using an attention-based\nend-to-end speech recognition system without using any Language Models\n(LMs). Using the shallow-fusion technique with a Transformer LM, we\nachieved 2.44% WER and 8.29% WER on the Librispeech test-clean and\ntest-other sets. To the best of our knowledge, the 2.44% WER on the\ntest-clean is the best result ever reported on this test set.\n"
      ],
      "doi": "10.21437/Interspeech.2019-3227",
      "author_area_id": 8,
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "zhu19_interspeech": {
      "authors": [
        [
          "Han",
          "Zhu"
        ],
        [
          "Li",
          "Wang"
        ],
        [
          "Pengyuan",
          "Zhang"
        ],
        [
          "Yonghong",
          "Yan"
        ]
      ],
      "title": "Multi-Accent Adaptation Based on Gate Mechanism",
      "original": "3155",
      "page_count": 5,
      "order": 153,
      "p1": "744",
      "pn": "748",
      "abstract": [
        "When only a limited amount of accented speech data is available, to\npromote multi-accent speech recognition performance, the conventional\napproach is accent-specific adaptation, which adapts the baseline model\nto multiple target accents independently. To simplify the adaptation\nprocedure, we explore adapting the baseline model to multiple target\naccents simultaneously with multi-accent mixed data. Thus, we propose\nusing accent-specific top layer with gate mechanism (AST-G) to realize\nmulti-accent adaptation. Compared with the baseline model and accent-specific\nadaptation, AST-G achieves 9.8% and 1.9% average relative WER reduction\nrespectively. However, in real-world applications, we can&#8217;t obtain\nthe accent category label for inference in advance. Therefore, we apply\nusing an accent classifier to predict the accent label. To jointly\ntrain the acoustic model and the accent classifier, we propose the\nmulti-task learning with gate mechanism (MTL-G). As the accent label\nprediction could be inaccurate, it performs worse than the accent-specific\nadaptation. Yet, in comparison with the baseline model, MTL-G achieves\n5.1% average relative WER reduction.\n"
      ],
      "doi": "10.21437/Interspeech.2019-3155",
      "author_area_id": 8,
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "guo19_interspeech": {
      "authors": [
        [
          "Pengcheng",
          "Guo"
        ],
        [
          "Sining",
          "Sun"
        ],
        [
          "Lei",
          "Xie"
        ]
      ],
      "title": "Unsupervised Adaptation with Adversarial Dropout Regularization for Robust Speech Recognition",
      "original": "2544",
      "page_count": 5,
      "order": 154,
      "p1": "749",
      "pn": "753",
      "abstract": [
        "Recent adversarial methods proposed for unsupervised domain adaptation\nof acoustic models try to fool a specific domain discriminator and\nlearn both senone-discriminative and domain-invariant hidden feature\nrepresentations. However, a drawback of these approaches is that the\nfeature generator simply aligns different features into the same distribution\nwithout considering the class boundaries of the target domain data.\nThus, ambiguous target domain features can be generated near the decision\nboundaries, decreasing speech recognition performance. In this study,\nwe propose to use Adversarial Dropout Regularization (ADR) in acoustic\nmodeling to overcome the foregoing issue. Specifically, we optimize\nthe senone classifier to make its decision boundaries lie in the class\nboundaries of unlabeled target data. Then, the feature generator learns\nto create features far away from the decision boundaries, which are\nmore discriminative. We apply the ADR approach on the CHiME-3 corpus\nand the proposed method yields up to 12.9% relative WER reductions\ncompared with the baseline trained on source domain data only and further\nimprovement over the widely used gradient reversal layer method.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2544",
      "author_area_id": 8,
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "kitza19_interspeech": {
      "authors": [
        [
          "Markus",
          "Kitza"
        ],
        [
          "Pavel",
          "Golik"
        ],
        [
          "Ralf",
          "Schl\u00fcter"
        ],
        [
          "Hermann",
          "Ney"
        ]
      ],
      "title": "Cumulative Adaptation for BLSTM Acoustic Models",
      "original": "2162",
      "page_count": 5,
      "order": 155,
      "p1": "754",
      "pn": "758",
      "abstract": [
        "This paper addresses the robust speech recognition problem as an adaptation\ntask. Specifically, we investigate the cumulative application of adaptation\nmethods. A bidirectional Long Short-Term Memory (BLSTM) based neural\nnetwork, capable of learning temporal relationships and translation\ninvariant representations, is used for robust acoustic modeling. Further,\ni-vectors were used as an input to the neural network to perform instantaneous\nspeaker and environment adaptation, providing 8% relative improvement\nin word error rate on the NIST Hub5 2000 evaluation testset. By enhancing\nthe first-pass i-vector based adaptation with a second-pass adaptation\nusing speaker and environment dependent transformations within the\nnetwork, a further relative improvement of 5% in word error rate was\nachieved. We have reevaluated the features used to estimate i-vectors\nand their normalization to achieve the best performance in a modern\nlarge scale automatic speech recognition system.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2162",
      "author_area_id": 8,
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "xie19b_interspeech": {
      "authors": [
        [
          "Xurong",
          "Xie"
        ],
        [
          "Xunying",
          "Liu"
        ],
        [
          "Tan",
          "Lee"
        ],
        [
          "Lan",
          "Wang"
        ]
      ],
      "title": "Fast DNN Acoustic Model Speaker Adaptation by Learning Hidden Unit Contribution Features",
      "original": "2050",
      "page_count": 5,
      "order": 156,
      "p1": "759",
      "pn": "763",
      "abstract": [
        "Speaker adaptation techniques play a key role in reducing the mismatch\nbetween automatic speech recognition (ASR) systems and target users.\nDeep neural network (DNN) acoustic model adaptation by learning speaker-dependent\nhidden unit contributions (LHUC) scaling vectors has been widely used.\nThe standard LHUC method not only requires multiple decoding passes\nin test time but also a substantial amount of adaptation data for robust\nparameter estimation. In order to address the issues, an efficient\nmethod of predicting and compressing the LHUC scaling vectors directly\nfrom acoustic features using a time-delay DNN (TDNN) and an online\naveraging layer is proposed in this paper. The resulting LHUC vectors\nare then used as auxiliary features to adapt DNN acoustic models. Experiments\nconducted on a 300-hour Switchboard corpus showed that the DNN and\nTDNN systems using the proposed predicted LHUC features consistently\noutperformed the corresponding baseline systems by up to about 9% relative\nreductions of word error rate. Being combined with i-Vector based adaptation,\nthe LHUC feature adapted TDNN systems demonstrated consistent improvement\nover comparable i-Vector adapted TDNN system.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2050",
      "author_area_id": 8,
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "tsunoo19_interspeech": {
      "authors": [
        [
          "Emiru",
          "Tsunoo"
        ],
        [
          "Yosuke",
          "Kashiwagi"
        ],
        [
          "Satoshi",
          "Asakawa"
        ],
        [
          "Toshiyuki",
          "Kumakura"
        ]
      ],
      "title": "End-to-End Adaptation with Backpropagation Through WFST for On-Device Speech Recognition System",
      "original": "1880",
      "page_count": 5,
      "order": 157,
      "p1": "764",
      "pn": "768",
      "abstract": [
        "An on-device DNN-HMM speech recognition system efficiently works with\na limited vocabulary in the presence of a variety of predictable noise.\nIn such a case, vocabulary and environment adaptation is highly effective.\nIn this paper, we propose a novel method of end-to-end (E2E) adaptation,\nwhich adjusts not only an acoustic model (AM) but also a weighted finite-state\ntransducer (WFST). We convert a pretrained WFST to a trainable neural\nnetwork and adapt the system to target environments/vocabulary by E2E\njoint training with an AM. We replicate Viterbi decoding with forward-backward\nneural network computation, which is similar to recurrent neural networks\n(RNNs). By pooling output score sequences, a vocabulary posterior for\neach utterance is obtained and used for discriminative loss computation.\nExperiments using 2&#8211;10 hours of English/Japanese adaptation datasets\nindicate that the fine-tuning of only WFSTs and that of only AMs are\nboth comparable to a state-of-the-art adaptation method, and E2E joint\ntraining of the two components achieves the best recognition performance.\nWe also adapt each language system to the other language using the\nadaptation data, and the results show that the proposed method also\nworks well for language adaptations.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1880",
      "author_area_id": 8,
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "sar19_interspeech": {
      "authors": [
        [
          "Leda",
          "Sar\u0131"
        ],
        [
          "Samuel",
          "Thomas"
        ],
        [
          "Mark A.",
          "Hasegawa-Johnson"
        ]
      ],
      "title": "Learning Speaker Aware Offsets for Speaker Adaptation of Neural Networks",
      "original": "1788",
      "page_count": 5,
      "order": 158,
      "p1": "769",
      "pn": "773",
      "abstract": [
        "In this work, we present an unsupervised long short-term memory (LSTM)\nlayer normalization technique that we call adaptation by speaker aware\noffsets (ASAO). These offsets are learned using an auxiliary network\nattached to the main senone classifier. The auxiliary network takes\nmain network LSTM activations as input and tries to reconstruct speaker,\n(speaker,phone) and (speaker,senone)-level averages of the activations\nby minimizing the mean-squared error. Once the auxiliary network is\njointly trained with the main network, during test time we do not need\nadditional information for the test data as the network will generate\nthe offset itself. Unlike many speaker adaptation studies which only\nadapt fully connected layers, our method is applicable to LSTM layers\nin addition to fully-connected layers. In our experiments, we investigate\nthe effect of ASAO of LSTM layers at different depths. We also show\nits performance when the inputs are already speaker adapted by feature\nspace maximum likelihood linear regression (fMLLR). In addition, we\ncompare ASAO with a speaker adversarial training framework. ASAO achieves\nhigher senone classification accuracy and lower word error rate (WER)\nthan both the unadapted models and the adversarial model on the HUB4\ndataset, with an absolute WER reduction of up to 2%.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1788",
      "author_area_id": 8,
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "sim19_interspeech": {
      "authors": [
        [
          "Khe Chai",
          "Sim"
        ],
        [
          "Petr",
          "Zadrazil"
        ],
        [
          "Fran\u00e7oise",
          "Beaufays"
        ]
      ],
      "title": "An Investigation into On-Device Personalization of End-to-End Automatic Speech Recognition Models",
      "original": "1752",
      "page_count": 5,
      "order": 159,
      "p1": "774",
      "pn": "778",
      "abstract": [
        "Speaker-independent speech recognition systems trained with data from\nmany users are generally robust against speaker variability and work\nwell for a large population of speakers. However, these systems do\nnot always generalize well for users with very different speech characteristics.\nThis issue can be addressed by building personalized systems that are\ndesigned to work well for each specific user. In this paper, we investigate\nthe idea of securely training personalized end-to-end speech recognition\nmodels on mobile devices so that user data and models never leave the\ndevice and are never stored on a server. We study how the mobile training\nenvironment impacts performance by simulating on-device data consumption.\nWe conduct experiments using data collected from speech impaired users\nfor personalization. Our results show that personalization achieved\n63.7% relative word error rate reduction when trained in a server environment\nand 58.1% in a mobile environment. Moving to on-device personalization\nresulted in 18.7% performance degradation, in exchange for improved\nscalability and data privacy. To train the model on device, we split\nthe gradient computation into two and achieved 45% memory reduction\nat the expense of 42% increase in training time.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1752",
      "author_area_id": 8,
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "jain19_interspeech": {
      "authors": [
        [
          "Abhinav",
          "Jain"
        ],
        [
          "Vishwanath P.",
          "Singh"
        ],
        [
          "Shakti P.",
          "Rath"
        ]
      ],
      "title": "A Multi-Accent Acoustic Model Using Mixture of Experts for Speech Recognition",
      "original": "1667",
      "page_count": 5,
      "order": 160,
      "p1": "779",
      "pn": "783",
      "abstract": [
        "A major challenge in Automatic Speech Recognition(ASR) systems is to\nhandle speech from a diverse set of accents. A model trained using\na single accent performs rather poorly when confronted with different\naccents. One of the solutions is a multi-condition model trained on\nall the accents. However the performance improvement in this approach\nmight be rather limited. Otherwise, accent-specific models might be\ntrained but they become impractical as number of accents increases.\nIn this paper, we propose a novel acoustic model architecture based\non Mixture of Experts (MoE) which works well on multiple accents without\nhaving the overhead of training separate models for separate accents.\nThe work is based on our earlier work, termed as MixNet, where we showed\nperformance improvement by separation of phonetic class distributions\nin the feature space. In this paper, we propose an architecture that\nhelps to compensate phonetic and accent variabilities which helps in\neven better discrimination among the classes. These variabilities are\nlearned in a joint frame-work, and produce consistent improvements\nover all the individual accents, amounting to an overall 18% relative\nimprovement in accuracy compared to baseline trained in multi-condition\nstyle.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1667",
      "author_area_id": 8,
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "shor19_interspeech": {
      "authors": [
        [
          "Joel",
          "Shor"
        ],
        [
          "Dotan",
          "Emanuel"
        ],
        [
          "Oran",
          "Lang"
        ],
        [
          "Omry",
          "Tuval"
        ],
        [
          "Michael",
          "Brenner"
        ],
        [
          "Julie",
          "Cattiau"
        ],
        [
          "Fernando",
          "Vieira"
        ],
        [
          "Maeve",
          "McNally"
        ],
        [
          "Taylor",
          "Charbonneau"
        ],
        [
          "Melissa",
          "Nollstadt"
        ],
        [
          "Avinatan",
          "Hassidim"
        ],
        [
          "Yossi",
          "Matias"
        ]
      ],
      "title": "Personalizing ASR for Dysarthric and Accented Speech with Limited Data",
      "original": "1427",
      "page_count": 5,
      "order": 161,
      "p1": "784",
      "pn": "788",
      "abstract": [
        "Automatic speech recognition (ASR) systems have dramatically improved\nover the last few years. ASR systems are most often trained from &#8216;typical&#8217;\nspeech, which means that underrepresented groups don&#8217;t experience\nthe same level of improvement. In this paper, we present and evaluate\nfinetuning techniques to improve ASR for users with non-standard speech.\nWe focus on two types of non-standard speech: speech from people with\namyotrophic lateral sclerosis (ALS) and accented speech. We train personalized\nmodels that achieve 62% and 35% relative WER improvement on these two\ngroups, bringing the absolute WER for ALS speakers, on a test set of\nmessage bank phrases, down to 10% for mild dysarthria and 20% for more\nserious dysarthria. We show that 71% of the improvement comes from\nonly 5 minutes of training data. Finetuning a particular subset of\nlayers (with many fewer parameters) often gives better results than\nfinetuning the entire model. This is the first step towards building\nstate of the art ASR models for dysarthric speech.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1427",
      "author_area_id": 8,
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "peskov19_interspeech": {
      "authors": [
        [
          "Denis",
          "Peskov"
        ],
        [
          "Joe",
          "Barrow"
        ],
        [
          "Pedro",
          "Rodriguez"
        ],
        [
          "Graham",
          "Neubig"
        ],
        [
          "Jordan",
          "Boyd-Graber"
        ]
      ],
      "title": "Mitigating Noisy Inputs for Question Answering",
      "original": "3154",
      "page_count": 5,
      "order": 162,
      "p1": "789",
      "pn": "793",
      "abstract": [
        "Natural language processing systems are often downstream of unreliable\ninputs: machine translation, optical character recognition, or speech\nrecognition. For instance, virtual assistants can only answer your\nquestions after understanding your speech. We investigate and mitigate\nthe effects of noise from Automatic Speech Recognition systems on two\nfactoid Question Answering ( qa) tasks. Integrating confidences into\nthe model and forced decoding of unknown words are empirically shown\nto improve the accuracy of downstream neural  qa systems. We create\nand train models on a synthetic corpus of over 500,000 noisy sentences\nand evaluate on two human corpora from Quizbowl and Jeopardy! competitions.\n"
      ],
      "doi": "10.21437/Interspeech.2019-3154",
      "author_area_id": 11,
      "author_area_label": "Spoken dialog systems and conversational analysis"
    },
    "gupta19_interspeech": {
      "authors": [
        [
          "Rahul",
          "Gupta"
        ],
        [
          "Aman",
          "Alok"
        ],
        [
          "Shankar",
          "Ananthakrishnan"
        ]
      ],
      "title": "One-vs-All Models for Asynchronous Training: An Empirical Analysis",
      "original": "2760",
      "page_count": 5,
      "order": 163,
      "p1": "794",
      "pn": "798",
      "abstract": [
        "Any given classification problem can be modeled using multiclass or\nOne-vs-All (OVA) architecture. An OVA system consists of as many OVA\nmodels as the number of classes, providing the advantage of asynchrony,\nwhere each OVA model can be re-trained independent of other models.\nThis is particularly advantageous in settings where scalable model\ntraining is a consideration (for instance in an industrial environment\nwhere multiple and frequent updates need to be made to the classification\nsystem). In this paper, we conduct empirical analysis on realizing\nindependent updates to OVA models and its impact on the accuracy of\nthe overall OVA system. Given that asynchronous updates lead to differences\nin training datasets for OVA models, we first define a metric to quantify\nthe differences in datasets. Thereafter, using Natural Language Understanding\nas a task of interest, we estimate the impact of three factors: (i)\nnumber of classes, (ii) number of data points and, (iii) divergences\nin training datasets across OVA models; on the OVA system accuracy.\nFinally, we observe the accuracy impact of increased asynchrony in\na Spoken Language Understanding system. We analyze the results and\nestablish that the proposed metric correlates strongly with the model\nperformances in both the experimental settings.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2760",
      "author_area_id": 11,
      "author_area_label": "Spoken dialog systems and conversational analysis"
    },
    "marzinotto19_interspeech": {
      "authors": [
        [
          "Gabriel",
          "Marzinotto"
        ],
        [
          "G\u00e9raldine",
          "Damnati"
        ],
        [
          "Fr\u00e9d\u00e9ric",
          "B\u00e9chet"
        ]
      ],
      "title": "Adapting a FrameNet Semantic Parser for Spoken Language Understanding Using Adversarial Learning",
      "original": "2732",
      "page_count": 5,
      "order": 164,
      "p1": "799",
      "pn": "803",
      "abstract": [
        "This paper presents a new semantic frame parsing model, based on Berkeley\nFrameNet, adapted to process spoken documents in order to perform information\nextraction from broadcast contents. Building upon previous work that\nhad shown the effectiveness of adversarial learning for domain generalization\nin the context of semantic parsing of encyclopedic written documents,\nwe propose to extend this approach to elocutionary style generalization.\nThe underlying question throughout this study is whether adversarial\nlearning can be used to combine data from different sources and train\nmodels on a higher level of abstraction in order to increase their\nrobustness to lexical and stylistic variations as well as automatic\nspeech recognition errors. The proposed strategy is evaluated on a\nFrench corpus of encyclopedic written documents and a smaller corpus\nof radio podcast transcriptions, both annotated with a FrameNet paradigm.\nWe show that adversarial learning increases all models generalization\ncapabilities both on manual and automatic speech transcription as well\nas on encyclopedic data.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2732",
      "author_area_id": 11,
      "author_area_label": "Spoken dialog systems and conversational analysis"
    },
    "parcollet19_interspeech": {
      "authors": [
        [
          "Titouan",
          "Parcollet"
        ],
        [
          "Mohamed",
          "Morchid"
        ],
        [
          "Xavier",
          "Bost"
        ],
        [
          "Georges",
          "Linar\u00e8s"
        ]
      ],
      "title": "M2H-GAN: A GAN-Based Mapping from Machine to Human Transcripts for Speech Understanding",
      "original": "2662",
      "page_count": 5,
      "order": 165,
      "p1": "804",
      "pn": "808",
      "abstract": [
        "Deep learning is at the core of recent spoken language understanding\n(SLU) related tasks. More precisely, deep neural networks (DNNs) drastically\nincreased the performances of SLU systems, and numerous architectures\nhave been proposed. In the real-life context of theme identification\nof telephone conversations, it is common to hold both a human, manual\n(TRS) and an automatically transcribed (ASR) versions of the conversations.\nNonetheless, and due to production constraints, only the ASR transcripts\nare considered to build automatic classifiers. TRS transcripts are\nonly used to measure the performances of ASR systems. Moreover, the\nrecent performances in term of classification accuracy, obtained by\nDNN related systems are close to the performances reached by humans,\nand it becomes difficult to further increase the performances by only\nconsidering the ASR transcripts. This paper proposes to distillates\nthe TRS knowledge available during the training phase within the ASR\nrepresentation, by using a new generative adversarial network called\nM2H-GAN to generate a TRS-like version of an ASR document, to improve\nthe theme identification performances.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2662",
      "author_area_id": 11,
      "author_area_label": "Spoken dialog systems and conversational analysis"
    },
    "georges19_interspeech": {
      "authors": [
        [
          "Munir",
          "Georges"
        ],
        [
          "Krzysztof",
          "Czarnowski"
        ],
        [
          "Tobias",
          "Bocklet"
        ]
      ],
      "title": "Ultra-Compact NLU: Neuronal Network Binarization as Regularization",
      "original": "2591",
      "page_count": 5,
      "order": 166,
      "p1": "809",
      "pn": "813",
      "abstract": [
        "This paper describes an approach for intent classification and tagging\non embedded devices, such as smart watches. We describe a technique\nto train neuronal networks where the final neuronal network weights\nare binary. This enables memory bandwidth optimized inference and efficient\ncomputation even on constrained/embedded platforms.<br style=\"mso-data-placement:same-cell;\"\n/><br style=\"mso-data-placement:same-cell;\" />\n  The flow of the approach\nis as follows: tf-idf word selection method reduces the number of overall\nweights. Bag-of-Words features are used with a feedforward and recurrent\nneuronal network for intent classification and tagging, respectively.\nA novel double Gaussian based regularization term is used to train\nthe network. Finally, the weights are almost clipped lossless to -1\nor 1 which results in a tiny binary neuronal network for intent classification\nand tagging.<br style=\"mso-data-placement:same-cell;\" /><br style=\"mso-data-placement:same-cell;\"\n/>\n  Our technique is evaluated using a text corpus of transcribed\nand annotated voice queries. The test domain is &#8220;lights control&#8221;.\nWe compare the intent and tagging accuracy of the ultra-compact binary\nneuronal network with our baseline system. The novel approach yields\ncomparable accuracy but reduces the model size by a factor of 16: from\n160kB to 10kB.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2591",
      "author_area_id": 11,
      "author_area_label": "Spoken dialog systems and conversational analysis"
    },
    "lugosch19_interspeech": {
      "authors": [
        [
          "Loren",
          "Lugosch"
        ],
        [
          "Mirco",
          "Ravanelli"
        ],
        [
          "Patrick",
          "Ignoto"
        ],
        [
          "Vikrant Singh",
          "Tomar"
        ],
        [
          "Yoshua",
          "Bengio"
        ]
      ],
      "title": "Speech Model Pre-Training for End-to-End Spoken Language Understanding",
      "original": "2396",
      "page_count": 5,
      "order": 167,
      "p1": "814",
      "pn": "818",
      "abstract": [
        "Whereas conventional spoken language understanding (SLU) systems map\nspeech to text, and then text to intent, end-to-end SLU systems map\nspeech directly to intent through a single trainable model. Achieving\nhigh accuracy with these end-to-end models without a large amount of\ntraining data is difficult. We propose a method to reduce the data\nrequirements of end-to-end SLU in which the model is first pre-trained\nto predict words and phonemes, thus learning good features for SLU.\nWe introduce a new SLU dataset, Fluent Speech Commands, and show that\nour method improves performance both when the full dataset is used\nfor training and when only a small subset is used. We also describe\npreliminary experiments to gauge the model&#8217;s ability to generalize\nto new phrases not heard during training.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2396",
      "author_area_id": 11,
      "author_area_label": "Spoken dialog systems and conversational analysis"
    },
    "shivakumar19_interspeech": {
      "authors": [
        [
          "Prashanth Gurunath",
          "Shivakumar"
        ],
        [
          "Mu",
          "Yang"
        ],
        [
          "Panayiotis",
          "Georgiou"
        ]
      ],
      "title": "Spoken Language Intent Detection Using Confusion2Vec",
      "original": "2226",
      "page_count": 5,
      "order": 168,
      "p1": "819",
      "pn": "823",
      "abstract": [
        "Decoding speaker&#8217;s intent is a crucial part of spoken language\nunderstanding (SLU). The presence of noise or errors in the text transcriptions,\nin real life scenarios make the task more challenging. In this paper,\nwe address the spoken language intent detection under noisy conditions\nimposed by automatic speech recognition (ASR) systems. We propose to\nemploy confusion2vec word feature representation to compensate for\nthe errors made by ASR and to increase the robustness of the SLU system.\nThe confusion2vec, motivated from human speech production and perception,\nmodels acoustic relationships between words in addition to the semantic\nand syntactic relations of words in human language. We hypothesize\nthat ASR often makes errors relating to acoustically similar words,\nand the confusion2vec with inherent model of acoustic relationships\nbetween words is able to compensate for the errors. We demonstrate\nthrough experiments on the ATIS benchmark dataset, the robustness of\nthe proposed model to achieve state-of-the-art results under noisy\nASR conditions. Our system reduces classification error rate (CER)\nby 20.84% and improves robustness by 37.48% (lower CER degradation)\nrelative to the previous state-of-the-art going from clean to noisy\ntranscripts. Improvements are also demonstrated when training the intent\ndetection models on noisy transcripts.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2226",
      "author_area_id": 11,
      "author_area_label": "Spoken dialog systems and conversational analysis"
    },
    "tomashenko19_interspeech": {
      "authors": [
        [
          "Natalia",
          "Tomashenko"
        ],
        [
          "Antoine",
          "Caubri\u00e8re"
        ],
        [
          "Yannick",
          "Est\u00e8ve"
        ]
      ],
      "title": "Investigating Adaptation and Transfer Learning for End-to-End Spoken Language Understanding from Speech",
      "original": "2158",
      "page_count": 5,
      "order": 169,
      "p1": "824",
      "pn": "828",
      "abstract": [
        "This work investigates speaker adaptation and transfer learning for\nspoken language understanding (SLU). We focus on the direct extraction\nof semantic tags from the audio signal using an end-to-end neural network\napproach. We demonstrate that the learning performance of the target\npredictive function for the semantic slot filling task can be substantially\nimproved by speaker adaptation and by various knowledge transfer approaches.\nFirst, we explore speaker adaptive training (SAT) for end-to-end SLU\nmodels and propose to use zero pseudo i-vectors for more efficient\nmodel initialization and pretraining in SAT. Second, in order to improve\nthe learning convergence for the target semantic slot filling (SF)\ntask, models trained for different tasks, such as automatic speech\nrecognition and named entity extraction are used to initialize neural\nend-to-end models trained for the target task. In addition, we explore\nthe impact of the knowledge transfer for SLU from a speech recognition\ntask trained in a different language. These approaches allow to develop\nend-to-end SLU systems in low-resource data scenarios when there is\nno enough in-domain semantically labeled data, but other resources,\nsuch as word transcriptions for the same or another language or named\nentity annotation, are available.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2158",
      "author_area_id": 11,
      "author_area_label": "Spoken dialog systems and conversational analysis"
    },
    "song19_interspeech": {
      "authors": [
        [
          "Yuanfeng",
          "Song"
        ],
        [
          "Di",
          "Jiang"
        ],
        [
          "Xueyang",
          "Wu"
        ],
        [
          "Qian",
          "Xu"
        ],
        [
          "Raymond Chi-Wing",
          "Wong"
        ],
        [
          "Qiang",
          "Yang"
        ]
      ],
      "title": "Topic-Aware Dialogue Speech Recognition with Transfer Learning",
      "original": "1694",
      "page_count": 5,
      "order": 170,
      "p1": "829",
      "pn": "833",
      "abstract": [
        "Dialogue speech widely exists in scenarios such as chitchat, meeting\nand customer service. General-purpose speech recognition systems usually\nneglect the topic information in the context of dialogue speech, which\nhas great potential for improving the performance of speech recognition.\nIn this paper, we propose a transfer learning mechanism to conduct\ntopic-aware recognition for dialogue speech. We first propose a new\nprobabilistic topic model named  Dialogue Speech Topic Model (DSTM)\nthat is specialized for modeling the context of dialogue speech. We\nfurther propose a novel transfer learning mechanism for DSTM to significantly\nreduce its training cost while preserving its effectiveness for accurate\ntopic inference. The experiment results demonstrate that proposed techniques\nin language model adaptation effectively improve the performance of\nthe state-of-the-art Automatic Speech Recognition (ASR) system.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1694",
      "author_area_id": 11,
      "author_area_label": "Spoken dialog systems and conversational analysis"
    },
    "masumura19_interspeech": {
      "authors": [
        [
          "Ryo",
          "Masumura"
        ],
        [
          "Tomohiro",
          "Tanaka"
        ],
        [
          "Atsushi",
          "Ando"
        ],
        [
          "Hosana",
          "Kamiyama"
        ],
        [
          "Takanobu",
          "Oba"
        ],
        [
          "Satoshi",
          "Kobashikawa"
        ],
        [
          "Yushi",
          "Aono"
        ]
      ],
      "title": "Improving Conversation-Context Language Models with Multiple Spoken Language Understanding Models",
      "original": "1534",
      "page_count": 5,
      "order": 171,
      "p1": "834",
      "pn": "838",
      "abstract": [
        "In this paper, we integrate fully neural network based conversation-context\nlanguage models (CCLMs) that are suitable for handling multi-turn conversational\nautomatic speech recognition (ASR) tasks, with multiple neural spoken\nlanguage understanding (SLU) models. A main strength of CCLMs is their\ncapacity to take long-range interactive contexts beyond utterance boundaries\ninto consideration. However, it is hard to optimize the CCLMs so as\nto fully exploit the long-range interactive contexts because conversation-level\ntraining datasets are often limited. In order to mitigate this problem,\nour key idea is to introduce various SLU models that are developed\nfor spoken dialogue systems into the CCLMs. In our proposed method\n(which we call &#8220;SLU-assisted CCLM&#8221;), hierarchical recurrent\nencoder-decoder based language modeling is extended so as to handle\nvarious utterance-level SLU results of preceding utterances in a continuous\nspace. We expect that the SLU models will help the CCLMs to properly\nunderstand semantic meanings of long-range interactive contexts and\nto fully leverage them for estimating a next utterance. Our experiments\non contact center dialogue ASR tasks demonstrate that SLU-assisted\nCCLMs combined with three types of SLU models can yield ASR performance\nimprovements.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1534",
      "author_area_id": 11,
      "author_area_label": "Spoken dialog systems and conversational analysis"
    },
    "chien19_interspeech": {
      "authors": [
        [
          "Jen-Tzung",
          "Chien"
        ],
        [
          "Wei Xiang",
          "Lieow"
        ]
      ],
      "title": "Meta Learning for Hyperparameter Optimization in Dialogue System",
      "original": "1383",
      "page_count": 5,
      "order": 172,
      "p1": "839",
      "pn": "843",
      "abstract": [
        "The performance of dialogue system based on deep reinforcement learning\n(DRL) highly depends on the selected hyperparameters in DRL algorithms.\nTraditionally, Gaussian process (GP) provides a probabilistic approach\nto Bayesian optimization for sequential search which is beneficial\nto select optimal hyperparameter. However, GP suffers from the expanding\ncomputation when the dimension of hyperparameters and the number of\nsearch points are increased. This paper presents a meta learning approach\nto carry out multifidelity Bayesian optimization where a two-level\nrecurrent neural network (RNN) is developed for sequential learning\nand optimization. The search space is explored via the first-level\nRNN with cheap and low fidelity over a global region of hyperparameters.\nThe optimization is then exploited and leveraged by the second-level\nRNN with a high fidelity on the successively small regions. The experiments\non the hyperparameter optimization for dialogue system based on the\ndeep Q network show the effectiveness and efficiency by using the proposed\nmultifidelity Bayesian optimization.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1383",
      "author_area_id": 11,
      "author_area_label": "Spoken dialog systems and conversational analysis"
    },
    "williams19_interspeech": {
      "authors": [
        [
          "Kyle",
          "Williams"
        ]
      ],
      "title": "Zero Shot Intent Classification Using Long-Short Term Memory Networks",
      "original": "1274",
      "page_count": 5,
      "order": 173,
      "p1": "844",
      "pn": "848",
      "abstract": [
        "We describe a zero shot approach to intent classification that allows\nfor the identification of intents that were not present during training.\nOur approach makes use of a Long-short Term Memory neural network to\nencode user queries and intents and uses these encodings to score previously\nunseen intents based on their semantic similarity to the queries. We\ntest our model on intent classification in a personal digital assistant\nand show an improvement of 15% over a strong baseline. We also investigate\nthe effect of adding a few training samples for the previously unseen\nintents in a few shot learning setting and show improvements of up\nto 16% over the baseline method.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1274",
      "author_area_id": 11,
      "author_area_label": "Spoken dialog systems and conversational analysis"
    },
    "korpusik19_interspeech": {
      "authors": [
        [
          "Mandy",
          "Korpusik"
        ],
        [
          "Zoe",
          "Liu"
        ],
        [
          "James",
          "Glass"
        ]
      ],
      "title": "A Comparison of Deep Learning Methods for Language Understanding",
      "original": "1262",
      "page_count": 5,
      "order": 174,
      "p1": "849",
      "pn": "853",
      "abstract": [
        "In this paper, we compare a suite of neural networks (recurrent, convolutional,\nand the recently proposed BERT model) to a CRF with hand-crafted features\non three semantic tagging corpora: the Air Travel Information System\n(ATIS) benchmark, restaurant queries, and written and spoken meal descriptions.\nOur motivation is to investigate pre-trained BERT&#8217;s transferability\nto the domains we are interested in. We demonstrate that neural networks\nwithout feature engineering outperform state-of-the-art statistical\nand deep learning approaches on all three tasks (except written meal\ndescriptions, where the CRF is slightly better) and that deep, attention-based\nBERT, in particular, surpasses state-of-the-art results on these tasks.\nError analysis shows the models are less confident when making errors,\nenabling the system to follow up with the user when uncertain.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1262",
      "author_area_id": 11,
      "author_area_label": "Spoken dialog systems and conversational analysis"
    },
    "kobayashi19_interspeech": {
      "authors": [
        [
          "Yuka",
          "Kobayashi"
        ],
        [
          "Takami",
          "Yoshida"
        ],
        [
          "Kenji",
          "Iwata"
        ],
        [
          "Hiroshi",
          "Fujimura"
        ]
      ],
      "title": "Slot Filling with Weighted Multi-Encoders for Out-of-Domain Values",
      "original": "1226",
      "page_count": 5,
      "order": 175,
      "p1": "854",
      "pn": "858",
      "abstract": [
        "This paper proposes a new method for slot filling of out-of-domain\n(OOD) slot values, which are not included in the training data, in\nspoken dialogue systems. Word embeddings have been proposed to estimate\nthe OOD slot values included in the word embedding model from keyword\ninformation. At the same time, context information is an important\nclue for estimation because the values in a given slot tend to appear\nin similar contexts. The proper use of either or both keyword and context\ninformation depends on the sentence. Conventional methods input a whole\nsentence into an encoder and extract important clues by the attention\nmechanism. However, it is difficult to properly distinguish context\nand keyword information from the encoder outputs because these two\nfeatures are already mixed. Our proposed method uses two encoders,\nwhich distinctly encode contexts and keywords, respectively. The model\ncalculates weights for the two encoders based on a user utterance and\nestimates a slot with weighted outputs from the two encoders. Experimental\nresults show that the proposed method achieves a 50% relative improvement\nin F1 score compared with a baseline model, which detects slot values\nfrom user utterances and estimates slots at once with a single encoder.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1226",
      "author_area_id": 11,
      "author_area_label": "Spoken dialog systems and conversational analysis"
    },
    "seneviratne19_interspeech": {
      "authors": [
        [
          "Nadee",
          "Seneviratne"
        ],
        [
          "Ganesh",
          "Sivaraman"
        ],
        [
          "Carol",
          "Espy-Wilson"
        ]
      ],
      "title": "Multi-Corpus Acoustic-to-Articulatory Speech Inversion",
      "original": "3168",
      "page_count": 5,
      "order": 176,
      "p1": "859",
      "pn": "863",
      "abstract": [
        "There are several technologies like Electromagnetic articulometry (EMA),\nultrasound, real-time Magnetic Resonance Imaging (MRI), and X-ray microbeam\nthat are used to measure speech articulatory movements. Each of these\ntechniques provides a different view of the vocal tract. The measurements\nperformed using the similar techniques also differ greatly due to differences\nin the placement of sensors, and the anatomy of speakers. This limits\nmost articulatory studies to single datasets. However to yield better\nresults in its applications, the speech inversion systems should be\nmore generalized, which requires the combination of data from multiple\nsources. This paper proposes a multi-task learning based deep neural\nnetwork architecture for acoustic-to-articulatory speech inversion\ntrained using three different articulatory datasets &#8212; two of\nthem were measured using EMA, and one using X-ray microbeam. Experiments\nshow improved accuracy of the proposed acoustic-to-articulatory mapping\ncompared to the systems trained using single datasets.\n"
      ],
      "doi": "10.21437/Interspeech.2019-3168",
      "author_area_id": 1,
      "author_area_label": "Speech Perception, Production and Acquisition"
    },
    "dash19_interspeech": {
      "authors": [
        [
          "Debadatta",
          "Dash"
        ],
        [
          "Alan",
          "Wisler"
        ],
        [
          "Paul",
          "Ferrari"
        ],
        [
          "Jun",
          "Wang"
        ]
      ],
      "title": "Towards a Speaker Independent Speech-BCI Using Speaker Adaptation",
      "original": "3109",
      "page_count": 5,
      "order": 177,
      "p1": "864",
      "pn": "868",
      "abstract": [
        "Neurodegenerative diseases such as amyotrophic lateral sclerosis (ALS)\ncan cause locked-in-syndrome (fully paralyzed but aware). Brain-computer\ninterface (BCI) may be the only option to restore their communication.\nCurrent BCIs typically use visual or attention correlates in neural\nactivities to select letters randomly displayed on a screen, which\nare extremely slow (a few words per minute). Speech-BCIs, which aim\nto convert the brain activity patterns to speech (neural speech decoding),\nhold the potential to enable faster communication. Although a few recent\nstudies have shown the potential of neural speech decoding, those are\nfocused on speaker-dependent models. In this study, we investigated\nspeaker-independent neural speech decoding of five continuous phrases\nfrom Magnetoencephalography (MEG) signals while 8 subjects produced\nspeech covertly (imagination) or overtly (articulation). We have used\nboth supervised and unsupervised speaker adaptation strategies for\nimplementing a speaker independent model. Experimental results demonstrated\nthat the proposed adaptation-based speaker-independent model has significantly\nimproved decoding performance. To our knowledge, this is the first\ndemonstration of the possibility of speaker-independent neural speech\ndecoding.\n"
      ],
      "doi": "10.21437/Interspeech.2019-3109",
      "author_area_id": 1,
      "author_area_label": "Speech Perception, Production and Acquisition"
    },
    "sheth19_interspeech": {
      "authors": [
        [
          "Janaki",
          "Sheth"
        ],
        [
          "Ariel",
          "Tankus"
        ],
        [
          "Michelle",
          "Tran"
        ],
        [
          "Lindy",
          "Comstock"
        ],
        [
          "Itzhak",
          "Fried"
        ],
        [
          "William",
          "Speier"
        ]
      ],
      "title": "Identifying Input Features for Development of Real-Time Translation of Neural Signals to Text",
      "original": "3092",
      "page_count": 5,
      "order": 178,
      "p1": "869",
      "pn": "873",
      "abstract": [
        "One of the main goals in Brain-Computer Interface (BCI) research is\nto help patients with faltering communication abilities due to neurodegenerative\ndiseases produce text or speech output using their neural recordings.\nHowever, practical implementation of such a system has proven difficult\ndue to limitations in the speed, accuracy, and training time of existing\ninterfaces. In this paper, we contribute to this endeavour by isolating\nappropriate input features from speech-producing neural signals that\nwill feed into a machine learning classifier to identify target phonemes.\nAnalysing data from six subjects, we discern frequency bands that encapsulate\ndifferential information regarding production of vowels and consonants\nbroadly, and more specifically nasals and semivowels. Subsequent spatial\nlocalization analysis reveals the underlying cortical regions responsible\nfor different phoneme categories. Anatomical locations along with their\nrespective frequency bands act as prospective feature sets for machine\nlearning classifiers. We demonstrate this classification ability in\na preliminary language reconstruction task and show an average word\nclassification accuracy of 30.6% (p&#60;0.001).\n"
      ],
      "doi": "10.21437/Interspeech.2019-3092",
      "author_area_id": 1,
      "author_area_label": "Speech Perception, Production and Acquisition"
    },
    "silva19_interspeech": {
      "authors": [
        [
          "Samuel",
          "Silva"
        ],
        [
          "Ant\u00f3nio",
          "Teixeira"
        ],
        [
          "Concei\u00e7\u00e3o",
          "Cunha"
        ],
        [
          "Nuno",
          "Almeida"
        ],
        [
          "Arun A.",
          "Joseph"
        ],
        [
          "Jens",
          "Frahm"
        ]
      ],
      "title": "Exploring Critical Articulator Identification from 50Hz RT-MRI Data of the Vocal Tract",
      "original": "2897",
      "page_count": 5,
      "order": 179,
      "p1": "874",
      "pn": "878",
      "abstract": [
        "The study of the static and dynamic aspects of speech production can\nprofit from technologies such as electromagnetic midsagittal articulography\n(EMA) and real-time magnetic resonance (RTMRI). These can improve our\nknowledge on which articulators and gestures are involved in producing\nspecific sounds and foster improved speech production models, paramount\nto advance, e.g., articulatory speech synthesis. Previous work, by\nthe authors, has shown that critical articulator identification could\nbe performed from RTMRI data of the vocal tract, with encouraging results,\nby extending the applicability of an unsupervised statistical identification\nmethod previously proposed for EMA data. Nevertheless, the slower time\nresolution of the considered RT-MRI corpus (14 Hz), when compared to\nEMA, potentially influencing the ability to select the most suitable\nrepresentative configuration for each phone &#8212; paramount for strongly\ndynamic phones, e.g., nasal vowels  &#8212;, and the lack of a richer\nset of contexts &#8212; relevant for observing coarticulation effects\n&#8212;, were identified as limitations. This article addresses these\nlimitations by exploring critical articulator identification from a\nfaster RTMRI corpus (50 Hz), for European Portuguese, providing a richer\nset of contexts, and testing how fusing the articulatory data of two\nspeakers might influence critical articulator determination.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2897",
      "author_area_id": 1,
      "author_area_label": "Speech Perception, Production and Acquisition"
    },
    "douros19_interspeech": {
      "authors": [
        [
          "Ioannis K.",
          "Douros"
        ],
        [
          "Anastasiia",
          "Tsukanova"
        ],
        [
          "Karyna",
          "Isaieva"
        ],
        [
          "Pierre-Andr\u00e9",
          "Vuissoz"
        ],
        [
          "Yves",
          "Laprie"
        ]
      ],
      "title": "Towards a Method of Dynamic Vocal Tract Shapes Generation by Combining Static 3D and Dynamic 2D MRI Speech Data",
      "original": "2880",
      "page_count": 5,
      "order": 180,
      "p1": "879",
      "pn": "883",
      "abstract": [
        "We present an algorithm for augmenting the shape of the vocal tract\nusing 3D static and 2D dynamic speech MRI data. While static 3D images\nhave better resolution and provide spatial information, 2D dynamic\nimages capture the transitions. The aim of this work is to combine\nstrong points of these two types of data to obtain better image quality\nof 2D dynamic images and extend the 2D dynamic images to the 3D domain.<br\nstyle=\"mso-data-placement:same-cell;\" /><br style=\"mso-data-placement:same-cell;\"\n/>\n  To produce a 3D dynamic consonant-vowel (CV) sequence, our algorithm\ntakes as input the 2D CV transition and the static 3D targets for C\nand V. To obtain the enhanced sequence of images, the first step is\nto find a transformation between the 2D images and the mid-sagittal\nslice of the acoustically corresponding 3D image stack, and then find\na transformation between neighbouring sagittal slices in the 3D static\nimage stack. Combination of these transformations allows producing\nthe final set of images. In the present study we first examined the\ntransformation from the 3D mid-sagittal frame to the 2D video in order\nto improve image quality and then we examined the extension of the\n2D video to the 3rd dimension with the aim to enrich spatial information.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2880",
      "author_area_id": 1,
      "author_area_label": "Speech Perception, Production and Acquisition"
    },
    "rasskazova19_interspeech": {
      "authors": [
        [
          "Oksana",
          "Rasskazova"
        ],
        [
          "Christine",
          "Mooshammer"
        ],
        [
          "Susanne",
          "Fuchs"
        ]
      ],
      "title": "Temporal Coordination of Articulatory and Respiratory Events Prior to Speech Initiation",
      "original": "2876",
      "page_count": 5,
      "order": 181,
      "p1": "884",
      "pn": "888",
      "abstract": [
        "The investigation of the speech planning processes, in particular the\ntiming between acoustic and articulatory onset, has recently received\na lot of attention. Respiration has not been considered in this process\nso far, although it is involved and may be well coordinated with the\noral articulators prior and at the onset of the utterance. In light\nof these considerations, we investigated the temporal coordination\nbetween acoustic, respiratory and articulatory events prior to utterance\nonset. For this purpose 12 native speakers of German have been recorded\nwith Electromagnetic Articulography and Inductance Plethysmography\nreading sentences that were controlled for length and stress of the\nfirst word. The initial segment of the utterance was either /t/ or\n/n/. The results for six speakers so far indicate that early speech\npreparation consists of mouth opening during the inhalation phase.\nThe onset of expiration seems to be tightly coupled with the acoustic\nand the articulatory onset, particularly with the constriction interval\nof the tongue tip gesture in the first segment. Manner of articulation\nof the initial segment seems to affect the temporal fine-tuning of\npreparatory events.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2876",
      "author_area_id": 1,
      "author_area_label": "Speech Perception, Production and Acquisition"
    },
    "gubian19b_interspeech": {
      "authors": [
        [
          "Michele",
          "Gubian"
        ],
        [
          "Manfred",
          "Past\u00e4tter"
        ],
        [
          "Marianne",
          "Pouplier"
        ]
      ],
      "title": "Zooming in on Spatiotemporal V-to-C Coarticulation with Functional PCA",
      "original": "2143",
      "page_count": 5,
      "order": 182,
      "p1": "889",
      "pn": "893",
      "abstract": [
        "It has long been proposed in speech production research that in CV\nsequences, the movement for consonant and vowel are initiated synchronously.\nHowever, mostly due to limitations on the statistical analysis of articulator\nmotion over time, this could only be shown in a limited fashion, based\non positional differences at a single time point during consonantal\nconstriction formation. It is unknown to which extent this observation\ngeneralizes to earlier timepoints. In this paper, we illustrate the\nuse of functional principal component analysis (FPCA) for the statistical\nanalysis of articulator motion over time. Using articulography data,\nwe quantify CV coarticulation during constriction formation of [k]\nin two vowel contexts. We show how FPCA enables us to analyse both\nhorizontal and vertical movement components over time in a single model\nwhile preserving information on temporal variability. We combine FPCA\nwith linear mixed modelling to obtain estimated mean trajectories and\nconfidence bands for [k] in the two vowel contexts. Results show that\nwell before the timepoint of peak velocity the vowel causes a substantial\nspatial separation of the consonantal trajectories, estimated to be\nat least 3 mm at peak velocity. This lends support to the hypothesis\nthat vowel and consonant are initiated synchronously.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2143",
      "author_area_id": 1,
      "author_area_label": "Speech Perception, Production and Acquisition"
    },
    "csapo19_interspeech": {
      "authors": [
        [
          "Tam\u00e1s G\u00e1bor",
          "Csap\u00f3"
        ],
        [
          "Mohammed Salah",
          "Al-Radhi"
        ],
        [
          "G\u00e9za",
          "N\u00e9meth"
        ],
        [
          "G\u00e1bor",
          "Gosztolya"
        ],
        [
          "Tam\u00e1s",
          "Gr\u00f3sz"
        ],
        [
          "L\u00e1szl\u00f3",
          "T\u00f3th"
        ],
        [
          "Alexandra",
          "Mark\u00f3"
        ]
      ],
      "title": "Ultrasound-Based Silent Speech Interface Built on a Continuous Vocoder",
      "original": "2046",
      "page_count": 5,
      "order": 183,
      "p1": "894",
      "pn": "898",
      "abstract": [
        "Recently it was shown that within the Silent Speech Interface (SSI)\nfield, the prediction of F0 is possible from Ultrasound Tongue Images\n(UTI) as the articulatory input, using Deep Neural Networks for articulatory-to-acoustic\nmapping. Moreover, text-to-speech synthesizers were shown to produce\nhigher quality speech when using a continuous pitch estimate, which\ntakes non-zero pitch values even when voicing is not present. Therefore,\nin this paper on UTI-based SSI, we use a simple continuous F0 tracker\nwhich does not apply a strict voiced /unvoiced decision. Continuous\nvocoder parameters (ContF0, Maximum Voiced Frequency and Mel-Generalized\nCepstrum) are predicted using a convolutional neural network, with\nUTI as input. The results demonstrate that during the articulatory-to-acoustic\nmapping experiments, the continuous F0 is predicted with lower error,\nand the continuous vocoder produces slightly more natural synthesized\nspeech than the baseline vocoder using standard discontinuous F0.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2046",
      "author_area_id": 1,
      "author_area_label": "Speech Perception, Production and Acquisition"
    },
    "klein19_interspeech": {
      "authors": [
        [
          "Eugen",
          "Klein"
        ],
        [
          "Jana",
          "Brunner"
        ],
        [
          "Phil",
          "Hoole"
        ]
      ],
      "title": "Assessing Acoustic and Articulatory Dimensions of Speech Motor Adaptation with Random Forests",
      "original": "1812",
      "page_count": 5,
      "order": 184,
      "p1": "899",
      "pn": "903",
      "abstract": [
        "Although most modern theories of speech production assume that representations\nof speech sounds are multidimensional encompassing acoustic and articulatory\ninformation, speech motor learning studies which assess the degree\nof adaptation in both dimensions are few and far between. In the current\npaper, we present an auditory perturbation study of German sibilant\n[s] in which speakers&#8217; audio and articulatory movements were\nrecorded by means of electromagnetic articulography. Random Forest,\na supervised learning algorithm, was employed to classify speakers&#8217;\nresponses produced under unaltered or perturbed feedback based either\non acoustic or articulatory parameters. Preliminary results demonstrate\nthat while classification accuracy increases in the acoustic dimension\nas the perturbation session goes on, the classification accuracy in\nthe articulatory dimension, although overall higher, remains approximately\nat the same level. This suggests that the adaptation process is characterized\nby active exploration of the articulatory space which is guided by\nspeakers&#8217; auditory feedback.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1812",
      "author_area_id": 1,
      "author_area_label": "Speech Perception, Production and Acquisition"
    },
    "takemoto19_interspeech": {
      "authors": [
        [
          "Hironori",
          "Takemoto"
        ],
        [
          "Tsubasa",
          "Goto"
        ],
        [
          "Yuya",
          "Hagihara"
        ],
        [
          "Sayaka",
          "Hamanaka"
        ],
        [
          "Tatsuya",
          "Kitamura"
        ],
        [
          "Yukiko",
          "Nota"
        ],
        [
          "Kikuo",
          "Maekawa"
        ]
      ],
      "title": "Speech Organ Contour Extraction Using Real-Time MRI and Machine Learning Method",
      "original": "1593",
      "page_count": 5,
      "order": 185,
      "p1": "904",
      "pn": "908",
      "abstract": [
        "Real-time MRI can be used to obtain videos that describe articulatory\nmovements during running speech. For detailed analysis based on a large\nnumber of video frames, it is necessary to extract the contours of\nspeech organs, such as the tongue, semi-automatically. The present\nstudy attempted to extract the contours of speech organs from videos\nusing a machine learning method. First, an expert operator manually\nextracted the contours from the frames of a video to build training\ndata sets. The learning operators, or learners, then extracted the\ncontours from each frame of the video. Finally, the errors representing\nthe geometrical distance between the extracted contours and the ground\ntruth, which were the contours excluded from the training data sets,\nwere examined. The results showed that the contours extracted using\nmachine learning were closer to the ground truth than the contours\ntraced by other expert and non-expert operators. In addition, using\nthe same learners, the contours were extracted from other naive videos\nobtained during different speech tasks of the same subject. As a result,\nthe errors in those videos were similar to those in the video in which\nthe learners were trained.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1593",
      "author_area_id": 1,
      "author_area_label": "Speech Perception, Production and Acquisition"
    },
    "leeuwen19_interspeech": {
      "authors": [
        [
          "K.G. van",
          "Leeuwen"
        ],
        [
          "P.",
          "Bos"
        ],
        [
          "S.",
          "Trebeschi"
        ],
        [
          "M.J.A. van",
          "Alphen"
        ],
        [
          "L.",
          "Voskuilen"
        ],
        [
          "L.E.",
          "Smeele"
        ],
        [
          "F. van der",
          "Heijden"
        ],
        [
          "R.J.J.H. van",
          "Son"
        ]
      ],
      "title": "CNN-Based Phoneme Classifier from Vocal Tract MRI Learns Embedding Consistent with Articulatory Topology",
      "original": "1173",
      "page_count": 5,
      "order": 186,
      "p1": "909",
      "pn": "913",
      "abstract": [
        "Recent advances in real-time magnetic resonance imaging (rtMRI) of\nthe vocal tract provides opportunities for studying human speech. This\nmodality together with acquired speech may enable the mapping of articulatory\nconfigurations to acoustic features. In this study, we take the first\nstep by training a deep learning model to classify 27 different phonemes\nfrom midsagittal MR images of the vocal tract.<br style=\"mso-data-placement:same-cell;\"\n/><br style=\"mso-data-placement:same-cell;\" />\n  An American English\ndatabase was used to train a convolutional neural network for classifying\nvowels (13 classes), consonants (14 classes) and all phonemes (27 classes)\nof 17 subjects. Classification top-1 accuracy of the test set for all\nphonemes was 57%. Error analysis showed voiced and unvoiced sounds\noften being confused. Moreover, we performed principal component analysis\non the network&#8217;s embedding and observed topological similarities\nbetween the network learned representation and the vowel diagram. Saliency\nmaps gave insight into the anatomical regions most important for classification\nand show congruence with known regions of articulatory importance.<br\nstyle=\"mso-data-placement:same-cell;\" /><br style=\"mso-data-placement:same-cell;\"\n/>\n  We demonstrate the feasibility for deep learning to distinguish\nbetween phonemes from MRI. Network analysis can be used to improve\nunderstanding of normal articulation and speech and, in the future,\nimpaired speech. This study brings us a step closer to the articulatory-to-acoustic\nmapping from rtMRI.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1173",
      "author_area_id": 1,
      "author_area_label": "Speech Perception, Production and Acquisition"
    },
    "mucke19_interspeech": {
      "authors": [
        [
          "Doris",
          "M\u00fccke"
        ],
        [
          "Anne",
          "Hermes"
        ],
        [
          "Sam",
          "Tilsen"
        ]
      ],
      "title": "Strength and Structure: Coupling Tones with Oral Constriction Gestures",
      "original": "2650",
      "page_count": 5,
      "order": 187,
      "p1": "914",
      "pn": "918",
      "abstract": [
        "According to the segmental anchor hypothesis within the Autosegmental-Metrical\napproach, tones are aligned with segmental boundaries of consonant\nand vowels in the acoustic domain. In prenuclear rising pitch accents\n(LH*), the rise is assumed to occur in the vicinity of the accented\nsyllable it is phonologically associated with. However, there are differences\nin the alignment patterns within and across languages that cannot be\ncaptured within the AM approach. In the present study, we investigate\nthe coordination of tonal and oral constriction gestures within Articulatory\nPhonology. Therefore, we model the coordination of prenuclear LH* pitch\naccents in Catalan, Northern and Southern German with respect to syllable\nproduction on the basis of recordings with a 2D electromagnetic articulography.\nWe provide an extended coupled oscillators model that allows for balanced\nand imbalanced coupling strengths. Based on examples, we show that\nthe observed differences in alignment patterns for prenuclear rising\npitch accents can be modelled with the same underlying coordinative\nstructures/coupling modes for vocalic and tonal gestures and that surface\ndifferences arise from gradient variation in coupling strengths.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2650",
      "author_area_id": 1,
      "author_area_label": "Speech Perception, Production and Acquisition"
    },
    "kleijn19_interspeech": {
      "authors": [
        [
          "W. Bastiaan",
          "Kleijn"
        ],
        [
          "Felicia S.C.",
          "Lim"
        ],
        [
          "Michael",
          "Chinen"
        ],
        [
          "Jan",
          "Skoglund"
        ]
      ],
      "title": "Salient Speech Representations Based on Cloned Networks",
      "original": "1861",
      "page_count": 5,
      "order": 188,
      "p1": "919",
      "pn": "923",
      "abstract": [
        "We define  salient features as features that are shared by signals\nthat are defined as being  equivalent by a system designer. The definition\nallows the designer to contribute qualitative information. We aim to\nfind salient features that are useful as conditioning for generative\nnetworks. We extract salient features by jointly training a set of\nclones of an encoder network. Each network clone receives as input\na different signal from a set of equivalent signals. The objective\nfunction encourages the network clones to map their input into a set\nof features that is identical across the clones. It additionally encourages\nfeature independence and, optionally, reconstruction of a desired target\nsignal by a decoder. As an application, we train a system that extracts\na time-sequence of feature vectors of speech and uses it as a conditioning\nof a WaveNet generative system, facilitating both coding and enhancement.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1861",
      "author_area_id": 5,
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "ramanathi19_interspeech": {
      "authors": [
        [
          "Manoj Kumar",
          "Ramanathi"
        ],
        [
          "Chiranjeevi",
          "Yarra"
        ],
        [
          "Prasanta Kumar",
          "Ghosh"
        ]
      ],
      "title": "ASR Inspired Syllable Stress Detection for Pronunciation Evaluation Without Using a Supervised Classifier and Syllable Level Features",
      "original": "2091",
      "page_count": 5,
      "order": 189,
      "p1": "924",
      "pn": "928",
      "abstract": [
        "Automatic syllable stress detection is typically performed with a supervised\nclassifier considering manually annotated stress markings and features\ncomputed within the syllable segments derived from phoneme transcriptions\nand their time-aligned boundaries. However, the manual annotation is\ntedious and the errors in estimating segmental information could degrade\nstress detection accuracy. In order to circumvent these, we propose\nto estimate stress markings in automatic speech recognition (ASR) framework\ninvolving finite-state-transducer (FST) without using annotated stress\nmarkings and segmental information. For this, we train the ASR system\nwith native English data along with pronunciation lexicon containing\ncanonical stress markings and decode non-native utterances as pronunciations\nembedded with stress markings. In the decoding, we use an FST encoded\nwith the pronunciations derived using phoneme transcriptions and the\ninstructions involved in a typical manual annotation. Experiments are\nconducted on polysyllabic words taken from ISLE corpus containing utterances\nspoken by Italian and German speakers and using the ASR models trained\nwith three corpora. Among all the three models, the highest stress\ndetection accuracies with the proposed approach respectively on Italian\n&amp; German speakers are found to be 2.07% &amp; 1.19% higher than\nand comparable to those with the two supervised classification approaches\nused as baselines.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2091",
      "author_area_id": 5,
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "mannem19_interspeech": {
      "authors": [
        [
          "Renuka",
          "Mannem"
        ],
        [
          "Jhansi",
          "Mallela"
        ],
        [
          "Aravind",
          "Illa"
        ],
        [
          "Prasanta Kumar",
          "Ghosh"
        ]
      ],
      "title": "Acoustic and Articulatory Feature Based Speech Rate Estimation Using a Convolutional Dense Neural Network",
      "original": "2295",
      "page_count": 5,
      "order": 190,
      "p1": "929",
      "pn": "933",
      "abstract": [
        "In this paper, we propose a speech rate estimation approach using a\nconvolutional dense neural network (CDNN). The CDNN based approach\nuses the acoustic and articulatory features for speech rate estimation.\nThe Mel Frequency Cepstral Coefficients (MFCCs) are used as acoustic\nfeatures and the articulograms representing time-varying vocal tract\nprofile are used as articulatory features. The articulogram is computed\nfrom a real-time magnetic resonance imaging (rtMRI) video in the midsagittal\nplane of a subject while speaking. However, in practice, the articulogram\nfeatures are not directly available, unlike acoustic features from\nspeech recording. Thus, we use an Acoustic-to-Articulatory Inversion\nmethod using a bidirectional long-short-term memory network which estimates\nthe articulogram features from the acoustics. The proposed CDNN based\napproach using estimated articulatory features requires both acoustic\nand articulatory features during training but it requires only acoustic\ndata during testing. Experiments are conducted using rtMRI videos from\nfour subjects each speaking 460 sentences. The Pearson correlation\ncoefficient is used to evaluate the speech rate estimation. It is found\nthat the CDNN based approach gives a better correlation coefficient\nthan the temporal and selected sub-band correlation (TCSSBC) based\nbaseline scheme by 81.58% and 73.68% (relative) in seen and unseen\nsubject conditions respectively.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2295",
      "author_area_id": 5,
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "springenberg19_interspeech": {
      "authors": [
        [
          "Sebastian",
          "Springenberg"
        ],
        [
          "Egor",
          "Lakomkin"
        ],
        [
          "Cornelius",
          "Weber"
        ],
        [
          "Stefan",
          "Wermter"
        ]
      ],
      "title": "Predictive Auxiliary Variational Autoencoder for Representation Learning of Global Speech Characteristics",
      "original": "2845",
      "page_count": 5,
      "order": 191,
      "p1": "934",
      "pn": "938",
      "abstract": [
        "Unsupervised learning represents an important opportunity for obtaining\nuseful speech representations. Recently, variational autoencoders (VAEs)\nhave been shown to extract useful representations in an unsupervised\nmanner. These models are usually not designed to explicitly disentangle\nspecific sources of information. When processing data of sequential\nnature which involves multi-timescale information, disentanglement\ncan however be beneficial. In this paper we address this issue by developing\na predictive auxiliary variational autoencoder to obtain speech representations\nat different timescales. We will present an auxiliary lower bound which\nis used to develop a model that we call the Predictive Aux-VAE. The\nmodel is designed to disentangle global from local information into\na dedicated auxiliary variable. Learned representations are analysed\nwith respect to their ability to capture global speech characteristics.\nWe observe that representations of individual speakers are separated\nwell in the latent space and can successfully be used in a subsequent\nspeaker identification task where they achieve high classification\naccuracy, comparable to a fully supervised model. Moreover, manipulating\nthe global variable allows to change global characteristics while retaining\nthe local content during generation which demonstrates the success\nof our model to disentangle global from local information.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2845",
      "author_area_id": 5,
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "paraskevopoulos19_interspeech": {
      "authors": [
        [
          "Georgios",
          "Paraskevopoulos"
        ],
        [
          "Efthymios",
          "Tzinis"
        ],
        [
          "Nikolaos",
          "Ellinas"
        ],
        [
          "Theodoros",
          "Giannakopoulos"
        ],
        [
          "Alexandros",
          "Potamianos"
        ]
      ],
      "title": "Unsupervised Low-Rank Representations for Speech Emotion Recognition",
      "original": "2769",
      "page_count": 5,
      "order": 192,
      "p1": "939",
      "pn": "943",
      "abstract": [
        "We examine the use of linear and non-linear dimensionality reduction\nalgorithms for extracting low-rank feature representations for speech\nemotion recognition. Two feature sets are used, one based on low-level\ndescriptors and their aggregations (IS10) and one modeling recurrence\ndynamics of speech (RQA), as well as their fusion. We report speech\nemotion recognition (SER) results for learned representations on two\ndatabases using different classification methods. Classification with\nlow-dimensional representations yields performance improvement in a\nvariety of settings. This indicates that dimensionality reduction is\nan effective way to combat the curse of dimensionality for SER. Visualization\nof features in two dimensions provides insight into discriminatory\nabilities of reduced feature sets.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2769",
      "author_area_id": 5,
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "dhiman19_interspeech": {
      "authors": [
        [
          "Jitendra Kumar",
          "Dhiman"
        ],
        [
          "Nagaraj",
          "Adiga"
        ],
        [
          "Chandra Sekhar",
          "Seelamantula"
        ]
      ],
      "title": "On the Suitability of the Riesz Spectro-Temporal Envelope for WaveNet Based Speech Synthesis",
      "original": "2626",
      "page_count": 5,
      "order": 193,
      "p1": "944",
      "pn": "948",
      "abstract": [
        "We address the problem of estimating the time-varying spectral envelope\nof a speech signal using a spectro-temporal demodulation technique.\nUnlike the conventional spectrogram, we consider a pitch-adaptive spectrogram\nand model a spectro-temporal patch using an amplitude- and frequency-modulated\ntwo-dimensional (2-D) cosine signal. We employ a demodulation technique\nbased on the Riesz transform that we proposed recently to estimate\nthe amplitude and frequency modulations. The amplitude modulation (AM)\ncorresponds to the vocal-tract filter magnitude response (or envelope)\nand the frequency modulation (FM) corresponds to the excitation. We\nconsider the AM and demonstrate its effectiveness by incorporating\nit as an acoustic feature for local conditioning in the statistical\nWaveNet vocoder for the task of speech synthesis. The quality of the\nsynthesized speech obtained with the Riesz envelope is compared with\nthat obtained using the envelope estimated by the WORLD vocoder. Objective\nmeasures and subjective listening tests on the CMU-Arctic database\nshow that the quality of synthesis is superior to that obtained using\nthe WORLD envelope. This study thus establishes the Riesz envelope\nas an efficient alternative to the WORLD envelope.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2626",
      "author_area_id": 5,
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "xu19_interspeech": {
      "authors": [
        [
          "Xinzhou",
          "Xu"
        ],
        [
          "Jun",
          "Deng"
        ],
        [
          "Nicholas",
          "Cummins"
        ],
        [
          "Zixing",
          "Zhang"
        ],
        [
          "Li",
          "Zhao"
        ],
        [
          "Bj\u00f6rn W.",
          "Schuller"
        ]
      ],
      "title": "Autonomous Emotion Learning in Speech: A View of Zero-Shot Speech Emotion Recognition",
      "original": "2406",
      "page_count": 5,
      "order": 194,
      "p1": "949",
      "pn": "953",
      "abstract": [
        "Conventionally, speech emotion recognition is achieved using passive\nlearning approaches. Differing from such approaches, we herein propose\nand develop a dynamic method of autonomous emotion learning based on\nzero-shot learning. The proposed methodology employs emotional dimensions\nas the attributes in the zero-shot learning paradigm, resulting in\ntwo phases of learning, namely attribute learning and label learning.\nAttribute learning connects the paralinguistic features and attributes\nutilising speech with known emotional labels, while label learning\naims at defining unseen emotions through the attributes. The experimental\nresults achieved on the CINEMO corpus indicate that zero-shot learning\nis a useful technique for autonomous speech-based emotion learning,\nachieving accuracies considerably better than chance level and an attribute-based\ngold-standard setup. Furthermore, different emotion recognition tasks,\nemotional attributes, and employed approaches strongly influence system\nperformance.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2406",
      "author_area_id": 5,
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "sudhakara19_interspeech": {
      "authors": [
        [
          "Sweekar",
          "Sudhakara"
        ],
        [
          "Manoj Kumar",
          "Ramanathi"
        ],
        [
          "Chiranjeevi",
          "Yarra"
        ],
        [
          "Prasanta Kumar",
          "Ghosh"
        ]
      ],
      "title": "An Improved Goodness of Pronunciation (GoP) Measure for Pronunciation Evaluation with DNN-HMM System Considering HMM Transition Probabilities",
      "original": "2363",
      "page_count": 5,
      "order": 195,
      "p1": "954",
      "pn": "958",
      "abstract": [
        "Goodness of pronunciation (GoP) is typically formulated with Gaussian\nmixture model-hidden Markov model (GMM-HMM) based acoustic models considering\nHMM state transition probabilities (STPs) and GMM likelihoods of context\ndependent phonemes. On the other hand, deep neural network (DNN)-HMM\nbased acoustic models employed sub-phonemic (senone) posteriors instead\nof GMM likelihoods along with STPs. However, each senone is shared\nacross many states; thus, there is no one-to-one correspondence between\nthem. In order to circumvent this, most of the existing works have\nproposed modifications to the GoP formulation considering only posteriors\nneglecting the STPs. In this work, we derive a formulation for the\nGoP and it results in the formulation involving both senone posteriors\nand STPs. Further, we illustrate the steps to implement the proposed\nGoP formulation in Kaldi, a state-of-the-art automatic speech recognition\ntoolkit. Experiments are conducted on English data collected from Indian\nspeakers using acoustic models trained with native English data from\nLibriSpeech and Fisher-English corpora. The highest improvement in\nthe correlation coefficient between the scores from the formulations\nand the expert ratings is found to be 14.89% (relative) better with\nthe proposed approach compared to the best of the existing formulations\nthat don&#8217;t include STPs.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2363",
      "author_area_id": 5,
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "saha19b_interspeech": {
      "authors": [
        [
          "Atreyee",
          "Saha"
        ],
        [
          "Chiranjeevi",
          "Yarra"
        ],
        [
          "Prasanta Kumar",
          "Ghosh"
        ]
      ],
      "title": "Low Resource Automatic Intonation Classification Using Gated Recurrent Unit (GRU) Networks Pre-Trained with Synthesized Pitch Patterns",
      "original": "2351",
      "page_count": 5,
      "order": 196,
      "p1": "959",
      "pn": "963",
      "abstract": [
        "Second language learners of British English (BE) are typically trained\nto learn four intonation classes &#8212; Glide-up, Glide-down, Dive\nand Take-off. We predict the intonation class in a learner&#8217;s\nutterance by modeling the temporal dependencies in the pitch patterns\nwith gated recurrent unit (GRU) networks. For these, we pre-train the\nGRU network using a set of synthesized pitch patterns representing\neach intonation class. For the synthesis, we propose to obtain pitch\npatterns from the tone sequences representing each intonation class\nobtained from domain knowledge. Experiments are conducted on speech\ndata collected from experts in a spoken English training material for\nteaching BE intonation. The absolute improvements in the unweighted\naverage recall (UAR) using the proposed scheme with pre-training are\nfound to be 4.14% and 6.01% respectively over the proposed approach\nwithout pre-training and the baseline scheme that uses hidden Markov\nmodels (HMMs).\n"
      ],
      "doi": "10.21437/Interspeech.2019-2351",
      "author_area_id": 5,
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "vasquezcorrea19b_interspeech": {
      "authors": [
        [
          "J.C.",
          "V\u00e1squez-Correa"
        ],
        [
          "T.",
          "Arias-Vergara"
        ],
        [
          "Philipp",
          "Klumpp"
        ],
        [
          "M.",
          "Strauss"
        ],
        [
          "A.",
          "K\u00fcderle"
        ],
        [
          "N.",
          "Roth"
        ],
        [
          "S.",
          "Bayerl"
        ],
        [
          "N.",
          "Garc\u00eda-Ospina"
        ],
        [
          "P.A.",
          "Perez-Toro"
        ],
        [
          "L.F.",
          "Parra-Gallego"
        ],
        [
          "Cristian David",
          "Rios-Urrego"
        ],
        [
          "D.",
          "Escobar-Grisales"
        ],
        [
          "Juan Rafael",
          "Orozco-Arroyave"
        ],
        [
          "B.",
          "Eskofier"
        ],
        [
          "Elmar",
          "N\u00f6th"
        ]
      ],
      "title": "Apkinson: A Mobile Solution for Multimodal Assessment of Patients with Parkinson&#8217;s Disease",
      "original": "8003",
      "page_count": 2,
      "order": 197,
      "p1": "964",
      "pn": "965",
      "abstract": [
        "Parkinson&#8217;s disease is a neurological disorder that produces\ndifferent motor impairments in the patients. The longitudinal assessment\nof the neurological state of patients is important to improve their\nquality of life. We introduced Apkinson, a smartphone application to\nevaluate continuously the speech and movement deficits of Parkinson&#8217;s\npatients, who receive feedback about their current state after performing\ndifferent exercises. The speech assessment considers phonation, articulation,\nand prosody capabilities of the patients. Movement exercises captured\nwith the inertial sensors of the smartphone evaluated symptoms in the\nupper and lower limbs.\n"
      ]
    },
    "kiss19_interspeech": {
      "authors": [
        [
          "G\u00e1bor",
          "Kiss"
        ],
        [
          "D\u00e1vid",
          "Sztah\u00f3"
        ],
        [
          "Kl\u00e1ra",
          "Vicsi"
        ]
      ],
      "title": "Depression State Assessment: Application for Detection of Depression by Speech",
      "original": "8004",
      "page_count": 2,
      "order": 198,
      "p1": "966",
      "pn": "967",
      "abstract": [
        "We present an application that detects depression by speech based on\na speech feature extraction engine. The input of the application is\na read speech sample and the output is predicted depression severity\nlevel (Beck Depression Inventory). The application analyses the speech\nsample and evaluates it using support vector regression (SVR). The\ndeveloped system could assist general medical staff if no specialist\nis present to aid the diagnosis. If there is a suspicion that the speaker\nis suffering from depression, it is inevitable to seek special medical\nassistance. The application supports five native languages: English,\nFrench, German, Hungarian and Italian.\n"
      ]
    },
    "yarra19_interspeech": {
      "authors": [
        [
          "Chiranjeevi",
          "Yarra"
        ],
        [
          "Aparna",
          "Srinivasan"
        ],
        [
          "Sravani",
          "Gottimukkala"
        ],
        [
          "Prasanta Kumar",
          "Ghosh"
        ]
      ],
      "title": "SPIRE-fluent: A Self-Learning App for Tutoring Oral Fluency to Second Language English Learners",
      "original": "8008",
      "page_count": 2,
      "order": 199,
      "p1": "968",
      "pn": "969",
      "abstract": [
        "Second language (L2) learners often achieve oral fluency by correct\npronunciation of words with appropriate pauses. It has been shown that\nthe L2 learners improve their language skills using mobile apps in\na self-learning manner. Effective learning is possible with apps that\nprovide detailed feedback. However, apps that train oral fluency in\nan automatic way are not available. In this work, we present SPIRE-fluent\napp, which provides an automatic feedback with scores representing\nlearner&#8217;s pronunciation quality, for each word in a sentence\nand for the entire sentence. The word specific scores are computed\nbased on the correctness of pronunciation with respect to the expert&#8217;s\naudio. Further, the app displays the syllables uttered and a set of\ntwo types of pauses produced by the learners and the expert while speaking\nthe sentence. Considering this as a feedback, the learner can correct\ntheir mistakes based on the mismatches between those utterances. In\naddition, it also estimates any pause made by the learners within a\nword and highlights the syllable containing the phoneme preceding the\npause.\n"
      ]
    },
    "nissen19b_interspeech": {
      "authors": [
        [
          "Shawn",
          "Nissen"
        ],
        [
          "Rebecca",
          "Nissen"
        ]
      ],
      "title": "Using Real-Time Visual Biofeedback for Second Language Instruction",
      "original": "8016",
      "page_count": 2,
      "order": 200,
      "p1": "970",
      "pn": "971",
      "abstract": [
        "This demonstration will illustrate how using real-time visual biofeedback,\nthrough a relatively new type of electropalatographic (EPG) sensor,\nmight facilitate improved pronunciation for learners of a second language\n(L2). The manner in which the EPG sensor is created and its use to\ntrack lingua-palatal articulation patterns will be described to individuals.\nThis presentation will also include an explanation of how a student\ncan visualize the contact patterns of their speech using the associated\ninstructional software. A brief tutorial on the features of the instructional\nsoftware will also be explained during the &#8220;show and tell&#8221;\npresentation.\n"
      ]
    },
    "miwardelli19_interspeech": {
      "authors": [
        [
          "A.",
          "Miwardelli"
        ],
        [
          "I.",
          "Gallagher"
        ],
        [
          "J.",
          "Gibson"
        ],
        [
          "N.",
          "Katsos"
        ],
        [
          "Kate M.",
          "Knill"
        ],
        [
          "H.",
          "Wood"
        ]
      ],
      "title": "Splash: Speech and Language Assessment in Schools and Homes",
      "original": "8027",
      "page_count": 2,
      "order": 201,
      "p1": "972",
      "pn": "973",
      "abstract": [
        "This paper presents a tablet-based app for Speech and Language Assessment\nin Schools and Homes ( Splash) to provide a first screening for young\nchildren aged 4&#8211;6 years to assess their speech and language skills.\nThe app aims to be easy-to-administer with an adult, such as a teacher\nor parent, directing the child through the tasks. Three fun games have\nbeen developed to assess receptive language, expressive language and\nconnected speech, respectively. Currently in proof-of-concept mode,\nwhen complete Splash will use automatic spoken language processing\nto give an instant estimate of a child&#8217;s communication ability\nand provide guidance on whether to speak specialist support. While\nnot a diagnostic tool, the aim is for Splash to be used to provide\nimmediate reassurance or direction to concerned parents, guardians\nor teachers as it can be administered by anyone, anywhere.\n"
      ]
    },
    "annand19_interspeech": {
      "authors": [
        [
          "Colin T.",
          "Annand"
        ],
        [
          "Maurice",
          "Lamb"
        ],
        [
          "Sarah",
          "Dugan"
        ],
        [
          "Sarah R.",
          "Li"
        ],
        [
          "Hannah M.",
          "Woeste"
        ],
        [
          "T. Douglas",
          "Mast"
        ],
        [
          "Michael A.",
          "Riley"
        ],
        [
          "Jack A.",
          "Masterson"
        ],
        [
          "Neeraja",
          "Mahalingam"
        ],
        [
          "Kathryn J.",
          "Eary"
        ],
        [
          "Caroline",
          "Spencer"
        ],
        [
          "Suzanne",
          "Boyce"
        ],
        [
          "Stephanie",
          "Jackson"
        ],
        [
          "Anoosha",
          "Baxi"
        ],
        [
          "Rene\u00e9",
          "Seward"
        ]
      ],
      "title": "Using Ultrasound Imaging to Create Augmented Visual Biofeedback for Articulatory Practice",
      "original": "8036",
      "page_count": 2,
      "order": 202,
      "p1": "974",
      "pn": "975",
      "abstract": [
        "Ultrasound images of the tongue surface can be used to provide real-time\nvisual feedback for clinical practitioners and speakers adjusting pronunciation\npatterns. However, rapid and complex movements of the tongue can be\ndifficult to interpret and directly relate to desired changes. We are\ndeveloping a method for simplified visual feedback controlled by efficient,\nreal-time tracking of tongue contours in ultrasound images. Our feedback\nand control paradigm are briefly discussed, and video of a potential\ngame-like biofeedback stimulus is demonstrated.\n"
      ]
    },
    "radostev19_interspeech": {
      "authors": [
        [
          "Vasiliy",
          "Radostev"
        ],
        [
          "Serge",
          "Berger"
        ],
        [
          "Justin",
          "Tabrizi"
        ],
        [
          "Pasha",
          "Kamyshev"
        ],
        [
          "Hisami",
          "Suzuki"
        ]
      ],
      "title": "Speech-Based Web Navigation for Limited Mobility Users",
      "original": "8042",
      "page_count": 2,
      "order": 203,
      "p1": "976",
      "pn": "977",
      "abstract": [
        "We present a novel approach that introduces the strengths of voice\nassistants into a web browser that makes the task of web navigation\na lot more accessible to all users, especially under limited mobility\ncircumstances. Voice assistants have now been widely adopted and is\nproviding great user experience for getting simple actions done quickly\nor getting a quick answer to a question. On the other hand, the benefits\nof voice assistants have not yet penetrated to the scenarios such as\nweb navigation, which has so far been driven by mouse, keyboard and\ntouch-based input only. In this paper, we demonstrate our speech-based\nweb navigation system, and show that our system improves the completion\nof the web navigation task on both PC and mobile phone significantly\nas compared with an out-of-the-box voice assistants on this task.\n"
      ]
    },
    "schultz19_interspeech": {
      "authors": [
        [
          "Tanja",
          "Schultz"
        ]
      ],
      "title": "Biosignal Processing for Human-Machine Interaction",
      "original": "abs6",
      "page_count": 0,
      "order": 204,
      "p1": "0",
      "pn": "",
      "abstract": [
        "Human interaction is a complex process involving modalities such as\nspeech, gestures, motion, and brain activities emitting a wide range\nof biosignals, which can be captured by a broad panoply of sensors.\nThe processing and interpretation of these biosignals offer an inside\nperspective on human physical and mental activities and thus complement\nthe traditional way of observing human interaction from the outside.\nAs recent years have seen major advances in sensor technologies integrated\ninto ubiquitous devices, and in machine learning methods to process\nand learn from the resulting data, the time is right to use of the\nfull range of biosignals to gain further insights into the process\nof human-machine interaction.<br style=\"mso-data-placement:same-cell;\"\n/><br style=\"mso-data-placement:same-cell;\" />\n  In my talk I will\npresent ongoing research at the Cognitive Systems Lab (CSL), where\nwe explore interaction-related biosignals with the goal of advancing\nmachine-mediated human communication and human-machine interaction.\nSeveral applications will be described such as Silent Speech Interfaces\nthat rely on articulatory muscle movement captured by electromyography\nto recognize and synthesize silently produced speech, as well as Brain\nComputer Interfaces that use brain activity captured by electrocorticography\nto recognize speech (brain-to-text) and directly convert electrocortical\nsignals into audible speech (brain-to-speech). I will also describe\nthe recording, processing and automatic structuring of human everyday\nactivities based on multimodal high-dimensional biosignals within the\nframework of EASE, a collaborative research center on cognition-enabled\nrobotics. This work aims to establish an open-source biosignals corpus\nfor investigations on how humans plan and execute interactions with\nthe aim of facilitating robotic mastery of everyday activities.\n"
      ]
    },
    "ryant19_interspeech": {
      "authors": [
        [
          "Neville",
          "Ryant"
        ],
        [
          "Kenneth",
          "Church"
        ],
        [
          "Christopher",
          "Cieri"
        ],
        [
          "Alejandrina",
          "Cristia"
        ],
        [
          "Jun",
          "Du"
        ],
        [
          "Sriram",
          "Ganapathy"
        ],
        [
          "Mark",
          "Liberman"
        ]
      ],
      "title": "The Second DIHARD Diarization Challenge: Dataset, Task, and Baselines",
      "original": "1268",
      "page_count": 5,
      "order": 205,
      "p1": "978",
      "pn": "982",
      "abstract": [
        "This paper introduces the second DIHARD challenge, the second in a\nseries of speaker diarization challenges intended to improve the robustness\nof diarization systems to variation in recording equipment, noise conditions,\nand conversational domain. The challenge comprises four tracks evaluating\ndiarization performance under two input conditions (single channel\nvs. multi-channel) and two segmentation conditions (diarization from\na reference speech segmentation vs. diarization from scratch). In order\nto prevent participants from overtuning to a particular combination\nof recording conditions and conversational domain, recordings are drawn\nfrom a variety of sources ranging from read audiobooks to meeting speech,\nto child language acquisition recordings, to dinner parties, to web\nvideo. We describe the task and metrics, challenge design, datasets,\nand baseline systems for speech enhancement, speech activity detection,\nand diarization.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1268",
      "author_area_id": 13,
      "author_area_label": "Special Sessions"
    },
    "singh19_interspeech": {
      "authors": [
        [
          "Prachi",
          "Singh"
        ],
        [
          "Harsha Vardhan",
          "M.A."
        ],
        [
          "Sriram",
          "Ganapathy"
        ],
        [
          "A.",
          "Kanagasundaram"
        ]
      ],
      "title": "LEAP Diarization System for the Second DIHARD Challenge",
      "original": "2716",
      "page_count": 5,
      "order": 206,
      "p1": "983",
      "pn": "987",
      "abstract": [
        "This paper presents the LEAP System, developed for the Second DIHARD\ndiarization Challenge. The evaluation data in the challenge is composed\nof multi-talker speech in restaurants, doctor-patient conversations,\nchild language acquisition recordings in home environments and audio\nextracted YouTube videos. The LEAP system is developed using two types\nof embeddings, one based on i-vector representations and the other\none based on x-vector representations. The initial diarization output\nobtained using agglomerative hierarchical clustering (AHC) done on\nthe probabilistic linear discriminant analysis (PLDA) scores is refined\nusing the Variational-Bayes hidden Markov model (VB-HMM) model. We\npropose a modified VB-HMM model with posterior scaling which provides\nsignificant improvements in the final diarization error rate (DER).\nWe also use a domain compensation on the i-vector features to reduce\nthe mis-match between training and evaluation conditions. N(s)TN(s)TN(s)T\nUsing the proposed approaches, we obtain relative improvements in DER\nof about 7.1% relative for the best individual system over the DIHARD\nbaseline system and about 13.7% relative for the final system combination\non evaluation set. An analysis performed using the proposed posterior\nscaling method shows that scaling results in improved discrimination\namong the HMM states in the VB-HMM.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2716",
      "author_area_id": 13,
      "author_area_label": "Special Sessions"
    },
    "vinals19_interspeech": {
      "authors": [
        [
          "Ignacio",
          "Vi\u00f1als"
        ],
        [
          "Pablo",
          "Gimeno"
        ],
        [
          "Alfonso",
          "Ortega"
        ],
        [
          "Antonio",
          "Miguel"
        ],
        [
          "Eduardo",
          "Lleida"
        ]
      ],
      "title": "ViVoLAB Speaker Diarization System for the DIHARD 2019 Challenge",
      "original": "2462",
      "page_count": 5,
      "order": 207,
      "p1": "988",
      "pn": "992",
      "abstract": [
        "This paper presents the latest improvements in Speaker Diarization\nobtained by ViVoLAB research group for the 2019 DIHARD Diarization\nChallenge. This evaluation seeks the improvement of the diarization\ntask in adverse conditions. For this purpose, the audio recordings\ninvolve multiple scenarios with no restrictions in terms of speakers,\noverlapped speech nor quality of the audio. Our submission follows\nthe traditional segmentation-clustering-resegmentation pipeline: Speaker\nembeddings are extracted from acoustic segments with a single speaker\non them, later clustered by means of a PLDA. Our contribution in this\nwork is focused on the clustering step. We present results with our\nVariational Bayes PLDA clustering and our tree-based clustering strategy,\nwhich sequentially assigns the different embeddings to its corresponding\nspeaker according to a PLDA model. Both strategies compare multiple\ndiarization hypotheses and choose their candidate one according to\na generative criterion. We also analyze the impact of the different\navailable embeddings in the state-of-the-art with both clustering approaches.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2462",
      "author_area_id": 13,
      "author_area_label": "Special Sessions"
    },
    "zajic19_interspeech": {
      "authors": [
        [
          "Zbyn\u011bk",
          "Zaj\u00edc"
        ],
        [
          "Marie",
          "Kune\u0161ov\u00e1"
        ],
        [
          "Marek",
          "Hr\u00faz"
        ],
        [
          "Jan",
          "Van\u011bk"
        ]
      ],
      "title": "UWB-NTIS Speaker Diarization System for the DIHARD II 2019 Challenge",
      "original": "1385",
      "page_count": 5,
      "order": 208,
      "p1": "993",
      "pn": "997",
      "abstract": [
        "In this paper, we present our system developed by the team from the\nNew Technologies for the Information Society (NTIS) research center\nof the University of West Bohemia in Pilsen, for the Second DIHARD\nSpeech Diarization Challenge. The base of our system follows the currently-standard\napproach of segmentation, i/x-vector extraction, clustering, and resegmentation.\nThe hyperparameters for each of the subsystems were selected according\nto the domain classifier trained on the development set of DIHARD II.\nWe compared our system with results from the Kaldi diarization (with\ni/x-vectors) and combined these systems. At the time of writing of\nthis abstract, our best submission achieved a DER of 23.47% and a JER\nof 48.99% on the evaluation set (in Track 1 using reference SAD).\n"
      ],
      "doi": "10.21437/Interspeech.2019-1385",
      "author_area_id": 13,
      "author_area_label": "Special Sessions"
    },
    "park19b_interspeech": {
      "authors": [
        [
          "Tae Jin",
          "Park"
        ],
        [
          "Manoj",
          "Kumar"
        ],
        [
          "Nikolaos",
          "Flemotomos"
        ],
        [
          "Monisankha",
          "Pal"
        ],
        [
          "Raghuveer",
          "Peri"
        ],
        [
          "Rimita",
          "Lahiri"
        ],
        [
          "Panayiotis",
          "Georgiou"
        ],
        [
          "Shrikanth",
          "Narayanan"
        ]
      ],
      "title": "The Second DIHARD Challenge: System Description for USC-SAIL Team",
      "original": "1903",
      "page_count": 5,
      "order": 209,
      "p1": "998",
      "pn": "1002",
      "abstract": [
        "In this paper, we describe components that form a part of USC-SAIL\nteam&#8217;s submissions to Track 1 and Track 2 of the second DIHARD\nspeaker diarization challenge. We describe each module in our speaker\ndiarization pipeline and explain the rationale behind our choice of\nalgorithms for each module, while comparing the Diarization Error Rate\n(DER) against different module combinations. We propose a clustering\nscheme based on spectral clustering that yields competitive performance.\nMoreover, we introduce an overlap detection scheme and a re-segmentation\nsystem for speaker diarization and investigate their performances using\ncontrolled and in-the-wild conditions. In addition, we describe the\nadditional components that will be integrated to our speaker diarization\nsystem. To pursue the best performance, we compare our system with\nthe state-of-the-art methods that are presented in the previous challenge\nand literature. We include preliminary results of our speaker diarization\nsystem on the evaluation data from the second DIHARD challenge.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1903",
      "author_area_id": 13,
      "author_area_label": "Special Sessions"
    },
    "novoselov19_interspeech": {
      "authors": [
        [
          "Sergey",
          "Novoselov"
        ],
        [
          "Aleksei",
          "Gusev"
        ],
        [
          "Artem",
          "Ivanov"
        ],
        [
          "Timur",
          "Pekhovsky"
        ],
        [
          "Andrey",
          "Shulipa"
        ],
        [
          "Anastasia",
          "Avdeeva"
        ],
        [
          "Artem",
          "Gorlanov"
        ],
        [
          "Alexandr",
          "Kozlov"
        ]
      ],
      "title": "Speaker Diarization with Deep Speaker Embeddings for DIHARD Challenge II",
      "original": "2757",
      "page_count": 5,
      "order": 210,
      "p1": "1003",
      "pn": "1007",
      "abstract": [
        "This paper describes the ITMO University (DI-IT team) speaker diarization\nsystems submitted to DIHARD Challenge II. As with DIHARD I, this challenge\nis focused on diarization task for microphone recordings in varying\ndifficult conditions. According to the results of the previous DIHARD\nI Challenge state-of-the-art diarization systems are based on x-vector\nembeddings. Such embeddings are clustered using agglomerative hierarchical\nclustering (AHC) algorithm by means of PLDA scoring. Current research\ncontinues the investigation of deep speaker embedding efficiency for\nthe speaker diarization task. This paper explores new types of embedding\nextractors with different deep neural network architectures and training\nstrategies. We also used AHC to perform embeddings clustering. Alternatively\nto the PLDA scoring in our AHC procedure we used discriminatively trained\ncosine similarity metric learning (CSML) model for scoring. Moreover\nwe focused on the optimal AHC threshold tuning according to the specific\nspeech quality. Environment classifier was preliminary trained on development\nset to predict acoustic conditions for this purpose. We show that such\nthreshold adaptation scheme allows to reduce diarization error rate\ncompared to common AHC threshold for all conditions.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2757",
      "author_area_id": 13,
      "author_area_label": "Special Sessions"
    },
    "todisco19_interspeech": {
      "authors": [
        [
          "Massimiliano",
          "Todisco"
        ],
        [
          "Xin",
          "Wang"
        ],
        [
          "Ville",
          "Vestman"
        ],
        [
          "Md.",
          "Sahidullah"
        ],
        [
          "H\u00e9ctor",
          "Delgado"
        ],
        [
          "Andreas",
          "Nautsch"
        ],
        [
          "Junichi",
          "Yamagishi"
        ],
        [
          "Nicholas",
          "Evans"
        ],
        [
          "Tomi H.",
          "Kinnunen"
        ],
        [
          "Kong Aik",
          "Lee"
        ]
      ],
      "title": "ASVspoof 2019: Future Horizons in Spoofed and Fake Audio Detection",
      "original": "2249",
      "page_count": 5,
      "order": 211,
      "p1": "1008",
      "pn": "1012",
      "abstract": [
        "ASVspoof, now in its third edition, is a series of community-led challenges\nwhich promote the development of countermeasures to protect automatic\nspeaker verification (ASV) from the threat of spoofing. Advances in\nthe 2019 edition include: (i) a consideration of both logical access\n(LA) and physical access (PA) scenarios and the three major forms of\nspoofing attack, namely synthetic, converted and replayed speech; (ii)\nspoofing attacks generated with state-of-the-art neural acoustic and\nwaveform models; (iii) an improved, controlled simulation of replay\nattacks; (iv) use of the tandem detection cost function (t-DCF) that\nreflects the impact of both spoofing and countermeasures upon ASV reliability.\nEven if ASV remains the core focus, in retaining the equal error rate\n(EER) as a secondary metric, ASVspoof also embraces the growing importance\nof  fake audio detection. ASVspoof 2019 attracted the participation\nof 63 research teams, with more than half of these reporting systems\nthat improve upon the performance of two baseline spoofing countermeasures.\nThis paper describes the 2019 database, protocols and challenge results.\nIt also outlines major findings which demonstrate the real progress\nmade in protecting against the threat of spoofing and fake audio.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2249",
      "author_area_id": 13,
      "author_area_label": "Special Sessions"
    },
    "lai19b_interspeech": {
      "authors": [
        [
          "Cheng-I",
          "Lai"
        ],
        [
          "Nanxin",
          "Chen"
        ],
        [
          "Jes\u00fas",
          "Villalba"
        ],
        [
          "Najim",
          "Dehak"
        ]
      ],
      "title": "ASSERT: Anti-Spoofing with Squeeze-Excitation and Residual Networks",
      "original": "1794",
      "page_count": 5,
      "order": 212,
      "p1": "1013",
      "pn": "1017",
      "abstract": [
        "We present JHU&#8217;s system submission to the ASVspoof 2019 Challenge:\nAnti-Spoofing with Squeeze-Excitation and Residual neTworks (ASSERT).\nAnti-spoofing has gathered more and more attention since the inauguration\nof the ASVspoof Challenges, and ASVspoof 2019 dedicates to address\nattacks from all three major types: text-to-speech, voice conversion,\nand replay. Built upon previous research work on Deep Neural Network\n(DNN), ASSERT is a pipeline for DNN-based approach to anti-spoofing.\nASSERT has four components: feature engineering, DNN models, network\noptimization and system combination, where the DNN models are variants\nof squeeze-excitation and residual networks. We conducted an ablation\nstudy of the effectiveness of each component on the ASVspoof 2019 corpus,\nand experimental results showed that ASSERT obtained more than 93%\nand 17% relative improvements over the baseline systems in the two\nsub-challenges in ASVspoof 2019, ranking ASSERT one of the top performing\nsystems. Code and pretrained models are made publicly available.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1794",
      "author_area_id": 13,
      "author_area_label": "Special Sessions"
    },
    "chettri19_interspeech": {
      "authors": [
        [
          "Bhusan",
          "Chettri"
        ],
        [
          "Daniel",
          "Stoller"
        ],
        [
          "Veronica",
          "Morfi"
        ],
        [
          "Marco A. Mart\u00ednez",
          "Ram\u00edrez"
        ],
        [
          "Emmanouil",
          "Benetos"
        ],
        [
          "Bob L.",
          "Sturm"
        ]
      ],
      "title": "Ensemble Models for Spoofing Detection in Automatic Speaker Verification",
      "original": "2505",
      "page_count": 5,
      "order": 213,
      "p1": "1018",
      "pn": "1022",
      "abstract": [
        "Detecting spoofing attempts of automatic speaker verification (ASV)\nsystems is challenging, especially when using only one modelling approach.\nFor robustness, we use both deep neural networks and traditional machine\nlearning models and combine them as ensemble models through logistic\nregression. They are trained to detect logical access (LA) and physical\naccess (PA) attacks on the dataset released as part of the ASV Spoofing\nand Countermeasures Challenge 2019. We propose dataset partitions that\nensure different attack types are present during training and validation\nto improve system robustness. Our ensemble model outperforms all our\nsingle models and the baselines from the challenge for both attack\ntypes. We investigate why some models on the PA dataset strongly outperform\nothers and find that spoofed recordings in the dataset tend to have\nlonger silences at the end than genuine ones. By removing them, the\nPA task becomes much more challenging, with the tandem detection cost\nfunction (t-DCF) of our best single model rising from 0.1672 to 0.5018\nand equal error rate (EER) increasing from 5.98% to 19.8% on the development\nset.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2505",
      "author_area_id": 13,
      "author_area_label": "Special Sessions"
    },
    "cai19_interspeech": {
      "authors": [
        [
          "Weicheng",
          "Cai"
        ],
        [
          "Haiwei",
          "Wu"
        ],
        [
          "Danwei",
          "Cai"
        ],
        [
          "Ming",
          "Li"
        ]
      ],
      "title": "The DKU Replay Detection System for the ASVspoof 2019 Challenge: On Data Augmentation, Feature Representation, Classification, and Fusion",
      "original": "1230",
      "page_count": 5,
      "order": 214,
      "p1": "1023",
      "pn": "1027",
      "abstract": [
        "This paper describes our DKU replay detection system for the ASVspoof\n2019 challenge. The goal is to develop spoofing countermeasure for\nautomatic speaker recognition in physical access scenario. We leverage\nthe countermeasure system pipeline from four aspects, including the\ndata augmentation, feature representation, classification, and fusion.\nFirst, we introduce an utterance-level deep learning framework for\nanti-spoofing. It receives the variable-length feature sequence and\noutputs the utterance-level scores directly. Based on the framework,\nwe try out various kinds of input feature representations extracted\nfrom either the magnitude spectrum or phase spectrum. Besides, we also\nperform the data augmentation strategy by applying the speed perturbation\non the raw waveform. Our best single system employs a residual neural\nnetwork trained by the speed-perturbed group delay gram. It achieves\nEER of 1.04% on the development set, as well as EER of 1.08% on the\nevaluation set. Finally, using the simple average score from several\nsingle systems can further improve the performance. EER of 0.24% on\nthe development set and 0.66% on the evaluation set is obtained for\nour primary system.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1230",
      "author_area_id": 13,
      "author_area_label": "Special Sessions"
    },
    "biaobrzeski19_interspeech": {
      "authors": [
        [
          "Rados\u0142aw",
          "Bia\u0142obrzeski"
        ],
        [
          "Micha\u0142",
          "Ko\u015bmider"
        ],
        [
          "Mateusz",
          "Matuszewski"
        ],
        [
          "Marcin",
          "Plata"
        ],
        [
          "Alexander",
          "Rakowski"
        ]
      ],
      "title": "Robust Bayesian and Light Neural Networks for Voice Spoofing Detection",
      "original": "2676",
      "page_count": 5,
      "order": 215,
      "p1": "1028",
      "pn": "1032",
      "abstract": [
        "We present a replay attack detection system consisting of two convolutional\nneural network models. The first model consists of a small Bayesian\nneural network, motivated by the hypothesis that Bayesian models are\nrobust to overfitting. The second one uses a bigger architecture, LCNN,\nextended with several regularization techniques to improve generalization.\nOur experiments, considering both size of the networks and use of the\nBayesian approach, indicated that smaller networks are sufficient to\nachieve competitive results. To better estimate the performance against\nunseen spoofing methods, the final models were selected using novel\nAttack-Out Cross-Validation. In this procedure each model was tested\non a subset of data containing not only previously unseen speakers,\nbut also unseen spoofing attacks. The system was submitted to ASVspoof\n2019 challenge&#8217;s PA condition and achieved a t-DCF score of 0.0219\nand EER of 0.88% on the evaluation dataset, which is a 10 times relative\nimprovement over the baseline.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2676",
      "author_area_id": 13,
      "author_area_label": "Special Sessions"
    },
    "lavrentyeva19_interspeech": {
      "authors": [
        [
          "Galina",
          "Lavrentyeva"
        ],
        [
          "Sergey",
          "Novoselov"
        ],
        [
          "Andzhukaev",
          "Tseren"
        ],
        [
          "Marina",
          "Volkova"
        ],
        [
          "Artem",
          "Gorlanov"
        ],
        [
          "Alexandr",
          "Kozlov"
        ]
      ],
      "title": "STC Antispoofing Systems for the ASVspoof2019 Challenge",
      "original": "1768",
      "page_count": 5,
      "order": 216,
      "p1": "1033",
      "pn": "1037",
      "abstract": [
        "This paper describes the Speech Technology Center (STC) antispoofing\nsystems submitted to the ASVspoof 2019 challenge. The ASVspoof2019\nis the extended version of the previous challenges and includes 2 evaluation\nconditions: logical access use-case scenario with speech synthesis\nand voice conversion attack types and physical access use-case scenario\nwith replay attacks. During the challenge we developed anti-spoofing\nsolutions for both scenarios. The proposed systems are implemented\nusing deep learning approach and are based on different types of acoustic\nfeatures. We enhanced Light CNN architecture previously considered\nby the authors for replay attacks detection and which performed high\nspoofing detection quality during the ASVspoof2017 challenge. In particular\nhere we investigate the efficiency of angular margin based softmax\nactivation for training robust deep Light CNN classifier to solve the\nmentioned-above tasks. Submitted systems achieved EER of 1.86% in logical\naccess scenario and 0.54% in physical access scenario on the evaluation\npart of the Challenge corpora. High performance obtained for the unknown\ntypes of spoofing attacks demonstrates the stability of the offered\napproach in both evaluation conditions.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1768",
      "author_area_id": 13,
      "author_area_label": "Special Sessions"
    },
    "yang19b_interspeech": {
      "authors": [
        [
          "Yexin",
          "Yang"
        ],
        [
          "Hongji",
          "Wang"
        ],
        [
          "Heinrich",
          "Dinkel"
        ],
        [
          "Zhengyang",
          "Chen"
        ],
        [
          "Shuai",
          "Wang"
        ],
        [
          "Yanmin",
          "Qian"
        ],
        [
          "Kai",
          "Yu"
        ]
      ],
      "title": "The SJTU Robust Anti-Spoofing System for the ASVspoof 2019 Challenge",
      "original": "2170",
      "page_count": 5,
      "order": 217,
      "p1": "1038",
      "pn": "1042",
      "abstract": [
        "The robustness of an anti-spoofing system is progressively more important\nin order to develop a reliable speaker verification system. Previous\nchallenges and datasets mainly focus on a specific type of spoofing\nattacks. The ASVspoof 2019 edition is the first challenge to address\ntwo major spoofing types &#8212; logical and physical access. This\npaper presents the SJTU&#8217;s submitted anti-spoofing system to the\nASVspoof 2019 challenge. Log-CQT features are developed in conjunction\nwith multi-layer convolutional neural networks for robust performance\nacross both subtasks. CNNs with gradient linear units (GLU) activations\nare utilized for spoofing detection. The proposed system shows consistent\nperformance improvement over all types of spoofing attacks. Our primary\nsubmissions achieve the 5<SUP>th</SUP> and 8<SUP>th</SUP> positions\nfor the logical and physical access respectively. Moreover, our contrastive\nsubmission to the PA task exhibits better generalization compared to\nour primary submission, and achieves a comparable performance to the\n3<SUP>rd</SUP> position of the challenge.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2170",
      "author_area_id": 13,
      "author_area_label": "Special Sessions"
    },
    "alluri19_interspeech": {
      "authors": [
        [
          "K.N.R.K. Raju",
          "Alluri"
        ],
        [
          "Anil Kumar",
          "Vuppala"
        ]
      ],
      "title": "IIIT-H Spoofing Countermeasures for Automatic Speaker Verification Spoofing and Countermeasures Challenge 2019",
      "original": "1623",
      "page_count": 5,
      "order": 218,
      "p1": "1043",
      "pn": "1047",
      "abstract": [
        "The ASVspoof 2019 challenge focuses on countermeasures for all major\nspoofing attacks, namely speech synthesis (SS), voice conversion (VC),\nand replay spoofing attacks. This paper describes the IIIT-H spoofing\ncountermeasures developed for ASVspoof 2019 challenge. In this study,\nthree instantaneous cepstral features namely, single frequency cepstral\ncoefficients, zero time windowing cepstral coefficients, and instantaneous\nfrequency cepstral coefficients are used as front-end features. A Gaussian\nmixture model is used as back-end classifier. The experimental results\non ASVspoof 2019 dataset reveal that the proposed instantaneous features\nare efficient in detecting VC and SS based attacks. In detecting replay\nattacks, proposed features are comparable with baseline systems. Further\nanalysis is carried out using metadata to assess the impact of proposed\ncountermeasures on different synthetic speech generating algorithm/replay\nconfigurations.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1623",
      "author_area_id": 13,
      "author_area_label": "Special Sessions"
    },
    "li19c_interspeech": {
      "authors": [
        [
          "Rongjin",
          "Li"
        ],
        [
          "Miao",
          "Zhao"
        ],
        [
          "Zheng",
          "Li"
        ],
        [
          "Lin",
          "Li"
        ],
        [
          "Qingyang",
          "Hong"
        ]
      ],
      "title": "Anti-Spoofing Speaker Verification System with Multi-Feature Integration and Multi-Task Learning",
      "original": "1698",
      "page_count": 5,
      "order": 219,
      "p1": "1048",
      "pn": "1052",
      "abstract": [
        "Speaker anti-spoofing is crucial to prevent security breaches when\nthe speaker verification systems encounter the spoofed attacks from\nthe advanced speech synthesis algorithms and high fidelity replay devices.\nIn this paper, we propose a framework based on multiple features integration\nand multi-task learning (MFMT) for improving anti-spoofing performance.\nIt is important to integrate the complementary information of multiple\nspectral features within the network, such as MFCC, CQCC, Fbank, etc.,\nas often a single kind of feature is not enough to grasp the global\nspoofing cues and it generalizes poorly. Furthermore, we propose a\nhelpful butterfly unit (BU) for multi-task learning to propagate the\nshared representations between the binary decision task and the other\nauxiliary task. The BU can obtain task representations of other branch\nduring forward propagation and prevent the gradient from assimilating\nthe branch during back propagation. Our proposed system yielded an\nEER of 9.01% on ASVspoof 2017, while the best single system and the\naverage scores fusion obtained the evaluation EER of 2.39% and 0.96%\non ASVspoof 2019 PA, respectively.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1698",
      "author_area_id": 13,
      "author_area_label": "Special Sessions"
    },
    "williams19b_interspeech": {
      "authors": [
        [
          "Jennifer",
          "Williams"
        ],
        [
          "Joanna",
          "Rownicka"
        ]
      ],
      "title": "Speech Replay Detection with x-Vector Attack Embeddings and Spectral Features",
      "original": "1760",
      "page_count": 5,
      "order": 220,
      "p1": "1053",
      "pn": "1057",
      "abstract": [
        "We present our system submission to the ASVspoof 2019 Challenge Physical\nAccess (PA) task. The objective for this challenge was to develop a\ncountermeasure that identifies speech audio as either bona fide or\nintercepted and replayed. The target prediction was a value indicating\nthat a speech segment was bona fide (positive values) or &#8220;spoofed&#8221;\n(negative values). Our system used convolutional neural networks (CNNs)\nand a representation of the speech audio that combined x-vector attack\nembeddings with signal processing features. The x-vector attack embeddings\nwere created from mel-frequency cepstral coefficients (MFCCs) using\na time-delay neural network (TDNN). These embeddings jointly modeled\n27 different environments and 9 types of attacks from the labeled data.\nWe also used sub-band spectral centroid magnitude coefficients (SCMCs)\nas features. We included an additive Gaussian noise layer during training\nas a way to augment the data to make our system more robust to previously\nunseen attack examples. We report system performance using the tandem\ndetection cost function (tDCF) and equal error rate (EER). Our approach\nperformed better that both of the challenge baselines. Our technique\nsuggests that our x-vector attack embeddings can help regularize the\nCNN predictions even when environments or attacks are more challenging.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1760",
      "author_area_id": 13,
      "author_area_label": "Special Sessions"
    },
    "das19_interspeech": {
      "authors": [
        [
          "Rohan Kumar",
          "Das"
        ],
        [
          "Jichen",
          "Yang"
        ],
        [
          "Haizhou",
          "Li"
        ]
      ],
      "title": "Long Range Acoustic Features for Spoofed Speech Detection",
      "original": "1887",
      "page_count": 5,
      "order": 221,
      "p1": "1058",
      "pn": "1062",
      "abstract": [
        "Speaker verification systems in practice are vulnerable to spoofing\nattacks. The high quality recording and playback devices make replay\nattack a real threat to speaker verification. Additionally, the furtherance\nin voice conversion and speech synthesis has produced perceptually\nnatural sounding speech. The ASVspoof 2019 challenge is organized to\nstudy the robustness of countermeasures against such attacks, which\ncover two common modes of attacks, logical and physical access. The\nformer deals with synthetic attacks arising from voice conversion and\ntext-to-speech techniques, whereas the latter deals with replay attacks.\nIn this work, we explore several novel countermeasures based on long\nrange acoustic features that are found to be effective for spoofing\nattack detection. The long range features capture different aspects\nof long range information as they are computed from subbands and octave\npower spectrum in contrast to the conventional way from linear power\nspectrum. These novel features are combined with the other known features\nfor improved detection of spoofing attacks. We obtain a tandem detection\ncost function of 0.1264 and 0.1381 (equal error rate 4.13% and 5.95%)\nfor logical and physical access on the best combined system submitted\nto the challenge.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1887",
      "author_area_id": 13,
      "author_area_label": "Special Sessions"
    },
    "chang19b_interspeech": {
      "authors": [
        [
          "Su-Yu",
          "Chang"
        ],
        [
          "Kai-Cheng",
          "Wu"
        ],
        [
          "Chia-Ping",
          "Chen"
        ]
      ],
      "title": "Transfer-Representation Learning for Detecting Spoofing Attacks with Converted and Synthesized Speech in Automatic Speaker Verification System",
      "original": "2014",
      "page_count": 5,
      "order": 222,
      "p1": "1063",
      "pn": "1067",
      "abstract": [
        "In this paper, we study a countermeasure module to detect spoofing\nattacks with converted or synthesized speech in tandem automatic speaker\nverification (ASV). Our approach integrates representation learning\nand transfer learning methods. For representation learning, good embedding\nnetwork functions are learned from audio signals with the goal to distinguish\ndifferent types of spoofing attacks. For transfer learning, the embedding\nnetwork functions are used to initialize fine-tuning networks. We experiment\nwell-known neural network architectures and front-end raw features\nto diversify and strengthen the information source for embedding. We\nparticipate in the 2019 Automatic Speaker Verification Spoofing and\nCountermeasures Challenge (ASVspoof 2019) and evaluate the proposed\nmethods with the logical access condition tasks for detecting converted\nspeech and synthesized speech. On the ASVspoof 2019 development set,\nour best single system achieves a minimum tandem decision cost function\nof nearly 0 during system development. On the ASVspoof 2019 evaluation\nset, our primary system achieves a minimum tandem decision cost of\n0.1791, and an equal error rate (EER) of 9.08%. Our system does not\nhave over-training issue as it achieves decent performance with unseen\ntest data of the types presented in training, yet the generalization\ngap is not small with mismatched test data types.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2014",
      "author_area_id": 13,
      "author_area_label": "Special Sessions"
    },
    "gomezalanis19_interspeech": {
      "authors": [
        [
          "Alejandro",
          "Gomez-Alanis"
        ],
        [
          "Antonio M.",
          "Peinado"
        ],
        [
          "Jose A.",
          "Gonzalez"
        ],
        [
          "Angel M.",
          "Gomez"
        ]
      ],
      "title": "A Light Convolutional GRU-RNN Deep Feature Extractor for ASV Spoofing Detection",
      "original": "2212",
      "page_count": 5,
      "order": 223,
      "p1": "1068",
      "pn": "1072",
      "abstract": [
        "The aim of this work is to develop a single anti-spoofing system which\ncan be applied to effectively detect all the types of spoofing attacks\nconsidered in the ASVspoof 2019 Challenge: text-to-speech, voice conversion\nand replay based attacks. To achieve this, we propose the use of a\nLight Convolutional Gated Recurrent Neural Network (LC-GRNN) as a deep\nfeature extractor to robustly represent speech signals as utterance-level\nembeddings, which are later used by a back-end recognizer which performs\nthe final genuine/spoofed classification. This novel architecture combines\nthe ability of light convolutional layers for extracting discriminative\nfeatures at frame level with the capacity of gated recurrent unit based\nRNNs for learning long-term dependencies of the subsequent deep features.\nThe proposed system has been presented as a contribution to the ASVspoof\n2019 Challenge, and the results show a significant improvement in comparison\nwith the baseline systems. Moreover, experiments were also carried\nout on the ASVspoof 2015 and 2017 corpora, and the results indicate\nthat our proposal clearly outperforms other popular methods recently\nproposed and other similar deep feature based systems.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2212",
      "author_area_id": 13,
      "author_area_label": "Special Sessions"
    },
    "zeinali19_interspeech": {
      "authors": [
        [
          "Hossein",
          "Zeinali"
        ],
        [
          "Themos",
          "Stafylakis"
        ],
        [
          "Georgia",
          "Athanasopoulou"
        ],
        [
          "Johan",
          "Rohdin"
        ],
        [
          "Ioannis",
          "Gkinis"
        ],
        [
          "Luk\u00e1\u0161",
          "Burget"
        ],
        [
          "Jan",
          "\u010cernock\u00fd"
        ]
      ],
      "title": "Detecting Spoofing Attacks Using VGG and SincNet: BUT-Omilia Submission to ASVspoof 2019 Challenge",
      "original": "2892",
      "page_count": 5,
      "order": 224,
      "p1": "1073",
      "pn": "1077",
      "abstract": [
        "In this paper, we present the system description of the joint efforts\nof Brno University of Technology (BUT) and Omilia &#8212; Conversational\nIntelligence for the ASVSpoof2019 Spoofing and Countermeasures Challenge.\nThe primary submission for Physical access (PA) is a fusion of two\nVGG networks, trained on single and two-channels features. For Logical\naccess (LA), our primary system is a fusion of VGG and the recently\nintroduced SincNet architecture. The results on PA show that the proposed\nnetworks yield very competitive performance in all conditions and achieved\n86% relative improvement compared to the official baseline. On the\nother hand, the results on LA showed that although the proposed architecture\nand training strategy performs very well on certain spoofing attacks,\nit fails to generalize to certain attacks that are unseen during training.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2892",
      "author_area_id": 13,
      "author_area_label": "Special Sessions"
    },
    "alzantot19_interspeech": {
      "authors": [
        [
          "Moustafa",
          "Alzantot"
        ],
        [
          "Ziqi",
          "Wang"
        ],
        [
          "Mani B.",
          "Srivastava"
        ]
      ],
      "title": "Deep Residual Neural Networks for Audio Spoofing Detection",
      "original": "3174",
      "page_count": 5,
      "order": 225,
      "p1": "1078",
      "pn": "1082",
      "abstract": [
        "The state-of-art models for speech synthesis and voice conversion are\ncapable of generating synthetic speech that is perceptually indistinguishable\nfrom bonafide human speech. These methods represent a threat to the\nautomatic speaker verification (ASV) systems. Additionally, replay\nattacks where the attacker uses a speaker to replay a previously recorded\ngenuine human speech are also possible. In this paper, we present our\nsolution for the ASVSpoof2019 competition, which aims to develop countermeasure\nsystems that distinguish between spoofing attacks and genuine speeches.\nOur model is inspired by the success of residual convolutional networks\nin many classification tasks. We build three variants of a residual\nconvolutional neural network that accept different feature representations\n(MFCC, log-magnitude STFT, and CQCC) of input. We compare the performance\nachieved by our model variants and the competition baseline models.\nIn the logical access scenario, the fusion of our models has zero t-DCF\ncost and zero equal error rate (EER), as evaluated on the development\nset. On the evaluation set, our model fusion improves the t-DCF and\nEER by 25% compared to the baseline algorithms. Against physical access\nreplay attacks, our model fusion improves the baseline algorithms t-DCF\nand EER scores by 71% and 75% on the evaluation set, respectively.\n"
      ],
      "doi": "10.21437/Interspeech.2019-3174",
      "author_area_id": 13,
      "author_area_label": "Special Sessions"
    },
    "jung19_interspeech": {
      "authors": [
        [
          "Jee-weon",
          "Jung"
        ],
        [
          "Hye-jin",
          "Shim"
        ],
        [
          "Hee-Soo",
          "Heo"
        ],
        [
          "Ha-Jin",
          "Yu"
        ]
      ],
      "title": "Replay Attack Detection with Complementary High-Resolution Information Using End-to-End DNN for the ASVspoof 2019 Challenge",
      "original": "1991",
      "page_count": 5,
      "order": 226,
      "p1": "1083",
      "pn": "1087",
      "abstract": [
        "In this study, we concentrate on replacing the process of extracting\nhand-crafted acoustic feature with end-to-end DNN using complementary\nhigh-resolution spectrograms. As a result of advance in audio devices,\ntypical characteristics of a replayed speech based on conventional\nknowledge alter or diminish in unknown replay configurations. Thus,\nit has become increasingly difficult to detect spoofed speech with\na conventional knowledge-based approach. To detect unrevealed characteristics\nthat reside in a replayed speech, we directly input spectrograms into\nan end-to-end DNN without knowledge-based intervention. Explorations\ndealt in this study that differentiates from existing spectrogram-based\nsystems are twofold: complementary information and high-resolution.\nSpectrograms with different information are explored, and it is shown\nthat additional information such as the phase information can be complementary.\nHigh-resolution spectrograms are employed with the assumption that\nthe difference between a bona-fide and a replayed speech exists in\nthe details. Additionally, to verify whether other features are complementary\nto spectrograms, we also examine raw waveform and an i-vector based\nsystem. Experiments conducted on the ASVspoof 2019 physical access\nchallenge show promising results, where t-DCF and equal error rates\nare 0.0570 and 2.45% for the evaluation set, respectively.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1991",
      "author_area_id": 13,
      "author_area_label": "Special Sessions"
    },
    "dunbar19_interspeech": {
      "authors": [
        [
          "Ewan",
          "Dunbar"
        ],
        [
          "Robin",
          "Algayres"
        ],
        [
          "Julien",
          "Karadayi"
        ],
        [
          "Mathieu",
          "Bernard"
        ],
        [
          "Juan",
          "Benjumea"
        ],
        [
          "Xuan-Nga",
          "Cao"
        ],
        [
          "Lucie",
          "Miskic"
        ],
        [
          "Charlotte",
          "Dugrain"
        ],
        [
          "Lucas",
          "Ondel"
        ],
        [
          "Alan W.",
          "Black"
        ],
        [
          "Laurent",
          "Besacier"
        ],
        [
          "Sakriani",
          "Sakti"
        ],
        [
          "Emmanuel",
          "Dupoux"
        ]
      ],
      "title": "The Zero Resource Speech Challenge 2019: TTS Without T",
      "original": "2904",
      "page_count": 5,
      "order": 227,
      "p1": "1088",
      "pn": "1092",
      "abstract": [
        "We present the Zero Resource Speech Challenge 2019, which proposes\nto build a speech synthesizer without any text or phonetic labels:\nhence, TTS without T (text-to-speech without text). We provide raw\naudio for a target voice in an unknown language (the Voice dataset),\nbut no alignment, text or labels. Participants must discover subword\nunits in an unsupervised way (using the Unit Discovery dataset) and\nalign them to the voice recordings in a way that works best for the\npurpose of synthesizing novel utterances from novel speakers, similar\nto the target speaker&#8217;s voice. We describe the metrics used for\nevaluation, a baseline system consisting of unsupervised subword unit\ndiscovery plus a standard TTS system, and a topline TTS using gold\nphoneme transcriptions. We present an overview of the 19 submitted\nsystems from 10 teams and discuss the main results.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2904",
      "author_area_id": 13,
      "author_area_label": "Special Sessions"
    },
    "feng19b_interspeech": {
      "authors": [
        [
          "Siyuan",
          "Feng"
        ],
        [
          "Tan",
          "Lee"
        ],
        [
          "Zhiyuan",
          "Peng"
        ]
      ],
      "title": "Combining Adversarial Training and Disentangled Speech Representation for Robust Zero-Resource Subword Modeling",
      "original": "1337",
      "page_count": 5,
      "order": 228,
      "p1": "1093",
      "pn": "1097",
      "abstract": [
        "This study addresses the problem of unsupervised subword unit discovery\nfrom untranscribed speech. It forms the basis of the ultimate goal\nof ZeroSpeech 2019, building text-to-speech systems without text labels.\nIn this work, unit discovery is formulated as a pipeline of phonetically\ndiscriminative feature learning and unit inference. One major difficulty\nin robust unsupervised feature learning is dealing with speaker variation.\nHere the robustness towards speaker variation is achieved by applying\nadversarial training and FHVAE based disentangled speech representation\nlearning. A comparison of the two approaches as well as their combination\nis studied in a DNN-bottleneck feature (DNN-BNF) architecture. Experiments\nare conducted on ZeroSpeech 2019 and 2017. Experimental results on\nZeroSpeech 2017 show that both approaches are effective while the latter\nis more prominent, and that their combination brings further marginal\nimprovement in across-speaker condition. Results on ZeroSpeech 2019\nshow that in the ABX discriminability task, our approaches significantly\noutperform the official baseline, and are competitive to or even outperform\nthe official topline. The proposed unit sequence smoothing algorithm\nimproves synthesis quality, at a cost of slight decrease in ABX discriminability.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1337",
      "author_area_id": 13,
      "author_area_label": "Special Sessions"
    },
    "yusuf19_interspeech": {
      "authors": [
        [
          "Bolaji",
          "Yusuf"
        ],
        [
          "Alican",
          "G\u00f6k"
        ],
        [
          "Batuhan",
          "Gundogdu"
        ],
        [
          "Oyku Deniz",
          "Kose"
        ],
        [
          "Murat",
          "Saraclar"
        ]
      ],
      "title": "Temporally-Aware Acoustic Unit Discovery for Zerospeech 2019 Challenge",
      "original": "1430",
      "page_count": 5,
      "order": 229,
      "p1": "1098",
      "pn": "1102",
      "abstract": [
        "Zero-resource speech processing efforts focus on unsupervised discovery\nof sub-word acoustic units. Common approaches work with spatial similarities\nbetween the acoustic frame representations within Bayesian or neural\nnetwork-based frameworks. We propose two methods that utilize the temporal\nproximity information in addition to the acoustic similarity for clustering\nframes into acoustic units. The first approach uses a temporally biased\nself-organizing map (SOM) to discover such units. Since the SOM unit\nindices are correlated with (vector) spatial distance, we pool neighboring\nunits and then train a recurrent neural network to predict each pooled\nunit. The second approach incorporates temporal awareness by training\na recurrent sparse autoencoder, in which unsupervised clustering is\ndone on the intermediate softmax layer. This network is then fine-tuned\nusing aligned pairs of acoustically similar sequences obtained via\nunsupervised term discovery. Our approaches outperform the provided\nbaseline system on two main metrics of the Zerospeech 2019 challenge,\nABX-discriminability and bitrate of the quantized embeddings, both\nfor English and the surprise language. Furthermore, the temporal-awareness\nand the post-filtering techniques adopted in this work resulted in\nan enhanced continuity of the decoding, yielding low bitrates.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1430",
      "author_area_id": 13,
      "author_area_label": "Special Sessions"
    },
    "eloff19_interspeech": {
      "authors": [
        [
          "Ryan",
          "Eloff"
        ],
        [
          "Andr\u00e9",
          "Nortje"
        ],
        [
          "Benjamin van",
          "Niekerk"
        ],
        [
          "Avashna",
          "Govender"
        ],
        [
          "Leanne",
          "Nortje"
        ],
        [
          "Arnu",
          "Pretorius"
        ],
        [
          "Elan van",
          "Biljon"
        ],
        [
          "Ewald van der",
          "Westhuizen"
        ],
        [
          "Lisa van",
          "Staden"
        ],
        [
          "Herman",
          "Kamper"
        ]
      ],
      "title": "Unsupervised Acoustic Unit Discovery for Speech Synthesis Using Discrete Latent-Variable Neural Networks",
      "original": "1518",
      "page_count": 5,
      "order": 230,
      "p1": "1103",
      "pn": "1107",
      "abstract": [
        "For our submission to the ZeroSpeech 2019 challenge, we apply discrete\nlatent-variable neural networks to unlabelled speech and use the discovered\nunits for speech synthesis. Unsupervised discrete subword modelling\ncould be useful for studies of phonetic category learning in infants\nor in low-resource speech technology requiring symbolic input. We use\nan autoencoder (AE) architecture with intermediate discretisation.\nWe decouple acoustic unit discovery from speaker modelling by conditioning\nthe AE&#8217;s decoder on the training speaker identity. At test time,\nunit discovery is performed on speech from an unseen speaker, followed\nby unit decoding conditioned on a known target speaker to obtain reconstructed\nfilterbanks. This output is fed to a neural vocoder to synthesise speech\nin the target speaker&#8217;s voice. For discretisation, categorical\nvariational autoencoders (CatVAEs), vector-quantised VAEs (VQ-VAEs)\nand straight-through estimation are compared at different compression\nlevels on two languages. Our final model uses convolutional encoding,\nVQ-VAE discretisation, deconvolutional decoding and an FFTNet vocoder.\nWe show that decoupled speaker conditioning intrinsically improves\ndiscrete acoustic representations, yielding competitive synthesis quality\ncompared to the challenge baseline.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1518",
      "author_area_id": 13,
      "author_area_label": "Special Sessions"
    },
    "liu19c_interspeech": {
      "authors": [
        [
          "Andy T.",
          "Liu"
        ],
        [
          "Po-chun",
          "Hsu"
        ],
        [
          "Hung-Yi",
          "Lee"
        ]
      ],
      "title": "Unsupervised End-to-End Learning of Discrete Linguistic Units for Voice Conversion",
      "original": "2048",
      "page_count": 5,
      "order": 231,
      "p1": "1108",
      "pn": "1112",
      "abstract": [
        "We present an unsupervised end-to-end training scheme where we discover\ndiscrete subword units from speech without using any labels. The discrete\nsubword units are learned under an ASR-TTS autoencoder reconstruction\nsetting, where an ASR-Encoder is trained to discover a set of common\nlinguistic units given a variety of speakers, and a TTS-Decoder trained\nto project the discovered units back to the designated speech. We propose\na discrete encoding method, Multilabel-Binary Vectors (MBV), to make\nthe ASR-TTS autoencoder differentiable. We found that the proposed\nencoding method offers automatic extraction of speech content from\nspeaker style, and is sufficient to cover full linguistic content in\na given language. Therefore, the TTS-Decoder can synthesize speech\nwith the same content as the input of ASR-Encoder but with different\nspeaker characteristics, which achieves voice conversion (VC). We further\nimprove the quality of VC using adversarial training, where we train\na TTS-Patcher that augments the output of TTS-Decoder. Objective and\nsubjective evaluations show that the proposed approach offers strong\nVC results as it eliminates speaker identity while preserving content\nwithin speech. In the ZeroSpeech 2019 Challenge, we achieved outstanding\nperformance in terms of low bitrate.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2048",
      "author_area_id": 13,
      "author_area_label": "Special Sessions"
    },
    "s19_interspeech": {
      "authors": [
        [
          "Karthik Pandia D.",
          "S."
        ],
        [
          "Hema A.",
          "Murthy"
        ]
      ],
      "title": "Zero Resource Speech Synthesis Using Transcripts Derived from Perceptual Acoustic Units",
      "original": "2336",
      "page_count": 5,
      "order": 232,
      "p1": "1113",
      "pn": "1117",
      "abstract": [
        "Zerospeech synthesis is the task of building vocabulary independent\nspeech synthesis systems, where transcriptions are unavailable for\ntraining data. It is, therefore, necessary to convert training data\ninto a sequence of fundamental acoustic units that can be used for\nsynthesis during the test. This paper attempts to discover, and model\nperceptual acoustic units consisting of steady state, and transient\nregions in speech. The transients roughly correspond to CV, VC units,\nwhile the steady-state corresponds to sonorants and fricatives. The\nspeech signal is first preprocessed by segmenting the same into CVC-like\nunits using a short-term energy-like contour. These CVC segments are\nclustered using a connected components-based graph clustering technique.\nThe clustered CVC segments are initialized such that the onset (CV)\nand decays (VC) correspond to transients, and the rhyme corresponds\nto steady-states. Following this initialization, the units are allowed\nto re-organise on the continuous speech into a final set of AUs in\nan HMM-GMM framework. AU sequences thus obtained are used to train\nsynthesis models. The performance of the proposed approach is evaluated\non the Zerospeech 2019 challenge database. Subjective and objective\nscores show that reasonably good quality synthesis with low bit rate\nencoding can be achieved using the proposed AUs.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2336",
      "author_area_id": 13,
      "author_area_label": "Special Sessions"
    },
    "tjandra19_interspeech": {
      "authors": [
        [
          "Andros",
          "Tjandra"
        ],
        [
          "Berrak",
          "Sisman"
        ],
        [
          "Mingyang",
          "Zhang"
        ],
        [
          "Sakriani",
          "Sakti"
        ],
        [
          "Haizhou",
          "Li"
        ],
        [
          "Satoshi",
          "Nakamura"
        ]
      ],
      "title": "VQVAE Unsupervised Unit Discovery and Multi-Scale Code2Spec Inverter for Zerospeech Challenge 2019",
      "original": "3232",
      "page_count": 5,
      "order": 233,
      "p1": "1118",
      "pn": "1122",
      "abstract": [
        "We describe our submitted system for the ZeroSpeech Challenge 2019.\nThe current challenge theme addresses the difficulty of constructing\na speech synthesizer without any text or phonetic labels and requires\na system that can (1) discover subword units in an unsupervised way,\nand (2) synthesize the speech with a target speaker&#8217;s voice.\nMoreover, the system should also balance the discrimination score ABX,\nthe bit-rate compression rate, and the naturalness and the intelligibility\nof the constructed voice. To tackle these problems and achieve the\nbest trade-off, we utilize a vector quantized variational autoencoder\n(VQ-VAE) and a multi-scale codebook-to-spectrogram (Code2Spec) inverter\ntrained by mean square error and adversarial loss. The VQ-VAE extracts\nthe speech to a latent space, forces itself to map it into the nearest\ncodebook and produces compressed representation. Next, the inverter\ngenerates a magnitude spectrogram to the target voice, given the codebook\nvectors from VQ-VAE. In our experiments, we also investigated several\nother clustering algorithms, including K-Means and GMM, and compared\nthem with the VQ-VAE result on ABX scores and bit rates. Our proposed\napproach significantly improved the intelligibility (in CER), the MOS,\nand discrimination ABX scores compared to the official ZeroSpeech 2019\nbaseline or even the topline.\n"
      ],
      "doi": "10.21437/Interspeech.2019-3232",
      "author_area_id": 13,
      "author_area_label": "Special Sessions"
    },
    "niehues19_interspeech": {
      "authors": [
        [
          "Jan",
          "Niehues"
        ]
      ],
      "title": "Survey Talk: A Survey on Speech Translation",
      "original": "abs9",
      "page_count": 0,
      "order": 234,
      "p1": "0",
      "pn": "",
      "abstract": [
        "We will start with an overview on the different use cases and difficulties\nof speech translation. Due to the wide range of possible application\nthese systems differ in data, difficulty of the language and spontaneous\neffects. Furthermore, the interaction with human has an important influence.\nIn the main part of the talk, we will review state-of-the-art methods\nto build speech translation system. We will start with reviewing the\ntranslation approach of spoken language translation, a cascade of an\nautomatic speech recognition system and a machine translation system.\nWe will highlight the challenges when combining both systems. Especially,\ntechniques to adapt the system to scenario will be reviewed. With the\nsuccess of neural models in both areas, we see a rising research interest\nin end-to-end speech translation. While we see promising results on\nthis approach, international evaluation campaigns like the Shared Task\nof the International Workshop on Spoken Language Translation (IWSLT)\nhave shown that currently often cascaded systems still achieve a better\ntranslation performance. We will highlight the main challenges of end-to-end\nspeech translation. In the final part of the talk, we will review techniques\nthat address key challenges of speech translation, e.g. Latency, spontaneous\neffects, sentence segmentation and stream decoding.\n"
      ]
    },
    "jia19_interspeech": {
      "authors": [
        [
          "Ye",
          "Jia"
        ],
        [
          "Ron J.",
          "Weiss"
        ],
        [
          "Fadi",
          "Biadsy"
        ],
        [
          "Wolfgang",
          "Macherey"
        ],
        [
          "Melvin",
          "Johnson"
        ],
        [
          "Zhifeng",
          "Chen"
        ],
        [
          "Yonghui",
          "Wu"
        ]
      ],
      "title": "Direct Speech-to-Speech Translation with a Sequence-to-Sequence Model",
      "original": "1951",
      "page_count": 5,
      "order": 235,
      "p1": "1123",
      "pn": "1127",
      "abstract": [
        "We present an attention-based sequence-to-sequence neural network which\ncan directly translate speech from one language into speech in another\nlanguage, without relying on an intermediate text representation. The\nnetwork is trained end-to-end, learning to map speech spectrograms\ninto target spectrograms in another language, corresponding to the\ntranslated content (in a different canonical voice). We further demonstrate\nthe ability to synthesize translated speech using the voice of the\nsource speaker. We conduct experiments on two Spanish-to-English speech\ntranslation datasets, and find that the proposed model slightly underperforms\na baseline cascade of a direct speech-to-text translation model and\na text-to-speech synthesis model, demonstrating the feasibility of\nthe approach on this very challenging task.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1951",
      "author_area_id": 12,
      "author_area_label": "Spoken Language Processing: Translation, Information Retrieval, Summarization, Resources and Evaluation"
    },
    "liu19d_interspeech": {
      "authors": [
        [
          "Yuchen",
          "Liu"
        ],
        [
          "Hao",
          "Xiong"
        ],
        [
          "Jiajun",
          "Zhang"
        ],
        [
          "Zhongjun",
          "He"
        ],
        [
          "Hua",
          "Wu"
        ],
        [
          "Haifeng",
          "Wang"
        ],
        [
          "Chengqing",
          "Zong"
        ]
      ],
      "title": "End-to-End Speech Translation with Knowledge Distillation",
      "original": "2582",
      "page_count": 5,
      "order": 236,
      "p1": "1128",
      "pn": "1132",
      "abstract": [
        "End-to-end speech translation (ST), which directly translates from\nsource language speech into target language text, has attracted intensive\nattentions in recent years. Compared to conventional pipeline systems,\nend-to-end ST model has potential benefits of lower latency, smaller\nmodel size and less error propagation. However, it is notoriously difficult\nto implement such model which combines automatic speech recognition\n(ASR) and machine translation (MT) together. In this paper, we propose\na  knowledge distillation approach to improve ST by transferring the\nknowledge from text translation. Specifically, we first train a text\ntranslation model, regarded as the teacher model, and then ST model\nis trained to learn the output probabilities of teacher model through\nknowledge distillation. Experiments on English-French Augmented LibriSpeech\nand English-Chinese TED corpus show that end-to-end ST is possible\nto implement on both similar and dissimilar language pairs. In addition,\nwith the instruction of the teacher model, end-to-end ST model can\ngain significant improvements by over 3.5 BLEU points.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2582",
      "author_area_id": 12,
      "author_area_label": "Spoken Language Processing: Translation, Information Retrieval, Summarization, Resources and Evaluation"
    },
    "gangi19_interspeech": {
      "authors": [
        [
          "Mattia A. Di",
          "Gangi"
        ],
        [
          "Matteo",
          "Negri"
        ],
        [
          "Marco",
          "Turchi"
        ]
      ],
      "title": "Adapting Transformer to End-to-End Spoken Language Translation",
      "original": "3045",
      "page_count": 5,
      "order": 237,
      "p1": "1133",
      "pn": "1137",
      "abstract": [
        "Neural end-to-end architectures for sequence-to-sequence learning represent\nthe state of the art in machine translation (MT) and speech recognition\n(ASR). Their use is also promising for end-to-end spoken language translation\n(SLT), which combines the main challenges of ASR and MT. Exploiting\nexisting neural architectures, however, requires task-specific adaptations.\nA network that has obtained state-of-the-art results in MT with reduced\ntraining time is Transformer. However, its direct application to speech\ninput is hindered by two limitations of the self-attention network\non which it is based: quadratic memory complexity and no explicit modeling\nof short-range dependencies between input features. High memory complexity\nposes constraints to the size of models trainable with a GPU, while\nthe inadequate modeling of local dependencies harms final translation\nquality. This paper presents an adaptation of Transformer to end-to-end\nSLT that consists in:  i) downsampling the input with convolutional\nneural networks to make the training process feasible on GPUs,  ii)\nmodeling the bidimensional nature of a spectrogram, and  iii) adding\na distance penalty to the attention, so to bias it towards local context.\nSLT experiments on 8 language directions show that, with our adaptation,\nTransformer outperforms a strong RNN-based baseline with a significant\nreduction in training time.\n"
      ],
      "doi": "10.21437/Interspeech.2019-3045",
      "author_area_id": 12,
      "author_area_label": "Spoken Language Processing: Translation, Information Retrieval, Summarization, Resources and Evaluation"
    },
    "hillis19_interspeech": {
      "authors": [
        [
          "Steven",
          "Hillis"
        ],
        [
          "Anushree Prasanna",
          "Kumar"
        ],
        [
          "Alan W.",
          "Black"
        ]
      ],
      "title": "Unsupervised Phonetic and Word Level Discovery for Speech to Speech Translation for Unwritten Languages",
      "original": "3026",
      "page_count": 5,
      "order": 238,
      "p1": "1138",
      "pn": "1142",
      "abstract": [
        "We experiment with unsupervised methods for deriving and clustering\nsymbolic representations of speech, working towards speech-to-speech\ntranslation for languages without regular (or any) written representations.\nWe consider five low-resource African languages, and we produce three\ndifferent segmental representations of text data for comparisons against\nfour different segmental representations derived solely from acoustic\ndata for each language. The text and speech data for each language\ncomes from the CMU Wilderness dataset introduced in [1], where speakers\nread a version of the New Testament in their language. Our goal is\nto evaluate the translation performance not only of acoustically derived\nunits but also of discovered sequences or &#8220;words&#8221; made\nfrom these units, with the intuition that such representations will\nencode more meaning than phones alone. We train statistical machine\ntranslation models for each representation and evaluate their outputs\non the basis of BLEU-1 scores to determine their efficacy. Our experiments\nproduce encouraging results: as we cluster our atomic phonetic representations\ninto more word-like units, the amount information retained generally\napproaches that of the actual words themselves.\n"
      ],
      "doi": "10.21437/Interspeech.2019-3026",
      "author_area_id": 12,
      "author_area_label": "Spoken Language Processing: Translation, Information Retrieval, Summarization, Resources and Evaluation"
    },
    "bhattacharya19_interspeech": {
      "authors": [
        [
          "Gautam",
          "Bhattacharya"
        ],
        [
          "Jahangir",
          "Alam"
        ],
        [
          "Patrick",
          "Kenny"
        ]
      ],
      "title": "Deep Speaker Recognition: Modular or Monolithic?",
      "original": "3146",
      "page_count": 5,
      "order": 239,
      "p1": "1143",
      "pn": "1147",
      "abstract": [
        "Speaker recognition has made extraordinary progress with the advent\nof deep neural networks. In this work, we analyze the performance of\nend-to-end deep speaker recognizers on two popular text-independent\ntasks - NIST-SRE 2016 and VoxCeleb. Through a combination of a deep\nconvolutional feature extractor, self-attentive pooling and large-margin\nloss functions, we achieve state-of-the-art performance on VoxCeleb.\nOur best individual and ensemble models show a relative improvement\nof 70% an 82% respectively over the best reported results on this task.<br\nstyle=\"mso-data-placement:same-cell;\" /><br style=\"mso-data-placement:same-cell;\"\n/>\n  On the challenging NIST-SRE 2016 task, our proposed end-to-end\nmodels show good performance but are unable to match a strong i-vector\nbaseline. State-of-the-art systems for this task use a modular framework\nthat combines neural network embeddings with a probabilistic linear\ndiscriminant analysis (PLDA) classifier. Drawing inspiration from this\napproach we propose to replace the PLDA classifier with a neural network.\nOur modular neural network approach is able to outperform the i-vector\nbaseline using cosine distance to score verification trials.\n"
      ],
      "doi": "10.21437/Interspeech.2019-3146",
      "author_area_id": 4,
      "author_area_label": "Speaker and Language Identification"
    },
    "wang19d_interspeech": {
      "authors": [
        [
          "Shuai",
          "Wang"
        ],
        [
          "Johan",
          "Rohdin"
        ],
        [
          "Luk\u00e1\u0161",
          "Burget"
        ],
        [
          "Old\u0159ich",
          "Plchot"
        ],
        [
          "Yanmin",
          "Qian"
        ],
        [
          "Kai",
          "Yu"
        ],
        [
          "Jan",
          "\u010cernock\u00fd"
        ]
      ],
      "title": "On the Usage of Phonetic Information for Text-Independent Speaker Embedding Extraction",
      "original": "3036",
      "page_count": 5,
      "order": 240,
      "p1": "1148",
      "pn": "1152",
      "abstract": [
        "Embeddings extracted by deep neural networks have become the state-of-the-art\nutterance representation in speaker recognition systems. It has recently\nbeen shown that incorporating frame-level phonetic information in the\nembedding extractor can improve the speaker recognition performance.\nOn the other hand, in the final embedding, phonetic information is\njust an additional source of session variability which may be harmful\nto the text-independent speaker recognition task. This suggests that\nat the embedding level phonetic information should be suppressed rather\nthan encouraged. To verify this hypothesis, we perform several experiments\nthat encourage or/and suppress phonetic information at various stages\nin the network. Our experiments confirm that multitask learning is\nbeneficial if it is applied at the frame-level stage of the network,\nwhereas adversarial training is beneficial if it is used at the segment-level\nstage of the network. Additionally, the combination of these two approaches\nimproves the performance further, resulting in an equal error rate\nof 3.17% on the VoxCeleb dataset.\n"
      ],
      "doi": "10.21437/Interspeech.2019-3036",
      "author_area_id": 4,
      "author_area_label": "Speaker and Language Identification"
    },
    "ravanelli19_interspeech": {
      "authors": [
        [
          "Mirco",
          "Ravanelli"
        ],
        [
          "Yoshua",
          "Bengio"
        ]
      ],
      "title": "Learning Speaker Representations with Mutual Information",
      "original": "2380",
      "page_count": 5,
      "order": 241,
      "p1": "1153",
      "pn": "1157",
      "abstract": [
        "Learning good representations is of crucial importance in deep learning.\nMutual Information (MI) or similar measures of statistical dependence\nare promising tools for learning these representations in an unsupervised\nway. Even though the mutual information between two random variables\nis hard to measure directly in high dimensional spaces, some recent\nstudies have shown that an implicit optimization of MI can be achieved\nwith an encoder-discriminator architecture similar to that of Generative\nAdversarial Networks (GANs).<br style=\"mso-data-placement:same-cell;\"\n/><br style=\"mso-data-placement:same-cell;\" />\n  In this work, we learn\nrepresentations that capture speaker identities by maximizing the mutual\ninformation between the encoded representations of chunks of speech\nrandomly sampled from the same sentence. The proposed encoder relies\non the SincNet architecture and transforms raw speech waveform into\na compact feature vector. The discriminator is fed by either positive\nsamples (of the joint distribution of encoded chunks) or negative samples\n(from the product of the marginals) and is trained to separate them.<br\nstyle=\"mso-data-placement:same-cell;\" /><br style=\"mso-data-placement:same-cell;\"\n/>\n  We report experiments showing that this approach effectively learns\nuseful speaker representations, leading to promising results on speaker\nidentification and verification tasks. Our experiments consider both\nunsupervised and semi-supervised settings and compare the performance\nachieved with different objective functions.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2380",
      "author_area_id": 4,
      "author_area_label": "Speaker and Language Identification"
    },
    "you19_interspeech": {
      "authors": [
        [
          "Lanhua",
          "You"
        ],
        [
          "Wu",
          "Guo"
        ],
        [
          "Li-Rong",
          "Dai"
        ],
        [
          "Jun",
          "Du"
        ]
      ],
      "title": "Multi-Task Learning with High-Order Statistics for x-Vector Based Text-Independent Speaker Verification",
      "original": "2264",
      "page_count": 5,
      "order": 242,
      "p1": "1158",
      "pn": "1162",
      "abstract": [
        "The x-vector based deep neural network (DNN) embedding systems have\ndemonstrated effectiveness for text-independent speaker verification.\nThis paper presents a multi-task learning architecture for training\nthe speaker embedding DNN with the primary task of classifying the\ntarget speakers, and the auxiliary task of reconstructing the first-\nand higher-order statistics of the original input utterance. The proposed\ntraining strategy aggregates both the supervised and unsupervised learning\ninto one framework to make the speaker embeddings more discriminative\nand robust. Experiments are carried out using the NIST SRE16 evaluation\ndataset and the VOiCES dataset. The results demonstrate that our proposed\nmethod outperforms the original x-vector approach with very low additional\ncomplexity added.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2264",
      "author_area_id": 4,
      "author_area_label": "Speaker and Language Identification"
    },
    "wu19e_interspeech": {
      "authors": [
        [
          "Zhanghao",
          "Wu"
        ],
        [
          "Shuai",
          "Wang"
        ],
        [
          "Yanmin",
          "Qian"
        ],
        [
          "Kai",
          "Yu"
        ]
      ],
      "title": "Data Augmentation Using Variational Autoencoder for Embedding Based Speaker Verification",
      "original": "2248",
      "page_count": 5,
      "order": 243,
      "p1": "1163",
      "pn": "1167",
      "abstract": [
        "Domain or environment mismatch between training and testing, such as\nvarious noises and channels, is a major challenge for speaker verification.\nIn this paper, a variational autoencoder (VAE) is designed to learn\nthe patterns of speaker embeddings extracted from noisy speech segments,\nincluding i-vector and x-vector, and generate embeddings with more\ndiversity to improve the robustness of speaker verification systems\nwith probabilistic linear discriminant analysis (PLDA) back-end. The\napproach is evaluated on the standard NIST SRE 2016 dataset. Compared\nto manual and generative adversarial network (GAN) based augmentation\napproaches, the proposed VAE based augmentation achieves a slightly\nbetter performance for i-vector on Tagalog and Cantonese with EERs\nof 15.54% and 7.84%, and a more significant improvement for x-vector\non those two languages with EERs of 11.86% and 4.20%.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2248",
      "author_area_id": 4,
      "author_area_label": "Speaker and Language Identification"
    },
    "you19b_interspeech": {
      "authors": [
        [
          "Lanhua",
          "You"
        ],
        [
          "Wu",
          "Guo"
        ],
        [
          "Li-Rong",
          "Dai"
        ],
        [
          "Jun",
          "Du"
        ]
      ],
      "title": "Deep Neural Network Embeddings with Gating Mechanisms for Text-Independent Speaker Verification",
      "original": "1746",
      "page_count": 5,
      "order": 244,
      "p1": "1168",
      "pn": "1172",
      "abstract": [
        "In this paper, gating mechanisms are applied in deep neural network\n(DNN) training for x-vector-based text-independent speaker verification.\nFirst, a gated convolution neural network (GCNN) is employed for modeling\nthe frame-level embedding layers. Compared with the time-delay DNN\n(TDNN), the GCNN can obtain more expressive frame-level representations\nthrough carefully designed memory cell and gating mechanisms. Moreover,\nwe propose a novel gated-attention statistics pooling strategy in which\nthe attention scores are shared with the output gate. The gated-attention\nstatistics pooling combines both gating and attention mechanisms into\none framework; therefore, we can capture more useful information in\nthe temporal pooling layer. Experiments are carried out using the NIST\nSRE16 and SRE18 evaluation datasets. The results demonstrate the effectiveness\nof the GCNN and show that the proposed gated-attention statistics pooling\ncan further improve the performance.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1746",
      "author_area_id": 4,
      "author_area_label": "Speaker and Language Identification"
    },
    "bhat19_interspeech": {
      "authors": [
        [
          "Riyaz",
          "Bhat"
        ],
        [
          "John",
          "Chen"
        ],
        [
          "Rashmi",
          "Prasad"
        ],
        [
          "Srinivas",
          "Bangalore"
        ]
      ],
      "title": "Neural Transition Systems for Modeling Hierarchical Semantic Representations",
      "original": "3075",
      "page_count": 5,
      "order": 245,
      "p1": "1173",
      "pn": "1177",
      "abstract": [
        "While virtual agents are becoming ubiquitous in our daily life, their\nfunctionality is limited to simple commands which involve a single\nintent and an unstructured set of entities. Typically, in such systems,\nthe natural language understanding (NLU) component uses a sequence\ntagging model to extract a flat meaning representation. However, in\norder to support complex user requests with multiple intents with their\nassociated entities, such as those in a product ordering domain, a\nstructured semantic representation is necessary. In this paper, we\npresent hierarchical semantic representations for product ordering\nin the food services domain and two NLU models that produce such representations\nefficiently using deep neural networks. The models are based on transition-based\nalgorithms which have been proven to be effective and scalable for\nmultiple NLP tasks such as syntactic parsing and slot filling. The\nfirst model uses a multitasking architecture containing multiple transition\nsystems with tree constraints to model the hierarchical annotations,\nwhile the second model treats the task as a constituency parsing problem\nby mapping the target domain annotations to a constituency tree. We\ndemonstrate that both multi-task and constituency-based transition\nsystems achieve competitive results and even show improvements over\nsequential models, showing their effectiveness in modeling hierarchical\nstructure.\n"
      ],
      "doi": "10.21437/Interspeech.2019-3075",
      "author_area_id": 11,
      "author_area_label": "Spoken dialog systems and conversational analysis"
    },
    "vukotic19_interspeech": {
      "authors": [
        [
          "Vedran",
          "Vukoti\u0107"
        ],
        [
          "Christian",
          "Raymond"
        ]
      ],
      "title": "Mining Polysemous Triplets with Recurrent Neural Networks for Spoken Language Understanding",
      "original": "2977",
      "page_count": 5,
      "order": 246,
      "p1": "1178",
      "pn": "1182",
      "abstract": [
        "The typical RNN (Recurrent Neural Network) pipeline in SLU (Spoken\nLanguage Understanding), and specifically in the slot-filling task,\nconsists of three stages: word embedding, context window representation,\nand label prediction. Label prediction, as a classification task, is\nthe one that creates a sensible context window representation during\nlearning through back-propagation. However, due to natural variations\nof the data, differences in two same-labeled samples can lead to dissimilar\nrepresentations, whereas similarities in two differently-labeled samples\ncan lead to them having close representations. In computer vision applications,\nspecifically in face recognition and person re-identification, this\nproblem has recently been successfully tackled by introducing data\ntriplets and a triplet loss function.<br style=\"mso-data-placement:same-cell;\"\n/><br style=\"mso-data-placement:same-cell;\" />\n  In SLU, each word\ncan be mapped to one or multiple labels depending on small variations\nof its context. We exploit this fact to construct data triplets consisting\nof the same words with different contexts that form a pair of datapoints\nwith matching target labels and an another pair with non-matching labels.\nBy using these triplets and an additional loss function, we update\nthe context window representation in order to improve it, make dissimilar\nsamples more distant and similar samples closer, leading to better\nclassification results and an improved rate of convergence.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2977",
      "author_area_id": 11,
      "author_area_label": "Spoken dialog systems and conversational analysis"
    },
    "ray19_interspeech": {
      "authors": [
        [
          "Avik",
          "Ray"
        ],
        [
          "Yilin",
          "Shen"
        ],
        [
          "Hongxia",
          "Jin"
        ]
      ],
      "title": "Iterative Delexicalization for Improved Spoken Language Understanding",
      "original": "2955",
      "page_count": 5,
      "order": 247,
      "p1": "1183",
      "pn": "1187",
      "abstract": [
        "Recurrent neural network (RNN) based joint intent classification and\nslot tagging models have achieved tremendous success in recent years\nfor building spoken language understanding and dialog systems. However,\nthese models suffer from poor performance for slots which often encounter\nlarge semantic variability in slot values after deployment (e.g. message\ntexts, partial movie/artist names). While greedy delexicalization of\nslots in the input utterance via substring matching can partly improve\nperformance, it often produces incorrect input. Moreover, such techniques\ncannot delexicalize slots with out-of-vocabulary slot values not seen\nat training. In this paper, we propose a novel iterative delexicalization\nalgorithm, which can accurately delexicalize the input, even with out-of-vocabulary\nslot values. Based on model confidence of the current delexicalized\ninput, our algorithm improves delexicalization in every iteration to\nconverge to the best input having the highest confidence. We show on\nbenchmark and in-house datasets that our algorithm can greatly improve\nparsing performance for RNN based models, especially for out-of-distribution\nslot values.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2955",
      "author_area_id": 11,
      "author_area_label": "Spoken dialog systems and conversational analysis"
    },
    "bhosale19_interspeech": {
      "authors": [
        [
          "Swapnil",
          "Bhosale"
        ],
        [
          "Imran",
          "Sheikh"
        ],
        [
          "Sri Harsha",
          "Dumpala"
        ],
        [
          "Sunil Kumar",
          "Kopparapu"
        ]
      ],
      "title": "End-to-End Spoken Language Understanding: Bootstrapping in Low Resource Scenarios",
      "original": "2366",
      "page_count": 5,
      "order": 248,
      "p1": "1188",
      "pn": "1192",
      "abstract": [
        "End-to-end Spoken Language Understanding (SLU) systems, without speech-to-text\nconversion, are more promising in low resource scenarios. They can\nbe more effective when there is not enough labeled data to train reliable\nspeech recognition and language understanding systems, or where running\nSLU on edge is preferred over cloud based services. In this paper,\nwe present an approach for bootstrapping end-to-end SLU in low resource\nscenarios. We show that incorporating layers extracted from pre-trained\nacoustic models, instead of using the typical Mel filter bank features,\nlead to better performing SLU models. Moreover, the layers extracted\nfrom a model pre-trained on one language perform well even for (a)\nSLU tasks on a different language and also (b) on utterances from speakers\nwith speech disorder.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2366",
      "author_area_id": 11,
      "author_area_label": "Spoken dialog systems and conversational analysis"
    },
    "takatsu19_interspeech": {
      "authors": [
        [
          "Hiroaki",
          "Takatsu"
        ],
        [
          "Katsuya",
          "Yokoyama"
        ],
        [
          "Yoichi",
          "Matsuyama"
        ],
        [
          "Hiroshi",
          "Honda"
        ],
        [
          "Shinya",
          "Fujie"
        ],
        [
          "Tetsunori",
          "Kobayashi"
        ]
      ],
      "title": "Recognition of Intentions of Users&#8217; Short Responses for Conversational News Delivery System",
      "original": "2121",
      "page_count": 5,
      "order": 249,
      "p1": "1193",
      "pn": "1197",
      "abstract": [
        "In human-human conversations, listeners often convey intentions to\ntheir speakers through feedbacks comprising reflexive short responses.\nThe speakers then recognize these intentions and dynamically change\nthe conversational plans to transmit information more efficiently.\nFor the design of spoken dialogue systems that deliver a massive amount\nof information, such as news, it is essential to accurately capture\nusers&#8217; intentions from reflexive short responses to efficiently\nselect or eliminate the information to be transmitted depending on\nthe user&#8217;s needs. However, such short responses from users are\nnormally too short to recognize their actual intentions only from the\nprosodic and linguistic features of their short responses. In this\npaper, we propose a user&#8217;s short-response intention-recognition\nmodel that accounts for the previous system&#8217;s utterances as the\ncontext of the conversation in addition to prosodic and linguistic\nfeatures of user&#8217;s utterances. To achieve this, we define types\nof short response intentions in terms of effective information transmission\nand created new dataset by annotating over the interaction data collected\nusing our spoken dialogue system. Our experimental results demonstrate\nthat the classification accuracy can be improved using the linguistic\nfeatures of the system&#8217;s previous utterances encoded by Bidirectional\nEncoder Representations from Transformers (BERT) as the conversational\ncontext.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2121",
      "author_area_id": 11,
      "author_area_label": "Spoken dialog systems and conversational analysis"
    },
    "caubriere19_interspeech": {
      "authors": [
        [
          "Antoine",
          "Caubri\u00e8re"
        ],
        [
          "Natalia",
          "Tomashenko"
        ],
        [
          "Antoine",
          "Laurent"
        ],
        [
          "Emmanuel",
          "Morin"
        ],
        [
          "Nathalie",
          "Camelin"
        ],
        [
          "Yannick",
          "Est\u00e8ve"
        ]
      ],
      "title": "Curriculum-Based Transfer Learning for an Effective End-to-End Spoken Language Understanding and Domain Portability",
      "original": "1832",
      "page_count": 5,
      "order": 250,
      "p1": "1198",
      "pn": "1202",
      "abstract": [
        "We present an end-to-end approach to extract semantic concepts directly\nfrom the speech audio signal. To overcome the lack of data available\nfor this spoken language understanding approach, we investigate the\nuse of a transfer learning strategy based on the principles of curriculum\nlearning. This approach allows us to exploit out-of-domain data that\ncan help to prepare a fully neural architecture. Experiments are carried\nout on the French MEDIA and PORTMEDIA corpora and show that this end-to-end\nSLU approach reaches the best results ever published on this task.\nWe compare our approach to a classical pipeline approach that uses\nASR, POS tagging, lemmatizer, chunker &#8230; and other NLP tools that\naim to enrich ASR outputs that feed an SLU text to concepts system.\nLast, we explore the promising capacity of our end-to-end SLU approach\nto address the problem of domain portability.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1832",
      "author_area_id": 11,
      "author_area_label": "Spoken dialog systems and conversational analysis"
    },
    "dash19b_interspeech": {
      "authors": [
        [
          "Debadatta",
          "Dash"
        ],
        [
          "Paul",
          "Ferrari"
        ],
        [
          "Jun",
          "Wang"
        ]
      ],
      "title": "Spatial and Spectral Fingerprint in the Brain: Speaker Identification from Single Trial MEG Signals",
      "original": "3105",
      "page_count": 5,
      "order": 251,
      "p1": "1203",
      "pn": "1207",
      "abstract": [
        "Brain activity signals are unique subject-specific biological features\nthat can not be forged or stolen. Recognizing this inherent trait,\nbrain waves are recently being acknowledged as a far more secure, sensitive,\nand confidential biometric approach for user identification. Yet, current\nelectroencephalography (EEG) based biometric systems are still in infancy\nconsidering their requirement of a large number of sensors and lower\nrecognition performance compared to present biometric modalities. In\nthis study, we investigated the spatial and spectral fingerprints in\nthe brain with magnetoencephalography (MEG) for speaker identification\nduring rest (pre-stimuli) and speech production. Experimental results\nsuggested that the frontal and the temporal regions of the brain and\nhigher frequency (gamma and high gamma) neural oscillations are more\ndominating for speaker identification. Moreover, we also found that\ntwo optimally located MEG sensors are sufficient to obtain a high speaker\nclassification accuracy during speech tasks whereas at least eight\noptimally located sensors are needed to accurately identify these subjects\nduring rest-state (pre-stimuli). These results indicated the unique\nneural traits of speech production across speakers.\n"
      ],
      "doi": "10.21437/Interspeech.2019-3105",
      "author_area_id": 1,
      "author_area_label": "Speech Perception, Production and Acquisition"
    },
    "nijveld19_interspeech": {
      "authors": [
        [
          "Annika",
          "Nijveld"
        ],
        [
          "L. ten",
          "Bosch"
        ],
        [
          "Mirjam",
          "Ernestus"
        ]
      ],
      "title": "ERP Signal Analysis with Temporal Resolution Using a Time Window Bank",
      "original": "2729",
      "page_count": 5,
      "order": 252,
      "p1": "1208",
      "pn": "1212",
      "abstract": [
        "In order to study the cognitive processes underlying speech comprehension,\nneuro-physiological measures (e.g., EEG and MEG), or behavioural measures\n(e.g., reaction times and response accuracy) can be applied. Compared\nto behavioural measures, EEG signals can provide a more fine-grained\nand complementary view of the processes that take place during the\nunfolding of an auditory stimulus.<br style=\"mso-data-placement:same-cell;\"\n/><br style=\"mso-data-placement:same-cell;\" />\n  EEG signals are often\nanalysed after having chosen specific time windows, which are usually\nbased on the temporal structure of ERP components expected to be sensitive\nto the experimental manipulation. However, as the timing of ERP components\nmay vary between experiments, trials, and participants, such a-priori\ndefined analysis time windows may significantly hamper the exploratory\npower of the analysis of components of interest. In this paper, we\nexplore a wide-window analysis method applied to EEG signals collected\nin an auditory repetition priming experiment.<br style=\"mso-data-placement:same-cell;\"\n/><br style=\"mso-data-placement:same-cell;\" />\n  This approach is based\non a bank of temporal filters arranged along the time axis in combination\nwith linear mixed effects modelling. Crucially, it permits a temporal\ndecomposition of effects in a single comprehensive statistical model\nwhich captures the entire EEG trace.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2729",
      "author_area_id": 1,
      "author_area_label": "Speech Perception, Production and Acquisition"
    },
    "bosch19_interspeech": {
      "authors": [
        [
          "L. ten",
          "Bosch"
        ],
        [
          "K.",
          "Mulder"
        ],
        [
          "L.",
          "Boves"
        ]
      ],
      "title": "Phase Synchronization Between EEG Signals as a Function of Differences Between Stimuli Characteristics",
      "original": "2443",
      "page_count": 5,
      "order": 253,
      "p1": "1213",
      "pn": "1217",
      "abstract": [
        "The neural processing of speech leads to specific patterns in the brain\nwhich can be measured as, e.g., EEG signals. When properly aligned\nwith the speech input and averaged over many tokens, the Event Related\nPotential (ERP) signal is able to differentiate specific contrasts\nbetween speech signals. Well-known effects relate to the difference\nbetween expected and unexpected words, in particular in the N400, while\neffects in N100 and P200 are related to attention and acoustic onset\neffects. Most EEG studies deal with the amplitude of EEG signals over\ntime, sidestepping the effect of phase and phase synchronization. This\npaper investigates the relation between phase in the EEG signals measured\nin an auditory lexical decision task by Dutch participants listening\nto full and reduced English word forms. We show that phase synchronization\ntakes place across stimulus conditions, and that the so-called circular\nvariance is narrowly related to the type of contrast between stimuli.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2443",
      "author_area_id": 1,
      "author_area_label": "Speech Perception, Production and Acquisition"
    },
    "kharaman19_interspeech": {
      "authors": [
        [
          "Mariya",
          "Kharaman"
        ],
        [
          "Manluolan",
          "Xu"
        ],
        [
          "Carsten",
          "Eulitz"
        ],
        [
          "Bettina",
          "Braun"
        ]
      ],
      "title": "The Processing of Prosodic Cues to Rhetorical Question Interpretation: Psycholinguistic and Neurolinguistics Evidence",
      "original": "2528",
      "page_count": 5,
      "order": 254,
      "p1": "1218",
      "pn": "1222",
      "abstract": [
        "In many languages, rhetorical questions (RQs) are produced with different\nprosodic realizations than string-identical information-seeking questions\n(ISQs). RQs typically have longer constituent durations and breathier\nvoice quality than ISQs and differ in nuclear accent type. This paper\nreports on an identification experiment (Experiment 1) and an EEG experiment\n(Experiment 2) on German  wh-questions. In the identification experiment,\nwe manipulated nuclear pitch accent type, voice quality and constituent\nduration and participants indicated whether they judged the realization\nas ISQ or RQ. The results showed additive effects of the three factors,\nwith pitch accent as strongest predictor. In the EEG experiment, participants\nheard the stimuli in two contexts, triggering an ISQ or RQ (blocked).\nWe manipulated pitch accent type and voice quality, resulting in RQ-coherent\nand ISQ-coherent stimuli, based on the outcome of Experiment 1. Results\nshowed a prosodic expectancy positivity (PEP) for prosodic realizations\nthat were incoherent with ISQ-contexts with an onset of &#126;120ms\nafter the onset of the word with nuclear accent. This effect might\nreflect the emotional prosodic aspect of RQs. Taken together, participants\nuse prosody to resolve the ambiguity and event-related potentials (ERPs)\nreact to prosodic realizations that do not match contextually triggered\nexpectations.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2528",
      "author_area_id": 1,
      "author_area_label": "Speech Perception, Production and Acquisition"
    },
    "scharenborg19_interspeech": {
      "authors": [
        [
          "Odette",
          "Scharenborg"
        ],
        [
          "Jiska",
          "Koemans"
        ],
        [
          "Cybelle",
          "Smith"
        ],
        [
          "Mark A.",
          "Hasegawa-Johnson"
        ],
        [
          "Kara D.",
          "Federmeier"
        ]
      ],
      "title": "The Neural Correlates Underlying Lexically-Guided Perceptual Learning",
      "original": "2328",
      "page_count": 5,
      "order": 255,
      "p1": "1223",
      "pn": "1227",
      "abstract": [
        "There is ample evidence showing that listeners are able to quickly\nadapt their phoneme classes to ambiguous sounds using a process called\nlexically-guided perceptual learning. This paper presents the first\nattempt to examine the neural correlates underlying this process. Specifically,\nwe compared the brain&#8217;s responses to ambiguous [f/s] sounds in\nDutch non-native listeners of English (N=36) before and after exposure\nto the ambiguous sound to induce learning, using Event-Related Potentials\n(ERPs). We identified a group of participants who showed lexically-guided\nperceptual learning in their phonetic categorization behavior as observed\nby a significant difference in /s/ responses between pretest and posttest\nand a group who did not. Moreover, we observed differences in mean\nERP amplitude to ambiguous phonemes at pretest and posttest, shown\nby a reliable reduction in amplitude of a positivity over medial central\nchannels from 250 to 550 ms. However, we observed no significant correlation\nbetween the size of behavioral and neural pre/posttest effects. Possibly,\nthe observed behavioral and ERP differences between pretest and posttest\nlink to different aspects of the sound classification task. In follow-up\nresearch, these differences will be further investigated by assessing\ntheir relationship to neural responses to the ambiguous sounds in the\nexposure phase.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2328",
      "author_area_id": 1,
      "author_area_label": "Speech Perception, Production and Acquisition"
    },
    "parmonangan19_interspeech": {
      "authors": [
        [
          "Ivan Halim",
          "Parmonangan"
        ],
        [
          "Hiroki",
          "Tanaka"
        ],
        [
          "Sakriani",
          "Sakti"
        ],
        [
          "Shinnosuke",
          "Takamichi"
        ],
        [
          "Satoshi",
          "Nakamura"
        ]
      ],
      "title": "Speech Quality Evaluation of Synthesized Japanese Speech Using EEG",
      "original": "2059",
      "page_count": 5,
      "order": 256,
      "p1": "1228",
      "pn": "1232",
      "abstract": [
        "As synthesized speech technology becomes more widely used, the synthesized\nspeech quality must be assessed to ensure that it is acceptable. Subjective\nevaluation metrics, such as mean opinion score (MOS), can only provide\nan overall impression without any further detailed information about\nthe speech. Therefore, this study proposes predicting speech quality\nusing electroencephalographs (EEG), which are more objective and have\nhigh temporal resolution. In this paper, we use one natural speech\nand four types of synthesized speech lasting two to six seconds. First,\nto obtain ground truth of MOS, we gathered ten subjects to give opinion\nscore on a scale of one to five for each recording. Second, another\nnine subjects were asked to measure how close to natural speech each\nsynthesized speech sounded. The subjects&#8217; EEGs were recorded\nwhile they were listening to and evaluating the listened speech. The\nbest accuracy achieved for classification was 96.61% using support\nvector machine, 80.36% using linear discriminant analysis, and 59.9%\nusing logistic regression. For regression, we achieved root mean squared\nerror as low as 1.133 using SVR and 1.353 using linear regression.\nThis study demonstrates that EEG could be used to evaluate the perceived\nspeech quality objectively.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2059",
      "author_area_id": 1,
      "author_area_label": "Speech Perception, Production and Acquisition"
    },
    "huang19d_interspeech": {
      "authors": [
        [
          "Yiteng",
          "Huang"
        ],
        [
          "Turaj Z.",
          "Shabestary"
        ],
        [
          "Alexander",
          "Gruenstein"
        ],
        [
          "Li",
          "Wan"
        ]
      ],
      "title": "Multi-Microphone Adaptive Noise Cancellation for Robust Hotword Detection",
      "original": "3006",
      "page_count": 5,
      "order": 257,
      "p1": "1233",
      "pn": "1237",
      "abstract": [
        "Recently we proposed a dual-microphone adaptive noise cancellation\n(ANC) algorithm with deferred filter coefficients for robust hotword\ndetection in [1]. It exploits two unique hotword-related features:\nhotwords are the leading phrase of valid voice queries and they are\nshort. These features allow us  not to compute a speech-noise mask\nthat is a common prerequisite for many multichannel speech enhancement\napproaches. This novel idea was found effective against strong and\nambiguous speech-like TV noise. In this paper, we show that it can\nbe generalized to support more than two microphones. The development\nis validated using re-recorded data with background TV noise from a\n3-mic array. By adding one more microphone, the false reject (FR) rate\ncan be further reduced relatively by 33.5%.\n"
      ],
      "doi": "10.21437/Interspeech.2019-3006",
      "author_area_id": 8,
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "zhao19c_interspeech": {
      "authors": [
        [
          "Shengkui",
          "Zhao"
        ],
        [
          "Chongjia",
          "Ni"
        ],
        [
          "Rong",
          "Tong"
        ],
        [
          "Bin",
          "Ma"
        ]
      ],
      "title": "Multi-Task Multi-Network Joint-Learning of Deep Residual Networks and Cycle-Consistency Generative Adversarial Networks for Robust Speech Recognition",
      "original": "2078",
      "page_count": 5,
      "order": 258,
      "p1": "1238",
      "pn": "1242",
      "abstract": [
        "Robustness of automatic speech recognition (ASR) systems is a critical\nissue due to noise and reverberations. Speech enhancement and model\nadaptation have been studied for long time to address this issue. Recently,\nthe developments of multi-task joint-learning scheme that addresses\nnoise reduction and ASR criteria in a unified modeling framework show\npromising improvements, but the model training highly relies on paired\nclean-noisy data. To overcome this limit, the generative adversarial\nnetworks (GANs) and the adversarial training method are deployed, which\nhave greatly simplified the model training process without the requirements\nof complex front-end design and paired training data. Despite the fast\ndevelopments of GANs for computer visions, only regular GANs have been\nadopted for robust ASR. In this work, we adopt a more advanced cycle-consistency\nGAN (CycleGAN) to address the training failure problem due to mode\ncollapse of regular GANs. Using deep residual networks (ResNets), we\nfurther expand the multi-task scheme to a multi-task multi-network\njoint-learning scheme for more robust noise reduction and model adaptation.\nExperiment results on CHiME-4 show that our proposed approach significantly\nimproves the noise robustness of the ASR system by achieving much lower\nword error rates (WERs) than the state-of-the-art joint-learning approaches.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2078",
      "author_area_id": 8,
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "khokhlov19_interspeech": {
      "authors": [
        [
          "Yuri",
          "Khokhlov"
        ],
        [
          "Alexander",
          "Zatvornitskiy"
        ],
        [
          "Ivan",
          "Medennikov"
        ],
        [
          "Ivan",
          "Sorokin"
        ],
        [
          "Tatiana",
          "Prisyach"
        ],
        [
          "Aleksei",
          "Romanenko"
        ],
        [
          "Anton",
          "Mitrofanov"
        ],
        [
          "Vladimir",
          "Bataev"
        ],
        [
          "Andrei",
          "Andrusenko"
        ],
        [
          "Mariya",
          "Korenevskaya"
        ],
        [
          "Oleg",
          "Petrov"
        ]
      ],
      "title": "R-Vectors: New Technique for Adaptation to Room Acoustics",
      "original": "2645",
      "page_count": 5,
      "order": 259,
      "p1": "1243",
      "pn": "1247",
      "abstract": [
        "Distant speech recognition is an important problem which is far from\nbeing solved. Reverberation and noise are in the list of main problems\nin this area. The most popular methods of dealing with them are data\naugmentation and speech enhancement. In this paper, we propose a novel\napproach, inspired by modern methods of speaker adaptation.<br style=\"mso-data-placement:same-cell;\"\n/><br style=\"mso-data-placement:same-cell;\" />\n  First of all, a feed-forward\nnetwork is trained to classify room impulse responses (RIRs) from speech\nrecordings. Then this network is used for extracting embeddings, which\nwe call R-vectors. These R-vectors are appended to input features of\nthe acoustic model. Due to the lack of labeled data for RIRs classification\ntask, we propose a self-supervised method of training the network,\nwhich consists of using artificial audio generated by room simulator.<br\nstyle=\"mso-data-placement:same-cell;\" /><br style=\"mso-data-placement:same-cell;\"\n/>\n  Experimental evaluation was conducted on VOiCES19 and AMI single-channel\ntasks as well as CHiME5 multi-channel task. It is shown that the R-vector-adapted\nASR systems achieve up to 14% relative WER reduction. Furthermore,\nit is additive with gains from state-of-the-art dereverberation (WPE)\nand speaker adaptation (x-vector) techniques.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2645",
      "author_area_id": 8,
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "kanda19b_interspeech": {
      "authors": [
        [
          "Naoyuki",
          "Kanda"
        ],
        [
          "Christoph",
          "Boeddeker"
        ],
        [
          "Jens",
          "Heitkaemper"
        ],
        [
          "Yusuke",
          "Fujita"
        ],
        [
          "Shota",
          "Horiguchi"
        ],
        [
          "Kenji",
          "Nagamatsu"
        ],
        [
          "Reinhold",
          "Haeb-Umbach"
        ]
      ],
      "title": "Guided Source Separation Meets a Strong ASR Backend: Hitachi/Paderborn University Joint Investigation for Dinner Party ASR",
      "original": "1167",
      "page_count": 5,
      "order": 260,
      "p1": "1248",
      "pn": "1252",
      "abstract": [
        "In this paper, we present Hitachi and Paderborn University&#8217;s\njoint effort for automatic speech recognition (ASR) in a dinner party\nscenario. The main challenges of ASR systems for dinner party recordings\nobtained by multiple microphone arrays are (1) heavy speech overlaps,\n(2) severe noise and reverberation, (3) very natural conversational\ncontent, and possibly (4) insufficient training data. As an example\nof a dinner party scenario, we have chosen the data presented during\nthe CHiME-5 speech recognition challenge, where the baseline ASR had\na 73.3% word error rate (WER), and even the best performing system\nat the CHiME-5 challenge had a 46.1% WER. We extensively investigated\na combination of the guided source separation-based speech enhancement\ntechnique and an already proposed strong ASR backend and found that\na tight combination of these techniques provided substantial accuracy\nimprovements. Our final system achieved WERs of 39.94% and 41.64% for\nthe development and evaluation data, respectively, both of which are\nthe best published results for the dataset. We also investigated with\nadditional training data on the official small data in the CHiME-5\ncorpus to assess the intrinsic difficulty of this ASR task.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1167",
      "author_area_id": 8,
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "drude19_interspeech": {
      "authors": [
        [
          "Lukas",
          "Drude"
        ],
        [
          "Jahn",
          "Heymann"
        ],
        [
          "Reinhold",
          "Haeb-Umbach"
        ]
      ],
      "title": "Unsupervised Training of Neural Mask-Based Beamforming",
      "original": "2549",
      "page_count": 5,
      "order": 261,
      "p1": "1253",
      "pn": "1257",
      "abstract": [
        "We present an unsupervised training approach for a neural network-based\nmask estimator in an acoustic beamforming application. The network\nis trained to maximize a likelihood criterion derived from a spatial\nmixture model of the observations. It is trained from scratch without\nrequiring any parallel data consisting of degraded input and clean\ntraining targets. Thus, training can be carried out on real recordings\nof noisy speech rather than simulated ones. In contrast to previous\nwork on unsupervised training of neural mask estimators, our approach\navoids the need for a possibly pre-trained teacher model entirely.\nWe demonstrate the effectiveness of our approach by speech recognition\nexperiments on two different datasets: one mainly deteriorated by noise\n(CHiME 4) and one by reverberation (REVERB). The results show that\nthe performance of the proposed system is on par with a supervised\nsystem using oracle target masks for training and with a system trained\nusing a model-based teacher.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2549",
      "author_area_id": 8,
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "ma19_interspeech": {
      "authors": [
        [
          "Feng",
          "Ma"
        ],
        [
          "Li",
          "Chai"
        ],
        [
          "Jun",
          "Du"
        ],
        [
          "Diyuan",
          "Liu"
        ],
        [
          "Zhongfu",
          "Ye"
        ],
        [
          "Chin-Hui",
          "Lee"
        ]
      ],
      "title": "Acoustic Model Ensembling Using Effective Data Augmentation for CHiME-5 Challenge",
      "original": "2601",
      "page_count": 5,
      "order": 262,
      "p1": "1258",
      "pn": "1262",
      "abstract": [
        "CHiME-5 is a research community challenge targeting the problem of\nfar-field and multi-talker conversational speech recognition in dinner\nparty scenarios involving background noises, reverberations and overlapping\nspeech. In this study, we present five different kinds of robust acoustic\nmodels which take advantages from both effective data augmentation\nand ensemble methods to improve the recognition performance for the\nCHiME-5 challenge. First, we detail the effective data augmentation\nfor far-field scenarios, especially the far-field data simulation.\nDifferent from the conventional data simulation methods, we use a signal\nprocessing method originally developed for channel identification to\nestimate the room impulse responses and then simulate the far-field\ndata. Second, we introduce the five different kinds of robust acoustic\nmodels. Finally, the effectiveness of our acoustic model ensembling\nstrategies at the lattice level and the state posterior level are evaluated\nand demonstrated. Our system achieves the best performance of all four\ntasks among submitted systems in the CHiME-5 challenge.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2601",
      "author_area_id": 8,
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "li19d_interspeech": {
      "authors": [
        [
          "Ming",
          "Li"
        ],
        [
          "Weicheng",
          "Cai"
        ],
        [
          "Danwei",
          "Cai"
        ]
      ],
      "title": "Survey Talk: End-to-End Deep Neural Network Based Speaker and Language Recognition",
      "original": "abs10",
      "page_count": 0,
      "order": 263,
      "p1": "0",
      "pn": "",
      "abstract": [
        "Speech signal not only contains lexicon information, but also delivers\nvarious kinds of paralinguistic speech attribute information, such\nas speaker, language, gender, age, emotion, etc. The core technique\nquestion behind it is utterance level supervised learning based on\ntext independent or text dependent speech signal with flexible duration.\nIn section 1, we will first formulate the problem of speaker and language\nrecognition. In section 2, we introduce the traditional framework with\ndifferent modules in a pipeline, namely, feature extraction, representation,\nvariability compensation and backend classification. Then we naturally\nintroduce the end-to-end idea and compare with the traditional framework.\nWe will show the correspondence between feature extraction and CNN\nlayers, representation and encoding layer, backend modeling and fully\nconnected layers. Specifically, we will introduce the modules in the\nend-to-end frameworks with more details here, e.g. variable length\ndata loader, frontend convolutional network structure design, encoding\n(or pooling) layer design, loss function design, data augmentation\ndesign, transfer learning and multitask learning, etc. In section 4,\nwe will introduce some robust methods using the end-to-end framework\nfor far-field and noisy conditions. Finally, we will connect the introduced\nend-to-end frameworks to other related tasks, e.g. speaker diarization,\nparalinguistic speech attribute recognition, anti-spoofing countermeasures,\netc.\n"
      ]
    },
    "padi19_interspeech": {
      "authors": [
        [
          "Bharat",
          "Padi"
        ],
        [
          "Anand",
          "Mohan"
        ],
        [
          "Sriram",
          "Ganapathy"
        ]
      ],
      "title": "Attention Based Hybrid i-Vector BLSTM Model for Language Recognition",
      "original": "2371",
      "page_count": 5,
      "order": 264,
      "p1": "1263",
      "pn": "1267",
      "abstract": [
        "In this paper, a hybrid i-vector neural network framework (i-BLSTM)\nwhich models the sequence information present in a series of short\nsegment i-vectors for the task of spoken language recognition (LRE)\nis proposed. A sequence of short segment i-vectors are extracted for\nevery speech utterance and are then modeled using a bidirectional long\nshort-term memory (BLSTM) recurrent neural network (RNN). Attention\nmechanism inside the neural network relevantly weights segments of\nthe speech utterance and the model learns to give higher weights to\nparts of speech data which are more helpful to the classification task.\nThe proposed framework performs better in short duration and noisy\nenvironments when compared with the conventional i-vector system. Experiments\nare performed on clean, noisy and multi-speaker speech data from NIST\nLRE 2017 and RATS language recognition corpus. In these experiments,\nthe proposed approach yields significant improvements (relative improvements\nof 7.6&#8211;13% in terms of accuracy for noisy conditions) over the\nconventional i-vector based language recognition approach and also\nover an end-to-end LSTM-RNN based approach.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2371",
      "author_area_id": 4,
      "author_area_label": "Speaker and Language Identification"
    },
    "jung19b_interspeech": {
      "authors": [
        [
          "Jee-weon",
          "Jung"
        ],
        [
          "Hee-Soo",
          "Heo"
        ],
        [
          "Ju-ho",
          "Kim"
        ],
        [
          "Hye-jin",
          "Shim"
        ],
        [
          "Ha-Jin",
          "Yu"
        ]
      ],
      "title": "RawNet: Advanced End-to-End Deep Neural Network Using Raw Waveforms for Text-Independent Speaker Verification",
      "original": "1982",
      "page_count": 5,
      "order": 265,
      "p1": "1268",
      "pn": "1272",
      "abstract": [
        "Recently, direct modeling of raw waveforms using deep neural networks\nhas been widely studied for a number of tasks in audio domains. In\nspeaker verification, however, utilization of raw waveforms is in its\npreliminary phase, requiring further investigation. In this study,\nwe explore end-to-end deep neural networks that input raw waveforms\nto improve various aspects: front-end speaker embedding extraction\nincluding model architecture, pre-training scheme, additional objective\nfunctions, and back-end classification. Adjustment of model architecture\nusing a pre-training scheme can extract speaker embeddings, giving\na significant improvement in performance. Additional objective functions\nsimplify the process of extracting speaker embeddings by merging conventional\ntwo-phase processes: extracting utterance-level features such as i-vectors\nor x-vectors and the feature enhancement phase, e.g., linear discriminant\nanalysis. Effective back-end classification models that suit the proposed\nspeaker embedding are also explored. We propose an end-to-end system\nthat comprises two deep neural networks, one frontend for utterance-level\nspeaker embedding extraction and the other for back-end classification.\nExperiments conducted on the VoxCeleb1 dataset demonstrate that the\nproposed model achieves state-of-the-art performance among systems\nwithout data augmentation. The proposed system is also comparable to\nthe state-of-the-art x-vector system that adopts heavy data augmentation.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1982",
      "author_area_id": 4,
      "author_area_label": "Speaker and Language Identification"
    },
    "rao19_interspeech": {
      "authors": [
        [
          "Wei",
          "Rao"
        ],
        [
          "Chenglin",
          "Xu"
        ],
        [
          "Eng Siong",
          "Chng"
        ],
        [
          "Haizhou",
          "Li"
        ]
      ],
      "title": "Target Speaker Extraction for Multi-Talker Speaker Verification",
      "original": "1410",
      "page_count": 5,
      "order": 266,
      "p1": "1273",
      "pn": "1277",
      "abstract": [
        "The performance of speaker verification degrades significantly when\nthe test speech is corrupted by interference from non-target speakers.\nSpeaker diarization separates speakers well only if the speakers are\nnot overlapped. However, if multiple talkers speak at the same time,\nwe need a technique to separate the speech in the spectral domain.\nIn this paper, we study a way to extract the target speaker&#8217;s\nspeech from an overlapped multi-talker speech. Specifically, given\nsome reference speech samples from the target speaker, the target speaker&#8217;s\nspeech is firstly extracted from the overlapped multi-talker speech,\nthen the extracted speech is processed in the speaker verification\nsystem. Experimental results show that the proposed approach significantly\nimproves the performance of overlapped multi-talker speaker verification\nand achieves 64.4% relative EER reduction over the zero-effort baseline.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1410",
      "author_area_id": 4,
      "author_area_label": "Speaker and Language Identification"
    },
    "mazzawi19_interspeech": {
      "authors": [
        [
          "Hanna",
          "Mazzawi"
        ],
        [
          "Xavi",
          "Gonzalvo"
        ],
        [
          "Aleks",
          "Kracun"
        ],
        [
          "Prashant",
          "Sridhar"
        ],
        [
          "Niranjan",
          "Subrahmanya"
        ],
        [
          "Ignacio Lopez",
          "Moreno"
        ],
        [
          "Hyun Jin",
          "Park"
        ],
        [
          "Patrick",
          "Violette"
        ]
      ],
      "title": "Improving Keyword Spotting and Language Identification via Neural Architecture Search at Scale",
      "original": "1916",
      "page_count": 5,
      "order": 267,
      "p1": "1278",
      "pn": "1282",
      "abstract": [
        "In this paper we present a novel Neural Architecture Search (NAS) framework\nto improve keyword spotting and spoken language identification models.\nEven with the huge success of deep neural networks (DNNs) in many different\ndomains, finding the best network architecture is still a laborious\ntask and very computationally expensive at best with existing searching\napproaches. Our search approach efficiently and robustly finds better\nmodel sequences with respect to hand-designed systems. We do this by\nconstructing architectures incrementally, using a custom mutation algorithm\nand leveraging the power of parameter transfer between layers. We demonstrate\nthat our approach can automatically design DNNs with an order of magnitude\nfewer parameters that achieves better performance than the current\nbest models. It leads to significant performance improvements: up to\n4.09% accuracy increase for language identification (6.1% if we allow\nan increase in the number of parameters) and 0.3% for phoneme classification\nin keyword spotting with half the size of the model.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1916",
      "author_area_id": 4,
      "author_area_label": "Speaker and Language Identification"
    },
    "zheng19_interspeech": {
      "authors": [
        [
          "Yibin",
          "Zheng"
        ],
        [
          "Xi",
          "Wang"
        ],
        [
          "Lei",
          "He"
        ],
        [
          "Shifeng",
          "Pan"
        ],
        [
          "Frank K.",
          "Soong"
        ],
        [
          "Zhengqi",
          "Wen"
        ],
        [
          "Jianhua",
          "Tao"
        ]
      ],
      "title": "Forward-Backward Decoding for Regularizing End-to-End TTS",
      "original": "2325",
      "page_count": 5,
      "order": 268,
      "p1": "1283",
      "pn": "1287",
      "abstract": [
        "Neural end-to-end TTS can generate very high-quality synthesized speech,\nand even close to human recording within similar domain text. However,\nit performs unsatisfactory when scaling it to challenging test sets.\nOne concern is that the encoder-decoder with attention-based network\nadopts autoregressive generative sequence model with the limitation\nof &#8220;exposure bias&#8221;. To address this issue, we propose two\nnovel methods, which learn to predict future by improving agreement\nbetween forward and backward decoding sequence. The first one is achieved\nby introducing divergence regularization terms into model training\nobjective to reduce the mismatch between two directional models, namely\nL2R and R2L (which generates targets from left-to-right and right-to-left,\nrespectively). While the second one operates on decoder-level and exploits\nthe future information during decoding. In addition, we employ a joint\ntraining strategy to allow forward and backward decoding to improve\neach other in an interactive process. Experimental results show our\nproposed methods especially the second one (bidirectional decoder regularization),\nleads a significantly improvement on both robustness and overall naturalness,\nas outperforming baseline (the revised version of Tacotron2) with a\nMOS gap of 0.14 in a challenging test, and achieving close to human\nquality (4.42 vs. 4.49 in MOS) on general test.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2325",
      "author_area_id": 7,
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "guo19b_interspeech": {
      "authors": [
        [
          "Haohan",
          "Guo"
        ],
        [
          "Frank K.",
          "Soong"
        ],
        [
          "Lei",
          "He"
        ],
        [
          "Lei",
          "Xie"
        ]
      ],
      "title": "A New GAN-Based End-to-End TTS Training Algorithm",
      "original": "2176",
      "page_count": 5,
      "order": 269,
      "p1": "1288",
      "pn": "1292",
      "abstract": [
        "End-to-end, autoregressive model-based TTS has shown significant performance\nimprovements over the conventional ones. However, the autoregressive\nmodule training is affected by the exposure bias, or the mismatch between\ndifferent distributions of real and predicted data. While real data\nis provided in training, in testing, predicted data is available only.\nBy introducing both real and generated data sequences in training,\nwe can alleviate the effects of the exposure bias. We propose to use\nGenerative Adversarial Network (GAN) along with the idea of &#8220;Professor\nForcing&#8221; in training. A discriminator in GAN is jointly trained\nto equalize the difference between real and the predicted data. In\nAB subjective listening test, the results show that the new approach\nis preferred over the standard transfer learning with a CMOS improvement\nof 0.1. Sentence level intelligibility tests also show significant\nimprovement in a pathological test set. The GAN-trained new model is\nshown more stable than the baseline to produce better alignments for\nthe Tacotron output.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2176",
      "author_area_id": 7,
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "he19_interspeech": {
      "authors": [
        [
          "Mutian",
          "He"
        ],
        [
          "Yan",
          "Deng"
        ],
        [
          "Lei",
          "He"
        ]
      ],
      "title": "Robust Sequence-to-Sequence Acoustic Modeling with Stepwise Monotonic Attention for Neural TTS",
      "original": "1972",
      "page_count": 5,
      "order": 270,
      "p1": "1293",
      "pn": "1297",
      "abstract": [
        "Neural TTS has demonstrated strong capabilities to generate human-like\nspeech with high quality and naturalness, while its generalization\nto out-of-domain texts is still a challenging task, with regard to\nthe design of attention-based sequence-to-sequence acoustic modeling.\nVarious errors occur in those inputs with unseen context, including\nattention collapse, skipping, repeating, etc., which limits the broader\napplications. In this paper, we propose a novel stepwise monotonic\nattention method in sequence-to-sequence acoustic modeling to improve\nthe robustness on out-of-domain inputs. The method utilizes the strict\nmonotonic property in TTS with constraints on monotonic hard attention\nthat the alignments between inputs and outputs sequence must be not\nonly monotonic but allowing no skipping on inputs. Soft attention could\nbe used to evade mismatch between training and inference. The experimental\nresults show that the proposed method could achieve significant improvements\nin robustness on out-of-domain scenarios for phoneme-based models,\nwithout any regression on the in-domain naturalness test.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1972",
      "author_area_id": 7,
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "zhang19b_interspeech": {
      "authors": [
        [
          "Mingyang",
          "Zhang"
        ],
        [
          "Xin",
          "Wang"
        ],
        [
          "Fuming",
          "Fang"
        ],
        [
          "Haizhou",
          "Li"
        ],
        [
          "Junichi",
          "Yamagishi"
        ]
      ],
      "title": "Joint Training Framework for Text-to-Speech and Voice Conversion Using Multi-Source Tacotron and WaveNet",
      "original": "1357",
      "page_count": 5,
      "order": 271,
      "p1": "1298",
      "pn": "1302",
      "abstract": [
        "We investigated the training of a shared model for both text-to-speech\n(TTS) and voice conversion (VC) tasks. We propose using an extended\nmodel architecture of Tacotron, that is a multi-source sequence-to-sequence\nmodel with a dual attention mechanism as the shared model for both\nthe TTS and VC tasks. This model can accomplish these two different\ntasks respectively according to the type of input. An end-to-end speech\nsynthesis task is conducted when the model is given text as the input\nwhile a sequence-to-sequence voice conversion task is conducted when\nit is given the speech of a source speaker as the input. Waveform signals\nare generated by using WaveNet, which is conditioned by using a predicted\nmel-spectrogram. We propose jointly training a shared model as a decoder\nfor a target speaker that supports multiple sources. Listening experiments\nshow that our proposed multi-source encoder-decoder model can efficiently\nachieve both the TTS and VC tasks.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1357",
      "author_area_id": 7,
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "luong19_interspeech": {
      "authors": [
        [
          "Hieu-Thi",
          "Luong"
        ],
        [
          "Xin",
          "Wang"
        ],
        [
          "Junichi",
          "Yamagishi"
        ],
        [
          "Nobuyuki",
          "Nishizawa"
        ]
      ],
      "title": "Training Multi-Speaker Neural Text-to-Speech Systems Using Speaker-Imbalanced Speech Corpora",
      "original": "1311",
      "page_count": 5,
      "order": 272,
      "p1": "1303",
      "pn": "1307",
      "abstract": [
        "When the available data of a target speaker is insufficient to train\na high quality speaker-dependent neural text-to-speech (TTS) system,\nwe can combine data from multiple speakers and train a multi-speaker\nTTS model instead. Many studies have shown that neural multi-speaker\nTTS model trained with a small amount data from multiple speakers combined\ncan generate synthetic speech with better quality and stability than\na speaker-dependent one. However when the amount of data from each\nspeaker is highly unbalanced, the best approach to make use of the\nexcessive data remains unknown. Our experiments showed that simply\ncombining all available data from every speaker to train a multi-speaker\nmodel produces better than or at least similar performance to its speaker-dependent\ncounterpart. Moreover by using an ensemble multi-speaker model, in\nwhich each subsystem is trained on a subset of available data, we can\nfurther improve the quality of the synthetic speech especially for\nunderrepresented speakers whose training data is limited.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1311",
      "author_area_id": 7,
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "okamoto19_interspeech": {
      "authors": [
        [
          "Takuma",
          "Okamoto"
        ],
        [
          "Tomoki",
          "Toda"
        ],
        [
          "Yoshinori",
          "Shiga"
        ],
        [
          "Hisashi",
          "Kawai"
        ]
      ],
      "title": "Real-Time Neural Text-to-Speech with Sequence-to-Sequence Acoustic Model and WaveGlow or Single Gaussian WaveRNN Vocoders",
      "original": "1288",
      "page_count": 5,
      "order": 273,
      "p1": "1308",
      "pn": "1312",
      "abstract": [
        "This paper investigates real-time high-fidelity neural text-to-speech\n(TTS) systems. For real-time neural vocoders, WaveGlow is introduced\nand single Gaussian (SG)WaveRNN is proposed. The proposed SG-WaveRNN\ncan predict continuous valued speech waveforms with half the synthesis\ntime compared with vanilla WaveRNN with dual-softmax for 16 bit audio\nprediction. Additionally, a sequence-to-sequence (seq2seq) acoustic\nmodel (AM) for pitch accent languages, such as Japanese, is investigated\nby introducing Tacotron 2 architecture. In the seq2seq AM, full-context\nlabels extracted from a text analyzer are used as input and they are\ndirectly converted into mel-spectrograms. The results of subjective\nexperiment using a Japanese female corpus indicate that the proposed\nSG-WaveRNN vocoder with noise shaping can synthesize high-quality speech\nwaveforms and real-time high-fidelity neural TTS systems can be realized\nwith the seq2seq AM and WaveGlow or SG-WaveRNN vocoders. Especially,\nthe seq2seq AM and WaveGlow vocoder conditioned on mel-spectrograms\nwith simple PyTorch implementations can be realized with real-time\nfactors 0.06 and 0.10 for inference using a GPU.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1288",
      "author_area_id": 7,
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "kafle19_interspeech": {
      "authors": [
        [
          "Sushant",
          "Kafle"
        ],
        [
          "Cecilia Ovesdotter",
          "Alm"
        ],
        [
          "Matt",
          "Huenerfauth"
        ]
      ],
      "title": "Fusion Strategy for Prosodic and Lexical Representations of Word Importance",
      "original": "1898",
      "page_count": 5,
      "order": 274,
      "p1": "1313",
      "pn": "1317",
      "abstract": [
        "We investigate whether, and if so when, prosodic features in spoken\ndialogue aid in modeling the importance of words to the overall meaning\nof a dialogue turn. Starting from the assumption that acoustic-prosodic\ncues help identify important speech content, we investigate representation\narchitectures that combine lexical and prosodic features and evaluate\nthem for predicting word importance. We propose an attention-based\nfeature fusion strategy and additionally show how the addition of strategic\nsupervision of the attention weights results in especially competitive\nmodels. We evaluate our fusion strategy on spoken dialogues and demonstrate\nperformance increases over state-of-the-art models. Specifically, our\napproach both achieves the lowest root mean square error on test data\nand generalizes better over out-of-vocabulary words.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1898",
      "author_area_id": 12,
      "author_area_label": "Spoken Language Processing: Translation, Information Retrieval, Summarization, Resources and Evaluation"
    },
    "chien19b_interspeech": {
      "authors": [
        [
          "Jen-Tzung",
          "Chien"
        ],
        [
          "Chun-Wei",
          "Wang"
        ]
      ],
      "title": "Self Attention in Variational Sequential Learning for Summarization",
      "original": "1548",
      "page_count": 5,
      "order": 275,
      "p1": "1318",
      "pn": "1322",
      "abstract": [
        "Attention mechanism plays a crucial role in sequential learning for\nmany speech and language applications. However, it is challenging to\ndevelop a  stochastic attention in a sequence-to-sequence model which\nconsists of two recurrent neural networks (RNNs) as the encoder and\ndecoder. The problem of  posterior collapse happens in variational\ninference and results in the estimated latent variables close to a\nstandard Gaussian prior so that the information from input sequence\nis disregarded in learning process. This paper presents a new recurrent\nautoencoder for sentence representation where a  self attention scheme\nis incorporated to activate the interaction between inference and generation\nin training procedure. In particular, a stochastic RNN decoder is implemented\nto provide additional latent variable to fulfill self attention for\nsentence reconstruction. The posterior collapse is alleviated. The\nlatent information is sufficiently attended in variational sequential\nlearning. During test phase, the estimated prior distribution of decoder\nis sampled for stochastic attention and generation. Experiments on\nPenn Treebank and Yelp 2013 show the desirable generation performance\nin terms of perplexity. The visualization of attention weights also\nillustrates the usefulness of self attention. The evaluation on DUC\n2007 demonstrates the merit of variational recurrent autoencoder for\ndocument summarization.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1548",
      "author_area_id": 12,
      "author_area_label": "Spoken Language Processing: Translation, Information Retrieval, Summarization, Resources and Evaluation"
    },
    "sun19_interspeech": {
      "authors": [
        [
          "Zhongkai",
          "Sun"
        ],
        [
          "Prathusha K.",
          "Sarma"
        ],
        [
          "William",
          "Sethares"
        ],
        [
          "Erik P.",
          "Bucy"
        ]
      ],
      "title": "Multi-Modal Sentiment Analysis Using Deep Canonical Correlation Analysis",
      "original": "2482",
      "page_count": 5,
      "order": 276,
      "p1": "1323",
      "pn": "1327",
      "abstract": [
        "This paper learns multi-modal embeddings from text, audio, and video\nviews/modes of data in order to improve upon downstream sentiment classification.\nThe experimental framework also allows investigation of the relative\ncontributions of the individual views in the final multi-modal embedding.\nIndividual features derived from the three views are combined into\na multi-modal embedding using Deep Canonical Correlation Analysis (DCCA)\nin two ways i) One-Step DCCA and ii) Two-Step DCCA. This paper learns\ntext embeddings using BERT, the current state-of-the-art in text encoders.\nWe posit that this highly optimized algorithm dominates over the contribution\nof other views, though each view does contribute to the final result.\nClassification tasks are carried out on two benchmark data sets and\non a new Debate Emotion data set, and together these demonstrate that\nthe one-Step DCCA outperforms the current state-of-the-art in learning\nmulti-modal embeddings.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2482",
      "author_area_id": 12,
      "author_area_label": "Spoken Language Processing: Translation, Information Retrieval, Summarization, Resources and Evaluation"
    },
    "shen19_interspeech": {
      "authors": [
        [
          "Yilin",
          "Shen"
        ],
        [
          "Wenhu",
          "Chen"
        ],
        [
          "Hongxia",
          "Jin"
        ]
      ],
      "title": "Interpreting and Improving Deep Neural SLU Models via Vocabulary Importance",
      "original": "3184",
      "page_count": 5,
      "order": 277,
      "p1": "1328",
      "pn": "1332",
      "abstract": [
        "Spoken language understanding (SLU) is a crucial component in virtual\npersonal assistants. It consists of two main tasks: intent detection\nand slot filling. State-of-the-art deep neural SLU models have demonstrated\ngood performance on benchmark datasets. However, these models suffer\nfrom the significant performance drop in practice after deployment\ndue to the data distribution discrepancy between training and real\nuser utterances. In this paper, we first propose four research questions\nthat help to understand what the state-of-the-art deep neural SLU models\nactually learn. To answer them, we study the vocabulary importance\nusing a novel  Embedding Sparse Structure Learning (SparseEmb) approach.\nIt can be applied onto various existing deep SLU models to efficiently\nprune the useless words without any additional manual hyperparameter\ntuning. We evaluate SparseEmb on benchmark datasets using two existing\nSLU models and answer the proposed research questions. Then, we utilize\nSparseEmb to sanitize the training data based on the selected useless\nwords as well as the model re-validation during training. Using both\nbenchmark and our collected testing data, we show that our sanitized\ntraining data helps to significantly improve the SLU model performance.\nBoth SparseEmb and training data sanitization approaches can be applied\nonto any deep learning based SLU models.\n"
      ],
      "doi": "10.21437/Interspeech.2019-3184",
      "author_area_id": 12,
      "author_area_label": "Spoken Language Processing: Translation, Information Retrieval, Summarization, Resources and Evaluation"
    },
    "tundik19_interspeech": {
      "authors": [
        [
          "M\u00e1t\u00e9 \u00c1kos",
          "T\u00fcndik"
        ],
        [
          "Val\u00e9r",
          "Kasz\u00e1s"
        ],
        [
          "Gy\u00f6rgy",
          "Szasz\u00e1k"
        ]
      ],
      "title": "Assessing the Semantic Space Bias Caused by ASR Error Propagation and its Effect on Spoken Document Summarization",
      "original": "2154",
      "page_count": 5,
      "order": 278,
      "p1": "1333",
      "pn": "1337",
      "abstract": [
        "Ambitions in artificial intelligence involve machine understanding\nof human language. The state-of-the-art approach for Spoken Language\nUnderstanding is using an Automatic Speech Recognizer (ASR) to generate\ntranscripts, which are further processed with text-based tools. ASR\nyields error prone transcripts, these errors then propagate further\ninto the processing pipeline. Subjective tests show on the other hand,\nthat humans understand quite well ASR closed captions despite the word\nand punctuation errors. Our goal is to assess and quantify the loss\nin the semantic space resulting from error propagation and also analyze\nerror propagation into speech summarization as a special use-case.\nWe show, that word errors cause a slight shift in the semantic space,\nwhich is fairly below the average semantic distance between the sentences\nwithin a document. We also show, that punctuation errors have higher\nimpact on summarization performance, which suggests that proper sentence\nlevel tokenization is crucial for this task.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2154",
      "author_area_id": 12,
      "author_area_label": "Spoken Language Processing: Translation, Information Retrieval, Summarization, Resources and Evaluation"
    },
    "huang19e_interspeech": {
      "authors": [
        [
          "Peisong",
          "Huang"
        ],
        [
          "Peijie",
          "Huang"
        ],
        [
          "Wencheng",
          "Ai"
        ],
        [
          "Jiande",
          "Ding"
        ],
        [
          "Jinchuan",
          "Zhang"
        ]
      ],
      "title": "Latent Topic Attention for Domain Classification",
      "original": "2228",
      "page_count": 5,
      "order": 279,
      "p1": "1338",
      "pn": "1342",
      "abstract": [
        "Attention-based bidirectional long short-term network (BiLSTM) models\nhave recently shown promising results in text classification tasks.\nHowever, when the amount of training data is restricted, or the distribution\nof the test data is quite different from the training data, some potential\ninformative words maybe hard to be captured in training. In this work,\nwe propose a new method to learn attention mechanism for domain classification.\nUnlike the past attention mechanisms only guided by domain tags of\ntraining data, we explore using the latent topics in the data set to\nlearn topic attention, and employ it for BiLSTM. Experiments on the\nSMP-ECDT benchmark corpus show that the proposed latent topic attention\nmechanism outperforms the state-of-the-art soft and hard attention\nmechanisms in domain classification. Moreover, experiment result shows\nthat the proposed method can be trained with additional unlabeled data\nand further improve the domain classification performance.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2228",
      "author_area_id": 12,
      "author_area_label": "Spoken Language Processing: Translation, Information Retrieval, Summarization, Resources and Evaluation"
    },
    "narisetty19_interspeech": {
      "authors": [
        [
          "Chaitanya",
          "Narisetty"
        ]
      ],
      "title": "A Unified Bayesian Source Modelling for Determined Blind Source Separation",
      "original": "1272",
      "page_count": 5,
      "order": 280,
      "p1": "1343",
      "pn": "1347",
      "abstract": [
        "This paper proposes a determined blind source separation (BSS) method\nwith a Bayesian generalization for unified modelling of multiple audio\nsources. Our probabilistic framework allows a flexible multi-source\nmodelling where the number of latent features required for the unified\nmodel is optimally estimated. When partitioning the latent features\nof the unified model to represent individual sources, the proposed\napproach helps to avoid over-fitting or under-fitting the correlations\namong sources. This adaptability of our Bayesian generalization therefore\nadds flexibility to conventional BSS approaches, where the number of\nlatent features in the unified model has to be specified in advance.\nIn the task of separating speech mixture signals, we show that our\nproposed method models diverse sources in a flexible manner and markedly\nimproves the separation performance as compared to the conventional\nmethods.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1272",
      "author_area_id": 5,
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "takahashi19_interspeech": {
      "authors": [
        [
          "Naoya",
          "Takahashi"
        ],
        [
          "Sudarsanam",
          "Parthasaarathy"
        ],
        [
          "Nabarun",
          "Goswami"
        ],
        [
          "Yuki",
          "Mitsufuji"
        ]
      ],
      "title": "Recursive Speech Separation for Unknown Number of Speakers",
      "original": "1550",
      "page_count": 5,
      "order": 281,
      "p1": "1348",
      "pn": "1352",
      "abstract": [
        "In this paper we propose a method of single-channel speaker-independent\nmulti-speaker speech separation for an unknown number of speakers.\nAs opposed to previous works, in which the number of speakers is assumed\nto be known in advance and speech separation models are specific for\nthe number of speakers, our proposed method can be applied to cases\nwith different numbers of speakers using a single model by recursively\nseparating a speaker. To make the separation model recursively applicable,\nwe propose one-and-rest permutation invariant training (OR-PIT). Evaluation\non WSJ0-2mix and WSJ0-3mix datasets show that our proposed method achieves\nstate-of-the-art results for two- and three-speaker mixtures with a\nsingle model. Moreover, the same model can separate four-speaker mixture,\nwhich was never seen during the training. We further propose the detection\nof the number of speakers in a mixture during recursive separation\nand show that this approach can more accurately estimate the number\nof speakers than detection in advance by using a deep neural network\nbased classifier.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1550",
      "author_area_id": 5,
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "appeltans19_interspeech": {
      "authors": [
        [
          "Pieter",
          "Appeltans"
        ],
        [
          "Jeroen",
          "Zegers"
        ],
        [
          "Hugo",
          "Van hamme"
        ]
      ],
      "title": "Practical Applicability of Deep Neural Networks for Overlapping Speaker Separation",
      "original": "1807",
      "page_count": 5,
      "order": 282,
      "p1": "1353",
      "pn": "1357",
      "abstract": [
        "This paper examines the applicability in realistic scenarios of two\ndeep learning based solutions to the overlapping speaker separation\nproblem. Firstly, we present experiments that show that these methods\nare applicable for a broad range of languages. Further experimentation\nindicates limited performance loss for untrained languages, when these\nhave common features with the trained language(s). Secondly, it investigates\nhow the methods deal with realistic background noise and proposes some\nmodifications to better cope with these disturbances. The deep learning\nmethods that will be examined are deep clustering and deep attractor\nnetworks.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1807",
      "author_area_id": 5,
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "gu19_interspeech": {
      "authors": [
        [
          "Zhaoyi",
          "Gu"
        ],
        [
          "Jing",
          "Lu"
        ],
        [
          "Kai",
          "Chen"
        ]
      ],
      "title": "Speech Separation Using Independent Vector Analysis with an Amplitude Variable Gaussian Mixture Model",
      "original": "2076",
      "page_count": 5,
      "order": 283,
      "p1": "1358",
      "pn": "1362",
      "abstract": [
        "Independent vector analysis (IVA) utilizing Gaussian mixture model\n(GMM) as source priors has been demonstrated as an effective algorithm\nfor joint blind source separation (JBSS). However, an extra pre-training\nprocess is required to provide initial parameter values for successful\nspeech separation. In this paper, we introduce a time-varying parameter\nin the GMM to adapt to the temporal power fluctuation embedded in the\nnonstationary speech signal so as to avoid the pre-training process.\nThe expectation-maximization (EM) process updating both the demixing\nmatrix and the signal model is altered correspondingly. Experimental\nresults confirm the efficacy of the proposed method under random initialization\nand further show its advantage in terms of a competitive separation\naccuracy and a faster convergence speed.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2076",
      "author_area_id": 5,
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "yang19c_interspeech": {
      "authors": [
        [
          "Gene-Ping",
          "Yang"
        ],
        [
          "Chao-I",
          "Tuan"
        ],
        [
          "Hung-Yi",
          "Lee"
        ],
        [
          "Lin-shan",
          "Lee"
        ]
      ],
      "title": "Improved Speech Separation with Time-and-Frequency Cross-Domain Joint Embedding and Clustering",
      "original": "2181",
      "page_count": 5,
      "order": 284,
      "p1": "1363",
      "pn": "1367",
      "abstract": [
        "Speech separation has been very successful with deep learning techniques.\nSubstantial effort has been reported based on approaches over magnitude\nspectrogram, which is well known as the standard time-and-frequency\ncross-domain representation for speech signals. It is highly correlated\nto the phonetic structure of speech, or &#8220;how the speech sounds&#8221;\nwhen perceived by human, but primarily frequency domain features carrying\ntemporal behaviour. Very impressive work achieving speech separation\nover time domain was reported recently, probably because waveforms\nin time domain may describe the different realizations of speech in\na more precise way than magnitude spectrogram lacking phase information.\nIn this paper, we propose a framework properly integrating the above\ntwo directions, hoping to achieve both purposes. We construct a time-and-frequency\nfeature map by concatenating 1-dim convolution encoded feature map\n(for time domain) and magnitude spectrogram (for frequency domain),\nwhich was then processed by an embedding network and clustering approaches\nvery similar to those used in time and frequency domain prior works.\nIn this way, the information in time and frequency domains, as well\nas the interactions between them, can be jointly considered during\nembedding and clustering. Very encouraging results (state-of-the-art\nto our knowledge) were obtained with WSJ0-2mix dataset in preliminary\nexperiments.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2181",
      "author_area_id": 5,
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "wichern19_interspeech": {
      "authors": [
        [
          "Gordon",
          "Wichern"
        ],
        [
          "Joe",
          "Antognini"
        ],
        [
          "Michael",
          "Flynn"
        ],
        [
          "Licheng Richard",
          "Zhu"
        ],
        [
          "Emmett",
          "McQuinn"
        ],
        [
          "Dwight",
          "Crow"
        ],
        [
          "Ethan",
          "Manilow"
        ],
        [
          "Jonathan Le",
          "Roux"
        ]
      ],
      "title": "WHAM!: Extending Speech Separation to Noisy Environments",
      "original": "2821",
      "page_count": 5,
      "order": 285,
      "p1": "1368",
      "pn": "1372",
      "abstract": [
        "Recent progress in separating the speech signals from multiple overlapping\nspeakers using a single audio channel has brought us closer to solving\nthe cocktail party problem. However, most studies in this area use\na constrained problem setup, comparing performance when speakers overlap\nalmost completely, at artificially low sampling rates, and with no\nexternal background noise. In this paper, we strive to move the field\ntowards more realistic and challenging scenarios. To that end, we created\nthe WSJ0 Hipster Ambient Mixtures (WHAM!) dataset, consisting of two\nspeaker mixtures from the wsj0-2mix dataset combined with real ambient\nnoise samples. The samples were collected in coffee shops, restaurants,\nand bars in the San Francisco Bay Area, and are made publicly available.\nWe benchmark various speech separation architectures and objective\nfunctions to evaluate their robustness to noise. While separation performance\ndecreases as a result of noise, we still observe substantial gains\nrelative to the noisy signals for most approaches.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2821",
      "author_area_id": 5,
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "nautsch19_interspeech": {
      "authors": [
        [
          "Andreas",
          "Nautsch"
        ]
      ],
      "title": "Survey Talk: Preserving Privacy in Speaker and Speech Characterisation",
      "original": "abs11",
      "page_count": 0,
      "order": 286,
      "p1": "0",
      "pn": "",
      "abstract": [
        "The survey addresses recent work that has the aim of preserving privacy\nin speech communication applications. The talk discusses recent privacy\nlegislation in the US and especially the European Union, and focuses\nupon the GDPR (EU Regulation 2016/679) and the Police Directive (EU\nDirective 2016/680), covering also &#8216;Privacy by Design&#8217;\nand &#8216;Privacy by Default&#8217; policy concepts. Emphasis is placed\non voice biometrics and non-biometric speech technology. Since there\nis no &#8220;one size fits all&#8221; solution, specific cryptographic\nsolutions to privacy preservation are highlighted. Among other classification\ntasks, voice biometrics can intrude on privacy when misused; the talk\nsurveys a number of privacy safeguards. The international standard\nfor biometric information protection is reviewed and figures of merit\nare proposed regarding, e.g., the extent to which privacy is preserved.\nMore interdisciplinary efforts are necessary to reach a common understanding\nbetween speech technology, legislation, and cryptography communities\n(among many others). Future challenges include the need to not only\ncarry out decision inference securely, but also to preserve privacy,\nwhere cryptographic methods need to meet the demands of speech signal\nprocessing. In communication, speech is a medium, not a message.\n"
      ]
    },
    "chermaz19_interspeech": {
      "authors": [
        [
          "Carol",
          "Chermaz"
        ],
        [
          "Cassia",
          "Valentini-Botinhao"
        ],
        [
          "Henning",
          "Schepker"
        ],
        [
          "Simon",
          "King"
        ]
      ],
      "title": "Evaluating Near End Listening Enhancement Algorithms in Realistic Environments",
      "original": "1800",
      "page_count": 5,
      "order": 287,
      "p1": "1373",
      "pn": "1377",
      "abstract": [
        "Speech playback (e.g., TV, radio, public address) becomes harder to\nunderstand in the presence of noise and reverberation. NELE (Near End\nListening Enhancement) algorithms can improve intelligibility by modifying\nthe signal before it is played back. Substantial intelligibility improvements\nhave been achieved in the lab for both natural and synthetic speech.\nHowever, evidence is still scarce on how these algorithms work under\nconditions of realistic noise and reverberation.<br style=\"mso-data-placement:same-cell;\"\n/><br style=\"mso-data-placement:same-cell;\" />\n  We present a realistic\ntest platform, featuring two representative everyday scenarios in which\nspeech playback may occur (in the presence of both noise and reverberation):\na domestic space (living room) and a public space (cafeteria). The\ngenerated stimuli are evaluated by measuring keyword accuracy rates\nin a listening test with normal hearing subjects.<br style=\"mso-data-placement:same-cell;\"\n/><br style=\"mso-data-placement:same-cell;\" />\n  We use the new platform\nto compare three state-of-the-art NELE algorithms, employing either\nnoise-adaptive or non-adaptive strategies, and with or without compensation\nfor reverberation.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1800",
      "author_area_id": 6,
      "author_area_label": "Speech Coding and Enhancement"
    },
    "edraki19_interspeech": {
      "authors": [
        [
          "Amin",
          "Edraki"
        ],
        [
          "Wai-Yip",
          "Chan"
        ],
        [
          "Jesper",
          "Jensen"
        ],
        [
          "Daniel",
          "Fogerty"
        ]
      ],
      "title": "Improvement and Assessment of Spectro-Temporal Modulation Analysis for Speech Intelligibility Estimation",
      "original": "2898",
      "page_count": 5,
      "order": 288,
      "p1": "1378",
      "pn": "1382",
      "abstract": [
        "Several recent high-performing intelligibility estimators of acoustically\ndegraded speech signals employ temporal modulation analysis. In this\npaper, we investigate the utility of using both spectro- and temporal-modulation\nfor estimating speech intelligibility. We modified a pre-existing speech\nintelligibility estimation scheme (STMI) that was inspired by human\nauditory spectro-temporal modulation analysis. We produced several\nvariants of the modified STMI and assessed their intelligibility prediction\naccuracy, in comparison with several high-performing estimators. Among\nthe estimators tested, one of the STMI variants and eSTOI performed\nconsistently well on both noisy and reverberated speech. These results\nsuggest that spectro-temporal modulation analysis is useful for certain\ndegradation conditions such as modulated noise and reverberation.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2898",
      "author_area_id": 6,
      "author_area_label": "Speech Coding and Enhancement"
    },
    "zhang19c_interspeech": {
      "authors": [
        [
          "Zhuohuang",
          "Zhang"
        ],
        [
          "Yi",
          "Shen"
        ]
      ],
      "title": "Listener Preference on the Local Criterion for Ideal Binary-Masked Speech",
      "original": "1369",
      "page_count": 5,
      "order": 289,
      "p1": "1383",
      "pn": "1387",
      "abstract": [
        "Ideal binary mask (IBM) is a signal-processing technique that retains\nthe time-frequency regions in a mixture of target speech and background\nnoise when the local signal-to-noise ratio (SNR) is higher than a local\ncriterion (LC) and removes the regions otherwise. The intelligibility\nof IBM-processed speech is typically high and does not depend on the\nchoice of LC for a wide range of LC values. The current study investigates\nthe listeners&#8217; preferences on the LC value for IBM processed\nspeech. Concatenated everyday sentences were mixed with three types\nof background noises (airplane noise, train noise, and multi-talker\nbabble) and were presented continuously to the listeners following\nthe IBM processing. The IBM algorithm was implemented so that the listeners\nwere able to adjust the LC value in real-time using a programmable\nknob. The listeners were instructed to adjust the LC value until the\nIBM-processed stimuli reached the most preferable quality. Across 20\nlisteners, large individual differences were observed for the preferred\nLC values. A cluster analysis identified that 11 of the 20 listeners\nexhibited consistent patterns of results. For this main cluster of\nlisteners, the preferred LC value depended on the noise type, overall\nSNR, and the difficulty of the target sentences.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1369",
      "author_area_id": 6,
      "author_area_label": "Speech Coding and Enhancement"
    },
    "dinh19_interspeech": {
      "authors": [
        [
          "Tuan",
          "Dinh"
        ],
        [
          "Alexander",
          "Kain"
        ],
        [
          "Kris",
          "Tjaden"
        ]
      ],
      "title": "Using a Manifold Vocoder for Spectral Voice and Style Conversion",
      "original": "1176",
      "page_count": 5,
      "order": 290,
      "p1": "1388",
      "pn": "1392",
      "abstract": [
        "We propose a new type of spectral feature that is both compact and\ninterpolable, and thus ideally suited for regression approaches that\ninvolve averaging. The feature is realized by means of a speaker-independent\nvariational autoencoder (VAE), which learns a latent space based on\nthe low-dimensional manifold of high-resolution speech spectra. In\nvocoding experiments, we showed that using a 12-dimensional VAE feature\n(VAE-12) resulted in significantly better perceived speech quality\ncompared to a 12-dimensional MCEP feature. In voice conversion experiments,\nusing VAE-12 resulted in significantly better perceived speech quality\nas compared to 40-dimensional MCEPs, with similar speaker accuracy.\nIn habitual to clear style conversion experiments, we significantly\nimproved the speech intelligibility for one of three speakers, using\na custom skip-connection deep neural network, with the average keyword\nrecall accuracy increasing from 24% to 46%.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1176",
      "author_area_id": 6,
      "author_area_label": "Speech Coding and Enhancement"
    },
    "platen19_interspeech": {
      "authors": [
        [
          "P. von",
          "Platen"
        ],
        [
          "Chao",
          "Zhang"
        ],
        [
          "P.C.",
          "Woodland"
        ]
      ],
      "title": "Multi-Span Acoustic Modelling Using Raw Waveform Signals",
      "original": "2454",
      "page_count": 5,
      "order": 291,
      "p1": "1393",
      "pn": "1397",
      "abstract": [
        "Traditional automatic speech recognition (ASR) systems often use an\nacoustic model (AM) built on handcrafted acoustic features, such as\nlog Mel-filter bank (FBANK) values. Recent studies found that AMs with\nconvolutional neural networks (CNNs) can directly use the raw waveform\nsignal as input. Given sufficient training data, these AMs can yield\na competitive word error rate (WER) to those built on FBANK features.\nThis paper proposes a novel multi-span structure for acoustic modelling\nbased on the raw waveform with multiple streams of CNN input layers,\neach processing a different span of the raw waveform signal. Evaluation\non both the single channel CHiME4 and AMI data sets show that multi-span\nAMs give a lower WER than FBANK AMs by an average of about 5% (relative).\nAnalysis of the trained multi-span model reveals that the CNNs can\nlearn filters that are rather different to the log Mel-filters. Furthermore,\nthe paper shows that a widely used single span raw waveform AM can\nbe improved by using a smaller CNN kernel size and increased stride\nto yield improved WERs.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2454",
      "author_area_id": 8,
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "merboldt19_interspeech": {
      "authors": [
        [
          "Andr\u00e9",
          "Merboldt"
        ],
        [
          "Albert",
          "Zeyer"
        ],
        [
          "Ralf",
          "Schl\u00fcter"
        ],
        [
          "Hermann",
          "Ney"
        ]
      ],
      "title": "An Analysis of Local Monotonic Attention Variants",
      "original": "2879",
      "page_count": 5,
      "order": 292,
      "p1": "1398",
      "pn": "1402",
      "abstract": [
        "Speech recognition using attention-based models is an effective approach\nto transcribing audio directly to text within an integrated end-to-end\narchitecture. Global attention approaches compute a weighting over\nthe complete input sequence, whereas local attention mechanisms are\nrestricted to only a localized window of the sequence. For speech,\nthe latter approach supports the monotonicity property of the speech-text\nalignment. Therefore, we revise several variants of such models and\nprovide a comprehensive comparison, which has been missing so far in\nthe literature. Additionally, we introduce a simple technique to implement\nwindowed attention. This can be applied on top of an existing global\nattention model. The goal is to transition into a local attention model,\nby using a local window for the otherwise unchanged attention mechanism,\nstarting from the temporal position with the most recent most active\nattention energy. We test this method on Switchboard and LibriSpeech\nand show that the proposed model can even be trained from random initialization\nand achieve results comparable to the global attention baseline.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2879",
      "author_area_id": 8,
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "sun19b_interspeech": {
      "authors": [
        [
          "Eric",
          "Sun"
        ],
        [
          "Jinyu",
          "Li"
        ],
        [
          "Yifan",
          "Gong"
        ]
      ],
      "title": "Layer Trajectory BLSTM",
      "original": "2971",
      "page_count": 5,
      "order": 293,
      "p1": "1403",
      "pn": "1407",
      "abstract": [
        "Recently, we proposed layer trajectory (LT) LSTM (ltLSTM) which significantly\noutperforms LSTM by decoupling the functions of senone classification\nand temporal modeling with separate depth and time LSTMs. We further\nimproved ltLSTM with contextual layer trajectory LSTM (cltLSTM) which\nuses the future context frames to predict target labels. Given bidirectional\nLSTM (BLSTM) also uses future context frames to improve its modeling\npower, in this study we first compare the performance between these\ntwo models. Then we apply the layer trajectory idea to further improve\nBLSTM models, in which BLSTM is in charge of modeling the temporal\ninformation while depth-LSTM takes care of senone classification. In\naddition, we also investigate the model performance among different\nLT component designs on BLSTM models. Trained with 30 thousand hours\nof EN-US Microsoft internal data, the proposed layer trajectory BLSTM\n(ltBLSTM) model improved the baseline BLSTM with up to 14.5% relative\nword error rate (WER) reduction across different tasks.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2971",
      "author_area_id": 8,
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "karita19_interspeech": {
      "authors": [
        [
          "Shigeki",
          "Karita"
        ],
        [
          "Nelson Enrique Yalta",
          "Soplin"
        ],
        [
          "Shinji",
          "Watanabe"
        ],
        [
          "Marc",
          "Delcroix"
        ],
        [
          "Atsunori",
          "Ogawa"
        ],
        [
          "Tomohiro",
          "Nakatani"
        ]
      ],
      "title": "Improving Transformer-Based End-to-End Speech Recognition with Connectionist Temporal Classification and Language Model Integration",
      "original": "1938",
      "page_count": 5,
      "order": 294,
      "p1": "1408",
      "pn": "1412",
      "abstract": [
        "The state-of-the-art neural network architecture named Transformer\nhas been used successfully for many sequence-to-sequence transformation\ntasks. The advantage of this architecture is that it has a fast iteration\nspeed in the training stage because there is no sequential operation\nas with recurrent neural networks (RNN). However, an RNN is still the\nbest option for end-to-end automatic speech recognition (ASR) tasks\nin terms of overall training speed (i.e., convergence) and word error\nrate (WER) because of effective joint training and decoding methods.\nTo realize a faster and more accurate ASR system, we combine Transformer\nand the advances in RNN-based ASR. In our experiments, we found that\nthe training of Transformer is slower than that of RNN as regards the\nlearning curve and integration with the naive language model (LM) is\ndifficult. To address these problems, we integrate connectionist temporal\nclassification (CTC) with Transformer for joint training and decoding.\nThis approach makes training faster than with RNNs and assists LM integration.\nOur proposed ASR system realizes significant improvements in various\nASR tasks. For example, it reduced the WERs from 11.1% to 4.5% on the\nWall Street Journal and from 16.1% to 11.6% on the TED-LIUM by introducing\nCTC and LM integration into the Transformer baseline.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1938",
      "author_area_id": 8,
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "zhang19d_interspeech": {
      "authors": [
        [
          "Shucong",
          "Zhang"
        ],
        [
          "Erfan",
          "Loweimi"
        ],
        [
          "Yumo",
          "Xu"
        ],
        [
          "Peter",
          "Bell"
        ],
        [
          "Steve",
          "Renals"
        ]
      ],
      "title": "Trainable Dynamic Subsampling for End-to-End Speech Recognition",
      "original": "2778",
      "page_count": 5,
      "order": 295,
      "p1": "1413",
      "pn": "1417",
      "abstract": [
        "Jointly optimised attention-based encoder-decoder models have yielded\nimpressive speech recognition results. The recurrent neural network\n(RNN) encoder is a key component in such models &#8212; it learns the\nhidden representations of the inputs. However, it is difficult for\nRNNs to model the long sequences characteristic of speech recognition.\nTo address this, subsampling between stacked recurrent layers of the\nencoder is commonly employed. This method reduces the length of the\ninput sequence and leads to gains in accuracy. However, static subsampling\nmay both include redundant information and miss relevant information.<br\nstyle=\"mso-data-placement:same-cell;\" /><br style=\"mso-data-placement:same-cell;\"\n/>\n  We propose using a dynamic subsampling RNN (dsRNN) encoder. Unlike\na statically subsampled RNN encoder, the dsRNN encoder can learn to\nskip redundant frames. Furthermore, the skip ratio may vary at different\nstages of training, thus allowing the encoder to learn the most relevant\ninformation for each epoch. Although the dsRNN is unidirectional, it\nyields lower phone error rates (PERs) than a bidirectional RNN on TIMIT.\nThe dsRNN encoder has a 16.8% PER on the TIMIT test set, a considerable\nimprovement over static subsampling methods used with unidirectional\nand bidirectional RNN encoders (23.5% and 20.4% PER respectively).\n"
      ],
      "doi": "10.21437/Interspeech.2019-2778",
      "author_area_id": 8,
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "zhao19d_interspeech": {
      "authors": [
        [
          "Ding",
          "Zhao"
        ],
        [
          "Tara N.",
          "Sainath"
        ],
        [
          "David",
          "Rybach"
        ],
        [
          "Pat",
          "Rondon"
        ],
        [
          "Deepti",
          "Bhatia"
        ],
        [
          "Bo",
          "Li"
        ],
        [
          "Ruoming",
          "Pang"
        ]
      ],
      "title": "Shallow-Fusion End-to-End Contextual Biasing",
      "original": "1209",
      "page_count": 5,
      "order": 296,
      "p1": "1418",
      "pn": "1422",
      "abstract": [
        "Contextual biasing to a specific domain, including a user&#8217;s song\nnames, app names and contact names, is an important component of any\nproduction-level automatic speech recognition (ASR) system. Contextual\nbiasing is particularly challenging in end-to-end models because these\nmodels keep a small list of candidates during beam search, and also\ndo poorly on proper nouns, which is the main source of biasing phrases.\nIn this paper, we present various algorithmic and training improvements\nto shallow-fusion-based biasing for end-to-end models. We will show\nthat the proposed approach obtains better performance than a state-of-the-art\nconventional model across a variety of tasks, the first time this has\nbeen demonstrated.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1209",
      "author_area_id": 8,
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "nasir19_interspeech": {
      "authors": [
        [
          "Md.",
          "Nasir"
        ],
        [
          "Sandeep Nallan",
          "Chakravarthula"
        ],
        [
          "Brian R.W.",
          "Baucom"
        ],
        [
          "David C.",
          "Atkins"
        ],
        [
          "Panayiotis",
          "Georgiou"
        ],
        [
          "Shrikanth",
          "Narayanan"
        ]
      ],
      "title": "Modeling Interpersonal Linguistic Coordination in Conversations Using Word Mover&#8217;s Distance",
      "original": "1900",
      "page_count": 5,
      "order": 297,
      "p1": "1423",
      "pn": "1427",
      "abstract": [
        "Linguistic coordination is a well-established phenomenon in spoken\nconversations and often associated with positive social behaviors and\noutcomes. While there have been many attempts to measure lexical coordination\nor entrainment in literature, only a few have explored coordination\nin syntactic or semantic space. In this work, we attempt to combine\nthese different aspects of coordination into a single measure by leveraging\ndistances in a neural word representation space. In particular, we\nadopt the recently proposed Word Mover&#8217;s Distance with  word2vec\nembeddings and extend it to measure the dissimilarity in language used\nin multiple consecutive speaker turns. To validate our approach, we\napply this measure for two case studies in the clinical psychology\ndomain. We find that our proposed measure is correlated with the therapist&#8217;s\nempathy towards their patient in Motivational Interviewing and with\naffective behaviors in Couples Therapy. In both case studies, our proposed\nmetric exhibits higher correlation than previously proposed measures.\nWhen applied to the couples with relationship improvement, we also\nnotice a significant decrease in the proposed measure over the course\nof therapy, indicating higher linguistic coordination.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1900",
      "author_area_id": 3,
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "du19_interspeech": {
      "authors": [
        [
          "Wenchao",
          "Du"
        ],
        [
          "Louis-Philippe",
          "Morency"
        ],
        [
          "Jeffrey",
          "Cohn"
        ],
        [
          "Alan W.",
          "Black"
        ]
      ],
      "title": "Bag-of-Acoustic-Words for Mental Health Assessment: A Deep Autoencoding Approach",
      "original": "3059",
      "page_count": 5,
      "order": 298,
      "p1": "1428",
      "pn": "1432",
      "abstract": [
        "Despite the recent success of deep learning, it is generally difficult\nto apply end-to-end deep neural networks to small datasets, such as\nthose from the health domain, due to the tendency of neural networks\nto over-fit. In addition, how neural models reach their decisions is\nnot well understood. In this paper, we present a two-stage approach\nto acoustic-based classification of behavior markers related to mental\nhealth disorders: first, a dictionary and the mapping from speech signals\nto the dictionary are learned jointly by a deep autoencoder, then the\nbag-of-words representation of speech is used for classification, using\nclassifiers with simple decision boundaries. This deep bag-of-features\napproach has the advantage of offering more interpretability, while\nthe use of deep autoencoder gains improvements in prediction by learning\nhigher level features with long range dependencies, comparing to previous\nwork using only low-level descriptors. In addition, we demonstrate\nthe use of labeled emotion recognition data from other domains to supervise\nacoustic word encoding in order to help predict psychological traits.\nExperiments are conducted on audio recordings of 65 clinically recorded\ninterviews with the self-reported level of post-traumatic stress disorder\n(PTSD), depression, and rapport with the interviewers.\n"
      ],
      "doi": "10.21437/Interspeech.2019-3059",
      "author_area_id": 3,
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "voleti19_interspeech": {
      "authors": [
        [
          "Rohit",
          "Voleti"
        ],
        [
          "Stephanie",
          "Woolridge"
        ],
        [
          "Julie M.",
          "Liss"
        ],
        [
          "Melissa",
          "Milanovic"
        ],
        [
          "Christopher R.",
          "Bowie"
        ],
        [
          "Visar",
          "Berisha"
        ]
      ],
      "title": "Objective Assessment of Social Skills Using Automated Language Analysis for Identification of Schizophrenia and Bipolar Disorder",
      "original": "2960",
      "page_count": 5,
      "order": 299,
      "p1": "1433",
      "pn": "1437",
      "abstract": [
        "Several studies have shown that speech and language features, automatically\nextracted from clinical interviews or spontaneous discourse, have diagnostic\nvalue for mental disorders such as schizophrenia and bipolar disorder.\nThey typically make use of a large feature set to train a classifier\nfor distinguishing between two groups of interest, i.e. a clinical\nand control group. However, a purely data-driven approach runs the\nrisk of overfitting to a particular data set, especially when sample\nsizes are limited. Here, we first down-select the set of language features\nto a small subset that is related to a well-validated test of functional\nability, the Social Skills Performance Assessment (SSPA). This helps\nestablish the concurrent validity of the selected features. We use\nonly these features to train a simple classifier to distinguish between\ngroups of interest. Linear regression reveals that a subset of language\nfeatures can effectively model the SSPA, with a correlation coefficient\nof 0.75. Furthermore, the same feature set can be used to build a strong\nbinary classifier to distinguish between healthy controls and a clinical\ngroup (AUC = 0.96) and also between patients within the clinical group\nwith schizophrenia and bipolar I disorder (AUC = 0.83).\n"
      ],
      "doi": "10.21437/Interspeech.2019-2960",
      "author_area_id": 3,
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "matton19_interspeech": {
      "authors": [
        [
          "Katie",
          "Matton"
        ],
        [
          "Melvin G.",
          "McInnis"
        ],
        [
          "Emily Mower",
          "Provost"
        ]
      ],
      "title": "Into the Wild: Transitioning from Recognizing Mood in Clinical Interactions to Personal Conversations for Individuals with Bipolar Disorder",
      "original": "2698",
      "page_count": 5,
      "order": 300,
      "p1": "1438",
      "pn": "1442",
      "abstract": [
        "Bipolar Disorder, a mood disorder with recurrent mania and depression,\nrequires ongoing monitoring and specialty management. Current monitoring\nstrategies are clinically-based, engaging highly specialized medical\nprofessionals who are becoming increasingly scarce. Automatic speech-based\nmonitoring via smartphones has the potential to augment clinical monitoring\nby providing inexpensive and unobtrusive measurements of a patient&#8217;s\ndaily life. The success of such an approach is contingent on the ability\nto successfully utilize &#8220;in-the-wild&#8221; data. However, most\nexisting work on automatic mood detection uses datasets collected in\nclinical or laboratory settings. This study presents experiments in\nautomatically detecting depression severity in individuals with Bipolar\nDisorder using data derived from clinical interviews and from personal\nconversations. We find that mood assessment is more accurate using\ndata collected from clinical interactions, in part because of their\nhighly structured nature. We demonstrate that although the features\nthat are most effective in clinical interactions do not extend well\nto personal conversational data, we can identify alternative features\nrelevant in personal conversational speech to detect mood symptom severity.\nOur results highlight the challenges unique to working with &#8220;in-the-wild&#8221;\ndata, providing insight into the degree to which the predictive ability\nof speech features is preserved outside of a clinical interview.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2698",
      "author_area_id": 3,
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "rohanian19_interspeech": {
      "authors": [
        [
          "Morteza",
          "Rohanian"
        ],
        [
          "Julian",
          "Hough"
        ],
        [
          "Matthew",
          "Purver"
        ]
      ],
      "title": "Detecting Depression with Word-Level Multimodal Fusion",
      "original": "2283",
      "page_count": 5,
      "order": 301,
      "p1": "1443",
      "pn": "1447",
      "abstract": [
        "Semi-structured clinical interviews are frequently used diagnostic\ntools for identifying depression during an assessment phase. In addition\nto the lexical content of a patient&#8217;s responses, multimodal cues\nconcurrent with the responses are indicators of their motor and cognitive\nstate, including those derivable from their voice quality and gestural\nbehaviour. In this paper, we use information from different modalities\nin order to train a classifier capable of detecting the binary state\nof a subject (clinically depressed or not), as well as the level of\ntheir depression. We propose a model that is able to perform modality\nfusion incrementally after each word in an utterance using a time-dependent\nrecurrent approach in a deep learning set-up. To mitigate noisy modalities,\nwe utilize fusion gates that control the degree to which the audio\nor visual modality contributes to the final prediction. Our results\nshow the effectiveness of word-level multimodal fusion, achieving state-of-the-art\nresults in depression detection and outperforming early feature-level\nand late fusion techniques.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2283",
      "author_area_id": 3,
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "espywilson19_interspeech": {
      "authors": [
        [
          "Carol",
          "Espy-Wilson"
        ],
        [
          "Adam C.",
          "Lammert"
        ],
        [
          "Nadee",
          "Seneviratne"
        ],
        [
          "Thomas F.",
          "Quatieri"
        ]
      ],
      "title": "Assessing Neuromotor Coordination in Depression Using Inverted Vocal Tract Variables",
      "original": "1815",
      "page_count": 5,
      "order": 302,
      "p1": "1448",
      "pn": "1452",
      "abstract": [
        "Speech articulation is a complex activity that requires finely timed\ncoordination across articulators, i.e., tongue, jaw, lips, and velum.\nIn a depressed state involving psychomotor retardation, this coordination\nchanges and in turn modifies the perceived speech signal. In previous\nwork, we used the correlation structure of formant trajectories as\na proxy for articulatory coordination, from which features were derived\nfor predicting the degree of depression. Ideally, however, we seek\ncoordination of the actual articulators using characteristics such\nas the degree and place of tongue constriction, often referred to as\na  tract variable (TV). In this paper, applying a novel articulatory\ninversion process, we investigate the relation between correlation\nstructure of formant tracks versus that of TVs. We show on a pilot\ndepressed/control dataset that, with the same number of variables,\nTV coordination-based features, although with some characteristics\nsimilar to their counterpart, outperform the corresponding formant\ntrack correlation features in detection of the depressed state. We\nspeculate on the latent information being captured by TVs that is not\npresent in formants.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1815",
      "author_area_id": 3,
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "paul19b_interspeech": {
      "authors": [
        [
          "Shachi",
          "Paul"
        ],
        [
          "Rahul",
          "Goel"
        ],
        [
          "Dilek",
          "Hakkani-T\u00fcr"
        ]
      ],
      "title": "Towards Universal Dialogue Act Tagging for Task-Oriented Dialogues",
      "original": "1866",
      "page_count": 5,
      "order": 303,
      "p1": "1453",
      "pn": "1457",
      "abstract": [
        "Machine learning approaches for building task-oriented dialogue systems\nrequire large conversational datasets with labels to train on. We are\ninterested in building task-oriented dialogue systems from human-human\nconversations, which may be available in ample amounts in existing\ncustomer care center logs or can be collected from crowd workers. Annotating\nthese datasets can be prohibitively expensive. Recently multiple annotated\ntask-oriented human-machine dialogue datasets have been released, however\ntheir annotation schema varies across different collections, even for\nwell-defined categories such as dialogue acts (DAs). We propose a Universal\nDA schema for task-oriented dialogues and align existing annotated\ndatasets with our schema. Our aim is to train a Universal DA tagger\n(U-DAT) for task-oriented dialogues and use it for tagging human-human\nconversations. We investigate multiple datasets, propose manual and\nautomated approaches for aligning the different schema, and present\nresults on a target corpus of human-human dialogues. In unsupervised\nlearning experiments we achieve an F1 score of 54.1% on system turns\nin human-human dialogues. In a semi-supervised setup, the F1 score\nincreases to 57.7% which would otherwise require at least 1.7K manually\nannotated turns. For new domains, we show further improvements when\nunlabeled or labeled target domain data is available.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1866",
      "author_area_id": 11,
      "author_area_label": "Spoken dialog systems and conversational analysis"
    },
    "goel19_interspeech": {
      "authors": [
        [
          "Rahul",
          "Goel"
        ],
        [
          "Shachi",
          "Paul"
        ],
        [
          "Dilek",
          "Hakkani-T\u00fcr"
        ]
      ],
      "title": "HyST: A Hybrid Approach for Flexible and Accurate Dialogue State Tracking",
      "original": "1863",
      "page_count": 5,
      "order": 304,
      "p1": "1458",
      "pn": "1462",
      "abstract": [
        "Recent works on end-to-end trainable neural network based approaches\nhave demonstrated state-of-the-art results on dialogue state tracking.\nThe best performing approaches estimate a probability distribution\nover all possible slot values. However, these approaches do not scale\nfor large value sets commonly present in real-life applications and\nare not ideal for tracking slot values that were not observed in the\ntraining set. To tackle these issues, candidate-generation-based approaches\nhave been proposed. These approaches estimate a set of values that\nare possible at each turn based on the conversation history and/or\nlanguage understanding outputs, and hence enable state tracking over\nunseen values and large value sets however, they fall short in terms\nof performance in comparison to the first group. In this work, we analyze\nthe performance of these two alternative dialogue state tracking methods,\nand present a hybrid approach (HyST) which learns the appropriate method\nfor each slot type. To demonstrate the effectiveness of HyST on a rich-set\nof slot types, we experiment with the recently released MultiWOZ-2.0\nmulti-domain, task-oriented dialogue-dataset. Our experiments show\nthat HyST scales to multi-domain applications. Our best performing\nmodel results in a relative improvement of 24% and 10% over the previous\nSOTA and our best baseline respectively.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1863",
      "author_area_id": 11,
      "author_area_label": "Spoken dialog systems and conversational analysis"
    },
    "martinek19_interspeech": {
      "authors": [
        [
          "Ji\u0159\u00ed",
          "Mart\u00ednek"
        ],
        [
          "Pavel",
          "Kr\u00e1l"
        ],
        [
          "Ladislav",
          "Lenc"
        ],
        [
          "Christophe",
          "Cerisara"
        ]
      ],
      "title": "Multi-Lingual Dialogue Act Recognition with Deep Learning Methods",
      "original": "1691",
      "page_count": 5,
      "order": 305,
      "p1": "1463",
      "pn": "1467",
      "abstract": [
        "This paper deals with multi-lingual dialogue act (DA) recognition.\nThe proposed approaches are based on deep neural networks and use word2vec\nembeddings for word representation. Two multi-lingual models are proposed\nfor this task. The first approach uses one general model trained on\nthe embeddings from all available languages. The second method trains\nthe model on a single pivot language and a linear transformation method\nis used to project other languages onto the pivot language. The popular\nconvolutional neural network and LSTM architectures with different\nset-ups are used as classifiers. To the best of our knowledge this\nis the first attempt at multi-lingual DA recognition using neural networks.\nThe multi-lingual models are validated experimentally on two languages\nfrom the Verbmobil corpus.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1691",
      "author_area_id": 11,
      "author_area_label": "Spoken dialog systems and conversational analysis"
    },
    "chao19_interspeech": {
      "authors": [
        [
          "Guan-Lin",
          "Chao"
        ],
        [
          "Ian",
          "Lane"
        ]
      ],
      "title": "BERT-DST: Scalable End-to-End Dialogue State Tracking with Bidirectional Encoder Representations from Transformer",
      "original": "1355",
      "page_count": 5,
      "order": 306,
      "p1": "1468",
      "pn": "1472",
      "abstract": [
        "An important yet rarely tackled problem in dialogue state tracking\n(DST) is scalability for dynamic ontology ( e.g., movie, restaurant)\nand unseen slot values. We focus on a specific condition, where the\nontology is unknown to the state tracker, but the target slot value\n(except for  none and  dontcare), possibly unseen during training,\ncan be found as word segment in the dialogue context. Prior approaches\noften rely on candidate generation from n-gram enumeration or slot\ntagger outputs, which can be inefficient or suffer from error propagation.\nWe propose BERT-DST, an end-to-end dialogue state tracker which directly\nextracts slot values from the dialogue context. We use BERT as dialogue\ncontext encoder whose contextualized language representations are suitable\nfor scalable DST to identify slot values from their semantic context.\nFurthermore, we employ encoder parameter sharing across all slots with\ntwo advantages: (1) Number of parameters does not grow linearly with\nthe ontology. (2) Language representation knowledge can be transferred\namong slots. Empirical evaluation shows BERT-DST with cross-slot parameter\nsharing outperforms prior work on the benchmark scalable DST datasets\nSim-M and Sim-R, and achieves competitive performance on the standard\nDSTC2 and WOZ 2.0 datasets.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1355",
      "author_area_id": 11,
      "author_area_label": "Spoken dialog systems and conversational analysis"
    },
    "griol19_interspeech": {
      "authors": [
        [
          "David",
          "Griol"
        ],
        [
          "Zoraida",
          "Callejas"
        ]
      ],
      "title": "Discovering Dialog Rules by Means of an Evolutionary Approach",
      "original": "2230",
      "page_count": 5,
      "order": 307,
      "p1": "1473",
      "pn": "1477",
      "abstract": [
        "Designing the rules for the dialog management process is one of the\nmost resources-consuming tasks when developing a dialog system. Although\nstatistical approaches to dialog management are becoming mainstream\nin research and industrial contexts, still many systems are being developed\nfollowing the rule-based or hybrid paradigms. For example, when developers\nrequire deterministic system responses to keep total control on the\ndecisions made by the system, or because the infrastructure employed\nis designed for rule-based systems using technologies currently used\nin commercial platforms. In this paper, we propose the use of evolutionary\nalgorithms to automatically obtain the dialog rules that are implicit\nin a dialog corpus. Our proposal makes it possible to exploit the benefits\nof statistical approaches to build rule-based systems. Our proposal\nhas been evaluated with a practical spoken dialog system, for which\nwe have automatically obtained a set of fuzzy rules to successfully\nmanage the dialog.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2230"
    },
    "chen19c_interspeech": {
      "authors": [
        [
          "Xi C.",
          "Chen"
        ],
        [
          "Adithya",
          "Sagar"
        ],
        [
          "Justine T.",
          "Kao"
        ],
        [
          "Tony Y.",
          "Li"
        ],
        [
          "Christopher",
          "Klein"
        ],
        [
          "Stephen",
          "Pulman"
        ],
        [
          "Ashish",
          "Garg"
        ],
        [
          "Jason D.",
          "Williams"
        ]
      ],
      "title": "Active Learning for Domain Classification in a Commercial Spoken Personal Assistant",
      "original": "1315",
      "page_count": 5,
      "order": 308,
      "p1": "1478",
      "pn": "1482",
      "abstract": [
        "We describe a method for selecting relevant new training data for the\nLSTM-based domain selection component of our personal assistant system.\nAdding more annotated training data for any ML system typically improves\naccuracy, but only if it provides examples not already adequately covered\nin the existing data. However, obtaining, selecting, and labeling relevant\ndata is expensive. This work presents a simple technique that automatically\nidentifies new helpful examples suitable for human annotation. Our\nexperimental results show that the proposed method, compared with random-selection\nand entropy-based methods, leads to higher accuracy improvements given\na fixed annotation budget. Although developed and tested in the setting\nof a commercial intelligent assistant, the technique is of wider applicability.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1315",
      "author_area_id": 11,
      "author_area_label": "Spoken dialog systems and conversational analysis"
    },
    "sadjadi19_interspeech": {
      "authors": [
        [
          "Seyed Omid",
          "Sadjadi"
        ],
        [
          "Craig",
          "Greenberg"
        ],
        [
          "Elliot",
          "Singer"
        ],
        [
          "Douglas",
          "Reynolds"
        ],
        [
          "Lisa",
          "Mason"
        ],
        [
          "Jaime",
          "Hernandez-Cordero"
        ]
      ],
      "title": "The 2018 NIST Speaker Recognition Evaluation",
      "original": "1351",
      "page_count": 5,
      "order": 309,
      "p1": "1483",
      "pn": "1487",
      "abstract": [
        "In 2018, the U.S. National Institute of Standards and Technology (NIST)\nconducted the most recent in an ongoing series of speaker recognition\nevaluations (SRE). SRE18 was organized in a similar manner to SRE16,\nfocusing on speaker detection over conversational telephony speech\n(CTS) collected outside north America. SRE18 also featured several\nnew aspects including: two new data domains, namely voice over internet\nprotocol (VoIP) and audio extracted from  amateur online videos (AfV),\nas well as a new language (Tunisian Arabic). A total of 78 organizations\n(forming 48 teams) from academia and industry participated in SRE18\nand submitted 129 valid system outputs under  fixed and  open training\nconditions first introduced in SRE16. This paper presents an overview\nof the evaluation and several analyses of system performance for all\nprimary conditions in SRE18. The evaluation results suggest that 1)\nspeaker recognition on AfV was more challenging than on telephony data,\n2) speaker representations (aka embeddings) extracted using end-to-end\nneural network frameworks were most effective, 3) top performing systems\nexhibited similar performance, and 4) greatest performance improvements\nwere largely due to data augmentation, use of extended and more complex\nmodels for data representation, as well as effective use of the provided\ndevelopment sets.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1351",
      "author_area_id": 4,
      "author_area_label": "Speaker and Language Identification"
    },
    "villalba19_interspeech": {
      "authors": [
        [
          "Jes\u00fas",
          "Villalba"
        ],
        [
          "Nanxin",
          "Chen"
        ],
        [
          "David",
          "Snyder"
        ],
        [
          "Daniel",
          "Garcia-Romero"
        ],
        [
          "Alan",
          "McCree"
        ],
        [
          "Gregory",
          "Sell"
        ],
        [
          "Jonas",
          "Borgstrom"
        ],
        [
          "Fred",
          "Richardson"
        ],
        [
          "Suwon",
          "Shon"
        ],
        [
          "Fran\u00e7ois",
          "Grondin"
        ],
        [
          "R\u00e9da",
          "Dehak"
        ],
        [
          "Leibny Paola",
          "Garc\u00eda-Perera"
        ],
        [
          "Daniel",
          "Povey"
        ],
        [
          "Pedro A.",
          "Torres-Carrasquillo"
        ],
        [
          "Sanjeev",
          "Khudanpur"
        ],
        [
          "Najim",
          "Dehak"
        ]
      ],
      "title": "State-of-the-Art Speaker Recognition for Telephone and Video Speech: The JHU-MIT Submission for NIST SRE18",
      "original": "2713",
      "page_count": 5,
      "order": 310,
      "p1": "1488",
      "pn": "1492",
      "abstract": [
        "We present a condensed description of the joint effort of JHU-CLSP,\nJHU-HLTCOE, MIT-LL., MIT CSAIL and LSE-EPITA for NIST SRE18. All the\ndeveloped systems consisted of x-vector/i-vector embeddings with some\nflavor of PLDA backend. Very deep x-vector architectures &#8212; Extended\nand Factorized TDNN, and ResNets &#8212; clearly outperformed shallower\nx-vectors and i-vectors. The systems were tailored to the video (VAST)\nor to the telephone (CMN2) condition. The VAST data was challenging,\nyielding 4 times worse performance than other video based datasets\nlike Speakers in the Wild. We were able to calibrate the VAST data\nwith very few development trials by using careful adaptation and score\nnormalization methods. The VAST primary fusion yielded EER=10.18% and\nCprimary=0.431. By improving calibration in post-eval, we reached Cprimary=0.369.\nIn CMN2, we used unsupervised SPLDA adaptation based on agglomerative\nclustering and score normalization to correct the domain shift between\nEnglish and Tunisian Arabic models. The CMN2 primary fusion yielded\nEER=4.5% and Cprimary=0.313. Extended TDNN x-vector was the best single\nsystem obtaining EER=11.1% and Cprimary=0.452 in VAST; and 4.95% and\n0.354 in CMN2.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2713",
      "author_area_id": 4,
      "author_area_label": "Speaker and Language Identification"
    },
    "garciaromero19_interspeech": {
      "authors": [
        [
          "Daniel",
          "Garcia-Romero"
        ],
        [
          "David",
          "Snyder"
        ],
        [
          "Gregory",
          "Sell"
        ],
        [
          "Alan",
          "McCree"
        ],
        [
          "Daniel",
          "Povey"
        ],
        [
          "Sanjeev",
          "Khudanpur"
        ]
      ],
      "title": "x-Vector DNN Refinement with Full-Length Recordings for Speaker Recognition",
      "original": "2205",
      "page_count": 4,
      "order": 311,
      "p1": "1493",
      "pn": "1496",
      "abstract": [
        "State-of-the-art text-independent speaker recognition systems for long\nrecordings (a few minutes) are based on deep neural network (DNN) speaker\nembeddings. Current implementations of this paradigm use short speech\nsegments (a few seconds) to train the DNN. This introduces a mismatch\nbetween training and inference when extracting embeddings for long\nduration recordings. To address this, we present a DNN refinement approach\nthat updates a subset of the DNN parameters with full recordings to\nreduce this mismatch. At the same time, we also modify the DNN architecture\nto produce embeddings optimized for cosine distance scoring. This is\naccomplished using a large-margin strategy with angular softmax. Experimental\nvalidation shows that our approach is capable of producing embeddings\nthat achieve record performance on the SITW benchmark.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2205"
    },
    "lee19_interspeech": {
      "authors": [
        [
          "Kong Aik",
          "Lee"
        ],
        [
          "Ville",
          "Hautam\u00e4ki"
        ],
        [
          "Tomi H.",
          "Kinnunen"
        ],
        [
          "Hitoshi",
          "Yamamoto"
        ],
        [
          "Koji",
          "Okabe"
        ],
        [
          "Ville",
          "Vestman"
        ],
        [
          "Jing",
          "Huang"
        ],
        [
          "Guohong",
          "Ding"
        ],
        [
          "Hanwu",
          "Sun"
        ],
        [
          "Anthony",
          "Larcher"
        ],
        [
          "Rohan Kumar",
          "Das"
        ],
        [
          "Haizhou",
          "Li"
        ],
        [
          "Mickael",
          "Rouvier"
        ],
        [
          "Pierre-Michel",
          "Bousquet"
        ],
        [
          "Wei",
          "Rao"
        ],
        [
          "Qing",
          "Wang"
        ],
        [
          "Chunlei",
          "Zhang"
        ],
        [
          "Fahimeh",
          "Bahmaninezhad"
        ],
        [
          "H\u00e9ctor",
          "Delgado"
        ],
        [
          "Massimiliano",
          "Todisco"
        ]
      ],
      "title": "I4U Submission to NIST SRE 2018: Leveraging from a Decade of Shared Experiences",
      "original": "1533",
      "page_count": 5,
      "order": 312,
      "p1": "1497",
      "pn": "1501",
      "abstract": [
        "The I4U consortium was established to facilitate a joint entry to NIST\nspeaker recognition evaluations (SRE). The latest edition of such joint\nsubmission was in SRE 2018, in which the I4U submission was among the\nbest-performing systems. SRE&#8217;18 also marks the 10-year anniversary\nof I4U consortium into NIST SRE series of evaluation. The primary objective\nof the current paper is to summarize the results and lessons learned\nbased on the twelve sub-systems and their fusion submitted to SRE&#8217;18.\nIt is also our intention to present a shared view on the advancements,\nprogresses, and major paradigm shifts that we have witnessed as an\nSRE participant in the past decade from SRE&#8217;08 to SRE&#8217;18.\nIn this regard, we have seen, among others, a paradigm shift from supervector\nrepresentation to deep speaker embedding, and a switch of research\nchallenge from channel compensation to domain adaptation.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1533",
      "author_area_id": 4,
      "author_area_label": "Speaker and Language Identification"
    },
    "khoury19_interspeech": {
      "authors": [
        [
          "Elie",
          "Khoury"
        ],
        [
          "Khaled",
          "Lakhdhar"
        ],
        [
          "Andrew",
          "Vaughan"
        ],
        [
          "Ganesh",
          "Sivaraman"
        ],
        [
          "Parav",
          "Nagarsheth"
        ]
      ],
      "title": "Pindrop Labs&#8217; Submission to the First Multi-Target Speaker Detection and Identification Challenge",
      "original": "3179",
      "page_count": 4,
      "order": 313,
      "p1": "1502",
      "pn": "1505",
      "abstract": [
        "This paper summarizes Pindrop Labs&#8217; submission to the multi-target\nspeaker detection and identification challenge evaluation (MCE 2018).\nThe MCE challenge is geared towards detecting blacklisted speakers\n(fraudsters) in the context of call centers. Particularly, it aims\nto answer the following two questions: Is the speaker of the test utterance\non the blacklist? If so, which speaker is it among the blacklisted\nspeakers? While one single system can answer both questions, this work\nlooks at them as two separate tasks: blacklist detection and closed-set\nidentification. The former is addressed using four different systems\nincluding probabilistic linear discriminant analysis (PLDA), two deep\nneural network (DNN) based systems, and a simple system based on cosine\nsimilarity and logistic regression. The latter is addressed by combining\nPLDA and neural network based systems. The proposed system was the\nbest performing system at the challenge on both tasks, reducing the\nblacklist detection error (Top-S EER) by 31.9% and the identification\nerror (Top-1 EER) by 46.4% over the MCE baseline on the evaluation\ndata.\n"
      ],
      "doi": "10.21437/Interspeech.2019-3179",
      "author_area_id": 4,
      "author_area_label": "Speaker and Language Identification"
    },
    "garciaromero19b_interspeech": {
      "authors": [
        [
          "Daniel",
          "Garcia-Romero"
        ],
        [
          "David",
          "Snyder"
        ],
        [
          "Shinji",
          "Watanabe"
        ],
        [
          "Gregory",
          "Sell"
        ],
        [
          "Alan",
          "McCree"
        ],
        [
          "Daniel",
          "Povey"
        ],
        [
          "Sanjeev",
          "Khudanpur"
        ]
      ],
      "title": "Speaker Recognition Benchmark Using the CHiME-5 Corpus",
      "original": "2174",
      "page_count": 5,
      "order": 314,
      "p1": "1506",
      "pn": "1510",
      "abstract": [
        "In this paper, we introduce a speaker recognition benchmark derived\nfrom the publicly-available CHiME-5 corpus. Our goal is to foster research\nthat tackles the challenging artifacts introduced by far-field multi-speaker\nrecordings of naturally occurring spoken interactions. The benchmark\ncomprises four tasks that involve enrollment and test conditions with\nsingle-speaker and/or multi-speaker recordings. Additionally, it supports\nperformance comparisons between close-talking vs distant/far-field\nmicrophone recordings, and single-microphone vs microphone-array approaches.\nWe validate the evaluation design with a single-microphone state-of-the-art\nDNN speaker recognition and diarization system (that we are making\npublicly available). The results show that the proposed tasks are very\nchallenging, and can be used to quantify the performance gap due to\nthe degradations present in far-field multi-speaker recordings.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2174",
      "author_area_id": 4,
      "author_area_label": "Speaker and Language Identification"
    },
    "ayllon19_interspeech": {
      "authors": [
        [
          "David",
          "Ayll\u00f3n"
        ],
        [
          "H\u00e9ctor A.",
          "S\u00e1nchez-Hevia"
        ],
        [
          "Carol",
          "Figueroa"
        ],
        [
          "Pierre",
          "Lanchantin"
        ]
      ],
      "title": "Investigating the Effects of Noisy and Reverberant Speech in Text-to-Speech Systems",
      "original": "3104",
      "page_count": 5,
      "order": 315,
      "p1": "1511",
      "pn": "1515",
      "abstract": [
        "The quality of the voices synthesized by a Text-to-Speech (TTS) system\ndepends on the quality of the training data. In real case scenario\nof TTS personalization from user&#8217;s voice recordings, the latter\nare usually affected by noise and reverberation. Speech enhancement\ncan be useful to clean the corrupted speech but it is necessary to\nunderstand the effects that noise and reverberation have on the different\nstatistical models that compose the TTS system. In this work we perform\na thorough study of how noise and reverberation impact the acoustic\nand duration models of the TTS system. We also evaluate the effectiveness\nof time-frequency masking for cleaning the training data. Objective\nand subjective evaluations reveal that under normal recording scenarios\nnoise leads to a higher degradation than reverberation in terms of\nnaturalness of the synthesized speech.\n"
      ],
      "doi": "10.21437/Interspeech.2019-3104",
      "author_area_id": 7,
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "kuo19_interspeech": {
      "authors": [
        [
          "F.-Y.",
          "Kuo"
        ],
        [
          "I.C.",
          "Ouyang"
        ],
        [
          "S.",
          "Aryal"
        ],
        [
          "Pierre",
          "Lanchantin"
        ]
      ],
      "title": "Selection and Training Schemes for Improving TTS Voice Built on Found Data",
      "original": "2816",
      "page_count": 5,
      "order": 316,
      "p1": "1516",
      "pn": "1520",
      "abstract": [
        "This work investigates different selection and training schemes to\nimprove the naturalness of synthesized text-to-speech voices built\non found data. The approach outlined in this paper examines the combinations\nof different metrics to detect and reject segments of training data\nthat can degrade the performance of the system. We conducted a series\nof objective and subjective experiments on two 24-hour single-speaker\ncorpuses of found data collected from diverse sources. We show that\nusing an even smaller, yet carefully selected, set of data can lead\nto a text-to-speech system able to generate more natural speech than\na system trained on the complete dataset. Moreover, we show that training\nthe system by fine-tuning from the system trained on the whole dataset\nleads to additional improvement in naturalness by allowing a more aggressive\nselection of training data.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2816",
      "author_area_id": 7,
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "braude19_interspeech": {
      "authors": [
        [
          "David A.",
          "Braude"
        ],
        [
          "Matthew P.",
          "Aylett"
        ],
        [
          "Caoimh\u00edn",
          "Laoide-Kemp"
        ],
        [
          "Simone",
          "Ashby"
        ],
        [
          "Kristen M.",
          "Scott"
        ],
        [
          "Brian \u00d3",
          "Raghallaigh"
        ],
        [
          "Anna",
          "Braudo"
        ],
        [
          "Alex",
          "Brouwer"
        ],
        [
          "Adriana",
          "Stan"
        ]
      ],
      "title": "All Together Now: The Living Audio Dataset",
      "original": "2448",
      "page_count": 5,
      "order": 317,
      "p1": "1521",
      "pn": "1525",
      "abstract": [
        "The ongoing focus in speech technology research on machine learning\nbased approaches leaves the community hungry for data. However, datasets\ntend to be recorded once and then released, sometimes behind registration\nrequirements or paywalls. In this paper we describe our Living Audio\nDataset. The aim is to provide audio data that is in the public domain,\nmultilingual, and expandable by communities. We discuss the role of\nlinguistic resources, given the success of systems such as Tacotron\nwhich use direct text-to-speech mappings, and consider how data provenance\ncould be built into such resources. So far the data has been collected\nfor TTS purposes, however, it is also suitable for ASR. At the time\nof publication audio resources already exist for Dutch, R.P. English,\nIrish, and Russian.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2448",
      "author_area_id": 7,
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "zen19_interspeech": {
      "authors": [
        [
          "Heiga",
          "Zen"
        ],
        [
          "Viet",
          "Dang"
        ],
        [
          "Rob",
          "Clark"
        ],
        [
          "Yu",
          "Zhang"
        ],
        [
          "Ron J.",
          "Weiss"
        ],
        [
          "Ye",
          "Jia"
        ],
        [
          "Zhifeng",
          "Chen"
        ],
        [
          "Yonghui",
          "Wu"
        ]
      ],
      "title": "LibriTTS: A Corpus Derived from LibriSpeech for Text-to-Speech",
      "original": "2441",
      "page_count": 5,
      "order": 318,
      "p1": "1526",
      "pn": "1530",
      "abstract": [
        "This paper introduces a new speech corpus called &#8220;LibriTTS&#8221;\ndesigned for text-to-speech use. It is derived from the original audio\nand text materials of the LibriSpeech corpus, which has been used for\ntraining and evaluating automatic speech recognition systems. The new\ncorpus inherits desired properties of the LibriSpeech corpus while\naddressing a number of issues which make LibriSpeech less than ideal\nfor text-to-speech work. The released corpus consists of 585 hours\nof speech data at 24kHz sampling rate from 2,456 speakers and the corresponding\ntexts. Experimental results show that neural end-to-end TTS models\ntrained from the LibriTTS corpus achieved above 4.0 in mean opinion\nscores in naturalness in five out of six evaluation speakers. The corpus\nis freely available for download from http://www.openslr.org/60/.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2441",
      "author_area_id": 7,
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "shamsi19_interspeech": {
      "authors": [
        [
          "Meysam",
          "Shamsi"
        ],
        [
          "Damien",
          "Lolive"
        ],
        [
          "Nelly",
          "Barbot"
        ],
        [
          "Jonathan",
          "Chevelu"
        ]
      ],
      "title": "Corpus Design Using Convolutional Auto-Encoder Embeddings for Audio-Book Synthesis",
      "original": "2190",
      "page_count": 5,
      "order": 319,
      "p1": "1531",
      "pn": "1535",
      "abstract": [
        "In this study, we propose an approach for script selection in order\nto design TTS speech corpora. A Deep Convolutional Neural Network (DCNN)\nis used to project linguistic information to an embedding space. The\nembedded representation of the corpus is then fed to a selection process\nto extract a subset of utterances which offers a good linguistic coverage\nwhile tending to limit the linguistic unit repetition. We present two\nselection processes: a clustering approach based on utterance distance\nand another method that tends to reach a target distribution of linguistic\nevents. We compare the synthetic signal quality of the proposed methods\nto state of art methods objectively and subjectively. The subjective\nand objective measures confirm the performance of the proposed methods\nin order to design speech corpora with better synthetic speech quality.\nThe perceptual test shows that our TTS global cost can be used as an\nalternative to synthetic overall quality.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2190",
      "author_area_id": 7,
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "hojo19_interspeech": {
      "authors": [
        [
          "Nobukatsu",
          "Hojo"
        ],
        [
          "Noboru",
          "Miyazaki"
        ]
      ],
      "title": "Evaluating Intention Communication by TTS Using Explicit Definitions of Illocutionary Act Performance",
      "original": "2188",
      "page_count": 5,
      "order": 320,
      "p1": "1536",
      "pn": "1540",
      "abstract": [
        "Text-to-speech (TTS) synthesis systems have been evaluated with respect\nto attributes such as quality, naturalness and intelligibility. However,\nan evaluation protocol with respect to communication of intentions\nhas not yet been established. Evaluating this sometimes produce unreliable\nresults because participants can misinterpret definitions of intentions.\nThis misinterpretation is caused by the colloquial and implicit description\nof intentions. To address this problem, this work explicitly defines\neach intention following theoretical definitions, &#8220;felicity conditions&#8221;,\nin speech-act theory. We define the communication of each intention\nwith one to four necessary and sufficient conditions to be satisfied.\nIn listening tests, participants rated whether each condition was satisfied\nor not. We compared the proposed protocol with the conventional baseline\nusing four different voice conditions; neutral TTS, conversational\nTTS w/ and w/o intention inputs, and recorded speech. The experimental\nresults with 10 participants showed that the proposed protocol produced\nsmaller within-group variation and larger between-group variation.\nThese results indicate that the proposed protocol can be used to evaluate\nintention communication with higher inter-rater reliability and sensitivity.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2188",
      "author_area_id": 7,
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "lo19_interspeech": {
      "authors": [
        [
          "Chen-Chou",
          "Lo"
        ],
        [
          "Szu-Wei",
          "Fu"
        ],
        [
          "Wen-Chin",
          "Huang"
        ],
        [
          "Xin",
          "Wang"
        ],
        [
          "Junichi",
          "Yamagishi"
        ],
        [
          "Yu",
          "Tsao"
        ],
        [
          "Hsin-Min",
          "Wang"
        ]
      ],
      "title": "MOSNet: Deep Learning-Based Objective Assessment for Voice Conversion",
      "original": "2003",
      "page_count": 5,
      "order": 321,
      "p1": "1541",
      "pn": "1545",
      "abstract": [
        "Existing objective evaluation metrics for voice conversion (VC) are\nnot always correlated with human perception. Therefore, training VC\nmodels with such criteria may not effectively improve naturalness and\nsimilarity of converted speech. In this paper, we propose deep learning-based\nassessment models to predict human ratings of converted speech. We\nadopt the convolutional and recurrent neural network models to build\na mean opinion score (MOS) predictor, termed as MOSNet. The proposed\nmodels are tested on large-scale listening test results of the Voice\nConversion Challenge (VCC) 2018. Experimental results show that the\npredicted scores of the proposed MOSNet are highly correlated with\nhuman MOS ratings at the system level while being fairly correlated\nwith human MOS ratings at the utterance level. Meanwhile, we have modified\nMOSNet to predict the similarity scores, and the preliminary results\nshow that the predicted scores are also fairly correlated with human\nratings. These results confirm that the proposed models could be used\nas a computational evaluator to measure the MOS of VC systems to reduce\nthe need for expensive human rating.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2003",
      "author_area_id": 7,
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "fong19_interspeech": {
      "authors": [
        [
          "Jason",
          "Fong"
        ],
        [
          "Pilar Oplustil",
          "Gallegos"
        ],
        [
          "Zack",
          "Hodari"
        ],
        [
          "Simon",
          "King"
        ]
      ],
      "title": "Investigating the Robustness of Sequence-to-Sequence Text-to-Speech Models to Imperfectly-Transcribed Training Data",
      "original": "1824",
      "page_count": 5,
      "order": 322,
      "p1": "1546",
      "pn": "1550",
      "abstract": [
        "Sequence-to-sequence (S2S) text-to-speech (TTS) models can synthesise\nhigh quality speech when large amounts of annotated training data are\navailable. Transcription errors exist in all data and are especially\nprevalent in found data such as audiobooks. In previous generations\nof TTS technology, alignment using Hidden Markov Models (HMMs) was\nwidely used to identify and eliminate bad data. In S2S models, the\nuse of attention replaces HMM-based alignment, and there is no explicit\nmechanism for removing bad data. It is not yet understood how such\nmodels deal with transcription errors in the training data.<br style=\"mso-data-placement:same-cell;\"\n/><br style=\"mso-data-placement:same-cell;\" />\n  We evaluate the quality\nof speech from S2S-TTS models when trained on data with imperfect transcripts,\nsimulated using corruption, or provided by an Automatic Speech Recogniser\n(ASR).We find that attention can skip over extraneous words in the\ninput sequence, providing robustness to insertion errors. But substitutions\nand deletions pose a problem because there is no ground truth input\navailable to align to the ground truth acoustics during teacher-forced\ntraining. We conclude that S2S-TTS systems are only partially robust\nto training on imperfectly-transcribed data and further work is needed.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1824",
      "author_area_id": 7,
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "govender19_interspeech": {
      "authors": [
        [
          "Avashna",
          "Govender"
        ],
        [
          "Anita E.",
          "Wagner"
        ],
        [
          "Simon",
          "King"
        ]
      ],
      "title": "Using Pupil Dilation to Measure Cognitive Load When Listening to Text-to-Speech in Quiet and in Noise",
      "original": "1783",
      "page_count": 5,
      "order": 323,
      "p1": "1551",
      "pn": "1555",
      "abstract": [
        "With increased use of text-to-speech (TTS) systems in real-world applications,\nevaluating how such systems influence the human cognitive processing\nsystem becomes important. Particularly in situations where cognitive\nload is high, there may be negative implications such as fatigue. For\nexample, noisy situations generally require the listener to exert increased\nmental effort. A better understanding of this could eventually suggest\nnew ways of generating synthetic speech that demands low cognitive\nload. In our previous study, pupil dilation was used as an index of\ncognitive effort. Pupil dilation was shown to be sensitive to the quality\nof synthetic speech, but there were some uncertainties regarding exactly\nwhat was being measured. The current study resolves some of those uncertainties.\nAdditionally, we investigate how the pupil dilates when listening to\nsynthetic speech in the presence of speech-shaped noise. Our results\nshow that, in quiet listening conditions, pupil dilation does not reflect\nlistening effort but rather attention and engagement. In noisy conditions,\nincreased pupil dilation indicates that listening effort increases\nas signal-to-noise ratio decreases, under all conditions tested.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1783",
      "author_area_id": 7,
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "douros19b_interspeech": {
      "authors": [
        [
          "Ioannis K.",
          "Douros"
        ],
        [
          "Jacques",
          "Felblinger"
        ],
        [
          "Jens",
          "Frahm"
        ],
        [
          "Karyna",
          "Isaieva"
        ],
        [
          "Arun A.",
          "Joseph"
        ],
        [
          "Yves",
          "Laprie"
        ],
        [
          "Freddy",
          "Odille"
        ],
        [
          "Anastasiia",
          "Tsukanova"
        ],
        [
          "Dirk",
          "Voit"
        ],
        [
          "Pierre-Andr\u00e9",
          "Vuissoz"
        ]
      ],
      "title": "A Multimodal Real-Time MRI Articulatory Corpus of French for Speech Research",
      "original": "1700",
      "page_count": 5,
      "order": 324,
      "p1": "1556",
      "pn": "1560",
      "abstract": [
        "In this work we describe the creation of ArtSpeechMRIfr: a real-time\nas well as static magnetic resonance imaging (rtMRI, 3D MRI) database\nof the vocal tract. The database contains also processed data: denoised\naudio, its phonetically aligned annotation, articulatory contours,\nand vocal tract volume information, which provides a rich resource\nfor speech research. The database is built on data from two male speakers\nof French.<br style=\"mso-data-placement:same-cell;\" /><br style=\"mso-data-placement:same-cell;\"\n/>\n  It covers a number of phonetic contexts in the controlled part,\nas well as spontaneous speech, 3D MRI scans of sustained vocalic articulations,\nand of the dental casts of the subjects. The corpus for rtMRI consists\nof 79 synthetic sentences constructed from a phonetized dictionary\nthat makes possible to shorten the duration of acquisitions while keeping\na very good coverage of the phonetic contexts which exist in French.\nThe 3D MRI includes acquisitions for 12 French vowels and 10 consonants,\neach of which was pronounced in several vocalic contexts. Articulatory\ncontours (tongue, jaw, epiglottis, larynx, velum, lips) as well as\n3D volumes were manually drawn for a part of the images.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1700",
      "author_area_id": 7,
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "chen19d_interspeech": {
      "authors": [
        [
          "Jia-Xiang",
          "Chen"
        ],
        [
          "Zhen-Hua",
          "Ling"
        ],
        [
          "Li-Rong",
          "Dai"
        ]
      ],
      "title": "A Chinese Dataset for Identifying Speakers in Novels",
      "original": "1614",
      "page_count": 5,
      "order": 325,
      "p1": "1561",
      "pn": "1565",
      "abstract": [
        "Identifying speakers in novels aims at determining who says a quote\nin a given context by text analysis. This task is important for speech\nsynthesis systems to assign appropriate voices to the quotes when producing\naudio books. Several English datasets have been constructed for this\ntask. However, the difference between English and Chinese impedes processing\nChinese novels using the models built on English datasets directly.\nTherefore, this paper presents a Chinese dataset, which contains 2,548\nquotes from  World of Plainness, a famous Chinese novel, with manually\nlabelled speaker identities. Furthermore, two baseline speaker identification\nmethods, i.e., a rule-based one and a classifier-based one, are designed\nand experimented using this Chinese dataset. These two methods achieve\naccuracies of 53.77% and 58.66% respectively on the test set.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1614",
      "author_area_id": 7,
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "park19c_interspeech": {
      "authors": [
        [
          "Kyubyong",
          "Park"
        ],
        [
          "Thomas",
          "Mulc"
        ]
      ],
      "title": "CSS10: A Collection of Single Speaker Speech Datasets for 10 Languages",
      "original": "1500",
      "page_count": 5,
      "order": 326,
      "p1": "1566",
      "pn": "1570",
      "abstract": [
        "We describe our development of CSS10, a collection of single speaker\nspeech datasets for ten languages. It is composed of short audio clips\nfrom LibriVox audiobooks and their aligned texts. To validate its quality\nwe train two neural text-to-speech models on each dataset. Subsequently,\nwe conduct Mean Opinion Score tests on the synthesized speech samples.\nWe make our datasets, pre-trained models, and test resources publicly\navailable. We hope they will be used for future speech tasks.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1500",
      "author_area_id": 7,
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "karaulov19_interspeech": {
      "authors": [
        [
          "Ievgen",
          "Karaulov"
        ],
        [
          "Dmytro",
          "Tkanov"
        ]
      ],
      "title": "Attention Model for Articulatory Features Detection",
      "original": "3020",
      "page_count": 5,
      "order": 327,
      "p1": "1571",
      "pn": "1575",
      "abstract": [
        "Articulatory distinctive features, as well as phonetic transcription,\nplay important role in speech-related tasks: computer-assisted pronunciation\ntraining, text-to-speech conversion (TTS), studying speech production\nmechanisms, speech recognition for low-resourced languages. End-to-end\napproaches to speech-related tasks got a lot of traction in recent\nyears. We apply Listen, Attend and Spell (LAS) [1] architecture to\nphones recognition on a small small training set, like TIMIT [2]. Also,\nwe introduce a novel decoding technique that allows to train manners\nand places of articulation detectors end-to-end using attention models.\nWe also explore joint phones recognition and articulatory features\ndetection in multitask learning setting.\n"
      ],
      "doi": "10.21437/Interspeech.2019-3020",
      "author_area_id": 8,
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "tong19_interspeech": {
      "authors": [
        [
          "Sibo",
          "Tong"
        ],
        [
          "Apoorv",
          "Vyas"
        ],
        [
          "Philip N.",
          "Garner"
        ],
        [
          "Herv\u00e9",
          "Bourlard"
        ]
      ],
      "title": "Unbiased Semi-Supervised LF-MMI Training Using Dropout",
      "original": "2678",
      "page_count": 5,
      "order": 328,
      "p1": "1576",
      "pn": "1580",
      "abstract": [
        "The lattice-free MMI objective (LF-MMI) with finite-state transducer\n(FST) supervision lattice has been used in semi-supervised training\nof state-of-the-art neural network acoustic models for automatic speech\nrecognition (ASR). However, the FST based supervision lattice does\nnot sample from the posterior predictive distribution of word-sequences\nbut only contains the decoding hypotheses corresponding to the Maximum\nLikelihood estimate of weights, so that the training might be biased\ntowards incorrect hypotheses in the supervision lattice even if the\nbest path is perfectly correct. In this paper, we propose a novel framework\nwhich uses Dropout at the test time to sample from the posterior predictive\ndistribution of word-sequences to produce unbiased supervision lattices\nfor semi-supervised training. We investigate the dropout sampling from\nboth the acoustic model and the language model to generate supervision.\nResults on Fisher English show that the proposed approach achieves\nWER recovery of &#126;51.6% over regular semi-supervised LF-MMI training.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2678",
      "author_area_id": 8,
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "cui19_interspeech": {
      "authors": [
        [
          "Xiaodong",
          "Cui"
        ],
        [
          "Michael",
          "Picheny"
        ]
      ],
      "title": "Acoustic Model Optimization Based on Evolutionary Stochastic Gradient Descent with Anchors for Automatic Speech Recognition",
      "original": "2620",
      "page_count": 5,
      "order": 329,
      "p1": "1581",
      "pn": "1585",
      "abstract": [
        "Evolutionary stochastic gradient descent (ESGD) was proposed as a population-based\napproach that combines the merits of gradient-aware and gradient-free\noptimization algorithms for superior overall optimization performance.\nIn this paper we investigate a variant of ESGD for optimization of\nacoustic models for automatic speech recognition (ASR). In this variant,\nwe assume the existence of a well-trained acoustic model and use it\nas an anchor in the parent population whose good &#8220;gene&#8221;\nwill prorogate in the evolution to the offsprings. We propose an ESGD\nalgorithm leveraging the anchor models such that it guarantees the\nbest fitness of the population will never degrade from the anchor model.\nExperiments on 50-hour Broadcast News (BN50) and 300-hour Switchboard\n(SWB300) show that the ESGD with anchors can further improve the loss\nand ASR performance over the existing well-trained acoustic models.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2620",
      "author_area_id": 8,
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "shah19b_interspeech": {
      "authors": [
        [
          "Nirmesh J.",
          "Shah"
        ],
        [
          "Hardik B.",
          "Sailor"
        ],
        [
          "Hemant A.",
          "Patil"
        ]
      ],
      "title": "Whether to Pretrain DNN or not?: An Empirical Analysis for Voice Conversion",
      "original": "2608",
      "page_count": 5,
      "order": 330,
      "p1": "1586",
      "pn": "1590",
      "abstract": [
        "Recently, Deep Neural Network (DNN)-based Voice Conversion (VC) techniques\nhave become popular in the VC literature. These techniques suffer from\nthe issue of overfitting due to less amount of available training data\nfrom a target speaker. To alleviate this, pre-training is used for\nbetter initialization of the DNN parameters, which leads to faster\nconvergence of parameters. Greedy layerwise pre-training of the stacked\nRestricted Boltzmann Machine (RBM) or the stacked De-noising AutoEncoder\n(DAE) is used with extra available speaker-pairs&#8216; data. This\npre-training is time-consuming and requires a separate network to learn\nthe parameters of the network. In this work, we propose to analyze\nthe DNN training strategies for the VC task, specifically with and\nwithout pre-training. In particular, we investigate whether an extra\npre-training step could be avoided by using recent advances in deep\nlearning. The VC experiments were performed on two VC Challenge (VCC)\ndatabases 2016 and 2018. Objective and subjective tests show that DNN\ntrained with Adam optimization and Exponential Linear Unit (ELU) performed\ncomparable or better than the pre-trained DNN without compromising\non speech quality and speaker similarity of the converted voices.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2608",
      "author_area_id": 8,
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "goyal19_interspeech": {
      "authors": [
        [
          "Mohit",
          "Goyal"
        ],
        [
          "Varun",
          "Srivastava"
        ],
        [
          "Prathosh",
          "A.P."
        ]
      ],
      "title": "Detection of Glottal Closure Instants from Raw Speech Using Convolutional Neural Networks",
      "original": "2587",
      "page_count": 5,
      "order": 331,
      "p1": "1591",
      "pn": "1595",
      "abstract": [
        "Glottal Closure Instants (GCIs) correspond to the temporal locations\nof significant excitation to the vocal tract occurring during the production\nof voiced speech. GCI detection from speech signals is a well-studied\nproblem given its importance in speech processing. Most of the existing\napproaches for GCI detection adopt a two-stage approach (i) Transformation\nof speech signal into a representative signal where GCIs are localized\nbetter, (ii) extraction of GCIs using the representative signal obtained\nin first stage. The former stage is accomplished using signal processing\ntechniques based on the principles of speech production and the latter\nwith heuristic-algorithms such as dynamic-programming and peak-picking.\nThese methods are thus task-specific and rely on the methods used for\nrepresentative signal extraction. However in this paper, we formulate\nthe GCI detection problem from a representation learning perspective\nwhere appropriate representation is implicitly learned from the raw-speech\ndata samples. Specifically, GCI detection is cast as a supervised multi-task\nlearning problem solved using a deep convolutional neural network jointly\noptimizing a classification and regression cost. The learning capability\nis demonstrated with several experiments on standard datasets. The\nresults compare well with the state-of- the-art algorithms while performing\nbetter in the case of presence of real-world non-stationary noise.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2587",
      "author_area_id": 8,
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "fainberg19_interspeech": {
      "authors": [
        [
          "Joachim",
          "Fainberg"
        ],
        [
          "Ond\u0159ej",
          "Klejch"
        ],
        [
          "Steve",
          "Renals"
        ],
        [
          "Peter",
          "Bell"
        ]
      ],
      "title": "Lattice-Based Lightly-Supervised Acoustic Model Training",
      "original": "2533",
      "page_count": 5,
      "order": 332,
      "p1": "1596",
      "pn": "1600",
      "abstract": [
        "In the broadcast domain there is an abundance of related text data\nand partial transcriptions, such as closed captions and subtitles.\nThis text data can be used for lightly supervised training, in which\ntext matching the audio is selected using an existing speech recognition\nmodel. Current approaches to light supervision typically filter the\ndata based on matching error rates between the transcriptions and biased\ndecoding hypotheses. In contrast, semi-supervised training does not\nrequire matching text data, instead generating a hypothesis using a\nbackground language model. State-of-the-art semi-supervised training\nuses lattice-based supervision with the lattice-free MMI (LF-MMI) objective\nfunction. We propose a technique to combine inaccurate transcriptions\nwith the lattices generated for semi-supervised training, thus preserving\nuncertainty in the lattice where appropriate. We demonstrate that this\ncombined approach reduces the expected error rates over the lattices,\nand reduces the word error rate (WER) on a broadcast task.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2533",
      "author_area_id": 8,
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "michel19_interspeech": {
      "authors": [
        [
          "Wilfried",
          "Michel"
        ],
        [
          "Ralf",
          "Schl\u00fcter"
        ],
        [
          "Hermann",
          "Ney"
        ]
      ],
      "title": "Comparison of Lattice-Free and Lattice-Based Sequence Discriminative Training Criteria for LVCSR",
      "original": "2254",
      "page_count": 5,
      "order": 333,
      "p1": "1601",
      "pn": "1605",
      "abstract": [
        "Sequence discriminative training criteria have long been a standard\ntool in automatic speech recognition for improving the performance\nof acoustic models over their maximum likelihood / cross entropy trained\ncounterparts. While previously a lattice approximation of the search\nspace has been necessary to reduce computational complexity, recently\nproposed methods use other approximations to dispense of the need for\nthe computationally expensive step of separate lattice creation.<br\nstyle=\"mso-data-placement:same-cell;\" /><br style=\"mso-data-placement:same-cell;\"\n/>\n  In this work we present a memory efficient implementation of the\nforward-backward computation that allows us to use unigram word-level\nlanguage models in the denominator calculation while still doing a\nfull summation on GPU. This allows for a direct comparison of lattice-based\nand lattice-free sequence discriminative training criteria such as\nMMI and sMBR, both using the same language model during training.<br\nstyle=\"mso-data-placement:same-cell;\" /><br style=\"mso-data-placement:same-cell;\"\n/>\n  We compared performance, speed of convergence, and stability on\nlarge vocabulary continuous speech recognition tasks like Switchboard\nand Quaero. We found that silence modeling seriously impacts the performance\nin the lattice-free case and needs special treatment. In our experiments\nlattice-free MMI comes on par with its lattice-based counterpart. Lattice-based\nsMBR still outperforms all lattice-free training criteria.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2254",
      "author_area_id": 8,
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "masumura19b_interspeech": {
      "authors": [
        [
          "Ryo",
          "Masumura"
        ],
        [
          "Hiroshi",
          "Sato"
        ],
        [
          "Tomohiro",
          "Tanaka"
        ],
        [
          "Takafumi",
          "Moriya"
        ],
        [
          "Yusuke",
          "Ijima"
        ],
        [
          "Takanobu",
          "Oba"
        ]
      ],
      "title": "End-to-End Automatic Speech Recognition with a Reconstruction Criterion Using Speech-to-Text and Text-to-Speech Encoder-Decoders",
      "original": "2111",
      "page_count": 5,
      "order": 334,
      "p1": "1606",
      "pn": "1610",
      "abstract": [
        "In this paper, we present a novel end-to-end automatic speech recognition\n(ASR) method that considers whether an input speech can be reconstructed\nfrom a generated text or not. A speech-to-text encoder-decoder model\nis one of the most powerful end-to-end ASR methods since it does not\nmake any conditional independence assumptions. However, encoder-decoder\nmodels often suffer from a problem that is caused from a gap between\nthe teacher forcing in a training phase and the free running in a testing\nphase. In fact, there is no guarantee that texts can be generated correctly\nwhen some generation errors occur in conditioning contexts. In order\nto mitigate this problem, our proposed method utilizes not only a generation\nprobability of the text computed from a speech-to-text encoder-decoder\nbut also a reconstruction probability of the speech computed from a\ntext-to-speech encoder-decoder on the basis of a maximum mutual information\ncriterion. We can expect that considering the reconstruction criterion\ncan impose a constraint against generation errors. In addition, in\norder to compute the reconstruction probability, we introduce a mixture\ndensity network into the text-to-speech encoder-decoder. Our experiments\non Japanese lecture ASR tasks demonstrate that considering the reconstruction\ncriterion can yield ASR performance improvements.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2111",
      "author_area_id": 8,
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "heba19_interspeech": {
      "authors": [
        [
          "Abdelwahab",
          "Heba"
        ],
        [
          "Thomas",
          "Pellegrini"
        ],
        [
          "Jean-Pierre",
          "Lorr\u00e9"
        ],
        [
          "R\u00e9gine",
          "Andre-Obrecht"
        ]
      ],
      "title": "Char+CV-CTC: Combining Graphemes and Consonant/Vowel Units for CTC-Based ASR Using Multitask Learning",
      "original": "1975",
      "page_count": 5,
      "order": 335,
      "p1": "1611",
      "pn": "1615",
      "abstract": [
        "Previous work has shown that end-to-end neural-based speech recognition\nsystems can be improved by adding auxiliary tasks at intermediate layers.\nIn this paper, we report multitask learning (MTL) experiments in the\ncontext of connectionist temporal classification (CTC) based speech\nrecognition at character level. We compare several MTL architectures\nthat jointly learn to predict characters (sometimes called graphemes)\nand consonant/vowel (CV) binary labels. The best approach, which we\ncall Char+CV-CTC, adds up the character and CV logits to obtain the\nfinal character predictions. The idea is to put more weight on the\nvowel (consonant) characters when the vowel (consonant) symbol &#8216;V&#8217;\n(&#8216;C&#8217;) is predicted in the auxiliary-task branch of the\nnetwork. Experiments were carried out on the Wall Street Journal (WSJ)\ncorpus. Char+CV-CTC achieved the best ASR results with a 2.2% Character\nError Rate and a 6.1% Word Error Rate (WER) on the Eval92 evaluation\nsubset. This model outperformed its monotask model counterpart by 0.7%\nabsolute in WER and also achieved almost the same performance of 6.0%\nas a strong baseline phone-based Time Delay Neural Network (&#8220;TDNN-Phone+TR2&#8221;)\nmodel.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1975",
      "author_area_id": 8,
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "kurata19_interspeech": {
      "authors": [
        [
          "Gakuto",
          "Kurata"
        ],
        [
          "Kartik",
          "Audhkhasi"
        ]
      ],
      "title": "Guiding CTC Posterior Spike Timings for Improved Posterior Fusion and Knowledge Distillation",
      "original": "1952",
      "page_count": 5,
      "order": 336,
      "p1": "1616",
      "pn": "1620",
      "abstract": [
        "Conventional automatic speech recognition (ASR) systems trained from\nframe-level alignments can easily leverage posterior fusion to improve\nASR accuracy and build a better single model with knowledge distillation.\nEnd-to-end ASR systems trained using the Connectionist Temporal Classification\n(CTC) loss do not require frame-level alignment and hence simplify\nmodel training. However, sparse and arbitrary posterior spike timings\nfrom CTC models pose a new set of challenges in posterior fusion from\nmultiple models and knowledge distillation between CTC models. We propose\na method to train a CTC model so that its spike timings are guided\nto align with those of a pre-trained  guiding CTC model. As a result,\nall models that share the same guiding model have aligned spike timings.\nWe show the advantage of our method in various scenarios including\nposterior fusion of CTC models and knowledge distillation between CTC\nmodels with different architectures. With the 300-hour Switchboard\ntraining data, the single word CTC model distilled from multiple models\nimproved the word error rates to 13.7%/23.1% from 14.9%/24.1% on the\nHub5 2000 Switchboard/CallHome test sets without using any data augmentation,\nlanguage model, or complex decoder.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1952",
      "author_area_id": 8,
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "fukuda19_interspeech": {
      "authors": [
        [
          "Takashi",
          "Fukuda"
        ],
        [
          "Masayuki",
          "Suzuki"
        ],
        [
          "Gakuto",
          "Kurata"
        ]
      ],
      "title": "Direct Neuron-Wise Fusion of Cognate Neural Networks",
      "original": "1930",
      "page_count": 5,
      "order": 337,
      "p1": "1621",
      "pn": "1625",
      "abstract": [
        "This paper proposes a method to create a robust acoustic model by directly\nfusing multiple neural networks that have dissimilar characteristics\nwithout any additional layers/nodes involving retraining procedures.\nThe fused neural networks derive from a shared parent neural network\nand are referred to as cognate (child) neural networks in this paper.\nThe neural networks are fused by interpolating weight and bias parameters\nassociated with each neuron with a different fusion weight, assuming\nthat cognate neural networks to be fused have the same topology. Therefore,\nno extra computational cost during decoding is required. The fusion\nweight is determined by considering a cosine similarity estimated from\nparameters connecting to the neuron and the fusion is performed for\nevery neuron. Experiments were carried out using a test suite consisting\nof various acoustic conditions with a wide SNR range, speakers including\nforeign accented speakers, and speaking styles. From the experiments,\nthe network created by fusing cognate neural networks showed consistent\nimprovement on average compared with the commercial-grade domain-free\nnetwork originating from the parent model. In addition, we demonstrate\nthat the fusion considering input connections to the neuron achieves\nthe highest accuracy in our experiments.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1930",
      "author_area_id": 8,
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "ladkat19_interspeech": {
      "authors": [
        [
          "Pranav",
          "Ladkat"
        ],
        [
          "Oleg",
          "Rybakov"
        ],
        [
          "Radhika",
          "Arava"
        ],
        [
          "Sree Hari Krishnan",
          "Parthasarathi"
        ],
        [
          "I-Fan",
          "Chen"
        ],
        [
          "Nikko",
          "Strom"
        ]
      ],
      "title": "Two Tiered Distributed Training Algorithm for Acoustic Modeling",
      "original": "1859",
      "page_count": 5,
      "order": 338,
      "p1": "1626",
      "pn": "1630",
      "abstract": [
        "We present a hybrid approach for scaling distributed training of neural\nnetworks by combining Gradient Threshold Compression (GTC) algorithm\n&#8212; a variant of stochastic gradient descent (SGD) &#8212; which\ncompresses gradients with thresholding and quantization techniques\nand Blockwise Model Update Filtering (BMUF) algorithm &#8212; a variant\nof model averaging (MA). In this proposed method, we divide total number\nof workers into smaller subgroups in a hierarchical manner and limit\nfrequent communication across subgroups. We update local model using\nGTC within a subgroup and global model using BMUF across different\nsubgroups. We evaluate this approach in an Automatic Speech Recognition\n(ASR) task, by training deep long short-term memory (LSTM) acoustic\nmodels on 2000 hours of speech. Experiments show that, for a wide range\nin the number of GPUs used for distributed training, the proposed approach\nachieves a better trade-off between accuracy and scalability compared\nto GTC and BMUF.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1859",
      "author_area_id": 8,
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "huang19f_interspeech": {
      "authors": [
        [
          "Pin-Tuan",
          "Huang"
        ],
        [
          "Hung-Shin",
          "Lee"
        ],
        [
          "Syu-Siang",
          "Wang"
        ],
        [
          "Kuan-Yu",
          "Chen"
        ],
        [
          "Yu",
          "Tsao"
        ],
        [
          "Hsin-Min",
          "Wang"
        ]
      ],
      "title": "Exploring the Encoder Layers of Discriminative Autoencoders for LVCSR",
      "original": "1717",
      "page_count": 5,
      "order": 339,
      "p1": "1631",
      "pn": "1635",
      "abstract": [
        "Discriminative autoencoders (DcAEs) have been proven to improve generalization\nof the learned acoustic models by increasing their reconstruction capacity\nof input features from the frame embeddings. In this paper, we integrate\nDcAEs into two models, namely TDNNs and LSTMs, which have been commonly\nadopted in the Kaldi recipes for LVCSR in recent years, using the modified\nnnet3 neural network library. We also explore two kinds of skip-connection\nmechanisms for DcAEs, namely concatenation and addition. The results\nof LVCSR experiments on the MATBN Mandarin Chinese corpus and the WSJ\nEnglish corpus show that the proposed DcAE-TDNN-based system achieves\nrelative word error rate reductions of 3% and 10% over the TDNN-based\nbaseline system, respectively. The DcAE-TDNN-LSTM-based system also\noutperforms the TDNN-LSTM-based baseline system. The results imply\nthe flexibility of DcAEs to be integrated with other existing or prospective\nneural network-based acoustic models.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1717",
      "author_area_id": 8,
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "kurata19b_interspeech": {
      "authors": [
        [
          "Gakuto",
          "Kurata"
        ],
        [
          "Kartik",
          "Audhkhasi"
        ]
      ],
      "title": "Multi-Task CTC Training with Auxiliary Feature Reconstruction for End-to-End Speech Recognition",
      "original": "1710",
      "page_count": 5,
      "order": 340,
      "p1": "1636",
      "pn": "1640",
      "abstract": [
        "We present a multi-task Connectionist Temporal Classification (CTC)\ntraining for end-to-end (E2E) automatic speech recognition with input\nfeature reconstruction as an auxiliary task. Whereas the main task\nof E2E CTC training and the auxiliary reconstruction task share the\nencoder network, the auxiliary task tries to reconstruct the input\nfeature from the encoded information. In addition to standard feature\nreconstruction, we distort the input feature only in the auxiliary\nreconstruction task, such as (1) swapping the former and latter parts\nof an utterance, or (2) using a part of an utterance by stripping the\nbeginning or end parts. These distortions intentionally suppress long-span\ndependencies in the time domain, which avoids overfitting to the training\ndata. We trained phone-based CTC and word-based CTC models with the\nproposed multi-task learning and demonstrated that it improves ASR\naccuracy on various test sets that are matched and unmatched with the\ntraining data.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1710",
      "author_area_id": 8,
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "li19e_interspeech": {
      "authors": [
        [
          "Mohan",
          "Li"
        ],
        [
          "Yuanjiang",
          "Cao"
        ],
        [
          "Weicong",
          "Zhou"
        ],
        [
          "Min",
          "Liu"
        ]
      ],
      "title": "Framewise Supervised Training Towards End-to-End Speech Recognition Models: First Results",
      "original": "1117",
      "page_count": 5,
      "order": 341,
      "p1": "1641",
      "pn": "1645",
      "abstract": [
        "Recurrent neural networks (RNNs) trained with connectionist temporal\nclassification (CTC) technique have delivered promising results in\nmany speech recognition tasks. However, the forward-backward algorithm\nthat CTC takes for model optimization requires a huge amount of computation.\nThis paper introduces a new training method towards RNN-based end-to-end\nmodels, which significantly saves computing power without losing accuracy.\nUnlike CTC, the label sequence is aligned to the labelling hypothesis\nand then to the input sequence by the Weighted Minimum Edit-Distance\nAligning (WMEDA) algorithm. Based on the alignment, the framewise supervised\ntraining is conducted. Moreover, Pronunciation Embedding (PE), the\nacoustic representation towards a linguistic target, is proposed in\norder to calculate the weights in WMEDA algorithm. The model is evaluated\non TIMIT and AIShell-1 datasets for English phoneme and Chinese character\nrecognitions. For TIMIT, the model achieves a comparable 18.57% PER\nto the 18.4% PER of the CTC baseline. As for AIShell-1, a joint Pinyin-character\nmodel is trained, giving a 19.38% CER, which is slightly better than\nthe 19.43% CER obtained by the CTC character model, and the training\ntime of this model is only 54.3% of the CTC model&#8217;s.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1117",
      "author_area_id": 8,
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "georgiou19_interspeech": {
      "authors": [
        [
          "Efthymios",
          "Georgiou"
        ],
        [
          "Charilaos",
          "Papaioannou"
        ],
        [
          "Alexandros",
          "Potamianos"
        ]
      ],
      "title": "Deep Hierarchical Fusion with Application in Sentiment Analysis",
      "original": "3243",
      "page_count": 5,
      "order": 342,
      "p1": "1646",
      "pn": "1650",
      "abstract": [
        "Recognizing the emotional tone in spoken language is a challenging\nresearch problem that requires modeling not only the acoustic and textual\nmodalities separately but also their cross-interactions. In this work,\nwe introduce a hierarchical fusion scheme for sentiment analysis of\nspoken sentences. Two bidirectional Long-Short-Term-Memory networks\n(BiLSTM), followed by multiple fully connected layers, are trained\nin order to extract feature representations for each of the textual\nand audio modalities. The representations of the unimodal encoders\nare both fused at each layer and propagated forward, thus achieving\nfusion at the word, sentence and high/sentiment levels. The proposed\napproach of deep hierarchical fusion achieves state-of-the-art results\nfor sentiment analysis tasks. Through an ablation study, we show that\nthe proposed fusion method achieves greater performance gains over\nthe unimodal baseline compared to other fusion approaches in the literature.\n"
      ],
      "doi": "10.21437/Interspeech.2019-3243",
      "author_area_id": 3,
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "mitra19_interspeech": {
      "authors": [
        [
          "Vikramjit",
          "Mitra"
        ],
        [
          "Sue",
          "Booker"
        ],
        [
          "Erik",
          "Marchi"
        ],
        [
          "David Scott",
          "Farrar"
        ],
        [
          "Ute Dorothea",
          "Peitz"
        ],
        [
          "Bridget",
          "Cheng"
        ],
        [
          "Ermine",
          "Teves"
        ],
        [
          "Anuj",
          "Mehta"
        ],
        [
          "Devang",
          "Naik"
        ]
      ],
      "title": "Leveraging Acoustic Cues and Paralinguistic Embeddings to Detect Expression from Voice",
      "original": "2998",
      "page_count": 5,
      "order": 343,
      "p1": "1651",
      "pn": "1655",
      "abstract": [
        "Millions of people reach out to digital assistants such as Siri every\nday, asking for information, making phone calls, seeking assistance,\nand much more. The expectation is that such assistants should understand\nthe intent of the user&#8217;s query. Detecting the intent of a query\nfrom a short, isolated utterance is a difficult task. Intent cannot\nalways be obtained from speech-recognized transcriptions. A transcription-driven\napproach can interpret what has been said but fails to acknowledge\nhow it has been said, and as a consequence, may ignore the expression\npresent in the voice. Our work investigates whether a system can reliably\ndetect vocal expression in queries using acoustic and paralinguistic\nembedding. Results show that the proposed method offers a relative\nequal error rate (EER) decrease of 60% compared to a bag-of-word based\nsystem, corroborating that expression is significantly represented\nby vocal attributes, rather than being purely lexical. Addition of\nemotion embedding helped to reduce the EER by 30% relative to the acoustic\nembedding, demonstrating the relevance of emotion in expressive voice.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2998",
      "author_area_id": 3,
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "parry19_interspeech": {
      "authors": [
        [
          "Jack",
          "Parry"
        ],
        [
          "Dimitri",
          "Palaz"
        ],
        [
          "Georgia",
          "Clarke"
        ],
        [
          "Pauline",
          "Lecomte"
        ],
        [
          "Rebecca",
          "Mead"
        ],
        [
          "Michael",
          "Berger"
        ],
        [
          "Gregor",
          "Hofer"
        ]
      ],
      "title": "Analysis of Deep Learning Architectures for Cross-Corpus Speech Emotion Recognition",
      "original": "2753",
      "page_count": 5,
      "order": 344,
      "p1": "1656",
      "pn": "1660",
      "abstract": [
        "Speech Emotion Recognition (SER) is an important and challenging task\nfor human-computer interaction. In the literature deep learning architectures\nhave been shown to yield state-of-the-art performance on this task\nwhen the model is trained and evaluated on the same corpus. However,\nprior work has indicated that such systems often yield poor performance\non unseen data. To improve the generalisation capabilities of emotion\nrecognition systems one possible approach is cross-corpus training,\nwhich consists of training the model on an aggregation of different\ncorpora. In this paper we present an analysis of the generalisation\ncapability of deep learning models using cross-corpus training with\nsix different speech emotion corpora. We evaluate the models on an\nunseen corpus and analyse the learned representations using the t-SNE\nalgorithm, showing that architectures based on recurrent neural networks\nare prone to overfit the corpora present in the training set, while\narchitectures based on convolutional neural networks (CNNs) show better\ngeneralisation capabilities. These findings indicate that (1) cross-corpus\ntraining is a promising approach for improving generalisation and (2)\nCNNs should be the architecture of choice for this approach.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2753",
      "author_area_id": 3,
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "wang19e_interspeech": {
      "authors": [
        [
          "Bo",
          "Wang"
        ],
        [
          "Maria",
          "Liakata"
        ],
        [
          "Hao",
          "Ni"
        ],
        [
          "Terry",
          "Lyons"
        ],
        [
          "Alejo J.",
          "Nevado-Holgado"
        ],
        [
          "Kate",
          "Saunders"
        ]
      ],
      "title": "A Path Signature Approach for Speech Emotion Recognition",
      "original": "2624",
      "page_count": 5,
      "order": 345,
      "p1": "1661",
      "pn": "1665",
      "abstract": [
        "Automatic speech emotion recognition (SER) remains a difficult task\nwithin human-computer interaction, despite increasing interest in the\nresearch community. One key challenge is how to effectively integrate\nshort-term characterisation of speech segments with long-term information\nsuch as temporal variations. Motivated by the numerical approximation\ntheory of stochastic differential equations (SDEs), we propose the\nnovel use of path signatures. The latter provide a pathwise definition\nto solve SDEs, for the integration of short speech frames. Furthermore\nwe propose a hierarchical tree structure of path signatures, to capture\nboth global and local information. A simple tree-based convolutional\nneural network (TBCNN) is used for learning the structural information\nstemming from dyadic path-tree signatures. Our experimental results\non a widely used benchmark dataset demonstrate comparable performance\nto complex neural network based systems.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2624",
      "author_area_id": 3,
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "egorow19_interspeech": {
      "authors": [
        [
          "Olga",
          "Egorow"
        ],
        [
          "Tarik",
          "Mrech"
        ],
        [
          "Norman",
          "Wei\u00dfkirchen"
        ],
        [
          "Andreas",
          "Wendemuth"
        ]
      ],
      "title": "Employing Bottleneck and Convolutional Features for Speech-Based Physical Load Detection on Limited Data Amounts",
      "original": "2502",
      "page_count": 5,
      "order": 346,
      "p1": "1666",
      "pn": "1670",
      "abstract": [
        "The detection of different levels of physical load from speech has\nmany applications: Besides telemedicine, non-contact detection of certain\nheart rate ranges can be useful for sports and other leisure time devices.\nAvailable approaches mainly use a high number of spectral and prosodic\nfeatures. In this setting of typically small data sets, such as the\nTalk &amp; Run data set and the Munich Biovoice Corpus, the high-dimensional\nfeature spaces are only sparsely populated. Therefore, we aim at a\nreduction of the feature number using modern neural net inspired features:\nBottleneck layer features, obtained from standard low-level descriptors\nvia a feed-forward neural network, and activation map features, obtained\nfrom spectrograms via a convolutional neural network. We use these\nfeatures for an SVM classification of high and low physical load and\ncompare their performance. We also discuss the possibility of hyperparameter\ntransfer of the extracting networks between different data sets. We\nshow that even for limited amounts of data, deep learning based methods\ncan bring a substantial improvement over &#8220;conventional&#8221;\nfeatures.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2502",
      "author_area_id": 3,
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "zhao19e_interspeech": {
      "authors": [
        [
          "Jinming",
          "Zhao"
        ],
        [
          "Shizhe",
          "Chen"
        ],
        [
          "Jingjun",
          "Liang"
        ],
        [
          "Qin",
          "Jin"
        ]
      ],
      "title": "Speech Emotion Recognition in Dyadic Dialogues with Attentive Interaction Modeling",
      "original": "2103",
      "page_count": 5,
      "order": 347,
      "p1": "1671",
      "pn": "1675",
      "abstract": [
        "In dyadic human-human interactions, a more complex interaction scenario,\na person&#8217;s emotional state can be influenced by both self emotional\nevolution and the interlocutor&#8217;s behaviors. However, previous\nspeech emotion recognition studies infer the speaker&#8217;s emotional\nstate mainly based on the targeted speech segment without considering\nthe above two contextual factors. In this paper, we propose an Attentive\nInteraction Model (AIM) to capture both self- and interlocutor-context\nto enhance the speech emotion recognition in the dyadic dialog. The\nmodel learns to dynamically focus on long-term relevant contexts of\nthe speaker and the interlocutor via the self-attention mechanism and\nfuse the adaptive context with the present behavior to predict the\ncurrent emotional state. We carry out extensive experiments on the\nIEMOCAP corpus for dimensional emotion recognition in arousal and valence.\nOur model achieves on par performance with baselines for arousal recognition\nand significantly outperforms baselines for valence recognition, which\ndemonstrates the effectiveness of the model to select useful contexts\nfor emotion recognition in dyadic interactions.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2103",
      "author_area_id": 3,
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "zhong19_interspeech": {
      "authors": [
        [
          "Shun-Chang",
          "Zhong"
        ],
        [
          "Yun-Shao",
          "Lin"
        ],
        [
          "Chun-Min",
          "Chang"
        ],
        [
          "Yi-Ching",
          "Liu"
        ],
        [
          "Chi-Chun",
          "Lee"
        ]
      ],
      "title": "Predicting Group Performances Using a Personality Composite-Network Architecture During Collaborative Task",
      "original": "2087",
      "page_count": 5,
      "order": 348,
      "p1": "1676",
      "pn": "1680",
      "abstract": [
        "Personality has not only been studied at an individual level, its composite\neffect between team members has also been indicated to be related to\nthe overall group performance. In this work, we propose a Personality\nComposite-Network (P-CompN) architecture that models the group-level\npersonality composition with its intertwining effect being integrated\ninto the network modeling of team members vocal behaviors in order\nto predict the group performances during collaborative problem solving\ntasks. In specific, we evaluate our proposed P-CompN in a large-scale\ndataset consist of three-person small group interactions. Our framework\nachieves a promising group performance classification accuracy of 70.0%,\nwhich outperforms baseline model of using only vocal behaviors without\npersonality attributes by 14.4% absolutely. Our analysis further indicates\nthat our proposed personality composite network impacts the vocal behavior\nmodels more significantly on the high performing groups versus the\nlow performing groups.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2087",
      "author_area_id": 3,
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "chao19b_interspeech": {
      "authors": [
        [
          "Gao-Yi",
          "Chao"
        ],
        [
          "Yun-Shao",
          "Lin"
        ],
        [
          "Chun-Min",
          "Chang"
        ],
        [
          "Chi-Chun",
          "Lee"
        ]
      ],
      "title": "Enforcing Semantic Consistency for Cross Corpus Valence Regression from Speech Using Adversarial Discrepancy Learning",
      "original": "2037",
      "page_count": 5,
      "order": 349,
      "p1": "1681",
      "pn": "1685",
      "abstract": [
        "Issues of mismatch between databases remain a major challenge in performing\nemotion recognition on target unlabeled corpus from labeled source\ndata. While studies have shown that by means of aligning source and\ntarget data distribution to learn a common feature space can mitigate\nthese issues partially, they neglect the effect of distortion in emotion\nsemantics across different databases. This distortion is especially\ncrucial when regressing higher level emotion attribute such as valence.\nIn this work, we propose a maximum regression discrepancy (MRD) network,\nwhich enforces cross corpus semantic consistency by learning a common\nacoustic feature space that minimizes discrepancy on those maximally-distorted\nsamples through adversarial training. We evaluate our framework on\ntwo large emotion corpus, the USC IEMOCAP and the MSP-IMPROV, for the\ntask of cross corpus valence regression from speech. Our MRD demonstrates\na significant 10% and 5% improvement in concordance correlation coefficients\n(CCC) compared to using baseline source-only methods, and we also show\nthat it outperforms two state-of-art domain adaptation techniques.\nFurther analysis reveals that our model is more effective in reducing\nsemantic distortion on low valence than high valence samples.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2037",
      "author_area_id": 3,
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "mao19_interspeech": {
      "authors": [
        [
          "Shuiyang",
          "Mao"
        ],
        [
          "P.C.",
          "Ching"
        ],
        [
          "Tan",
          "Lee"
        ]
      ],
      "title": "Deep Learning of Segment-Level Feature Representation with Multiple Instance Learning for Utterance-Level Speech Emotion Recognition",
      "original": "1968",
      "page_count": 5,
      "order": 350,
      "p1": "1686",
      "pn": "1690",
      "abstract": [
        "In this paper, we propose to combine the deep learning of feature representation\nwith multiple instance learning (MIL) to recognize emotion from speech.\nThe key idea of our approach is to first consciously classify the emotional\nstate of each segment. Then the utterance-level classification is constructed\nas an aggregation of the segment-level decisions. For the segment-level\nclassification, we attempt two different deep neural network (DNN)\narchitectures called SegMLP and SegCNN, respectively. SegMLP is a multilayer\nperceptron (MLP) that extracts high-level feature representation from\nthe manually designed perceptual features, and SegCNN is a convolutional\nneural network (CNN) that automatically learn emotion-specific features\nfrom the log Mel filterbanks. Extensive emotion recognition experiments\nare carried out on the CASIA corpus and the IEMOCAP database. We find\nthat: (1) the aggregation of segment-level decisions provides richer\ninformation than the statistics over the low-level descriptors (LLDs)\nacross the whole utterance; (2) automatic feature learning outperforms\nmanual features. Our experimental results are also compared with those\nof state-of-the-art methods, further demonstrating the effectiveness\nof the proposed approach.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1968",
      "author_area_id": 3,
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "triantafyllopoulos19_interspeech": {
      "authors": [
        [
          "Andreas",
          "Triantafyllopoulos"
        ],
        [
          "Gil",
          "Keren"
        ],
        [
          "Johannes",
          "Wagner"
        ],
        [
          "Ingmar",
          "Steiner"
        ],
        [
          "Bj\u00f6rn W.",
          "Schuller"
        ]
      ],
      "title": "Towards Robust Speech Emotion Recognition Using Deep Residual Networks for Speech Enhancement",
      "original": "1811",
      "page_count": 5,
      "order": 351,
      "p1": "1691",
      "pn": "1695",
      "abstract": [
        "The use of deep learning (DL) architectures for speech enhancement\nhas recently improved the robustness of voice applications under diverse\nnoise conditions. These improvements are usually evaluated based on\nthe perceptual quality of the enhanced audio or on the performance\nof automatic speech recognition (ASR) systems. We are interested instead\nin the usefulness of these algorithms in the field of speech emotion\nrecognition (SER), and specifically in whether an enhancement architecture\ncan effectively remove noise while preserving enough information for\nan SER algorithm to accurately identify emotion in speech. We first\nshow how a scalable DL architecture can be trained to enhance audio\nsignals in a large number of unseen environments, and go on to show\nhow that can benefit common SER pipelines in terms of noise robustness.\nOur results show that incorporating a speech enhancement architecture\nis beneficial, especially for low signal-to-noise ratio (SNR) conditions.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1811",
      "author_area_id": 3,
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "li19f_interspeech": {
      "authors": [
        [
          "Zhixuan",
          "Li"
        ],
        [
          "Liang",
          "He"
        ],
        [
          "Jingyang",
          "Li"
        ],
        [
          "Li",
          "Wang"
        ],
        [
          "Wei-Qiang",
          "Zhang"
        ]
      ],
      "title": "Towards Discriminative Representations and Unbiased Predictions: Class-Specific Angular Softmax for Speech Emotion Recognition",
      "original": "1683",
      "page_count": 5,
      "order": 352,
      "p1": "1696",
      "pn": "1700",
      "abstract": [
        "Speech emotion recognition (SER) is a challenging task: the complex\nemotional expressions make it difficult to discriminate different emotions;\nthe unbalanced data misleads models to give biased predictions. In\nthis work, we tackle these two problems by the angular softmax loss.\nFirst, we replace the vanilla softmax with angular softmax to learn\nemotional representations with strong discriminant power. Besides,\ninspired by its novel geometric interpretation, we establish a general\ncalculation model and deduce a concise formula of decision domain.\nBased on these derivations, we propose our solution to data imbalance:\nclass-specific angular softmax by which we can directly adjust decision\ndomains of different emotion classes. Experimental results on the IEMOCAP\ncorpus indicate significant improvements on two state-of-the-art models\ntherefore demonstrate the effectiveness of our proposed methods.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1683",
      "author_area_id": 3,
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "jalal19_interspeech": {
      "authors": [
        [
          "Md. Asif",
          "Jalal"
        ],
        [
          "Erfan",
          "Loweimi"
        ],
        [
          "Roger K.",
          "Moore"
        ],
        [
          "Thomas",
          "Hain"
        ]
      ],
      "title": "Learning Temporal Clusters Using Capsule Routing for Speech Emotion Recognition",
      "original": "3068",
      "page_count": 5,
      "order": 353,
      "p1": "1701",
      "pn": "1705",
      "abstract": [
        "Emotion recognition from speech plays a significant role in adding\nemotional intelligence to machines and making human-machine interaction\nmore natural. One of the key challenges from machine learning standpoint\nis to extract patterns which bear maximum correlation with the emotion\ninformation encoded in this signal while being as insensitive as possible\nto other types of information carried by speech. In this paper, we\npropose a novel temporal modelling framework for robust emotion classification\nusing bidirectional long short-term memory network (BLSTM), CNN and\nCapsule networks. The BLSTM deals with the temporal dynamics of the\nspeech signal by effectively representing forward/backward contextual\ninformation while the CNN along with the dynamic routing of the Capsule\nnet learn temporal clusters which altogether provide a state-of-the-art\ntechnique for classifying the extracted patterns. The proposed approach\nwas compared with a wide range of architectures on the FAU-Aibo and\nRAVDESS corpora and remarkable gain over state-of-the-art systems were\nobtained. For FAO-Aibo and RAVDESS 77.6% and 56.2% accuracy was achieved,\nrespectively, which is 3% and 14% (absolute) higher than the best-reported\nresult for the respective tasks.\n"
      ],
      "doi": "10.21437/Interspeech.2019-3068",
      "author_area_id": 3,
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "dapolito19_interspeech": {
      "authors": [
        [
          "Sonia",
          "d\u2019Apolito"
        ],
        [
          "Barbara Gili",
          "Fivela"
        ]
      ],
      "title": "L2 Pronunciation Accuracy and Context: A Pilot Study on the Realization of Geminates in Italian as L2 by French Learners",
      "original": "2934",
      "page_count": 5,
      "order": 354,
      "p1": "1706",
      "pn": "1710",
      "abstract": [
        "This paper investigates the interaction between the characteristics\nof both L1 and L2 phonetic-phonological systems and how the context,\nin terms of the amount of information available (less vs more information),\nmay influence the accuracy in producing L2 sounds as well as speech\nfluency. Specifically, it focuses on how French learners of Italian\nas L2, representing two different competence levels (lower and higher),\nrealize geminates (non-native sounds) in two different contexts (less\nand more rich). A rich context is expected to induce lower accuracy.\nAcoustic data of nine subjects (three beginners, three advanced and\nthree natives as control) were collected and analyzed in order to observe:\n1) the realization of geminates (duration of the consonant and preceding\nvowel as an index of accuracy); and 2) the speech fluency (number and\nduration of disfluencies; speech/articulation rate). Results suggest\nthat learners&#8217; productions are affected by L1, above all in the\ncase of beginners, who show a lower degree of accuracy. As regards\nthe accuracy and context interaction, results show that the production\nof geminates is more accurate (longer duration) in poor than in rich\ncontext. Further, a higher number of disfluencies is found in rich\nthan in poor context.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2934",
      "author_area_id": 2,
      "author_area_label": "Phonetics, Phonology, and Prosody"
    },
    "jamakovic19_interspeech": {
      "authors": [
        [
          "Nisad",
          "Jamakovic"
        ],
        [
          "Robert",
          "Fuchs"
        ]
      ],
      "title": "The Monophthongs of Formal Nigerian English: An Acoustic Analysis",
      "original": "2866",
      "page_count": 5,
      "order": 355,
      "p1": "1711",
      "pn": "1715",
      "abstract": [
        "Postcolonial varieties of English, used in countries such as Nigeria,\nthe Philippines and India, are influenced by local (&#8220;endonormative&#8221;)\nand external (&#8220;exonormative&#8221;) forces, the latter often\nin the form of British/American English. In the ensuing stylistic continuum,\ninformal speech is more endonormatively oriented than formal/educated\nspeech &#8212; which is, in turn, clearly distinguishable from British/American\nEnglish. The formal subvariety is often regarded as the incipient local\nstandard and is commonly less marked by L1 influence than the informal\nsubvariety.<br style=\"mso-data-placement:same-cell;\" /><br style=\"mso-data-placement:same-cell;\"\n/>\n  Nigerian English (NigE) is the most widely spoken African variety\nof English, but empirical/quantitative descriptions are rare. In this\npilot study, we present an acoustic analysis of eleven phonological\nmonophthongs and two phonological diphthongs that are commonly monophthongised.\nA total of 811 occurrences, produced in formal contexts by nine educated\nspeakers of NigE with L1 Igbo, was extracted from the ICE Nigeria corpus\nand analysed acoustically (Lobanov-normalised vowel formants at vowel\nmidpoint).<br style=\"mso-data-placement:same-cell;\" /><br style=\"mso-data-placement:same-cell;\"\n/>\n  Results show that the NigE speakers reduced the thirteen vowel\nsystem to a total of nine distinct phonemes that closely resembles\nthe L1 Igbo vowel inventory. This result suggests substantial L1 influence\neven at the level of Formal NigE.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2866",
      "author_area_id": 2,
      "author_area_label": "Phonetics, Phonology, and Prosody"
    },
    "arantes19_interspeech": {
      "authors": [
        [
          "Pablo",
          "Arantes"
        ],
        [
          "Anders",
          "Eriksson"
        ]
      ],
      "title": "Quantifying Fundamental Frequency Modulation as a Function of Language, Speaking Style and Speaker",
      "original": "2857",
      "page_count": 5,
      "order": 356,
      "p1": "1716",
      "pn": "1720",
      "abstract": [
        "In this study, we outline a methodology to quantify the degree of similarity\nbetween pairs of f<SUB>0</SUB> distributions based on the Anderson-Darling\nmeasure that underlies its namesake goodness-of-fit test. The procedure\nemphasizes differences due to more fine-grained f<SUB>0</SUB> modulations\nrather than differences in measures of central tendency, such as the\nmean and median. In order to assess the procedure&#8217;s usefulness\nfor speaker comparison, we applied it to a multilingual corpus in which\nparticipants contributed speech delivered in three speaking styles.\nThe similarity measure was calculated separately as function of speaking\nstyle and speaker. Between-speaker variability (different speakers,\nsame style) in distribution similarity varied significantly between\nstyles &#8212; spontaneous interview shows greater variability than\nread sentences and word list in five languages (English, French, Italian,\nPortuguese and Swedish); in Estonian and German, read sentences yield\nmore variability. Within-speaker variability (same speaker, different\nstyles) levels are lower than between-speaker in the style that exhibit\nthe greatest variability. The results point to the potential use of\nthe proposed methodology as a way to identify possible idiosyncratic\ntraits in f<SUB>0</SUB> distributions. Also, they further demonstrate\nthe effect of speaking styles on intonation patterns.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2857",
      "author_area_id": 2,
      "author_area_label": "Phonetics, Phonology, and Prosody"
    },
    "kelly19_interspeech": {
      "authors": [
        [
          "Niamh E.",
          "Kelly"
        ],
        [
          "Lara",
          "Keshishian"
        ]
      ],
      "title": "The Voicing Contrast in Stops and Affricates in the Western Armenian of Lebanon",
      "original": "2529",
      "page_count": 5,
      "order": 357,
      "p1": "1721",
      "pn": "1725",
      "abstract": [
        "Research on Western Armenian has described it as having a contrast\nbetween voiceless aspirated stops and affricates, and voiced stops\nand affricates [1, 2]. The variety of Western Armenian spoken by a\nlarge population in Lebanon has not yet been examined phonetically,\nto determine the acoustic correlates of this contrast. The current\nstudy examines the alveolar and postalveolar affricates and alveolar\nstops (voiceless aspirated and voiced) in both word-initial and word-medial\nposition, using nonsense words written in the Armenian script. The\nresults indicate that voiced sounds have prevoicing, voiceless affricates\nhave some aspiration, but voiceless stops have very short VOT, which\naligns better with an analysis of them being classified as unaspirated.\nIt was also found that position in the word does not affect VOT, duration\nof the closure or frication.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2529",
      "author_area_id": 2,
      "author_area_label": "Phonetics, Phonology, and Prosody"
    },
    "jatteau19_interspeech": {
      "authors": [
        [
          "Ad\u00e8le",
          "Jatteau"
        ],
        [
          "Ioana",
          "Vasilescu"
        ],
        [
          "Lori",
          "Lamel"
        ],
        [
          "Martine",
          "Adda-Decker"
        ],
        [
          "Nicolas",
          "Audibert"
        ]
      ],
      "title": "&#8220; Gra[f] e!&#8221; Word-Final Devoicing of Obstruents in Standard French: An Acoustic Study Based on Large Corpora",
      "original": "2329",
      "page_count": 5,
      "order": 358,
      "p1": "1726",
      "pn": "1730",
      "abstract": [
        "This study investigates the tendency towards word-final devoicing of\nvoiced obstruents in Standard French, and how devoicing is influenced\nby domain, speech style, manner and place of articulation. Three large\ncorpora with automatic segmentations produced by forced alignment are\nused: ESTER, ETAPE and NCCFr. A voicing-ratio is established for each\nobstruent via F0 extraction in Praat, and the percentage of fully voiced\nsegments is computed. We find a salient pattern of devoicing before\npause, with no clear effect of speech style. Fricatives devoice more\nthan stops, and posterior fricatives devoice more than anterior ones.\nSince voicing plays a central role in the cross-linguistic pattern\nof word-final [voice] neutralisation, this study gives insight into\nthe potential phonetic precursors of this process.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2329",
      "author_area_id": 2,
      "author_area_label": "Phonetics, Phonology, and Prosody"
    },
    "huang19g_interspeech": {
      "authors": [
        [
          "Chih-Hsiang",
          "Huang"
        ],
        [
          "Huang-Cheng",
          "Chou"
        ],
        [
          "Yi-Tong",
          "Wu"
        ],
        [
          "Chi-Chun",
          "Lee"
        ],
        [
          "Yi-Wen",
          "Liu"
        ]
      ],
      "title": "Acoustic Indicators of Deception in Mandarin Daily Conversations Recorded from an Interactive Game",
      "original": "2216",
      "page_count": 5,
      "order": 359,
      "p1": "1731",
      "pn": "1735",
      "abstract": [
        "Being able to distinguish the differences between deceptive and truthful\nstatements in a dialogue is an important skill in daily life. Extensive\nstudies on the acoustic features of deceptive English speech have been\nreported, but such research in Mandarin is relatively scarce. We constructed\na Mandarin deception database of daily dialogues from native speakers\nin Taiwan. College students were recruited to participate in a game\nin which they were encouraged to lie and convince their opponents of\nexperiences that they did not have. After data collection, acoustic-prosodic\nfeatures were extracted. The statistics of these features were calculated\nso that the differences between truthful and deceptive sentences, both\nas they were intended and perceived, can be compared. Results indicate\nthat different people tend to use different acoustic features when\ntelling a lie; the participants could be put into 10 categories in\na dendrogram, with an exception of 31 people from whom no acoustic\nindicators for deception were found. Without considering interpersonal\ndifferences, our best classifier reached an F1 score of 53.37% in distinguishing\ndeceptive and truthful segmentation units. We hope to present this\nnew database as a corpus for future studies on deception in Mandarin\nconversations.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2216",
      "author_area_id": 2,
      "author_area_label": "Phonetics, Phonology, and Prosody"
    },
    "schuppler19_interspeech": {
      "authors": [
        [
          "Barbara",
          "Schuppler"
        ],
        [
          "Margaret",
          "Zellers"
        ]
      ],
      "title": "Prosodic Effects on Plosive Duration in German and Austrian German",
      "original": "2197",
      "page_count": 5,
      "order": 360,
      "p1": "1736",
      "pn": "1740",
      "abstract": [
        "This study investigates the acoustic cues used to mark prosodic boundaries\nin two varieties of German, with a specific focus on variations in\nproduction of fortis and lenis plosives. We extracted prosodic-boundary-adjacent\nand non-boundary-adjacent plosives from GRASS (Austrian German) and\nthe Kiel Corpus of Read Speech (Northern German), and investigated\nclosure duration, burst features, and duration characteristics of the\nsurrounding segments. We find that closure and burst duration features,\nas well as duration of a preceding adjacent segment, vary consistently\nin relationship to the presence or absence of a prosodic boundary,\nbut that the relative weights of these features differ in the two varieties\nstudied.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2197",
      "author_area_id": 2,
      "author_area_label": "Phonetics, Phonology, and Prosody"
    },
    "johny19_interspeech": {
      "authors": [
        [
          "Cibu",
          "Johny"
        ],
        [
          "Alexander",
          "Gutkin"
        ],
        [
          "Martin",
          "Jansche"
        ]
      ],
      "title": "Cross-Lingual Consistency of Phonological Features: An Empirical Study",
      "original": "2184",
      "page_count": 5,
      "order": 361,
      "p1": "1741",
      "pn": "1745",
      "abstract": [
        "The concept of a phoneme arose historically as a theoretical abstraction\nthat applies language-internally. Using phonemes and phonological features\nin cross-linguistic settings raises an important question of conceptual\nvalidity: Are contrasts that are meaningful within a language also\nempirically robust across languages? This paper develops a method for\nassessing the crosslinguistic consistency of phonological features\nin phoneme inventories. The method involves training separate binary\nneural classifiers for several phonological contrast in audio spans\ncentered on particular segments within continuous speech. To assess\ncross-linguistic consistency, these classifiers are evaluated on held-out\nlanguages and classification quality is reported. We apply this method\nto several common phonological contrasts, including vowel height, vowel\nfrontness, and retroflex consonants, in the context of multi-speaker\ncorpora for ten languages from three language families (Indo-Aryan,\nDravidian, and Malayo-Polynesian). We empirically evaluate and discuss\nthe consistency of phonological contrasts derived from features found\nin phonological ontologies such as  PanPhon and PHOIBLE.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2184",
      "author_area_id": 2,
      "author_area_label": "Phonetics, Phonology, and Prosody"
    },
    "guitardivent19_interspeech": {
      "authors": [
        [
          "Fanny",
          "Guitard-Ivent"
        ],
        [
          "Gabriele",
          "Chignoli"
        ],
        [
          "C\u00e9cile",
          "Fougeron"
        ],
        [
          "Laurianne",
          "Georgeton"
        ]
      ],
      "title": "Are IP Initial Vowels Acoustically More Distinct? Results from LDA and CNN Classifications",
      "original": "2153",
      "page_count": 5,
      "order": 362,
      "p1": "1746",
      "pn": "1750",
      "abstract": [
        "Past results have suggested that initial strengthening (IS) effects\ntarget the contrastive phonetic properties of segments, with a maximization\nof acoustic contrasts in initial position of strong prosodic domains.\nHere, we investigate whether IS effects translate into a better acoustic\ndiscriminability within the French oral vowels system. Discriminability\nis assessed on the basis of classification results of two types of\nclassifiers: a linear discriminant analysis (LDA) based on the four\nformants frequencies, and a deep convolutional neural network (CNN)\nbased on spectrograms. The test set includes 720 exemplars of /i, y,\ne, &#949;, a, x, u, o, &#596;/ (with /x/=/&#248;, &#339;/) produced\nin a labial context, either in intonational phrase initial (IPi) or\nword initial (Wi) position. Classifiers were trained using a set of\n4500 vowels extracted from a large read speech corpus. Results show\na better discriminability of vowels (overall better classification\nrate) in IPi than in Wi with the two methods. Less confusion in IPi\nis found between rounded and unrounded, and between back and front\nvowels, but not between the vowels along the four-way height contrast.\nLess confusion between peripheral and central vowels also expresses\na maximization of contrasts within the acoustic space in IPi position.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2153",
      "author_area_id": 2,
      "author_area_label": "Phonetics, Phonology, and Prosody"
    },
    "wei19_interspeech": {
      "authors": [
        [
          "Xizi",
          "Wei"
        ],
        [
          "Melvyn",
          "Hunt"
        ],
        [
          "Adrian",
          "Skilling"
        ]
      ],
      "title": "Neural Network-Based Modeling of Phonetic Durations",
      "original": "2102",
      "page_count": 5,
      "order": 363,
      "p1": "1751",
      "pn": "1755",
      "abstract": [
        "A deep neural network (DNN)-based model has been developed to predict\nnon-parametric distributions of durations of phonemes in specified\nphonetic contexts and used to explore which factors influence durations\nmost. Major factors in US English are pre-pausal lengthening, lexical\nstress, and speaking rate. The model can be used to check that text-to-speech\n(TTS) training speech follows the script and words are pronounced as\nexpected. Duration prediction is poorer with training speech for automatic\nspeech recognition (ASR) because the training corpus typically consists\nof single utterances from many speakers and is often noisy or casually\nspoken. Low probability durations in ASR training material nevertheless\nmostly correspond to non-standard speech, with some having disfluencies.\nChildren&#8217;s speech is disproportionately present in these utterances,\nsince children show much more variation in timing.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2102",
      "author_area_id": 2,
      "author_area_label": "Phonetics, Phonology, and Prosody"
    },
    "moczanow19_interspeech": {
      "authors": [
        [
          "Janina",
          "Mo\u0142czanow"
        ],
        [
          "Beata",
          "\u0141ukaszewicz"
        ],
        [
          "Anna",
          "\u0141ukaszewicz"
        ]
      ],
      "title": "An Acoustic Study of Vowel Undershoot in a System with Several Degrees of Prominence",
      "original": "1806",
      "page_count": 5,
      "order": 364,
      "p1": "1756",
      "pn": "1760",
      "abstract": [
        "The paper presents the results of a pilot study investigating the relationship\nbetween vowel quality and duration in Ukrainian. In this language,\nlexical stress is cued by increased duration; smaller but systematic\ndifferences in length occur between unstressed, rhythmic stress-bearing,\nand pretonic syllables. The presence of several degrees of lengthening\nwithin one word makes it possible to test the long-established theories\nof vowel reduction posing a direct link between decreased duration\nand vowel undershoot. Overall, the analysis of the aggregated data\ncollected from four native speakers of Ukrainian points to a strong\ncorrelation between decreasing duration and the undershoot of F1 targets.\nHowever, in separate by-position and by-speaker analyses, no correlation\nbetween F1 and duration is observed in the positions of rhythmic and\nlexical stress. We thus conclude that the stability of the F1 target\n vis-&#224;-vis temporal parameters may constitute another parameter\nexpressing metrical prominence. In addition, our data suggests that\nformant undershoot may be affected by an articulatory effort. \n"
      ],
      "doi": "10.21437/Interspeech.2019-1806",
      "author_area_id": 2,
      "author_area_label": "Phonetics, Phonology, and Prosody"
    },
    "berger19_interspeech": {
      "authors": [
        [
          "Stephanie",
          "Berger"
        ],
        [
          "Oliver",
          "Niebuhr"
        ],
        [
          "Margaret",
          "Zellers"
        ]
      ],
      "title": "A Preliminary Study of Charismatic Speech on YouTube: Correlating Prosodic Variation with Counts of Subscribers, Views and Likes",
      "original": "1664",
      "page_count": 5,
      "order": 365,
      "p1": "1761",
      "pn": "1765",
      "abstract": [
        "This paper is a first investigation into the influence of the pitch\nrange and the intensity variation on the number of subscribers, views\nand likes of YouTube Creators. A total of ten minutes of speech material\nfrom five English and five North-American YouTubers was analyzed. The\nresults for pitch range and intensity variation suggest that an increase\nin both parameters results in higher subscriber counts. For views,\nthere was no influence of pitch range, but an increase in intensity\nvariation results in a lower number of views. Pitch range and intensity\nvariation had no influence on the like count. Furthermore, both origin\nand gender had an influence on the results. Ultimately, this study\nwill provide further information for the phonetic research of charisma\n(i.e., the perceived charm, competence, power, and persuasiveness of\na speaker), as it is suspected that the acoustic features that have\nso far been connected to charisma also play an important role in the\nsuccess of a YouTuber and their channel.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1664",
      "author_area_id": 2,
      "author_area_label": "Phonetics, Phonology, and Prosody"
    },
    "luo19b_interspeech": {
      "authors": [
        [
          "Shan",
          "Luo"
        ]
      ],
      "title": "Phonetic Detail Encoding in Explaining the Size of Speech Planning Window",
      "original": "1412",
      "page_count": 5,
      "order": 366,
      "p1": "1766",
      "pn": "1770",
      "abstract": [
        "With the ultimate goal of understanding the production planning scope,\nthis study manipulates phonetic information (place of articulation\nand voicing) and measures three acoustic cues to analyze consonant\nclusters across words produced by English (L1) and Mandarin (L2) speakers.\nWe continue to explore a) how phonetic detail interacts with prosodic\nboundary in modulating surface realization, and b) the roles of phonetic\ninformation in speech planning motor control. The results show that\nL2 speakers exhibited different acoustic deviations varying with their\nproficiency level. The group with lower L2 proficiency significantly\ndeviated from the L1 group in release likelihood and closure shortening,\nwhile the higher-proficiency group exhibited less nativelike performance\nin terms of closure durations. The results also discover that all speakers\nare subject to language-independent articulatory constraint at word\nboundaries, while language-specific phonetic detail accounts for more\nnonnative deviations. The core findings highlight a long-distance speech\nplanning scope in native speech, with cross-word phonetic information\ninteracting with prosodic encoding. It is argued that phonology applies\nblindly across words and is independent of lexical cognitive load.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1412",
      "author_area_id": 2,
      "author_area_label": "Phonetics, Phonology, and Prosody"
    },
    "zarka19_interspeech": {
      "authors": [
        [
          "Dina El",
          "Zarka"
        ],
        [
          "Barbara",
          "Schuppler"
        ],
        [
          "Francesco",
          "Cangemi"
        ]
      ],
      "title": "Acoustic Cues to Topic and Narrow Focus in Egyptian Arabic",
      "original": "1189",
      "page_count": 5,
      "order": 367,
      "p1": "1771",
      "pn": "1775",
      "abstract": [
        "This study investigates acoustic cues (duration, scaling and alignment\nof peaks and valleys) to the prosodic realization of  topics and  narrow\nsubject foci in a declarative SVO sentence in Egyptian Arabic. Morpho-syntactically\nidentical sentences were elicited in appropriately designed contexts\nfrom 18 native speakers by means of a question-answer paradigm. The\nresults show that the stressed syllable of a focused word is longer\nthan the stressed syllable of the same word in topic condition. Additionally,\nthe peaks of foci are generally scaled higher than those of topics.\nThese differences clearly point to varying degrees of prosodic prominence.\nFurthermore, the alignment of the F0 peak and the subsequent low endpoint\nof a rising-falling tonal contour is earlier in foci than in topics,\nindicating that focus is signaled by an early sharp fall whereas the\nfalling part of the tonal gesture starts later and is shallower in\nthe case of a topic. Overall, our results suggest that narrow subject\nfoci and topics tend to be associated with different pitch events.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1189",
      "author_area_id": 2,
      "author_area_label": "Phonetics, Phonology, and Prosody"
    },
    "alowonou19_interspeech": {
      "authors": [
        [
          "Kowovi Comivi",
          "Alowonou"
        ],
        [
          "Jianguo",
          "Wei"
        ],
        [
          "Wenhuan",
          "Lu"
        ],
        [
          "Zhicheng",
          "Liu"
        ],
        [
          "Kiyoshi",
          "Honda"
        ],
        [
          "Jianwu",
          "Dang"
        ]
      ],
      "title": "Acoustic and Articulatory Study of Ewe Vowels: A Comparative Study of Male and Female",
      "original": "2196",
      "page_count": 5,
      "order": 368,
      "p1": "1776",
      "pn": "1780",
      "abstract": [
        "In order to investigate the difference in Ewe males and Ewe females\nduring the production of Ewe vowels, results from the comparative quantitative\nand qualitative assessments of tongue shape and movement using ultrasound\nimaging as well as the comparative evaluation of F1 and F2 frequency\nvalues from data collected from 9 Ewe male speakers and 6 Ewe female\nspeakers, were presented in this study. The results showed that vowels\nare produced with higher formant frequencies by Ewe female speakers\ncompared to Ewe male speakers, except for the vowel /&#949;/ produced\nwith a lower F1 frequency by Ewe females. The articulatory results\nshowed a higher and more forwarder tongue configuration for Ewe male\ncompared to female counterparts.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2196"
    },
    "guo19c_interspeech": {
      "authors": [
        [
          "Ya\u2019nan",
          "Guo"
        ],
        [
          "Ziping",
          "Zhao"
        ],
        [
          "Yide",
          "Ma"
        ],
        [
          "Bj\u00f6rn W.",
          "Schuller"
        ]
      ],
      "title": "Speech Augmentation via Speaker-Specific Noise in Unseen Environment",
      "original": "2712",
      "page_count": 5,
      "order": 369,
      "p1": "1781",
      "pn": "1785",
      "abstract": [
        "Speech augmentation is a common and effective strategy to avoid overfitting\nand improve on the robustness of an emotion recognition model. In this\npaper, we investigate for the first time the intrinsic attributes in\na speech signal using the multi-resolution analysis theory and the\nHilbert-Huang Spectrum, with the goal of developing a robust speech\naugmentation approach from raw speech data. Specifically, speech decomposition\nin a double tree complex wavelet transform domain is realized, to obtain\nsub-speech signals; then, the Hilbert Spectrum using Hilbert-Huang\nTransform is calculated for each sub-band to capture the noise content\nin unseen environments with the voice restriction to 100&#8211;4000\nHz; finally, the speech-specific noise that varies with the speaker\nindividual, scenarios, environment, and voice recording equipment,\ncan be reconstructed from the top two high-frequency sub-bands to enhance\nthe raw signal. Our proposed speech augmentation is demonstrated using\nfive robust machine learning architectures based on the RAVDESS database,\nachieving up to 9.3% higher accuracy compared to the performance on\nraw data for an emotion recognition task.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2712",
      "author_area_id": 6,
      "author_area_label": "Speech Coding and Enhancement"
    },
    "hao19_interspeech": {
      "authors": [
        [
          "Xiang",
          "Hao"
        ],
        [
          "Xiangdong",
          "Su"
        ],
        [
          "Zhiyu",
          "Wang"
        ],
        [
          "Hui",
          "Zhang"
        ],
        [
          "",
          "Batushiren"
        ]
      ],
      "title": "UNetGAN: A Robust Speech Enhancement Approach in Time Domain for Extremely Low Signal-to-Noise Ratio Condition",
      "original": "1567",
      "page_count": 5,
      "order": 370,
      "p1": "1786",
      "pn": "1790",
      "abstract": [
        "Speech enhancement at extremely low signal-to-noise ratio (SNR) condition\nis a very challenging problem and rarely investigated in previous works.\nThis paper proposes a robust speech enhancement approach (UNetGAN)\nbased on U-Net and generative adversarial learning to deal with this\nproblem. This approach consists of a generator network and a discriminator\nnetwork, which operate directly in the time domain. The generator network\nadopts a U-Net like structure and employs dilated convolution in the\nbottleneck of it. We evaluate the performance of the UNetGAN at low\nSNR conditions (up to -20dB) on the public benchmark. The result demonstrates\nthat it significantly improves the speech quality and substantially\noutperforms the representative deep learning models, including SEGAN,\ncGAN fo SE, Bidirectional LSTM using phase-sensitive spectrum approximation\ncost function (PSA-BLSTM) and Wave-U-Net regarding Short-Time Objective\nIntelligibility (STOI) and Perceptual evaluation of speech quality\n(PESQ).\n"
      ],
      "doi": "10.21437/Interspeech.2019-1567",
      "author_area_id": 6,
      "author_area_label": "Speech Coding and Enhancement"
    },
    "pascual19b_interspeech": {
      "authors": [
        [
          "Santiago",
          "Pascual"
        ],
        [
          "Joan",
          "Serr\u00e0"
        ],
        [
          "Antonio",
          "Bonafonte"
        ]
      ],
      "title": "Towards Generalized Speech Enhancement with Generative Adversarial Networks",
      "original": "2688",
      "page_count": 5,
      "order": 371,
      "p1": "1791",
      "pn": "1795",
      "abstract": [
        "The speech enhancement task usually consists of removing additive noise\nor reverberation that partially mask spoken utterances, affecting their\nintelligibility. However, little attention is drawn to other, perhaps\nmore aggressive signal distortions like clipping, chunk elimination,\nor frequency-band removal. Such distortions can have a large impact\nnot only on intelligibility, but also on naturalness or even speaker\nidentity, and require of careful signal reconstruction. In this work,\nwe give full consideration to this generalized speech enhancement task,\nand show it can be tackled with a time-domain generative adversarial\nnetwork (GAN). In particular, we extend a previous GAN-based speech\nenhancement system to deal with mixtures of four types of aggressive\ndistortions. Firstly, we propose the addition of an adversarial acoustic\nregression loss that promotes a richer feature extraction at the discriminator.\nSecondly, we also make use of a two-step adversarial training schedule,\nacting as a warm up-and-fine-tune sequence. Both objective and subjective\nevaluations show that these two additions bring improved speech reconstructions\nthat better match the original speaker identity and naturalness.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2688",
      "author_area_id": 6,
      "author_area_label": "Speech Coding and Enhancement"
    },
    "li19g_interspeech": {
      "authors": [
        [
          "Xiaoqi",
          "Li"
        ],
        [
          "Yaxing",
          "Li"
        ],
        [
          "Meng",
          "Li"
        ],
        [
          "Shan",
          "Xu"
        ],
        [
          "Yuanjie",
          "Dong"
        ],
        [
          "Xinrong",
          "Sun"
        ],
        [
          "Shengwu",
          "Xiong"
        ]
      ],
      "title": "A Convolutional Neural Network with Non-Local Module for Speech Enhancement",
      "original": "2472",
      "page_count": 5,
      "order": 372,
      "p1": "1796",
      "pn": "1800",
      "abstract": [
        "Convolution neural networks (CNNs) are achieving increasing attention\nfor the speech enhancement task recently. However, the convolutional\noperations only process a local neighborhood (several nearest neighboring\nneurons) at a time across either space or time direction. The long-range\ndependencies can only be captured when the convolutional operations\nare applied recursively, but the problems of computationally inefficient\nand optimization difficulties are introduced. Inspired by the recent\nimpressive performance of the non-local module in many computer vision\ntasks, we propose a convolutional neural network with non-local module\nfor speech enhancement in this paper. The non-local operations are\ncapable of capturing the global information in the frequency domain\nthrough passing information between distant time-frequency units. The\nnon-local operations are able to set the dimension of the input as\nan arbitrary value, which results in the easy integration with our\nproposed network framework. Experimental results demonstrate that the\nproposed method not only improves the computational efficiency significantly\nbut also outperforms the competing methods in terms of objective speech\nintelligibility and quality metrics.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2472",
      "author_area_id": 6,
      "author_area_label": "Speech Coding and Enhancement"
    },
    "lin19b_interspeech": {
      "authors": [
        [
          "Yu-Chen",
          "Lin"
        ],
        [
          "Yi-Te",
          "Hsu"
        ],
        [
          "Szu-Wei",
          "Fu"
        ],
        [
          "Yu",
          "Tsao"
        ],
        [
          "Tei-Wei",
          "Kuo"
        ]
      ],
      "title": "IA-NET: Acceleration and Compression of Speech Enhancement Using Integer-Adder Deep Neural Network",
      "original": "1207",
      "page_count": 5,
      "order": 373,
      "p1": "1801",
      "pn": "1805",
      "abstract": [
        "Numerous compression and acceleration techniques achieved state-of-the-art\nresults for classification tasks in speech processing. However, the\nsame techniques produce unsatisfactory performance for regression tasks,\nbecause of the different natures of classification and regression tasks.\nThis paper presents a novel integer-adder deep neural network (IA-Net),\nwhich compresses model size and accelerates the inference process in\nspeech enhancement, an important task in speech-signal processing,\nby replacing the floating-point multiplier with an integer-adder. The\nexperimental results show that the inference time of IA-Net can be\nsignificantly reduced by 20% and the model size can be compressed by\n71.9% without any performance degradation. To the best of our knowledge,\nthis is the first study that decreases the inference time and compresses\nthe model size, simultaneously, while producing good performance for\nspeech enhancement. Based on the promising results, we believe that\nthe proposed framework can be deployed in various mobile and edge-computing\ndevices.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1207",
      "author_area_id": 6,
      "author_area_label": "Speech Coding and Enhancement"
    },
    "chai19_interspeech": {
      "authors": [
        [
          "Li",
          "Chai"
        ],
        [
          "Jun",
          "Du"
        ],
        [
          "Chin-Hui",
          "Lee"
        ]
      ],
      "title": "KL-Divergence Regularized Deep Neural Network Adaptation for Low-Resource Speaker-Dependent Speech Enhancement",
      "original": "2426",
      "page_count": 5,
      "order": 374,
      "p1": "1806",
      "pn": "1810",
      "abstract": [
        "In this paper, we propose a Kullback-Leibler divergence (KLD) regularized\napproach to adapting speaker-independent (SI) speech enhancement model\nbased on regression deep neural networks (DNNs) to another speaker-dependent\n(SD) model using a tiny amount of speaker-specific adaptation data.\nThis algorithm adapts the DNN model conservatively by forcing the conditional\ntarget distribution estimated from the SD model to be close to that\nfrom the SI model. The constraint is realized by adding KLD regularization\nto our previously proposed maximum likelihood objective function. Experimental\nresults demonstrate that, even with only 10 seconds of SD adaptation\ndata, the proposed framework consistently achieves speech intelligibility\nimprovements under all 15 unseen noise types evaluated and at all signal-to-noise\nratio levels for all 8 test speakers from the WSJ0 evaluation set.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2426",
      "author_area_id": 6,
      "author_area_label": "Speech Coding and Enhancement"
    },
    "llombart19_interspeech": {
      "authors": [
        [
          "Jorge",
          "Llombart"
        ],
        [
          "Dayana",
          "Ribas"
        ],
        [
          "Antonio",
          "Miguel"
        ],
        [
          "Luis",
          "Vicente"
        ],
        [
          "Alfonso",
          "Ortega"
        ],
        [
          "Eduardo",
          "Lleida"
        ]
      ],
      "title": "Speech Enhancement with Wide Residual Networks in Reverberant Environments",
      "original": "1745",
      "page_count": 5,
      "order": 375,
      "p1": "1811",
      "pn": "1815",
      "abstract": [
        "This paper proposes a speech enhancement method which exploits the\nhigh potential of residual connections in a Wide Residual Network architecture.\nThis is supported on single dimensional convolutions computed alongside\nthe time domain, which is a powerful approach to process contextually\ncorrelated representations through the temporal domain, such as speech\nfeature sequences. We find the residual mechanism extremely useful\nfor the enhancement task since the signal always has a linear shortcut\nand the non-linear path enhances it in several steps by adding or subtracting\ncorrections. The enhancement capability of the proposal is assessed\nby objective quality metrics evaluated with simulated and real samples\nof reverberated speech signals. Results show that the proposal outperforms\nthe state-of-the-art method called WPE, which is known to effectively\nreduce reverberation and greatly enhance the signal. The proposed model,\ntrained with artificial synthesized reverberation data, was able to\ngeneralize to real room impulse responses for a variety of conditions\n(e.g. different room sizes,  RT<SUB>60</SUB>, near &amp; far field).\nFurthermore, it achieves accuracy for real speech with reverberation\nfrom two different datasets.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1745",
      "author_area_id": 6,
      "author_area_label": "Speech Coding and Enhancement"
    },
    "reddy19_interspeech": {
      "authors": [
        [
          "Chandan K.A.",
          "Reddy"
        ],
        [
          "Ebrahim",
          "Beyrami"
        ],
        [
          "Jamie",
          "Pool"
        ],
        [
          "Ross",
          "Cutler"
        ],
        [
          "Sriram",
          "Srinivasan"
        ],
        [
          "Johannes",
          "Gehrke"
        ]
      ],
      "title": "A Scalable Noisy Speech Dataset and Online Subjective Test Framework",
      "original": "3087",
      "page_count": 5,
      "order": 376,
      "p1": "1816",
      "pn": "1820",
      "abstract": [
        "Background noise is a major source of quality impairments in Voice\nover Internet Protocol (VoIP) and Public Switched Telephone Network\n(PSTN) calls. Recent work shows the efficacy of deep learning for noise\nsuppression, but the datasets have been relatively small compared to\nthose used in other domains (e.g., ImageNet) and the associated evaluations\nhave been more focused. In order to better facilitate deep learning\nresearch in Speech Enhancement, we present a noisy speech dataset (MS-SNSD)\nthat can scale to arbitrary sizes depending on the number of speakers,\nnoise types, and Speech to Noise Ratio (SNR) levels desired. We show\nthat increasing dataset sizes increases noise suppression performance\nas expected. In addition, we provide an open-source evaluation methodology\nto evaluate the results subjectively at scale using crowdsourcing,\nwith a reference algorithm to normalize the results. To demonstrate\nthe dataset and evaluation framework we apply it to several noise suppressors\nand compare the subjective Mean Opinion Score (MOS) with objective\nquality measures such as SNR, PESQ, POLQA, and VISQOL and show why\nMOS is still required. Our subjective MOS evaluation is the first large\nscale evaluation of Speech Enhancement algorithms that we are aware\nof.\n"
      ],
      "doi": "10.21437/Interspeech.2019-3087",
      "author_area_id": 6,
      "author_area_label": "Speech Coding and Enhancement"
    },
    "adiga19_interspeech": {
      "authors": [
        [
          "Nagaraj",
          "Adiga"
        ],
        [
          "Yannis",
          "Pantazis"
        ],
        [
          "Vassilis",
          "Tsiaras"
        ],
        [
          "Yannis",
          "Stylianou"
        ]
      ],
      "title": "Speech Enhancement for Noise-Robust Speech Synthesis Using Wasserstein GAN",
      "original": "2648",
      "page_count": 5,
      "order": 377,
      "p1": "1821",
      "pn": "1825",
      "abstract": [
        "The quality of speech synthesis systems can be significantly deteriorated\nby the presence of background noise in the recordings. Despite the\nexistence of speech enhancement techniques for effectively suppressing\nadditive noise under low signal-to-noise (SNR) conditions, these techniques\nhave been neither designed nor tested in speech synthesis tasks where\nbackground noise has relatively lower energy. In this paper, we propose\na speech enhancement technique based on generative adversarial networks\n(GANs) which acts as a preprocessing step of speech synthesis. Motivated\nby the speech enhancement generative adversarial network (SEGAN) approach\nand recent advances in deep learning, we propose to use Wasserstein\nGAN (WGAN) with gradient penalty and gated activation functions to\nthe autoencoder network of SEGAN. We studied the impact of the proposed\nmethod on a data set consisting of 28 speakers and different noise\ntypes with 3 different SNR level. The effectiveness of the proposed\nmethod in the context of speech synthesis is demonstrated through the\ntraining of WaveNet vocoder.  We compare our method against SEGAN.\nBoth subjective and objective metrics confirm that the proposed speech\nenhancement approach outperforms SEGAN in terms of speech synthesis\nquality.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2648",
      "author_area_id": 6,
      "author_area_label": "Speech Coding and Enhancement"
    },
    "pv19_interspeech": {
      "authors": [
        [
          "Muhammed Shifas",
          "P.V."
        ],
        [
          "Nagaraj",
          "Adiga"
        ],
        [
          "Vassilis",
          "Tsiaras"
        ],
        [
          "Yannis",
          "Stylianou"
        ]
      ],
      "title": "A Non-Causal FFTNet Architecture for Speech Enhancement",
      "original": "2622",
      "page_count": 5,
      "order": 378,
      "p1": "1826",
      "pn": "1830",
      "abstract": [
        "In this paper, we suggest a new parallel, non-causal and shallow waveform\ndomain architecture for speech enhancement based on FFTNet, a neural\nnetwork for generating high quality audio waveform. In contrast to\nother waveform based approaches like WaveNet, FFTNet uses an initial\nwide dilation pattern. Such an architecture better represents the long\nterm correlated structure of speech in the time domain, where noise\nis usually highly non-correlated, and therefore it is suitable for\nwaveform domain based speech enhancement. To further strengthen this\nfeature of FFTNet, we suggest a non-causal FFTNet architecture, where\nthe present sample in each layer is estimated from the past and future\nsamples of the previous layer. By suggesting a shallow network and\napplying non-causality within certain limits, the suggested FFTNet\nfor speech enhancement (SE-FFTNet) uses much fewer parameters compared\nto other neural network based approaches for speech enhancement like\nWaveNet and SEGAN. Specifically, the suggested network has considerably\nreduced model parameters: 32% fewer compared to WaveNet and 87% fewer\ncompared to SEGAN. Finally, based on subjective and objective metrics,\nSE-FFTNet outperforms WaveNet in terms of enhanced signal quality,\nwhile it provides equally good performance as SEGAN.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2622",
      "author_area_id": 6,
      "author_area_label": "Speech Coding and Enhancement"
    },
    "braithwaite19_interspeech": {
      "authors": [
        [
          "D.T.",
          "Braithwaite"
        ],
        [
          "W. Bastiaan",
          "Kleijn"
        ]
      ],
      "title": "Speech Enhancement with Variance Constrained Autoencoders",
      "original": "1809",
      "page_count": 5,
      "order": 379,
      "p1": "1831",
      "pn": "1835",
      "abstract": [
        "Recent machine learning based approaches to speech enhancement operate\nin the time domain and have been shown to outperform the classical\nenhancement methods. Two such models are SE-GAN and SE-WaveNet, both\nof which rely on complex neural network architectures, making them\nexpensive to train. We propose using the Variance Constrained Autoencoder\n(VCAE) for speech enhancement. Our model uses a more straightforward\nneural network structure than competing solutions and is a natural\nmodel for the task of speech enhancement. We demonstrate experimentally\nthat the proposed enhancement model outperforms SE-GAN and SE-WaveNet\nin terms of perceptual quality of enhanced signals.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1809",
      "author_area_id": 6,
      "author_area_label": "Speech Coding and Enhancement"
    },
    "kyriakopoulos19_interspeech": {
      "authors": [
        [
          "Konstantinos",
          "Kyriakopoulos"
        ],
        [
          "Kate M.",
          "Knill"
        ],
        [
          "Mark J.F.",
          "Gales"
        ]
      ],
      "title": "A Deep Learning Approach to Automatic Characterisation of Rhythm in Non-Native English Speech",
      "original": "3186",
      "page_count": 5,
      "order": 380,
      "p1": "1836",
      "pn": "1840",
      "abstract": [
        "A speaker&#8217;s rhythm contributes to the intelligibility of their\nspeech and can be characteristic of their language and accent. For\nnon-native learners of a language, the extent to which they match its\nnatural rhythm is an important predictor of their proficiency. As a\nlearner improves, their rhythm is expected to become less similar to\ntheir L1 and more to the L2. Metrics based on the variability of the\ndurations of vocalic and consonantal intervals have been shown to be\neffective at detecting language and accent. In this paper, pairwise\nvariability (PVI, CCI) and variance (varcoV, varcoC) metrics are first\nused to predict proficiency and L1 of non-native speakers taking an\nEnglish spoken exam. A deep learning alternative to generalise these\nfeatures is then presented, in the form of a tunable duration embedding,\nbased on attention over an RNN over durations. The RNN allows relationships\nbeyond pairwise to be captured, while attention allows sensitivity\nto the different relative importance of durations. The system is trained\nend-to-end for proficiency and L1 prediction and compared to the baseline.\nThe values of both sets of features for different proficiency levels\nare then visualised and compared to native speech in the L1 and the\nL2.\n"
      ],
      "doi": "10.21437/Interspeech.2019-3186",
      "author_area_id": 10,
      "author_area_label": "Speech Recognition: Technologies and Systems for New Applications"
    },
    "merkx19_interspeech": {
      "authors": [
        [
          "Danny",
          "Merkx"
        ],
        [
          "Stefan L.",
          "Frank"
        ],
        [
          "Mirjam",
          "Ernestus"
        ]
      ],
      "title": "Language Learning Using Speech to Image Retrieval",
      "original": "3067",
      "page_count": 5,
      "order": 381,
      "p1": "1841",
      "pn": "1845",
      "abstract": [
        "Humans learn language by interaction with their environment and listening\nto other humans. It should also be possible for computational models\nto learn language directly from speech but so far most approaches require\ntext. We improve on existing neural network approaches to create visually\ngrounded embeddings for spoken utterances. Using a combination of a\nmulti-layer GRU, importance sampling, cyclic learning rates, ensembling\nand vectorial self-attention our results show a remarkable increase\nin image-caption retrieval performance over previous work. Furthermore,\nwe investigate which layers in the model learn to recognise words in\nthe input. We find that deeper network layers are better at encoding\nword presence, although the final layer has slightly lower performance.\nThis shows that our visually grounded sentence encoder learns to recognise\nwords from the input even though it is not explicitly trained for word\nrecognition.\n"
      ],
      "doi": "10.21437/Interspeech.2019-3067",
      "author_area_id": 10,
      "author_area_label": "Speech Recognition: Technologies and Systems for New Applications"
    },
    "skidmore19_interspeech": {
      "authors": [
        [
          "Lucy",
          "Skidmore"
        ],
        [
          "Roger K.",
          "Moore"
        ]
      ],
      "title": "Using Alexa for Flashcard-Based Learning",
      "original": "2893",
      "page_count": 5,
      "order": 382,
      "p1": "1846",
      "pn": "1850",
      "abstract": [
        "Despite increasing awareness of Alexa&#8217;s potential as an educational\ntool, there remains a limited scope for Alexa skills to accommodate\nthe features required for effective language learning. This paper describes\nan investigation into implementing &#8216;spaced-repetition&#8217;,\na non-trivial feature of flashcard-based learning, through the development\nof an Alexa skill called &#8216;Japanese Flashcards&#8217;. Here we\nshow that existing Alexa development features such as skill persistence\nallow for the effective implementation of spaced-repetition and suggest\na heuristic adaptation of the spaced-repetition model that is appropriate\nfor use with voice assistants (VAs). We also highlight areas of the\nAlexa development process that limit the facilitation of language learning,\nnamely the lack of multilingual speech recognition, and offer solutions\nto these current limitations. Overall, the investigation shows that\nAlexa can successfully facilitate simple L2-L1 flashcard-based language\nlearning and highlights the potential for Alexa to be used as a sophisticated\nand effective language learning tool.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2893"
    },
    "hansen19_interspeech": {
      "authors": [
        [
          "John H.L.",
          "Hansen"
        ],
        [
          "Aditya",
          "Joglekar"
        ],
        [
          "Meena Chandra",
          "Shekhar"
        ],
        [
          "Vinay",
          "Kothapally"
        ],
        [
          "Chengzhu",
          "Yu"
        ],
        [
          "Lakshmish",
          "Kaushik"
        ],
        [
          "Abhijeet",
          "Sangwan"
        ]
      ],
      "title": "The 2019 Inaugural Fearless Steps Challenge: A Giant Leap for Naturalistic Audio",
      "original": "2301",
      "page_count": 5,
      "order": 383,
      "p1": "1851",
      "pn": "1855",
      "abstract": [
        "The 2019 FEARLESS STEPS (FS-1) Challenge is an initial step to motivate\na streamlined and collaborative effort from the speech and language\ncommunity towards addressing massive naturalistic audio, the first\nof its kind. The Fearless Steps Corpus is a collection of 19,000 hours\nof multi-channel recordings of spontaneous speech from over 450 speakers\nunder multiple noise conditions. A majority of the Apollo Missions\noriginal analog data is unlabeled and has thus far motivated the development\nof both unsupervised and semi-supervised strategies. This edition of\nthe challenge encourages the development of core speech and language\ntechnology systems for data with limited ground-truth / low resource\navailability and is intended to serve as the &#8220;First Step&#8221;\ntowards extracting high-level information from such massive unlabeled\ncorpora. In conjunction with the Challenge, 11,000 hours of synchronized\n30-channel Apollo-11 audio data has also been released to the public\nby CRSS-UTDallas. We describe in this paper the Fearless Steps Corpus,\nChallenge Tasks, their associated baseline systems, and results. In\nconclusion, we also provide insights gained by the CRSS-UTDallas team\nduring the inaugural Fearless Steps Challenge.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2301",
      "author_area_id": 10,
      "author_area_label": "Speech Recognition: Technologies and Systems for New Applications"
    },
    "chen19e_interspeech": {
      "authors": [
        [
          "Kuan-Yu",
          "Chen"
        ],
        [
          "Che-Ping",
          "Tsai"
        ],
        [
          "Da-Rong",
          "Liu"
        ],
        [
          "Hung-Yi",
          "Lee"
        ],
        [
          "Lin-shan",
          "Lee"
        ]
      ],
      "title": "Completely Unsupervised Phoneme Recognition by a Generative Adversarial Network Harmonized with Iteratively Refined Hidden Markov Models",
      "original": "2068",
      "page_count": 5,
      "order": 384,
      "p1": "1856",
      "pn": "1860",
      "abstract": [
        "Producing a large annotated speech corpus for training ASR systems\nremains difficult for more than 95% of languages all over the world\nwhich are low-resourced, but collecting a relatively big unlabeled\ndata set for such languages is more achievable. This is why some initial\neffort have been reported on completely unsupervised speech recognition\nlearned from unlabeled data only, although with relatively high error\nrates. In this paper, we develop a Generative Adversarial Network (GAN)\nto achieve this purpose, in which a Generator and a Discriminator learn\nfrom each other iteratively to improve the performance. We further\nuse a set of Hidden Markov Models (HMMs) iteratively refined from the\nmachine generated labels to work in harmony with the GAN. The initial\nexperiments on TIMIT data set achieve an phone error rate of 33.1%,\nwhich is 8.5% lower than the previous state-of-the-art.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2068",
      "author_area_id": 10,
      "author_area_label": "Speech Recognition: Technologies and Systems for New Applications"
    },
    "trisitichoke19_interspeech": {
      "authors": [
        [
          "Tasavat",
          "Trisitichoke"
        ],
        [
          "Shintaro",
          "Ando"
        ],
        [
          "Daisuke",
          "Saito"
        ],
        [
          "Nobuaki",
          "Minematsu"
        ]
      ],
      "title": "Analysis of Native Listeners&#8217; Facial Microexpressions While Shadowing Non-Native Speech &#8212; Potential of Shadowers&#8217; Facial Expressions for Comprehensibility Prediction",
      "original": "1953",
      "page_count": 5,
      "order": 385,
      "p1": "1861",
      "pn": "1865",
      "abstract": [
        "Recently, researchers&#8217; attention has been paid to pronunciation\nassessment not based on comparison between L2 speech and native models,\nbut based on comprehensibility of L2 speech [1, 2, 3]. In our previous\nstudies [4, 5, 6], native listeners&#8217; shadowing of L2 speech was\nexamined and it was shown that delay of shadowing and accuracy of articulation\nin shadowing utterances, both of which were acoustically calculated,\nare strongly influenced by the amount of cognitive load imposed for\nunderstanding L2 speech, especially when it is with strong accents.\nIn this paper, aside from acoustic analysis of shadowings, we focus\non shadowers&#8217; facial microexpressions and examine how they are\ncorrelated with perceived comprehensibility. To extract facial expression\nfeatures, two methods are tested. One is a computer-vision-based method\nand recorded videos of shadowers&#8217; facial expressions are analyzed.\nThe other is a method using a physiological sensor that can detect\nsubtle movements of facial muscles. In experiments, four shadowers&#8217;\nfacial expressions are analyzed, each of whom shadowed approximately\n800 L2 utterances. Results show that some of shadowers&#8217; facial\nexpressions are highly correlated with perceived comprehensibility,\nand that those facial expressions are strongly shadower-dependent.\nThese results indicate a high potential of shadowers&#8217; facial\nexpressions for comprehensibility prediction.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1953"
    },
    "karhila19_interspeech": {
      "authors": [
        [
          "Reima",
          "Karhila"
        ],
        [
          "Anna-Riikka",
          "Smolander"
        ],
        [
          "Sari",
          "Ylinen"
        ],
        [
          "Mikko",
          "Kurimo"
        ]
      ],
      "title": "Transparent Pronunciation Scoring Using Articulatorily Weighted Phoneme Edit Distance",
      "original": "1785",
      "page_count": 5,
      "order": 386,
      "p1": "1866",
      "pn": "1870",
      "abstract": [
        "For researching effects of gamification in foreign language learning\nfor children in the &#8220;Say It Again, Kid!&#8221; project we developed\na feedback paradigm that can drive gameplay in pronunciation learning\ngames. We describe our scoring system based on the difference between\na reference phone sequence and the output of a multilingual CTC phoneme\nrecogniser. We present a white-box scoring model of mapped weighted\nLevenshtein edit distance between reference and error with error weights\nfor articulatory differences computed from a training set of scored\nutterances. The system can produce a human-readable list of each detected\nmispronunciation&#8217;s contribution to the utterance score. We compare\nour scoring method to established black box methods.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1785",
      "author_area_id": 10,
      "author_area_label": "Speech Recognition: Technologies and Systems for New Applications"
    },
    "yoon19_interspeech": {
      "authors": [
        [
          "Su-Youn",
          "Yoon"
        ],
        [
          "Chong Min",
          "Lee"
        ],
        [
          "Klaus",
          "Zechner"
        ],
        [
          "Keelan",
          "Evanini"
        ]
      ],
      "title": "Development of Robust Automated Scoring Models Using Adversarial Input for Oral Proficiency Assessment",
      "original": "1711",
      "page_count": 5,
      "order": 387,
      "p1": "1871",
      "pn": "1875",
      "abstract": [
        "In this study, we developed an automated scoring model for an oral\nproficiency test eliciting spontaneous speech from non-native speakers\nof English. In a large-scale oral proficiency test, a small number\nof responses may have atypical characteristics that make it difficult\neven for state-of-the-art automated scoring models to assign fair scores.\nThe oral proficiency test in this study consisted of questions asking\nabout content in materials provided to the test takers, and the atypical\nresponses frequently had serious content abnormalities. In order to\ndevelop an automated scoring system that is robust to these atypical\nresponses, we first developed a set of content features to capture\ncontent abnormalities. Next, we trained scoring models using the augmented\ntraining dataset, including synthetic atypical responses. Compared\nto the baseline scoring model, the new model showed comparable performance\nin scoring normal responses, while it assigned fairer scores for authentic\natypical responses extracted from operational test administrations.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1711",
      "author_area_id": 10,
      "author_area_label": "Speech Recognition: Technologies and Systems for New Applications"
    },
    "lu19b_interspeech": {
      "authors": [
        [
          "Y.",
          "Lu"
        ],
        [
          "Mark J.F.",
          "Gales"
        ],
        [
          "Kate M.",
          "Knill"
        ],
        [
          "P.",
          "Manakul"
        ],
        [
          "L.",
          "Wang"
        ],
        [
          "Y.",
          "Wang"
        ]
      ],
      "title": "Impact of ASR Performance on Spoken Grammatical Error Detection",
      "original": "1706",
      "page_count": 5,
      "order": 388,
      "p1": "1876",
      "pn": "1880",
      "abstract": [
        "Computer assisted language learning (CALL) systems aid learners to\nmonitor their progress by providing scoring and feedback on language\nassessment tasks. Free speaking tests allow assessment of what a learner\nhas said, as well as how they said it. For these tasks, Automatic Speech\nRecognition (ASR) is required to generate transcriptions of a candidate&#8217;s\nresponses, the quality of these transcriptions is crucial to provide\nreliable feedback in downstream processes. This paper considers the\nimpact of ASR performance on Grammatical Error Detection (GED) for\nfree speaking tasks, as an example of providing feedback on a learner&#8217;s\nuse of English. The performance of an advanced deep-learning based\nGED system, initially trained on written corpora, is used to evaluate\nthe influence of ASR errors. One consequence of these errors is that\ngrammatical errors can result from incorrect transcriptions as well\nas learner errors, this may yield confusing feedback. To mitigate the\neffect of these errors, and reduce erroneous feedback, ASR confidence\nscores are incorporated into the GED system. By additionally adapting\nthe written text GED system to the speech domain, using ASR transcriptions,\nsignificant gains in performance can be achieved. Analysis of the GED\nperformance for different grammatical error types and across grade\nis also presented.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1706",
      "author_area_id": 10,
      "author_area_label": "Speech Recognition: Technologies and Systems for New Applications"
    },
    "yang19d_interspeech": {
      "authors": [
        [
          "Seung Hee",
          "Yang"
        ],
        [
          "Minhwa",
          "Chung"
        ]
      ],
      "title": "Self-Imitating Feedback Generation Using GAN for Computer-Assisted Pronunciation Training",
      "original": "1478",
      "page_count": 5,
      "order": 389,
      "p1": "1881",
      "pn": "1885",
      "abstract": [
        "Self-imitating feedback is an effective and learner-friendly method\nfor non-native learners in Computer-Assisted Pronunciation Training.\nAcoustic characteristics in native utterances are extracted and transplanted\nonto learner&#8217;s own speech input, and given back to the learner\nas a corrective feedback. Previous works focused on speech conversion\nusing prosodic transplantation techniques based on PSOLA algorithm.\nMotivated by the visual differences found in spectrograms of native\nand non-native speeches, we investigated applying GAN to generate self-imitating\nfeedback by utilizing generator&#8217; s ability through adversarial\ntraining. Because this mapping is highly under-constrained, we also\nadopt cycle consistency loss to encourage the output to preserve the\nglobal structure, which is shared by native and non-native utterances.\nTrained on 97,200 spectrogram images of short utterances produced by\nnative and non-native speakers of Korean, the generator is able to\nsuccessfully transform the non-native spectrogram input to a spectrogram\nwith properties of self-imitating feedback. Furthermore, the transformed\nspectrogram shows segmental corrections that cannot be obtained by\nprosodic transplantation. Perceptual test comparing the self-imitating\nand correcting abilities of our method with the baseline PSOLA method\nshows that the generative approach with cycle consistency loss is promising.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1478",
      "author_area_id": 10,
      "author_area_label": "Speech Recognition: Technologies and Systems for New Applications"
    },
    "hori19_interspeech": {
      "authors": [
        [
          "Chiori",
          "Hori"
        ],
        [
          "Anoop",
          "Cherian"
        ],
        [
          "Tim K.",
          "Marks"
        ],
        [
          "Takaaki",
          "Hori"
        ]
      ],
      "title": "Joint Student-Teacher Learning for Audio-Visual Scene-Aware Dialog",
      "original": "3143",
      "page_count": 5,
      "order": 390,
      "p1": "1886",
      "pn": "1890",
      "abstract": [
        "Multimodal fusion of audio, vision, and text has demonstrated significant\nbenefits in advancing the performance of several tasks, including machine\ntranslation, video captioning, and video summarization. Audio-Visual\nScene-aware Dialog (AVSD) is a new and more challenging task, proposed\nrecently, that focuses on generating sentence responses to questions\nthat are asked in a dialog about video content. While prior approaches\ndesigned to tackle this task have shown the need for multimodal fusion\nto improve response quality, the best-performing systems often rely\nheavily on human-generated summaries of the video content, which are\nunavailable when such systems are deployed in real-world. This paper\ninvestigates how to compensate for such information, which is missing\nin the inference phase but available during the training phase. To\nthis end, we propose a novel AVSD system using student-teacher learning,\nin which a student network is (jointly) trained to mimic the teacher&#8217;s\nresponses. Our experiments demonstrate that in addition to yielding\nstate-of-the-art accuracy against the baseline DSTC7-AVSD system, the\nproposed approach (which does not use human-generated summaries at\ntest time) performs competitively with methods that do use those summaries.\n"
      ],
      "doi": "10.21437/Interspeech.2019-3143",
      "author_area_id": 11,
      "author_area_label": "Spoken dialog systems and conversational analysis"
    },
    "gopalakrishnan19_interspeech": {
      "authors": [
        [
          "Karthik",
          "Gopalakrishnan"
        ],
        [
          "Behnam",
          "Hedayatnia"
        ],
        [
          "Qinlang",
          "Chen"
        ],
        [
          "Anna",
          "Gottardi"
        ],
        [
          "Sanjeev",
          "Kwatra"
        ],
        [
          "Anu",
          "Venkatesh"
        ],
        [
          "Raefer",
          "Gabriel"
        ],
        [
          "Dilek",
          "Hakkani-T\u00fcr"
        ]
      ],
      "title": "Topical-Chat: Towards Knowledge-Grounded Open-Domain Conversations",
      "original": "3079",
      "page_count": 5,
      "order": 391,
      "p1": "1891",
      "pn": "1895",
      "abstract": [
        "Building socialbots that can have deep, engaging open-domain conversations\nwith humans is one of the grand challenges of artificial intelligence\n(AI). To this end, bots need to be able to leverage world knowledge\nspanning several domains effectively when conversing with humans who\nhave their own world knowledge. Existing knowledge-grounded conversation\ndatasets are primarily stylized with explicit roles for conversation\npartners. These datasets also do not explore depth or breadth of topical\ncoverage with transitions in conversations. We introduce Topical-Chat,\na knowledge-grounded human-human conversation dataset where the underlying\nknowledge spans 8 broad topics and conversation partners don&#8217;t\nhave explicitly defined roles, to help further research in open-domain\nconversational AI. We also train several state-of-the-art encoder-decoder\nconversational models on Topical-Chat and perform automated and human\nevaluation for benchmarking.\n"
      ],
      "doi": "10.21437/Interspeech.2019-3079",
      "author_area_id": 11,
      "author_area_label": "Spoken dialog systems and conversational analysis"
    },
    "kubasova19_interspeech": {
      "authors": [
        [
          "Uliyana",
          "Kubasova"
        ],
        [
          "Gabriel",
          "Murray"
        ],
        [
          "McKenzie",
          "Braley"
        ]
      ],
      "title": "Analyzing Verbal and Nonverbal Features for Predicting Group Performance",
      "original": "3062",
      "page_count": 5,
      "order": 392,
      "p1": "1896",
      "pn": "1900",
      "abstract": [
        "This work analyzes the efficacy of verbal and nonverbal features of\ngroup conversation for the task of automatic prediction of group task\nperformance. We describe a new publicly available survival task dataset\nthat was collected and annotated to facilitate this prediction task.\nIn these experiments, the new dataset is merged with an existing survival\ntask dataset, allowing us to compare feature sets on a much larger\namount of data than has been used in recent related work. This work\nis also distinct from related research on social signal processing\n(SSP) in that we compare verbal and nonverbal features, whereas SSP\nis almost exclusively concerned with nonverbal aspects of social interaction.\nA key finding is that nonverbal features from the speech signal are\nextremely effective for this task, even on their own. However, the\nmost effective individual features are verbal features, and we highlight\nthe most important ones.\n"
      ],
      "doi": "10.21437/Interspeech.2019-3062",
      "author_area_id": 11,
      "author_area_label": "Spoken dialog systems and conversational analysis"
    },
    "martinez19_interspeech": {
      "authors": [
        [
          "Victor R.",
          "Martinez"
        ],
        [
          "Nikolaos",
          "Flemotomos"
        ],
        [
          "Victor",
          "Ardulov"
        ],
        [
          "Krishna",
          "Somandepalli"
        ],
        [
          "Simon B.",
          "Goldberg"
        ],
        [
          "Zac E.",
          "Imel"
        ],
        [
          "David C.",
          "Atkins"
        ],
        [
          "Shrikanth",
          "Narayanan"
        ]
      ],
      "title": "Identifying Therapist and Client Personae for Therapeutic Alliance Estimation",
      "original": "2829",
      "page_count": 5,
      "order": 393,
      "p1": "1901",
      "pn": "1905",
      "abstract": [
        "Psychotherapy, from a narrative perspective, is the process in which\na client relates an on-going life-story to a therapist. In each session,\na client will recount events from their life, some of which stand out\nas more significant than others. These significant stories can ultimately\nshape one&#8217;s identity. In this work we study these narratives\nin the context of therapeutic alliance &#8212; a self-reported measure\non the perception of a shared bond between client and therapist. We\npropose that alliance can be predicted from the interactions between\ncertain types of clients with types of therapists. To validate this\nmethod, we obtained 1235 transcribed sessions with client-reported\nalliance to train an unsupervised approach to discover groups of therapists\nand clients based on common types of narrative characters, or  personae.\nWe measure the strength of the relation between personae and alliance\nin two experiments. Our results show that (1) alliance can be explained\nby the interactions between the discovered character types, and (2)\nmodels trained on therapist and client personae achieve significant\nperformance gains compared to competitive supervised baselines. Finally,\nexploratory analysis reveals important character traits that lead to\nan improved perception of alliance.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2829",
      "author_area_id": 11,
      "author_area_label": "Spoken dialog systems and conversational analysis"
    },
    "haake19_interspeech": {
      "authors": [
        [
          "Kristin",
          "Haake"
        ],
        [
          "Sarah",
          "Schimke"
        ],
        [
          "Simon",
          "Betz"
        ],
        [
          "Sina",
          "Zarrie\u00df"
        ]
      ],
      "title": "Do Hesitations Facilitate Processing of Partially Defective System Utterances? An Exploratory Eye Tracking Study",
      "original": "2820",
      "page_count": 5,
      "order": 394,
      "p1": "1906",
      "pn": "1910",
      "abstract": [
        "Spoken dialogue systems are predominantly evaluated using offline methods\nsuch as user ratings or task-oriented measures. Various phenomena in\nconversational speech, however, are known to affect the way the listener&#8217;s\ncomprehension  unfolds over time, and not necessarily the final result\nof the comprehension process. For instance, in human reference comprehension,\nconversational signals like hesitations have been shown to ease processing\nof expressions referring to difficult-to-describe targets, as can primarily\nbe observed in listeners&#8217; anticipatory eye movements rather than\nin their final reference resolution decision. In this study, we explore\neye tracking for testing conversational dialogue systems, looking at\nhow listeners process automatically generated referring expressions\ncontaining defective attributes. We investigate whether hesitations\nfacilitate the processing of partially defective system utterances\nand track the user&#8217;s eye movements when listening to expressions\nwith: (i) semantically defective but fluently synthesized adjectives,\n(ii) defective and lengthened adjectives, i.e. containing a conversational\nuncertainty signal. Our results are encouraging: whereas the offline\nmeasure of task success does not show any differences between the two\nconditions, the listeners&#8217; eye movements suggest that processing\nof partially defective utterances might be facilitated by conversational\nhesitations.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2820",
      "author_area_id": 11,
      "author_area_label": "Spoken dialog systems and conversational analysis"
    },
    "li19h_interspeech": {
      "authors": [
        [
          "Bin",
          "Li"
        ],
        [
          "Yuan",
          "Jia"
        ]
      ],
      "title": "Influence of Contextuality on Prosodic Realization of Information Structure in Chinese Dialogues",
      "original": "2291",
      "page_count": 5,
      "order": 395,
      "p1": "1911",
      "pn": "1915",
      "abstract": [
        "In this paper, we present a detailed investigation on the influence\nof contextuality on the prosodic realization of information structure\nin Chinese dialogues. The materials were selected from the 863 corpus,\nwhich contains both isolated sentences and spontaneous dialogues. RefLex\nwas selected as the annotation scheme, which differentiates information\nstructure on the lexical and referential levels. Prosodic data (including\nduration and pitch range) from 12 groups of spontaneous dialogues were\nanalyzed with the linear mixed effects mode, and each of them consists\nof 13&#8211;22 turns. The isolated sentences corresponding to these\ndialogues were also analyzed. The analysis results reveal the influence\nof contextuality. Specifically, the features of prosodic realization\nof information structure on the lexical and referential levels show\na contrary tendency. The statistical analysis indicates that the speakers\nuse duration and pitch ranges as phonetic cues to distinguish information\nstructures on both levels. On the other hand, duration on the referential\nlevel is the only phonetic cue affected by contextuality.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2291",
      "author_area_id": 11,
      "author_area_label": "Spoken dialog systems and conversational analysis"
    },
    "gjoreski19_interspeech": {
      "authors": [
        [
          "Kristijan",
          "Gjoreski"
        ],
        [
          "Aleksandar",
          "Gjoreski"
        ],
        [
          "Ivan",
          "Kraljevski"
        ],
        [
          "Diane",
          "Hirschfeld"
        ]
      ],
      "title": "Cross-Lingual Transfer Learning for Affective Spoken Dialogue Systems",
      "original": "2163",
      "page_count": 5,
      "order": 396,
      "p1": "1916",
      "pn": "1920",
      "abstract": [
        "This paper presents a case study of cross-lingual transfer learning\napplied for affective computing in the domain of spoken dialogue systems.\nProsodic features of correction dialog acts are modeled on a group\nof languages and compared with languages excluded from the analysis.<br\nstyle=\"mso-data-placement:same-cell;\" /><br style=\"mso-data-placement:same-cell;\"\n/>\n  Speech from different languages was recorded in carefully staged\nWizard-of-Oz experiments, however, without the possibility to ensure\nbalanced distribution of speakers per language. In order to assess\nthe possibility of cross-lingual transfer learning and to ensure reliable\nclassification of corrections independently of language, we employed\ndifferent machine learning approaches along with relevant acoustic-prosodic\nfeatures sets.<br style=\"mso-data-placement:same-cell;\" /><br style=\"mso-data-placement:same-cell;\"\n/>\n  The results of the experiments with mono-lingual corpora (trained\nand tested on a single language) and cross-lingual (trained on several\nlanguages and tested on the rest) were analyzed and compared in the\nterms of accuracy and F1 score.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2163",
      "author_area_id": 11,
      "author_area_label": "Spoken dialog systems and conversational analysis"
    },
    "yu19_interspeech": {
      "authors": [
        [
          "Mingzhi",
          "Yu"
        ],
        [
          "Emer",
          "Gilmartin"
        ],
        [
          "Diane",
          "Litman"
        ]
      ],
      "title": "Identifying Personality Traits Using Overlap Dynamics in Multiparty Dialogue",
      "original": "1886",
      "page_count": 5,
      "order": 397,
      "p1": "1921",
      "pn": "1925",
      "abstract": [
        "Research on human spoken language has shown that speech plays an important\nrole in identifying speaker personality traits. In this work, we propose\nan approach for identifying speaker personality traits using overlap\ndynamics in multiparty spoken dialogues. We first define a set of novel\nfeatures representing the overlap dynamics of each speaker. We then\ninvestigate the impact of speaker personality traits on these features\nusing ANOVA tests. We find that features of overlap dynamics significantly\nvary for speakers with different levels of both Extraversion and Conscientiousness.\nFinally, we find that classifiers using only overlap dynamics features\noutperform random guessing in identifying Extraversion and Agreeableness,\nand that the improvements are statistically significant.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1886",
      "author_area_id": 11,
      "author_area_label": "Spoken dialog systems and conversational analysis"
    },
    "aldeneh19_interspeech": {
      "authors": [
        [
          "Zakaria",
          "Aldeneh"
        ],
        [
          "Mimansa",
          "Jaiswal"
        ],
        [
          "Michael",
          "Picheny"
        ],
        [
          "Melvin G.",
          "McInnis"
        ],
        [
          "Emily Mower",
          "Provost"
        ]
      ],
      "title": "Identifying Mood Episodes Using Dialogue Features from Clinical Interviews",
      "original": "1878",
      "page_count": 5,
      "order": 398,
      "p1": "1926",
      "pn": "1930",
      "abstract": [
        "Bipolar disorder, a severe chronic mental illness characterized by\npathological mood swings from depression to mania, requires ongoing\nsymptom severity tracking to both guide and measure treatments that\nare critical for maintaining long-term health. Mental health professionals\nassess symptom severity through semi-structured clinical interviews.\nDuring these interviews, they observe their patients&#8217; spoken\nbehaviors, including both what the patients say and how they say it.\nIn this work, we move beyond acoustic and lexical information, investigating\nhow higher-level interactive patterns also change during mood episodes.\nWe then perform a secondary analysis, asking if these interactive patterns,\nmeasured through dialogue features, can be used in conjunction with\nacoustic features to automatically recognize mood episodes. Our results\nshow that it is beneficial to consider dialogue features when analyzing\nand building automated systems for predicting and monitoring mood.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1878",
      "author_area_id": 11,
      "author_area_label": "Spoken dialog systems and conversational analysis"
    },
    "lubold19_interspeech": {
      "authors": [
        [
          "Nichola",
          "Lubold"
        ],
        [
          "Stephanie A.",
          "Borrie"
        ],
        [
          "Tyson S.",
          "Barrett"
        ],
        [
          "Megan",
          "Willi"
        ],
        [
          "Visar",
          "Berisha"
        ]
      ],
      "title": "Do Conversational Partners Entrain on Articulatory Precision?",
      "original": "1786",
      "page_count": 5,
      "order": 399,
      "p1": "1931",
      "pn": "1935",
      "abstract": [
        "The communication phenomenon known as conversational entrainment occurs\nwhen dialogue partners align or adapt their behavior to one another\nwhile conversing. Associated with rapport, trust, and communicative\nefficiency, entrainment appears to facilitate conversational success.\nIn this work, we explore how conversational partners entrain or align\non articulatory precision or the clarity with which speakers articulate\ntheir spoken productions. Articulatory precision also has implications\nfor conversational success as precise articulation can enhance speech\nunderstanding and intelligibility. However, in conversational speech,\nspeakers tend to reduce their articulatory precision, preferring low-cost,\nimprecise speech. Speakers may adapt their articulation and become\nmore precise depending on feedback from their listeners. Given the\npotential of entrainment, we are interested in how conversational partners\nadapt or entrain their articulatory precision to one another. We explore\nthis phenomenon in 57 task-based dialogues. Controlling for the influence\nof speaking rate, we find that speakers entrain on articulatory precision,\nwith significant alignment on articulation of consonants. We discuss\nthe potential applications that speaker alignment on precision might\nhave for modeling conversation and implementing strategies for enhancing\ncommunicative success in human-human and human-computer interactions.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1786",
      "author_area_id": 11,
      "author_area_label": "Spoken dialog systems and conversational analysis"
    },
    "lian19_interspeech": {
      "authors": [
        [
          "Zheng",
          "Lian"
        ],
        [
          "Jianhua",
          "Tao"
        ],
        [
          "Bin",
          "Liu"
        ],
        [
          "Jian",
          "Huang"
        ]
      ],
      "title": "Conversational Emotion Analysis via Attention Mechanisms",
      "original": "1577",
      "page_count": 5,
      "order": 400,
      "p1": "1936",
      "pn": "1940",
      "abstract": [
        "Different from the emotion recognition in individual utterances, we\npropose a multimodal learning framework using relation and dependencies\namong the utterances for conversational emotion analysis. The attention\nmechanism is applied to the fusion of the acoustic and lexical features.\nThen these fusion representations are fed into the self-attention based\nbi-directional gated recurrent unit (GRU) layer to capture long-term\ncontextual information. To imitate real interaction patterns of different\nspeakers, speaker embeddings are also utilized as additional inputs\nto distinguish the speaker identities during conversational dialogs.\nTo verify the effectiveness of the proposed method, we conduct experiments\non the IEMOCAP database. Experimental results demonstrate that our\nmethod shows absolute 2.42% performance improvement over the state-of-the-art\nstrategies.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1577",
      "author_area_id": 11,
      "author_area_label": "Spoken dialog systems and conversational analysis"
    },
    "oneill19_interspeech": {
      "authors": [
        [
          "Emma",
          "O\u2019Neill"
        ],
        [
          "Julie",
          "Carson-Berndsen"
        ]
      ],
      "title": "The Effect of Phoneme Distribution on Perceptual Similarity in English",
      "original": "3042",
      "page_count": 5,
      "order": 401,
      "p1": "1941",
      "pn": "1945",
      "abstract": [
        "This paper investigates the extent to which native speaker perceptions\nregarding the similarity between phonemes of English are influenced\nby their distributional properties. A similarity hierarchy model based\non the distribution of consonantal phonemes in the English language\nwas generated by creating phoneme-embeddings from contextual information.\nWe compare this to similarity models based on phonological feature\ntheory and on native speaker perception. Characteristics of the perception-based\nmodel are shown to appear in the distribution-based model whilst not\nbeing captured by the feature-based model. This not only provides evidence\nof similarity perceptions being influenced by distributional properties\nbut is an argument for incorporating distributional information alongside\nphonological features when modelling perceptual similarity.\n"
      ],
      "doi": "10.21437/Interspeech.2019-3042",
      "author_area_id": 2,
      "author_area_label": "Phonetics, Phonology, and Prosody"
    },
    "kakouros19_interspeech": {
      "authors": [
        [
          "Sofoklis",
          "Kakouros"
        ],
        [
          "Antti",
          "Suni"
        ],
        [
          "Juraj",
          "\u0160imko"
        ],
        [
          "Martti",
          "Vainio"
        ]
      ],
      "title": "Prosodic Representations of Prominence Classification Neural Networks and Autoencoders Using Bottleneck Features",
      "original": "2984",
      "page_count": 5,
      "order": 402,
      "p1": "1946",
      "pn": "1950",
      "abstract": [
        "Prominence perception has been known to correlate with a complex interplay\nof the acoustic features of energy, fundamental frequency, spectral\ntilt, and duration. The contribution and importance of each of these\nfeatures in distinguishing between prominent and non-prominent units\nin speech is not always easy to determine, and more so, the prosodic\nrepresentations that humans and automatic classifiers learn have been\ndifficult to interpret. This work focuses on examining the acoustic\nprosodic representations that binary prominence classification neural\nnetworks and autoencoders learn for prominence. We investigate the\ncomplex features learned at different layers of the network as well\nas the 10-dimensional bottleneck features (BNFs), for the standard\nacoustic prosodic correlates of prominence separately and in combination.\nWe analyze and visualize the BNFs obtained from the prominence classification\nneural networks as well as their network activations. The experiments\nare conducted on a corpus of Dutch continuous speech with manually\nannotated prominence labels. Our results show that the prosodic representations\nobtained from the BNFs and higher-dimensional non-BNFs provide good\nseparation of the two prominence categories, with, however, different\npartitioning of the BNF space for the distinct features, and the best\noverall separation obtained for F0.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2984",
      "author_area_id": 2,
      "author_area_label": "Phonetics, Phonology, and Prosody"
    },
    "peperkamp19_interspeech": {
      "authors": [
        [
          "Sharon",
          "Peperkamp"
        ],
        [
          "Alvaro Martin Iturralde",
          "Zurita"
        ]
      ],
      "title": "Compensation for French Liquid Deletion During Auditory Sentence Processing",
      "original": "2950",
      "page_count": 5,
      "order": 403,
      "p1": "1951",
      "pn": "1955",
      "abstract": [
        "Phonological rules change the surface realization of words. Listeners\nundo these changes in order to retrieve the canonical word form. We\ninvestigate this so-called compensation for a French deletion rule,\ni.e. liquid deletion. This rule optionally deletes the final consonant\nof a word-final obstruent-liquid cluster. It can apply both before\nconsonants and before vowels, but its application is about twice as\nfrequent before consonants. Using a word detection task, we find an\noverall relatively low rate of compensation, which we argue is due\nto the relatively high perceptual salience of the rule. We also observe\na clear effect of context, though: listeners compensate more than twice\nas often for a deleted liquid before a consonant than before a vowel.\nThis is evidence that compensation involves fine-grained knowledge\nabout the probability of the rule&#8217;s application in different\ncontexts.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2950",
      "author_area_id": 2,
      "author_area_label": "Phonetics, Phonology, and Prosody"
    },
    "kocharov19_interspeech": {
      "authors": [
        [
          "Daniil",
          "Kocharov"
        ],
        [
          "Tatiana",
          "Kachkovskaia"
        ],
        [
          "Pavel",
          "Skrelin"
        ]
      ],
      "title": "Prosodic Factors Influencing Vowel Reduction in Russian",
      "original": "2918",
      "page_count": 5,
      "order": 404,
      "p1": "1956",
      "pn": "1960",
      "abstract": [
        "Unstressed vowels in Russian are reduced in both duration and quality,\nbut these two manifestations of vowel reduction do not have to be observed\nsimultaneously. In order to investigate this question, we analysed\nthe reduction pattern of words in such contexts where lengthening is\ninduced by prosodic factors: prominence and pre-boundary lengthening.\nThe study is based on a large corpus of read speech. The following\nresults were obtained: (1) as expected, both contexts increase vowel\nduration; (2) under prosodic prominence vowels undergo less qualitative\nreduction, while pre-boundary lengthening has no effect on qualitative\nreduction; (3) additionally, it was shown that prominence mainly affects\nthe pretonic part of the word, while pre-boundary lengthening &#8212;\nthe post-tonic part. Thus, an increase in vowel duration does not always\ncause a decrease in qualitative reduction, which may serve as evidence\nagainst the idea that qualitative reduction is caused by quantitative\nreduction. Additionally, these results may serve as an argument for\nthe idea that the two processes &#8212; vowel reduction and temporal\norganization of utterance &#8212; are autonomous.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2918",
      "author_area_id": 2,
      "author_area_label": "Phonetics, Phonology, and Prosody"
    },
    "gobl19_interspeech": {
      "authors": [
        [
          "Christer",
          "Gobl"
        ],
        [
          "Ailbhe N\u00ed",
          "Chasaide"
        ]
      ],
      "title": "Time to Frequency Domain Mapping of the Voice Source: The Influence of Open Quotient and Glottal Skew on the Low End of the Source Spectrum",
      "original": "2888",
      "page_count": 5,
      "order": 405,
      "p1": "1961",
      "pn": "1965",
      "abstract": [
        "This paper explores the mapping of time and frequency domain aspects\nof the voice source, focussing on the low end of the source spectrum.\nIt refines and extends an earlier study, where the LF model was used\nto explore the correspondences between the open quotient (O<SUB>q</SUB>),\nglottal skew (R<SUB>k</SUB>) and harmonic levels of the source spectrum,\nincluding the H1-H2 measure, widely assumed to reflect differences\nin O<SUB>q</SUB>. Here we use a different model (the F-model) as it\nbetter reflects the effective open quotient and glottal skew in certain\nconditions. As in the earlier study, a series of glottal pulses were\ngenerated, keeping peak glottal flow constant, while systematically\nvarying O<SUB>q</SUB> and R<SUB>k</SUB>. Results suggest that the effects\nof R<SUB>k</SUB> on the low harmonics is considerably less than estimated\nin the earlier study, and its main impact is on the level of H2 (and\nconsequently H1-H2) when O<SUB>q</SUB> is relatively high. The conclusion\nremains that the H1-H2 is not simply a direct reflection of O<SUB>q</SUB>.\nHowever, for O<SUB>q</SUB> values of up to about 0.6, it maps closely\nto H1-H2: beyond this point, H1-H2 reflects a more complex interaction\nof open quotient and glottal skew.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2888",
      "author_area_id": 2,
      "author_area_label": "Phonetics, Phonology, and Prosody"
    },
    "chodroff19_interspeech": {
      "authors": [
        [
          "Eleanor",
          "Chodroff"
        ],
        [
          "Jennifer S.",
          "Cole"
        ]
      ],
      "title": "Testing the Distinctiveness of Intonational Tunes: Evidence from Imitative Productions in American English",
      "original": "2684",
      "page_count": 5,
      "order": 406,
      "p1": "1966",
      "pn": "1970",
      "abstract": [
        "Understanding the structure of intonational variation is a longstanding\nissue in prosodic research. A given utterance can be realized with\ncountless intonational contours, and while variation in prosodic meaning\nis also large, listeners nevertheless converge on relatively consistent\nform-function mappings. While this suggests the existence of abstract\nintonational representations, it has been unclear how exactly these\nare defined. The present study examines the validity of a well-defined\nset of phonological representations for the generation of intonation\nin the nuclear region of an intonational phrase in American English:\nnamely, the combination of binary pitch accents (H*/L*), phrase accents\n(H-/L-), and boundary tones (H%/L%) proposed in Pierrehumbert (1980).\nIn an exploratory study, we examined whether speakers maintained the\neight-way distinction among intonational contours posited to exist\nin this representational system. We created eight synthesized contours\naccording to Pierrehumbert (1980) and examined whether listeners generalized\nthese contours to novel productions. Speakers largely distinguished\nrising from non-rising contours in production, but few other distinctions\nwere maintained. While this does not rule out the existence of additional\ncontours in production, these findings do suggest that the representation\nof rising and non-rising contours may be privileged and more readily\naccessible in the intonational grammar.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2684",
      "author_area_id": 2,
      "author_area_label": "Phonetics, Phonology, and Prosody"
    },
    "park19d_interspeech": {
      "authors": [
        [
          "Sangwook",
          "Park"
        ],
        [
          "David K.",
          "Han"
        ],
        [
          "Mounya",
          "Elhilali"
        ]
      ],
      "title": "A Study of a Cross-Language Perception Based on Cortical Analysis Using Biomimetic STRFs",
      "original": "2507",
      "page_count": 5,
      "order": 407,
      "p1": "1971",
      "pn": "1975",
      "abstract": [
        "For those in the early stage of learning a foreign language, they commonly\nexperience difficulties in understanding spoken words in the second\nlanguage, while they have no problem in recognizing words spoken in\ntheir mother tongue. This paper examines this phenomenon using biomimetic\nreceptive fields that can be interpreted as a transfer function between\nacoustic stimulus and cortical responses in the brain. While receptive\nfields of individual subjects are often optimized to recognize unique\nphonemes in their mother language, it is unclear whether challenges\nassociated with acquiring a new language (especially in adulthood)\nis due to a mismatch between phonemic characteristics in the new language\nand optimized processing in the system. We explore this question by\ncontrasting biomimetic systems optimized for four different languages\nwith sufficiently different characteristics. We perform English phoneme\nclassification with these language-optimized systems. We observed distinctive\ncharacteristics in receptive fields emerging from each language, and\nthe differences of English phoneme recognition performance accordingly.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2507",
      "author_area_id": 2,
      "author_area_label": "Phonetics, Phonology, and Prosody"
    },
    "sturm19_interspeech": {
      "authors": [
        [
          "Pavel",
          "\u0160turm"
        ],
        [
          "Jan",
          "Vol\u00edn"
        ]
      ],
      "title": "Perceptual Evaluation of Early versus Late F0 Peaks in the Intonation Structure of Czech Question-Word Questions",
      "original": "2082",
      "page_count": 5,
      "order": 408,
      "p1": "1976",
      "pn": "1980",
      "abstract": [
        "Question-word questions in Czech lexically mark their interrogative\nfunction in the initial position: in their standard form, they begin\nwith an interrogative lexeme. For many linguists, this is a sufficient\nreason for resigning on intonation marking, so they claim that the\nspeech melody in these questions is identical to the melody of statements.\nA careful observation of the current Czech speech suggests otherwise.<br\nstyle=\"mso-data-placement:same-cell;\" /><br style=\"mso-data-placement:same-cell;\"\n/>\n  This paper presents a perceptual experiment in which Czech speakers\nevaluated two contrastive forms of the interrogative melody, specifically\nthe one with a late peak modelled after statements (as suggested by\nsome authors), and the one with an early peak modelled after our empirical\ndata collected previously. Thirty-two listeners expressed a statistically\nsignificant preference for the early peak in a perception test. This\noutcome resonates with the sample of speech production of the questions.\nHowever, the late peak is also possible and acceptable: we assume that\nit might be a signal of contrastive emphasis or an implicational cue.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2082",
      "author_area_id": 2,
      "author_area_label": "Phonetics, Phonology, and Prosody"
    },
    "kelterer19_interspeech": {
      "authors": [
        [
          "Anneliese",
          "Kelterer"
        ],
        [
          "Barbara",
          "Schuppler"
        ]
      ],
      "title": "Acoustic Correlates of Phonation Type in Chichimec",
      "original": "2066",
      "page_count": 5,
      "order": 409,
      "p1": "1981",
      "pn": "1985",
      "abstract": [
        "Chichimec is an Oto-Manguean language of Mexico with a phonological\ncontrast between modal, breathy and creaky vowels. This study is the\nfirst acoustic investigation of this contrast in Chichimec, based on\nspectral tilt and Cepstral Peak Prominence (CPP) measures. We consider\nthe change of these measures over the course of the vowel and include\na high vowel, which was omitted in most phonation studies of other\nlanguages. The present study not only contributes to the description\nof Chichimec with respect to the different portions of the vowel, but\nalso explores the adequacy of the acoustic measures of phonation type\nfor low and high vowels.<br style=\"mso-data-placement:same-cell;\" /><br\nstyle=\"mso-data-placement:same-cell;\" />\n  Our results show that phonation\nchanges in the course of the vowel, and that this change is a relevant\nfactor for phonation types in Chichimec. We find that CPP is the best\nmeasure to characterize Chichimec phonation contrasts in all vowels.\nFor the vowel /a/, spectral tilt measures are better indicators of\nphonation type for women than for men. The results for /i/ indicate\nthat spectral tilt distinguishes breathy from modal vowels for men,\nbut that these measures might generally not be appropriate to describe\nphonation contrasts in women&#8217;s high vowels.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2066",
      "author_area_id": 2,
      "author_area_label": "Phonetics, Phonology, and Prosody"
    },
    "chien19c_interspeech": {
      "authors": [
        [
          "Yu-Ren",
          "Chien"
        ],
        [
          "Michal",
          "Borsk\u00fd"
        ],
        [
          "J\u00f3n",
          "Gu\u00f0nason"
        ]
      ],
      "title": "F0 Variability Measures Based on Glottal Closure Instants",
      "original": "1326",
      "page_count": 4,
      "order": 410,
      "p1": "1986",
      "pn": "1989",
      "abstract": [
        "The periodicity of a voiced-sound signal can reflect physiological\nconditions such as identity, age, and voice disorder. One way to look\ninto this periodicity is to measure the temporal variability of vocal\nfundamental frequency (F0). This paper proposes 2 measures of F0 variability\nbased on glottal closure instant (GCI). GCI is essential to the detection\nof F0 when the signal waveform varies substantially between adjacent\ncycles, e.g., in breathy voice. Frequency-selective variability measurements\nare taken from an interpolated sequence of fundamental-period values\nbased on GCIs, including certain spectral-shape parameters which constitute\na multi-variate measure. The utility of these measures was demonstrated\nin two experiments designed for inter- and intra-speaker comparisons,\nrespectively.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1326",
      "author_area_id": 2,
      "author_area_label": "Phonetics, Phonology, and Prosody"
    },
    "tavi19_interspeech": {
      "authors": [
        [
          "Lauri",
          "Tavi"
        ],
        [
          "Tanel",
          "Alum\u00e4e"
        ],
        [
          "Stefan",
          "Werner"
        ]
      ],
      "title": "Recognition of Creaky Voice from Emergency Calls",
      "original": "1253",
      "page_count": 5,
      "order": 411,
      "p1": "1990",
      "pn": "1994",
      "abstract": [
        "Although creaky voice, or vocal fry, is widely studied phonation mode,\nopen questions still exist in creak&#8217;s acoustic characterization\nand automatic recognition. Many questions are open since creak varies\nsignificantly depending on conversational context. In this study, we\nintroduce an exploratory creak recognizer based on convolutional neural\nnetwork (CNN), which is generated specifically for emergency calls.\nThe study focuses on recognition of creaky voice from authentic emergency\ncalls because creak detection could potentially provide information\nabout the caller&#8217;s emotional state or attempt of voice disguise.\nWe generated the CNN recognition system using emergency call recordings\nand other out-of-domain speech recordings and compared the results\nwith an already existing and widely used creaky voice detection system:\nusing poor quality emergency call recordings as test data, this system\nachieved F1 of 0.41 whereas our CNN system accomplished an F1 of 0.64.\nThe results show that the CNN system can perform moderately well using\na limited amount of training data on challenging testing data and has\nthe potential to achieve higher F scores when more emergency calls\nare used for model training.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1253",
      "author_area_id": 2,
      "author_area_label": "Phonetics, Phonology, and Prosody"
    },
    "xu19b_interspeech": {
      "authors": [
        [
          "Shuzhuang",
          "Xu"
        ],
        [
          "Hiroshi",
          "Shimodaira"
        ]
      ],
      "title": "Direct F0 Estimation with Neural-Network-Based Regression",
      "original": "3267",
      "page_count": 5,
      "order": 412,
      "p1": "1995",
      "pn": "1999",
      "abstract": [
        "Pitch tracking, or the continuous extraction of fundamental frequency\nfrom speech waveforms, is of vital importance to many applications\nin speech analysis and synthesis. Many existing trackers, including\nconventional ones such as Praat, RAPT and YIN, and newly proposed neural-network-based\nones such as DNN-CLS, CREPE and RNN-REG, have conducted an extensive\ninvestigation into speech pitch tracking. This work developed a different\nend-to-end regression model based on neural networks, where a voice\ndetector and a newly proposed value estimator work jointly to highlight\nthe trajectory of fundamental frequency. Experiments on the PTDB-TUG\ncorpus showed that the system surpasses canonical neural networks in\nterms of gross error rate. It further outperformed conventional trackers\nunder clean condition and neural-network classifiers under noisy condition\nby the NOISEX-92 corpus.\n"
      ],
      "doi": "10.21437/Interspeech.2019-3267",
      "author_area_id": 5,
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "sharma19b_interspeech": {
      "authors": [
        [
          "Tanay",
          "Sharma"
        ],
        [
          "Rohith Chandrashekar",
          "Aralikatti"
        ],
        [
          "Dilip Kumar",
          "Margam"
        ],
        [
          "Abhinav",
          "Thanda"
        ],
        [
          "Sharad",
          "Roy"
        ],
        [
          "Pujitha Appan",
          "Kandala"
        ],
        [
          "Shankar M.",
          "Venkatesan"
        ]
      ],
      "title": "Real Time Online Visual End Point Detection Using Unidirectional LSTM",
      "original": "3253",
      "page_count": 5,
      "order": 413,
      "p1": "2000",
      "pn": "2004",
      "abstract": [
        "Visual Voice Activity Detection (V-VAD) involves the detection of speech\nactivity of a speaker using visual features. The V-VAD is useful in\ndetecting the end point of an utterance under noisy acoustic conditions\nor for maintaining speaker privacy. In this paper, we propose a speaker\nindependent, real-time solution for V-VAD. The focus is on real-time\naspect and accuracy as such algorithms will play a key role in detecting\nend point especially while interacting with speech assistants. We propose\ntwo novel methods one using CNN and the other using 2D-DCT features.\nUnidirectional LSTMs are used in both the methods to make it online\nand learn temporal dependence. The methods are tested on two publicly\navailable datasets. Additionally the methods are also tested on a locally\ncollected dataset which further validates our hypothesis. Additionally\nit has been observed through experiments that both the approaches generalize\nto unseen speakers. It has been shown that our best approach gives\nsubstantial improvement over earlier methods done on the same dataset.\n"
      ],
      "doi": "10.21437/Interspeech.2019-3253",
      "author_area_id": 5,
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "ardaillon19_interspeech": {
      "authors": [
        [
          "Luc",
          "Ardaillon"
        ],
        [
          "Axel",
          "Roebel"
        ]
      ],
      "title": "Fully-Convolutional Network for Pitch Estimation of Speech Signals",
      "original": "2815",
      "page_count": 5,
      "order": 414,
      "p1": "2005",
      "pn": "2009",
      "abstract": [
        "The estimation of fundamental frequency (F<SUB>0</SUB>) from audio\nis a necessary step in many speech processing tasks such as speech\nsynthesis, that require to accurately analyze big datasets, or real-time\nvoice transformations, that require low computation times. New approaches\nusing neural networks have been recently proposed for F<SUB>0</SUB>\nestimation, outperforming previous approaches in terms of accuracy.\nThe work presented here aims at bringing some more improvements over\nsuch CNN-based state-of-the-art approaches, especially when targeting\nspeech data. More specifically, we first propose to use the recent\nPaN speech synthesis engine in order to generate a high-quality speech\ndatabase with a reliable ground truth F<SUB>0</SUB> annotation. Then,\nwe propose 3 variants of a new fully-convolutional network (FCN) architecture\nthat are shown to perform better than other similar data-driven methods,\nwith a significantly reduced computational load making them more suitable\nfor real-time purposes.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2815",
      "author_area_id": 5,
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "dong19_interspeech": {
      "authors": [
        [
          "Mingye",
          "Dong"
        ],
        [
          "Jie",
          "Wu"
        ],
        [
          "Jian",
          "Luan"
        ]
      ],
      "title": "Vocal Pitch Extraction in Polyphonic Music Using Convolutional Residual Network",
      "original": "2286",
      "page_count": 5,
      "order": 415,
      "p1": "2010",
      "pn": "2014",
      "abstract": [
        "Pitch extraction, also known as fundamental frequency estimation, is\na long-term task in audio signal processing. Especially, due to the\npresence of accompaniment, vocal pitch extraction in polyphonic music\nis more challenging. So far, most of deep learning approaches use log\nmel spectrogram as input, which neglect the phase information. In addition,\nshallow networks have been applied on waveform directly, which may\nnot handle contaminated vocal data well. In this paper, a deep convolutional\nresidual network is proposed. It analyzes and extracts effective feature\nfrom waveform automatically. Residual learning can reduce model degradation\ndue to the skip connection and residual mapping. In comparison to reported\nresults, the proposed approach shows 5% and 4% improvement on overall\naccuracy(OA) and raw pitch accuracy(RPA) respectively.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2286",
      "author_area_id": 5,
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "sharma19c_interspeech": {
      "authors": [
        [
          "Bidisha",
          "Sharma"
        ],
        [
          "Rohan Kumar",
          "Das"
        ],
        [
          "Haizhou",
          "Li"
        ]
      ],
      "title": "Multi-Level Adaptive Speech Activity Detector for Speech in Naturalistic Environments",
      "original": "1928",
      "page_count": 5,
      "order": 416,
      "p1": "2015",
      "pn": "2019",
      "abstract": [
        "Speech activity detection (SAD) is a part of many speech processing\napplications. The traditional SAD approaches use signal energy as the\nevidence to identify the speech regions. However, such methods perform\npoorly under uncontrolled environments. In this work, we propose a\nnovel SAD approach using a multi-level decision with signal knowledge\nin an adaptive manner. The multi-level evidence considered are modulation\nspectrum and smoothed Hilbert envelope of linear prediction (LP) residual.\nModulation spectrum has compelling parallels to the dynamics of speech\nproduction and captures information only for the speech component.\nContrarily, Hilbert envelope of LP residual captures excitation source\naspect of speech. Under uncontrolled scenario, these evidence are found\nto be robust towards the signal distortions and thus expected to work\nwell. In view of different levels of interference present in the signal,\nwe propose to use a quality factor to control the speech/non-speech\ndecision in an adaptive manner. We refer this method as multi-level\nadaptive SAD and evaluate on Fearless Steps corpus that is collected\nduring Apollo-11 Mission in naturalistic environments. We achieve a\ndetection cost function of 7.35% with the proposed multi-level adaptive\nSAD on the evaluation set of Fearless Steps 2019 challenge corpus.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1928",
      "author_area_id": 5,
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "sharma19d_interspeech": {
      "authors": [
        [
          "Bidisha",
          "Sharma"
        ],
        [
          "Rohan Kumar",
          "Das"
        ],
        [
          "Haizhou",
          "Li"
        ]
      ],
      "title": "On the Importance of Audio-Source Separation for Singer Identification in Polyphonic Music",
      "original": "1925",
      "page_count": 5,
      "order": 417,
      "p1": "2020",
      "pn": "2024",
      "abstract": [
        "Singer identification is to automatically identify the singer in a\nmusic recording, such as a polyphonic song. A song has two major acoustic\ncomponents that are singing vocals and background accompaniment. Although\nidentifying singers is similar to speaker identification, it is challenging\ndue to the interference of background accompaniment on the singer-specific\ninformation in singing vocals. We believe that separating the background\naccompaniment from the singing vocal will help us to overcome the interference.\nIn this work, we extract the singing vocals from polyphonic songs using\nWave-U-Net based audio-source separation approach. The extracted singing\nvocals are then used in i-vector based singer identification system.\nFurther, we explore different state-of-the-art audio-source separation\nmethods to establish the role of considered method in application to\nsinger identification. The proposed singer identification framework\nachieves an absolute accuracy improvement of 5.66% over the baseline\nwithout audio-source separation.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1925",
      "author_area_id": 5,
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "terasawa19_interspeech": {
      "authors": [
        [
          "Hiroko",
          "Terasawa"
        ],
        [
          "Kenta",
          "Wakasa"
        ],
        [
          "Hideki",
          "Kawahara"
        ],
        [
          "Ken-Ichi",
          "Sakakibara"
        ]
      ],
      "title": "Investigating the Physiological and Acoustic Contrasts Between Choral and Operatic Singing",
      "original": "1864",
      "page_count": 5,
      "order": 418,
      "p1": "2025",
      "pn": "2029",
      "abstract": [
        "In this study, the difference in glottal vibration and timbre of singing\nvoice in choral and operatic singing was investigated. Eight professional\nsingers with active careers in operatic and choral performances participated\nin the experiment and sang excerpts from three operatic songs and two\nchoral songs. Audio and electroglottograph signals were simultaneously\nrecorded. The open quotient (O<SUB>q</SUB>) and singing power ratio\n(SPR) of the voices were analyzed, and it was found that the O<SUB>q</SUB>\nof choral singing tends to be higher and the SPR of choral singing\ntends to be lower than those of operatic singing. This suggests that\nchoral singing is conducted with laxer vocal fold coordination, and\nit has less ringing timbre than operatic singing. However, the O<SUB>q</SUB>\nand SPR were not directly correlated: the degree of adjustment of SPR\ndiffered across singers, suggesting that the strategy to achieve a\ndesired voice quality is individualistic in nature.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1864",
      "author_area_id": 5,
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "lin19c_interspeech": {
      "authors": [
        [
          "Ruixi",
          "Lin"
        ],
        [
          "Charles",
          "Costello"
        ],
        [
          "Charles",
          "Jankowski"
        ],
        [
          "Vishwas",
          "Mruthyunjaya"
        ]
      ],
      "title": "Optimizing Voice Activity Detection for Noisy Conditions",
      "original": "1776",
      "page_count": 5,
      "order": 419,
      "p1": "2030",
      "pn": "2034",
      "abstract": [
        "In this work, we focus our attention on how to improve Voice Activity\nDetection (VAD) in noisy conditions. We propose a Convolutional Neural\nNetwork (CNN) based model, as well as a Denoising Autoencoder (DAE),\nand experiment against acoustic features and their delta features in\nnoise levels ranging from SNR 35 dB to 0 dB. The experiments compare\nand find the best model configuration for robust performance in noisy\nconditions. We observe that combining more expressive audio features\nwith the use of DAEs improve accuracy, especially as noise increases.\nAt 0 dB, the proposed model trained with the best feature set could\nachieve a lab test accuracy of 93.2% (averaged across all noise levels)\nand 88.6% inference accuracy on device. We also compress the neural\nnetwork and deploy the inference model that is optimized for the app\nso that the average on-device CPU usage is reduced to 14% from 37%.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1776",
      "author_area_id": 5,
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "yamamoto19c_interspeech": {
      "authors": [
        [
          "Taiki",
          "Yamamoto"
        ],
        [
          "Ryota",
          "Nishimura"
        ],
        [
          "Masayuki",
          "Misaki"
        ],
        [
          "Norihide",
          "Kitaoka"
        ]
      ],
      "title": "Small-Footprint Magic Word Detection Method Using Convolutional LSTM Neural Network",
      "original": "1662",
      "page_count": 5,
      "order": 420,
      "p1": "2035",
      "pn": "2039",
      "abstract": [
        "The number of consumer devices which can be operated by voice is increasing\nevery year. Magic Word Detection (MWD), the detection of an activation\nkeyword in continuous speech, has become an essential technology for\nthe hands-free operation of such devices. Because MWD systems need\nto run constantly in order to detect Magic Words at any time, many\nstudies have focused on the development of a small-footprint system.\nIn this paper, we propose a novel, small-footprint MWD method which\nuses a convolutional Long Short-Term Memory (LSTM) neural network to\ncapture frequency and time domain features over time. As a result,\nthe proposed method outperforms the baseline method while reducing\nthe number of parameters by more than 80%. An experiment on a small-scale\ndevice demonstrates that our model is efficient enough to function\nin real time.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1662",
      "author_area_id": 5,
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "gupta19b_interspeech": {
      "authors": [
        [
          "Chitralekha",
          "Gupta"
        ],
        [
          "Emre",
          "Y\u0131lmaz"
        ],
        [
          "Haizhou",
          "Li"
        ]
      ],
      "title": "Acoustic Modeling for Automatic Lyrics-to-Audio Alignment",
      "original": "1520",
      "page_count": 5,
      "order": 421,
      "p1": "2040",
      "pn": "2044",
      "abstract": [
        "Automatic lyrics to polyphonic audio alignment is a challenging task\nnot only because the vocals are corrupted by background music, but\nalso there is a lack of annotated polyphonic corpus for effective acoustic\nmodeling. In this work, we propose (1) using additional speech and\nmusic-informed features and (2) adapting the acoustic models trained\non a large amount of solo singing vocals towards polyphonic music using\na small amount of in-domain data. Incorporating additional information\nsuch as voicing and auditory features together with conventional acoustic\nfeatures aims to bring robustness against the increased spectro-temporal\nvariations in singing vocals. By adapting the acoustic model using\na small amount of polyphonic audio data, we reduce the domain mismatch\nbetween training and testing data. We perform several alignment experiments\nand present an in-depth alignment error analysis on acoustic features,\nand model adaptation techniques. The results demonstrate that the proposed\nstrategy provides a significant error reduction of word boundary alignment\nover comparable existing systems, especially on more challenging polyphonic\ndata with long-duration musical interludes.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1520",
      "author_area_id": 5,
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "vafeiadis19_interspeech": {
      "authors": [
        [
          "Anastasios",
          "Vafeiadis"
        ],
        [
          "Eleftherios",
          "Fanioudakis"
        ],
        [
          "Ilyas",
          "Potamitis"
        ],
        [
          "Konstantinos",
          "Votis"
        ],
        [
          "Dimitrios",
          "Giakoumis"
        ],
        [
          "Dimitrios",
          "Tzovaras"
        ],
        [
          "Liming",
          "Chen"
        ],
        [
          "Raouf",
          "Hamzaoui"
        ]
      ],
      "title": "Two-Dimensional Convolutional Recurrent Neural Networks for Speech Activity Detection",
      "original": "1354",
      "page_count": 5,
      "order": 422,
      "p1": "2045",
      "pn": "2049",
      "abstract": [
        "Speech Activity Detection (SAD) plays an important role in mobile communications\nand automatic speech recognition (ASR). Developing efficient SAD systems\nfor real-world applications is a challenging task due to the presence\nof noise. We propose a new approach to SAD where we treat it as a two-dimensional\nmultilabel image classification problem. To classify the audio segments,\nwe compute their Short-time Fourier Transform spectrograms and classify\nthem with a Convolutional Recurrent Neural Network (CRNN), traditionally\nused in image recognition. Our CRNN uses a sigmoid activation function,\nmax-pooling in the frequency domain, and a convolutional operation\nas a moving average filter to remove misclassified spikes. On the development\nset of Task 1 of the 2019 Fearless Steps Challenge, our system achieved\na decision cost function (DCF) of 2.89%, a 66.4% improvement over the\nbaseline. Moreover, it achieved a DCF score of 3.318% on the evaluation\ndataset of the challenge, ranking first among all submissions.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1354",
      "author_area_id": 13,
      "author_area_label": "Special Sessions"
    },
    "kaburagi19_interspeech": {
      "authors": [
        [
          "Tokihiko",
          "Kaburagi"
        ]
      ],
      "title": "A Study of Soprano Singing in Light of the Source-Filter Interaction",
      "original": "1153",
      "page_count": 5,
      "order": 423,
      "p1": "2050",
      "pn": "2054",
      "abstract": [
        "We examined the physical interaction between the voice source system\nin the larynx and the acoustic filter of the vocal tract. The vocal\ntract of a soprano was first scanned in three dimensions using magnetic\nresonance imaging (MRI) while she produced four musical notes with\ndifferent vowels. These images were used to simulate voice production,\nincluding the vibratory motion of the vocal folds and the behavior\nof glottal airflow. Images for the /i/ vowel were used in the simulation,\nbecause a good proximity relationship was found between the fundamental\nfrequency and the first impedance peak of the vocal tract. The simulation\nresults revealed that the fundamental frequency (vibration frequency\nof the vocal folds) was decreased to a large extent by the source-filter\ninteraction especially when their natural frequency was in the proximity\nof the impedance peak. In a specific case, this frequency lowering\nhad the effect of changing the acoustic load of the vocal tract exerted\non the vocal folds so that their vibratory motion was effectively assisted.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1153",
      "author_area_id": 5,
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "zou19_interspeech": {
      "authors": [
        [
          "Yuxiang",
          "Zou"
        ],
        [
          "Linhao",
          "Dong"
        ],
        [
          "Bo",
          "Xu"
        ]
      ],
      "title": "Boosting Character-Based Chinese Speech Synthesis via Multi-Task Learning and Dictionary Tutoring",
      "original": "3233",
      "page_count": 5,
      "order": 424,
      "p1": "2055",
      "pn": "2059",
      "abstract": [
        "Recent character-based end-to-end text-to-speech (TTS) systems have\nshown promising performance in natural speech generation, especially\nfor English. However, for Chinese TTS, the character-based model is\neasy to generate speech with wrong pronunciation due to the label sparsity\nissue. To address this issue, we introduce an additional learning task\nof character-to-pinyin mapping to boost the pronunciation learning\nof characters, and leverage a pre-trained dictionary network to correct\nthe pronunciation mistake through joint training. Specifically, our\nmodel predicts pinyin labels as an auxiliary task to assist learning\nbetter hidden representations of Chinese characters, where pinyin is\na standard phonetic representation for Chinese characters. The dictionary\nnetwork plays a role as a tutor to further help hidden representation\nlearning. Experiments demonstrate that employing the pinyin auxiliary\ntask and an external dictionary network clearly enhances the naturalness\nand intelligibility of the synthetic speech directly from the Chinese\ncharacter sequences.\n"
      ],
      "doi": "10.21437/Interspeech.2019-3233",
      "author_area_id": 7,
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "xue19_interspeech": {
      "authors": [
        [
          "Liumeng",
          "Xue"
        ],
        [
          "Wei",
          "Song"
        ],
        [
          "Guanghui",
          "Xu"
        ],
        [
          "Lei",
          "Xie"
        ],
        [
          "Zhizheng",
          "Wu"
        ]
      ],
      "title": "Building a Mixed-Lingual Neural TTS System with Only Monolingual Data",
      "original": "3191",
      "page_count": 5,
      "order": 425,
      "p1": "2060",
      "pn": "2064",
      "abstract": [
        "When deploying a Chinese neural Text-to-Speech (TTS) system, one of\nthe challenges is to synthesize Chinese utterances with English phrases\nor words embedded. This paper looks into the problem in the encoder-decoder\nframework when only monolingual data from a target speaker is available.\nSpecifically, we view the problem from two aspects: speaker consistency\nwithin an utterance and naturalness. We start the investigation with\nan average voice model which is built from multi-speaker monolingual\ndata, i.e., Mandarin and English data. On the basis of that, we look\ninto speaker embedding for speaker consistency within an utterance\nand phoneme embedding for naturalness and intelligibility, and study\nthe choice of data for model training. We report the findings and discuss\nthe challenges to build a mixed-lingual TTS system with only monolingual\ndata.\n"
      ],
      "doi": "10.21437/Interspeech.2019-3191",
      "author_area_id": 7,
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "sokolov19_interspeech": {
      "authors": [
        [
          "Alex",
          "Sokolov"
        ],
        [
          "Tracy",
          "Rohlin"
        ],
        [
          "Ariya",
          "Rastrow"
        ]
      ],
      "title": "Neural Machine Translation for Multilingual Grapheme-to-Phoneme Conversion",
      "original": "3176",
      "page_count": 5,
      "order": 426,
      "p1": "2065",
      "pn": "2069",
      "abstract": [
        "Grapheme-to-phoneme (G2P) models are a key component in Automatic Speech\nRecognition (ASR) systems, such as the ASR system in Alexa, as they\nare used to generate pronunciations for out-of-vocabulary words that\ndo not exist in the pronunciation lexicons (mappings like  &#8220;e\nc h o&#8221; &#8594;  &#8220;E k oU&#8221;).<br style=\"mso-data-placement:same-cell;\"\n/><br style=\"mso-data-placement:same-cell;\" />\n  Most G2P systems are\nmonolingual and based on traditional joint-sequence based n-gram models\n[1, 2]. As an alternative, we present a single end-to-end trained neural\nG2P model that shares same encoder and decoder across multiple languages.\nThis allows the model to utilize a combination of universal symbol\ninventories of Latin-like alphabets and cross-linguistically shared\nfeature representations. Such model is especially useful in the scenarios\nof low resource languages and code switching/ foreign words, where\nthe pronunciations in one language need to be adapted to other locales\nor accents. We further experiment with word language distribution vector\nas an additional training target in order to improve system performance\nby helping the model decouple pronunciations across a variety of languages\nin the parameter space. We show 7.2% average improvement in phoneme\nerror rate over low resource languages and no degradation over high\nresource ones compared to monolingual baselines.\n"
      ],
      "doi": "10.21437/Interspeech.2019-3176",
      "author_area_id": 7,
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "taylor19_interspeech": {
      "authors": [
        [
          "Jason",
          "Taylor"
        ],
        [
          "Korin",
          "Richmond"
        ]
      ],
      "title": "Analysis of Pronunciation Learning in End-to-End Speech Synthesis",
      "original": "2830",
      "page_count": 5,
      "order": 427,
      "p1": "2070",
      "pn": "2074",
      "abstract": [
        "Ensuring correct pronunciation for the widest possible variety of text\ninput is vital for deployed text-to-speech (TTS) systems. For languages\nsuch as English that do not have trivial spelling, systems have always\nrelied heavily upon a lexicon, both for pronunciation lookup and for\ntraining letter-to-sound (LTS) models as a fall-back to handle out-of-vocabulary\nwords (OOVs). In contrast, recently proposed models that are trained\n&#8220;end-to-end&#8221; (E2E) aim to avoid linguistic text analysis\nand any explicit phone representation, instead learning pronunciation\nimplicitly as part of a direct mapping from input characters to speech\naudio. This might be termed  implicit LTS. In this paper, we explore\nthe nature of this approach by training  explicit LTS models with datasets\ncommonly used to build E2E systems. We compare their performance with\nLTS models trained on a high quality English lexicon. We find that\nLTS errors for words with ambiguous or unpredictable pronunciations\nare mirrored as mispronunciations by an E2E model. Overall, our analysis\nsuggests that limited and unbalanced lexical coverage in E2E training\ndata may pose significant confounding factors that complicate learning\naccurate pronunciations in a purely E2E system.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2830",
      "author_area_id": 7,
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "chen19f_interspeech": {
      "authors": [
        [
          "Yuan-Jui",
          "Chen"
        ],
        [
          "Tao",
          "Tu"
        ],
        [
          "Cheng-chieh",
          "Yeh"
        ],
        [
          "Hung-Yi",
          "Lee"
        ]
      ],
      "title": "End-to-End Text-to-Speech for Low-Resource Languages by Cross-Lingual Transfer Learning",
      "original": "2730",
      "page_count": 5,
      "order": 428,
      "p1": "2075",
      "pn": "2079",
      "abstract": [
        "End-to-end text-to-speech (TTS) has shown great success on large quantities\nof paired text plus speech data. However, laborious data collection\nremains difficult for at least 95% of the languages over the world,\nwhich hinders the development of TTS in different languages. In this\npaper, we aim to build TTS systems for such low-resource (target) languages\nwhere only very limited paired data are available. We show such TTS\ncan be effectively constructed by transferring knowledge from a high-resource\n(source) language. Since the model trained on source language cannot\nbe directly applied to target language due to input space mismatch,\nwe propose a method to learn a mapping between source and target linguistic\nsymbols. Benefiting from this learned mapping, pronunciation information\ncan be preserved throughout the transferring procedure. Preliminary\nexperiments show that we only need around 15 minutes of paired data\nto obtain a relatively good TTS system. Furthermore, analytic studies\ndemonstrated that the automatically discovered mapping correlate well\nwith the phonetic expertise.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2730",
      "author_area_id": 7,
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "zhang19e_interspeech": {
      "authors": [
        [
          "Yu",
          "Zhang"
        ],
        [
          "Ron J.",
          "Weiss"
        ],
        [
          "Heiga",
          "Zen"
        ],
        [
          "Yonghui",
          "Wu"
        ],
        [
          "Zhifeng",
          "Chen"
        ],
        [
          "R.J.",
          "Skerry-Ryan"
        ],
        [
          "Ye",
          "Jia"
        ],
        [
          "Andrew",
          "Rosenberg"
        ],
        [
          "Bhuvana",
          "Ramabhadran"
        ]
      ],
      "title": "Learning to Speak Fluently in a Foreign Language: Multilingual Speech Synthesis and Cross-Language Voice Cloning",
      "original": "2668",
      "page_count": 5,
      "order": 429,
      "p1": "2080",
      "pn": "2084",
      "abstract": [
        "We present a multispeaker, multilingual text-to-speech (TTS) synthesis\nmodel based on Tacotron that is able to produce high quality speech\nin multiple languages. Moreover, the model is able to transfer voices\nacross languages, e.g. synthesize fluent Spanish speech using an English\nspeaker&#8217;s voice, without training on any bilingual or parallel\nexamples. Such transfer works across distantly related languages, e.g.\nEnglish and Mandarin.<br style=\"mso-data-placement:same-cell;\" /><br\nstyle=\"mso-data-placement:same-cell;\" />\n  Critical to achieving this\nresult are: 1. using a phonemic input representation to encourage sharing\nof model capacity across languages, and 2. incorporating an adversarial\nloss term to encourage the model to disentangle its representation\nof speaker identity (which is perfectly correlated with language in\nthe training data) from the speech content. Further scaling up the\nmodel by training on multiple speakers of each language, and incorporating\nan autoencoding input to help stabilize attention during training,\nresults in a model which can be used to consistently synthesize intelligible\nspeech for training speakers in all languages seen during training,\nand in native or foreign accents.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2668",
      "author_area_id": 7,
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "juzova19_interspeech": {
      "authors": [
        [
          "Mark\u00e9ta",
          "J\u016fzov\u00e1"
        ],
        [
          "Daniel",
          "Tihelka"
        ],
        [
          "Jakub",
          "V\u00edt"
        ]
      ],
      "title": "Unified Language-Independent DNN-Based G2P Converter",
      "original": "2335",
      "page_count": 5,
      "order": 430,
      "p1": "2085",
      "pn": "2089",
      "abstract": [
        "We introduce a unified Grapheme-to-phoneme conversion framework based\non the composition of deep neural networks. In contrary to the usual\napproaches building the G2P frameworks from the dictionary, we use\nwhole phrases, which allows us to capture various language properties,\ne.g. cross-word assimilation, without the need for any special care\nor topology adjustments. The evaluation is carried out on three different\nlanguages &#8212; English, Czech and Russian. Each requires dealing\nwith specific properties, stressing the proposed framework in various\nways. The very first results show promising performance of the proposed\nframework, dealing with all the phenomena specific to the tested languages.\nThus, we consider the framework to be language-independent for a wide\nrange of languages.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2335",
      "author_area_id": 7,
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "dai19_interspeech": {
      "authors": [
        [
          "Dongyang",
          "Dai"
        ],
        [
          "Zhiyong",
          "Wu"
        ],
        [
          "Shiyin",
          "Kang"
        ],
        [
          "Xixin",
          "Wu"
        ],
        [
          "Jia",
          "Jia"
        ],
        [
          "Dan",
          "Su"
        ],
        [
          "Dong",
          "Yu"
        ],
        [
          "Helen",
          "Meng"
        ]
      ],
      "title": "Disambiguation of Chinese Polyphones in an End-to-End Framework with Semantic Features Extracted by Pre-Trained BERT",
      "original": "2292",
      "page_count": 5,
      "order": 431,
      "p1": "2090",
      "pn": "2094",
      "abstract": [
        "Grapheme-to-phoneme (G2P) conversion serves as an essential component\nin Chinese Mandarin text-to-speech (TTS) system, where polyphone disambiguation\nis the core issue. In this paper, we propose an end-to-end framework\nto predict the pronunciation of polyphonic character, which accepts\nsentence containing polyphonic character as input in the form of Chinese\ncharacter sequence without the necessity of any preprocessing. The\nproposed method consists of a pre-trained bidirectional encoder representations\nfrom Transformers (BERT) model and a neural network (NN) based classifier.\nThe pre-trained BERT model extracts semantic features from raw Chinese\ncharacter sequence and the NN based classifier predicts the polyphonic\ncharacter&#8217;s pronunciation according to BERT output. To explore\nthe impact of contextual information on polyphone disambiguation, three\ndifferent classifiers are investigated: a fully-connected network based\nclassifier, a long short-term memory (LSTM) network based classifier\nand a Transformer block based classifier. Experimental results demonstrate\nthe effectiveness of the proposed end-to-end framework for polyphone\ndisambiguation and the semantic features extracted by BERT can greatly\nenhance the performance.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2292",
      "author_area_id": 7,
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "yolchuyeva19_interspeech": {
      "authors": [
        [
          "Sevinj",
          "Yolchuyeva"
        ],
        [
          "G\u00e9za",
          "N\u00e9meth"
        ],
        [
          "B\u00e1lint",
          "Gyires-T\u00f3th"
        ]
      ],
      "title": "Transformer Based Grapheme-to-Phoneme Conversion",
      "original": "1954",
      "page_count": 5,
      "order": 432,
      "p1": "2095",
      "pn": "2099",
      "abstract": [
        "Attention mechanism is one of the most successful techniques in deep\nlearning based Natural Language Processing (NLP). The transformer network\narchitecture is completely based on attention mechanisms, and it outperforms\nsequence-to-sequence models in neural machine translation without recurrent\nand convolutional layers. Grapheme-to-phoneme (G2P) conversion is a\ntask of converting letters (grapheme sequence) to their pronunciations\n(phoneme sequence). It plays a significant role in text-to-speech (TTS)\nand automatic speech recognition (ASR) systems. In this paper, we investigate\nthe application of transformer architecture to G2P conversion and compare\nits performance with recurrent and convolutional neural network based\napproaches. Phoneme and word error rates are evaluated on the CMUDict\ndataset for US English and the NetTalk dataset. The results show that\ntransformer based G2P outperforms the convolutional-based approach\nin terms of word error rate and our results significantly exceeded\nprevious recurrent approaches (without attention) regarding word and\nphoneme error rates on both datasets. Furthermore, the size of the\nproposed model is much smaller than the size of the previous approaches.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1954",
      "author_area_id": 7,
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "bleyan19_interspeech": {
      "authors": [
        [
          "Harry",
          "Bleyan"
        ],
        [
          "Sandy",
          "Ritchie"
        ],
        [
          "Jonas Fromseier",
          "Mortensen"
        ],
        [
          "Daan van",
          "Esch"
        ]
      ],
      "title": "Developing Pronunciation Models in New Languages Faster by Exploiting Common Grapheme-to-Phoneme Correspondences Across Languages",
      "original": "1781",
      "page_count": 5,
      "order": 433,
      "p1": "2100",
      "pn": "2104",
      "abstract": [
        "We discuss two methods that let us easily create grapheme-to-phoneme\n(G2P) conversion systems for languages without any human-curated pronunciation\nlexicons, as long as we know the phoneme inventory of the target language\nand as long as we have some pronunciation lexicons for other languages\nwritten in the same script. We use these resources to infer what grapheme-to-phoneme\ncorrespondences we would expect, and predict pronunciations for words\nin the target language with minimal or no language-specific human work.\nOur first approach uses finite-state transducers, while our second\napproach uses a sequence-to-sequence neural network. Our G2P models\nreach high degrees of accuracy, and can be used for various applications,\ne.g. in developing an automatic speech recognition system. Our methods\ngreatly simplify a task that has historically required extensive manual\nlabor.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1781",
      "author_area_id": 7,
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "chen19g_interspeech": {
      "authors": [
        [
          "Mengnan",
          "Chen"
        ],
        [
          "Minchuan",
          "Chen"
        ],
        [
          "Shuang",
          "Liang"
        ],
        [
          "Jun",
          "Ma"
        ],
        [
          "Lei",
          "Chen"
        ],
        [
          "Shaojun",
          "Wang"
        ],
        [
          "Jing",
          "Xiao"
        ]
      ],
      "title": "Cross-Lingual, Multi-Speaker Text-To-Speech Synthesis Using Neural Speaker Embedding",
      "original": "1632",
      "page_count": 5,
      "order": 434,
      "p1": "2105",
      "pn": "2109",
      "abstract": [
        "Neural network-based model for text-to-speech (TTS) synthesis has made\nsignificant progress in recent years. In this paper, we present a cross-lingual,\nmulti-speaker neural end-to-end TTS framework which can model speaker\ncharacteristics and synthesize speech in different languages. We implement\nthe model by introducing a separately trained neural speaker embedding\nnetwork, which can represent the latent structure of different speakers\nand language pronunciations. We train the speech synthesis network\nbilingually and prove the possibility of synthesizing Chinese speaker&#8217;s\nEnglish speech and vice versa. We explore different methods to fit\na new speaker using only a few speech samples. The experimental results\nshow that, with only several minutes of audio from a new speaker, the\nproposed model can synthesize speech bilingually and acquire decent\nnaturalness and similarity for both languages.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1632",
      "author_area_id": 7,
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "cai19b_interspeech": {
      "authors": [
        [
          "Zexin",
          "Cai"
        ],
        [
          "Yaogen",
          "Yang"
        ],
        [
          "Chuxiong",
          "Zhang"
        ],
        [
          "Xiaoyi",
          "Qin"
        ],
        [
          "Ming",
          "Li"
        ]
      ],
      "title": "Polyphone Disambiguation for Mandarin Chinese Using Conditional Neural Network with Multi-Level Embedding Features",
      "original": "1235",
      "page_count": 5,
      "order": 435,
      "p1": "2110",
      "pn": "2114",
      "abstract": [
        "This paper describes a conditional neural network architecture for\nMandarin Chinese polyphone disambiguation. The system is composed of\na bidirectional recurrent neural network component acting as a sentence\nencoder to accumulate the context correlations, followed by a prediction\nnetwork that maps the polyphonic character embeddings along with the\nconditions to corresponding pronunciations. We obtain the word-level\ncondition from a pre-trained word-to-vector lookup table. One goal\nof polyphone disambiguation is to address the homograph problem existing\nin the front-end processing of Mandarin Chinese text-to-speech system.\nOur system achieves an accuracy of 94.69% on a publicly available polyphonic\ncharacter dataset. To further validate our choices on the conditional\nfeature, we investigate polyphone disambiguation systems with multi-level\nconditions respectively. The experimental results show that both the\nsentence-level and the word-level conditional embedding features are\nable to attain good performance for Mandarin Chinese polyphone disambiguation.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1235",
      "author_area_id": 7,
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "sun19c_interspeech": {
      "authors": [
        [
          "Hao",
          "Sun"
        ],
        [
          "Xu",
          "Tan"
        ],
        [
          "Jun-Wei",
          "Gan"
        ],
        [
          "Hongzhi",
          "Liu"
        ],
        [
          "Sheng",
          "Zhao"
        ],
        [
          "Tao",
          "Qin"
        ],
        [
          "Tie-Yan",
          "Liu"
        ]
      ],
      "title": "Token-Level Ensemble Distillation for Grapheme-to-Phoneme Conversion",
      "original": "1208",
      "page_count": 5,
      "order": 436,
      "p1": "2115",
      "pn": "2119",
      "abstract": [
        "Grapheme-to-phoneme (G2P) conversion is an important task in automatic\nspeech recognition and text-to-speech systems. Recently, G2P conversion\nis viewed as a sequence to sequence task and modeled by RNN or CNN\nbased encoder-decoder framework. However, previous works do not consider\nthe practical issues when deploying G2P model in the production system,\nsuch as how to leverage additional unlabeled data to boost the accuracy,\nas well as reduce model size for online deployment. In this work, we\npropose token-level ensemble distillation for G2P conversion, which\ncan (1) boost the accuracy by distilling the knowledge from additional\nunlabeled data, and (2) reduce the model size but maintain the high\naccuracy, both of which are very practical and helpful in the online\nproduction system. We use token-level knowledge distillation, which\nresults in better accuracy than the sequence-level counterpart. What\nis more, we adopt the Transformer instead of RNN or CNN based models\nto further boost the accuracy of G2P conversion. Experiments on the\npublicly available CMUDict dataset and an internal English dataset\ndemonstrate the effectiveness of our proposed method. Particularly,\nour method achieves 19.88% WER on CMUDict dataset, outperforming the\nprevious works by more than 4.22% WER, and setting the new state-of-the-art\nresults.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1208",
      "author_area_id": 7,
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "li19i_interspeech": {
      "authors": [
        [
          "Xinjian",
          "Li"
        ],
        [
          "Siddharth",
          "Dalmia"
        ],
        [
          "Alan W.",
          "Black"
        ],
        [
          "Florian",
          "Metze"
        ]
      ],
      "title": "Multilingual Speech Recognition with Corpus Relatedness Sampling",
      "original": "3052",
      "page_count": 5,
      "order": 437,
      "p1": "2120",
      "pn": "2124",
      "abstract": [
        "Multilingual acoustic models have been successfully applied to low-resource\nspeech recognition. Most existing works have combined many small corpora\ntogether, and pretrained a multilingual model by sampling from each\ncorpus uniformly. The model is eventually fine-tuned on each target\ncorpus. This approach, however, fails to exploit the relatedness and\nsimilarity among corpora in the training set. For example, the target\ncorpus might benefit more from a corpus in the same domain or a corpus\nfrom a close language. In this work, we propose a simple but useful\nsampling strategy to take advantage of this relatedness. We first compute\nthe corpus-level embeddings and estimate the similarity between each\ncorpus. Next we start training the multilingual model with uniform-sampling\nfrom each corpus at first, then we gradually increase the probability\nto sample from related corpora based on its similarity with the target\ncorpus. Finally the model would be fine-tuned automatically on the\ntarget corpus. Our sampling strategy outperforms the baseline multilingual\nmodel on 16 low-resource tasks. Additionally, we demonstrate that our\ncorpus embeddings capture the language and domain information of each\ncorpus.\n"
      ],
      "doi": "10.21437/Interspeech.2019-3052",
      "author_area_id": 8,
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "arsikere19_interspeech": {
      "authors": [
        [
          "Harish",
          "Arsikere"
        ],
        [
          "Ashtosh",
          "Sapru"
        ],
        [
          "Sri",
          "Garimella"
        ]
      ],
      "title": "Multi-Dialect Acoustic Modeling Using Phone Mapping and Online i-Vectors",
      "original": "2881",
      "page_count": 5,
      "order": 438,
      "p1": "2125",
      "pn": "2129",
      "abstract": [
        "This paper proposes a simple phone mapping approach to multi-dialect\nacoustic modeling. In contrast to the widely used shared hidden layer\n(SHL) training approach (hidden layers are shared across dialects whereas\noutput layers are kept separate), phone mapping simplifies model training\nand maintenance by allowing all the network parameters to be shared;\nit also simplifies online adaptation via HMM-based i-vectors by allowing\nthe same T-matrix to be used for all the dialects. Using the LSTM-HMM\nframework, we compare phone mapping with transfer learning and SHL\ntraining, and we also compare the efficacy of online i-vectors with\nthat of one-hot dialect encoding. Experiments with a 2K hour dataset\ncomprising four English dialects show that (1) phone mapping yields\nsignificant WER reductions over dialect-specific training (14%, on\naverage) and transfer learning (5%, on average); (2) SHL training is\nonly slightly better than phone mapping; and (3) i-vectors provide\nuseful additional reductions (3%, on average) while one-hot encoding\nhas little effect. Even with a large 40K hour dataset (comprising the\nsame four English dialects) and fully optimized sequence discriminative\ntraining, we show that phone mapping provides healthy WER reductions\nover dialect-specific models (10%, on average).\n"
      ],
      "doi": "10.21437/Interspeech.2019-2881",
      "author_area_id": 8,
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "kannan19_interspeech": {
      "authors": [
        [
          "Anjuli",
          "Kannan"
        ],
        [
          "Arindrima",
          "Datta"
        ],
        [
          "Tara N.",
          "Sainath"
        ],
        [
          "Eugene",
          "Weinstein"
        ],
        [
          "Bhuvana",
          "Ramabhadran"
        ],
        [
          "Yonghui",
          "Wu"
        ],
        [
          "Ankur",
          "Bapna"
        ],
        [
          "Zhifeng",
          "Chen"
        ],
        [
          "Seungji",
          "Lee"
        ]
      ],
      "title": "Large-Scale Multilingual Speech Recognition with a Streaming End-to-End Model",
      "original": "2858",
      "page_count": 5,
      "order": 439,
      "p1": "2130",
      "pn": "2134",
      "abstract": [
        "Multilingual end-to-end (E2E) models have shown great promise in expansion\nof automatic speech recognition (ASR) coverage of the world&#8217;s\nlanguages. They have shown improvement over monolingual systems, and\nhave simplified training and serving by eliminating language-specific\nacoustic, pronunciation, and language models. This work presents an\nE2E multilingual system which is equipped to operate in low-latency\ninteractive applications, as well as handle a key challenge of real\nworld data: the imbalance in training data across languages. Using\nnine Indic languages, we compare a variety of techniques, and find\nthat a combination of conditioning on a language vector and training\nlanguage-specific adapter layers produces the best model. The resulting\nE2E multilingual model achieves a lower word error rate (WER) than\nboth monolingual E2E models (eight of nine languages) and monolingual\nconventional systems (all nine languages).\n"
      ],
      "doi": "10.21437/Interspeech.2019-2858",
      "author_area_id": 8,
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "mendes19_interspeech": {
      "authors": [
        [
          "Carlos",
          "Mendes"
        ],
        [
          "Alberto",
          "Abad"
        ],
        [
          "Jo\u00e3o Paulo",
          "Neto"
        ],
        [
          "Isabel",
          "Trancoso"
        ]
      ],
      "title": "Recognition of Latin American Spanish Using Multi-Task Learning",
      "original": "2772",
      "page_count": 5,
      "order": 440,
      "p1": "2135",
      "pn": "2139",
      "abstract": [
        "In the broadcast news domain, national wide newscasters typically interact\nwith communities with a diverse set of accents. One of the challenges\nin speech recognition is the performance degradation in the presence\nof these diverse conditions. Performance further aggravates when the\naccents are from other countries that share the same language. Extensive\nwork has been conducted in this topic for languages such as English\nand Mandarin. Recently, TDNN based multi-task learning has received\nsome attention in this area, with interesting results, typically using\nmodels trained with a variety of different accented corpora from a\nparticular language. In this work, we look at the case of LATAM (Latin\nAmerican) Spanish for its unique and distinctive accent variations.\nBecause LATAM Spanish has historically been influenced by non-Spanish\nEuropean migrations, we anticipated that LATAM based speech recognition\nperformance can be further improved by including these influential\nlanguages, during a TDNN based multi-task training. Experiments show\nthat including such languages in the training setup outperforms the\nsingle task acoustic model baseline. We also propose an automatic per-language\nweight selection strategy to regularize each language contribution\nduring multi-task training.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2772",
      "author_area_id": 8,
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "viglino19_interspeech": {
      "authors": [
        [
          "Thibault",
          "Viglino"
        ],
        [
          "Petr",
          "Motlicek"
        ],
        [
          "Milos",
          "Cernak"
        ]
      ],
      "title": "End-to-End Accented Speech Recognition",
      "original": "2122",
      "page_count": 5,
      "order": 441,
      "p1": "2140",
      "pn": "2144",
      "abstract": [
        "Correct pronunciation is known to be the most difficult part to acquire\nfor (native or non-native) language learners. The accented speech is\nthus more variable, and standard Automatic Speech Recognition (ASR)\ntraining approaches that rely on intermediate phone alignment might\nintroduce errors during the ASR training. With end-to-end training\nwe could alleviate this problem. In this work, we explore the use of\nmulti-task training and accent embedding in the context of end-to-end\nASR trained with the connectionist temporal classification loss. Comparing\nto the baseline developed using conventional ASR framework exploiting\ntime-delay neural networks trained on accented English, we show significant\nrelative improvement of about 25% in word error rate. Additional evaluation\non unseen accent data yields relative improvements of of 31% and 2%\nfor New Zealand English and Indian English, respectively.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2122",
      "author_area_id": 8,
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "li19j_interspeech": {
      "authors": [
        [
          "Sheng",
          "Li"
        ],
        [
          "Chenchen",
          "Ding"
        ],
        [
          "Xugang",
          "Lu"
        ],
        [
          "Peng",
          "Shen"
        ],
        [
          "Tatsuya",
          "Kawahara"
        ],
        [
          "Hisashi",
          "Kawai"
        ]
      ],
      "title": "End-to-End Articulatory Attribute Modeling for Low-Resource Multilingual Speech Recognition",
      "original": "2092",
      "page_count": 5,
      "order": 442,
      "p1": "2145",
      "pn": "2149",
      "abstract": [
        "The end-to-end (E2E) model allows for training of automatic speech\nrecognition (ASR) systems without the hand-designed language-specific\npronunciation lexicons. However, constructing the multilingual low-resource\nE2E ASR system is still challenging due to the vast number of symbols\n(e.g., words and characters). In this paper, we investigate an efficient\nmethod of encoding multilingual transcriptions for training E2E ASR\nsystems. We directly encode the symbols of multilingual writing systems\nto universal articulatory representations, which is much more compact\nthan characters and words. Compared with traditional multilingual modeling\nmethods, we directly build a single acoustic-articulatory within recent\ntransformer-based E2E framework for ASR tasks. The speech recognition\nresults of our proposed method significantly outperform the conventional\nword-based and character-based E2E models.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2092",
      "author_area_id": 8,
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "taneja19_interspeech": {
      "authors": [
        [
          "Karan",
          "Taneja"
        ],
        [
          "Satarupa",
          "Guha"
        ],
        [
          "Preethi",
          "Jyothi"
        ],
        [
          "Basil",
          "Abraham"
        ]
      ],
      "title": "Exploiting Monolingual Speech Corpora for Code-Mixed Speech Recognition",
      "original": "1959",
      "page_count": 5,
      "order": 443,
      "p1": "2150",
      "pn": "2154",
      "abstract": [
        "One of the main challenges in building code-mixed ASR systems is the\nlack of annotated speech data. Often, however, monolingual speech corpora\nare available in abundance for the languages in the code-mixed speech.\nIn this paper, we explore different techniques that use monolingual\nspeech to create synthetic code-mixed speech and examine their effect\non training models for code-mixed ASR. We assume access to a small\namount of real code-mixed text, from which we extract probability distributions\nthat govern the transition of phones across languages at code-switch\nboundaries and the span lengths corresponding to a particular language.\nWe extract segments from monolingual data and concatenate them to form\ncode-mixed utterances such that these probability distributions are\npreserved. Using this synthetic speech, we show significant improvements\nin Hindi-English code-mixed ASR performance compared to using synthetic\nspeech naively constructed from complete utterances in different languages.\nWe also present language modelling experiments that use synthetically\nconstructed code-mixed text and discuss their benefits.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1959",
      "author_area_id": 8,
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "hu19_interspeech": {
      "authors": [
        [
          "Ke",
          "Hu"
        ],
        [
          "Antoine",
          "Bruguier"
        ],
        [
          "Tara N.",
          "Sainath"
        ],
        [
          "Rohit",
          "Prabhavalkar"
        ],
        [
          "Golan",
          "Pundak"
        ]
      ],
      "title": "Phoneme-Based Contextualization for Cross-Lingual Speech Recognition in End-to-End Models",
      "original": "1868",
      "page_count": 5,
      "order": 444,
      "p1": "2155",
      "pn": "2159",
      "abstract": [
        "Contextual automatic speech recognition, i.e., biasing recognition\ntowards a given context (e.g. user&#8217;s playlists, or contacts),\nis challenging in end-to-end (E2E) models. Such models maintain a limited\nnumber of candidates during beam-search decoding, and have been found\nto recognize rare named entities poorly. The problem is exacerbated\nwhen biasing towards proper nouns in foreign languages, e.g., geographic\nlocation names, which are virtually unseen in training and are thus\nout-of-vocabulary (OOV). While grapheme or wordpiece E2E models might\nhave a difficult time spelling OOV words, phonemes are more acoustically\nsalient and past work has shown that E2E phoneme models can better\npredict such words. In this work, we propose an E2E model containing\nboth English wordpieces and phonemes in the modeling space, and perform\ncontextual biasing of foreign words at the phoneme level by mapping\npronunciations of foreign words into similar English phonemes. In experimental\nevaluations, we find that the proposed approach performs 16% better\nthan a grapheme-only biasing model, and 8% better than a wordpiece-only\nbiasing model on a foreign place name recognition task, with only slight\ndegradation on regular English tasks.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1868",
      "author_area_id": 8,
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "khassanov19_interspeech": {
      "authors": [
        [
          "Yerbolat",
          "Khassanov"
        ],
        [
          "Haihua",
          "Xu"
        ],
        [
          "Van Tung",
          "Pham"
        ],
        [
          "Zhiping",
          "Zeng"
        ],
        [
          "Eng Siong",
          "Chng"
        ],
        [
          "Chongjia",
          "Ni"
        ],
        [
          "Bin",
          "Ma"
        ]
      ],
      "title": "Constrained Output Embeddings for End-to-End Code-Switching Speech Recognition with Only Monolingual Data",
      "original": "1867",
      "page_count": 5,
      "order": 445,
      "p1": "2160",
      "pn": "2164",
      "abstract": [
        "The lack of code-switch training data is one of the major concerns\nin the development of end-to-end code-switching automatic speech recognition\n(ASR) models. In this work, we propose a method to train an improved\nend-to-end code-switching ASR using only monolingual data. Our method\nencourages the distributions of output token embeddings of monolingual\nlanguages to be similar, and hence, promotes the ASR model to easily\ncode-switch between languages. Specifically, we propose to use Jensen-Shannon\ndivergence and cosine distance based constraints. The former will enforce\noutput embeddings of monolingual languages to possess similar distributions,\nwhile the later simply brings the centroids of two distributions to\nbe close to each other. Experimental results demonstrate high effectiveness\nof the proposed method, yielding up to 4.5% absolute mixed error rate\nimprovement on Mandarin-English code-switching ASR task.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1867",
      "author_area_id": 8,
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "zeng19_interspeech": {
      "authors": [
        [
          "Zhiping",
          "Zeng"
        ],
        [
          "Yerbolat",
          "Khassanov"
        ],
        [
          "Van Tung",
          "Pham"
        ],
        [
          "Haihua",
          "Xu"
        ],
        [
          "Eng Siong",
          "Chng"
        ],
        [
          "Haizhou",
          "Li"
        ]
      ],
      "title": "On the End-to-End Solution to Mandarin-English Code-Switching Speech Recognition",
      "original": "1429",
      "page_count": 5,
      "order": 446,
      "p1": "2165",
      "pn": "2169",
      "abstract": [
        "Code-switching (CS) refers to a linguistic phenomenon where a speaker\nuses different languages in an utterance or between alternating utterances.\nIn this work, we study end-to-end (E2E) approaches to the Mandarin-English\ncode-switching speech recognition task. We first examine the effectiveness\nof using data augmentation and byte-pair encoding (BPE) subword units.\nMore importantly, we propose a multitask learning recipe, where a language\nidentification task is explicitly learned in addition to the E2E speech\nrecognition task. Furthermore, we introduce an efficient word vocabulary\nexpansion method for language modeling to alleviate data sparsity issues\nunder the code-switching scenario. Experimental results on the SEAME\ndata, a Mandarin-English code-switching corpus, demonstrate the effectiveness\nof the proposed methods.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1429",
      "author_area_id": 8,
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "zhang19f_interspeech": {
      "authors": [
        [
          "Shiliang",
          "Zhang"
        ],
        [
          "Yuan",
          "Liu"
        ],
        [
          "Ming",
          "Lei"
        ],
        [
          "Bin",
          "Ma"
        ],
        [
          "Lei",
          "Xie"
        ]
      ],
      "title": "Towards Language-Universal Mandarin-English Speech Recognition",
      "original": "1365",
      "page_count": 5,
      "order": 447,
      "p1": "2170",
      "pn": "2174",
      "abstract": [
        "Multilingual and code-switching speech recognition are two challenging\ntasks that are studied separately in many previous works. In this work,\nwe jointly study multilingual and code-switching problems, and present\na language-universal bilingual system for Mandarin-English speech recognition.\nSpecifically, we propose a novel bilingual acoustic model, which consists\nof two monolingual system initialized subnets and a shared output layer\ncorresponding to the  Character-Subword acoustic modeling units. The\nbilingual acoustic model is trained using a large Mandarin-English\ncorpus with CTC and sMBR criteria. We find that this model, which is\nnot given any information about language identity, can achieve comparable\nperformance in monolingual Mandarin and English test sets compared\nto the well-trained language-specific Mandarin and English ASR systems,\nrespectively. More importantly, the proposed bilingual model can automatically\nlearn the language switching. Experimental results on a Mandarin-English\ncode-switching test set show that it can achieve 11.8% and 17.9% relative\nerror reduction on Mandarin and English parts, respectively.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1365",
      "author_area_id": 8,
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "swarup19_interspeech": {
      "authors": [
        [
          "Prakhar",
          "Swarup"
        ],
        [
          "Roland",
          "Maas"
        ],
        [
          "Sri",
          "Garimella"
        ],
        [
          "Sri Harish",
          "Mallidi"
        ],
        [
          "Bj\u00f6rn",
          "Hoffmeister"
        ]
      ],
      "title": "Improving ASR Confidence Scores for Alexa Using Acoustic and Hypothesis Embeddings",
      "original": "1241",
      "page_count": 5,
      "order": 448,
      "p1": "2175",
      "pn": "2179",
      "abstract": [
        "In automatic speech recognition, confidence measures provide a quantitative\nrepresentation used to assess whether a generated hypothesis text is\ncorrect or not. For personal assistant devices like Alexa, automatic\nspeech recognition (ASR) errors are inevitable due to the imperfection\nof today&#8217;s speech recognition technology. Hence, confidence scores\nprovide an important metric to gauge the correctness of ASR hypothesis\ntext and enable downstream consumers to subsequently initiate appropriate\nactions. In this work, our aim is to improve the correctness of our\nconfidence scores by enhancing our baseline model architecture with\nlearned features, namely acoustic and 1-best hypothesis embeddings.\nThese embeddings are obtained by training separate networks on acoustic\nfeatures and ASR 1-best hypothesis respectively. We present an experimental\nevaluation on a large US English data set showing a 6% relative equal\nerror rate reduction and 13% relative normalized cross-entropy improvement\nover our baseline system by incorporating these embeddings. We also\npresent a deeper analysis of the embeddings revealing that the acoustic\nembedding results in a better prediction of insertion errors whereas\nthe 1-best hypothesis embedding helps to better predict substitution\nerrors.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1241",
      "author_area_id": 9,
      "author_area_label": "Speech Recognition: Architecture, Search, and Linguistic Components"
    },
    "zhang19g_interspeech": {
      "authors": [
        [
          "Shiliang",
          "Zhang"
        ],
        [
          "Ming",
          "Lei"
        ],
        [
          "Zhijie",
          "Yan"
        ]
      ],
      "title": "Investigation of Transformer Based Spelling Correction Model for CTC-Based End-to-End Mandarin Speech Recognition",
      "original": "1290",
      "page_count": 5,
      "order": 449,
      "p1": "2180",
      "pn": "2184",
      "abstract": [
        "Connectionist Temporal Classification (CTC) based end-to-end speech\nrecognition system usually need to incorporate an external language\nmodel by using WFST-based decoding in order to achieve promising results.\nThis is more essential to Mandarin speech recognition since it owns\na special phenomenon, namely  homophone, which causes a lot of substitution\nerrors. The linguistic information introduced by language model is\nsomehow helpful to distinguish these substitution errors. In this work,\nwe propose a transformer based spelling correction model to automatically\ncorrect errors, especially the substitution errors, made by CTC-based\nMandarin speech recognition system. Specifically, we investigate to\nuse the recognition results generated by CTC-based systems as input\nand the ground-truth transcriptions as output to train a transformer\nwith encoder-decoder architecture, which is much similar to machine\ntranslation. Experimental results in a 20,000 hours Mandarin speech\nrecognition task show that the proposed spelling correction model can\nachieve a CER of 3.41%, which results in 22.9% and 53.2% relative improvement\ncompared to the baseline CTC-based systems decoded with and without\nlanguage model, respectively.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1290",
      "author_area_id": 9,
      "author_area_label": "Speech Recognition: Architecture, Search, and Linguistic Components"
    },
    "peyser19_interspeech": {
      "authors": [
        [
          "Cal",
          "Peyser"
        ],
        [
          "Hao",
          "Zhang"
        ],
        [
          "Tara N.",
          "Sainath"
        ],
        [
          "Zelin",
          "Wu"
        ]
      ],
      "title": "Improving Performance of End-to-End ASR on Numeric Sequences",
      "original": "1345",
      "page_count": 5,
      "order": 450,
      "p1": "2185",
      "pn": "2189",
      "abstract": [
        "Recognizing written domain numeric utterances (e.g., I need 1.25.)\ncan be challenging for ASR systems, particularly when numeric sequences\nare not seen during training. This out-of-vocabulary (OOV) issue is\naddressed in conventional ASR systems by training part of the model\non spoken domain utterances (e.g., I need one dollar and twenty five\ncents.), for which numeric sequences are composed of in-vocabulary\nnumbers, and then using an FST verbalizer to denormalize the result.\nUnfortunately, conventional ASR models are not suitable for the low\nmemory setting of on-device speech recognition. E2E models such as\nRNN-T are attractive for on-device ASR, as they fold the AM, PM and\nLM of a conventional model into one neural network. However, in the\non-device setting the large memory footprint of an FST denormer makes\nspoken domain training more difficult. In this paper, we investigate\ntechniques to improve E2E model performance on numeric data. We find\nthat using a text-to-speech system to generate additional numeric training\ndata, as well as using a small-footprint neural network to perform\nspoken-to-written domain denorming, yields improvement in several numeric\nclasses. In the case of the longest numeric sequences, we see reduction\nof WER by up to a factor of 8.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1345",
      "author_area_id": 9,
      "author_area_label": "Speech Recognition: Architecture, Search, and Linguistic Components"
    },
    "bai19_interspeech": {
      "authors": [
        [
          "Ye",
          "Bai"
        ],
        [
          "Jiangyan",
          "Yi"
        ],
        [
          "Jianhua",
          "Tao"
        ],
        [
          "Zhengqi",
          "Wen"
        ],
        [
          "Zhengkun",
          "Tian"
        ],
        [
          "Chenghao",
          "Zhao"
        ],
        [
          "Cunhang",
          "Fan"
        ]
      ],
      "title": "A Time Delay Neural Network with Shared Weight Self-Attention for Small-Footprint Keyword Spotting",
      "original": "1676",
      "page_count": 5,
      "order": 451,
      "p1": "2190",
      "pn": "2194",
      "abstract": [
        "Keyword spotting requires a small memory footprint to run on mobile\ndevices. However, previous works still use several hundred thousand\nparameters to achieve good performance. To address this issue, we propose\na time delay neural network with shared weight self-attention for small-footprint\nkeyword spotting. By sharing weights, the parameters of self-attention\nare reduced but without performance reduction. The publicly available\nGoogle Speech Commands dataset is used to evaluate the models. The\nnumber of parameters (12K) of our model is 1/20 of state-of-the-art\nResNet model (239K). The proposed model achieves an error rate of 4.19%\n, which is comparable to the ResNet model.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1676",
      "author_area_id": 9,
      "author_area_label": "Speech Recognition: Architecture, Search, and Linguistic Components"
    },
    "kao19_interspeech": {
      "authors": [
        [
          "Chieh-Chi",
          "Kao"
        ],
        [
          "Ming",
          "Sun"
        ],
        [
          "Yixin",
          "Gao"
        ],
        [
          "Shiv",
          "Vitaladevuni"
        ],
        [
          "Chao",
          "Wang"
        ]
      ],
      "title": "Sub-Band Convolutional Neural Networks for Small-Footprint Spoken Term Classification",
      "original": "1766",
      "page_count": 5,
      "order": 452,
      "p1": "2195",
      "pn": "2199",
      "abstract": [
        "This paper proposes a Sub-band Convolutional Neural Network for spoken\nterm classification. Convolutional neural networks (CNNs) have proven\nto be very effective in acoustic applications such as spoken term classification,\nkeyword spotting, speaker identification, acoustic event detection,\netc. Unlike applications in computer vision, the spatial invariance\nproperty of 2D convolutional kernels does not fit acoustic applications\nwell since the meaning of a specific 2D kernel varies a lot along the\nfeature axis in an input feature map. We propose a sub-band CNN architecture\nto apply different convolutional kernels on each feature sub-band,\nwhich makes the overall computation more efficient. Experimental results\nshow that the computational efficiency brought by sub-band CNN is more\nbeneficial for small-footprint models. Compared to a baseline full\nband CNN for spoken term classification on a publicly available Speech\nCommands dataset, the proposed sub-band CNN architecture reduces the\ncomputation by 39.7% on commands classification, and 49.3% on digits\nclassification with accuracy maintained.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1766",
      "author_area_id": 9,
      "author_area_label": "Speech Recognition: Architecture, Search, and Linguistic Components"
    },
    "li19k_interspeech": {
      "authors": [
        [
          "Sheng",
          "Li"
        ],
        [
          "Xugang",
          "Lu"
        ],
        [
          "Chenchen",
          "Ding"
        ],
        [
          "Peng",
          "Shen"
        ],
        [
          "Tatsuya",
          "Kawahara"
        ],
        [
          "Hisashi",
          "Kawai"
        ]
      ],
      "title": "Investigating Radical-Based End-to-End Speech Recognition Systems for Chinese Dialects and Japanese",
      "original": "2104",
      "page_count": 5,
      "order": 453,
      "p1": "2200",
      "pn": "2204",
      "abstract": [
        "Training automatic speech recognition (ASR) systems for East Asian\nlanguages (e.g., Chinese and Japanese) is tough work because of the\ncharacters existing in the writing systems of these languages. Traditionally,\nwe first need to get the pronunciation of these characters by morphological\nanalysis. The end-to-end (E2E) model allows for directly using characters\nor words as the modeling unit. However, since different groups of people\n(e.g., residents in Chinese mainland, Hong Kong, Taiwan, and Japan)\nadopts different writing forms for a character, this also leads to\na large increase in the number of vocabulary, especially when building\nASR systems across languages or dialects. In this paper, we propose\na new E2E ASR modeling method by decomposing the characters into a\nset of radicals. Our experiments demonstrate that it is possible to\neffectively reduce the vocabulary size by sharing the basic radicals\nacross different dialect of Chinese. Moreover, we also demonstrate\nthis method could also be used to construct a Japanese E2E ASR system.\nThe system modeled with radicals and kana achieved similar performance\ncompared to state-of-the-art E2E system built with word-piece units.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2104",
      "author_area_id": 9,
      "author_area_label": "Speech Recognition: Architecture, Search, and Linguistic Components"
    },
    "guo19d_interspeech": {
      "authors": [
        [
          "Jiaqi",
          "Guo"
        ],
        [
          "Yongbin",
          "You"
        ],
        [
          "Yanmin",
          "Qian"
        ],
        [
          "Kai",
          "Yu"
        ]
      ],
      "title": "Joint Decoding of CTC Based Systems for Speech Recognition",
      "original": "2026",
      "page_count": 5,
      "order": 454,
      "p1": "2205",
      "pn": "2209",
      "abstract": [
        "Connectionist temporal classification (CTC) has been successfully used\nin speech recognition. It learns the alignments between speech frames\nand label sequences automatically without explicit pre-generated frame-level\nlabels. While this property is convenient for shortening the training\npipeline, it may become a potential disadvantage for the frame-level\nsystem combination due to inaccurate alignments. In this paper, a novel\nDynamic Time Warping (DTW) based position calibration algorithm is\nproposed for joint decoding on two CTC based acoustic models. Furthermore,\njoint decoding for CTC and conventional hybrid NN-HMM models is also\nexplored. Experiments on a large vocabulary Mandarin speech recognition\ntask show that the proposed joint decoding of both CTC based and CTC-Hybrid\nbased systems can achieve a significant and consistent character error\nrate reduction.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2026",
      "author_area_id": 9,
      "author_area_label": "Speech Recognition: Architecture, Search, and Linguistic Components"
    },
    "tanaka19_interspeech": {
      "authors": [
        [
          "Tomohiro",
          "Tanaka"
        ],
        [
          "Ryo",
          "Masumura"
        ],
        [
          "Takafumi",
          "Moriya"
        ],
        [
          "Takanobu",
          "Oba"
        ],
        [
          "Yushi",
          "Aono"
        ]
      ],
      "title": "A Joint End-to-End and DNN-HMM Hybrid Automatic Speech Recognition System with Transferring Sharable Knowledge",
      "original": "2263",
      "page_count": 5,
      "order": 455,
      "p1": "2210",
      "pn": "2214",
      "abstract": [
        "This paper presents joint end-to-end and deep neural network-hidden\nMarkov model (DNN-HMM) hybrid automatic speech recognition (ASR) systems\nthat share network components. End-to-end ASR systems have been shown\ncompetitive performance compared with the DNN-HMM hybrid ASR systems\nin recent studies. These systems have different advantages, which are\nan estimation ability based on the totally optimized model of the end-to-end\nASR system and a stable processing based on a frame-by-frame manner\nof the DNN-HMM hybrid ASR system. In our previous study, we proposed\na method to utilize an end-to-end ASR system for rescoring hypotheses\ngenerated from a DNN-HMM hybrid ASR system. However, the conventional\nmethod cannot efficiently leverage the advantages since network components\nare independently modeled. In order to tackle this problem, we propose\na joint end-to-end and DNN-HMM hybrid ASR systems that share the network\nto transfer knowledge of the systems. In the proposed method, end-to-end\nASR systems utilize the information from an output of an internal layer\nin a DNN acoustic model in the DNN-HMM hybrid ASR system for enhancing\nthe end-to-end ASR system. This enables us to efficiently leverage\nsharable information for improving the joint ASR system. Experimental\nresults show that the proposed method outperforms the conventional\nmethod.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2263"
    },
    "malhotra19_interspeech": {
      "authors": [
        [
          "Karan",
          "Malhotra"
        ],
        [
          "Shubham",
          "Bansal"
        ],
        [
          "Sriram",
          "Ganapathy"
        ]
      ],
      "title": "Active Learning Methods for Low Resource End-to-End Speech Recognition",
      "original": "2316",
      "page_count": 5,
      "order": 456,
      "p1": "2215",
      "pn": "2219",
      "abstract": [
        "Recently developed end-to-end (E2E) automatic speech recognition (ASR)\nsystems demand abundance of transcribed speech data, there are several\nscenarios where the labeling of speech data is cumbersome and expensive.\nFor a fixed annotation cost, active learning for speech recognition\nallows to efficiently train the ASR model. In this work, we advance\nthe most common approach for active learning methods which relies on\nuncertainty sampling technique. In particular, we explore the use of\npath probability of the decoded sequence as a confidence measure and\nselect the samples with the least confidence for active learning. In\norder to reduce the sampling bias in active learning, we propose a\nregularized uncertainty sampling approach that incorporates an i-vector\ndiversity measure. Thus, the active learning in the proposed framework\nuses a joint score of uncertainty and i-vector diversity. The benefits\nof the proposed approach are illustrated for an E2E ASR task performed\non CSJ and Librispeech datasets. In these experiments, we show that\nthe proposed approach yields considerable improvements over the baseline\nmodel using random sampling.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2316"
    },
    "karafiat19_interspeech": {
      "authors": [
        [
          "Martin",
          "Karafi\u00e1t"
        ],
        [
          "Murali Karthick",
          "Baskar"
        ],
        [
          "Shinji",
          "Watanabe"
        ],
        [
          "Takaaki",
          "Hori"
        ],
        [
          "Matthew",
          "Wiesner"
        ],
        [
          "Jan",
          "\u010cernock\u00fd"
        ]
      ],
      "title": "Analysis of Multilingual Sequence-to-Sequence Speech Recognition Systems",
      "original": "2355",
      "page_count": 5,
      "order": 457,
      "p1": "2220",
      "pn": "2224",
      "abstract": [
        "This paper investigates the applications of various multilingual approaches\ndeveloped in conventional deep neural network - hidden Markov model\n(DNN-HMM) systems to sequence-to-sequence (seq2seq) automatic speech\nrecognition (ASR). We employ a joint connectionist temporal classification-attention\nnetwork as our base model. Our main contribution is separated into\ntwo parts. First, we investigate the effectiveness of the seq2seq model\nwith stacked multilingual bottle-neck features obtained from a conventional\nDNN-HMM system on the Babel multilingual speech corpus. Second, we\ninvestigate the effectiveness of transfer learning from a pre-trained\nmultilingual seq2seq model with and without the target language included\nin the original multilingual training data. In this experiment, we\nalso explore various architectures and training strategies of the multilingual\nseq2seq model by making use of knowledge obtained in the DNN-HMM based\ntransfer-learning. Although both approaches significantly improved\nthe performance from a monolingual seq2seq baseline, interestingly,\nwe found the multilingual bottle-neck features to be superior to multilingual\nmodels with transfer learning. This finding suggests that we can efficiently\ncombine the benefits of the DNN-HMM system with the seq2seq system\nthrough multilingual bottle-neck feature techniques.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2355",
      "author_area_id": 9,
      "author_area_label": "Speech Recognition: Architecture, Search, and Linguistic Components"
    },
    "zapotoczny19_interspeech": {
      "authors": [
        [
          "Micha\u0142",
          "Zapotoczny"
        ],
        [
          "Piotr",
          "Pietrzak"
        ],
        [
          "Adrian",
          "\u0141a\u0144cucki"
        ],
        [
          "Jan",
          "Chorowski"
        ]
      ],
      "title": "Lattice Generation in Attention-Based Speech Recognition Models",
      "original": "2667",
      "page_count": 5,
      "order": 458,
      "p1": "2225",
      "pn": "2229",
      "abstract": [
        "Attention-based neural speech recognition models are frequently decoded\nwith beam search, which produces a tree of hypotheses. In many cases,\nsuch as when using external language models, numerous decoding hypotheses\nneed to be considered, requiring large beam sizes during decoding.\nWe demonstrate that it is possible to merge certain nodes in a tree\nof hypotheses, in order to obtain a decoding lattice, which increases\nthe number of decoding hypotheses without increasing the number of\ncandidates that are scored by the neural network. We propose a convolutional\narchitecture, which facilitates comparing states of the model at different\npi The experiments are carried on the Wall Street Journal dataset,\nwhere the lattice decoder obtains lower word error rates with smaller\nbeam sizes, than an otherwise similar architecture with regular beam\nsearch.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2667",
      "author_area_id": 9,
      "author_area_label": "Speech Recognition: Architecture, Search, and Linguistic Components"
    },
    "jansche19_interspeech": {
      "authors": [
        [
          "Martin",
          "Jansche"
        ],
        [
          "Alexander",
          "Gutkin"
        ]
      ],
      "title": "Sampling from Stochastic Finite Automata with Applications to CTC Decoding",
      "original": "2740",
      "page_count": 5,
      "order": 459,
      "p1": "2230",
      "pn": "2234",
      "abstract": [
        "Stochastic finite automata arise naturally in many language and speech\nprocessing tasks. They include stochastic acceptors, which represent\ncertain probability distributions over random strings. We consider\nthe problem of efficient sampling: drawing random string variates from\nthe probability distribution represented by stochastic automata and\ntransformations of those. We show that path-sampling is effective and\ncan be efficient if the epsilon-graph of a finite automaton is acyclic.\nWe provide an algorithm that ensures this by conflating epsilon-cycles\nwithin strongly connected components. Sampling is also effective in\nthe presence of non-injective transformations of strings. We illustrate\nthis in the context of decoding for Connectionist Temporal Classification\n(CTC), where the predictive probabilities yield auxiliary sequences\nwhich are transformed into shorter labeling strings. We can sample\nefficiently from the transformed labeling distribution and use this\nin two different strategies for finding the most probable CTC labeling.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2740",
      "author_area_id": 9,
      "author_area_label": "Speech Recognition: Architecture, Search, and Linguistic Components"
    },
    "dudziak19_interspeech": {
      "authors": [
        [
          "\u0141ukasz",
          "Dudziak"
        ],
        [
          "Mohamed S.",
          "Abdelfattah"
        ],
        [
          "Ravichander",
          "Vipperla"
        ],
        [
          "Stefanos",
          "Laskaridis"
        ],
        [
          "Nicholas D.",
          "Lane"
        ]
      ],
      "title": "ShrinkML: End-to-End ASR Model Compression Using Reinforcement Learning",
      "original": "2811",
      "page_count": 5,
      "order": 460,
      "p1": "2235",
      "pn": "2239",
      "abstract": [
        "End-to-end automatic speech recognition (ASR) models are increasingly\nlarge and complex to achieve the best possible accuracy. In this paper,\nwe build an AutoML system that uses reinforcement learning (RL) to\noptimize the per-layer compression ratios when applied to a state-of-the-art\nattention based end-to-end ASR model composed of several LSTM layers.\nWe use singular value decomposition (SVD) low-rank matrix factorization\nas the compression method. For our RL-based AutoML system, we focus\non practical considerations such as the choice of the reward/punishment\nfunctions, the formation of an effective search space, and the creation\nof a representative but small data set for quick evaluation between\nsearch steps. Finally, we present accuracy results on LibriSpeech of\nthe model compressed by our AutoML system, and we compare it to manually-compressed\nmodels. Our results show that in the absence of retraining our RL-based\nsearch is an effective and practical method to compress a production-grade\nASR system. When retraining is possible, we show that our AutoML system\ncan select better highly-compressed seed models compared to manually\nhand-crafted rank selection, thus allowing for more compression than\npreviously possible.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2811",
      "author_area_id": 9,
      "author_area_label": "Speech Recognition: Architecture, Search, and Linguistic Components"
    },
    "gaur19_interspeech": {
      "authors": [
        [
          "Yashesh",
          "Gaur"
        ],
        [
          "Jinyu",
          "Li"
        ],
        [
          "Zhong",
          "Meng"
        ],
        [
          "Yifan",
          "Gong"
        ]
      ],
      "title": "Acoustic-to-Phrase Models for Speech Recognition",
      "original": "3056",
      "page_count": 5,
      "order": 461,
      "p1": "2240",
      "pn": "2244",
      "abstract": [
        "Directly emitting words and sub-words from speech spectrogram has been\nshown to produce good results using end-to-end (E2E) trained models.\nConnectionist Temporal Classification (CTC) and Sequence-to-Sequence\nattention (Seq2Seq) models have both shown better success when directly\ntargeting words or sub-words. In this work, we ask the question: Can\nan E2E model go beyond words and transcribe directly to phrases (i.e.,\na group of words)? Directly modeling frequent phrases might be better\nthan modeling its constituent words. Also, emitting multiple words\ntogether might speed up inference in models like Seq2Seq where decoding\nis inherently sequential. To answer this, we undertake a study on a\n3400-hour Microsoft Cortana voice assistant task. We present a side-by-side\ncomparison for CTC and Seq2Seq models that have been trained to target\na variety of tokens including letters, sub-words, words and phrases.\nWe show that an E2E model can indeed transcribe directly to phrases.\nWe see that while CTC has difficulty in accurately modeling phrases,\na more powerful model like Seq2Seq can effortlessly target phrases\nthat are up to 4 words long, with only a reasonable degradation in\nthe final word error rate.\n"
      ],
      "doi": "10.21437/Interspeech.2019-3056",
      "author_area_id": 9,
      "author_area_label": "Speech Recognition: Architecture, Search, and Linguistic Components"
    },
    "li19l_interspeech": {
      "authors": [
        [
          "Ruizhi",
          "Li"
        ],
        [
          "Gregory",
          "Sell"
        ],
        [
          "Hynek",
          "Hermansky"
        ]
      ],
      "title": "Performance Monitoring for End-to-End Speech Recognition",
      "original": "3137",
      "page_count": 5,
      "order": 462,
      "p1": "2245",
      "pn": "2249",
      "abstract": [
        "Measuring performance of an automatic speech recognition (ASR) system\nwithout ground-truth could be beneficial in many scenarios, especially\nwith data from unseen domains, where performance can be highly inconsistent.\nIn conventional ASR systems, several performance monitoring (PM) techniques\nhave been well-developed to monitor performance by looking at tri-phone\nposteriors or pre-softmax activations from neural network acoustic\nmodeling. However, strategies for monitoring more recently developed\nend-to-end ASR systems have not yet been explored, and so that is the\nfocus of this paper. We adapt previous PM measures (Entropy, M-measure\nand Autoencoder) and apply our proposed RNN predictor in the end-to-end\nsetting. These measures utilize the decoder output layer and attention\nprobability vectors, and their predictive power is measured with simple\nlinear models. Our findings suggest that decoder-level features are\nmore feasible and informative than attention-level probabilities for\nPM measures, and that M-measure on the decoder posteriors achieves\nthe best overall predictive performance with an average prediction\nerror 8.8%. Entropy measures and RNN-based prediction also show competitive\npredictability, especially for unseen conditions.\n"
      ],
      "doi": "10.21437/Interspeech.2019-3137",
      "author_area_id": 9,
      "author_area_label": "Speech Recognition: Architecture, Search, and Linguistic Components"
    },
    "cohn19b_interspeech": {
      "authors": [
        [
          "Michelle",
          "Cohn"
        ],
        [
          "Georgia",
          "Zellou"
        ],
        [
          "Santiago",
          "Barreda"
        ]
      ],
      "title": "The Role of Musical Experience in the Perceptual Weighting of Acoustic Cues for the Obstruent Coda Voicing Contrast in American English",
      "original": "3103",
      "page_count": 5,
      "order": 463,
      "p1": "2250",
      "pn": "2254",
      "abstract": [
        "This study examines the role of musical experience on listeners&#8217;\nphoneme judgments across noise conditions. Individuals with 10+ years\nof musical training were compared with nonmusicians in their use of\nthree acoustic cues in categorizing post-vocalic obstruent voicing:\nfundamental frequency, vowel duration, and spectral composition in\ntwo listening conditions (silence and multitalker babble, MTB). Results\ndemonstrate that musicians display steeper phonemic categorization\nfor coda /t/ and /d/ on the basis of all three cues of interest. Additionally,\nmusicians and nonmusicians show different cue weighting patterns in\nMTB than in silence. The findings are discussed with reference to their\nimplications for theories of experience-driven plasticity and individual\ndifferences in the perceptual organization of phonemes.\n"
      ],
      "doi": "10.21437/Interspeech.2019-3103",
      "author_area_id": 1,
      "author_area_label": "Speech Perception, Production and Acquisition"
    },
    "lewandowski19_interspeech": {
      "authors": [
        [
          "Natalie",
          "Lewandowski"
        ],
        [
          "Daniel",
          "Duran"
        ]
      ],
      "title": "Individual Differences in Implicit Attention to Phonetic Detail in Speech Perception",
      "original": "2989",
      "page_count": 5,
      "order": 464,
      "p1": "2255",
      "pn": "2259",
      "abstract": [
        "We present a study on the interactions between implicit attention to\nacoustic-phonetic detail in speech and individual differences (IDs).\nAttention to phonetic detail was assessed with acoustically manipulated\nspeech stimuli within a computer game, an alternative to regular highly-controlled\ncategorization tests. Twenty-two native German speakers (11f) completed\nthe game and further tests including individual attention test measures\n(e.g. Simon Test), the BFI-10 (short version of the Big Five Inventory),\nand a Self-monitoring Test (need for social approval). With this study,\nwe contribute to the understanding of the processes underlying human\nspeech perception and the impact of cognitive and personality features\non the attention to phonetic detail. Our results show that the general\n(non-verbal) attention capacity (mental flexibility, inhibition), interacts\nwith implicit attention to phonetic detail. Furthermore, IDs in personality,\nsuch as  sensitivity to social cues or  conscientiousness significantly\nadd to the effects. Understanding these interactions, especially arising\nin an intuitive and non-explicit study design, is an important step\non the way towards explaining not only the influence of IDs on attention\nto phonetic detail, but also the dynamics of speech interaction (e.g.\nphonetic convergence).\n"
      ],
      "doi": "10.21437/Interspeech.2019-2989",
      "author_area_id": 1,
      "author_area_label": "Speech Perception, Production and Acquisition"
    },
    "lalonde19_interspeech": {
      "authors": [
        [
          "Kaylah",
          "Lalonde"
        ]
      ],
      "title": "Effects of Natural Variability in Cross-Modal Temporal Correlations on Audiovisual Speech Recognition Benefit",
      "original": "2931",
      "page_count": 5,
      "order": 465,
      "p1": "2260",
      "pn": "2264",
      "abstract": [
        "In audiovisual (AV) speech, correlations over time between visible\nmouth movements and the amplitude envelope of auditory speech help\nto reduce uncertainty as to when peaks in the auditory signal will\noccur. Previous studies demonstrated greater AV benefit to speech detection\nin noise for sentences with higher cross-modal correlations than sentences\nwith lower cross-modal correlations.<br style=\"mso-data-placement:same-cell;\"\n/><br style=\"mso-data-placement:same-cell;\" />\n  This study examined\nwhether the mechanisms that underlie AV detection benefits have downstream\neffects on speech recognition in noise. Participants were presented\n72 sentences in noise, in auditory-only and AV conditions, at either\ntheir 50% auditory speech recognition threshold in noise (SRT-50) or\nat a signal-to-noise ratio (SNR) 6 dB poorer than their SRT-50. They\nwere asked to repeat each sentence. Mean AV benefit across subjects\nwas calculated for each sentence. Pearson correlations and mixed modeling\nwere used to examined whether variability in AV benefit across sentences\nwas related to natural variation in the degree of cross-modal correlation\nacross sentences.<br style=\"mso-data-placement:same-cell;\" /><br style=\"mso-data-placement:same-cell;\"\n/>\n  In the more difficult listening condition, higher cross-modal\ncorrelations were associated with higher AV sentence recognition benefit.\nThe relationship was strongest in the 0.8&#8211;2.2 kHz and 0.8&#8211;6\nkHz frequency regions. These results demonstrate that cross-modal correlations\ncontribute to variability in AV speech recognition in noise.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2931",
      "author_area_id": 1,
      "author_area_label": "Speech Perception, Production and Acquisition"
    },
    "bentum19_interspeech": {
      "authors": [
        [
          "M.",
          "Bentum"
        ],
        [
          "L. ten",
          "Bosch"
        ],
        [
          "A. van den",
          "Bosch"
        ],
        [
          "Mirjam",
          "Ernestus"
        ]
      ],
      "title": "Listening with Great Expectations: An Investigation of Word Form Anticipations in Naturalistic Speech",
      "original": "2741",
      "page_count": 5,
      "order": 466,
      "p1": "2265",
      "pn": "2269",
      "abstract": [
        "The event-related potential (ERP) component named  phonological mismatch\nnegativity (PMN) arises when listeners hear an unexpected word form\nin a spoken sentence [1]. The PMN is thought to reflect the mismatch\nbetween expected and perceived auditory speech input. In this paper,\nwe use the PMN to test a central premise in the predictive coding framework\n[2], namely that the mismatch between prior expectations and sensory\ninput is an important mechanism of perception. We test this with natural\nspeech materials containing approximately 50,000 word tokens. The corresponding\nEEG-signal was recorded while participants (n = 48) listened to these\nmaterials. Following [3], we quantify the mismatch with two word probability\ndistributions (WPD): a WPD based on preceding context, and a WPD that\nis additionally updated based on the incoming audio of the current\nword. We use the between-WPD cross entropy for each word in the utterances\nand show that a higher cross entropy correlates with a more negative\nPMN. Our results show that listeners anticipate auditory input while\nprocessing each word in naturalistic speech. Moreover, complementing\nprevious research, we show that predictive language processing occurs\nacross the whole probability spectrum.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2741",
      "author_area_id": 1,
      "author_area_label": "Speech Perception, Production and Acquisition"
    },
    "bentum19b_interspeech": {
      "authors": [
        [
          "M.",
          "Bentum"
        ],
        [
          "L. ten",
          "Bosch"
        ],
        [
          "A. van den",
          "Bosch"
        ],
        [
          "Mirjam",
          "Ernestus"
        ]
      ],
      "title": "Quantifying Expectation Modulation in Human Speech Processing",
      "original": "2685",
      "page_count": 5,
      "order": 467,
      "p1": "2270",
      "pn": "2274",
      "abstract": [
        "The mismatch between top-down predicted and bottom-up perceptual input\nis an important mechanism of perception according to the predictive\ncoding framework (Friston, [1]). In this paper we develop and validate\na new information-theoretic measure that quantifies the mismatch between\nexpected and observed auditory input during speech processing. We argue\nthat such a mismatch measure is useful for the study of speech processing.\nTo compute the mismatch measure, we use naturalistic speech materials\ncontaining approximately 50,000 word tokens. For each word token we\nfirst estimate the prior word probability distribution with the aid\nof statistical language modelling, and next use automatic speech recognition\nto update this word probability distribution based on the unfolding\nspeech signal. We validate the mismatch measure with multiple analyses,\nand show that the auditory-based update improves the probability of\nthe correct word and lowers the uncertainty of the word probability\ndistribution. Based on these results, we argue that it is possible\nto explicitly estimate the mismatch between predicted and perceived\nspeech input with the cross entropy between word expectations computed\nbefore and after an auditory update.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2685",
      "author_area_id": 1,
      "author_area_label": "Speech Perception, Production and Acquisition"
    },
    "turner19_interspeech": {
      "authors": [
        [
          "Daniel R.",
          "Turner"
        ],
        [
          "Ann R.",
          "Bradlow"
        ],
        [
          "Jennifer S.",
          "Cole"
        ]
      ],
      "title": "Perception of Pitch Contours in Speech and Nonspeech",
      "original": "2619",
      "page_count": 5,
      "order": 468,
      "p1": "2275",
      "pn": "2279",
      "abstract": [
        "The pitch perception literature has been largely built on experimental\ndata collected using nonspeech stimuli, which has then been generalized\nto speech. In the present study, we compare the perceptibility of identical\npitch movements in speech and nonspeech that vary in duration and in\npitch range. Our nonspeech results closely replicate earlier findings\nand we show that speech is a significantly more difficult medium for\npitch discrimination. Pitch movements in speech have to be larger and\nlonger to achieve the salience of the most common speech analog, pulse\ntrains. The direction of pitch movement also affects one&#8217;s ability\nto discern pitch; in particular falling excursions are the most difficult.\nWe found that the perceptual threshold for falling pitch in speech\nwas more than 100 times that of previous estimates with nonspeech stimuli.\nOur findings show that the perceptual response to nonspeech does not\nadequately map onto speech, and future work in speech research and\nits applications should use speech-like stimuli, rather than convenient\nsubstitutes like pulse trains, pure tones, or isolated vowels.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2619"
    },
    "bosch19b_interspeech": {
      "authors": [
        [
          "L. ten",
          "Bosch"
        ],
        [
          "L.",
          "Boves"
        ],
        [
          "K.",
          "Mulder"
        ]
      ],
      "title": "Analyzing Reaction Time and Error Sequences in Lexical Decision Experiments",
      "original": "2611",
      "page_count": 5,
      "order": 469,
      "p1": "2280",
      "pn": "2284",
      "abstract": [
        "Reaction times (RTs) are used widely in psychological and psycholinguistic\nresearch as inexpensive measures of underlying cognitive processes.\nHowever, inferring cognitive processes from RTs is hampered by the\nfact that actual responses are the result of multiple factors, many\nof which may not be related to the process of interest. In lexical\ndecision experiments, the use of RTs is further complicated by the\nfact that the response to some stimuli is missing, and the fact that\npart of the responses are &#8216;incorrect&#8217;.<br style=\"mso-data-placement:same-cell;\"\n/><br style=\"mso-data-placement:same-cell;\" />\n  In this paper we investigate\nthe distribution of missing and incorrect responses in the RT sequences\nof two large lexical decision experiments. It appears that a substantial\npart of incorrect responses cluster together. Then, we investigate\nthe effect of clusters of incorrect responses on surrounding RTs.<br\nstyle=\"mso-data-placement:same-cell;\" /><br style=\"mso-data-placement:same-cell;\"\n/>\n  Also, we extend previous research on methods for discovering and\nremoving so-called local speed effects from RT sequences. For this\npurpose, we show that a recently introduced graph-based RT analysis\nmethod can help to better understand and analyze RT sequences.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2611",
      "author_area_id": 1,
      "author_area_label": "Speech Perception, Production and Acquisition"
    },
    "liu19e_interspeech": {
      "authors": [
        [
          "Li",
          "Liu"
        ],
        [
          "Jianze",
          "Li"
        ],
        [
          "Gang",
          "Feng"
        ],
        [
          "Xiao-Ping",
          "Zhang"
        ]
      ],
      "title": "Automatic Detection of the Temporal Segmentation of Hand Movements in British English Cued Speech",
      "original": "2353",
      "page_count": 5,
      "order": 470,
      "p1": "2285",
      "pn": "2289",
      "abstract": [
        "Cued Speech (CS) is a multi-modal system, which complements the lip\nreading with manual hand cues in the phonetic level to make the spoken\nlanguage visible. It has been found that lip and hand movements are\nasynchronous in CS, and thus the study of hand temporal organization\nis very important for the multi-modal CS feature fusion. In this work,\nwe propose a novel diphthong-hand preceding model (D-HPM) by investigating\nthe relationship between hand preceding time (HPT) and diphthong time\ninstants in sentences for British English CS. Besides, we demonstrate\nthat HPT of the first and second parts of diphthongs has a very strong\ncorrelation. Combining the monophthong-HPM (M-HPM) and D-HPM, we present\na hybrid temporal segmentation detection algorithm (HTSDA) for the\nhand movement in CS. The evaluation of the proposed algorithm is carried\nout by a hand position recognition experiment using the multi-Gaussian\nclassifier as well as the long-short term memory (LSTM). The results\nshow that the HTSDA significantly improves the recognition performance\ncompared with the baseline (i.e., audio-based segmentation) and the\nstate-of-the-art M-HPM. To the best of our knowledge, this is the first\nwork to study the temporal organization of hand movements in British\nEnglish CS.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2353",
      "author_area_id": 1,
      "author_area_label": "Speech Perception, Production and Acquisition"
    },
    "yokoe19_interspeech": {
      "authors": [
        [
          "Yuriko",
          "Yokoe"
        ]
      ],
      "title": "Place Shift as an Autonomous Process: Evidence from Japanese Listeners",
      "original": "2302",
      "page_count": 5,
      "order": 471,
      "p1": "2290",
      "pn": "2294",
      "abstract": [
        "A perception experiment with Japanese listeners is conducted to investigate\nthe nature of place shift phenomenon that was previously found with\nFrench and English listeners. Hall&#233; et al. [1] showed that unattested\nconsonant sequences /tl, dl/ are perceptually repaired to form grammatically\nacceptable consonant clusters /kl, gl/ in the listeners&#8217; native\nlanguage.<br style=\"mso-data-placement:same-cell;\" /><br style=\"mso-data-placement:same-cell;\"\n/>\n  In this study, a similar experiment with Japanese listeners, whose\nmother tongue lacks the onset clusters altogether, is conducted. The\nresult explicitly shows that the place shift phenomenon ought not to\nbe interpreted in relation to the top-down phonotactic feedback. Rather,\nI will argue that both labial and velar shift reflect an autonomous,\nsignal-driven process. As such, language specificity in speech perception\nmust reside in the listeners&#8217; cue weighting, rather than encoded\nlinguistic knowledge.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2302",
      "author_area_id": 1,
      "author_area_label": "Speech Perception, Production and Acquisition"
    },
    "meyer19_interspeech": {
      "authors": [
        [
          "Julien",
          "Meyer"
        ],
        [
          "Laure",
          "Dentel"
        ],
        [
          "Silvain",
          "Gerber"
        ],
        [
          "Rachid",
          "Ridouane"
        ]
      ],
      "title": "A Perceptual Study of CV Syllables in Both Spoken and Whistled Speech: A Tashlhiyt Berber Perspective",
      "original": "2251",
      "page_count": 5,
      "order": 472,
      "p1": "2295",
      "pn": "2299",
      "abstract": [
        "The present study compares the perceptual categorization of four CV\nsyllables /ta, da, ka, ga/ in two different speech registers &#8212;\nmodal speech and whistled speech &#8212; of Tashlhiyt Berber used in\nthe Moroccan High Atlas. Whistled speech in a non-tonal language such\nas Tashlhiyt is a special speech register used for long distance dialogues\nthat consists of the natural production of vocalic and consonantal\nqualities in a simple modulated whistled signal. The technique of whistling\nimposes various restrictions on speech articulation, which result in\na simplification of the phonetics of spoken speech into a &#8216;whistled\nformant&#8217;. Here, we describe this simplification for Tashlhiyt\nsyllables /ta, da, ka, ga/ and use them as stimuli in a behavioral\nexperiment. We analyze and compare the perceptual categorization obtained\nfrom native Tashlhiyt listeners (trained since childhood in whistled\nspeech) for both speech registers on these 4 syllable types. Results\nshow that whistled stimuli were fairly well identified (&#126;42%)\nabove chance (25%), though less well than spoken ones (&#126;84%).\nThe detailed analysis of confusions between CVs enabled us to understand\nbetter how whistled consonants are perceived, highlighting the phonological\ncontrasts that are best perceived and retained from spoken to whistled\nspeech in this language.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2251",
      "author_area_id": 1,
      "author_area_label": "Speech Perception, Production and Acquisition"
    },
    "hsieh19_interspeech": {
      "authors": [
        [
          "Han-Chi",
          "Hsieh"
        ],
        [
          "Wei-Zhong",
          "Zheng"
        ],
        [
          "Ko-Chiang",
          "Chen"
        ],
        [
          "Ying-Hui",
          "Lai"
        ]
      ],
      "title": "Consonant Classification in Mandarin Based on the Depth Image Feature: A Pilot Study",
      "original": "1893",
      "page_count": 5,
      "order": 473,
      "p1": "2300",
      "pn": "2304",
      "abstract": [
        "The consonant is an important element in Mandarin, and various categories\nof consonant generation effectuate various facial expressions. Specifically,\nthere are changes in facial muscles when speaking, and these changes\nare closely related to pronunciation; the facial muscles are associated\nwith these hidden articulators, and the effects on the facial changes\ncan be seen as 3D changes. However, in most studies, 2D images are\nused to analyze facial features when people talk. The 2D images serve\nto provide information in two dimensions (x- and y-axis); however,\nsubtle deep motions (z-axis changes) of facial muscles when speaking\ncan be difficult to detect accurately. Hence, the depth feature of\nthe face (the point cloud feature in this study) was used to investigate\nthe potential for consonant recognition, recorded by a time-of-flight\n3D camera. In this study, we propose an algorithm to recognize the\nseven categories of Mandarin consonants using the depth features of\nthe speaker&#8217;s face. The proposed system yielded suitable classification\naccuracy for the recognition of seven categories of Mandarin consonants.\nThis result implies that depth features can be used for speech-processing\napplications.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1893",
      "author_area_id": 1,
      "author_area_label": "Speech Perception, Production and Acquisition"
    },
    "levari19_interspeech": {
      "authors": [
        [
          "Shiri",
          "Lev-Ari"
        ],
        [
          "Robin",
          "Dodsworth"
        ],
        [
          "Jeff",
          "Mielke"
        ],
        [
          "Sharon",
          "Peperkamp"
        ]
      ],
      "title": "The Different Roles of Expectations in Phonetic and Lexical Processing",
      "original": "1795",
      "page_count": 5,
      "order": 474,
      "p1": "2305",
      "pn": "2309",
      "abstract": [
        "The way people speak reflects their demographic background. Listeners\nexploit this contingent variation and make use of information about\nspeakers&#8217; background to process their speech. Evidence for this\ncomes from both phonetic and lexical tasks, and the two are assumed\nto tap into the same mechanism and provide equivalent results. Curiously,\nthis assumption has never been tested. Additionally, while it has been\nestablished that expectations can influence language processing in\ngeneral, the role of individual differences in susceptibility to this\ninfluence is relatively unexplored. We investigate these two questions\nin the context of Southern and General American speech varieties in\nthe USA. We show that phonetic and lexical tasks are not equivalent,\nand furthermore, that the two are driven by mechanisms that are sensitive\nto different individual variables: while performance at the lexical\nlevel is influenced by implicit bias, performance at the phonetic level\nis influenced by working memory. These results thus change our understanding\nof how expectations influence processing, and have implications for\nhow to conduct and interpret studies on the topic.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1795",
      "author_area_id": 1,
      "author_area_label": "Speech Perception, Production and Acquisition"
    },
    "segedin19_interspeech": {
      "authors": [
        [
          "Bruno Ferenc",
          "Segedin"
        ],
        [
          "Michelle",
          "Cohn"
        ],
        [
          "Georgia",
          "Zellou"
        ]
      ],
      "title": "Perceptual Adaptation to Device and Human Voices: Learning and Generalization of a Phonetic Shift Across Real and Voice-AI Talkers",
      "original": "1433",
      "page_count": 5,
      "order": 475,
      "p1": "2310",
      "pn": "2314",
      "abstract": [
        "Voice-activated artificially-intelligent digital devices are a new\ntype of interlocutor. Like for human talkers, they have idiosyncratic\nspeech patterns that require listeners to perceptually adapt to during\nlanguage comprehension. One question is how perceptual adaptation to\na novel accent in speech produced by a digital device voice compares\nto adaptation to human voices. Furthermore, adaptation to one talker\ncan  generalize to novel voices. Hence, we also tested whether perceptual\nadaptation to accented device voices generalizes to novel human voices,\nand vice versa. In this study, listeners were first exposed to words\nwith a shifted phoneme realization in either a device or human voice.\nLater, participants were tested on whether they shifted their identification\nof words in the shifted talker. Additionally, we tested whether listeners\napplied the shift to novel device and human voices not heard in exposure.\nResults reveal talker-specific learning for both device and human voices.\nYet, the size of the shift was larger for the device voices. Furthermore,\nlisteners exposed to the shift in device voices showed generalization\nto novel human voices, and vice versa. These patterns of adaptation\nand generalization for device and human talkers have implications for\nmodels of speech perception models and human-computer interaction.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1433",
      "author_area_id": 1,
      "author_area_label": "Speech Perception, Production and Acquisition"
    },
    "papadimitriou19_interspeech": {
      "authors": [
        [
          "Katerina",
          "Papadimitriou"
        ],
        [
          "Gerasimos",
          "Potamianos"
        ]
      ],
      "title": "End-to-End Convolutional Sequence Learning for ASL Fingerspelling Recognition",
      "original": "2422",
      "page_count": 5,
      "order": 476,
      "p1": "2315",
      "pn": "2319",
      "abstract": [
        "Although fingerspelling is an often overlooked component of sign languages,\nit has great practical value in the communication of important context\nwords that lack dedicated signs. In this paper we consider the problem\nof fingerspelling recognition in videos, introducing an end-to-end\nlexicon-free model that consists of a deep auto-encoder image feature\nlearner followed by an attention-based encoder-decoder for prediction.\nThe feature extractor is a vanilla auto-encoder variant, employing\na quadratic activation function. The learned features are subsequently\nfed into the attention-based encoder-decoder. The latter deviates from\ntraditional recurrent neural network architectures, being a fully convolutional\nattention-based encoder-decoder that is equipped with a multi-step\nattention mechanism relying on a quadratic alignment function and gated\nlinear units over the convolution output. The introduced model is evaluated\non the TTIC/UChicago fingerspelling video dataset, where it outperforms\nprevious approaches in letter accuracy under all three, signer-dependent,\n-adapted, and -independent, experimental paradigms.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2422",
      "author_area_id": 1,
      "author_area_label": "Speech Perception, Production and Acquisition"
    },
    "somandepalli19_interspeech": {
      "authors": [
        [
          "Krishna",
          "Somandepalli"
        ],
        [
          "Naveen",
          "Kumar"
        ],
        [
          "Arindam",
          "Jati"
        ],
        [
          "Panayiotis",
          "Georgiou"
        ],
        [
          "Shrikanth",
          "Narayanan"
        ]
      ],
      "title": "Multiview Shared Subspace Learning Across Speakers and Speech Commands",
      "original": "3130",
      "page_count": 5,
      "order": 477,
      "p1": "2320",
      "pn": "2324",
      "abstract": [
        "In many speech processing applications, the objective is to model different\nmodes of variability to obtain robust speech features. In this paper,\nwe learn speech representations in a multiview paradigm by constraining\nthe views to known modes of variability such as speakers or spoken\nwords. We use deep multiset canonical correlation (dMCCA) because it\ncan model more than two views in parallel to learn a shared subspace\nacross them. In order to model thousands of views (e.g., speakers),\nwe demonstrate that stochastically sampling a small number of views\ngeneralizes dMCCA to the larger set of views. To evaluate our approach,\nwe study two different aspects of the Speech Commands Dataset: variability\namong the speakers and speech commands. We show that, by treating observations\nfrom one mode of variability as multiple parallel views, we can learn\nrepresentations that are discriminative to the other mode. We first\nconsider different speakers as views of the same word to learn their\nshared subspace to represent an utterance. We then constrain the different\nwords spoken by the same person as multiple views to learn speaker\nrepresentations. Using classification and unsupervised clustering,\nwe evaluate the efficacy of multiview representations to identify speech\ncommands and speakers.\n"
      ],
      "doi": "10.21437/Interspeech.2019-3130",
      "author_area_id": 5,
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "belitz19_interspeech": {
      "authors": [
        [
          "Chelzy",
          "Belitz"
        ],
        [
          "Hussnain",
          "Ali"
        ],
        [
          "John H.L.",
          "Hansen"
        ]
      ],
      "title": "A Machine Learning Based Clustering Protocol for Determining Hearing Aid Initial Configurations from Pure-Tone Audiograms",
      "original": "3091",
      "page_count": 5,
      "order": 478,
      "p1": "2325",
      "pn": "2329",
      "abstract": [
        "Of the nearly 35 million people in the USA who are hearing impaired,\nonly an estimated 25% use hearing aids (HA). A good number of HAs are\nprescribed but not used partially because of the time to convergence\nfor best operation between the audiologist and user. To improve HA\nretention, it is suggested that a machine learning (ML) protocol could\nbe established which improves initial HA configurations given a user&#8217;s\npure-tone audiogram. This study examines a ML clustering method to\npredict the best initial HA fitting from a corpus of over 90,000 audiogram-fitting\npairs collected from hearing centers throughout the USA. We first examine\nthe final HA comfort targets to determine a limited number of preset\nconfigurations using several multi-dimensional clustering methods (Birch,\nWard, and k-means). The goal is to reduce the amount of adjustments\nbetween the centroid, selected as a fitting configuration to represent\nthe cluster, and the final HA configurations. This may be used to reduce\nthe adjustment cycles for HAs or as preset starting configurations\nfor personal sound amplification products (PSAPs). Using various classification\nmethods, audiograms are mapped to a limited number of potential preset\nconfigurations. Finally, the average adjustment between the preset\nfitting targets and the final fitting targets is examined.\n"
      ],
      "doi": "10.21437/Interspeech.2019-3091",
      "author_area_id": 5,
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "nguyen19_interspeech": {
      "authors": [
        [
          "Truc",
          "Nguyen"
        ],
        [
          "Franz",
          "Pernkopf"
        ]
      ],
      "title": "Acoustic Scene Classification with Mismatched Devices Using CliqueNets and Mixup Data Augmentation",
      "original": "3002",
      "page_count": 5,
      "order": 479,
      "p1": "2330",
      "pn": "2334",
      "abstract": [
        "Deep learning (DL) is key for the recent boost of acoustic scene classification\n(ASC) performance. Especially, convolutional neural networks (CNNs)\nare widely adopted with affirmed success. However, models are large\nand cumbersome, i.e. they have many layers, parallel branches or large\nensemble of individual models. In this paper, we propose a resource-efficient\nmodel using CliqueNets for feature learning and a mixture-of-experts\n(MoEs) layer. CliqueNets are a recurrent feedback structure enabling\nfeature refinement by the alternate propagation between constructed\nloop layers. In addition, we use mixup data augmentation to construct\nadversarial training examples. It is used for balancing the dataset\nof DCASE 2018 task 1B over the recordings of the mismatched devices\nA, B and C. This prevents over-fitting on the dataset of Device A,\ncaused by the gap of data amount between the different recording devices.\nExperimental results show that the proposed model achieves 64.7% average\nclassification accuracy for Device C and B, and 70.0% for Device A\nwith less than one million of parameters.\n"
      ],
      "doi": "10.21437/Interspeech.2019-3002",
      "author_area_id": 5,
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "ahmed19_interspeech": {
      "authors": [
        [
          "Mohsin Y.",
          "Ahmed"
        ],
        [
          "Md. Mahbubur",
          "Rahman"
        ],
        [
          "Jilong",
          "Kuang"
        ]
      ],
      "title": "DeepLung: Smartphone Convolutional Neural Network-Based Inference of Lung Anomalies for Pulmonary Patients",
      "original": "2953",
      "page_count": 5,
      "order": 480,
      "p1": "2335",
      "pn": "2339",
      "abstract": [
        "DeepLung is an end-to-end deep learning based audio sensing and classification\nframework for lung anomaly (e.g. cough, wheeze) detection for pulmonary\npatients from streaming audio and inertial sensor data from a chest-held\nsmartphone. We design and develop 1-D and 2-D convolutional neural\nnetworks for DeepLung, and train them using the Interspeech 2010 Paralinguistic\nChallenge features. Two different audio windowing schemes: i) real-time\nrespiration cycle based natural windowing, and ii) static length windowing\nare compared and experimented with. Classifiers are developed considering\n2 different system architectures: i) mobile-cloud hybrid architecture,\nand ii) mobile in-situ architecture. Patient privacy is preserved in\nthe phone by filtering speech with a shallow classifier. To evaluate\nDeepLung, a novel and rigorous lung activity dataset is made by collecting\naudio and inertial sensor data from more than 131 real pulmonary patients\nand healthy subjects and annotated accurately by professional crowdsourcing.\nExperimental results show that the best combination of DeepLung convolutional\nneural network is 15&#8211;27% more accurate when compared to a state-of-the-art\nsmartphone based body sound detection system, with a best F1 score\nof 98%.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2953",
      "author_area_id": 5,
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "moore19_interspeech": {
      "authors": [
        [
          "Roger K.",
          "Moore"
        ],
        [
          "Lucy",
          "Skidmore"
        ]
      ],
      "title": "On the Use/Misuse of the Term &#8216;Phoneme&#8217;",
      "original": "2711",
      "page_count": 5,
      "order": 481,
      "p1": "2340",
      "pn": "2344",
      "abstract": [
        "The term &#8216;phoneme&#8217; lies at the heart of speech science\nand technology, and yet it is not clear that the research community\nfully appreciates its meaning and implications. In particular, it is\nsuspected that many researchers use the term in a casual sense to refer\nto the sounds of speech, rather than as a well defined abstract concept.\nIf true, this means that some sections of the community may be missing\nan opportunity to understand and exploit the implications of this important\npsychological phenomenon. Here we review the correct meaning of the\nterm &#8216;phoneme&#8217; and report the results of an investigation\ninto its use/misuse in the accepted papers at INTERSPEECH-2018. It\nis confirmed that a significant proportion of the community (i) may\nnot be aware of the critical difference between &#8216;phonetic&#8217;\nand &#8216;phonemic&#8217; levels of description, (ii) may not fully\nunderstand the significance of &#8216;phonemic contrast&#8217;, and\nas a consequence, (iii) consistently misuse the term &#8216;phoneme&#8217;.\nThese findings are discussed, and recommendations are made as to how\nthis situation might be mitigated.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2711",
      "author_area_id": 5,
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "muckenhirn19_interspeech": {
      "authors": [
        [
          "Hannah",
          "Muckenhirn"
        ],
        [
          "Vinayak",
          "Abrol"
        ],
        [
          "Mathew",
          "Magimai-Doss"
        ],
        [
          "S\u00e9bastien",
          "Marcel"
        ]
      ],
      "title": "Understanding and Visualizing Raw Waveform-Based CNNs",
      "original": "2341",
      "page_count": 5,
      "order": 482,
      "p1": "2345",
      "pn": "2349",
      "abstract": [
        "Modeling directly raw waveforms through neural networks for speech\nprocessing is gaining more and more attention. Despite its varied success,\na question that remains is: what kind of information are such neural\nnetworks capturing or learning for different tasks from the speech\nsignal? Such an insight is not only interesting for advancing those\ntechniques but also for understanding better speech signal characteristics.\nThis paper takes a step in that direction, where we develop a gradient\nbased approach to estimate the relevance of each speech sample input\non the output score. We show that analysis of the resulting &#8220;relevance\nsignal&#8221; through conventional speech signal processing techniques\ncan reveal the information modeled by the whole network. We demonstrate\nthe potential of the proposed approach by analyzing raw waveform CNN-based\nphone recognition and speaker identification systems.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2341",
      "author_area_id": 5,
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "kilgour19_interspeech": {
      "authors": [
        [
          "Kevin",
          "Kilgour"
        ],
        [
          "Mauricio",
          "Zuluaga"
        ],
        [
          "Dominik",
          "Roblek"
        ],
        [
          "Matthew",
          "Sharifi"
        ]
      ],
      "title": "Fr&#233;chet Audio Distance: A Reference-Free Metric for Evaluating Music Enhancement Algorithms",
      "original": "2219",
      "page_count": 5,
      "order": 483,
      "p1": "2350",
      "pn": "2354",
      "abstract": [
        "We propose the Fr&#233;chet Audio Distance (FAD), a novel, reference-free\nevaluation metric for music enhancement algorithms. We demonstrate\nhow typical evaluation metrics for speech enhancement and blind source\nseparation can fail to accurately measure the perceived effect of a\nwide variety of distortions. As an alternative, we propose adapting\nthe Fr&#233;chet Inception Distance (FID) metric used to evaluate generative\nimage models to the audio domain. FAD is validated using a wide variety\nof artificial distortions and is compared to the signal based metrics\nsignal to distortion ratio (SDR), cosine distance, and magnitude L2\ndistance. We show that, with a correlation coefficient of 0.52, FAD\ncorrelates more closely with human perception than either SDR, cosine\ndistance or magnitude L2 distance, with correlation coefficients of\n0.39, -0.15 and -0.01 respectively.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2219",
      "author_area_id": 5,
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "gong19_interspeech": {
      "authors": [
        [
          "Yuan",
          "Gong"
        ],
        [
          "Jian",
          "Yang"
        ],
        [
          "Jacob",
          "Huber"
        ],
        [
          "Mitchell",
          "MacKnight"
        ],
        [
          "Christian",
          "Poellabauer"
        ]
      ],
      "title": "ReMASC: Realistic Replay Attack Corpus for Voice Controlled Systems",
      "original": "1541",
      "page_count": 5,
      "order": 484,
      "p1": "2355",
      "pn": "2359",
      "abstract": [
        "This paper introduces a new database of voice recordings with the goal\nof supporting research on vulnerabilities and protection of voice-controlled\nsystems (VCSs). In contrast to prior efforts, the proposed database\ncontains both genuine voice commands and replayed recordings of such\ncommands, collected in  realistic VCSs usage scenarios and using  modern\nvoice assistant development kits. Specifically, the database contains\nrecordings from four systems (each with a different microphone array)\nin a variety of environmental conditions with different forms of background\nnoise and relative positions between speaker and device. To the best\nof our knowledge, this is the first publicly available database1 that\nhas been specifically designed for the protection of state-of-the-art\nvoice-controlled systems against various replay attacks in various\nconditions and environments.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1541",
      "author_area_id": 5,
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "bt19_interspeech": {
      "authors": [
        [
          "Balamurali",
          "B.T."
        ],
        [
          "Jer-Ming",
          "Chen"
        ]
      ],
      "title": "Analyzing Intra-Speaker and Inter-Speaker Vocal Tract Impedance Characteristics in a Low-Dimensional Feature Space Using t-SNE",
      "original": "1492",
      "page_count": 4,
      "order": 485,
      "p1": "2360",
      "pn": "2363",
      "abstract": [
        "In an earlier study [1], we have successfully classified a vowel-gesture\nparameter, gamma &#947;(f) (relative vocal tract impedance spectrum\nmeasured using broadband signal excitation applied at the speaker&#8217;s\nmouth during vowel phonation), via ensemble classification yielding\naccuracy exceeding 80% for six nominal regions of the vowel plane.\nIn this follow-up investigation, we analyze gamma using t-SNE, a dimension\nreduction technique to allow visualizing gamma in low dimensional space,\nat two levels: inter-speaker and intra-speaker. Examining the same\ngamma dataset from [1], t-SNE yielded good spatial clustering in identifying\nthe 6 different speakers with an accuracy exceeding 90%, attributable\nto the inter-speaker variation. Next, we further evaluated gamma of\nmeasurements only from a particular speaker in the lower dimension,\nwhich indicates intra-speaker distribution which may be associated\nwith different measurement sessions. Using gamma may be seen as a meaningful\nparameter deserving further study, because it is inherently a function\nof the calibration load &#8212; unique for every speaker and measurement\nsession. Because the calibration is made with the subject&#8217;s mouth\nclosed, so the measurement field during calibration is loaded solely\nby the impedance of the radiation field as seen at the subject&#8217;s\nlips and baffled by the subject&#8217;s face (geometrical information).\n"
      ],
      "doi": "10.21437/Interspeech.2019-1492",
      "author_area_id": 5,
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "lee19b_interspeech": {
      "authors": [
        [
          "Geon Woo",
          "Lee"
        ],
        [
          "Jung Hyuk",
          "Lee"
        ],
        [
          "Seong Ju",
          "Kim"
        ],
        [
          "Hong Kook",
          "Kim"
        ]
      ],
      "title": "Directional Audio Rendering Using a Neural Network Based Personalized HRTF",
      "original": "8005",
      "page_count": 2,
      "order": 486,
      "p1": "2364",
      "pn": "2365",
      "abstract": [
        "Multi-channel speech/audio separation and enhancement methods are popularly\nused for many speech/audio related applications. However, these methods\nmay cause a loss of spatial cues, including the interaural time difference\nand interaural level difference, for further processing of monoaural\nsignals. Thus, listeners may encounter difficulties in understanding\nthe direction of the source signal. We present a directional audio\nrenderer using a personalized HRTF, which is estimated by a neural\nnetwork that combines DNN and CNN with anthropometric parameters and\near images of the listener. This demonstrated directional audio renderer\nconcept aims to help foster research on audio processing for virtual\nreality/augmented reality to improve the quality of service of such\ndevices.\n"
      ]
    },
    "pienaar19_interspeech": {
      "authors": [
        [
          "Wikus",
          "Pienaar"
        ],
        [
          "Daan",
          "Wissing"
        ]
      ],
      "title": "Online Speech Processing and Analysis Suite",
      "original": "8007",
      "page_count": 2,
      "order": 487,
      "p1": "2366",
      "pn": "2367",
      "abstract": [
        "Proper phonological analyses, descriptions and explanations as well\nas gaining insight into language variation and change rely heavily\nupon ample and trustworthy phonetic data. Our  Online Speech Processing\nand Analysis Suite is a positive development in just this direction.\n"
      ]
    },
    "maurer19_interspeech": {
      "authors": [
        [
          "Dieter",
          "Maurer"
        ],
        [
          "Heidy",
          "Suter"
        ],
        [
          "Christian",
          "d\u2019Hereuse"
        ],
        [
          "Volker",
          "Dellwo"
        ]
      ],
      "title": "Formant Pattern and Spectral Shape Ambiguity of Vowel Sounds, and Related Phenomena of Vowel Acoustics &#8212; Exemplary Evidence",
      "original": "8017",
      "page_count": 2,
      "order": 488,
      "p1": "2368",
      "pn": "2369",
      "abstract": [
        "In the specialist literature on vowel acoustics, there is an extensive\nand often controversial debate on whether the primary acoustic cues\nof vowel quality are contained in the formant patterns or, alternatively,\nin the spectral shape. Yet, recent studies have shown that neither\nformant patterns nor spectral shapes are vowel quality-specific but\nthat they are ambiguous because of a complex interaction between pitch\nand vowel-related spectral characteristics. In order to give insight\ninto the phenomenon of formant pattern and spectral shape ambiguity\nof vowel sounds and its role for vowel acoustics, exemplary series\nof speech and of vowel sounds are presented in an online documentation,\nmost of them selected from the Zurich Corpus. The presentation includes\nsound playbacks and results of an acoustic analysis (FFT spectra, LPC\ncurves, spectrograms, f<SUB>o</SUB> contours, formant patterns) and\nof a vowel recognition test. A Klatt synthesiser is also included for\nresynthesis and synthesis purposes. The presentation intends (i) to\nsupport researchers in their evaluation of existing and future studies,\nquestioning whether the actual variation and pitch-dependency of the\nvowel spectrum is taken into account when attempting to generalise\nexperimental results, and (ii) to support students in their acquisition\nof state-of-the-art knowledge of vowel acoustics.\n"
      ]
    },
    "noll19_interspeech": {
      "authors": [
        [
          "Anton",
          "Noll"
        ],
        [
          "Jonathan",
          "Stuefer"
        ],
        [
          "Nicola",
          "Klingler"
        ],
        [
          "Hannah",
          "Leykum"
        ],
        [
          "Carina",
          "Lozo"
        ],
        [
          "Jan",
          "Luttenberger"
        ],
        [
          "Michael",
          "Pucher"
        ],
        [
          "Carolin",
          "Schmid"
        ]
      ],
      "title": " Sound Tools eXtended (STx) 5.0 &#8212; A Powerful Sound Analysis Tool Optimized for Speech",
      "original": "8022",
      "page_count": 2,
      "order": 489,
      "p1": "2370",
      "pn": "2371",
      "abstract": [
        "In this paper, we introduce Sound Tools eXtended (STx) version 5.0,\nan acoustic speech and sound processing application. STx 5.0 contains\nan integrated, simplified and compact GUI, specifically designed for\nspeech analysis for phoneticians, linguists, psychologists, and researchers\nin related fields. It features a well structured user interface, compatibility\nwith established tools (TextGrid [1], MAUS [2]), and top-notch signal\nanalysis tools. STx 5.0 enables researchers as well as students to\nconduct advanced analysis of audio files, especially of speech recordings.\nSTx 5.0 implements a new interface for the already established profiles\nin STx 5.0, which helps customize settings according to the researcher&#8217;s\nneeds.\n"
      ]
    },
    "eldesouki19_interspeech": {
      "authors": [
        [
          "Mohamed",
          "Eldesouki"
        ],
        [
          "Naassih",
          "Gopee"
        ],
        [
          "Ahmed",
          "Ali"
        ],
        [
          "Kareem",
          "Darwish"
        ]
      ],
      "title": "FarSpeech: Arabic Natural Language Processing for Live Arabic Speech",
      "original": "8030",
      "page_count": 2,
      "order": 490,
      "p1": "2372",
      "pn": "2373",
      "abstract": [
        "This paper presents FarSpeech, QCRI&#8217;s combined Arabic speech\nrecognition, natural language processing (NLP), and dialect identification\npipeline. It features modern web technologies to capture live audio,\ntranscribes Arabic audio, NLP processes the transcripts, and identifies\nthe dialect of the speaker. For transcription, we use QATS, which is\na Kaldi-based ASR system that uses Time Delay Neural Networks (TDNN).\nFor NLP, we use a SOTA Arabic NLP toolkit that employs various deep\nneural network and SVM based models. Finally, our dialect identification\nsystem uses multi-modality from both acoustic and linguistic input.\nFarSpeech1 presents different screens to display the transcripts, text\nsegmentation, part-of-speech tags, recognized named entities, diacritized\ntext, and the identified dialect of the speech.\n"
      ]
    },
    "haider19_interspeech": {
      "authors": [
        [
          "Fasih",
          "Haider"
        ],
        [
          "Saturnino",
          "Luz"
        ]
      ],
      "title": "A System for Real-Time Privacy Preserving Data Collection for Ambient Assisted Living",
      "original": "8037",
      "page_count": 2,
      "order": 491,
      "p1": "2374",
      "pn": "2375",
      "abstract": [
        "Ambient Assisted Living (AAL) technologies are being developed which\ncould assist elderly people to live healthy and active lives. These\ntechnologies have been used to monitor people&#8217;s daily exercises,\nconsumption of calories and sleeping patterns, and to provide coaching\ninterventions to foster positive behaviour. Speech and audio processing\ncan be used to complement such AAL technologies to inform interventions\nfor healthy ageing by analyzing acoustic data captured in the user&#8217;s\nhome. However, collection of data in home settings present a number\nof challenges. One of the most pressing challenges concerns how to\nmanage privacy and data protection. To address this issue, we have\ndeveloped a low-cost system which can extract audio features while\nprotecting the actual spoken content upon detection of voice activity,\nand store audio features for further processing which offer privacy\nguarantees. These privacy preserving features are being tested in the\ncontext of a larger project which includes health and well-being monitoring\nand coaching.\n"
      ]
    },
    "gupta19c_interspeech": {
      "authors": [
        [
          "Chitralekha",
          "Gupta"
        ],
        [
          "Karthika",
          "Vijayan"
        ],
        [
          "Bidisha",
          "Sharma"
        ],
        [
          "Xiaoxue",
          "Gao"
        ],
        [
          "Haizhou",
          "Li"
        ]
      ],
      "title": " NUS Speak-to-Sing: A Web Platform for Personalized Speech-to-Singing Conversion",
      "original": "8041",
      "page_count": 2,
      "order": 492,
      "p1": "2376",
      "pn": "2377",
      "abstract": [
        "Singing like a professional singer is extremely appealing to the general\npublic. However, many individuals are not able to sing like a singer\nwho has received formal training over several years. We develop a web\nplatform, where users can perform personalized singing synthesis. A\nuser has to read and record the lyrics of a song in our web platform,\nand enjoy good quality singing vocals synthesized in his/her own voice.\nWe perform a template-based speech-to-singing voice conversion at the\nbackend of the web interface, that uses the prosody characteristics\nof the song derived from good quality singing by a trained singer and\nretains the speaker characteristics from the respective user. We utilize\nan improved temporal alignment scheme between speech and singing signals\nusing tandem features, and employ a deep-spectral map to incorporate\nsinging spectral characteristics into user&#8217;s voice. The singing\nvocals are later synthesized by a vocoder. Using this web platform,\nwe advocate that &#8216;everyone can sing as they desire&#8217;.\n"
      ]
    },
    "kaltenbacher19_interspeech": {
      "authors": [
        [
          "Manfred",
          "Kaltenbacher"
        ]
      ],
      "title": "Physiology and Physics of Voice Production",
      "original": "abs12",
      "page_count": 0,
      "order": 493,
      "p1": "0",
      "pn": "",
      "abstract": [
        "Our knowledge-based societies in the information age are highly dependent\non efficient verbal communication. Today most people have employments\nwhich rely on their communication competence. Consequently, communication\ndisorders became a worldwide socio-economic factor. To increase the\nquality of life on one hand and to keep the economic costs under control\non the other, new medical strategies are needed to prevent communication\ndisorders, enable early diagnosis and eventually treat and rehabilitate\npeople concerned.<br style=\"mso-data-placement:same-cell;\" /><br style=\"mso-data-placement:same-cell;\"\n/>\n  The key issue for communication is phonation, a complex process\nof voice production taking place in the larynx. Based on aeroacoustic\nprinciples, the sound is generated by the pulsating air jet and supra-glottal\nturbulent structures. The laryngeal sound is further filtered and amplified\nby the supra-glottal acoustic resonance spaces, radiated at the lips\nand perceived as voice. There is no doubt that the possibility to produce\nvoice is crucial for human communication, although many people do not\nrealize this until they lose their voice temporarily, e.g. due to common\nrespiratory inflammations.\n"
      ]
    },
    "schuller19_interspeech": {
      "authors": [
        [
          "Bj\u00f6rn W.",
          "Schuller"
        ],
        [
          "Anton",
          "Batliner"
        ],
        [
          "Christian",
          "Bergler"
        ],
        [
          "Florian B.",
          "Pokorny"
        ],
        [
          "Jarek",
          "Krajewski"
        ],
        [
          "Margaret",
          "Cychosz"
        ],
        [
          "Ralf",
          "Vollmann"
        ],
        [
          "Sonja-Dana",
          "Roelen"
        ],
        [
          "Sebastian",
          "Schnieder"
        ],
        [
          "Elika",
          "Bergelson"
        ],
        [
          "Alejandrina",
          "Cristia"
        ],
        [
          "Amanda",
          "Seidl"
        ],
        [
          "Anne S.",
          "Warlaumont"
        ],
        [
          "Lisa",
          "Yankowitz"
        ],
        [
          "Elmar",
          "N\u00f6th"
        ],
        [
          "Shahin",
          "Amiriparian"
        ],
        [
          "Simone",
          "Hantke"
        ],
        [
          "Maximilian",
          "Schmitt"
        ]
      ],
      "title": "The INTERSPEECH 2019 Computational Paralinguistics Challenge: Styrian Dialects, Continuous Sleepiness, Baby Sounds &amp; Orca Activity",
      "original": "1122",
      "page_count": 5,
      "order": 494,
      "p1": "2378",
      "pn": "2382",
      "abstract": [
        "The INTERSPEECH 2019 Computational Paralinguistics Challenge addresses\nfour different problems for the first time in a research competition\nunder well-defined conditions: In the  Styrian Dialects Sub-Challenge,\nthree types of Austrian-German dialects have to be classified; in the\n Continuous Sleepiness Sub-Challenge, the sleepiness of a speaker has\nto be assessed as regression problem; in the  Baby Sound Sub-Challenge,\nfive types of infant sounds have to be classified; and in the  Orca\nActivity Sub-Challenge, orca sounds have to be detected. We describe\nthe Sub-Challenges and baseline feature extraction and classifiers,\nwhich include data-learnt (supervised) feature representations by the\n&#8216;usual&#8217; ComParE and BoAWfeatures, and deep unsupervised\nrepresentation learning using the  auDeep toolkit.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1122"
    },
    "dubagunta19_interspeech": {
      "authors": [
        [
          "S. Pavankumar",
          "Dubagunta"
        ],
        [
          "Mathew",
          "Magimai-Doss"
        ]
      ],
      "title": "Using Speech Production Knowledge for Raw Waveform Modelling Based Styrian Dialect Identification",
      "original": "2398",
      "page_count": 5,
      "order": 495,
      "p1": "2383",
      "pn": "2387",
      "abstract": [
        "This paper addresses the Styrian Dialect sub-challenge of the INTERSPEECH\n2019 Computational Paralinguistics Challenge. We treat this challenge\nas dialect identification with no linguistic resources/knowledge and\nwith limited acoustic resources, and develop end-to-end raw waveform\nmodelling based methods that incorporate knowledge related to speech\nproduction. In this direction, we investigate two methods: (a) modelling\nthe signals after source system decomposition and (b) transferring\nknowledge from articulatory feature models trained on English language.\nOur investigations show that the proposed approaches on the ComParE\n2019 Styrian dialect data yield systems that perform better than low\nlevel descriptor-based and bag-of-audio-word representation based approaches\nand comparable to sequence-to-sequence auto-encoder based approach.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2398",
      "author_area_id": 13,
      "author_area_label": "Special Sessions"
    },
    "elsner19_interspeech": {
      "authors": [
        [
          "Daniel",
          "Elsner"
        ],
        [
          "Stefan",
          "Langer"
        ],
        [
          "Fabian",
          "Ritz"
        ],
        [
          "Robert",
          "Mueller"
        ],
        [
          "Steffen",
          "Illium"
        ]
      ],
      "title": "Deep Neural Baselines for Computational Paralinguistics",
      "original": "2478",
      "page_count": 5,
      "order": 496,
      "p1": "2388",
      "pn": "2392",
      "abstract": [
        "Detecting sleepiness from spoken language is an ambitious task, which\nis addressed by the Interspeech 2019 Computational Paralinguistics\nChallenge (ComParE). We propose an end-to-end deep learning approach\nto detect and classify patterns reflecting sleepiness in the human\nvoice. Our approach is based solely on a moderately complex deep neural\nnetwork architecture. It may be applied directly on the audio data\nwithout requiring any specific feature engineering, thus remaining\ntransferable to other audio classification tasks. Nevertheless, our\napproach performs similar to state-of-the-art machine learning models.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2478",
      "author_area_id": 13,
      "author_area_label": "Special Sessions"
    },
    "kisler19_interspeech": {
      "authors": [
        [
          "Thomas",
          "Kisler"
        ],
        [
          "Raphael",
          "Winkelmann"
        ],
        [
          "Florian",
          "Schiel"
        ]
      ],
      "title": "Styrian Dialect Classification: Comparing and Fusing Classifiers Based on a Feature Selection Using a Genetic Algorithm",
      "original": "2540",
      "page_count": 5,
      "order": 497,
      "p1": "2393",
      "pn": "2397",
      "abstract": [
        "Many classifiers struggle when confronted with a high dimensional feature\nspace like in the data sets provided for the Interspeech ComParE challenge.\nThis is because most features do not significantly contribute to the\nprediction. To alleviate this problem, we propose a feature selection\nbased on a Genetic Algorithm (GA) that uses an SVM as the fitness function.\nWe show that this yields a reduced subset (1) which results in an Unweighted\nAverage Recall (UAR) that beats the challenge baseline on the development\nset for the 3-class classification problem. Further, we extract an\nadditional per-phoneme feature set, where the features are inspired\nby the ComParE features. On this set the same GA-based feature selection\nis performed and the resulting set is used for training in isolation\n(2) and in combination with the aforementioned reduced challenge features\n(3). Five classifiers were tested on the three subsets, namely SVMs,\nDNNs, GBMs, RFs, and regularized regression. All classifiers achieved\na UAR above the baseline on all three sets. The best performance on\nset (1) was achieved by an SVM using an RBF kernel and on sets (2)\nand (3) by a fusion of classifiers.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2540",
      "author_area_id": 13,
      "author_area_label": "Special Sessions"
    },
    "yeh19_interspeech": {
      "authors": [
        [
          "Sung-Lin",
          "Yeh"
        ],
        [
          "Gao-Yi",
          "Chao"
        ],
        [
          "Bo-Hao",
          "Su"
        ],
        [
          "Yu-Lin",
          "Huang"
        ],
        [
          "Meng-Han",
          "Lin"
        ],
        [
          "Yin-Chun",
          "Tsai"
        ],
        [
          "Yu-Wen",
          "Tai"
        ],
        [
          "Zheng-Chi",
          "Lu"
        ],
        [
          "Chieh-Yu",
          "Chen"
        ],
        [
          "Tsung-Ming",
          "Tai"
        ],
        [
          "Chiu-Wang",
          "Tseng"
        ],
        [
          "Cheng-Kuang",
          "Lee"
        ],
        [
          "Chi-Chun",
          "Lee"
        ]
      ],
      "title": "Using Attention Networks and Adversarial Augmentation for Styrian Dialect Continuous Sleepiness and Baby Sound Recognition",
      "original": "2110",
      "page_count": 5,
      "order": 498,
      "p1": "2398",
      "pn": "2402",
      "abstract": [
        "In this study, we present extensive attention-based networks with data\naugmentation methods to participate in the INTERSPEECH 2019 ComPareE\nChallenge, specifically the three Sub-challenges: Styrian Dialect Recognition,\nContinuous Sleepiness Regression, and Baby Sound Classification. For\nStyrian Dialect Sub-challenge, these dialects are classified into Northern\nStyrian (NorthernS), Urban Sytrian (UrbanS), and Eastern Styrian (EasternS).\nOur proposed model achieves an UAR 49.5% on the test set, which is\n2.5% higher than the baseline. For Continuous Sleepiness Sub-challenge,\nit is defined as a regression task with score range from 1 (extremely\nalert) to 9 (very sleepy). In this work, our proposed architecture\nachieves a Spearman correlation 0.369 on the test set, which surpasses\nthe baseline model by 0.026. For Baby Sound Sub-challenge, the infant\nsounds are classified into canonical babbling, non-canonical babbling,\ncrying, laughing and junk/other, and our proposed augmentation framework\nachieves an UAR of 62.39% on the test set, which outperforms the baseline\nby about 3.7%. Overall, our analyses demonstrate that by fusing attention\nnetwork models with conventional support vector machine benefits the\ntest set robustness, and the recognition rates of these paralinguistic\nattributes generally improve when performing data augmentation.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2110",
      "author_area_id": 13,
      "author_area_label": "Special Sessions"
    },
    "wu19f_interspeech": {
      "authors": [
        [
          "Peter",
          "Wu"
        ],
        [
          "SaiKrishna",
          "Rallabandi"
        ],
        [
          "Alan W.",
          "Black"
        ],
        [
          "Eric",
          "Nyberg"
        ]
      ],
      "title": "Ordinal Triplet Loss: Investigating Sleepiness Detection from Speech",
      "original": "2278",
      "page_count": 5,
      "order": 499,
      "p1": "2403",
      "pn": "2407",
      "abstract": [
        "In this paper we present our submission to the INTERSPEECH 2019 ComParE\nSleepiness challenge. By nature, the given speech dataset is an archetype\nof one with relatively limited samples, a complex underlying data distribution,\nand subjective ordinal labels. We propose a novel approach termed ordinal\ntriplet loss (OTL) that can be readily added to any deep architecture\nin order to address the above data constraints. Ordinal triplet loss\nimplicitly maps inputs into a space where similar samples are closer\nto each other than different ones. We demonstrate the efficacy of our\napproach on the aforementioned task.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2278",
      "author_area_id": 13,
      "author_area_label": "Special Sessions"
    },
    "ravi19_interspeech": {
      "authors": [
        [
          "Vijay",
          "Ravi"
        ],
        [
          "Soo Jin",
          "Park"
        ],
        [
          "Amber",
          "Afshan"
        ],
        [
          "Abeer",
          "Alwan"
        ]
      ],
      "title": "Voice Quality and Between-Frame Entropy for Sleepiness Estimation",
      "original": "2988",
      "page_count": 5,
      "order": 500,
      "p1": "2408",
      "pn": "2412",
      "abstract": [
        "Sleepiness monitoring and prediction has many potential applications,\nsuch as being a safety feature in driver-assistance systems. In this\nstudy, we address the ComparE 2019 Continuous Sleepiness task of estimating\nthe degree of sleepiness from voice data. The voice quality feature\nset was proposed to capture the acoustic characteristics related to\nthe degree of sleepiness of a speaker, and between-frame entropy was\nproposed as an instantaneous measure of the speaking rate. An outlier\nelimination on the training data using between-frame entropy enhanced\nthe system robustness in all conditions. This was followed by a regression\nsystem to predict the degree of sleepiness. Utterances were represented\nusing i-vectors computed from voice quality features. Similar systems\nwere also developed using mel-frequency cepstral coefficients and the\nComParE16 feature set. These three systems were combined using score-level\nfusion. Results suggested complementarity between these feature sets.\nThe complete system outperformed the baseline system which used the\nComParE16 feature set. A relative improvement of 19.5% and 5.4% was\nachieved on the development and the test datasets, respectively.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2988",
      "author_area_id": 13,
      "author_area_label": "Special Sessions"
    },
    "gosztolya19b_interspeech": {
      "authors": [
        [
          "G\u00e1bor",
          "Gosztolya"
        ]
      ],
      "title": "Using Fisher Vector and Bag-of-Audio-Words Representations to Identify Styrian Dialects, Sleepiness, Baby &amp; Orca Sounds",
      "original": "1726",
      "page_count": 5,
      "order": 501,
      "p1": "2413",
      "pn": "2417",
      "abstract": [
        "The 2019 INTERSPEECH Computational Paralinguistics Challenge (ComParE)\nconsists of four Sub-Challenges, where the tasks are to identify different\nGerman (Austrian) dialects, estimate how sleepy the speaker is, what\ntype of sound a given baby uttered, and whether there is a sound of\nan orca (killer whale) present in the recording. Following our team&#8217;s\nlast year entry, we continue our research by looking for feature set\ntypes that might be employed on a wide variety of tasks without alteration.\nThis year, besides the standard 6373-sized ComParE functionals, we\nexperimented with the Fisher vector representation along with the Bag-of-Audio-Words\ntechnique. To adapt Fisher vectors from the field of image processing,\nwe utilized them on standard MFCC features instead of the originally\nintended SIFT attributes (which describe local objects found in the\nimage). Our results indicate that using these feature representation\ntechniques was indeed beneficial, as we could outperform the baseline\nvalues in three of the four Sub-Challenges; the performance of our\napproach seems to be even higher if we consider that the baseline scores\nwere obtained by combining different methods as well.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1726"
    },
    "das19b_interspeech": {
      "authors": [
        [
          "Rohan Kumar",
          "Das"
        ],
        [
          "Haizhou",
          "Li"
        ]
      ],
      "title": "Instantaneous Phase and Long-Term Acoustic Cues for Orca Activity Detection",
      "original": "1894",
      "page_count": 5,
      "order": 502,
      "p1": "2418",
      "pn": "2422",
      "abstract": [
        "The orca activity detection is a challenging task that prevails in\nunderwater acoustics. Signal level discrimination of orca activity\nto that of noise signal is minimum, hence a topic of interest. The\norca activity detection is a subtask of Computational Paralinguistics\nChallenge (ComParE) 2019. In this work, we study a few novel acoustic\ncues based on phase and long-term information to capture the artifacts\nfrom signal to detect orca activity. The phase of signal possesses\ndefinite signal characteristics which is completely random in case\nof noise signal. In this regard, we investigate instantaneous phase\nas an artifact for orca activity detection. Additionally, we believe\nthat the long-term features can be more helpful to detect such artifacts\nthan the conventional short-term acoustic features. We explore these\ntwo directions along with the state-of-the-art baselines on ComParE\nfunctionals, bag-of-audio-words and auDeep features for ComParE 2019.\nThe studies reveal that the instantaneous phase as a single feature\ncan perform better than the fusion of three baselines given as a benchmark\nfor the challenge. Further, we perform a score level fusion of the\nacoustic features and the three baselines that further enhances the\nperformance.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1894",
      "author_area_id": 13,
      "author_area_label": "Special Sessions"
    },
    "schiller19_interspeech": {
      "authors": [
        [
          "Dominik",
          "Schiller"
        ],
        [
          "Tobias",
          "Huber"
        ],
        [
          "Florian",
          "Lingenfelser"
        ],
        [
          "Michael",
          "Dietz"
        ],
        [
          "Andreas",
          "Seiderer"
        ],
        [
          "Elisabeth",
          "Andr\u00e9"
        ]
      ],
      "title": "Relevance-Based Feature Masking: Improving Neural Network Based Whale Classification Through Explainable Artificial Intelligence",
      "original": "2707",
      "page_count": 5,
      "order": 503,
      "p1": "2423",
      "pn": "2427",
      "abstract": [
        "Underwater sounds provide essential information for marine researchers\nto study sea mammals. During long-term studies large amounts of sound\nsignals are being recorded using hydrophones. To facilitate the time\nconsuming process of manually evaluating the recorded data, computational\nsystems are often employed. Recent approaches utilize Convolutional\nNeural Networks (CNNs) to analyze spectrograms extracted from the audio\nsignal. In this paper we explore the potential of relevance analysis\nto enhance the performance of existing CNN approaches. For this purpose,\nwe present a fusion system that utilizes intermediate outputs of three\nstate of the art CNNs, which are fine tuned to recognize whale sounds\nin spectrograms. Hereby we use Explainable Artificial Intelligence\n(XAI) to asses the relevance of each feature within the obtained representations.\nBased on those relevance values, we create novel masking algorithms\nto extract significant subsets of respective representations. These\nsubsets are used to train an ensemble of classification systems that\nare serving as input for the final fusion step. We observe that a classification\nsystem can benefit from the inclusion of Relevance-based Feature Masking\nin terms of improved performance and reduced input dimensionality.\nThe presented work is part of the INTERSPEECH 2019 Computational Paralinguistics\nChallenge.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2707",
      "author_area_id": 13,
      "author_area_label": "Special Sessions"
    },
    "caraty19_interspeech": {
      "authors": [
        [
          "Marie-Jos\u00e9",
          "Caraty"
        ],
        [
          "Claude",
          "Montaci\u00e9"
        ]
      ],
      "title": "Spatial, Temporal and Spectral Multiresolution Analysis for the INTERSPEECH 2019 ComParE Challenge",
      "original": "1693",
      "page_count": 5,
      "order": 504,
      "p1": "2428",
      "pn": "2432",
      "abstract": [
        "The INTERSPEECH 2019 Orca Activity Challenge consists in the detection\nof the Orca sounds from underwater audio signal. Orca can produce a\nwide variety of sounds categorized in clicks, whistles and pulsed calls.\nClicks are useful for echolocation, whistles and pulsed calls are used\nas social signals. Experiments were conducted on DeepAL Fieldwork Data\n(DLFD). Underwater sounds were recorded in northern British Columbia\nby a hydrophones array. Recordings were labeled by marine biologists\nin Orca sounds or Noise. We have investigated multiresolution analysis\naccording to the three main relevant acoustic levels: spatial, temporal\nand spectral. For this purpose, we studied the beamforming array analysis,\nthe multitemporal resolution and the multilevel wavelet decomposition.\nFor the spatial level, a beamforming algorithm was used for denoising\nthe underwater audio signal. For the temporal level, two sets of multitemporal\nthree-level features were extracted using pyramidal representation.\nFor the spectral level, in order to detect transient sound, wavelet\nanalysis was computed using various wavelet families. At last, an Orca\nActivity detector was designed combining ComParE set with multitemporal\nand multilevel wavelet features. Experiments on the Test set have shown\na significant improvement of 0.051, compared to the baseline performance\nof the Challenge (0.866).\n"
      ],
      "doi": "10.21437/Interspeech.2019-1693",
      "author_area_id": 13,
      "author_area_label": "Special Sessions"
    },
    "wu19g_interspeech": {
      "authors": [
        [
          "Haiwei",
          "Wu"
        ],
        [
          "Weiqing",
          "Wang"
        ],
        [
          "Ming",
          "Li"
        ]
      ],
      "title": "The DKU-LENOVO Systems for the INTERSPEECH 2019 Computational Paralinguistic Challenge",
      "original": "1386",
      "page_count": 5,
      "order": 505,
      "p1": "2433",
      "pn": "2437",
      "abstract": [
        "This paper introduces our approaches for the orca activity and continuous\nsleepiness tasks in the Interspeech ComParE Challenge 2019. For the\norca activity detection task, we extract deep embeddings using several\ndeep convolutional neural networks, followed by the Support Vector\nMachine (SVM) based back end classifier. Both STFT spectrogram and\nlog mel-spectrogram are explored as input features. To increase the\nsize of training data and deal with the data imbalance, we propose\nfour kinds of data augmentation. We also investigate the different\nways of fusion for multi-channel input data. Besides the official baseline\nsystem, to better evaluate the performance of our deep embedding system,\nwe employ the Fisher Vector (FV) encoding on various kinds of acoustic\nfeatures as an alternative baseline. Experimental results show that\nour proposed methods significantly outperform the baselines and achieve\n0.948 AUC and 0.365 Spearman&#8217;s Correlation Coefficient on the\norca activity and continuous sleepiness evaluation data, respectively.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1386",
      "author_area_id": 13,
      "author_area_label": "Special Sessions"
    },
    "nandwana19_interspeech": {
      "authors": [
        [
          "Mahesh Kumar",
          "Nandwana"
        ],
        [
          "Julien van",
          "Hout"
        ],
        [
          "Colleen",
          "Richey"
        ],
        [
          "Mitchell",
          "McLaren"
        ],
        [
          "Maria A.",
          "Barrios"
        ],
        [
          "Aaron",
          "Lawson"
        ]
      ],
      "title": "The VOiCES from a Distance Challenge 2019",
      "original": "1837",
      "page_count": 5,
      "order": 506,
      "p1": "2438",
      "pn": "2442",
      "abstract": [
        "The VOiCES from a Distance Challenge 2019 was designed to foster research\nin the area of speaker recognition and automatic speech recognition\n(ASR) with a special focus on single-channel distant/far-field audio\nunder various noisy conditions. The challenge was based on the recently\nreleased VOiCES corpus, with 60 international teams involved, of which\n24 teams participated in the evaluation. In this paper, we separately\npresent the challenge&#8217;s speaker recognition and ASR tasks. For\neach task, we outline the training, development, and test data, as\nwell as the evaluation metrics. Then, we report and discuss the results\nin light of the participant-provided system descriptions, to highlight\nthe major factors contributing to high performance in distant speech\nprocessing.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1837"
    },
    "novoselov19b_interspeech": {
      "authors": [
        [
          "Sergey",
          "Novoselov"
        ],
        [
          "Aleksei",
          "Gusev"
        ],
        [
          "Artem",
          "Ivanov"
        ],
        [
          "Timur",
          "Pekhovsky"
        ],
        [
          "Andrey",
          "Shulipa"
        ],
        [
          "Galina",
          "Lavrentyeva"
        ],
        [
          "Vladimir",
          "Volokhov"
        ],
        [
          "Alexandr",
          "Kozlov"
        ]
      ],
      "title": "STC Speaker Recognition Systems for the VOiCES from a Distance Challenge",
      "original": "2783",
      "page_count": 5,
      "order": 507,
      "p1": "2443",
      "pn": "2447",
      "abstract": [
        "This paper presents the Speech Technology Center (STC) speaker recognition\n(SR) systems submitted to the VOiCES From a Distance challenge 2019.\nThe challenge&#8217;s SR task is focused on the problem of speaker\nrecognition in single channel distant/far-field audio under noisy conditions.\nIn this work we investigate different deep neural networks architectures\nfor speaker embedding extraction to solve the task. We show that deep\nnetworks with residual frame level connections outperform more shallow\narchitectures. Simple energy based speech activity detector (SAD) and\nautomatic speech recognition (ASR) based SAD are investigated in this\nwork. We also address the problem of data preparation for robust embedding\nextractors training. The reverberation for the data augmentation was\nperformed using automatic room impulse response generator. In our systems\nwe used discriminatively trained cosine similarity metric learning\nmodel as embedding backend. Scores normalization procedure was applied\nfor each individual subsystem we used. Our final submitted systems\nwere based on the fusion of different subsystems. The results obtained\non the VOiCES development and evaluation sets demonstrate effectiveness\nand robustness of the proposed systems when dealing with distant/far-field\naudio under noisy conditions.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2783"
    },
    "matejka19_interspeech": {
      "authors": [
        [
          "Pavel",
          "Mat\u011bjka"
        ],
        [
          "Old\u0159ich",
          "Plchot"
        ],
        [
          "Hossein",
          "Zeinali"
        ],
        [
          "Ladislav",
          "Mo\u0161ner"
        ],
        [
          "Anna",
          "Silnova"
        ],
        [
          "Luk\u00e1\u0161",
          "Burget"
        ],
        [
          "Ond\u0159ej",
          "Novotn\u00fd"
        ],
        [
          "Ond\u0159ej",
          "Glembek"
        ]
      ],
      "title": "Analysis of BUT Submission in Far-Field Scenarios of VOiCES 2019 Challenge",
      "original": "2471",
      "page_count": 5,
      "order": 508,
      "p1": "2448",
      "pn": "2452",
      "abstract": [
        "This paper is a post-evaluation analysis of our efforts in VOiCES 2019\nSpeaker Recognition challenge. All systems in the fixed condition are\nbased on x-vectors with different features and DNN topologies. The\nsingle best system reaches minDCF of 0.38 (5.25% EER) and a fusion\nof 3 systems yields minDCF of 0.34 (4.87% EER).We also analyze how\nspeaker verification (SV) systems evolved in last few years and show\nresults also on SITW 2016 Challenge. EER on the core-core condition\nof the SITW 2016 challenge dropped from 5.85% to 1.65% for system fusions\nsubmitted for SITW 2016 and VOiCES 2019, respectively. The less restrictive\nopen condition allowed us to use external data for PLDA adaptation\nand achieve additional small performance improvement. In our submission\nto open condition, we used three x-vector systems and also one system\nbased on i-vectors.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2471"
    },
    "medennikov19_interspeech": {
      "authors": [
        [
          "Ivan",
          "Medennikov"
        ],
        [
          "Yuri",
          "Khokhlov"
        ],
        [
          "Aleksei",
          "Romanenko"
        ],
        [
          "Ivan",
          "Sorokin"
        ],
        [
          "Anton",
          "Mitrofanov"
        ],
        [
          "Vladimir",
          "Bataev"
        ],
        [
          "Andrei",
          "Andrusenko"
        ],
        [
          "Tatiana",
          "Prisyach"
        ],
        [
          "Mariya",
          "Korenevskaya"
        ],
        [
          "Oleg",
          "Petrov"
        ],
        [
          "Alexander",
          "Zatvornitskiy"
        ]
      ],
      "title": "The STC ASR System for the VOiCES from a Distance Challenge 2019",
      "original": "1574",
      "page_count": 5,
      "order": 509,
      "p1": "2453",
      "pn": "2457",
      "abstract": [
        "This paper is a description of the Speech Technology Center (STC) automatic\nspeech recognition (ASR) system for the &#8220;VOiCES from a Distance\nChallenge 2019&#8221;. We participated in the Fixed condition of the\nASR task, which means that the only training data available was an\n80-hour subset of the LibriSpeech corpus. The main difficulty of the\nchallenge is a mismatch between clean training data and distant noisy\ndevelopment/ evaluation data. In order to tackle this, we applied room\nacoustics simulation and weighted prediction error (WPE) dereverberation.\nWe also utilized well-known speaker adaptation using x-vector speaker\nembeddings, as well as novel room acoustics adaptation with R-vector\nroom impulse response (RIR) embeddings. The system used a lattice-level\ncombination of 6 acoustic models based on different pronunciation dictionaries\nand input features. N-best hypotheses were rescored with 3 neural network\nlanguage models (NNLMs) trained on both words and sub-word units. NNLMs\nwere also explored for out-of-vocabulary (OOV) words handling by means\nof artificial texts generation. The final system achieved Word Error\nRate (WER) of 14.7% on the evaluation data, which is the best result\nin the challenge.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1574"
    },
    "chong19_interspeech": {
      "authors": [
        [
          "Tze Yuang",
          "Chong"
        ],
        [
          "Kye Min",
          "Tan"
        ],
        [
          "Kah Kuan",
          "Teh"
        ],
        [
          "Chang Huai",
          "You"
        ],
        [
          "Hanwu",
          "Sun"
        ],
        [
          "Huy Dat",
          "Tran"
        ]
      ],
      "title": "The I2R&#8217;s ASR System for the VOiCES from a Distance Challenge 2019",
      "original": "2130",
      "page_count": 5,
      "order": 510,
      "p1": "2458",
      "pn": "2462",
      "abstract": [
        "This paper describes the development of the automatic speech recognition\n(ASR) system for the submission to the VOiCES from a Distance Challenge\n2019. In this challenge, we focused on the fixed condition, where the\ntask is to recognize reverberant and noisy speech based on a limited\namount of clean training data. In our system, the mismatch between\nthe training and testing conditions was reduced by using multi-style\ntraining where the training data was artificially contaminated with\ndifferent reverberation and noise sources. Also, the Weighted Prediction\nError (WPE) algorithm was used to reduce the reverberant effect in\nthe evaluation data. To boost the system performance, acoustic models\nof different neural network architectures were trained and the respective\nsystems were fused to give the final output. Moreover, an LSTM language\nmodel was used to rescore the lattice to compensate the weak n-gram\nmodel trained from only the transcription text. Evaluated on the development\nset, our system showed an average word error rate (WER) of 27.04%.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2130"
    },
    "nandwana19b_interspeech": {
      "authors": [
        [
          "Mahesh Kumar",
          "Nandwana"
        ],
        [
          "Julien van",
          "Hout"
        ],
        [
          "Colleen",
          "Richey"
        ],
        [
          "Mitchell",
          "McLaren"
        ],
        [
          "Maria A.",
          "Barrios"
        ],
        [
          "Aaron",
          "Lawson"
        ]
      ],
      "title": "The VOiCES from a Distance Challenge 2019",
      "original": "abs14",
      "page_count": 0,
      "order": 511,
      "p1": "0",
      "pn": "",
      "abstract": [
        "The VOiCES from a Distance Challenge 2019 was designed to foster research\nin the area of speaker recognition and automatic speech recognition\n(ASR) with a special focus on single-channel distant/far-field audio\nunder various noisy conditions. The challenge was based on the recently\nreleased VOiCES corpus, with 60 international teams involved, of which\n24 teams participated in the evaluation. In this paper, we separately\npresent the challenge&#8217;s speaker recognition and ASR tasks. For\neach task, we outline the training, development, and test data, as\nwell as the evaluation metrics. Then, we report and discuss the results\nin light of the participant-provided system descriptions, to highlight\nthe major factors contributing to high performance in distant speech\nprocessing.<br style=\"mso-data-placement:same-cell;\" /><br style=\"mso-data-placement:same-cell;\"\n/>\n  This paper also appears in session Wed-SS-7-3.\n"
      ],
      "author_area_id": 13,
      "author_area_label": "Special Sessions"
    },
    "novoselov19c_interspeech": {
      "authors": [
        [
          "Sergey",
          "Novoselov"
        ],
        [
          "Aleksei",
          "Gusev"
        ],
        [
          "Artem",
          "Ivanov"
        ],
        [
          "Timur",
          "Pekhovsky"
        ],
        [
          "Andrey",
          "Shulipa"
        ],
        [
          "Galina",
          "Lavrentyeva"
        ],
        [
          "Vladimir",
          "Volokhov"
        ],
        [
          "Alexandr",
          "Kozlov"
        ]
      ],
      "title": "STC Speaker Recognition Systems for the VOiCES from a Distance Challenge",
      "original": "abs15",
      "page_count": 0,
      "order": 512,
      "p1": "0",
      "pn": "",
      "abstract": [
        "This paper presents the Speech Technology Center (STC) speaker recognition\n(SR) systems submitted to the VOiCES From a Distance challenge 2019.\nThe challenge&#8217;s SR task is focused on the problem of speaker\nrecognition in single channel distant/far-field audio under noisy conditions.\nIn this work we investigate different deep neural networks architectures\nfor speaker embedding extraction to solve the task. We show that deep\nnetworks with residual frame level connections outperform more shallow\narchitectures. Simple energy based speech activity detector (SAD) and\nautomatic speech recognition (ASR) based SAD are investigated in this\nwork. We also address the problem of data preparation for robust embedding\nextractors training. The reverberation for the data augmentation was\nperformed using automatic room impulse response generator. In our systems\nwe used discriminatively trained cosine similarity metric learning\nmodel as embedding backend. Scores normalization procedure was applied\nfor each individual subsystem we used. Our final submitted systems\nwere based on the fusion of different subsystems. The results obtained\non the VOiCES development and evaluation sets demonstrate effectiveness\nand robustness of the proposed systems when dealing with distant/far-field\naudio under noisy conditions.<br style=\"mso-data-placement:same-cell;\"\n/><br style=\"mso-data-placement:same-cell;\" />\n  This paper also appears\nin session Wed-SS-7-3.\n"
      ],
      "author_area_id": 13,
      "author_area_label": "Special Sessions"
    },
    "matejka19b_interspeech": {
      "authors": [
        [
          "Pavel",
          "Mat\u011bjka"
        ],
        [
          "Old\u0159ich",
          "Plchot"
        ],
        [
          "Hossein",
          "Zeinali"
        ],
        [
          "Ladislav",
          "Mo\u0161ner"
        ],
        [
          "Anna",
          "Silnova"
        ],
        [
          "Luk\u00e1\u0161",
          "Burget"
        ],
        [
          "Ond\u0159ej",
          "Novotn\u00fd"
        ],
        [
          "Ond\u0159ej",
          "Glembek"
        ]
      ],
      "title": "Analysis of BUT Submission in Far-Field Scenarios of VOiCES 2019 Challenge",
      "original": "abs16",
      "page_count": 0,
      "order": 513,
      "p1": "0",
      "pn": "",
      "abstract": [
        "This paper is a post-evaluation analysis of our efforts in VOiCES 2019\nSpeaker Recognition challenge. All systems in the fixed condition are\nbased on x-vectors with different features and DNN topologies. The\nsingle best system reaches minDCF of 0.38 (5.25% EER) and a fusion\nof 3 systems yields minDCF of 0.34 (4.87% EER).We also analyze how\nspeaker verification (SV) systems evolved in last few years and show\nresults also on SITW 2016 Challenge. EER on the core-core condition\nof the SITW 2016 challenge dropped from 5.85% to 1.65% for system fusions\nsubmitted for SITW 2016 and VOiCES 2019, respectively. The less restrictive\nopen condition allowed us to use external data for PLDA adaptation\nand achieve additional small performance improvement. In our submission\nto open condition, we used three x-vector systems and also one system\nbased on i-vectors.<br style=\"mso-data-placement:same-cell;\" /><br\nstyle=\"mso-data-placement:same-cell;\" />\n  This paper also appears\nin session Wed-SS-7-3.\n"
      ],
      "author_area_id": 13,
      "author_area_label": "Special Sessions"
    },
    "medennikov19b_interspeech": {
      "authors": [
        [
          "Ivan",
          "Medennikov"
        ],
        [
          "Yuri",
          "Khokhlov"
        ],
        [
          "Aleksei",
          "Romanenko"
        ],
        [
          "Ivan",
          "Sorokin"
        ],
        [
          "Anton",
          "Mitrofanov"
        ],
        [
          "Vladimir",
          "Bataev"
        ],
        [
          "Andrei",
          "Andrusenko"
        ],
        [
          "Tatiana",
          "Prisyach"
        ],
        [
          "Mariya",
          "Korenevskaya"
        ],
        [
          "Oleg",
          "Petrov"
        ],
        [
          "Alexander",
          "Zatvornitskiy"
        ]
      ],
      "title": "The STC ASR System for the VOiCES from a Distance Challenge 2019",
      "original": "abs17",
      "page_count": 0,
      "order": 514,
      "p1": "0",
      "pn": "",
      "abstract": [
        "This paper is a description of the Speech Technology Center (STC) automatic\nspeech recognition (ASR) system for the &#8220;VOiCES from a Distance\nChallenge 2019&#8221;. We participated in the Fixed condition of the\nASR task, which means that the only training data available was an\n80-hour subset of the LibriSpeech corpus. The main difficulty of the\nchallenge is a mismatch between clean training data and distant noisy\ndevelopment/ evaluation data. In order to tackle this, we applied room\nacoustics simulation and weighted prediction error (WPE) dereverberation.\nWe also utilized well-known speaker adaptation using x-vector speaker\nembeddings, as well as novel room acoustics adaptation with R-vector\nroom impulse response (RIR) embeddings. The system used a lattice-level\ncombination of 6 acoustic models based on different pronunciation dictionaries\nand input features. N-best hypotheses were rescored with 3 neural network\nlanguage models (NNLMs) trained on both words and sub-word units. NNLMs\nwere also explored for out-of-vocabulary (OOV) words handling by means\nof artificial texts generation. The final system achieved Word Error\nRate (WER) of 14.7% on the evaluation data, which is the best result\nin the challenge.<br style=\"mso-data-placement:same-cell;\" /><br style=\"mso-data-placement:same-cell;\"\n/>\n  This paper also appears in session Wed-SS-7-3.\n"
      ],
      "author_area_id": 13,
      "author_area_label": "Special Sessions"
    },
    "chong19b_interspeech": {
      "authors": [
        [
          "Tze Yuang",
          "Chong"
        ],
        [
          "Kye Min",
          "Tan"
        ],
        [
          "Kah Kuan",
          "Teh"
        ],
        [
          "Chang Huai",
          "You"
        ],
        [
          "Hanwu",
          "Sun"
        ],
        [
          "Huy Dat",
          "Tran"
        ]
      ],
      "title": "The I2R&#8217;s ASR System for the VOiCES from a Distance Challenge 2019",
      "original": "abs18",
      "page_count": 0,
      "order": 515,
      "p1": "0",
      "pn": "",
      "abstract": [
        "This paper describes the development of the automatic speech recognition\n(ASR) system for the submission to the VOiCES from a Distance Challenge\n2019. In this challenge, we focused on the fixed condition, where the\ntask is to recognize reverberant and noisy speech based on a limited\namount of clean training data. In our system, the mismatch between\nthe training and testing conditions was reduced by using multi-style\ntraining where the training data was artificially contaminated with\ndifferent reverberation and noise sources. Also, the Weighted Prediction\nError (WPE) algorithm was used to reduce the reverberant effect in\nthe evaluation data. To boost the system performance, acoustic models\nof different neural network architectures were trained and the respective\nsystems were fused to give the final output. Moreover, an LSTM language\nmodel was used to rescore the lattice to compensate the weak n-gram\nmodel trained from only the transcription text. Evaluated on the development\nset, our system showed an average word error rate (WER) of 27.04%.<br\nstyle=\"mso-data-placement:same-cell;\" /><br style=\"mso-data-placement:same-cell;\"\n/>\n  This paper also appears in session Wed-SS-7-3.\n"
      ],
      "author_area_id": 13,
      "author_area_label": "Special Sessions"
    },
    "jati19_interspeech": {
      "authors": [
        [
          "Arindam",
          "Jati"
        ],
        [
          "Raghuveer",
          "Peri"
        ],
        [
          "Monisankha",
          "Pal"
        ],
        [
          "Tae Jin",
          "Park"
        ],
        [
          "Naveen",
          "Kumar"
        ],
        [
          "Ruchir",
          "Travadi"
        ],
        [
          "Panayiotis",
          "Georgiou"
        ],
        [
          "Shrikanth",
          "Narayanan"
        ]
      ],
      "title": "Multi-Task Discriminative Training of Hybrid DNN-TVM Model for Speaker Verification with Noisy and Far-Field Speech",
      "original": "3010",
      "page_count": 5,
      "order": 516,
      "p1": "2463",
      "pn": "2467",
      "abstract": [
        "The paper aims to address the task of speaker verification with single-channel,\nnoisy and far-field speech by learning an embedding or feature representation\nthat is invariant to different acoustic environments. We approach from\ntwo different directions. First, we adopt a newly proposed discriminative\nmodel that hybridizes Deep Neural Network (DNN) and Total Variability\nModel (TVM) with the goal of integrating their strengths. DNN helps\nlearning a unique variable length representation of the feature sequence\nwhile TVM accumulates them into a fixed dimensional vector. Second,\nwe propose a multitask training scheme with cross entropy and triplet\nlosses in order to obtain good classification performance as well as\ndistinctive speaker embeddings. The multi-task training is applied\non both the DNN-TVM model and state-of-the-art x-vector system. The\nresults on the development and evaluation sets of the  VOiCES challenge\nreveal that the proposed multi-task training helps improving models\nthat are solely based on cross entropy, and it works better with DNN-TVM\narchitecture than x-vector for the current task. Moreover, the multi-task\nmodels tend to show complementary relationship with cross entropy models,\nand thus improved performance is observed after fusion.\n"
      ],
      "doi": "10.21437/Interspeech.2019-3010",
      "author_area_id": 13,
      "author_area_label": "Special Sessions"
    },
    "snyder19b_interspeech": {
      "authors": [
        [
          "David",
          "Snyder"
        ],
        [
          "Jes\u00fas",
          "Villalba"
        ],
        [
          "Nanxin",
          "Chen"
        ],
        [
          "Daniel",
          "Povey"
        ],
        [
          "Gregory",
          "Sell"
        ],
        [
          "Najim",
          "Dehak"
        ],
        [
          "Sanjeev",
          "Khudanpur"
        ]
      ],
      "title": "The JHU Speaker Recognition System for the VOiCES 2019 Challenge",
      "original": "2979",
      "page_count": 5,
      "order": 517,
      "p1": "2468",
      "pn": "2472",
      "abstract": [
        "This paper describes the systems developed by the JHU team for the\nspeaker recognition track of the 2019 VOiCES from a Distance Challenge.\nOn this far-field task, we achieved good performance using systems\nbased on state-of-the-art deep neural network (DNN) embeddings. In\nthis paradigm, a DNN maps variable-length speech segments to speaker\nembeddings, called x-vectors, that are then classified using probabilistic\nlinear discriminant analysis (PLDA). Our submissions were composed\nof three x-vector-based systems that differed primarily in the DNN\narchitecture, temporal pooling mechanism, and training objective function.\nOn the evaluation set, our best single-system submission used an extended\ntime-delay architecture, and achieved 0.435 in actual DCF, the primary\nevaluation metric. A fusion of all three x-vector systems was our primary\nsubmission, and it obtained an actual DCF of 0.362.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2979",
      "author_area_id": 13,
      "author_area_label": "Special Sessions"
    },
    "huang19h_interspeech": {
      "authors": [
        [
          "Jonathan",
          "Huang"
        ],
        [
          "Tobias",
          "Bocklet"
        ]
      ],
      "title": "Intel Far-Field Speaker Recognition System for VOiCES Challenge 2019",
      "original": "2894",
      "page_count": 5,
      "order": 518,
      "p1": "2473",
      "pn": "2477",
      "abstract": [
        "This paper describes Intel&#8217;s speaker recognition systems for\nthe VOiCES from a Distance Challenge 2019. Our submission consists\nof a Resnet50, and four Xvector systems trained with different data\naugmentation and input features. Our novel contributions include the\nuse of additive margin softmax loss function and the use of invariant\nrepresentation learning for some of our systems. To our knowledge,\nthis has not been proposed for speaker recognition. We found that such\ncomplementary subsystems greatly improved the performance on the development\nset by late fusion on score level based on linear logistic regression.\nAfter fusion our system achieved on the development set EER, minDCF\nand actDCF of 2.2%, 0.27 and 0.27; and on the evaluation set 6.08%,\n0.451 and 0.458, respectively. We discuss our results and give some\ninsight on accuracy with respect to recording distance.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2894",
      "author_area_id": 13,
      "author_area_label": "Special Sessions"
    },
    "sun19d_interspeech": {
      "authors": [
        [
          "Hanwu",
          "Sun"
        ],
        [
          "Kah Kuan",
          "Teh"
        ],
        [
          "Ivan",
          "Kukanov"
        ],
        [
          "Huy Dat",
          "Tran"
        ]
      ],
      "title": "The I2R&#8217;s Submission to VOiCES Distance Speaker Recognition Challenge 2019",
      "original": "1997",
      "page_count": 5,
      "order": 519,
      "p1": "2478",
      "pn": "2482",
      "abstract": [
        "This paper is about the I2R&#8217;s submission to the VOiCES from a\ndistance speaker recognition challenge 2019. The submissions were based\non the fusion of two x-vectors and two i-vectors subsystems. Main efforts\nhave been focused on the frontend de-reverberation processing, PLDA\nbackend design, score normalization and fusion studies in order to\nimprove the system performance on single channel distant/far-field\naudio, under noisy conditions. We contribute to the fixed condition\ntask under specific training and development data set. The experimental\nresults showed that the de-reverberation approach can achieve 5% to\n10% relative improvement on both EER and DCF for all subsystems and\nmore than 10% improvement in the final fusion system on the Dev dataset\nand more than 15% relative improvement on the final evaluation dataset.\nOur final fusion system achieved about 2% EER rate and 0.240  minDCF\non the Development Dataset.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1997"
    },
    "liang19_interspeech": {
      "authors": [
        [
          "Yulong",
          "Liang"
        ],
        [
          "Lin",
          "Yang"
        ],
        [
          "Xuyang",
          "Wang"
        ],
        [
          "Yingjie",
          "Li"
        ],
        [
          "Chen",
          "Jia"
        ],
        [
          "Junjie",
          "Wang"
        ]
      ],
      "title": "The LeVoice Far-Field Speech Recognition System for VOiCES from a Distance Challenge 2019",
      "original": "1944",
      "page_count": 5,
      "order": 520,
      "p1": "2483",
      "pn": "2487",
      "abstract": [
        "This paper describes our submission to the &#8220;VOiCES from a Distance\nChallenge 2019&#8221;, which is designed to foster research in the\narea of speaker recognition and automatic speech recognition (ASR)\nwith a special focus on single channel distant/far-field audio under\nnoisy conditions. We focused on the ASR task under a fixed condition\nin which the training data was clean and small, but the development\ndata and test data were noisy and unmatched. Thus we developed the\nfollowing major technical points for our system, which included data\naugmentation, weighted-prediction-error based speech enhancement, acoustic\nmodels based on different networks, TDNN or LSTM based language model\nrescore, and ROVER. Experiments on the development set and the evaluation\nset showed that the front-end processing, data augmentation and system\nfusion made the main contributions for the performance increasing,\nand the final word error rate results based on our system scored 15.91%\nand 19.6% respectively.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1944"
    },
    "wang19f_interspeech": {
      "authors": [
        [
          "Yiming",
          "Wang"
        ],
        [
          "David",
          "Snyder"
        ],
        [
          "Hainan",
          "Xu"
        ],
        [
          "Vimal",
          "Manohar"
        ],
        [
          "Phani Sankar",
          "Nidadavolu"
        ],
        [
          "Daniel",
          "Povey"
        ],
        [
          "Sanjeev",
          "Khudanpur"
        ]
      ],
      "title": "The JHU ASR System for VOiCES from a Distance Challenge 2019",
      "original": "1948",
      "page_count": 5,
      "order": 521,
      "p1": "2488",
      "pn": "2492",
      "abstract": [
        "This paper describes the system developed by the JHU team for automatic\nspeech recognition (ASR) of the VOiCES from a Distance Challenge 2019,\nfocusing on single channel distant/farfield audio under noisy conditions.\nWe participated in the Fixed Condition track, where the systems are\nonly trained on an 80-hour subset of the Librispeech corpus provided\nby the organizer. The training data was first augmented with both background\nnoises and simulated reverberation. We then trained factorized TDNN\nacoustic models that differed only in their use of i-vectors for adaptation.\nBoth systems utilized RNN language models trained on original and reversed\ntext for rescoring. We submitted three systems: the system using i-vectors\nwith WER 19.4% on the development set, the system without i-vectors\nthat achieved WER 19.0%, and the their lattice-level fusion with WER\n17.8%. On the evaluation set, our best system achieves 23.9% WER.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1948",
      "author_area_id": 13,
      "author_area_label": "Special Sessions"
    },
    "cai19c_interspeech": {
      "authors": [
        [
          "Danwei",
          "Cai"
        ],
        [
          "Xiaoyi",
          "Qin"
        ],
        [
          "Weicheng",
          "Cai"
        ],
        [
          "Ming",
          "Li"
        ]
      ],
      "title": "The DKU System for the Speaker Recognition Task of the 2019 VOiCES from a Distance Challenge",
      "original": "1435",
      "page_count": 5,
      "order": 522,
      "p1": "2493",
      "pn": "2497",
      "abstract": [
        "In this paper, we present the DKU system for the speaker recognition\ntask of the VOiCES from a distance challenge 2019. We investigate the\nwhole system pipeline for the far-field speaker verification, including\ndata pre-processing, short-term spectral feature representation, utterance-level\nspeaker modeling, backend scoring, and score normalization. Our best\nsingle system employs a residual neural network trained with angular\nsoftmax loss. Also, the weighted prediction error algorithms can further\nimprove performance. It achieves 0.3668 minDCF and 5.58% EER on the\nevaluation set by using a simple cosine similarity scoring. Finally,\nthe submitted primary system obtains 0.3532 minDCF and 4.96% EER on\nthe evaluation set.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1435",
      "author_area_id": 13,
      "author_area_label": "Special Sessions"
    },
    "hauptman19_interspeech": {
      "authors": [
        [
          "Yermiyahu",
          "Hauptman"
        ],
        [
          "Ruth",
          "Aloni-Lavi"
        ],
        [
          "Itshak",
          "Lapidot"
        ],
        [
          "Tanya",
          "Gurevich"
        ],
        [
          "Yael",
          "Manor"
        ],
        [
          "Stav",
          "Naor"
        ],
        [
          "Noa",
          "Diamant"
        ],
        [
          "Irit",
          "Opher"
        ]
      ],
      "title": "Identifying Distinctive Acoustic and Spectral Features in Parkinson&#8217;s Disease",
      "original": "2465",
      "page_count": 5,
      "order": 523,
      "p1": "2498",
      "pn": "2502",
      "abstract": [
        "In this paper we try to identify spectral and acoustic features that\nare distinctive of Parkinson&#8217;s disease patients&#8217; speech.\nWe investigate the contribution of several features&#8217; families\nto a simple classification task that distinguishes between two balanced\ngroups &#8212; patients with Parkinson&#8217;s disease and their age\nand gender matched group of Healthy Controls, both uttering sustained\nvowels. We achieve over 75% correct classification using a combination\nof acoustic and spectral features. We show that combining a few statistical\nfunctionals of these features yields very good results. This can be\nexplained by two reasons: the first is that the statistics of Parkinson&#8217;s\ndisease patients&#8217; speech defer from those of Healthy people&#8217;s\nspeech; the second and more important one is the gradual nature of\nthe Parkinsonian speech that is manifested by the changes within an\nutterance. We speculate that the feature families that most contribute\nto the classification task are the most distinctive for detecting the\ndisease and suggest testing this hypothesis by performing long-term\nanalysis of both patient and healthy control subjects. Similar accuracy\nis obtained when analyzing spontaneous speech where each utterance\nis represented by a single normalized i-vector.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2465",
      "author_area_id": 13,
      "author_area_label": "Special Sessions"
    },
    "drioli19_interspeech": {
      "authors": [
        [
          "Carlo",
          "Drioli"
        ],
        [
          "Philipp",
          "Aichinger"
        ]
      ],
      "title": "Aerodynamics and Lumped-Masses Combined with Delay Lines for Modeling Vertical and Anterior-Posterior Phase Differences in Pathological Vocal Fold Vibration",
      "original": "2338",
      "page_count": 5,
      "order": 524,
      "p1": "2503",
      "pn": "2507",
      "abstract": [
        "We discuss the representation of anterior-posterior (A-P) phase differences\nin vocal cord oscillations through a numerical biomechanical model\ninvolving lumped elements as well as distributed elements, i.e., delay\nlines. A dynamic glottal source model is illustrated in which the fold\ndisplacement along the vertical and the longitudinal dimensions is\nexplicitly modeled by numerical waveguide components representing the\npropagation on the fold cover tissue. In contrast to other models of\nthe same class, in which the reproduction of longitudinal phase differences\nare intrinsically impossible (e.g., in two-mass models) or not easy\nto control explicitly (e.g., in 3D 16-mass and multi-mass models in\ngeneral), the one proposed here provides direct control over the amount\nof phase delay between folds oscillations at the posterior and anterior\nside of the glottis, while keeping the dynamic model simple and computationally\nefficient. The model is assessed by addressing the reproduction of\ntypical oscillatory patterns observed in high-speed videoendoscopic\ndata, in which A-P phase differences are observed. Experimental results\nare provided which demonstrate the ability of the approach to effectively\nreproduce different oscillatory patterns of the vocal folds.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2338",
      "author_area_id": 13,
      "author_area_label": "Special Sessions"
    },
    "kadiri19_interspeech": {
      "authors": [
        [
          "Sudarsana Reddy",
          "Kadiri"
        ],
        [
          "Paavo",
          "Alku"
        ]
      ],
      "title": "Mel-Frequency Cepstral Coefficients of Voice Source Waveforms for Classification of Phonation Types in Speech",
      "original": "2863",
      "page_count": 5,
      "order": 525,
      "p1": "2508",
      "pn": "2512",
      "abstract": [
        "Voice source characteristics in different phonation types vary due\nto the tension of laryngeal muscles along with the respiratory effort.\nThis study investigates the use of mel-frequency cepstral coefficients\n(MFCCs) derived from voice source waveforms for classification of phonation\ntypes in speech. The cepstral coefficients are computed using two source\nwaveforms: (1) glottal flow waveforms estimated by the quasi-closed\nphase (QCP) glottal inverse filtering method and (2) approximate voice\nsource waveforms obtained using the zero frequency filtering (ZFF)\nmethod. QCP estimates voice source waveforms based on the source-filter\ndecomposition while ZFF yields source waveforms without explicitly\ncomputing the source-filter decomposition. Experiments using MFCCs\ncomputed from the two source waveforms show improved accuracy in classification\nof phonation types compared to the existing voice source features and\nconventional MFCC features. Further, it is observed that the proposed\nfeatures have complimentary information to the existing features.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2863"
    },
    "cho19_interspeech": {
      "authors": [
        [
          "Sunghye",
          "Cho"
        ],
        [
          "Mark",
          "Liberman"
        ],
        [
          "Neville",
          "Ryant"
        ],
        [
          "Meredith",
          "Cola"
        ],
        [
          "Robert T.",
          "Schultz"
        ],
        [
          "Julia",
          "Parish-Morris"
        ]
      ],
      "title": "Automatic Detection of Autism Spectrum Disorder in Children Using Acoustic and Text Features from Brief Natural Conversations",
      "original": "1452",
      "page_count": 5,
      "order": 526,
      "p1": "2513",
      "pn": "2517",
      "abstract": [
        "Autism Spectrum Disorder (ASD) is increasingly prevalent [1], but long\nwaitlists hinder children&#8217;s access to expedient diagnosis and\ntreatment. To begin addressing this problem, we developed an automated\nsystem to detect ASD using acoustic and text features drawn from short,\nunstructured conversations with na&#239;ve conversation partners (confederates).\nSeventy children (35 with ASD and 35 typically developing (TD)) discussed\na range of generic topics (e.g., pets, family, hobbies, and sports)\nwith confederates for approximately 5 minutes. A total of 624 features\n(352 acoustic + 272 text) were incorporated into a Gradient Boosting\nModel. To reduce dimensionality and avoid overfitting, we dropped insignificant\nfeatures and applied feature reduction using Principal Component Analysis.\nOur final model was accurate substantially above chance levels. Predictive\nfeatures were both acoustic-phonetic and lexical, from both participants\nand confederates. The goal of this project is to develop an automatic\ndetection system for ASD that relies on very brief, generic, and natural\nconversations, which can eventually be used for ASD prescreening and\ntriage in real-world settings such as doctor&#8217;s offices and schools.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1452"
    },
    "schoentgen19_interspeech": {
      "authors": [
        [
          "Jean",
          "Schoentgen"
        ],
        [
          "Philipp",
          "Aichinger"
        ]
      ],
      "title": "Analysis and Synthesis of Vocal Flutter and Vocal Jitter",
      "original": "1998",
      "page_count": 5,
      "order": 527,
      "p1": "2518",
      "pn": "2522",
      "abstract": [
        "Perturbations of the strict periodicity of the glottal vibrations are\nrelevant features of the voice quality of normophonic and dysphonic\nspeakers. Vocal perturbations in healthy speakers are assigned different\nnames according to the range of the typical perturbation frequencies.\nThe objective of the presentation is to model jitter and flutter, which\nare in the &#62; 20Hz and 10Hz &#8211; 20Hz range respectively, via\na simulation of the fluctuations of the tension of the thyro-arytenoid\nmuscle and compare simulated perturbations to jitter and flutter observed\nin vowels sustained by normophonic speakers. Perturbations of the strict\nperiodicity of the glottal vibrations are relevant features of the\nvoice quality of normophonic and dysphonic speakers. Vocal perturbations\nin healthy speakers are assigned different names according to the range\nof the typical perturbation frequencies. The objective of the presentation\nis to model jitter and flutter, which are in the &#62; 20Hz and 10Hz\n&#8211; 20Hz range respectively, via a simulation of the fluctuations\nof the tension of the thyro-arytenoid muscle and compare simulated\nperturbations to jitter and flutter observed in vowels sustained by\nnormophonic speakers.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1998",
      "author_area_id": 2,
      "author_area_label": "Phonetics, Phonology, and Prosody"
    },
    "schaeffler19_interspeech": {
      "authors": [
        [
          "Felix",
          "Schaeffler"
        ],
        [
          "Stephen",
          "Jannetts"
        ],
        [
          "Janet",
          "Beck"
        ]
      ],
      "title": "Reliability of Clinical Voice Parameters Captured with Smartphones &#8212; Measurements of Added Noise and Spectral Tilt",
      "original": "2910",
      "page_count": 5,
      "order": 528,
      "p1": "2523",
      "pn": "2527",
      "abstract": [
        "Smartphones have become powerful tools for data capture due to their\ncomputational power, internet connectivity, high quality sensors and\nuser-friendly interfaces. This also makes them attractive for the recording\nof voice data that can be analysed for clinical or other voice health\npurposes. This however requires detailed assessment of the reliability\nof voice parameters extracted from smartphone recordings. In a previous\nstudy we analysed reliability of measures of periodicity and periodicity\ndeviation, with very mixed results across parameters. In the present\nstudy we extended this analysis to measures of added noise and spectral\ntilt. We analysed systematic and random error for six frequently used\nacoustic parameters in clinical acoustic voice quality analysis. 22\nspeakers recorded sustained [a] and a short passage with a studio microphone\nand four popular smartphones simultaneously. Acoustic parameters were\nextracted with Praat and smartphone recordings were compared to the\nstudio microphone. Results indicate a small systematic error for almost\nall parameters and smartphones. Random errors differed substantially\nbetween parameters. Our results suggest that extraction of acoustic\nvoice parameters with mobile phones is not without problems and different\nparameters show substantial differences in reliability. Careful individual\nassessment of parameters is therefore recommended before use in practice.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2910",
      "author_area_id": 13,
      "author_area_label": "Special Sessions"
    },
    "moore19b_interspeech": {
      "authors": [
        [
          "Meredith",
          "Moore"
        ],
        [
          "Michael",
          "Saxon"
        ],
        [
          "Hemanth",
          "Venkateswara"
        ],
        [
          "Visar",
          "Berisha"
        ],
        [
          "Sethuraman",
          "Panchanathan"
        ]
      ],
      "title": "Say What? A Dataset for Exploring the Error Patterns That Two ASR Engines Make",
      "original": "3096",
      "page_count": 5,
      "order": 529,
      "p1": "2528",
      "pn": "2532",
      "abstract": [
        "We present a new metadataset which provides insight into where and\nhow two ASR systems make errors on several different speech datasets.\nBy making this data readily available to researchers, we hope to stimulate\nresearch in the area of WER estimation models, in order to gain a deeper\nunderstanding of how intelligibility is encoded in speech. Using this\ndataset, we attempt to estimate intelligibility using a state-of-the-art\nmodel for speech quality estimation and found that this model did not\nwork to model speech intelligibility. This finding sheds light on the\nrelationship between how speech quality is encoded in acoustic features\nand how intelligibility is encoded. It shows that we have a lot more\nto learn in how to effectively model intelligibility. It is our hope\nthat the metadataset we present will stimulate research into creating\nsystems that more effectively model intelligibility.\n"
      ],
      "doi": "10.21437/Interspeech.2019-3096",
      "author_area_id": 13,
      "author_area_label": "Special Sessions"
    },
    "ward19_interspeech": {
      "authors": [
        [
          "Nigel G.",
          "Ward"
        ]
      ],
      "title": "Survey Talk: Prosody Research and Applications: The State of the Art",
      "original": "abs20",
      "page_count": 0,
      "order": 530,
      "p1": "0",
      "pn": "",
      "abstract": [
        "Prosody is essential in human interaction and relevant to every area\nof speech science and technology. Our understanding of prosody, although\nstill fragmentary, is rapidly advancing. This survey will give non-specialists\nthe knowledge needed to decide whether and how to integrate prosodic\ninformation into their models and systems. It will start with the basics:\nthe paralinguistic, phonological and pragmatic functions of prosody,\nits physiology and perception, commonly and less-commonly-used prosodic\nfeatures, and the three main approaches to modeling prosody. Regarding\npractical applications, it will overview ways to use prosody in speech\nrecognition, speech synthesis, dialog systems, and the inference of\nspeaker states and traits. Recent trends will then be presented, including\nmodeling pitch as more than a single scalar value, modeling prosody\nbeyond just intonation, representing prosodic knowledge with constructions\nof multiple prosodic features in specific temporal configurations,\nmodeling observed prosody as the result of the superposition of patterns\nrepresenting independent intents, modeling multi-speaker phenomenon,\nand the use of unsupervised methods. Finally, we will consider remaining\nchallenges in research and applications.\n"
      ]
    },
    "roessig19_interspeech": {
      "authors": [
        [
          "Simon",
          "Roessig"
        ],
        [
          "Doris",
          "M\u00fccke"
        ],
        [
          "Lena",
          "Pagel"
        ]
      ],
      "title": "Dimensions of Prosodic Prominence in an Attractor Model",
      "original": "2227",
      "page_count": 5,
      "order": 531,
      "p1": "2533",
      "pn": "2537",
      "abstract": [
        "Speakers of intonation languages use bundles of cues to express prosodic\nprominence. This work contributes further evidence for the multi-dimensionality\nof prosodic prominence in German reporting articulatory (3D EMA) and\nacoustic recordings from 27 speakers. In particular, we show that speakers\nuse specific categorical and continuous modifications of the laryngeal\nsystem (tonal onglide) as well as continuous modifications of the supra-laryngeal\nsystem (lip aperture and tongue body position) to mark focus structure\nprosodically. These modifications are found between unaccented and\naccented but also within the group of accented words, revealing that\nspeakers use prosodic modulations to directly encode prominence. On\nthe basis of these findings we develop a dynamical model of prosodic\npatterns that is able to capture the manipulations as the modulation\nof an attractor landscape that is shaped by the different prosodic\ndimensions involved.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2227",
      "author_area_id": 2,
      "author_area_label": "Phonetics, Phonology, and Prosody"
    },
    "suni19_interspeech": {
      "authors": [
        [
          "Antti",
          "Suni"
        ],
        [
          "Marcin",
          "W\u0142odarczak"
        ],
        [
          "Martti",
          "Vainio"
        ],
        [
          "Juraj",
          "\u0160imko"
        ]
      ],
      "title": "Comparative Analysis of Prosodic Characteristics Using WaveNet Embeddings",
      "original": "2373",
      "page_count": 5,
      "order": 532,
      "p1": "2538",
      "pn": "2542",
      "abstract": [
        "We present a methodology for assessing similarities and differences\nbetween language varieties and dialects in terms of prosodic characteristics.\nA multi-speaker, multi-dialect WaveNet network is trained on low sample-rate\nsignal retaining only prosodic characteristics of the original speech.\nThe network is conditioned on labels related to speakers&#8217; region\nor dialect. The resulting conditioning embeddings are subsequently\nused as a multi-dimensional characteristics of different language varieties,\nwith results consistent with dialectological studies. The method and\nresults are illustrated on a Swedia 2000 corpus of Swedish dialectal\nvariation.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2373",
      "author_area_id": 2,
      "author_area_label": "Phonetics, Phonology, and Prosody"
    },
    "murphy19_interspeech": {
      "authors": [
        [
          "Andy",
          "Murphy"
        ],
        [
          "Irena",
          "Yanushevskaya"
        ],
        [
          "Ailbhe N\u00ed",
          "Chasaide"
        ],
        [
          "Christer",
          "Gobl"
        ]
      ],
      "title": "The Role of Voice Quality in the Perception of Prominence in Synthetic Speech",
      "original": "2761",
      "page_count": 5,
      "order": 533,
      "p1": "2543",
      "pn": "2547",
      "abstract": [
        "This paper explores how prominence can be modelled in speech synthesis\nthrough voice quality variation. Synthetic utterances varying in voice\nquality (breathy, modal, tense) were generated using a glottal source\nmodel where the global waveshape parameter R<SUB>d</SUB> was the main\ncontrol parameter and f<SUB>0</SUB> was not varied. A manipulation\ntask perception experiment was conducted to establish perceptually\nsalient R<SUB>d</SUB> values in the signalling of focus. The participants\nwere presented with mini-dialogues designed to elicit narrow focus\n(with different focal syllable locations) and were asked to manipulate\nan unknown parameter in the synthetic utterances to produce a natural\nresponse. The results showed that participants manipulated R<SUB>d</SUB>\nnot only in focal syllables, but also in the pre- and postfocal material.\nThe direction of R<SUB>d</SUB> manipulation in the focal syllables\nwas the same across the three voice qualities &#8212; towards decreased\nR<SUB>d</SUB> values (tenser phonation). The magnitude of the decrease\nin R<SUB>d</SUB> was significantly less for tense voice compared to\nbreathy and modal voice, but did not vary with the location of the\nfocal syllable in the utterance. Overall, the results suggest that\nR<SUB>d</SUB> is effective as a control parameter for modelling prominence\nin synthetic speech.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2761",
      "author_area_id": 2,
      "author_area_label": "Phonetics, Phonology, and Prosody"
    },
    "albar19_interspeech": {
      "authors": [
        [
          "Rachel",
          "Albar"
        ],
        [
          "Hiyon",
          "Yoo"
        ]
      ],
      "title": "Phonological Awareness of French Rising Contours in Japanese Learners",
      "original": "2856",
      "page_count": 5,
      "order": 534,
      "p1": "2548",
      "pn": "2552",
      "abstract": [
        "We investigate Japanese learners&#8217; ability to produce and understand\nthe French continuative rising contour. In French, rising contours\ncan be linked to syntactic, metrical, interactional and phrasing functions,\nwhile in Japanese, prosodic boundaries are marked with a default low\ntone (L%).<br style=\"mso-data-placement:same-cell;\" /><br style=\"mso-data-placement:same-cell;\"\n/>\n  Our main hypothesis is that Japanese learners&#8217; proficiency\nis linked to their phonological awareness of rising contours in French.\nWe expect that advanced learners will be able to correctly produce\nrising contours in internal AP and IP positions, and even distinguish\nbetween subtle differences in rising contours.<br style=\"mso-data-placement:same-cell;\"\n/><br style=\"mso-data-placement:same-cell;\" />\n  We present the results\nfrom two different experiments. To test learners&#8217; ability to\nproduce rising contours, subjects were asked to naturally reproduce\nutterances containing violations in certain prosodic contours. Results\nshow that, although the task remains difficult, learners were able\nto correct non-rising contours to varying degrees. We then conducted\na sentence completion task where subjects listened to the beginning\nof a statement and chose the adequate sequence of words that followed\nwhat they had heard. Results show that Japanese learners, no matter\ntheir proficiency, are not able to distinguish the different types\nof rising contours that are dependent on different syntactic boundaries.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2856",
      "author_area_id": 2,
      "author_area_label": "Phonetics, Phonology, and Prosody"
    },
    "okawa19_interspeech": {
      "authors": [
        [
          "Masaki",
          "Okawa"
        ],
        [
          "Takuya",
          "Saito"
        ],
        [
          "Naoki",
          "Sawada"
        ],
        [
          "Hiromitsu",
          "Nishizaki"
        ]
      ],
      "title": "Audio Classification of Bit-Representation Waveform",
      "original": "1855",
      "page_count": 5,
      "order": 535,
      "p1": "2553",
      "pn": "2557",
      "abstract": [
        "This study investigated the waveform representation for audio signal\nclassification. Recently, many studies on audio waveform classification\nsuch as acoustic event detection and music genre classification have\nbeen published. Most studies on audio waveform classification have\nproposed the use of a deep learning (neural network) framework. Generally,\na frequency analysis method such as Fourier transform is applied to\nextract the frequency or spectral information from the input audio\nwaveform before inputting the raw audio waveform into the neural network.\nIn contrast to these previous studies, in this paper, we propose a\nnovel waveform representation method, in which audio waveforms are\nrepresented as a bit sequence, for audio classification. In our experiment,\nwe compare the proposed bit representation waveform, which is directly\ngiven to a neural network, to other representations of audio waveforms\nsuch as a raw audio waveform and a power spectrum with two classification\ntasks: one is an acoustic event classification task and the other is\na sound/music classification task. The experimental results showed\nthat the bit representation waveform achieved the best classification\nperformance for both the tasks.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1855",
      "author_area_id": 5,
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "mulimani19_interspeech": {
      "authors": [
        [
          "Manjunath",
          "Mulimani"
        ],
        [
          "Shashidhar G.",
          "Koolagudi"
        ]
      ],
      "title": "Locality-Constrained Linear Coding Based Fused Visual Features for Robust Acoustic Event Classification",
      "original": "1421",
      "page_count": 5,
      "order": 536,
      "p1": "2558",
      "pn": "2562",
      "abstract": [
        "In this paper, a novel Fused Visual Features (FVFs) are proposed for\nAcoustic Event Classification (AEC) in the meeting room and office\nenvironments. The codes of Visual Features (VFs) are evaluated from\nrow vectors and Scale Invariant Feature Transform (SIFT) vectors of\nthe grayscale Gammatonegram of an acoustic event separately using Locality-constrained\nLinear Coding (LLC). Further, VFs from row vectors and SIFT vectors\nof the grayscale Gammatonegram are fused to get FVFs. Performance of\nthe proposed FVFs is evaluated on acoustic events of publicly available\nUPC-TALP and DCASE datasets in clean and noisy conditions. Results\nshow that proposed FVFs are robust to noise and achieve overall recognition\naccuracy of 96.40% and 90.45% on UPC-TALP and DCASE datasets, respectively.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1421",
      "author_area_id": 5,
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "shen19b_interspeech": {
      "authors": [
        [
          "Yu-Han",
          "Shen"
        ],
        [
          "Ke-Xin",
          "He"
        ],
        [
          "Wei-Qiang",
          "Zhang"
        ]
      ],
      "title": "Learning How to Listen: A Temporal-Frequential Attention Model for Sound Event Detection",
      "original": "2045",
      "page_count": 5,
      "order": 537,
      "p1": "2563",
      "pn": "2567",
      "abstract": [
        "In this paper, we propose a temporal-frequential attention model for\nsound event detection (SED). Our network learns how to listen with\ntwo attention models: a temporal attention model and a frequential\nattention model. Proposed system learns when to listen using the temporal\nattention model while it learns where to listen on the frequency axis\nusing the frequential attention model. With these two models, we attempt\nto make our system pay more attention to important frames or segments\nand important frequency components for sound event detection. Our proposed\nmethod is demonstrated on the task 2 of Detection and Classification\nof Acoustic Scenes and Events (DCASE) 2017 Challenge and outperforms\nstate-of-the-art methods.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2045",
      "author_area_id": 5,
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "ford19_interspeech": {
      "authors": [
        [
          "Logan",
          "Ford"
        ],
        [
          "Hao",
          "Tang"
        ],
        [
          "Fran\u00e7ois",
          "Grondin"
        ],
        [
          "James",
          "Glass"
        ]
      ],
      "title": "A Deep Residual Network for Large-Scale Acoustic Scene Analysis",
      "original": "2731",
      "page_count": 5,
      "order": 538,
      "p1": "2568",
      "pn": "2572",
      "abstract": [
        "Many of the recent advances in audio event detection, particularly\non the AudioSet data set, have focused on improving performance using\nthe released embeddings produced by a pre-trained model. In this work,\nwe instead study the task of training a multi-label event classifier\ndirectly from the audio recordings of AudioSet. Using the audio recordings,\nnot only are we able to reproduce results from prior work, we have\nalso confirmed improvements of other proposed additions, such as an\nattention module. Moreover, by training the embedding network jointly\nwith the additions, we achieve an mAP of 0.392 and an AUC of 0.971,\nsurpassing the state of the art without transfer learning from a large\ndata set. We also analyze the output activations of the network and\nfind that the models are able to localize audio events when a finer\ntime resolution is needed.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2731",
      "author_area_id": 5,
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "reddy19b_interspeech": {
      "authors": [
        [
          "Chandan K.A.",
          "Reddy"
        ],
        [
          "Ross",
          "Cutler"
        ],
        [
          "Johannes",
          "Gehrke"
        ]
      ],
      "title": "Supervised Classifiers for Audio Impairments with Noisy Labels",
      "original": "3074",
      "page_count": 5,
      "order": 539,
      "p1": "2573",
      "pn": "2577",
      "abstract": [
        "Voice-over-Internet-Protocol (VoIP) calls are prone to various speech\nimpairments due to environmental and network conditions resulting in\nbad user experience. A reliable audio impairment classifier helps to\nidentify the cause for bad audio quality. The user feedback after the\ncall can act as the ground truth labels for training a supervised classifier\non a large audio dataset. However, the labels are noisy as most of\nthe users lack the expertise to precisely articulate the impairment\nin the perceived speech. In this paper, we analyze the effects of massive\nnoise in labels in training dense networks and Convolutional Neural\nNetworks (CNN) using engineered features, spectrograms and raw audio\nsamples as inputs. We demonstrate that CNN can generalize better on\nthe training data with a large number of noisy labels and gives remarkably\nhigher test performance. The classifiers were trained both on randomly\ngenerated label noise and the label noise introduced by human errors.\nWe also show that training with noisy labels requires a significant\nincrease in the training dataset size, which is in proportion to the\namount of noise in the labels.\n"
      ],
      "doi": "10.21437/Interspeech.2019-3074",
      "author_area_id": 5,
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "tarantino19_interspeech": {
      "authors": [
        [
          "Lorenzo",
          "Tarantino"
        ],
        [
          "Philip N.",
          "Garner"
        ],
        [
          "Alexandros",
          "Lazaridis"
        ]
      ],
      "title": "Self-Attention for Speech Emotion Recognition",
      "original": "2822",
      "page_count": 5,
      "order": 540,
      "p1": "2578",
      "pn": "2582",
      "abstract": [
        "Speech Emotion Recognition (SER) has been shown to benefit from many\nof the recent advances in deep learning, including recurrent based\nand attention based neural network architectures as well. Nevertheless,\nperformance still falls short of that of humans. In this work, we investigate\nwhether SER could benefit from the self-attention and global windowing\nof the transformer model. We show on the IEMOCAP database that this\nis indeed the case. Finally, we investigate whether using the distribution\nof, possibly conflicting, annotations in the training data, as soft\ntargets could outperform a majority voting. We prove that this performance\nincreases with the agreement level of the annotators.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2822",
      "author_area_id": 5,
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "nachmani19_interspeech": {
      "authors": [
        [
          "Eliya",
          "Nachmani"
        ],
        [
          "Lior",
          "Wolf"
        ]
      ],
      "title": "Unsupervised Singing Voice Conversion",
      "original": "1761",
      "page_count": 5,
      "order": 541,
      "p1": "2583",
      "pn": "2587",
      "abstract": [
        "We present a deep learning method for singing voice conversion. The\nproposed network is not conditioned on the text or on the notes, and\nit directly converts the audio of one singer to the voice of another.\nTraining is performed without any form of supervision: no lyrics or\nany kind of phonetic features, no notes, and no matching samples between\nsingers. The proposed network employs a single CNN encoder for all\nsingers, a single WaveNet decoder, and a classifier that enforces the\nlatent representation to be singer-agnostic. Each singer is represented\nby one embedding vector, which the decoder is conditioned on. In order\nto deal with relatively small datasets, we propose a new data augmentation\nscheme, as well as new training losses and protocols that are based\non backtranslation. Our evaluation presents evidence that the conversion\nproduces natural signing voices that are highly recognizable as the\ntarget singer.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1761",
      "author_area_id": 7,
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "lee19c_interspeech": {
      "authors": [
        [
          "Juheon",
          "Lee"
        ],
        [
          "Hyeong-Seok",
          "Choi"
        ],
        [
          "Chang-Bin",
          "Jeon"
        ],
        [
          "Junghyun",
          "Koo"
        ],
        [
          "Kyogu",
          "Lee"
        ]
      ],
      "title": "Adversarially Trained End-to-End Korean Singing Voice Synthesis System",
      "original": "1722",
      "page_count": 5,
      "order": 542,
      "p1": "2588",
      "pn": "2592",
      "abstract": [
        "In this paper, we propose an end-to-end Korean singing voice synthesis\nsystem from lyrics and a symbolic melody using the following three\nnovel approaches: 1) phonetic enhancement masking, 2) local conditioning\nof text and pitch to the super-resolution network, and 3) conditional\nadversarial training. The proposed system consists of two main modules;\na mel-synthesis network that generates a mel-spectrogram from the given\ninput information, and a super-resolution network that upsamples the\ngenerated mel-spectrogram into a linear-spectrogram. In the mel-synthesis\nnetwork, phonetic enhancement masking is applied to generate implicit\nformant masks solely from the input text, which enables a more accurate\nphonetic control of singing voice. In addition, we show that two other\nproposed methods &#8212; local conditioning of text and pitch, and\nconditional adversarial training &#8212; are crucial for a realistic\ngeneration of the human singing voice in the super-resolution process.\nFinally, both quantitative and qualitative evaluations are conducted,\nconfirming the validity of all proposed methods.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1722",
      "author_area_id": 7,
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "yi19_interspeech": {
      "authors": [
        [
          "Yuan-Hao",
          "Yi"
        ],
        [
          "Yang",
          "Ai"
        ],
        [
          "Zhen-Hua",
          "Ling"
        ],
        [
          "Li-Rong",
          "Dai"
        ]
      ],
      "title": "Singing Voice Synthesis Using Deep Autoregressive Neural Networks for Acoustic Modeling",
      "original": "1563",
      "page_count": 5,
      "order": 543,
      "p1": "2593",
      "pn": "2597",
      "abstract": [
        "This paper presents a method of using autoregressive neural networks\nfor the acoustic modeling of singing voice synthesis (SVS). Singing\nvoice differs from speech and it contains more local dynamic movements\nof acoustic features, e.g., vibratos. Therefore, our method adopts\ndeep autoregressive (DAR) models to predict the F0 and spectral features\nof singing voice in order to better describe the dependencies among\nthe acoustic features of consecutive frames. For F0 modeling, discretized\nF0 values are used and the influences of the history length in DAR\nare analyzed by experiments. An F0 post-processing strategy is also\ndesigned to alleviate the inconsistency between the predicted F0 contours\nand the F0 values determined by music notes. Furthermore, we extend\nthe DAR model to deal with continuous spectral features, and a prenet\nmodule with self-attention layers is introduced to process historical\nframes. Experiments on a Chinese singing voice corpus demonstrate that\nour method using DARs can produce F0 contours with vibratos effectively,\nand can achieve better objective and subjective performance than the\nconventional method using recurrent neural networks (RNNs).\n"
      ],
      "doi": "10.21437/Interspeech.2019-1563",
      "author_area_id": 7,
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "dahmani19_interspeech": {
      "authors": [
        [
          "Sara",
          "Dahmani"
        ],
        [
          "Vincent",
          "Colotte"
        ],
        [
          "Val\u00e9rian",
          "Girard"
        ],
        [
          "Slim",
          "Ouni"
        ]
      ],
      "title": "Conditional Variational Auto-Encoder for Text-Driven Expressive AudioVisual Speech Synthesis",
      "original": "2848",
      "page_count": 5,
      "order": 544,
      "p1": "2598",
      "pn": "2602",
      "abstract": [
        "In recent years, the performance of speech synthesis systems has been\nimproved thanks to deep learning-based models, but generating expressive\naudiovisual speech is still an open issue. The variational auto-encoders\n(VAE)s are recently proposed to learn latent representations of data.\nIn this paper, we present a system for expressive text-to-audiovisual\nspeech synthesis that learns a latent embedding space of emotions using\na conditional generative model based on the variational auto-encoder\nframework. When conditioned on textual input, the VAE is able to learn\nan embedded representation that captures emotion characteristics from\nthe signal, while being invariant to the phonetic content of the utterances.\nWe applied this method in an unsupervised manner to generate duration,\nacoustic and visual features of speech. This conditional variational\nauto-encoder (CVAE) has been used to blend emotions together. This\nmodel was able to generate nuances of a given emotion or to generate\nnew emotions that do not exist in our database. We conducted three\nperceptive experiments to evaluate our findings.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2848",
      "author_area_id": 7,
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "ayllon19b_interspeech": {
      "authors": [
        [
          "David",
          "Ayll\u00f3n"
        ],
        [
          "Fernando",
          "Villavicencio"
        ],
        [
          "Pierre",
          "Lanchantin"
        ]
      ],
      "title": "A Strategy for Improved Phone-Level Lyrics-to-Audio Alignment for Speech-to-Singing Synthesis",
      "original": "3049",
      "page_count": 5,
      "order": 545,
      "p1": "2603",
      "pn": "2607",
      "abstract": [
        "Speech-to-Singing refers to techniques that transform speech to a singing\nvoice. A major performance factor of this process relies on the precision\nto align the phonetic sequence of the input speech to the timing of\nthe target singing. Unfortunately, the precision of existing techniques\nfor phone-level lyrics-to-audio alignment has been found insufficient\nfor this task. We propose a complete pipeline for automatic phone-level\nlyrics-to-audio alignment based on an HMM-based forced-aligner and\nsinging acoustics normalization. The system obtains phone-level precision\nin the range of a few tens of milliseconds as we report in the objective\nevaluation. The subjective evaluation reveals that the smoothness of\nthe singing voice generated with the proposed methodology was found\nclose to the one obtained using manual alignments.\n"
      ],
      "doi": "10.21437/Interspeech.2019-3049",
      "author_area_id": 7,
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "biasuttolervat19_interspeech": {
      "authors": [
        [
          "Th\u00e9o",
          "Biasutto--Lervat"
        ],
        [
          "Sara",
          "Dahmani"
        ],
        [
          "Slim",
          "Ouni"
        ]
      ],
      "title": "Modeling Labial Coarticulation with Bidirectional Gated Recurrent Networks and Transfer Learning",
      "original": "2097",
      "page_count": 5,
      "order": 546,
      "p1": "2608",
      "pn": "2612",
      "abstract": [
        "In this study, we investigate how to learn labial coarticulation to\ngenerate a sparse representation of the face from speech. To do so,\nwe experiment a sequential deep learning model, bidirectional gated\nrecurrent networks, which have reached nice result in addressing the\narticulatory inversion problem and so should be able to handle coarticulation\neffects. As acquiring audiovisual corpora is expensive and time-consuming,\nwe designed our solution to counteract the lack of data. Firstly, we\nhave used phonetic information (phoneme label and respective duration)\nas input to ensure speaker independence, and in second hand, we have\nexperimented around pretraining strategies to reach acceptable performances.\nWe demonstrate how a careful initialization of the last layers of the\nnetwork can greatly ease the training and help to handle coarticulation\neffect. This initialization relies on dimensionality reduction strategies,\nallowing injecting knowledge of useful latent representation of the\nvisual data into the network. We focused on two data-driven tools (PCA\nand autoencoder) and one hand-crafted latent space coming from animation\ncommunity, blendshapes decomposition. We have trained and evaluated\nthe model with a corpus consisting of 4 hours of French speech, and\nwe have gotten an average RMSE close to 1.3mm.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2097",
      "author_area_id": 7,
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "park19e_interspeech": {
      "authors": [
        [
          "Daniel S.",
          "Park"
        ],
        [
          "William",
          "Chan"
        ],
        [
          "Yu",
          "Zhang"
        ],
        [
          "Chung-Cheng",
          "Chiu"
        ],
        [
          "Barret",
          "Zoph"
        ],
        [
          "Ekin D.",
          "Cubuk"
        ],
        [
          "Quoc V.",
          "Le"
        ]
      ],
      "title": "SpecAugment: A Simple Data Augmentation Method for Automatic Speech Recognition",
      "original": "2680",
      "page_count": 5,
      "order": 547,
      "p1": "2613",
      "pn": "2617",
      "abstract": [
        "We present SpecAugment, a simple data augmentation method for speech\nrecognition. SpecAugment is applied directly to the feature inputs\nof a neural network (i.e., filter bank coefficients). The augmentation\npolicy consists of warping the features, masking blocks of frequency\nchannels, and masking blocks of time steps. We apply SpecAugment on\nListen, Attend and Spell networks for end-to-end speech recognition\ntasks. We achieve state-of-the-art performance on the LibriSpeech 960h\nand Switchboard 300h tasks, outperforming all prior work. On LibriSpeech,\nwe achieve 6.8% WER on test-other without the use of a language model,\nand 5.8% WER with shallow fusion with a language model. This compares\nto the previous state-of-the-art hybrid system of 7.5% WER. For Switchboard,\nwe achieve 7.2%/14.6% on the Switchboard/CallHome portion of the Hub5&#8217;00\ntest set without the use of a language model, and 6.8%/14.1% with shallow\nfusion, which compares to the previous state-of-the-art hybrid system\nat 8.3%/17.3% WER.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2680",
      "author_area_id": 8,
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "audhkhasi19_interspeech": {
      "authors": [
        [
          "Kartik",
          "Audhkhasi"
        ],
        [
          "George",
          "Saon"
        ],
        [
          "Zolt\u00e1n",
          "T\u00fcske"
        ],
        [
          "Brian",
          "Kingsbury"
        ],
        [
          "Michael",
          "Picheny"
        ]
      ],
      "title": "Forget a Bit to Learn Better: Soft Forgetting for CTC-Based Automatic Speech Recognition",
      "original": "2841",
      "page_count": 5,
      "order": 548,
      "p1": "2618",
      "pn": "2622",
      "abstract": [
        "Prior work has shown that connectionist temporal classification (CTC)-based\nautomatic speech recognition systems perform well when using bidirectional\nlong short-term memory (BLSTM) networks unrolled over the whole speech\nutterance. This is because whole-utterance BLSTMs better capture long-term\ncontext. We hypothesize that this also leads to overfitting and propose\nsoft forgetting as a solution. During training, we unroll the BLSTM\nnetwork only over small non-overlapping chunks of the input utterance.\nWe randomly pick a chunk size for each batch instead of a fixed global\nchunk size. In order to retain some utterance-level information, we\nencourage the hidden states of the BLSTM network to approximate those\nof a pre-trained whole-utterance BLSTM. Our experiments on the 300-hour\nEnglish Switchboard dataset show that soft forgetting improves the\nword error rate (WER) above a competitive whole-utterance phone CTC\nBLSTM by an average of 7&#8211;9% relative. We obtain WERs of 9.1%/17.4%\nusing speaker-independent and 8.7%/16.8% using speaker-adapted models\nrespectively on the Hub5-2000 Switchboard/CallHome test sets. We also\nshow that soft forgetting improves the WER when the model is used with\nlimited temporal context for streaming recognition. Finally, we present\nsome empirical insights into the regularization and data augmentation\neffects of soft forgetting.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2841",
      "author_area_id": 8,
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "miao19_interspeech": {
      "authors": [
        [
          "Haoran",
          "Miao"
        ],
        [
          "Gaofeng",
          "Cheng"
        ],
        [
          "Pengyuan",
          "Zhang"
        ],
        [
          "Ta",
          "Li"
        ],
        [
          "Yonghong",
          "Yan"
        ]
      ],
      "title": "Online Hybrid CTC/Attention Architecture for End-to-End Speech Recognition",
      "original": "2018",
      "page_count": 5,
      "order": 549,
      "p1": "2623",
      "pn": "2627",
      "abstract": [
        "The hybrid CTC/attention end-to-end automatic speech recognition (ASR)\ncombines CTC ASR system and attention ASR system into a single neural\nnetwork. Although the hybrid CTC/attention ASR system takes the advantages\nof both CTC and attention architectures in training and decoding, it\nremains challenging to be used for streaming speech recognition for\nits attention mechanism, CTC prefix probability and bidirectional encoder.\nIn this paper, we propose a stable monotonic chunkwise attention (sMoChA)\nto stream its attention branch and a truncated CTC prefix probability\n(T-CTC) to stream its CTC branch. On the acoustic model side, we utilize\nthe latency-controlled bidirectional long short-term memory (LC-BLSTM)\nto stream its encoder. On the joint CTC/attention decoding side, we\npropose the dynamic waiting joint decoding (DWDJ) algorithm to collect\nthe decoding hypotheses from the CTC and attention branches. Through\nthe combination of the above methods, we stream the hybrid CTC/attention\nASR system without much word error rate degradation.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2018",
      "author_area_id": 8,
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "zhang19h_interspeech": {
      "authors": [
        [
          "Wei",
          "Zhang"
        ],
        [
          "Xiaodong",
          "Cui"
        ],
        [
          "Ulrich",
          "Finkler"
        ],
        [
          "George",
          "Saon"
        ],
        [
          "Abdullah",
          "Kayi"
        ],
        [
          "Alper",
          "Buyuktosunoglu"
        ],
        [
          "Brian",
          "Kingsbury"
        ],
        [
          "David",
          "Kung"
        ],
        [
          "Michael",
          "Picheny"
        ]
      ],
      "title": "A Highly Efficient Distributed Deep Learning System for Automatic Speech Recognition",
      "original": "2700",
      "page_count": 5,
      "order": 550,
      "p1": "2628",
      "pn": "2632",
      "abstract": [
        "Modern Automatic Speech Recognition (ASR) systems rely on distributed\ndeep learning to for quick training completion. To enable efficient\ndistributed training, it is imperative that the training algorithms\ncan converge with a large mini-batch size. In this work, we discovered\nthat Asynchronous Decentralized Parallel Stochastic Gradient Descent\n(ADPSGD) can work with much larger batch size than commonly used Synchronous\nSGD (SSGD) algorithm. On commonly used public SWB-300 and SWB-2000\nASR datasets, ADPSGD can converge with a batch size 3X as large as\nthe one used in SSGD, thus enable training at a much larger scale.\nFurther, we proposed a Hierarchical-ADPSGD (H-ADPSGD) system in which\nlearners on the same computing node construct a super learner via a\nfast allreduce implementation, and super learners deploy ADPSGD algorithm\namong themselves. On a 64 Nvidia V100 GPU cluster connected via a 100Gb/s\nEthernet network, our system is able to train SWB-2000 to reach a 7.6%\nWER on the Hub5-2000 Switchboard (SWB) test-set and a 13.2% WER on\nthe Call-Home (CH) test-set in 5.2 hours. To the best of our knowledge,\nthis is the fastest ASR training system that attains this level of\nmodel accuracy for SWB-2000 task to be ever reported in the literature.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2700",
      "author_area_id": 8,
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "zhang19i_interspeech": {
      "authors": [
        [
          "Wangyou",
          "Zhang"
        ],
        [
          "Xuankai",
          "Chang"
        ],
        [
          "Yanmin",
          "Qian"
        ]
      ],
      "title": "Knowledge Distillation for End-to-End Monaural Multi-Talker ASR System",
      "original": "3192",
      "page_count": 5,
      "order": 551,
      "p1": "2633",
      "pn": "2637",
      "abstract": [
        "End-to-end models for monaural multi-speaker automatic speech recognition\n(ASR) have become an important and interesting approach when dealing\nwith the multi-talker mixed speech under cocktail party scenario. However,\nthere is still a large performance gap between the multi-speaker and\nsingle-speaker speech recognition systems. In this paper, we propose\na novel framework that integrates teacher-student training with the\nattention-based end-to-end ASR model, which can do the knowledge distillation\nfrom the single-talker ASR system to multi-talker one effectively.\nFirst the objective function is revised to combine the knowledge from\nboth single-talker and multi-talker labels. Then we extend the original\nsingle attention to speaker parallel attention modules in the teacher-student\ntraining based end-to-end framework to boost the performance more.\nMoreover, a curriculum learning strategy on the training data with\nan ordered signal-to-noise ratios (SNRs) is designed to obtain a further\nimprovement. The proposed methods are evaluated on two-speaker mixed\nspeech generated from the WSJ0 corpus, which is commonly used for this\ntask recently. The experimental results show that the newly proposed\nknowledge transfer architecture with an end-to-end model can significantly\nimprove the system performance for monaural multi-talker speech recognition,\nand more than 15% relative WER reduction is achieved against the traditional\nend-to-end model.\n"
      ],
      "doi": "10.21437/Interspeech.2019-3192",
      "author_area_id": 8,
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "menne19_interspeech": {
      "authors": [
        [
          "Tobias",
          "Menne"
        ],
        [
          "Ilya",
          "Sklyar"
        ],
        [
          "Ralf",
          "Schl\u00fcter"
        ],
        [
          "Hermann",
          "Ney"
        ]
      ],
      "title": "Analysis of Deep Clustering as Preprocessing for Automatic Speech Recognition of Sparsely Overlapping Speech",
      "original": "1728",
      "page_count": 5,
      "order": 552,
      "p1": "2638",
      "pn": "2642",
      "abstract": [
        "Significant performance degradation of automatic speech recognition\n(ASR) systems is observed when the audio signal contains cross-talk.\nOne of the recently proposed approaches to solve the problem of multi-speaker\nASR is the deep clustering (DPCL) approach. Combining DPCL with a state-of-the-art\nhybrid acoustic model, we obtain a word error rate (WER) of 16.5% on\nthe commonly used wsj0-2mix dataset, which is the best performance\nreported thus far to the best of our knowledge. The wsj0-2mix dataset\ncontains simulated cross-talk where the speech of multiple speakers\noverlaps for almost the entire utterance. In a more realistic ASR scenario\nthe audio signal contains significant portions of single-speaker speech\nand only part of the signal contains speech of multiple competing speakers.\nThis paper investigates obstacles of applying DPCL as a preprocessing\nmethod for ASR in such a scenario of sparsely overlapping speech. To\nthis end we present a data simulation approach, closely related to\nthe wsj0-2mix dataset, generating sparsely overlapping speech datasets\nof arbitrary overlap ratio. The analysis of applying DPCL to sparsely\noverlapping speech is an important interim step between the fully overlapping\ndatasets like wsj0-2mix and more realistic ASR datasets, such as CHiME-5\nor AMI.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1728",
      "author_area_id": 8,
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "bradlow19_interspeech": {
      "authors": [
        [
          "Ann R.",
          "Bradlow"
        ]
      ],
      "title": "Survey Talk: Recognition of Foreign-Accented Speech: Challenges and Opportunities for Human and Computer Speech Communication",
      "original": "abs21",
      "page_count": 0,
      "order": 553,
      "p1": "0",
      "pn": "",
      "abstract": [
        "This presentation will consider the causes, characteristics, and consequences\nof second-language (L2) speech production through the lens of a talker-listener\nalignment model. Rather than focusing on L2 speech as deviant from\nthe L1 target, this model views speech communication as a cooperative\nactivity in which interlocutors adjust their speech production and\nperception in a bi-directional, dynamic manner. Three lines of support\nwill be presented. First, principled accounts of salient acoustic-phonetic\nmarkers of L2 speech will be developed with reference to language-general\nchallenges of L2 speech production and to language-specific L1-L2 structural\ninteractions. Next, we will examine recognition of L2 speech by listeners\nfrom various language backgrounds, noting in particular that for L2\nlisteners, L2 speech can be equally (or sometimes, more) intelligible\nthan L1 speech. Finally, we will examine perceptual adaptation to L2\nspeech by L1 listeners, highlighting studies that focused on interactive,\ndialogue-based test settings where we can observe the dynamics of talker\nadaptation to the listener and vice versa. Throughout this survey,\nI will refer to current methodological and technical developments in\ncorpus-based phonetics and interactive testing paradigms that open\nnew windows on the dynamics of speech communication across a language\nbarrier.\n"
      ]
    },
    "novak19_interspeech": {
      "authors": [
        [
          "John S.",
          "Novak"
        ],
        [
          "Daniel",
          "Bunn"
        ],
        [
          "Robert V.",
          "Kenyon"
        ]
      ],
      "title": "The Effects of Time Expansion on English as a Second Language Individuals",
      "original": "2763",
      "page_count": 5,
      "order": 554,
      "p1": "2643",
      "pn": "2647",
      "abstract": [
        "When speaking to second language learners, talkers often reduce their\nrate of speech to assist their listeners&#8217; understanding and comprehension.\nThis study grants English as a Second Language subjects fine-grained,\nreal-time control over the playback rates of lengthy audio tracks of\nconversational speech, and tests the subjects&#8217; listening comprehension\nat their desired playback speeds and at unmodified speeds. We find\nevidence that slower playback rates are preferred, but no evidence\nthat such playback rates affect listener comprehension.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2763",
      "author_area_id": 1,
      "author_area_label": "Speech Perception, Production and Acquisition"
    },
    "shi19_interspeech": {
      "authors": [
        [
          "Shuju",
          "Shi"
        ],
        [
          "Chilin",
          "Shih"
        ],
        [
          "Jinsong",
          "Zhang"
        ]
      ],
      "title": "Capturing L1 Influence on L2 Pronunciation by Simulating Perceptual Space Using Acoustic Features",
      "original": "3183",
      "page_count": 5,
      "order": 555,
      "p1": "2648",
      "pn": "2652",
      "abstract": [
        "Theories of second language (L2) acquisition of phonology / phonetics\n/ pronunciation / accent often resort to the similarity/ dissimilarity\nbetween the first language (L1) and L2 sound inventories. Measuring\nthe similarity of two speech sounds could involve many acoustic dimensions,\ne.g., fundamental frequency (F0), formants, duration, etc.. The measurement\nof the sound inventories of two languages can be further complicated\nby the distribution of sounds within each inventory as well as the\ninteraction of phonology and phonetics between the two inventories.\nThis paper attempts to propose a tentative approach to quantify similarity/dissimilarity\nof sound pairs between two language inventories and to incorporate\nphonological influence in the acoustic measures used. The language\npairs studied are English and Mandarin Chinese and only their vowel\ninventories are considered. Mel-Frequency Cepstral Coefficients (MFCCs)\nare used as features, and Principle Component Analysis (PCA) is used\nand slightly adjusted to simulate the perceptual space. Similarity/dissimilarity\nof sound pairs between the language inventories are examined and potential\nL2 error patterns are predicted based on the proposed approach. Results\nshowed that predicted results using the proposed approach can be well\nrelated to those by Speech Learning Model (SLM), Perceptual Assimilation\nModel for L2 (PAM-L2) and Native Language Magnet Model (NLM).\n"
      ],
      "doi": "10.21437/Interspeech.2019-3183",
      "author_area_id": 1,
      "author_area_label": "Speech Perception, Production and Acquisition"
    },
    "chen19h_interspeech": {
      "authors": [
        [
          "Juqiang",
          "Chen"
        ],
        [
          "Catherine T.",
          "Best"
        ],
        [
          "Mark",
          "Antoniou"
        ]
      ],
      "title": "Cognitive Factors in Thai-Na&#239;ve Mandarin Speakers&#8217; Imitation of Thai Lexical Tones",
      "original": "1403",
      "page_count": 5,
      "order": 556,
      "p1": "2653",
      "pn": "2657",
      "abstract": [
        "The present study investigated how cognitive factors, memory load and\nattention control, affected imitation of Thai tones by Mandarin speakers\nwith no prior Thai experience. Mandarin speakers lengthened the syllable\nduration, enlarged the F0 excursion and moved some F0 max location\nearlier compared with the stimuli, even in the immediate imitation\ncondition. Talker variability had a larger impact on imitation than\nmemory load, whereas vowel variability did not have any effect. Perceptual\nassimilation patterns partially influenced imitation performance, suggesting\nphonological categorization in imitation and a perception-production\nlink.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1403",
      "author_area_id": 1,
      "author_area_label": "Speech Perception, Production and Acquisition"
    },
    "tremblay19_interspeech": {
      "authors": [
        [
          "Annie",
          "Tremblay"
        ],
        [
          "Mirjam",
          "Broersma"
        ]
      ],
      "title": "Foreign-Language Knowledge Enhances Artificial-Language Segmentation",
      "original": "2446",
      "page_count": 5,
      "order": 557,
      "p1": "2658",
      "pn": "2662",
      "abstract": [
        "This study investigates whether and how foreign-language knowledge\naffects the use of non-native cues in speech segmentation. It does\nso by testing whether Dutch listeners&#8217; French knowledge enhances\ntheir use of word-final fundamental-frequency (F0) rise &#8212; consistent\nwith the typical French prosodic pattern &#8212; in artificial-language\n(AL) speech segmentation. More specifically, this study examines whether\nDutch listeners with good French knowledge outperform Dutch listeners\nwith limited French knowledge in the selection of AL words over (nonword\nor partword) foils, following exposure to an AL with word-final F0\nrises. Dutch listeners with good French knowledge completed the AL-segmentation\ntask from Kim et al.&#8217;s [2] word-final F0-rise condition. The\nresults were compared to Kim et al.&#8217;s [2] Dutch listeners with\nlimited French knowledge and Tremblay et al.&#8217;s [1] native French\nlisteners in the same condition. Dutch listeners with good French knowledge\nperformed more accurately than Dutch listeners with limited French\nknowledge but less accurately than native French listeners on trials\nwith partword foils, with the three groups not differing on trials\nwith nonword foils. Given these results, we propose that foreign-language\nknowledge can help listeners compute the conditional probability of\nco-occurrence of successive syllables in an AL and can thus enhance\nAL speech segmentation.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2446",
      "author_area_id": 1,
      "author_area_label": "Speech Perception, Production and Acquisition"
    },
    "abujabal19_interspeech": {
      "authors": [
        [
          "Abdalghani",
          "Abujabal"
        ],
        [
          "Judith",
          "Gaspers"
        ]
      ],
      "title": "Neural Named Entity Recognition from Subword Units",
      "original": "1305",
      "page_count": 5,
      "order": 558,
      "p1": "2663",
      "pn": "2667",
      "abstract": [
        "Named entity recognition (NER) is a vital task in spoken language understanding,\nwhich aims to identify mentions of named entities in text e.g., from\ntranscribed speech. Existing neural models for NER rely mostly on dedicated\nword-level representations, which suffer from two main shortcomings.\nFirst, the vocabulary size is large, yielding large memory requirements\nand training time. Second, these models are not able to learn morphological\nor phonological representations. To remedy the above shortcomings,\nwe adopt a neural solution based on bidirectional LSTMs and conditional\nrandom fields, where we rely on subword units, namely  characters,\n phonemes, and  bytes. For each word in an utterance, our model learns\na representation from each of the subword units. We conducted experiments\nin a real-world large-scale setting for the use case of a voice-controlled\ndevice covering four languages with up to 5.5M utterances per language.\nOur experiments show that (1) with increasing training data, performance\nof models trained solely on subword units becomes closer to that of\nmodels with dedicated word-level embeddings (91.35 vs 93.92 F1 for\nEnglish), while using a much smaller vocabulary size (332 vs 74K),\n(2) subword units enhance models with dedicated word-level embeddings,\nand (3) combining different subword units improves performance.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1305",
      "author_area_id": 12,
      "author_area_label": "Spoken Language Processing: Translation, Information Retrieval, Summarization, Resources and Evaluation"
    },
    "bhati19_interspeech": {
      "authors": [
        [
          "Saurabhchand",
          "Bhati"
        ],
        [
          "Shekhar",
          "Nayak"
        ],
        [
          "K. Sri Rama",
          "Murty"
        ],
        [
          "Najim",
          "Dehak"
        ]
      ],
      "title": "Unsupervised Acoustic Segmentation and Clustering Using Siamese Network Embeddings",
      "original": "2981",
      "page_count": 5,
      "order": 559,
      "p1": "2668",
      "pn": "2672",
      "abstract": [
        "Unsupervised discovery of acoustic units from the raw speech signal\nforms the core objective of zero-resource speech processing. It involves\nidentifying the acoustic segment boundaries and consistently assigning\nunique labels to acoustically similar segments. In this work, the possible\ncandidates for segment boundaries are identified in an unsupervised\nmanner from the kernel Gram matrix computed from the Mel-frequency\ncepstral coefficients (MFCC). These segment boundary candidates are\nused to train a siamese network, that is intended to learn embeddings\nthat minimize intrasegment distances and maximize the intersegment\ndistances. The siamese embeddings capture phonetic information from\nlonger contexts of the speech signal and enhance the intersegment discriminability.\nThese properties make the siamese embeddings better suited for acoustic\nsegmentation and clustering than the raw MFCC features. The Gram matrix\ncomputed from the siamese embeddings provides unambiguous evidence\nfor boundary locations. The initial candidate boundaries are refined\nusing this evidence, and siamese embeddings are extracted for the new\nacoustic segments. A graph growing approach is used to cluster the\nsiamese embeddings, and a unique label is assigned to acoustically\nsimilar segments. The performance of the proposed method for acoustic\nsegmentation and clustering is evaluated on Zero Resource 2017 database.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2981",
      "author_area_id": 12,
      "author_area_label": "Spoken Language Processing: Translation, Information Retrieval, Summarization, Resources and Evaluation"
    },
    "yusuf19b_interspeech": {
      "authors": [
        [
          "Bolaji",
          "Yusuf"
        ],
        [
          "Murat",
          "Saraclar"
        ]
      ],
      "title": "An Empirical Evaluation of DTW Subsampling Methods for Keyword Search",
      "original": "2413",
      "page_count": 5,
      "order": 560,
      "p1": "2673",
      "pn": "2677",
      "abstract": [
        "State of the art vocabulary-independent spoken term detection methods\nare typically based on variants of the dynamic time warping (DTW) algorithm\nsince DTW, being based on acoustic sequence matching, allows robust\nretrieval in settings with scarcity of linguistic resources. However,\nthe DTW comes with a high computational cost which limits its practicality\nin a deployed server. To this end, we investigate the efficacy of subsampling\nand propose a neural network architecture to reduce the computational\nload of DTW-based keyword search. We use a time-subsampled RNN to reduce\nthe frame rate of the document as well as the dimensionality of representation\nwhile training it to maintain the cost incurred along the DTW alignment\npath, thus allowing us to reduce the computational complexity (both\nspace and time) of the search algorithm.<br style=\"mso-data-placement:same-cell;\"\n/><br style=\"mso-data-placement:same-cell;\" />\n  Experiments on the\nTurkish and Zulu limited language packs of the IARPA Babel program\nshow that the proposed methods allow considerable reduction in CPU\ntime (88 times) and memory usage (18 times) without significant loss\nin search accuracy (0.0270 ATWV). Moreover, even at very high compression\nlevels with lower search precision, high recall rates are maintained,\nallowing the potential of multi-resolution search.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2413",
      "author_area_id": 12,
      "author_area_label": "Spoken Language Processing: Translation, Information Retrieval, Summarization, Resources and Evaluation"
    },
    "yang19e_interspeech": {
      "authors": [
        [
          "Zixiaofan",
          "Yang"
        ],
        [
          "Julia",
          "Hirschberg"
        ]
      ],
      "title": "Linguistically-Informed Training of Acoustic Word Embeddings for Low-Resource Languages",
      "original": "3119",
      "page_count": 5,
      "order": 561,
      "p1": "2678",
      "pn": "2682",
      "abstract": [
        "Acoustic word embeddings have been proven to be useful in query-by-example\nkeyword search. Such embeddings are typically trained to distinguish\nthe same word from a different word using exact orthographic representations;\nso, two different words will have dissimilar embeddings even if they\nare pronounced similarly or share the same stem. However, in real-world\napplications such as keyword search in low-resource languages, models\nare expected to find all derived and inflected forms for a certain\nkeyword. In this paper, we address this mismatch by incorporating linguistic\ninformation when training neural acoustic word embeddings. We propose\ntwo linguistically-informed methods for training these embeddings,\nboth of which, when we use metrics that consider non-exact matches,\noutperform state-of-the-art models on the Switchboard dataset. We also\npresent results on Sinhala to show that models trained on English can\nbe directly transferred to embed spoken words in a very different language\nwith high accuracy.\n"
      ],
      "doi": "10.21437/Interspeech.2019-3119",
      "author_area_id": 12,
      "author_area_label": "Spoken Language Processing: Translation, Information Retrieval, Summarization, Resources and Evaluation"
    },
    "wang19g_interspeech": {
      "authors": [
        [
          "Liming",
          "Wang"
        ],
        [
          "Mark A.",
          "Hasegawa-Johnson"
        ]
      ],
      "title": "Multimodal Word Discovery and Retrieval with Phone Sequence and Image Concepts",
      "original": "1487",
      "page_count": 5,
      "order": 562,
      "p1": "2683",
      "pn": "2687",
      "abstract": [
        "This paper demonstrates three different systems capable of performing\nthe multimodal word discovery task. A multimodal word discovery system\naccepts, as input, a database of spoken descriptions of images (or\na set of corresponding phone transcripts), and learns a lexicon which\nis a mapping from phone strings to their associated image concepts.\nThree systems are demonstrated: one based on a statistical machine\ntranslation (SMT) model, two based on neural machine translation (NMT).\nOn Flickr8k, the SMT-based model performs much better than the NMT-based\none, achieving a 49.6% F1 score. Finally, we apply our word discovery\nsystem to the task of image retrieval and achieve 29.1% recall@10 on\nthe standard 1000-image Flickr8k tests set.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1487",
      "author_area_id": 12,
      "author_area_label": "Spoken Language Processing: Translation, Information Retrieval, Summarization, Resources and Evaluation"
    },
    "boito19_interspeech": {
      "authors": [
        [
          "Marcely Zanon",
          "Boito"
        ],
        [
          "Aline",
          "Villavicencio"
        ],
        [
          "Laurent",
          "Besacier"
        ]
      ],
      "title": "Empirical Evaluation of Sequence-to-Sequence Models for Word Discovery in Low-Resource Settings",
      "original": "2029",
      "page_count": 5,
      "order": 563,
      "p1": "2688",
      "pn": "2692",
      "abstract": [
        "Since Bahdanau et al. [1] first introduced attention for neural machine\ntranslation, most sequence-to-sequence models made use of attention\nmechanisms [2, 3, 4]. While they produce soft-alignment matrices that\ncould be interpreted as alignment between target and source languages,\nwe lack metrics to quantify their quality, being unclear which approach\nproduces the best alignments. This paper presents an empirical evaluation\nof 3 of the main sequence-to-sequence models for word discovery from\nunsegmented phoneme sequences: CNN, RNN and Transformer-based. This\ntask consists in aligning word sequences in a source language with\nphoneme sequences in a target language, inferring from it word segmentation\non the target side [5]. Evaluating word segmentation quality can be\nseen as an extrinsic evaluation of the soft-alignment matrices produced\nduring training. Our experiments in a low-resource scenario on Mboshi\nand English languages (both aligned to French) show that RNNs surprisingly\noutperform CNNs and Transformer for this task. Our results are confirmed\nby an intrinsic evaluation of alignment quality through the use Average\nNormalized Entropy (ANE). Lastly, we improve our best word discovery\nmodel by using an alignment entropy confidence measure that accumulates\nANE over all the occurrences of a given alignment pair in the collection.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2029",
      "author_area_id": 12,
      "author_area_label": "Spoken Language Processing: Translation, Information Retrieval, Summarization, Resources and Evaluation"
    },
    "xue19b_interspeech": {
      "authors": [
        [
          "Wei",
          "Xue"
        ],
        [
          "Ying",
          "Tong"
        ],
        [
          "Guohong",
          "Ding"
        ],
        [
          "Chao",
          "Zhang"
        ],
        [
          "Tao",
          "Ma"
        ],
        [
          "Xiaodong",
          "He"
        ],
        [
          "Bowen",
          "Zhou"
        ]
      ],
      "title": "Direct-Path Signal Cross-Correlation Estimation for Sound Source Localization in Reverberation",
      "original": "1488",
      "page_count": 5,
      "order": 564,
      "p1": "2693",
      "pn": "2697",
      "abstract": [
        "Sound source localization (SSL) is challenging in presence of reverberation\nsince the cross-correlation between the direct-path signals in different\nmicrophones, which indicates the spatial information of the sound source,\nis interfered by the reverberation signal components. A novel algorithm\nis proposed in this paper to estimate the cross-correlation of the\n direct-path speech signals, such that the robustness of SSL to reverberation\ncan be improved. The proposed method follows a similar scheme to the\nmultichannel linear prediction (MCLP), which is commonly used for speech\ndereverberation, while avoids the explicit estimation of the direct-path\nsignal of each channel. This is achieved by revealing the relationship\nbetween the direct-path signal cross-correlation (DPCC) and the MCLP\ncoefficient vector, and finally deriving the DPCC by using only the\nmultichannel reverberant signals. It is also shown that the pre-whitening\noperation, which is widely used for SSL, can be inherently integrated\ninto the estimated DPCC. An adaptive method is further derived to facilitate\nonline frame-level SSL. The proposed method can be easily applied to\nconventional cross-correlation based SSL methods by using the DPCC\nrather than the full cross-correlation. Experiments conducted in various\nreverberant conditions demonstrate the effectiveness of the proposed\nmethod.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1488",
      "author_area_id": 5,
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "grondin19_interspeech": {
      "authors": [
        [
          "Fran\u00e7ois",
          "Grondin"
        ],
        [
          "James",
          "Glass"
        ]
      ],
      "title": "Multiple Sound Source Localization with SVD-PHAT",
      "original": "2653",
      "page_count": 5,
      "order": 565,
      "p1": "2698",
      "pn": "2702",
      "abstract": [
        "This paper introduces a modification of phase transform on singular\nvalue decomposition (SVD-PHAT) to localize multiple sound sources.\nThis work aims to improve localization accuracy and keeps the algorithm\ncomplexity low for real-time applications. This method relies on multiple\nscans of the search space, with projection of each low-dimensional\nobservation onto orthogonal subspaces. We show that this method localizes\nmultiple sound sources more accurately than discrete SRP-PHAT, with\na reduction in the Root Mean Square Error up to 0.0395 radians.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2653",
      "author_area_id": 5,
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "zhang19j_interspeech": {
      "authors": [
        [
          "Wangyou",
          "Zhang"
        ],
        [
          "Ying",
          "Zhou"
        ],
        [
          "Yanmin",
          "Qian"
        ]
      ],
      "title": "Robust DOA Estimation Based on Convolutional Neural Network and Time-Frequency Masking",
      "original": "3158",
      "page_count": 5,
      "order": 566,
      "p1": "2703",
      "pn": "2707",
      "abstract": [
        "In the scenario with noise and reverberation, the performance of current\nmethods for direction of arrival (DOA) estimation usually degrades\nsignificantly. Inspired by the success of time-frequency masking in\nspeech enhancement and speech separation, this paper proposes new methods\nto better utilize time-frequency masking in convolution neural network\nto improve the robustness of localization. First a mask estimation\nnetwork is developed to assist DOA estimation by either appending or\nmultiplying the estimated masks to the original input feature. Then\nwe further propose a multi-task learning architecture to optimize the\nmask and DOA estimation networks jointly, and two modes are designed\nand compared. Experiments show that all the proposed methods have better\nrobustness and generalization in noisy and reverberant conditions compared\nto the conventional methods, and the multi-task methods have the best\nperformance among all approaches.\n"
      ],
      "doi": "10.21437/Interspeech.2019-3158",
      "author_area_id": 5,
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "masuyama19_interspeech": {
      "authors": [
        [
          "Yoshiki",
          "Masuyama"
        ],
        [
          "Masahito",
          "Togami"
        ],
        [
          "Tatsuya",
          "Komatsu"
        ]
      ],
      "title": "Multichannel Loss Function for Supervised Speech Source Separation by Mask-Based Beamforming",
      "original": "1289",
      "page_count": 5,
      "order": 567,
      "p1": "2708",
      "pn": "2712",
      "abstract": [
        "In this paper, we propose two mask-based beamforming methods using\na deep neural network (DNN) trained by multichannel loss functions.\nBeamforming technique using time-frequency (TF)-masks estimated by\na DNN have been applied to many applications where TF-masks are used\nfor estimating spatial covariance matrices. To train a DNN for mask-based\nbeamforming, loss functions designed for monaural speech enhancement/separation\nhave been employed. Although such a training criterion is simple, it\ndoes not directly correspond to the performance of mask-based beamforming.\nTo overcome this problem, we use multichannel loss functions which\nevaluate the estimated spatial covariance matrices based on the multichannel\nItakura&#8211;Saito divergence. DNNs trained by the multichannel loss\nfunctions can be applied to construct several beamformers. Experimental\nresults confirmed their effectiveness and robustness to microphone\nconfigurations.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1289",
      "author_area_id": 5,
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "li19m_interspeech": {
      "authors": [
        [
          "Guanjun",
          "Li"
        ],
        [
          "Shan",
          "Liang"
        ],
        [
          "Shuai",
          "Nie"
        ],
        [
          "Wenju",
          "Liu"
        ],
        [
          "Meng",
          "Yu"
        ],
        [
          "Lianwu",
          "Chen"
        ],
        [
          "Shouye",
          "Peng"
        ],
        [
          "Changliang",
          "Li"
        ]
      ],
      "title": "Direction-Aware Speaker Beam for Multi-Channel Speaker Extraction",
      "original": "1474",
      "page_count": 5,
      "order": 568,
      "p1": "2713",
      "pn": "2717",
      "abstract": [
        "SpeakerBeam is a state-of-the-art method for extracting a speech signal\nof target speaker from a mixture using an adaption utterance. The existing\nmulti-channel SpeakerBeam utilizes the spectral features of the signals\nwith the ignorance of the spatial discriminability of the multi-channel\nprocessing. In this paper, we tightly integrate spectral and spatial\ninformation for target speaker extraction. In the proposed scheme,\na multi-channel mixture signal is firstly filtered into a set of beamformed\nsignals using fixed beam patterns. An attention network is then designed\nto identify the direction of the target speaker and to combine the\nbeamformed signals into an enhanced signal dominated by the target\nspeaker energy. Further, SpeakerBeam inputs the enhanced signal and\noutputs the mask of the target speaker. Finally, the attention network\nand SpeakerBeam are jointly trained. Experimental results demonstrate\nthat the proposed scheme largely improves the existing multi-channel\nSpeakerBeam in low signal-to-interference ratio or same-gender scenarios.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1474",
      "author_area_id": 5,
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "ochiai19_interspeech": {
      "authors": [
        [
          "Tsubasa",
          "Ochiai"
        ],
        [
          "Marc",
          "Delcroix"
        ],
        [
          "Keisuke",
          "Kinoshita"
        ],
        [
          "Atsunori",
          "Ogawa"
        ],
        [
          "Tomohiro",
          "Nakatani"
        ]
      ],
      "title": "Multimodal SpeakerBeam: Single Channel Target Speech Extraction with Audio-Visual Speaker Clues",
      "original": "1513",
      "page_count": 5,
      "order": 569,
      "p1": "2718",
      "pn": "2722",
      "abstract": [
        "Recently, with the advent of deep learning, there has been significant\nprogress in the processing of speech mixtures. In particular, the use\nof neural networks has enabled target speech extraction, which extracts\nspeech signal of a target speaker from a speech mixture by utilizing\nauxiliary clue representing the characteristics of the target speaker.\nFor example, audio clues derived from an auxiliary utterance spoken\nby the target speaker have been used to characterize the target speaker.\nAudio clues should capture the fine-grained characteristic of the target\nspeaker&#8217;s voice (e.g., pitch). Alternatively, visual clues derived\nfrom a video of the target speaker&#8217;s face speaking in the mixture\nhave also been investigated. Visual clues should mainly capture the\nphonetic information derived from lip movements. In this paper, we\npropose a novel target speech extraction scheme that combines audio\nand visual clues about the target speaker to take advantage of the\ninformation provided by both modalities. We introduce an attention\nmechanism that emphasizes the most informative speaker clue at every\ntime frame. Experiments on mixture of two speakers demonstrated that\nour proposed method using audio-visual speaker clues significantly\nimproved the extraction performance compared with the conventional\nmethods using either audio or visual speaker clues.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1513",
      "author_area_id": 5,
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "germain19_interspeech": {
      "authors": [
        [
          "Fran\u00e7ois G.",
          "Germain"
        ],
        [
          "Qifeng",
          "Chen"
        ],
        [
          "Vladlen",
          "Koltun"
        ]
      ],
      "title": "Speech Denoising with Deep Feature Losses",
      "original": "1924",
      "page_count": 5,
      "order": 570,
      "p1": "2723",
      "pn": "2727",
      "abstract": [
        "We present an end-to-end deep learning approach to denoising speech\nsignals by processing the raw waveform directly. Given input audio\ncontaining speech corrupted by an additive background signal, the system\naims to produce a processed signal that contains only the speech content.\nRecent approaches have shown promising results using various deep network\narchitectures. In this paper, we propose to train a fully-convolutional\ncontext aggregation network using a deep feature loss. That loss is\nbased on comparing the internal feature activations in a different\nnetwork, trained for audio classification. Our approach outperforms\nthe state of the art in objective speech quality metrics and in large-scale\nperceptual experiments with human listeners. It also outperforms an\nidentical network trained using traditional regression losses. The\nadvantage of the new approach is particularly pronounced for the hardest\ndata with the most intrusive background noise, for which denoising\nis most needed and most challenging. \n"
      ],
      "doi": "10.21437/Interspeech.2019-1924",
      "author_area_id": 6,
      "author_area_label": "Speech Coding and Enhancement"
    },
    "wang19h_interspeech": {
      "authors": [
        [
          "Quan",
          "Wang"
        ],
        [
          "Hannah",
          "Muckenhirn"
        ],
        [
          "Kevin",
          "Wilson"
        ],
        [
          "Prashant",
          "Sridhar"
        ],
        [
          "Zelin",
          "Wu"
        ],
        [
          "John R.",
          "Hershey"
        ],
        [
          "Rif A.",
          "Saurous"
        ],
        [
          "Ron J.",
          "Weiss"
        ],
        [
          "Ye",
          "Jia"
        ],
        [
          "Ignacio Lopez",
          "Moreno"
        ]
      ],
      "title": "VoiceFilter: Targeted Voice Separation by Speaker-Conditioned Spectrogram Masking",
      "original": "1101",
      "page_count": 5,
      "order": 571,
      "p1": "2728",
      "pn": "2732",
      "abstract": [
        "In this paper, we present a novel system that separates the voice of\na target speaker from multi-speaker signals, by making use of a reference\nsignal from the target speaker. We achieve this by training two separate\nneural networks: (1) A speaker recognition network that produces speaker-discriminative\nembeddings; (2) A spectrogram masking network that takes both noisy\nspectrogram and speaker embedding as input, and produces a mask. Our\nsystem significantly reduces the speech recognition WER on multi-speaker\nsignals, with minimal WER degradation on single-speaker signals.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1101",
      "author_area_id": 6,
      "author_area_label": "Speech Coding and Enhancement"
    },
    "liao19_interspeech": {
      "authors": [
        [
          "Chien-Feng",
          "Liao"
        ],
        [
          "Yu",
          "Tsao"
        ],
        [
          "Xugang",
          "Lu"
        ],
        [
          "Hisashi",
          "Kawai"
        ]
      ],
      "title": "Incorporating Symbolic Sequential Modeling for Speech Enhancement",
      "original": "1777",
      "page_count": 5,
      "order": 572,
      "p1": "2733",
      "pn": "2737",
      "abstract": [
        "In a noisy environment, a lossy speech signal can be automatically\nrestored by a listener if he/she knows the language well. That is,\nwith the built-in knowledge of a &#8220;language model&#8221;, a listener\nmay effectively suppress noise interference and retrieve the target\nspeech signals. Accordingly, we argue that familiarity with the underlying\nlinguistic content of spoken utterances benefits speech enhancement\n(SE) in noisy environments. In this study, in addition to the conventional\nmodeling for learning the acoustic noisy-clean speech mapping, an abstract\nsymbolic sequential modeling is incorporated into the SE framework.\nThis symbolic sequential modeling can be regarded as a &#8220;linguistic\nconstraint&#8221; in learning the acoustic noisy-clean speech mapping\nfunction. In this study, the symbolic sequences for acoustic signals\nare obtained as discrete representations with a Vector Quantized Variational\nAutoencoder algorithm. The obtained symbols are able to capture high-level\nphoneme-like content from speech signals. The experimental results\ndemonstrate that the proposed framework can obtain notable performance\nimprovement in terms of perceptual evaluation of speech quality (PESQ)\nand short-time objective intelligibility (STOI) on the TIMIT dataset.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1777",
      "author_area_id": 6,
      "author_area_label": "Speech Coding and Enhancement"
    },
    "mowlaee19_interspeech": {
      "authors": [
        [
          "Pejman",
          "Mowlaee"
        ],
        [
          "Daniel",
          "Scheran"
        ],
        [
          "Johannes",
          "Stahl"
        ],
        [
          "Sean U.N.",
          "Wood"
        ],
        [
          "W. Bastiaan",
          "Kleijn"
        ]
      ],
      "title": "Maximum a posteriori Speech Enhancement Based on Double Spectrum",
      "original": "1197",
      "page_count": 5,
      "order": 573,
      "p1": "2738",
      "pn": "2742",
      "abstract": [
        "While the acoustic frequency domain has been widely used for speech\nenhancement, usage of the modulation domain is less common. In this\npaper, we investigate single-channel speech enhancement in the recently\nproposed Double Spectrum (DS) framework and provide insights on the\nstatistical properties of speech and noise in the DS domain. Relying\non our statistical analysis in the DS, we derive a maximum a posteriori\nestimator of speech in the DS domain. By means of experiments, we evaluate\nthe speech enhancement performance of the proposed method and relevant\nbenchmarks in the acoustic frequency and modulation domains and show\nthat the proposed method achieves a good balance between noise attenuation\nand speech distortion for various SNRs and noise types.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1197",
      "author_area_id": 6,
      "author_area_label": "Speech Coding and Enhancement"
    },
    "yao19_interspeech": {
      "authors": [
        [
          "Jian",
          "Yao"
        ],
        [
          "Ahmad",
          "Al-Dahle"
        ]
      ],
      "title": "Coarse-to-Fine Optimization for Speech Enhancement",
      "original": "2792",
      "page_count": 5,
      "order": 574,
      "p1": "2743",
      "pn": "2747",
      "abstract": [
        "In this paper, we propose the coarse-to-fine optimization for the task\nof speech enhancement. Cosine similarity loss [1] has proven to be\nan effective metric to measure similarity of speech signals. However,\ndue to the large variance of the enhanced speech with even the same\ncosine similarity loss in high dimensional space, a deep neural network\nlearnt with this loss might not be able to predict enhanced speech\nwith good quality. Our coarse-to-fine strategy optimizes the cosine\nsimilarity loss for different granularities so that more constraints\nare added to the prediction from high dimension to relatively low dimension.\nIn this way, the enhanced speech will better resemble the clean speech.\nExperimental results show the effectiveness of our proposed coarse-to-fine\noptimization in both discriminative models and generative models. Moreover,\nwe apply the coarse-to-fine strategy to the adversarial loss in generative\nadversarial network (GAN) and propose dynamic perceptual loss, which\ndynamically computes the adversarial loss from coarse resolution to\nfine resolution. Dynamic perceptual loss further improves the accuracy\nand achieves state-of-the-art results compared with other generative\nmodels.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2792",
      "author_area_id": 6,
      "author_area_label": "Speech Coding and Enhancement"
    },
    "hui19b_interspeech": {
      "authors": [
        [
          "Like",
          "Hui"
        ],
        [
          "Siyuan",
          "Ma"
        ],
        [
          "Mikhail",
          "Belkin"
        ]
      ],
      "title": "Kernel Machines Beat Deep Neural Networks on Mask-Based Single-Channel Speech Enhancement",
      "original": "1344",
      "page_count": 5,
      "order": 575,
      "p1": "2748",
      "pn": "2752",
      "abstract": [
        "We apply a fast kernel method for mask-based single-channel speech\nenhancement. Specifically, our method solves a kernel regression problem\nassociated to a non-smooth kernel function (exponential power kernel)\nwith a highly efficient iterative method (EigenPro). Due to the simplicity\nof this method, its hyper-parameters such as kernel bandwidth can be\nautomatically and efficiently selected using line search with subsamples\nof training data. We observe an empirical correlation between the regression\nloss (mean square error) and regular metrics for speech enhancement.\nThis observation justifies our training target and motivates us to\nachieve lower regression loss by training separate kernel models for\ndifferent frequency subbands. We compare our method with the state-of-the-art\ndeep neural networks on mask-based HINT and TIMIT. Experimental results\nshow that our kernel method consistently outperforms deep neural networks\nwhile requiring less training time.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1344",
      "author_area_id": 6,
      "author_area_label": "Speech Coding and Enhancement"
    },
    "metze19_interspeech": {
      "authors": [
        [
          "Florian",
          "Metze"
        ]
      ],
      "title": "Survey Talk: Multimodal Processing of Speech and Language",
      "original": "abs22",
      "page_count": 0,
      "order": 576,
      "p1": "0",
      "pn": "",
      "abstract": [
        "Human information processing is inherently multimodal. Speech and language\nare therefore best processed and generated in a situated context. Future\nhuman language technologies must be able to jointly process multimodal\ndata, and not just text, images, acoustics or speech in isolation.\nDespite advances in Computer Vision, Automatic Speech Recognition,\nMultimedia Analysis and Natural Language Processing, state-of-the-art\ncomputational models are not integrating multiple modalities nowhere\nnear as effectively and efficiently as humans. Researchers are only\nbeginning to tackle these challenges in &#8220;vision and language&#8221;\nresearch. In this talk, I will show the potential of multi-modal processing\nto (1) improve recognition for challenging conditions (i.e. lip-reading),\n(2) adapt models to new conditions (i.e. context or personalization),\n(3) ground semantics across modalities or languages (i.e. translation\nand language acquisition), (4) training models with weak or non-existent\nlabels (i.e. SoundNet or bootstrapping of recognizers without parallel\ndata), and (5) make models interpretable (i.e. representation learning).\nI will present and discuss significant recent research results from\neach of these areas and will highlight the commonalities and differences.\nI hope to stimulate exchange and cross-fertilization of ideas by presenting\nnot just abstract concepts, but by pointing the audience to new and\nexisting tasks, datasets, and challenges.\n"
      ]
    },
    "shrivastava19_interspeech": {
      "authors": [
        [
          "Nilay",
          "Shrivastava"
        ],
        [
          "Astitwa",
          "Saxena"
        ],
        [
          "Yaman",
          "Kumar"
        ],
        [
          "Rajiv Ratn",
          "Shah"
        ],
        [
          "Amanda",
          "Stent"
        ],
        [
          "Debanjan",
          "Mahata"
        ],
        [
          "Preeti",
          "Kaur"
        ],
        [
          "Roger",
          "Zimmermann"
        ]
      ],
      "title": "MobiVSR : Efficient and Light-Weight Neural Network for Visual Speech Recognition on Mobile Devices",
      "original": "3273",
      "page_count": 5,
      "order": 577,
      "p1": "2753",
      "pn": "2757",
      "abstract": [
        "Visual speech recognition (VSR) is the task of recognizing spoken language\nfrom video input only, without any audio. VSR has many applications\nas an assistive technology, especially if it could be deployed in mobile\ndevices and embedded systems. The need for intensive computational\nresources and large memory footprint are two major obstacles in deploying\nneural network models for VSR in a resource constrained environment.\nWe propose a novel end-to-end deep neural network architecture for\nword level VSR called MobiVSR with a design parameter that aids in\nbalancing the model&#8217;s accuracy and parameter count. We use depthwise\n3D convolution along with channel shuffling for the first time in the\ndomain of VSR and show how it makes our model efficient. MobiVSR achieves\nan accuracy of 70% on a challenging Lip Reading in the Wild dataset\nwith 6 times fewer parameters and 20 times smaller memory footprint\nthan the current state of the art. MobiVSR can also be compressed to\n6 MB by applying post training quantization.\n"
      ],
      "doi": "10.21437/Interspeech.2019-3273",
      "author_area_id": 10,
      "author_area_label": "Speech Recognition: Technologies and Systems for New Applications"
    },
    "kandala19_interspeech": {
      "authors": [
        [
          "Pujitha Appan",
          "Kandala"
        ],
        [
          "Abhinav",
          "Thanda"
        ],
        [
          "Dilip Kumar",
          "Margam"
        ],
        [
          "Rohith Chandrashekar",
          "Aralikatti"
        ],
        [
          "Tanay",
          "Sharma"
        ],
        [
          "Sharad",
          "Roy"
        ],
        [
          "Shankar M.",
          "Venkatesan"
        ]
      ],
      "title": "Speaker Adaptation for Lip-Reading Using Visual Identity Vectors",
      "original": "3237",
      "page_count": 5,
      "order": 578,
      "p1": "2758",
      "pn": "2762",
      "abstract": [
        "Visual speech recognition or lip-reading suffers from high word error\nrate (WER) as lip-reading is based solely on articulators that are\nvisible to the camera. Recent works mitigated this problem using complex\narchitectures of deep neural networks. I-vector based speaker adaptation\nis a well known technique in ASR systems used to reduce WER on unseen\nspeakers. In this work, we explore speaker adaptation of lip-reading\nmodels using latent identity vectors (visual i-vectors) obtained by\nfactor analysis on visual features. In order to estimate the visual\ni-vectors, we employ two ways to collect sufficient statistics: first\nusing GMM based universal background model (UBM) and second using RNN-HMM\nbased UBM. The speaker-specific visual i-vector is given as an additional\ninput to the hidden layers of the lip-reading model during train and\ntest phases. On GRID corpus, use of visual i-vectors results in 15%\nand 10% relative improvements over current state of the art lip-reading\narchitectures on unseen speakers using RNN-HMM and GMM based methods\nrespectively. Furthermore, we explore the variation of WER with dimension\nof visual i-vectors, and with the amount of unseen speaker data required\nfor visual i-vector estimation. We also report the results on Korean\nvisual corpus that we created.\n"
      ],
      "doi": "10.21437/Interspeech.2019-3237",
      "author_area_id": 10,
      "author_area_label": "Speech Recognition: Technologies and Systems for New Applications"
    },
    "koumparoulis19_interspeech": {
      "authors": [
        [
          "Alexandros",
          "Koumparoulis"
        ],
        [
          "Gerasimos",
          "Potamianos"
        ]
      ],
      "title": "MobiLipNet: Resource-Efficient Deep Learning Based Lipreading",
      "original": "2618",
      "page_count": 5,
      "order": 579,
      "p1": "2763",
      "pn": "2767",
      "abstract": [
        "Recent works in visual speech recognition utilize deep learning advances\nto improve accuracy. Focus however has been primarily on recognition\nperformance, while ignoring the computational burden of deep architectures.\nIn this paper we address these issues concurrently, aiming at both\nhigh computational efficiency and recognition accuracy in lipreading.\nFor this purpose, we investigate the MobileNet convolutional neural\nnetwork architectures, recently proposed for image classification.\nIn addition, we extend the 2D convolutions of MobileNets to 3D ones,\nin order to better model the spatio-temporal nature of the lipreading\nproblem. We investigate two architectures in this extension, introducing\nthe temporal dimension as part of either the depthwise or the pointwise\nMobileNet convolutions. To further boost computational efficiency,\nwe also consider using pointwise convolutions alone, as well as networks\noperating on half the mouth region. We evaluate the proposed architectures\non speaker-independent visual-only continuous speech recognition on\nthe popular TCD-TIMIT corpus. Our best system outperforms a baseline\nCNN by 4.27% absolute in word error rate and over 12 times in computational\nefficiency, whereas, compared to a state-of-the-art ResNet, it is 37\ntimes more efficient at a minor 0.07% absolute error rate degradation.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2618",
      "author_area_id": 10,
      "author_area_label": "Speech Recognition: Technologies and Systems for New Applications"
    },
    "qu19_interspeech": {
      "authors": [
        [
          "Leyuan",
          "Qu"
        ],
        [
          "Cornelius",
          "Weber"
        ],
        [
          "Stefan",
          "Wermter"
        ]
      ],
      "title": "LipSound: Neural Mel-Spectrogram Reconstruction for Lip Reading",
      "original": "1393",
      "page_count": 5,
      "order": 580,
      "p1": "2768",
      "pn": "2772",
      "abstract": [
        "Lip reading, also known as visual speech recognition, has recently\nreceived considerable attention. Although advanced feature engineering\nand powerful deep neural network architectures have been proposed for\nthis task, the performance still cannot be competitive with speech\nrecognition tasks using the audio modality as input. This is mainly\nbecause compared with audio, visual features carry less information\nrelevant to word recognition. For example, the voiced sound made while\nthe vocal cords vibrate can be represented by audio but is not reflected\nby mouth or lip movement. In this paper, we map the sequence of mouth\nmovement images directly to mel-spectrogram to reconstruct the speech\nrelevant information. Our proposed architecture consists of two components:\n(a) the mel-spectrogram reconstruction front-end which includes an\nencoder-decoder architecture with attention mechanism to predict mel-spectrogram\nfrom videos; (b) the lip reading back-end consisting of convolutional\nlayers, bi-directional gated recurrent units, and connectionist temporal\nclassification loss, which consumes the generated mel-spectrogram representation\nto predict text transcriptions. The speaker-dependent evaluation results\ndemonstrate that our proposed model not only generates quality mel-spectrograms\nbut also outperforms state-of-the-art models on the GRID benchmark\nlip reading dataset, with 0.843% character error rate and 2.525% word\nerror rate.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1393",
      "author_area_id": 10,
      "author_area_label": "Speech Recognition: Technologies and Systems for New Applications"
    },
    "sainath19_interspeech": {
      "authors": [
        [
          "Tara N.",
          "Sainath"
        ],
        [
          "Ruoming",
          "Pang"
        ],
        [
          "David",
          "Rybach"
        ],
        [
          "Yanzhang",
          "He"
        ],
        [
          "Rohit",
          "Prabhavalkar"
        ],
        [
          "Wei",
          "Li"
        ],
        [
          "Mirk\u00f3",
          "Visontai"
        ],
        [
          "Qiao",
          "Liang"
        ],
        [
          "Trevor",
          "Strohman"
        ],
        [
          "Yonghui",
          "Wu"
        ],
        [
          "Ian",
          "McGraw"
        ],
        [
          "Chung-Cheng",
          "Chiu"
        ]
      ],
      "title": "Two-Pass End-to-End Speech Recognition",
      "original": "1341",
      "page_count": 5,
      "order": 581,
      "p1": "2773",
      "pn": "2777",
      "abstract": [
        "The requirements for many applications of state-of-the-art speech recognition\nsystems include not only low word error rate (WER) but also low latency.\nSpecifically, for many use-cases, the system must be able to decode\nutterances in a streaming fashion and faster than real-time. Recently,\na streaming recurrent neural network transducer (RNN-T) end-to-end\n(E2E) model has shown to be a good candidate for on-device speech recognition,\nwith improved WER and latency metrics compared to conventional on-device\nmodels [1]. However, this model still lags behind a large state-of-the-art\nconventional model in quality [2]. On the other hand, a non-streaming\nE2E Listen, Attend and Spell (LAS) model has shown comparable quality\nto large conventional models [3]. This work aims to bring the quality\nof an E2E streaming model closer to that of a conventional system by\nincorporating a LAS network as a second-pass component, while still\nabiding by latency constraints. Our proposed two-pass model achieves\na 17%&#8211;22% relative reduction in WER compared to RNN-T alone and\nincreases latency by a small fraction over RNN-T.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1341",
      "author_area_id": 8,
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "lam19_interspeech": {
      "authors": [
        [
          "Max W.Y.",
          "Lam"
        ],
        [
          "Jun",
          "Wang"
        ],
        [
          "Xunying",
          "Liu"
        ],
        [
          "Helen",
          "Meng"
        ],
        [
          "Dan",
          "Su"
        ],
        [
          "Dong",
          "Yu"
        ]
      ],
      "title": "Extract, Adapt and Recognize: An End-to-End Neural Network for Corrupted Monaural Speech Recognition",
      "original": "1626",
      "page_count": 5,
      "order": 582,
      "p1": "2778",
      "pn": "2782",
      "abstract": [
        "Automatic speech recognition (ASR) in challenging conditions, such\nas in the presence of interfering speakers or music, remains an unsolved\nproblem. This paper presents Extract, Adapt, and Recognize (EAR), an\nend-to-end neural network that allows fully learnable separation and\nrecognition components towards optimizing the ASR criterion. In between\na state-of-the-art speech separation module as an  extractor and an\nacoustic modeling module as a  recognizer, the EAR introduces an  adaptor,\nwhere adapted acoustic features are learned from the separation outputs\nusing a bi-directional long short term memory network trained to minimize\nthe recognition loss directly. Relative to a conventional joint training\nmodel, the EAR model can achieve 8.5% to 22.3%, and 1.2% to 26.9% word\nerror rate reductions (WERR), under various dBs of music corruption\nand speaker interference respectively. With speaker tracing the WERR\ncan be further promoted to 12.4% to 29.0%.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1626",
      "author_area_id": 8,
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "gowda19_interspeech": {
      "authors": [
        [
          "Dhananjaya",
          "Gowda"
        ],
        [
          "Abhinav",
          "Garg"
        ],
        [
          "Kwangyoun",
          "Kim"
        ],
        [
          "Mehul",
          "Kumar"
        ],
        [
          "Chanwoo",
          "Kim"
        ]
      ],
      "title": "Multi-Task Multi-Resolution Char-to-BPE Cross-Attention Decoder for End-to-End Speech Recognition",
      "original": "3216",
      "page_count": 5,
      "order": 583,
      "p1": "2783",
      "pn": "2787",
      "abstract": [
        "In this paper we present a new hierarchical character to byte-pair\nencoding (C2B) end-to-end neural network architecture for improving\nthe performance of attention based encoder-decoder ASR models. We explore\ndifferent strategies for building the hierarchical C2B models such\nas building the individual blocks one at a time, as well as training\nthe entire model as a monolith in a single step. We show that C2B model\ntrained simultaneously with four losses, two for character and two\nfor BPE sequences help regularize the learning of character sequences\nas well as BPE sequences. The proposed multi-task multi-resolution\nhierarchical architecture improves the WER of a small footprint bidirectional\nfull-attention E2E model on the 960 hours LibriSpeech corpus by around\n15% relative and is comparable to the state-of-the-art performance\nof an almost 3 times bigger model on the same dataset.\n"
      ],
      "doi": "10.21437/Interspeech.2019-3216",
      "author_area_id": 8,
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "han19b_interspeech": {
      "authors": [
        [
          "Kyu J.",
          "Han"
        ],
        [
          "Jing",
          "Huang"
        ],
        [
          "Yun",
          "Tang"
        ],
        [
          "Xiaodong",
          "He"
        ],
        [
          "Bowen",
          "Zhou"
        ]
      ],
      "title": "Multi-Stride Self-Attention for Speech Recognition",
      "original": "1973",
      "page_count": 5,
      "order": 584,
      "p1": "2788",
      "pn": "2792",
      "abstract": [
        "In contrast to the huge success of self-attention based neural networks\nin various NLP tasks, the efficacy of self-attention in speech applications\nis yet limited. This is partly because the full effectiveness of the\nself-attention mechanism could not be achieved without proper down-sampling\nschemes in speech tasks. To address this issue, we propose a new self-attention\nmechanism suitable for speech recognition, namely,  multi-stride self-attention.\nThe proposed multi-stride approach lets each group of heads in self-attention\nprocess speech frames with a unique stride over neighboring frames.\nThus, the entire attention mechanism would not be confined in a fixed\nframe shift and can have diverse contextual views for a given frame\nto determine attention weights more effectively. To validate our proposal\nwe evaluated it on various speech corpora for speech recognition, both\nEnglish and Chinese, and observed a consistent improvement, especially\nin terms of substitution and deletion errors, without the increase\nof model complexity. The average WER improvement of 7.5% (relative)\nobtained by the TDNNs having the multi-stride self-attention layer\nas compared to the baseline TDNN model shows the effectiveness of the\nproposed multi-stride self-attention mechanism.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1973",
      "author_area_id": 8,
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "hu19b_interspeech": {
      "authors": [
        [
          "Shoukang",
          "Hu"
        ],
        [
          "Xurong",
          "Xie"
        ],
        [
          "Shansong",
          "Liu"
        ],
        [
          "Max W.Y.",
          "Lam"
        ],
        [
          "Jianwei",
          "Yu"
        ],
        [
          "Xixin",
          "Wu"
        ],
        [
          "Xunying",
          "Liu"
        ],
        [
          "Helen",
          "Meng"
        ]
      ],
      "title": "LF-MMI Training of Bayesian and Gaussian Process Time Delay Neural Networks for Speech Recognition",
      "original": "2379",
      "page_count": 5,
      "order": 585,
      "p1": "2793",
      "pn": "2797",
      "abstract": [
        "Discriminative training techniques define state-of-the-art performance\nfor deep neural networks (DNNs) based speech recognition systems across\na wide range of tasks. Conventional discriminative training methods\nproduce deterministic DNN parameter estimates. They are inherently\nprone to overfitting, leading to poor generalization when given limited\ntraining data. In order to address this issue, this paper investigates\nthe use of Bayesian learning and Gaussian Process (GP) based hidden\nactivations to replace the deterministic parameter estimates of standard\nlattice-free maximum mutual information (LF-MMI) criterion trained\ntime delay neural network (TDNN) acoustic models. Experiments conducted\non the Switchboard conversational telephone speech recognition tasks\nsuggest the proposed technique consistently outperforms the baseline\nLF-MMI trained TDNN systems using fixed parameter hidden activations.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2379",
      "author_area_id": 8,
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "lu19c_interspeech": {
      "authors": [
        [
          "Liang",
          "Lu"
        ],
        [
          "Eric",
          "Sun"
        ],
        [
          "Yifan",
          "Gong"
        ]
      ],
      "title": "Self-Teaching Networks",
      "original": "1467",
      "page_count": 5,
      "order": 586,
      "p1": "2798",
      "pn": "2802",
      "abstract": [
        "We propose self-teaching networks to improve the generalization capacity\nof deep neural networks. The idea is to generate soft supervision labels\nusing the output layer for training the lower layers of the network.\nDuring the network training, we seek an auxiliary loss that drives\nthe lower layer to mimic the behavior of the output layer. The connection\nbetween the two network layers through the auxiliary loss can help\nthe gradient flow, which works similar to the residual networks. Furthermore,\nthe auxiliary loss also works as a regularizer, which improves the\ngeneralization capacity of the network. We evaluated the self-teaching\nnetwork with deep recurrent neural networks on speech recognition tasks,\nwhere we trained the acoustic model using 30 thousand hours of data.\nWe tested the acoustic model using data collected from 4 scenarios.\nWe show that the self-teaching network can achieve consistent improvements\nand outperform existing methods such as label smoothing and confidence\npenalization.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1467",
      "author_area_id": 8,
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "li19n_interspeech": {
      "authors": [
        [
          "Yuanchao",
          "Li"
        ],
        [
          "Tianyu",
          "Zhao"
        ],
        [
          "Tatsuya",
          "Kawahara"
        ]
      ],
      "title": "Improved End-to-End Speech Emotion Recognition Using Self Attention Mechanism and Multitask Learning",
      "original": "2594",
      "page_count": 5,
      "order": 587,
      "p1": "2803",
      "pn": "2807",
      "abstract": [
        "Accurately recognizing emotion from speech is a necessary yet challenging\ntask due to the variability in speech and emotion. In this paper, we\npropose a speech emotion recognition (SER) method using end-to-end\n(E2E) multitask learning with self attention to deal with several issues.\nFirst, we extract features directly from speech spectrogram instead\nof using traditional hand-crafted features to better represent emotion.\nSecond, we adopt self attention mechanism to focus on the salient periods\nof emotion in speech utterances. Finally, giving consideration to mutual\nfeatures between emotion and gender classification tasks, we incorporate\ngender classification as an auxiliary task by using multitask learning\nto share useful information with emotion classification task. Evaluation\non IEMOCAP (a commonly used database for SER research) demonstrates\nthat the proposed method outperforms the state-of-the-art methods and\nimproves the overall accuracy by an absolute of 7.7% compared to the\nbest existing result.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2594",
      "author_area_id": 3,
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "schmitt19_interspeech": {
      "authors": [
        [
          "Maximilian",
          "Schmitt"
        ],
        [
          "Nicholas",
          "Cummins"
        ],
        [
          "Bj\u00f6rn W.",
          "Schuller"
        ]
      ],
      "title": "Continuous Emotion Recognition in Speech &#8212; Do We Need Recurrence?",
      "original": "2710",
      "page_count": 5,
      "order": 588,
      "p1": "2808",
      "pn": "2812",
      "abstract": [
        "Emotion recognition in speech is a meaningful task in affective computing\nand human-computer interaction. As human emotion is a frequently changing\nstate, it is usually represented as a densely sampled time series of\nemotional dimensions, typically arousal and valence. For this, recurrent\nneural network (RNN) architectures are employed by default when it\ncomes to modelling the contours with deep learning approaches. However,\nthe amount of temporal context required is questionable, and it has\nnot yet been clarified whether the consideration of long-term dependencies\nis actually beneficial. In this contribution, we demonstrate that RNNs\nare not necessary to accomplish the task of time-continuous emotion\nrecognition. Indeed, results gained indicate that deep neural networks\nincorporating less complex convolutional layers can provide more accurate\nmodels. We highlight the pros and cons of recurrent and non-recurrent\napproaches and evaluate our methods on the public SEWA database, which\nwas used as a benchmark in the 2017 and 2018 editions of the Audio-Visual\nEmotion Challenge.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2710",
      "author_area_id": 3,
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "ouyang19_interspeech": {
      "authors": [
        [
          "Anda",
          "Ouyang"
        ],
        [
          "Ting",
          "Dang"
        ],
        [
          "Vidhyasaharan",
          "Sethu"
        ],
        [
          "Eliathamby",
          "Ambikairajah"
        ]
      ],
      "title": "Speech Based Emotion Prediction: Can a Linear Model Work?",
      "original": "3149",
      "page_count": 5,
      "order": 589,
      "p1": "2813",
      "pn": "2817",
      "abstract": [
        "Speech based continuous emotion prediction systems have predominantly\nbeen based on complex non-linear back-ends, with an increasing attention\non long-short term memory recurrent neural networks. While this has\nled to accurate predictions, complex models may suffer from issues\nwith interpretability, model selection and overfitting. In this paper,\nwe demonstrate that a linear model can capture most of the relationship\nbetween speech features and emotion labels in the continuous arousal-valence\nspace. Specifically, an autoregressive exogenous model (ARX) is shown\nto be an effective backend. This approach is validated on three commonly\nused databases, namely RECOLA, SEWA and USC CreativeIT, and shown to\nbe comparable in terms of performance to state-of-the-art LSTM systems.\nMore importantly, this approach allows for the use of well-established\nlinear system theory to aid with model interpretability.\n"
      ],
      "doi": "10.21437/Interspeech.2019-3149",
      "author_area_id": 3,
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "ando19_interspeech": {
      "authors": [
        [
          "Atsushi",
          "Ando"
        ],
        [
          "Ryo",
          "Masumura"
        ],
        [
          "Hosana",
          "Kamiyama"
        ],
        [
          "Satoshi",
          "Kobashikawa"
        ],
        [
          "Yushi",
          "Aono"
        ]
      ],
      "title": "Speech Emotion Recognition Based on Multi-Label Emotion Existence Model",
      "original": "2524",
      "page_count": 5,
      "order": 590,
      "p1": "2818",
      "pn": "2822",
      "abstract": [
        "This paper presents a novel speech emotion recognition method that\naddresses the ambiguous nature of emotions in speech. Most conventional\nmethods assume there is only a single ground truth, the dominant emotion,\nthough utterances can contain multiple emotions. In order to solve\nthis problem, several methods that consider ambiguous emotions (e.g.\nsoft-target training) have been proposed. Unfortunately, training them\nis difficult since they work by estimating the proportions of all emotions.\nThe proposed method improves both frameworks by evaluating the presence\nor absence of each emotion. We expect that it is much easier to estimate\njust presence/absence of emotions rather than trying to determine proportions\nof each, and the deliberate assessment of emotion existence information\nwill help to estimate the proportion of each or dominant class more\nprecisely. The proposed method employs two-step training. Multi-Label\nEmotion Existence (MLEE) model is trained first to estimate whether\neach emotion is present or absent. Then, the dominant emotion recognition\nmodel with hard- or soft-target labels is trained by means of the intermediate\noutputs of the MLEE model so as to utilize cues of emotion existence\nfor inferring the dominant. Experiments demonstrate that the proposed\nmethod outperforms both hard- or soft-target based conventional emotion\nrecognition schemes.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2524",
      "author_area_id": 3,
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "gorrostieta19_interspeech": {
      "authors": [
        [
          "Cristina",
          "Gorrostieta"
        ],
        [
          "Reza",
          "Lotfian"
        ],
        [
          "Kye",
          "Taylor"
        ],
        [
          "Richard",
          "Brutti"
        ],
        [
          "John",
          "Kane"
        ]
      ],
      "title": "Gender De-Biasing in Speech Emotion Recognition",
      "original": "1708",
      "page_count": 5,
      "order": 591,
      "p1": "2823",
      "pn": "2827",
      "abstract": [
        "Machine learning can unintentionally encode and amplify negative bias\nand stereotypes present in humans, be they conscious or unconscious.\nThis has led to high-profile cases where machine learning systems have\nbeen found to exhibit bias towards gender, race, and ethnicity, among\nother demographic categories. Negative bias can be encoded in these\nalgorithms based on: the representation of different population categories\nin the training data; bias arising from manual human labeling of these\ndata; as well as modeling types and optimisation approaches. In this\npaper we assess the effect of gender bias in speech emotion recognition\nand find that emotional activation model accuracy is consistently lower\nfor female compared to male audio samples. Further, we demonstrate\nthat a fairer and more consistent model accuracy can be achieved by\napplying a simple de-biasing training technique.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1708",
      "author_area_id": 3,
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "bao19_interspeech": {
      "authors": [
        [
          "Fang",
          "Bao"
        ],
        [
          "Michael",
          "Neumann"
        ],
        [
          "Ngoc Thang",
          "Vu"
        ]
      ],
      "title": "CycleGAN-Based Emotion Style Transfer as Data Augmentation for Speech Emotion Recognition",
      "original": "2293",
      "page_count": 5,
      "order": 592,
      "p1": "2828",
      "pn": "2832",
      "abstract": [
        "Cycle consistent adversarial networks (CycleGAN) have shown great success\nin image style transfer with unpaired datasets. Inspired by this, we\ninvestigate emotion style transfer to generate synthetic data, which\naims at addressing the data scarcity problem in speech emotion recognition.\nSpecifically, we propose a CycleGAN-based method to transfer feature\nvectors extracted from a large unlabeled speech corpus into synthetic\nfeatures representing the given target emotions. We extend the CycleGAN\nframework with a classification loss which improves the discriminability\nof the generated data. To show the effectiveness of the proposed method,\nwe present results for speech emotion recognition using the generated\nfeature vectors as (i) augmentation of the training data, and (ii)\nas standalone training set. Our experimental results reveal that when\nutilizing synthetic feature vectors, the classification performance\nimproves in within-corpus and cross-corpus evaluation.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2293",
      "author_area_id": 3,
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "bollepalli19_interspeech": {
      "authors": [
        [
          "Bajibabu",
          "Bollepalli"
        ],
        [
          "Lauri",
          "Juvela"
        ],
        [
          "Paavo",
          "Alku"
        ]
      ],
      "title": "Lombard Speech Synthesis Using Transfer Learning in a Tacotron Text-to-Speech System",
      "original": "1333",
      "page_count": 5,
      "order": 593,
      "p1": "2833",
      "pn": "2837",
      "abstract": [
        "Currently, there is increasing interest to use sequence-to-sequence\nmodels in text-to-speech (TTS) synthesis with attention like that in\nTacotron models. These models are end-to-end, meaning that they learn\nboth co-articulation and duration properties directly from text and\nspeech. Since these models are entirely data-driven, they need large\namounts of data to generate synthetic speech of good quality. However,\nin challenging speaking styles, such as Lombard speech, it is difficult\nto record sufficiently large speech corpora. Therefore, we propose\na transfer learning method to adapt a TTS system of normal speaking\nstyle to Lombard style. We also experiment with a WaveNet vocoder along\nwith a traditional vocoder (WORLD) in the synthesis of Lombard speech.\nThe subjective and objective evaluation results indicated that the\nproposed adaptation system coupled with the WaveNet vocoder clearly\noutperformed the conventional deep neural network based TTS system\nin the synthesis of Lombard speech.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1333",
      "author_area_id": 7,
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "seshadri19_interspeech": {
      "authors": [
        [
          "Shreyas",
          "Seshadri"
        ],
        [
          "Lauri",
          "Juvela"
        ],
        [
          "Paavo",
          "Alku"
        ],
        [
          "Okko",
          "R\u00e4s\u00e4nen"
        ]
      ],
      "title": "Augmented CycleGANs for Continuous Scale Normal-to-Lombard Speaking Style Conversion",
      "original": "1681",
      "page_count": 5,
      "order": 594,
      "p1": "2838",
      "pn": "2842",
      "abstract": [
        "Lombard speech is a speaking style associated with increased vocal\neffort that is naturally used by humans to improve intelligibility\nin the presence of noise. It is hence desirable to have a system capable\nof converting speech from normal to Lombard style. Moreover, it would\nbe useful if one could adjust the degree of Lombardness in the converted\nspeech so that the system is more adaptable to different noise environments.\nIn this study, we propose the use of recently developed Augmented cycle-consistent\nadversarial networks (Augmented CycleGANs) for conversion between normal\nand Lombard speaking styles. The proposed system gives a smooth control\non the degree of Lombardness of the mapped utterances by traversing\nthrough different points in the latent space of the trained model.\nWe utilize a parametric approach that uses the Pulse Model in Log domain\n(PML) vocoder to extract features from normal speech that are then\nmapped to Lombard-style features using the Augmented CycleGAN. Finally,\nthe mapped features are converted to Lombard speech with PML. The model\nis trained on multi-language data recorded in different noise conditions,\nand we compare its effectiveness to a previously proposed CycleGAN\nsystem in experiments for intelligibility and quality of mapped speech.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1681",
      "author_area_id": 7,
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "zhao19f_interspeech": {
      "authors": [
        [
          "Guanlong",
          "Zhao"
        ],
        [
          "Shaojin",
          "Ding"
        ],
        [
          "Ricardo",
          "Gutierrez-Osuna"
        ]
      ],
      "title": "Foreign Accent Conversion by Synthesizing Speech from Phonetic Posteriorgrams",
      "original": "1778",
      "page_count": 5,
      "order": 595,
      "p1": "2843",
      "pn": "2847",
      "abstract": [
        "Methods for foreign accent conversion (FAC) aim to generate speech\nthat sounds similar to a given non-native speaker but with the accent\nof a native speaker. Conventional FAC methods borrow excitation information\n(F0 and aperiodicity; produced by a conventional vocoder) from a reference\n(i.e., native) utterance during synthesis time. As such, the generated\nspeech retains some aspects of the voice quality of the native speaker.\nWe present a framework for FAC that eliminates the need for conventional\nvocoders (e.g., STRAIGHT, World) and therefore the need to use the\nnative speaker&#8217;s excitation. Our approach uses an acoustic model\ntrained on a native speech corpus to extract speaker-independent phonetic\nposteriorgrams (PPGs), and then train a speech synthesizer to map PPGs\nfrom the non-native speaker into the corresponding spectral features,\nwhich in turn are converted into the audio waveform using a high-quality\nneural vocoder. At runtime, we drive the synthesizer with the PPG extracted\nfrom a native reference utterance. Listening tests show that the proposed\nsystem produces speech that sounds more clear, natural, and similar\nto the non-native speaker compared with a baseline system, while significantly\nreducing the perceived foreign accent of non-native utterances.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1778",
      "author_area_id": 7,
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "shankar19b_interspeech": {
      "authors": [
        [
          "Ravi",
          "Shankar"
        ],
        [
          "Jacob",
          "Sager"
        ],
        [
          "Archana",
          "Venkataraman"
        ]
      ],
      "title": "A Multi-Speaker Emotion Morphing Model Using Highway Networks and Maximum Likelihood Objective",
      "original": "2512",
      "page_count": 5,
      "order": 596,
      "p1": "2848",
      "pn": "2852",
      "abstract": [
        "We introduce a new model for emotion conversion in speech based on\nhighway neural networks. Our model uses the contextual pitch, energy\nand spectral information of a source emotional utterance to predict\nthe framewise fundamental frequency and signal intensity under a target\nemotion. We also incorporate a latent gender representation to promote\ncross-speaker generalizability. Our neural network is trained to maximize\nthe error log-likelihood under an assumed Laplacian distribution. We\nvalidate our model on the VESUS repository collected at Johns Hopkins\nUniversity, which contains parallel emotional utterances from 10 actors\nacross 5 emotional classes. The proposed algorithm outperforms three\nstate-of-the-art baselines in terms of the mean absolute error and\ncorrelation between the predicted and target values. We evaluate the\nquality of our emotion manipulations via crowd-sourcing. Finally, we\napply our emotion morphing model to utterances generated by Wavenet\nto demonstrate our unique ability to inject emotion into synthetic\nspeech.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2512",
      "author_area_id": 7,
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "lapidot19_interspeech": {
      "authors": [
        [
          "Itshak",
          "Lapidot"
        ],
        [
          "Jean-Fran\u00e7ois",
          "Bonastre"
        ]
      ],
      "title": "Effects of Waveform PMF on Anti-Spoofing Detection",
      "original": "2607",
      "page_count": 5,
      "order": 597,
      "p1": "2853",
      "pn": "2857",
      "abstract": [
        "In the context of detection of speaker recognition identity impersonation,\nwe observed that the waveform  probability mass function (PMF) of genuine\nspeech differs from significantly of of PMF from identity theft extracts.\nThis is true for synthesized or converted speech as well as for replayed\nspeech. In this work, we mainly ask whether this observation has a\nsignificant impact on spoofing detection performance. In a second step,\nwe want to reduce the distribution gap of waveforms between authentic\nspeech and spoofing speech. We propose a  genuinization of the spoofing\nspeech (by analogy with  Gaussianisation), i.e. to obtain spoofing\nspeech with a PMF close to the PMF of genuine speech. Our  genuinization\nis evaluated on ASVspoof 2019 challenge datasets, using the baseline\nsystem provided by the challenge organization. In the case of  constant\nQ cepstral coefficients (CQCC) features, the  genuinization leads to\na degradation of the baseline system performance by a factor of 10,\nwhich shows a potentially large impact of the distribution os waveforms\non spoofing detection performance. However, by &#8220;playing&#8221;\nwith all configurations, we also observed different behaviors, including\nperformance improvements in specific cases. This leads us to conclude\nthat waveform distribution plays an important role and must be taken\ninto account by anti-spoofing systems.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2607",
      "author_area_id": 7,
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "gao19b_interspeech": {
      "authors": [
        [
          "Jian",
          "Gao"
        ],
        [
          "Deep",
          "Chakraborty"
        ],
        [
          "Hamidou",
          "Tembine"
        ],
        [
          "Olaitan",
          "Olaleye"
        ]
      ],
      "title": "Nonparallel Emotional Speech Conversion",
      "original": "2878",
      "page_count": 5,
      "order": 598,
      "p1": "2858",
      "pn": "2862",
      "abstract": [
        "We propose a nonparallel data-driven emotional speech conversion method.\nIt enables the transfer of emotion-related characteristics of a speech\nsignal while preserving the speaker&#8217;s identity and linguistic\ncontent. Most existing approaches require parallel data and time alignment,\nwhich is not available in many real applications. We achieve nonparallel\ntraining based on an unsupervised style transfer technique, which learns\na translation model between two distributions instead of a deterministic\none-to-one mapping between paired examples. The conversion model consists\nof an encoder and a decoder for each emotion domain. We assume that\nthe speech signal can be decomposed into an emotion-invariant content\ncode and an emotion-related style code in latent space. Emotion conversion\nis performed by extracting and recombining the content code of the\nsource speech and the style code of the target emotion. We tested our\nmethod on a nonparallel corpora with four emotions. The evaluation\nresults show the effectiveness of our approach.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2878",
      "author_area_id": 7,
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "stafylakis19_interspeech": {
      "authors": [
        [
          "Themos",
          "Stafylakis"
        ],
        [
          "Johan",
          "Rohdin"
        ],
        [
          "Old\u0159ich",
          "Plchot"
        ],
        [
          "Petr",
          "Mizera"
        ],
        [
          "Luk\u00e1\u0161",
          "Burget"
        ]
      ],
      "title": "Self-Supervised Speaker Embeddings",
      "original": "2842",
      "page_count": 5,
      "order": 599,
      "p1": "2863",
      "pn": "2867",
      "abstract": [
        "Contrary to i-vectors, speaker embeddings such as x-vectors are incapable\nof leveraging unlabelled utterances, due to the classification loss\nover training speakers. In this paper, we explore an alternative training\nstrategy to enable the use of unlabelled utterances in training. We\npropose to train speaker embedding extractors via reconstructing the\nframes of a target speech segment, given the inferred embedding of\nanother speech segment of the same utterance. We do this by attaching\nto the standard speaker embedding extractor a decoder network, which\nwe feed not merely with the speaker embedding, but also with the estimated\nphone sequence of the target frame sequence.<br style=\"mso-data-placement:same-cell;\"\n/><br style=\"mso-data-placement:same-cell;\" />\n  The reconstruction\nloss can be used either as a single objective, or be combined with\nthe standard speaker classification loss. In the latter case, it acts\nas a regularizer, encouraging generalizability to speakers unseen during\ntraining. In all cases, the proposed architectures are trained from\nscratch and in an end-to-end fashion. We demonstrate the benefits from\nthe proposed approach on the VoxCeleb and Speakers in the Wild Databases,\nand we report notable improvements over the baseline.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2842",
      "author_area_id": 4,
      "author_area_label": "Speaker and Language Identification"
    },
    "nautsch19b_interspeech": {
      "authors": [
        [
          "Andreas",
          "Nautsch"
        ],
        [
          "Jose",
          "Patino"
        ],
        [
          "Amos",
          "Treiber"
        ],
        [
          "Themos",
          "Stafylakis"
        ],
        [
          "Petr",
          "Mizera"
        ],
        [
          "Massimiliano",
          "Todisco"
        ],
        [
          "Thomas",
          "Schneider"
        ],
        [
          "Nicholas",
          "Evans"
        ]
      ],
      "title": "Privacy-Preserving Speaker Recognition with Cohort Score Normalisation",
      "original": "2638",
      "page_count": 5,
      "order": 600,
      "p1": "2868",
      "pn": "2872",
      "abstract": [
        "In many voice biometrics applications there is a requirement to preserve\nprivacy, not least because of the recently enforced General Data Protection\nRegulation (GDPR). Though progress in bringing privacy preservation\nto voice biometrics is lagging behind developments in other biometrics\ncommunities, recent years have seen rapid progress, with secure computation\nmechanisms such as homomorphic encryption being applied successfully\nto speaker recognition. Even so, the computational overhead incurred\nby processing speech data in the encrypted domain is substantial. While\nstill tolerable for single biometric comparisons, most state-of-the-art\nsystems perform some form of cohort-based score normalisation, requiring\n many thousands of biometric comparisons. The computational overhead\nis then prohibitive, meaning that one must accept either degraded performance\n(no score normalisation) or potential for privacy violations. This\npaper proposes the first computationally feasible approach to privacy-preserving\ncohort score normalisation. Our solution is a cohort pruning scheme\nbased on secure multi-party computation which enables privacy-preserving\nscore normalisation using probabilistic linear discriminant analysis\n(PLDA) comparisons. The solution operates upon binary voice representations.\nWhile the binarisation is lossy in biometric rank-1 performance, it\nsupports computationally-feasible biometric rank-n comparisons in the\nencrypted domain.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2638",
      "author_area_id": 4,
      "author_area_label": "Speaker and Language Identification"
    },
    "liu19f_interspeech": {
      "authors": [
        [
          "Yi",
          "Liu"
        ],
        [
          "Liang",
          "He"
        ],
        [
          "Jia",
          "Liu"
        ]
      ],
      "title": "Large Margin Softmax Loss for Speaker Verification",
      "original": "2357",
      "page_count": 5,
      "order": 601,
      "p1": "2873",
      "pn": "2877",
      "abstract": [
        "In neural network based speaker verification, speaker embedding is\nexpected to be discriminative between speakers while the intra-speaker\ndistance should remain small. A variety of loss functions have been\nproposed to achieve this goal. In this paper, we investigate the large\nmargin softmax loss with different configurations in speaker verification.\nRing loss and minimum hyperspherical energy criterion are introduced\nto further improve the performance. Results on VoxCeleb show that our\nbest system outperforms the baseline approach by 15% in EER, and by\n13%, 33% in minDCF08 and minDCF10, respectively.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2357",
      "author_area_id": 4,
      "author_area_label": "Speaker and Language Identification"
    },
    "hajavi19_interspeech": {
      "authors": [
        [
          "Amirhossein",
          "Hajavi"
        ],
        [
          "Ali",
          "Etemad"
        ]
      ],
      "title": "A Deep Neural Network for Short-Segment Speaker Recognition",
      "original": "2240",
      "page_count": 5,
      "order": 602,
      "p1": "2878",
      "pn": "2882",
      "abstract": [
        "Today&#8217;s interactive devices such as smart-phone assistants and\nsmart speakers often deal with short-duration speech segments. As a\nresult, speaker recognition systems integrated into such devices will\nbe much better suited with models capable of performing the recognition\ntask with short-duration utterances. In this paper, a new deep neural\nnetwork, UtterIdNet, capable of performing speaker recognition with\nshort speech segments is proposed. Our proposed model utilizes a novel\narchitecture that makes it suitable for short-segment speaker recognition\nthrough an efficiently increased use of information in short speech\nsegments. UtterIdNet has been trained and tested on the VoxCeleb datasets,\nthe latest benchmarks in speaker recognition. Evaluations for different\nsegment durations show consistent and stable performance for short\nsegments, with significant improvement over the previous models for\nsegments of 2 seconds, 1 second, and especially sub-second durations\n(250 ms and 500 ms).\n"
      ],
      "doi": "10.21437/Interspeech.2019-2240",
      "author_area_id": 4,
      "author_area_label": "Speaker and Language Identification"
    },
    "zhou19_interspeech": {
      "authors": [
        [
          "Jianfeng",
          "Zhou"
        ],
        [
          "Tao",
          "Jiang"
        ],
        [
          "Zheng",
          "Li"
        ],
        [
          "Lin",
          "Li"
        ],
        [
          "Qingyang",
          "Hong"
        ]
      ],
      "title": "Deep Speaker Embedding Extraction with Channel-Wise Feature Responses and Additive Supervision Softmax Loss Function",
      "original": "1704",
      "page_count": 5,
      "order": 603,
      "p1": "2883",
      "pn": "2887",
      "abstract": [
        "In speaker verification, the convolutional neural networks (CNN) have\nbeen successfully leveraged to achieve a great performance. Most of\nthe models based on CNN primarily focus on learning the distinctive\nspeaker embedding from the horizontal direction (time-axis). However,\nthe feature relationship between channels is usually neglected. In\nthis paper, we firstly aim toward an alternate direction of recalibrating\nthe channel-wise features by introducing the recently proposed &#8220;squeeze-and-excitation&#8221;\n(SE) module for image classification. We effectively incorporate the\nSE blocks in the deep residual networks (ResNet-SE) and demonstrate\na slightly improvement on VoxCeleb corpuses. Additionally, we propose\na new loss function, namely additive supervision softmax (AS-Softmax),\nto make full use of the prior knowledge of the mis-classified samples\nat training stage by imposing more penalty on the mis-classified samples\nto regularize the training process. The experimental results on VoxCeleb\ncorpuses demonstrate that the proposed loss could further improve the\nperformance of speaker system, especially on the case that the combination\nof the ResNet-SE and the AS-Softmax.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1704",
      "author_area_id": 4,
      "author_area_label": "Speaker and Language Identification"
    },
    "shon19b_interspeech": {
      "authors": [
        [
          "Suwon",
          "Shon"
        ],
        [
          "Hao",
          "Tang"
        ],
        [
          "James",
          "Glass"
        ]
      ],
      "title": "VoiceID Loss: Speech Enhancement for Speaker Verification",
      "original": "1496",
      "page_count": 5,
      "order": 604,
      "p1": "2888",
      "pn": "2892",
      "abstract": [
        "In this paper, we propose VoiceID loss, a novel loss function for training\na speech enhancement model to improve the robustness of speaker verification.\nIn contrast to the commonly used loss functions for speech enhancement\nsuch as the L2 loss, the VoiceID loss is based on the feedback from\na speaker verification model to generate a ratio mask. The generated\nratio mask is multiplied pointwise with the original spectrogram to\nfilter out unnecessary components for speaker verification. In the\nexperiments, we observed that the enhancement network, after training\nwith the VoiceID loss, is able to ignore a substantial amount of time-frequency\nbins, such as those dominated by noise, for verification. The resulting\nmodel consistently improves the speaker verification system on both\nclean and noisy conditions.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1496",
      "author_area_id": 4,
      "author_area_label": "Speaker and Language Identification"
    },
    "avila19_interspeech": {
      "authors": [
        [
          "Anderson R.",
          "Avila"
        ],
        [
          "Jahangir",
          "Alam"
        ],
        [
          "Douglas",
          "O\u2019Shaughnessy"
        ],
        [
          "Tiago H.",
          "Falk"
        ]
      ],
      "title": "Blind Channel Response Estimation for Replay Attack Detection",
      "original": "2956",
      "page_count": 5,
      "order": 605,
      "p1": "2893",
      "pn": "2897",
      "abstract": [
        "Recently, automatic speaker verification (ASV) systems have been acknowledged\nto be vulnerable to replay attacks. Multiple efforts have been taken\nby the research community to improve ASV robustness. In this paper,\nwe propose a replay attack countermeasure based on the blind estimation\nof the magnitude of channel responses. For that, the log-spectrum average\nof the clean speech signal is predicted from a Gaussian mixture model\n(GMM) of RASTA filtered mel-frequency cepstral coefficients (MFCCs)\ntrained on clean speech. The magnitude response of the channel is obtained\nby subtracting the log-spectrum of the observed signal from the predicted\nlog-spectrum average of the clean signal. Two datasets are used in\nour experiments: (1) the TIMIT dataset, which is used to train the\nlog-spectrum average of the clean signal; and (2) a dataset containing\nreplay attacks used during the second Automatic Speaker Verification\nSpoofing and Countermeasures Challenge (ASVspoof 2017). Performance\nis compared to two benchmarks. The discrete Fourier transform power\nspectral (DFTspec) and the constant Q cepstral coefficients (CQCCs).\nResults show the proposed method outperforming the two benchmarks in\nmost scenarios with equal error rate (EER) as low as 6.87% when testing\non the development set and as low as 11.28% on the evaluation set.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2956",
      "author_area_id": 4,
      "author_area_label": "Speaker and Language Identification"
    },
    "patil19_interspeech": {
      "authors": [
        [
          "Ankur T.",
          "Patil"
        ],
        [
          "Rajul",
          "Acharya"
        ],
        [
          "Pulikonda Aditya",
          "Sai"
        ],
        [
          "Hemant A.",
          "Patil"
        ]
      ],
      "title": "Energy Separation-Based Instantaneous Frequency Estimation for Cochlear Cepstral Feature for Replay Spoof Detection",
      "original": "2742",
      "page_count": 5,
      "order": 606,
      "p1": "2898",
      "pn": "2902",
      "abstract": [
        "Replay attack poses significant threat to Automatic Speaker Verification\n(ASV) system among various spoofing attacks, as it is easily accessible\nby low cost and high quality recording and playback devices. This paper\npresents a novel feature set, i.e., Cochlear Filter Cepstral Coefficient\nInstantaneous Frequency using Energy Separation Algorithm (CFCCIF-ESA)\nto develop countermeasure against replay spoofing attacks. Experimental\nresults on ASVspoof 2017 Version 2.0 database reveal that the proposed\nCFCCIF-ESA performs better than the earlier proposed CFCCIF (using\nanalytic signal generation via Hilbert transform) feature set. This\nis because ESA uses extremely short window to estimate instantaneous\nfrequency being able to adapt during speech transitions across phonemes.\nExperiments are performed using Gaussian Mixture Model (GMM) as a classifier.\nBaseline Constant Q Cepstral Coefficient (CQCC) performs slightly better\nthan CFCCIF-ESA on development set (i.e., 12.47% and 12.98% Equal Error\nRate (EER) for CQCC and CFCCIF-ESA, respectively). However, contrasting\nresults on evaluation set (i.e., 18.81% and 14.77% EER for CQCC and\nCFCCIF-ESA, respectively) indicates that the proposed CFCCIF-ESA gives\nrelatively better performance for unseen attacks in evaluation data.\nAlso, the proposed feature set gives an EER of 11.56% and 13.26% on\ndevelopment and evaluation dataset when fused with state-of-the-art\nMel Frequency Cepstral Coefficient (MFCC).\n"
      ],
      "doi": "10.21437/Interspeech.2019-2742",
      "author_area_id": 4,
      "author_area_label": "Speaker and Language Identification"
    },
    "mingote19_interspeech": {
      "authors": [
        [
          "Victoria",
          "Mingote"
        ],
        [
          "Antonio",
          "Miguel"
        ],
        [
          "Dayana",
          "Ribas"
        ],
        [
          "Alfonso",
          "Ortega"
        ],
        [
          "Eduardo",
          "Lleida"
        ]
      ],
      "title": "Optimization of False Acceptance/Rejection Rates and Decision Threshold for End-to-End Text-Dependent Speaker Verification Systems",
      "original": "2550",
      "page_count": 5,
      "order": 607,
      "p1": "2903",
      "pn": "2907",
      "abstract": [
        "Currently, most Speaker Verification (SV) systems based on neural networks\nuse Cross-Entropy and/or Triplet loss functions. Despite these functions\nprovide competitive results, they might not fully exploit the system\nperformance, because they are not designed to optimize the verification\ntask considering the performance measures, e.g. the Detection Cost\nFunction (DCF) or the Equal Error Rate (EER). This paper proposes a\nfirst approach to this issue through the optimization of a loss function\nbased on the DCF. This mechanism allows the end-to-end system to directly\nmanage the threshold used to compute the ratio between the False Rejection\nRate (FRR) and the False Acceptance Rate (FAR). This way connecting\nthe system training directly to the operating point. Results in a text-dependent\nspeaker verification framework, based on neural network super-vectors\nover the RSR2015 dataset, outperform reference systems using Cross-Entropy\nand Triplet loss, as well as our previously proposal based on an approximation\nof the Area Under the Curve ( aAUC).\n"
      ],
      "doi": "10.21437/Interspeech.2019-2550",
      "author_area_id": 4,
      "author_area_label": "Speaker and Language Identification"
    },
    "fan19_interspeech": {
      "authors": [
        [
          "Lei",
          "Fan"
        ],
        [
          "Qing-Yuan",
          "Jiang"
        ],
        [
          "Ya-Qi",
          "Yu"
        ],
        [
          "Wu-Jun",
          "Li"
        ]
      ],
      "title": "Deep Hashing for Speaker Identification and Retrieval",
      "original": "2457",
      "page_count": 5,
      "order": 608,
      "p1": "2908",
      "pn": "2912",
      "abstract": [
        "Speaker identification and retrieval have been widely used in real\napplications. To overcome the inefficiency problem caused by real-valued\nrepresentations, there have appeared some speaker hashing methods for\nspeaker identification and retrieval by learning binary codes as representations.\nHowever, these hashing methods are based on i-vector and cannot achieve\nsatisfactory retrieval accuracy as they cannot learn discriminative\nfeature representations. In this paper, we propose a novel deep hashing\nmethod, called deep additive margin hashing (DAMH), to improve retrieval\nperformance for speaker identification and retrieval task. Compared\nwith existing speaker hashing methods, DAMH can perform feature learning\nand binary code learning seamlessly by incorporating these two procedures\ninto an end-to-end architecture. Experimental results on a large-scale\naudio dataset VoxCeleb2 show that DAMH can outperform existing speaker\nhashing methods to achieve state-of-the-art performance.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2457",
      "author_area_id": 4,
      "author_area_label": "Speaker and Language Identification"
    },
    "marras19_interspeech": {
      "authors": [
        [
          "Mirko",
          "Marras"
        ],
        [
          "Pawe\u0142",
          "Korus"
        ],
        [
          "Nasir",
          "Memon"
        ],
        [
          "Gianni",
          "Fenu"
        ]
      ],
      "title": "Adversarial Optimization for Dictionary Attacks on Speaker Verification",
      "original": "2430",
      "page_count": 5,
      "order": 609,
      "p1": "2913",
      "pn": "2917",
      "abstract": [
        "In this paper, we assess vulnerability of speaker verification systems\nto dictionary attacks. We seek master voices, i.e., adversarial utterances\noptimized to match against a large number of users by pure chance.\nFirst, we perform menagerie analysis to identify utterances which intrinsically\nhold this property. Then, we propose an adversarial optimization approach\nfor generating master voices synthetically. Our experiments show that,\neven in the most secure configuration, on average, a master voice can\nmatch approx. 20% of females and 10% of males without any knowledge\nabout the population. We demonstrate that dictionary attacks should\nbe considered as a feasible threat model for sensitive and high-stakes\ndeployments of speaker verification.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2430",
      "author_area_id": 4,
      "author_area_label": "Speaker and Language Identification"
    },
    "gunendradasan19_interspeech": {
      "authors": [
        [
          "Tharshini",
          "Gunendradasan"
        ],
        [
          "Eliathamby",
          "Ambikairajah"
        ],
        [
          "Julien",
          "Epps"
        ],
        [
          "Haizhou",
          "Li"
        ]
      ],
      "title": "An Adaptive-Q Cochlear Model for Replay Spoofing Detection",
      "original": "2361",
      "page_count": 5,
      "order": 610,
      "p1": "2918",
      "pn": "2922",
      "abstract": [
        "Replay attack poses a key threat for automatic speaker verification\nsystems. Spoofing detection systems inspired by auditory perception\nhave shown promise to date, however some aspects of auditory processing\nhave not been investigated in this context. In this paper, a transmission\nline cochlear model that incorporates an active feedback mechanism\nis proposed for replay attack detection. This model compresses the\nconsiderable energy variation in each auditory sub-band filter by boosting\nlow-amplitude signal, an effect that is not considered in many auditory\nmodels. To perform the compression, the parameters of each auditory\nsub-band filter are modified based on the sub-band energy, analogous\nto the effect of the closed-loop adaptation mechanism that allows perception\nof a wide dynamic range from a physically constrained system, which\nwe term adaptive-Q. Evaluation on the ASVspoof 2017 version 2 database\nsuggests that the adaptive-Q compression provided by the proposed model\nhelps to improve the performance of replay detection, and a relative\nreduction in EER of 26% was achieved compared with the best results\nreported for auditory system-based feature proposed for replay attack\ndetection.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2361",
      "author_area_id": 4,
      "author_area_label": "Speaker and Language Identification"
    },
    "yun19_interspeech": {
      "authors": [
        [
          "Sungrack",
          "Yun"
        ],
        [
          "Janghoon",
          "Cho"
        ],
        [
          "Jungyun",
          "Eum"
        ],
        [
          "Wonil",
          "Chang"
        ],
        [
          "Kyuwoong",
          "Hwang"
        ]
      ],
      "title": "An End-to-End Text-Independent Speaker Verification Framework with a Keyword Adversarial Network",
      "original": "2208",
      "page_count": 5,
      "order": 611,
      "p1": "2923",
      "pn": "2927",
      "abstract": [
        "This paper presents an end-to-end text-independent speaker verification\nframework by jointly considering the speaker embedding (SE) network\nand automatic speech recognition (ASR) network. The SE network learns\nto output an embedding vector which distinguishes the speaker characteristics\nof the input utterance, while the ASR network learns to recognize the\nphonetic context of the input. In training our speaker verification\nframework, we consider both the triplet loss minimization and adversarial\ngradient of the ASR network to obtain more discriminative and text-independent\nspeaker embedding vectors. With the triplet loss, the distances between\nthe embedding vectors of the same speaker are minimized while those\nof different speakers are maximized. Also, with the adversarial gradient\nof the ASR network, the text-dependency of the speaker embedding vector\ncan be reduced. In the experiments, we evaluated our speaker verification\nframework using the LibriSpeech and CHiME 2013 dataset, and the evaluation\nresults show that our speaker verification framework shows lower equal\nerror rate and better text-independency compared to the other approaches.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2208",
      "author_area_id": 4,
      "author_area_label": "Speaker and Language Identification"
    },
    "seo19_interspeech": {
      "authors": [
        [
          "Soonshin",
          "Seo"
        ],
        [
          "Daniel Jun",
          "Rim"
        ],
        [
          "Minkyu",
          "Lim"
        ],
        [
          "Donghyun",
          "Lee"
        ],
        [
          "Hosung",
          "Park"
        ],
        [
          "Junseok",
          "Oh"
        ],
        [
          "Changmin",
          "Kim"
        ],
        [
          "Ji-Hwan",
          "Kim"
        ]
      ],
      "title": "Shortcut Connections Based Deep Speaker Embeddings for End-to-End Speaker Verification System",
      "original": "2195",
      "page_count": 5,
      "order": 612,
      "p1": "2928",
      "pn": "2932",
      "abstract": [
        "The objective of speaker verification is to reject or accept whether\nor not the input speech is that of a enrolled speaker. Traditionally,\ni-vector or speaker embeddings system such as d-vector representing\nthe speaker information has been showing high performance with similarity\nmetrics at the backend. Recently it has been proposed an end-to-end\nsystem based on previous speaker embeddings approach without additional\nstrategy after extraction. Among the various models, CNN based end-to-end\nsystem is showing state-of-the-art performance. CNN based model is\ntrained to classify multiple speakers and speaker embeddings are extracted.<br\nstyle=\"mso-data-placement:same-cell;\" /><br style=\"mso-data-placement:same-cell;\"\n/>\n  In this paper, we propose shortcut connections based deep speaker\nembeddings for end-to-end speaker verification system. We construct\nmodified ResNet-18 model so that the activation outputs from bottleneck\narchitecture have shortcut connections to speaker embeddings. Deep\nspeaker embeddings are extracted by jointly training in end-to-end\napproach. The model was constructed without other sophisticated methods\nsuch as length normalization, or additive margin softmax loss. When\nwe tested proposed model on the unconstrained conditions data set called\nVoxCeleb1, the result showed EER of 3.03% when tested with high dimensional\ndeep speaker embeddings. This is the state-of-the-art performance of\nend-to-end speaker verification model on VoxCeleb1.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2195",
      "author_area_id": 4,
      "author_area_label": "Speaker and Language Identification"
    },
    "you19c_interspeech": {
      "authors": [
        [
          "Chang Huai",
          "You"
        ],
        [
          "Jichen",
          "Yang"
        ],
        [
          "Huy Dat",
          "Tran"
        ]
      ],
      "title": "Device Feature Extractor for Replay Spoofing Detection",
      "original": "2137",
      "page_count": 5,
      "order": 613,
      "p1": "2933",
      "pn": "2937",
      "abstract": [
        "Device feature, which contains the information of both recording channel\nand playback channel, is the critical trait for replay spoofing detection.\nSo far there have not been any technical reports about the usage of\ndevice information in spoofing detection for speaker verification.\nIn this paper, we propose to build a replay device feature (RDF) extractor\non the basis of the genuine-replay-pair training database. The RDF\nextractor is trained in constant-Q transform (CQT) spectrum domain.\nA bidirectional long short-term memory (BLSTM) is used in the neural\nnetwork and finally the RDF extractor is formed by applying discrete\ncosine transform (DCT) to the output vector of the BLSTM. The experimental\nresult on ASVspoof 2017 corpus version 2.0 shows that equal error rate\n(EER) of replay detection system with the proposed RDF reaches 15.08%.\nFurthermore, by combining the RDF with constant-Q cepstral coefficients\nplus log energy (CQCCE), the EER of the detection system can reduce\nto 8.99%. In addition, the experimental results also show that the\nRDF has much complementarity with conventional features.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2137",
      "author_area_id": 4,
      "author_area_label": "Speaker and Language Identification"
    },
    "wang19i_interspeech": {
      "authors": [
        [
          "Hongji",
          "Wang"
        ],
        [
          "Heinrich",
          "Dinkel"
        ],
        [
          "Shuai",
          "Wang"
        ],
        [
          "Yanmin",
          "Qian"
        ],
        [
          "Kai",
          "Yu"
        ]
      ],
      "title": "Cross-Domain Replay Spoofing Attack Detection Using Domain Adversarial Training",
      "original": "2120",
      "page_count": 5,
      "order": 614,
      "p1": "2938",
      "pn": "2942",
      "abstract": [
        "Replay spoofing attacks are a major threat for speaker verification\nsystems. Although many anti-spoofing systems or countermeasures are\nproposed to detect dataset-specific replay attacks with promising performance,\nthey generalize poorly when applied on unseen datasets. In this work,\nthe cross-dataset scenario is treated as a domain-mismatch problem\nand dealt with using a domain adversarial training framework. Compared\nwith previous approaches, features learned from this newly-designed\narchitecture are more discriminative for spoofing detection, but more\nindistinguishable across different domains. Only labeled source-domain\ndata and unlabeled target-domain data are required during the adversarial\ntraining process, which can be regarded as unsupervised domain adaptation.\nExperiments on the ASVspoof 2017 V.2 dataset as well as the physical\naccess condition part of BTAS 2016 dataset demonstrate that a significant\nEER reduction of over relative 30% can be obtained after applying the\nproposed domain adversarial training framework. It is shown that our\nproposed model can benefit from a large amount of unlabeled target-domain\ntraining data to improve detection accuracy.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2120",
      "author_area_id": 4,
      "author_area_label": "Speaker and Language Identification"
    },
    "kanagasundaram19_interspeech": {
      "authors": [
        [
          "A.",
          "Kanagasundaram"
        ],
        [
          "S.",
          "Sridharan"
        ],
        [
          "G.",
          "Sriram"
        ],
        [
          "S.",
          "Prachi"
        ],
        [
          "C.",
          "Fookes"
        ]
      ],
      "title": "A Study of x-Vector Based Speaker Recognition on Short Utterances",
      "original": "1891",
      "page_count": 5,
      "order": 615,
      "p1": "2943",
      "pn": "2947",
      "abstract": [
        "The aim of this work is to gain insights into how the deep neural network\n(DNN) models should be trained for short utterance evaluation conditions\nin an x-vector based speaker verification system. The study suggests\nthat the speaker embedding can be extracted with reduced dimensions\nfor short utterance evaluation conditions. When the speaker embedding\nis extracted from deeper layer which has lower dimension, the x-vector\nsystem achieves 14% relative improvement over baseline approach on\nEER on NIST2010 5sec-5sec truncated conditions. We surmise that since\nshort utterances have less phonetic information speaker discriminative\nx-vectors can be extracted from a deeper layer of the DNN which captures\nless phonetic information. Another interesting finding is that the\nx-vector system achieves 5% relative improvement on NIST2010 5sec-5sec\nevaluation condition when the back-end PLDA is trained using short\nutterance development data. The results confirms the intuitive expectation\nthat duration of development utterances and the duration of evaluation\nutterances should be matched. Finally, for the duration mismatch condition,\nwe propose a variance normalization approach for PLDA training that\nprovides a 4% relative improvement on EER over baseline approach.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1891",
      "author_area_id": 4,
      "author_area_label": "Speaker and Language Identification"
    },
    "chen19i_interspeech": {
      "authors": [
        [
          "Nanxin",
          "Chen"
        ],
        [
          "Jes\u00fas",
          "Villalba"
        ],
        [
          "Najim",
          "Dehak"
        ]
      ],
      "title": "Tied Mixture of Factor Analyzers Layer to Combine Frame Level Representations in Neural Speaker Embeddings",
      "original": "1782",
      "page_count": 5,
      "order": 616,
      "p1": "2948",
      "pn": "2952",
      "abstract": [
        "In this paper, a novel neural network layer is proposed to combine\nframe-level into utterance-level representations for speaker modeling.\nWe followed the assumption that the frame-level outputs of the speaker\nembedding (a.k.a x-vector) encoder are multi-modal. Therefore, we modeled\nthe frame-level information as a mixture of factor analyzers with latent\nvariable (utterance embedding) tied across frames and mixture components,\nin as similar way as in the i-vector approach. We denote this layer\nas Tied Mixture of Factor Analyzers (TMFA) layer. The optimal value\nof the embedding is obtained by minimizing the reconstruction error\nof the frame-level representations given the embedding and the TMFA\nmodel parameters. However, the TMFA layer parameters (factor loading\nmatrices, means and precisions) were trained with cross-entropy loss\nas the rest of parameters of the network. We experimented on the Speaker\nRecognition Evaluation 2016 Cantonese as well as in the Speaker in\nthe Wild datasets. The proposed pooling layer improved w.r.t. mean\nplus standard deviation pooling &#8212; standard in x-vector approach\n&#8212; in most of the conditions evaluated; and obtained competitive\nperformance w.r.t. the recently proposed learnable dictionary encoding\npooling method, which also assumes multi-modal frame-level representations.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1782",
      "author_area_id": 4,
      "author_area_label": "Speaker and Language Identification"
    },
    "wickramasinghe19_interspeech": {
      "authors": [
        [
          "Buddhi",
          "Wickramasinghe"
        ],
        [
          "Eliathamby",
          "Ambikairajah"
        ],
        [
          "Julien",
          "Epps"
        ]
      ],
      "title": "Biologically Inspired Adaptive-Q Filterbanks for Replay Spoofing Attack Detection",
      "original": "1535",
      "page_count": 5,
      "order": 617,
      "p1": "2953",
      "pn": "2957",
      "abstract": [
        "Development of generalizable countermeasures for replay spoofing attacks\non Automatic Speaker Verification (ASV) systems is still an open problem.\nMany countermeasures to date utilize bandpass filters to extract a\nvariety of frequency band-based features. This paper proposes the use\nof adaptive bandpass filters, a concept adopted from human cochlear\nmodelling to improve detection performance. Gains of filters used for\nsubband based feature extraction are adaptively adjusted by varying\ntheir Q factors (Quality factor) as a function of input signal level\nto boost low amplitude signal components and improve the front-end&#8217;s\nsensitivity to them. This method is used to enhance information embedded\nin speech signals such as device channel effects which could be instrumental\nin distinguishing genuine speech signals from replayed ones. Three\nfeatures extracted using the adaptive filter process yielded performance\nimprovements over other auditory concepts-based baselines, showing\nthe potential of using an adaptive filter mechanism for replay spoofing\nattack detection.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1535",
      "author_area_id": 4,
      "author_area_label": "Speaker and Language Identification"
    },
    "bousquet19_interspeech": {
      "authors": [
        [
          "Pierre-Michel",
          "Bousquet"
        ],
        [
          "Mickael",
          "Rouvier"
        ]
      ],
      "title": "On Robustness of Unsupervised Domain Adaptation for Speaker Recognition",
      "original": "1524",
      "page_count": 5,
      "order": 618,
      "p1": "2958",
      "pn": "2962",
      "abstract": [
        "Current speaker recognition systems, that are learned by using wide\ntraining datasets and include sophisticated modelings, turn out to\nbe very specific, providing sometimes disappointing results in real-life\napplications. Any shift between training and test data, in terms of\ndevice, language, duration, noise or other tends to degrade accuracy\nof speaker detection. This study investigates unsupervised domain adaptation,when\nonly a scarce and unlabeled &#8220;in-domain&#8221; development dataset\nis available. Details and relevance of different approaches are described\nand commented, leading to a new robust method that we call feature-Distribution\nAdaptor. Efficiency of the proposed technique is experimentally validated\non the recent NIST 2016 and 2018 Speaker Recognition Evaluation datasets.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1524",
      "author_area_id": 4,
      "author_area_label": "Speaker and Language Identification"
    },
    "shon19c_interspeech": {
      "authors": [
        [
          "Suwon",
          "Shon"
        ],
        [
          "Younggun",
          "Lee"
        ],
        [
          "Taesu",
          "Kim"
        ]
      ],
      "title": "Large-Scale Speaker Retrieval on Random Speaker Variability Subspace",
      "original": "1498",
      "page_count": 5,
      "order": 619,
      "p1": "2963",
      "pn": "2967",
      "abstract": [
        "This paper describes a fast speaker search system to retrieve segments\nof the same voice identity in the large-scale data. A recent study\nshows that Locality Sensitive Hashing (LSH) enables quick retrieval\nof a relevant voice in the large-scale data in conjunction with i-vector\nwhile maintaining accuracy. In this paper, we proposed Random Speaker-variability\nSubspace (RSS) projection to map a data into LSH based hash tables.\nWe hypothesized that rather than projecting on completely random subspace\nwithout considering data, projecting on randomly generated speaker\nvariability space would give more chance to put the same speaker representation\ninto the same hash bins, so we can use less number of hash tables.\nMultiple RSS can be generated by randomly selecting a subset of speakers\nfrom a large speaker cohort. From the experimental result, the proposed\napproach shows 100 times and 7 times faster than the linear search\nand LSH, respectively.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1498",
      "author_area_id": 4,
      "author_area_label": "Speaker and Language Identification"
    },
    "yoshioka19_interspeech": {
      "authors": [
        [
          "Takuya",
          "Yoshioka"
        ],
        [
          "Dimitrios",
          "Dimitriadis"
        ],
        [
          "Andreas",
          "Stolcke"
        ],
        [
          "William",
          "Hinthorn"
        ],
        [
          "Zhuo",
          "Chen"
        ],
        [
          "Michael",
          "Zeng"
        ],
        [
          "Xuedong",
          "Huang"
        ]
      ],
      "title": "Meeting Transcription Using Asynchronous Distant Microphones",
      "original": "3088",
      "page_count": 5,
      "order": 620,
      "p1": "2968",
      "pn": "2972",
      "abstract": [
        "We describe a system that generates speaker-annotated transcripts of\nmeetings by using multiple asynchronous distant microphones. The system\nis composed of continuous audio stream alignment, blind beamforming,\nspeech recognition, speaker diarization, and system combination. While\nthe idea of improving the meeting transcription accuracy by leveraging\nmultiple recordings has been investigated in certain specific technology\nareas such as beamforming, our objective is to assess the feasibility\nof a complete system with a set of mobile devices and conduct a detailed\nanalysis. With seven input audio streams, our system achieves a word\nerror rate (WER) of 22.3% and a speaker-attributed WER (SAWER) of 26.7%,\nand comes within 3% of the close-talking microphone WER on non-overlapping\nspeech. The relative gains in SAWER over a single-device system are\n14.8%, 20.3%, and 22.4% for three, five, and seven microphones, respectively.\nThe full system achieves a 13.6% diarization error rate, 10% of which\nare due to overlapped speech.\n"
      ],
      "doi": "10.21437/Interspeech.2019-3088",
      "author_area_id": 10,
      "author_area_label": "Speech Recognition: Technologies and Systems for New Applications"
    },
    "thomas19_interspeech": {
      "authors": [
        [
          "Samuel",
          "Thomas"
        ],
        [
          "Kartik",
          "Audhkhasi"
        ],
        [
          "Zolt\u00e1n",
          "T\u00fcske"
        ],
        [
          "Yinghui",
          "Huang"
        ],
        [
          "Michael",
          "Picheny"
        ]
      ],
      "title": "Detection and Recovery of OOVs for Improved English Broadcast News Captioning",
      "original": "2793",
      "page_count": 5,
      "order": 621,
      "p1": "2973",
      "pn": "2977",
      "abstract": [
        "In this paper we present a study on building various deep neural network-based\nspeech recognition systems for automatic caption generation that can\ndeal with out-of-vocabulary (OOV) words. We develop several kinds of\nsystems using various acoustic (hybrid, CTC, attention-based neural\nnetworks) and language modeling (n-gram and RNN-based neural networks)\ntechniques on broadcast news. We discuss various limitations that the\nproposed systems have and introduce methods to effectively use them\nto detect OOVs. For automatic OOV recovery, we compare the use of different\nkinds of phonetic and graphemic sub-word units, that can be synthesized\ninto word outputs. On an experimental three hour broadcast news test\nset with a 4% OOV rate, the proposed CTC and attention-based systems\nare capable of reliably detecting OOVs much better (0.52 F-score) than\na traditional hybrid baseline system (0.21 F-score). These improved\ndetection gains translate further to better WER performance. With reference\nto a non-OOV oracle baseline, the proposed systems at just 12% relative\n(1.4% absolute) loss in word error rate (WER), perform significantly\nbetter than the traditional hybrid system (with close to 50% relative\nloss), by recovering OOVs using their sub-word outputs.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2793",
      "author_area_id": 10,
      "author_area_label": "Speech Recognition: Technologies and Systems for New Applications"
    },
    "farooq19_interspeech": {
      "authors": [
        [
          "Muhammad Umar",
          "Farooq"
        ],
        [
          "Farah",
          "Adeeba"
        ],
        [
          "Sahar",
          "Rauf"
        ],
        [
          "Sarmad",
          "Hussain"
        ]
      ],
      "title": "Improving Large Vocabulary Urdu Speech Recognition System Using Deep Neural Networks",
      "original": "2629",
      "page_count": 5,
      "order": 622,
      "p1": "2978",
      "pn": "2982",
      "abstract": [
        "Development of Large Vocabulary Continuous Speech Recognition (LVCSR)\nsystem is a cumbersome task, especially for low resource languages.\nUrdu is the national language and lingua franca of Pakistan, with 100\nmillion speakers worldwide. Due to resource scarcity, limited work\nhas been done in the domain of Urdu speech recognition. In this paper,\ncollection of Urdu speech corpus and development of Urdu speech recognition\nsystem is presented. Urdu LVCSR is developed using 300 hours of read\nspeech data with a vocabulary size of 199K words. Microphone speech\nis recorded from 1671 Urdu and Punjabi speakers in both indoor and\noutdoor environments. Different acoustic modeling techniques such as\nGaussian Mixture Models based Hidden Markov Models (GMM-HMM), Time\nDelay Neural Networks (TDNN), Long-Short Term Memory (LSTM) and Bidirectional\nLong-Short Term Memory (BLSTM) networks are investigated. Cross entropy\nand Lattice Free Maximum Mutual Information (LF-MMI) objective functions\nare employed during acoustic modeling. In addition, Recurrent Neural\nNetwork Language Model (RNNLM) is also being used for re-scoring. Developed\nspeech recognition system has been evaluated on 9.5 hours of collected\ntest data and a minimum Word Error Rate (%WER) of 13.50% is achieved.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2629",
      "author_area_id": 10,
      "author_area_label": "Speech Recognition: Technologies and Systems for New Applications"
    },
    "tang19b_interspeech": {
      "authors": [
        [
          "Min",
          "Tang"
        ]
      ],
      "title": "Hybrid Arbitration Using Raw ASR String and NLU Information &#8212; Taking the Best of Both Embedded World and Cloud World",
      "original": "2586",
      "page_count": 5,
      "order": 623,
      "p1": "2983",
      "pn": "2987",
      "abstract": [
        "Hybrid arbitration is a process where we select the best Automatic\nSpeech Recognition (ASR) and Natural Language Understanding (NLU) result\nfrom embedded/client and cloud-based system outputs. It is a common\napproach that a lot of real world applications use to unify knowledge\nsources that are not available to client and cloud at the same time.\nIn the past, people primarily relied on ASR confidence features and\nsome application specific heuristics in the arbitration process. However,\nconfidence features are unable to capture subtle context specific differences.\nIn this paper, besides confidence, we also use raw ASR strings and\nNLU results in the hybrid arbitration process. We model the arbitration\nprocess as two steps &#8212; first, decide whether to wait for a slower\nsystem, and second, pick the best result. We compared multiple machine\nlearning approaches and it turns out the Deep Neural Network (DNN)\nbased classifier, using word embeddings to process ASR strings and\nNLU embeddings to process NLU information, can deliver the best performance.\nWe conducted experiments on two production system setups, using field\ndata from real users. Compared with traditional confidence score based\napproach, we obtain about 30% relative word error reduction and 30%\nrelative sentence error rate reduction.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2586",
      "author_area_id": 10,
      "author_area_label": "Speech Recognition: Technologies and Systems for New Applications"
    },
    "szaszak19_interspeech": {
      "authors": [
        [
          "Gy\u00f6rgy",
          "Szasz\u00e1k"
        ],
        [
          "M\u00e1t\u00e9 \u00c1kos",
          "T\u00fcndik"
        ]
      ],
      "title": "Leveraging a Character, Word and Prosody Triplet for an ASR Error Robust and Agglutination Friendly Punctuation Approach",
      "original": "2132",
      "page_count": 5,
      "order": 624,
      "p1": "2988",
      "pn": "2992",
      "abstract": [
        "Punctuating ASR transcript has received increasing attention recently,\nand well-performing approaches were presented based on sequence-to-sequence\nmodelling, exploiting textual (word and character) and/or acoustic-prosodic\nfeatures. In this work we propose to consider character, word and prosody\nbased features all at once to provide a robust and highly language\nindependent platform for punctuation recovery, which can deal also\nwell with highly agglutinating languages with less constrained word\norder. We demonstrate that using such a feature triplet improves ASR\nerror robustness of punctuation in two quite differently organized\nlanguages, English and Hungarian. Moreover, in the highly agglutinating\nHungarian, where word-based approaches suffer from the exploding vocabulary\n(poorer semantic representation through embeddings) and less constrained\nword order, we show that prosodic cues and the character-based model\ncan powerfully counteract this loss of information. We also perform\na deep analysis of punctuation w.r.t. both ASR errors and agglutination\nto explain the improvements we observed on a solid basis.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2132",
      "author_area_id": 10,
      "author_area_label": "Speech Recognition: Technologies and Systems for New Applications"
    },
    "pellegrini19_interspeech": {
      "authors": [
        [
          "Thomas",
          "Pellegrini"
        ],
        [
          "J\u00e9r\u00f4me",
          "Farinas"
        ],
        [
          "Estelle",
          "Delpech"
        ],
        [
          "Fran\u00e7ois",
          "Lancelot"
        ]
      ],
      "title": "The Airbus Air Traffic Control Speech Recognition 2018 Challenge: Towards ATC Automatic Transcription and Call Sign Detection",
      "original": "1962",
      "page_count": 5,
      "order": 625,
      "p1": "2993",
      "pn": "2997",
      "abstract": [
        "In this paper, we describe the outcomes of the challenge organized\nand run by Airbus and partners in 2018 on Air Traffic Control (ATC)\nspeech recognition. The challenge consisted of two tasks applied to\nEnglish ATC speech: 1) automatic speech-to-text transcription, 2) call\nsign detection (CSD). The registered participants were provided with\n40 hours of speech along with manual transcriptions. Twenty-two teams\nsubmitted predictions on a five hour evaluation set. ATC speech processing\nis challenging for several reasons: high speech rate, foreign-accented\nspeech with a great diversity of accents, noisy communication channels.\nThe best ranked team achieved a 7.62% Word Error Rate and a 82.41%\nCSD F1-score. Transcribing pilots&#8217; speech was found to be twice\nas harder as controllers&#8217; speech. Remaining issues towards solving\nATC ASR are also discussed in the paper.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1962",
      "author_area_id": 10,
      "author_area_label": "Speech Recognition: Technologies and Systems for New Applications"
    },
    "oneata19_interspeech": {
      "authors": [
        [
          "Dan",
          "Onea\u021b\u0103"
        ],
        [
          "Horia",
          "Cucu"
        ]
      ],
      "title": " Kite: Automatic Speech Recognition for Unmanned Aerial Vehicles",
      "original": "1390",
      "page_count": 5,
      "order": 626,
      "p1": "2998",
      "pn": "3002",
      "abstract": [
        "This paper addresses the problem of building a speech recognition system\nattuned to the control of unmanned aerial vehicles (UAVs). Even though\nUAVs are becoming widespread, the task of creating voice interfaces\nfor them is largely unaddressed. To this end, we introduce a multi-modal\nevaluation dataset for UAV control, consisting of spoken commands and\nassociated images, which represent the visual context of what the UAV\n&#8220;sees&#8221; when the pilot utters the command. We provide baseline\nresults and address two research directions:  (i) how robust the language\nmodels are, given an incomplete list of commands at train time;  (ii)\nhow to incorporate visual information in the language model. We find\nthat recurrent neural networks (RNNs) are a solution to both tasks:\nthey can be successfully adapted using a small number of commands and\nthey can be extended to use visual cues. Our results show that the\nimage-based RNN outperforms its text-only counterpart even if the command&#8211;image\ntraining associations are automatically generated and inherently imperfect.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1390",
      "author_area_id": 10,
      "author_area_label": "Speech Recognition: Technologies and Systems for New Applications"
    },
    "wang19j_interspeech": {
      "authors": [
        [
          "Xiaofei",
          "Wang"
        ],
        [
          "Jinyi",
          "Yang"
        ],
        [
          "Ruizhi",
          "Li"
        ],
        [
          "Samik",
          "Sadhu"
        ],
        [
          "Hynek",
          "Hermansky"
        ]
      ],
      "title": "Exploring Methods for the Automatic Detection of Errors in Manual Transcription",
      "original": "1343",
      "page_count": 5,
      "order": 627,
      "p1": "3003",
      "pn": "3007",
      "abstract": [
        "Quality of data plays an important role in most deep learning tasks.\nIn the speech community, transcription of speech recording is indispensable.\nSince the transcription is usually generated artificially, automatically\nfinding errors in manual transcriptions not only saves time and labors\nbut benefits the performance of tasks that need the training process.\nInspired by the success of hybrid automatic speech recognition using\nboth language model and acoustic model, two approaches of automatic\nerror detection in the transcriptions have been explored in this work.\nPrevious study using a biased language model approach, relying on a\nstrong transcription-dependent language model, has been reviewed. In\nthis work, we propose a novel acoustic model based approach, focusing\non the phonetic sequence of speech. Both methods have been evaluated\non a completely real dataset, which was originally transcribed with\nerrors and strictly corrected manually afterwards.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1343",
      "author_area_id": 10,
      "author_area_label": "Speech Recognition: Technologies and Systems for New Applications"
    },
    "biswas19_interspeech": {
      "authors": [
        [
          "Astik",
          "Biswas"
        ],
        [
          "Raghav",
          "Menon"
        ],
        [
          "Ewald van der",
          "Westhuizen"
        ],
        [
          "Thomas",
          "Niesler"
        ]
      ],
      "title": "Improved Low-Resource Somali Speech Recognition by Semi-Supervised Acoustic and Language Model Training",
      "original": "1328",
      "page_count": 5,
      "order": 628,
      "p1": "3008",
      "pn": "3012",
      "abstract": [
        "We present improvements in automatic speech recognition (ASR) for Somali,\na currently extremely under-resourced language. This forms part of\na continuing United Nations (UN) effort to employ ASR-based keyword\nspotting systems to support humanitarian relief programmes in rural\nAfrica. Using just 1.57 hours of annotated speech data as a seed corpus,\nwe increase the pool of training data by applying semi-supervised training\nto 17.55 hours of untranscribed speech. We make use of factorised time-delay\nneural networks (TDNN-F) for acoustic modelling, since these have recently\nbeen shown to be effective in resource-scarce situations. Three semi-supervised\ntraining passes were performed, where the decoded output from each\npass was used for acoustic model training in the subsequent pass. The\nautomatic transcriptions from the best performing pass were used for\nlanguage model augmentation. To ensure the quality of automatic transcriptions,\ndecoder confidence is used as a threshold. The acoustic and language\nmodels obtained from the semi-supervised approach show significant\nimprovement in terms of WER and perplexity compared to the baseline.\nIncorporating the automatically generated transcriptions yields a 6.55%\nimprovement in language model perplexity. The use of 17.55 hour of\nSomali acoustic data in semi-supervised training shows an improvement\nof 7.74% relative over the baseline.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1328",
      "author_area_id": 10,
      "author_area_label": "Speech Recognition: Technologies and Systems for New Applications"
    },
    "helgadottir19_interspeech": {
      "authors": [
        [
          "Inga R.",
          "Helgad\u00f3ttir"
        ],
        [
          "Anna Bj\u00f6rk",
          "Nikul\u00e1sd\u00f3ttir"
        ],
        [
          "Michal",
          "Borsk\u00fd"
        ],
        [
          "Judy Y.",
          "Fong"
        ],
        [
          "R\u00f3bert",
          "Kjaran"
        ],
        [
          "J\u00f3n",
          "Gu\u00f0nason"
        ]
      ],
      "title": "The Althingi ASR System",
      "original": "1248",
      "page_count": 5,
      "order": 629,
      "p1": "3013",
      "pn": "3017",
      "abstract": [
        "All performed speeches in the Icelandic parliament, Althingi, are transcribed\nand published. An automatic speech recognition system (ASR) has been\ndeveloped to reduce the manual work involved. To our knowledge, this\nis the first open source speech recognizer in use for Icelandic. In\nthis paper the development of the ASR is described. In-lab system performance\nis evaluated and first results from the users are described. A word\nerror rate (WER) of 7.91% was obtained on our in-lab speech recognition\ntest set using time-delay deep neural network (TDNN) and re-scoring\nwith a bidirectional recurrent neural network language model (RNN-LM).\nNo further processing of the text is included in that number. In-lab\nF-score for the punctuation model is 80.6 and 61.6 for the paragraph\nmodel. The WER of the ASR, including punctuation marks and other post-processing,\nwas 15.0 &#177; 6.0%, over 625 speeches, when tested in the wild. This\nis an upper limit since not all mismatches with the reference text\nare true errors of the ASR. The transcribers of Althingi graded 77%\nof the speech transcripts as Good. The Althingi corpus and ASR recipe,\nconstitute a valuable resource for further developments within Icelandic\nlanguage technology.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1248",
      "author_area_id": 10,
      "author_area_label": "Speech Recognition: Technologies and Systems for New Applications"
    },
    "gupta19d_interspeech": {
      "authors": [
        [
          "Vishwa",
          "Gupta"
        ],
        [
          "Lise",
          "Rebout"
        ],
        [
          "Gilles",
          "Boulianne"
        ],
        [
          "Pierre-Andr\u00e9",
          "M\u00e9nard"
        ],
        [
          "Jahangir",
          "Alam"
        ]
      ],
      "title": "CRIM&#8217;s Speech Transcription and Call Sign Detection System for the ATC Airbus Challenge Task",
      "original": "1131",
      "page_count": 5,
      "order": 630,
      "p1": "3018",
      "pn": "3022",
      "abstract": [
        "The Airbus air traffic control challenge evaluates speech recognition\nand call sign detection using real conversations between air traffic\ncontrollers and pilots at Toulouse airport in France. CRIM&#8217;s\nmain contribution in acoustic modeling for transcribing these conversations\nis experimentation with bidirectional LSTM (BLSTM) models and lattice-free\nMMI (LF-MMI) trained TDNN models. Adapting these acoustic models trained\nfrom a large dataset to 40 hours of ATC acoustic training data reduces\nWER significantly compared to training them with the ATC data only.\nMultiple iterations of adaptation reduce WER for the BLSTM acoustic\nmodels significantly, but only marginally for the LF-MMI TDNN acoustic\nmodels. Constrained dialog between the air traffic controller and the\npilot leads to language model perplexity below 12, and WER for leaderboard\nand evaluation sets of 9.98% and 9.41% respectively.<br style=\"mso-data-placement:same-cell;\"\n/><br style=\"mso-data-placement:same-cell;\" />\n  For call sign detection\nfrom the decoded transcript, we use a bidirectional LSTM followed by\nconditional random field classifier. This DNN architecture worked better\nthan a finite state transducer based call sign detection. Taking a\nmajority vote over call signs from multiple decodes reduced the call\nsign errors. The best F1 for call sign detection for leaderboard was\n0.8289 and for evaluation 0.8017. Overall, we came 3rd in this evaluation.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1131",
      "author_area_id": 10,
      "author_area_label": "Speech Recognition: Technologies and Systems for New Applications"
    },
    "rutowski19_interspeech": {
      "authors": [
        [
          "Tomasz",
          "Rutowski"
        ],
        [
          "Amir",
          "Harati"
        ],
        [
          "Yang",
          "Lu"
        ],
        [
          "Elizabeth",
          "Shriberg"
        ]
      ],
      "title": "Optimizing Speech-Input Length for Speaker-Independent Depression Classification",
      "original": "3095",
      "page_count": 5,
      "order": 631,
      "p1": "3023",
      "pn": "3027",
      "abstract": [
        "Machine learning models for speech-based depression classification\noffer promise for health care applications. Despite growing work on\ndepression classification, little is understood about how the length\nof speech-input impacts model performance. We analyze results for speaker-independent\ndepression classification using a corpus of over 1400 hours of speech\nfrom a human-machine health screening application. We examine performance\nas a function of response input length for two NLP systems that differ\nin overall performance.<br style=\"mso-data-placement:same-cell;\" /><br\nstyle=\"mso-data-placement:same-cell;\" />\n  Results for both systems\nshow that performance depends on natural length, elapsed length, and\nordering of the response within a session. Systems share a minimum\nlength threshold, but differ in a response saturation threshold, with\nthe latter higher for the better system. At saturation it is better\nto pose a new question to the speaker, than to continue the current\nresponse. These and additional reported results suggest how applications\ncan be better designed to both elicit and process optimal input lengths\nfor depression classification.\n"
      ],
      "doi": "10.21437/Interspeech.2019-3095",
      "author_area_id": 3,
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "pietrowicz19_interspeech": {
      "authors": [
        [
          "Mary",
          "Pietrowicz"
        ],
        [
          "Carla",
          "Agurto"
        ],
        [
          "Raquel",
          "Norel"
        ],
        [
          "Elif",
          "Eyigoz"
        ],
        [
          "Guillermo",
          "Cecchi"
        ],
        [
          "Zarina R.",
          "Bilgrami"
        ],
        [
          "Cheryl",
          "Corcoran"
        ]
      ],
      "title": "A New Approach for Automating Analysis of Responses on Verbal Fluency Tests from Subjects At-Risk for Schizophrenia",
      "original": "2987",
      "page_count": 5,
      "order": 632,
      "p1": "3028",
      "pn": "3032",
      "abstract": [
        "What if young people at risk for developing schizophrenia could be\nidentified early, via a fast, automated, non-invasive test of language,\nwhich could be administered remotely? These youths could then receive\nintervention which might mitigate course and possibly prevent psychosis.\nTimed word fluency tests, in which individuals name words starting\nwith a designated sound (typically F/A/S) or represent a given concept\ncategory (commonly animals/fruits/vegetables), have been used in the\nassessment of schizophrenia and its risk states, and in many other\nmental health conditions. Typically, psychologists manually record\nthe number and size of valid phoneme clusters and switches observed\nin the phonemic tests and count the number of valid words belonging\nto a given category in the categorical tests. We present a new technique\nfor automating the analysis of category fluency data and apply it to\nthe problem of detecting youths at risk of developing schizophrenia,\nwith best results over 85% accuracy when applying phonemic analysis\nto categorical data. The technique supports the separate quantification\nof structural and sequential phonemic similarity measures, supports\nan arbitrary range of pronunciations and dialects in the analysis,\nand may be extended to the assessment of other mental and physical\nhealth conditions, and their risk states.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2987",
      "author_area_id": 3,
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "jeancolas19_interspeech": {
      "authors": [
        [
          "Laetitia",
          "Jeancolas"
        ],
        [
          "Graziella",
          "Mangone"
        ],
        [
          "Jean-Christophe",
          "Corvol"
        ],
        [
          "Marie",
          "Vidailhet"
        ],
        [
          "St\u00e9phane",
          "Leh\u00e9ricy"
        ],
        [
          "Badr-Eddine",
          "Benkelfat"
        ],
        [
          "Habib",
          "Benali"
        ],
        [
          "Dijana",
          "Petrovska-Delacr\u00e9taz"
        ]
      ],
      "title": "Comparison of Telephone Recordings and Professional Microphone Recordings for Early Detection of Parkinson&#8217;s Disease, Using Mel-Frequency Cepstral Coefficients with Gaussian Mixture Models",
      "original": "2825",
      "page_count": 5,
      "order": 633,
      "p1": "3033",
      "pn": "3037",
      "abstract": [
        "Vocal impairments are among the earliest symptoms in Parkinson&#8217;s\nDisease (PD). We adapted a method classically used in speech and speaker\nrecognition, based on Mel-Frequency Cepstral Coefficients (MFCC) extraction\nand Gaussian Mixture Model (GMM) to detect recently diagnosed and pharmacologically\ntreated PD patients. We classified early PD subjects from controls\nwith an accuracy of 83%, using recordings obtained with a professional\nmicrophone. More interestingly, we were able to classify PD from controls\nwith an accuracy of 75% based on telephone recordings. As far as we\nknow, this is the first time that audio recordings from telephone network\nhave been used for early PD detection. This is a promising result for\na potential future telediagnosis of Parkinson&#8217;s disease.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2825",
      "author_area_id": 3,
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "janbakhshi19_interspeech": {
      "authors": [
        [
          "Parvaneh",
          "Janbakhshi"
        ],
        [
          "Ina",
          "Kodrasi"
        ],
        [
          "Herv\u00e9",
          "Bourlard"
        ]
      ],
      "title": "Spectral Subspace Analysis for Automatic Assessment of Pathological Speech Intelligibility",
      "original": "2791",
      "page_count": 5,
      "order": 634,
      "p1": "3038",
      "pn": "3042",
      "abstract": [
        "Speech intelligibility is an important assessment criterion of the\ncommunicative performance of pathological speakers. To assist clinicians\nin their assessment, time- and cost-efficient automatic intelligibility\nmeasures offering a repeatable and reliable assessment are desired.\nIn this paper, we propose to automatically assess pathological speech\nintelligibility based on a distance measure between the subspaces of\nspectral patterns of the pathological speech signal and of a fully\nintelligible (healthy) speech signal. To extract the subspace of spectral\npatterns we investigate two linear decomposition methods, i.e., Principal\nComponent Analysis and Approximate Joint Diagonalization. Pathological\nspeech intelligibility is then derived using a Grassman distance measure\nwhich quantifies the difference between the extracted subspaces of\npathological and healthy speech. Experiments on an English database\nof Cerebral Palsy patients show that the proposed intelligibility measure\nis significantly correlated with subjective intelligibility ratings.\nIn addition, comparisons to state-of-the-art measures show that the\nproposed subspace-based measure achieves a high performance with a\nsignificantly lower computational cost and without imposing any constraints\non the speech material of the speakers.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2791",
      "author_area_id": 3,
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "pasquale19_interspeech": {
      "authors": [
        [
          "Carolina De",
          "Pasquale"
        ],
        [
          "Charlie",
          "Cullen"
        ],
        [
          "Brian",
          "Vaughan"
        ]
      ],
      "title": "An Investigation of Therapeutic Rapport Through Prosody in Brief Psychodynamic Psychotherapy",
      "original": "2551",
      "page_count": 5,
      "order": 635,
      "p1": "3043",
      "pn": "3047",
      "abstract": [
        "Therapeutic alliance, a concept closely related to rapport, is one\nof the most important variables in psychotherapy. High degrees of synchrony/coordination\nin the therapeutic session are considered to contribute to rapport,\nand have received attention in the psychotherapy literature.<br style=\"mso-data-placement:same-cell;\"\n/><br style=\"mso-data-placement:same-cell;\" />\n  Coordinative behaviours\nare observable in speech, and they manifest in phenomena such as prosodic\naccommodation, a dynamic phenomenon closely related to conversational\nsuccess.<br style=\"mso-data-placement:same-cell;\" /><br style=\"mso-data-placement:same-cell;\"\n/>\n  A preliminary investigation of interpersonal prosodic dynamics\nin psychotherapy was performed on a database obtained in collaboration\nwith the University of Padua, consisting of 16 recordings making up\nthe entire course of a brief psychodynamic psychotherapy intervention\nfor a 25 year old female volunteer and a 41 years old male psychotherapist.<br\nstyle=\"mso-data-placement:same-cell;\" /><br style=\"mso-data-placement:same-cell;\"\n/>\n  The data was analysed with Time Aligned Moving Averages, a method\ncommonly used in interpersonal speech research. Issues of data sparsity\nare discussed, and preliminary results on the relationship between\nempathy and anxiety with interpersonal speech dynamics are presented.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2551",
      "author_area_id": 3,
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "rueda19_interspeech": {
      "authors": [
        [
          "Alice",
          "Rueda"
        ],
        [
          "J.C.",
          "V\u00e1squez-Correa"
        ],
        [
          "Cristian David",
          "Rios-Urrego"
        ],
        [
          "Juan Rafael",
          "Orozco-Arroyave"
        ],
        [
          "Sridhar",
          "Krishnan"
        ],
        [
          "Elmar",
          "N\u00f6th"
        ]
      ],
      "title": "Feature Representation of Pathophysiology of Parkinsonian Dysarthria",
      "original": "2490",
      "page_count": 5,
      "order": 636,
      "p1": "3048",
      "pn": "3052",
      "abstract": [
        "This paper focuses on selecting features that can best represent the\npathophysiology of Parkinson&#8217;s disease (PD) dysarthria. PD dysarthria\nhas often been the subject of feature selection and classification\nexperiments, but rarely have the selected features been attempted to\nbe matched to the pathophysiology of PD dysarthria. PD dysarthria manifests\nthrough changes in control of a person&#8217;s speech production muscles\nand affects respiration, articulation, resonance, and laryngeal properties,\nresulting in speech characteristics such as short phrases separated\nby pauses, reduced speed for non-repetitive syllables or supernormal\nspeed of repetitive syllables, reduced resonance, irregular vowel generation,\netc. Articulation, phonation, diadochokinesis (DDK) rhythm, and Empirical\nMode Decomposition (EMD) features were extracted from the DDK and sustained\n/a/ recordings of the Spanish GITA Corpus. These recordings were captured\nfrom 50 healthy (HC) and 50 PD subjects. A two-stage filter-wrapper\nfeature selection process was applied to reduce the number of features\nfrom 3,534 to 15. These 15 features mainly represent the instability\nof the voice and rhythm. SVM, Random Forest and Naive Bayes were used\nto test the discriminative power of the selected features. The results\nshowed that these sustained /a/ and /pa-ta-ka/ stability features could\nsuccessfully discriminate PD from HC with 70% accuracy.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2490",
      "author_area_id": 3,
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "onu19_interspeech": {
      "authors": [
        [
          "Charles C.",
          "Onu"
        ],
        [
          "Jonathan",
          "Lebensold"
        ],
        [
          "William L.",
          "Hamilton"
        ],
        [
          "Doina",
          "Precup"
        ]
      ],
      "title": "Neural Transfer Learning for Cry-Based Diagnosis of Perinatal Asphyxia",
      "original": "2340",
      "page_count": 5,
      "order": 637,
      "p1": "3053",
      "pn": "3057",
      "abstract": [
        "Despite continuing medical advances, the rate of newborn morbidity\nand mortality globally remains high, with over 6 million casualties\nevery year. The prediction of pathologies affecting newborns based\non their cry is thus of significant clinical interest, as it would\nfacilitate the development of accessible, low-cost diagnostic tools.\nHowever, the inadequacy of clinically annotated datasets of infant\ncries limits progress on this task. This study explores a neural transfer\nlearning approach to developing accurate and robust models for identifying\ninfants that have suffered from perinatal asphyxia. In particular,\nwe explore the hypothesis that representations learned from adult speech\ncould inform and improve performance of models developed on infant\nspeech. Our experiments show that models based on such representation\ntransfer are resilient to different types and degrees of noise, as\nwell as to signal loss in time and frequency domains.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2340",
      "author_area_id": 3,
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "hong19_interspeech": {
      "authors": [
        [
          "Hui-Ting",
          "Hong"
        ],
        [
          "Jeng-Lin",
          "Li"
        ],
        [
          "Yi-Ming",
          "Weng"
        ],
        [
          "Chip-Jin",
          "Ng"
        ],
        [
          "Chi-Chun",
          "Lee"
        ]
      ],
      "title": "Investigating the Variability of Voice Quality and Pain Levels as a Function of Multiple Clinical Parameters",
      "original": "2247",
      "page_count": 5,
      "order": 638,
      "p1": "3058",
      "pn": "3062",
      "abstract": [
        "Pain is an internal construct with vocal manifestation that varies\nas a function of personal and clinical attributes. Understanding the\nvocal indicators of pain-levels is important in providing an objective\nanalytic in clinical assessment and intervention. In this work, we\nfocus on investigating the variability of voice quality as a function\nof multiple clinical parameters at different pain-levels, specifically\nfor emergency room patients during triage. Their pain-induced pathological\nvoice quality characteristics are naturally affected by an individual\nattributes such as age, gender and pain-sites. We conduct a detailed\nmultivariate statistical analysis on a 181 unique patient&#8217;s vocal\nquality using recordings of real triage sessions. Our analysis show\nseveral important insights, 1) voice quality only varies statistically\nwith pain-levels when interacting effect from other clinical parameters\nis considered, 2) senior group shows a higher value of voicing probability\nand shimmer when experiencing severe pain, 3) patients with abdomen\npain have a lower jitter and shimmer during severe pain that is different\nfrom patients experiencing musculoskeletal pathology, and 4) there\ncould be a relationship between the variation in the voice quality\nand the neural pathway of pain as evident by interacting with the pain-site\nfactor.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2247",
      "author_area_id": 3,
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "lopez19_interspeech": {
      "authors": [
        [
          "Jos\u00e9 Vicente Egas",
          "L\u00f3pez"
        ],
        [
          "Juan Rafael",
          "Orozco-Arroyave"
        ],
        [
          "G\u00e1bor",
          "Gosztolya"
        ]
      ],
      "title": "Assessing Parkinson&#8217;s Disease from Speech Using Fisher Vectors",
      "original": "2217",
      "page_count": 5,
      "order": 639,
      "p1": "3063",
      "pn": "3067",
      "abstract": [
        "Parkinson&#8217;s Disease (PD) is a neuro-degenerative disorder that\naffects primarily the motor system of the body. Besides other functions,\nthe subject&#8217;s speech also deteriorates during the disease, which\nallows for a non-invasive way of automatic screening. In this study,\nwe represent the utterances of subjects having PD and those of healthy\ncontrols by means of the Fisher Vector approach. This technique is\nvery common in the area of image recognition, where it provides a representation\nof the local image descriptors via frequency and high order statistics.\nIn the present work, we used four frame-level feature sets as the input\nof the FV method, and applied (linear) Support Vector Machines (SVM)\nfor classifying the speech of subjects. We found that our approach\noffers superior performance compared to classification based on the\ni-vector and cosine distance approach, and it also provides an efficient\ncombination of machine learning models trained on different feature\nsets or on different speaker tasks.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2217"
    },
    "klumpp19_interspeech": {
      "authors": [
        [
          "Philipp",
          "Klumpp"
        ],
        [
          "J.C.",
          "V\u00e1squez-Correa"
        ],
        [
          "Tino",
          "Haderlein"
        ],
        [
          "Elmar",
          "N\u00f6th"
        ]
      ],
      "title": "Feature Space Visualization with Spatial Similarity Maps for Pathological Speech Data",
      "original": "2080",
      "page_count": 5,
      "order": 640,
      "p1": "3068",
      "pn": "3072",
      "abstract": [
        "The feature vectors of a data set encode information about relations\nbetween speaker groups, clusters and outliers. Based on the assumption\nthat these relations are conserved within the spatial properties of\nfeature vectors, we introduce similarity maps to visualize consistencies\nand deviations in magnitude and orientation between two feature vectors.\nWe also present an iterative approach to find subspaces of a high-dimensional\nfeature space that encode information about predefined speaker clusters.\nThe methods were evaluated with two different data sets, one from chronically\nhoarse speakers and a second one from Parkinson&#8217;s Disease patients\nand a healthy control group. The results showed that similarity maps\nprovide a decent visualization of speaker groups and the spatial properties\nof their respective feature vectors. With the iterative optimization,\nit was possible to find features that show pronounced spatial differences\nbetween predefined clusters.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2080",
      "author_area_id": 3,
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "chakravarthula19_interspeech": {
      "authors": [
        [
          "Sandeep Nallan",
          "Chakravarthula"
        ],
        [
          "Haoqi",
          "Li"
        ],
        [
          "Shao-Yen",
          "Tseng"
        ],
        [
          "Maija",
          "Reblin"
        ],
        [
          "Panayiotis",
          "Georgiou"
        ]
      ],
      "title": "Predicting Behavior in Cancer-Afflicted Patient and Spouse Interactions Using Speech and Language",
      "original": "1888",
      "page_count": 5,
      "order": 641,
      "p1": "3073",
      "pn": "3077",
      "abstract": [
        "Cancer impacts the quality of life of those diagnosed as well as their\nspouse caregivers, in addition to potentially influencing their day-to-day\nbehaviors. There is evidence that effective communication between spouses\ncan improve well-being related to cancer but it is difficult to efficiently\nevaluate the quality of daily life interactions using manual annotation\nframeworks. Automated recognition of behaviors based on the interaction\ncues of speakers can help analyze interactions in such couples and\nidentify behaviors which are beneficial for effective communication.\nIn this paper, we present and detail a dataset of dyadic interactions\nin 85 real-life cancer-afflicted couples and a set of observational\nbehavior codes pertaining to interpersonal communication attributes.\nWe describe and employ neural network-based systems for classifying\nthese behaviors based on turn-level acoustic and lexical speech patterns.\nFurthermore, we investigate the effect of controlling for factors such\nas gender, patient/caregiver role and conversation content on behavior\nclassification. Analysis of our preliminary results indicates the challenges\nin this task due to the nature of the targeted behaviors and suggests\nthat techniques incorporating contextual processing might be better\nsuited to tackle this problem.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1888",
      "author_area_id": 3,
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "qin19_interspeech": {
      "authors": [
        [
          "Ying",
          "Qin"
        ],
        [
          "Tan",
          "Lee"
        ],
        [
          "Anthony Pak Hin",
          "Kong"
        ]
      ],
      "title": "Automatic Assessment of Language Impairment Based on Raw ASR Output",
      "original": "1688",
      "page_count": 5,
      "order": 642,
      "p1": "3078",
      "pn": "3082",
      "abstract": [
        "For automatic assessment of language impairment in natural speech,\nproperly designed text-based features are needed. The feature design\nrelies on experts&#8217; domain knowledge and the feature extraction\nprocess may undesirably involve manual effort on transcribing. This\npaper describes a novel approach to automatic assessment of language\nimpairment in narrative speech of people with aphasia (PWA), without\nexplicit knowledge-driven feature design. A convolutional neural network\n(CNN) is used to extract language impairment related text features\nfrom the output of an automatic speech recognition (ASR) system or,\nif available, the manual transcription of input speech. To mitigate\nthe adverse effect of ASR errors, confusion network is adopted to improve\nthe robustness of embedding representation of ASR output. The proposed\napproach is evaluated on the task of discriminating severe PWA from\nmild PWA based on Cantonese narrative speech. Experimental results\nconfirm the effectiveness of automatically learned text features. It\nis also shown that CNN models trained with text input and acoustic\nfeatures are complementary to each other.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1688",
      "author_area_id": 3,
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "fu19_interspeech": {
      "authors": [
        [
          "Zhen",
          "Fu"
        ],
        [
          "Xihong",
          "Wu"
        ],
        [
          "Jing",
          "Chen"
        ]
      ],
      "title": "Effects of Spectral and Temporal Cues to Mandarin Concurrent-Vowels Identification for Normal-Hearing and Hearing-Impaired Listeners",
      "original": "3209",
      "page_count": 5,
      "order": 643,
      "p1": "3083",
      "pn": "3087",
      "abstract": [
        "In Mandarin Chinese, lexical Tones are inherently bonded with vowels,\nmaking both spectral and temporal cues available for speech perception.\nTemporal cues provided by Tone contrast have been shown facilitating\nsegregation in Mandarin concurrent-vowels identification (MCVI). The\npresent study investigated the effect of spectral cue measured by vowel\ncontrast within the syllable-pair on MCVI, both for normal-hearing\n(NH) and hearing-impaired (HI) listeners. Acoustic cues of duration\nand mean F0 difference were carefully controlled. Results exhibited\nthat facilitation from vowel contrast existed for NH listeners but\nwas reduced for HI listeners. Identification score positively correlated\nwith the spectral envelope contrast of different vowel-pairs for both\ngroups, but the coefficient for HI listeners was lower. Further analyses\nbased on a power function model revealed more weighting of temporal\ncues than spectral cues for NH listeners, while the contributions were\nequal for HI listeners. These results suggested that the spectral cue\nprovided by vowel contrast could facilitate the MCVI, and auditory\nprocessing of temporal cues might be more susceptible to hearing loss\nthan that of spectral cues. These findings have instructions for designing\nspeech processing algorithms for Mandarin-speaking HI listeners.\n"
      ],
      "doi": "10.21437/Interspeech.2019-3209",
      "author_area_id": 1,
      "author_area_label": "Speech Perception, Production and Acquisition"
    },
    "zayats19_interspeech": {
      "authors": [
        [
          "Vicky",
          "Zayats"
        ],
        [
          "Trang",
          "Tran"
        ],
        [
          "Richard",
          "Wright"
        ],
        [
          "Courtney",
          "Mansfield"
        ],
        [
          "Mari",
          "Ostendorf"
        ]
      ],
      "title": "Disfluencies and Human Speech Transcription Errors",
      "original": "3134",
      "page_count": 5,
      "order": 644,
      "p1": "3088",
      "pn": "3092",
      "abstract": [
        "This paper explores contexts associated with errors in transcription\nof spontaneous speech, shedding light on human perception of disfluencies\nand other conversational speech phenomena. A new version of the Switchboard\ncorpus is provided with disfluency annotations for careful speech transcripts,\ntogether with results showing the impact of transcription errors on\nevaluation of automatic disfluency detection.\n"
      ],
      "doi": "10.21437/Interspeech.2019-3134",
      "author_area_id": 1,
      "author_area_label": "Speech Perception, Production and Acquisition"
    },
    "parhammer19_interspeech": {
      "authors": [
        [
          "Sandra I.",
          "Parhammer"
        ],
        [
          "Miriam",
          "Ebersberg"
        ],
        [
          "Jenny",
          "Tippmann"
        ],
        [
          "Katja",
          "St\u00e4rk"
        ],
        [
          "Andreas",
          "Opitz"
        ],
        [
          "Barbara",
          "Hinger"
        ],
        [
          "Sonja",
          "Rossi"
        ]
      ],
      "title": "The Influence of Distraction on Speech Processing: How Selective is Selective Attention?",
      "original": "2699",
      "page_count": 5,
      "order": 645,
      "p1": "3093",
      "pn": "3097",
      "abstract": [
        "The present study investigated the effects of selective attention on\nthe processing of morphosyntactic errors in unattended parts of speech.\nTwo groups of German native (L1) speakers participated in the present\nstudy. Participants listened to sentences in which irregular verbs\nwere manipulated in three different conditions (correct, incorrect\nbut attested ablaut pattern, incorrect and crosslinguistically unattested\nablaut pattern). In order to track fast dynamic neural reactions to\nthe stimuli, electroencephalography was used. After each sentence,\nparticipants in Experiment 1 performed a semantic judgement task, which\ndeliberately distracted the participants from the syntactic manipulations\nand directed their attention to the semantic content of the sentence.\nIn Experiment 2, participants carried out a syntactic judgement task,\nwhich put their attention on the critical stimuli. The use of two different\nattentional tasks allowed for investigating the impact of selective\nattention on speech processing and whether morphosyntactic processing\nsteps are performed automatically. In Experiment 2, the incorrect attested\ncondition elicited a larger N400 component compared to the correct\ncondition, whereas in Experiment 1 no differences between conditions\nwere found. These results suggest that the processing of morphosyntactic\nviolations in irregular verbs is not entirely automatic but seems to\nbe strongly affected by selective attention.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2699",
      "author_area_id": 1,
      "author_area_label": "Speech Perception, Production and Acquisition"
    },
    "hazan19_interspeech": {
      "authors": [
        [
          "Valerie",
          "Hazan"
        ],
        [
          "Outi",
          "Tuomainen"
        ],
        [
          "Linda",
          "Taschenberger"
        ]
      ],
      "title": "Subjective Evaluation of Communicative Effort for Younger and Older Adults in Interactive Tasks with Energetic and Informational Masking",
      "original": "2215",
      "page_count": 5,
      "order": 646,
      "p1": "3098",
      "pn": "3102",
      "abstract": [
        "The impact of energetic (EM) and informational masking (IM) on speech\ncommunication is typically evaluated using perception tests that do\nnot involve actual communication. Here, ratings of effort, concentration\nand degree of interference were obtained for 51 young, middle-aged\nand older adults after they had completed communicative tasks (Diapix)\nwith another participant in conditions in which no noise, speech-shaped\nnoise, or three voices were heard in the background. They also completed\nbackground sensory and cognitive tests and a quality of hearing questionnaire.\nThe EM condition was perceived as less effortful, requiring less concentration\nand easier to ignore than those involving IM. Effects were generally\ngreater for talkers taking the lead in the interaction. Even though\nthe two older groups were more affected by IM than young adults in\na speech in noise perception test, age did not impact on ratings of\neffort and ability to ignore the noise in the diapix communicative\ntask. Only for concentration ratings, did the Older Adult group give\nsimilar ratings in quiet as when EM was present. Together, these results\nsuggest that evaluations that purely assess receptive speech in older\nadults do not fully represent the impact of sources of interference\non speech communication.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2215",
      "author_area_id": 1,
      "author_area_label": "Speech Perception, Production and Acquisition"
    },
    "davis19_interspeech": {
      "authors": [
        [
          "Chris",
          "Davis"
        ],
        [
          "Jeesun",
          "Kim"
        ]
      ],
      "title": "Perceiving Older Adults Producing Clear and Lombard Speech",
      "original": "2210",
      "page_count": 5,
      "order": 647,
      "p1": "3103",
      "pn": "3107",
      "abstract": [
        "We investigated the perceptual salience of clear and Lombard speech\nadaptations by older adults (OA) communicating to a younger partner\nin a diapix task. The aim was to determine whether these two speech\nstyles are perceptually distinct (for auditory and visual speech).\nThe communication setting involved either the younger partner only\nin babble noise (BAB_partner) or both talkers in babble noise (BAB_both).\nIn the control condition (NORM), both talkers heard normally. To determine\nhow perceptible OA adaptions to these noise conditions were, short\n(1&#8211;4 s) auditory only and visual only recordings of the OA talking\nto their partner were presented in two perception experiments. In Experiment\n1, half of the OA stimuli were from the BAB_partner and half from the\nNORM condition; and participants were asked to judge whether the older\nadult was taking to a person who could hear them well or to someone\nwho has trouble hearing them. In Experiment 2 participants decided\nbetween NORM and BAB_both stimuli. Participants did both sound-only\nand visual-only versions. Results showed both adaptations were perceived\nbetter than chance; the BAB_both condition was discriminated better\nfrom NORM than the BAB_partner one, and auditory judgements were better\nthan visual ones (although these were correlated).\n"
      ],
      "doi": "10.21437/Interspeech.2019-2210",
      "author_area_id": 1,
      "author_area_label": "Speech Perception, Production and Acquisition"
    },
    "ariasvergara19_interspeech": {
      "authors": [
        [
          "T.",
          "Arias-Vergara"
        ],
        [
          "Juan Rafael",
          "Orozco-Arroyave"
        ],
        [
          "Milos",
          "Cernak"
        ],
        [
          "S.",
          "Gollwitzer"
        ],
        [
          "M.",
          "Schuster"
        ],
        [
          "Elmar",
          "N\u00f6th"
        ]
      ],
      "title": "Phone-Attribute Posteriors to Evaluate the Speech of Cochlear Implant Users",
      "original": "2144",
      "page_count": 5,
      "order": 648,
      "p1": "3108",
      "pn": "3112",
      "abstract": [
        "People with pre- and postlingual onset of deafness, i.e, age of occurrence\nof hearing loss, often present speech production problems even after\nhearing rehabilitation by cochlear implantation. In this paper, the\nspeech of 20 prelinguals (aged between 18 to 71 years old), 20 postlinguals\n(aged between 33 to 78 years old) and 20 healthy control (aged between\n31 to 62 years old) German native speakers are analyzed considering\nphone-attribute features extracted with pre-trained Deep Neural Networks.\nSpeech signals are analyzed with reference to the manner of articulation\nof consonants according to 5 groups: nasals, sibilants, fricatives,\nvoiced-stops, and voiceless-stops. According to the results, it is\npossible to detect alterations in the consonant production of CI users\nwhen compared with healthy speakers. A comprehensive evaluation of\nspeech changes of CI users will help in the rehabilitation after deafening.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2144",
      "author_area_id": 1,
      "author_area_label": "Speech Perception, Production and Acquisition"
    },
    "hodoshima19_interspeech": {
      "authors": [
        [
          "Nao",
          "Hodoshima"
        ]
      ],
      "title": "Effects of Urgent Speech and Congruent/Incongruent Text on Speech Intelligibility in Noise and Reverberation",
      "original": "1902",
      "page_count": 5,
      "order": 649,
      "p1": "3113",
      "pn": "3117",
      "abstract": [
        "Public-address (PA) announcements are widely used, but noise and reverberation\ncan render them unintelligible. Furthermore, in an emergency, textual\ninformation available to smartphone users or displayed on electronic\nbulletin boards may not coincide with PA announcements, and this mismatch\nmay degrade the intelligibility of PA announcements. This study investigated\nhow speech spoken in a normal/urgent style and preceding congruent/incongruent\ntextual information affected word intelligibility and perceived urgency\nin noisy and reverberant environments. The results obtained from 18\nparticipants showed that the word correct rate (WCR) was significantly\nhigher for urgently spoken speech than for normal speech, and for congruent\ntext than for incongruent/no text. However, there was no speaking style-text\ninteraction, indicating that the improvement in WCR provided by urgent\nspeech over normal speech was the same regardless of the preceding\ntext condition. This suggests that listeners rely more on visual information\nwhen speech intelligibility is poor. The results for perceived urgency\nalso showed that the congruent condition was rated &#8220;evacuate\nnow&#8221;, while the incongruent condition was rated &#8220;wait and\nsee&#8221;. These results suggest that simple combinations of speaking\nstyle and textual information decrease the intelligibility of emergency\nPA announcements, and audio-visual incongruence must be considered.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1902",
      "author_area_id": 1,
      "author_area_label": "Speech Perception, Production and Acquisition"
    },
    "mamun19_interspeech": {
      "authors": [
        [
          "Nursadul",
          "Mamun"
        ],
        [
          "Ria",
          "Ghosh"
        ],
        [
          "John H.L.",
          "Hansen"
        ]
      ],
      "title": "Quantifying Cochlear Implant Users&#8217; Ability for Speaker Identification Using CI Auditory Stimuli",
      "original": "1852",
      "page_count": 5,
      "order": 650,
      "p1": "3118",
      "pn": "3122",
      "abstract": [
        "Speaker recognition is a biometric modality that uses underlying speech\ninformation to determine the identity of the speaker. Speaker Identification\n(SID) under noisy conditions is one of the challenging topics in the\nfield of speech processing, specifically when it comes to individuals\nwith cochlear implants (CI). This study analyzes and quantifies the\nability of CI-users to perform speaker identification based on direct\nelectric auditory stimuli. CI users employ a limited number of frequency\nbands (8&#126;22) and use electrodes to directly stimulate the Basilar\nMembrane/Cochlear in order to recognize the speech signal. The sparsity\nof electric stimulation within the CI frequency range is a prime reason\nfor loss in human speech recognition, as well as SID performance. Therefore,\nit is assumed that CI-users might be unable to recognize and distinguish\na speaker given dependent information such as formant frequencies,\npitch etc. which are lost to un-simulated electrodes. To quantify this\nassumption, the input speech signal is processed using a CI Advanced\nCombined Encoder (ACE) signal processing strategy to construct the\nCI auditory electrodogram. The proposed study uses 50 speakers from\neach of three different databases for training the system using two\ndifferent classifiers under quiet, and tested under both quiet and\nnoisy conditions. The objective result shows that, the CI users can\neffectively identify a limited number of speakers. However, their performance\ndecreases when more speakers are added in the system, as well as when\nnoisy conditions are introduced. This information could therefore be\nused for improving CI-user signal processing techniques to improve\nhuman SID.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1852",
      "author_area_id": 1,
      "author_area_label": "Speech Perception, Production and Acquisition"
    },
    "felker19_interspeech": {
      "authors": [
        [
          "E.",
          "Felker"
        ],
        [
          "Mirjam",
          "Ernestus"
        ],
        [
          "Mirjam",
          "Broersma"
        ]
      ],
      "title": "Lexically Guided Perceptual Learning of a Vowel Shift in an Interactive L2 Listening Context",
      "original": "1414",
      "page_count": 5,
      "order": 651,
      "p1": "3123",
      "pn": "3127",
      "abstract": [
        "Lexically guided perceptual learning has traditionally been studied\nwith ambiguous consonant sounds to which native listeners are exposed\nin a purely receptive listening context. To extend previous research,\nwe investigate whether lexically guided learning applies to a vowel\nshift encountered by non-native listeners in an interactive dialogue.\nDutch participants played a two-player game in English in either a\ncontrol condition, which contained no evidence for a vowel shift, or\na lexically constraining condition, in which onscreen lexical information\nrequired them to re-interpret their interlocutor&#8217;s /&#618;/ pronunciations\nas representing /&#949;/. A phonetic categorization pre-test and post-test\nwere used to assess whether the game shifted listeners&#8217; phonemic\nboundaries such that more of the /&#949;/-/&#618;/ continuum came to\nbe perceived as /&#949;/. Both listener groups showed an overall post-test\nshift toward /&#618;/, suggesting that vowel perception may be sensitive\nto directional biases related to properties of the speaker&#8217;s\nvowel space. Importantly, listeners in the lexically constraining condition\nmade relatively more post-test /&#949;/ responses than the control\ngroup, thereby exhibiting an effect of lexically guided adaptation.\nThe results thus demonstrate that non-native listeners can adjust their\nphonemic boundaries on the basis of lexical information to accommodate\na vowel shift learned in interactive conversation.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1414",
      "author_area_id": 1,
      "author_area_label": "Speech Perception, Production and Acquisition"
    },
    "paulus19_interspeech": {
      "authors": [
        [
          "Maximillian",
          "Paulus"
        ],
        [
          "Valerie",
          "Hazan"
        ],
        [
          "Patti",
          "Adank"
        ]
      ],
      "title": "Talker Intelligibility and Listening Effort with Temporally Modified Speech",
      "original": "1402",
      "page_count": 5,
      "order": 652,
      "p1": "3128",
      "pn": "3132",
      "abstract": [
        "Individual differences in talker acoustics substantially affect intelligibility\nin adverse listening conditions. Spectral enhancement has been found\nto reliably boost intelligibility in noise while temporal enhancement\nremains less effective. A potentially mediating factor that has been\nignored so far is listening effort, as objectively assessed by the\npupil dilation response. In two perception experiments, we measured\nintelligibility (keyword recall scores) and listening effort (pupil\ndilation) for two talkers in two listening conditions and with varying\ndegrees of temporal modification. Results suggest that while keyword\nrecall scores are sensitive to individual talker differences across\nlistening conditions, the pupil dilation response reflects the degree\nof temporal and spectral distortion introduced by the signal processing\ntechniques.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1402",
      "author_area_id": 1,
      "author_area_label": "Speech Perception, Production and Acquisition"
    },
    "ward19b_interspeech": {
      "authors": [
        [
          "Lauren",
          "Ward"
        ],
        [
          "Catherine",
          "Robinson"
        ],
        [
          "Matthew",
          "Paradis"
        ],
        [
          "Katherine M.",
          "Tucker"
        ],
        [
          "Ben",
          "Shirley"
        ]
      ],
      "title": "R<SUP>2</SUP>SPIN: Re-Recording the Revised Speech Perception in Noise Test",
      "original": "1281",
      "page_count": 5,
      "order": 653,
      "p1": "3133",
      "pn": "3137",
      "abstract": [
        "Speech in noise tests are an important clinical and research tool for\nunderstanding speech perception in realistic, adverse listening conditions.\nThough relatively simple to implement, their development is time and\nresource intensive. As a result, many tests still in use (and their\ncorresponding recordings) are outdated and no longer fit for purpose.\nThis work takes the popular Revised Speech Perception In Noise (RSPIN)\nTest and updates it with improved recordings and the addition of a\nfemale speaker. It outlines and evaluates a methodology which others\ncan apply to legacy recordings of speech in noise tests to update them\nand ensure their ongoing usability. This paper describes the original\ntest along with its use over the last four decades and the rationale\nfor re-recording. The new speakers, new accent (Received Pronunciation)\nand recording methodology are then outlined. Subjective and objective\nanalysis of the new recordings for normal hearing listeners are then\ngiven. The paper concludes with recommendations for using the R<SUP>2</SUP>SPIN.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1281"
    },
    "chen19j_interspeech": {
      "authors": [
        [
          "Fei",
          "Chen"
        ]
      ],
      "title": "Contributions of Consonant-Vowel Transitions to Mandarin Tone Identification in Simulated Electric-Acoustic Hearing",
      "original": "1124",
      "page_count": 5,
      "order": 654,
      "p1": "3138",
      "pn": "3142",
      "abstract": [
        "For hearing-impaired listeners fitted with cochlear implants (CIs),\nthey rely on electric (E) stimulation with primarily slow-varying temporal\ninformation but limited spectral information for their speech perception.\nMany recent studies showed that for those implanted listeners with\nresidual low-frequency hearing, the combined electric-acoustic (E+A)\nstimulation could significantly improve their speech perception in\nadverse listening conditions. The present work assessed the contributions\nof consonant-vowel transitions to Mandarin tone identification via\na vocoder based simulation of E+A stimulation. Isolated Mandarin words\nwere processed to preserve full consonants and vowel onsets across\nconsonant-vowel transitions, and replace the rest with noise. The two\ntypes of vocoded stimuli, simulating E and E+A stimulations, were presented\nto normal-hearing Mandarin-speaking listeners to identify lexical tones.\nResults consistently showed the advantage of E+A stimulation over E-only\nstimulation when full consonants and the same amount of vowel onset\nsegments were preserved for lexical tone identification. In addition,\ncompared with E stimulation with full vowel segments, the combined-stimulation\nadvantage was observed even when only a small portion of vowel onset\nsegments were presented. Results in this work suggested that in E+A\nstimulation, segmental contributions were able to provide tone identification\nbenefit relative to E stimulation with the entire Mandarin words.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1124",
      "author_area_id": 1,
      "author_area_label": "Speech Perception, Production and Acquisition"
    },
    "pirhosseinloo19_interspeech": {
      "authors": [
        [
          "Shadi",
          "Pirhosseinloo"
        ],
        [
          "Jonathan S.",
          "Brumberg"
        ]
      ],
      "title": "Monaural Speech Enhancement with Dilated Convolutions",
      "original": "2782",
      "page_count": 5,
      "order": 655,
      "p1": "3143",
      "pn": "3147",
      "abstract": [
        "In this study, we propose a novel dilated convolutional neural network\nfor enhancing speech in noisy and reverberant environments. The proposed\nmodel incorporates dilated convolutions for tracking a target speaker\nthrough context aggregations, skip connections, and residual learning\nfor mapping-based monaural speech enhancement. The performance of our\nmodel was evaluated in a variety of simulated environments having different\nreverberation times and quantified using two objective measures. Experimental\nresults show that the proposed model outperforms a long short-term\nmemory (LSTM), a gated residual network (GRN) and convolutional recurrent\nnetwork (CRN) model in terms of objective speech intelligibility and\nspeech quality in noisy and reverberant environments. Compared to LSTM,\nCRN and GRN, our method has improved generalization to untrained speakers\nand noise, and has fewer training parameters resulting in greater computational\nefficiency.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2782",
      "author_area_id": 6,
      "author_area_label": "Speech Coding and Enhancement"
    },
    "liao19b_interspeech": {
      "authors": [
        [
          "Chien-Feng",
          "Liao"
        ],
        [
          "Yu",
          "Tsao"
        ],
        [
          "Hung-Yi",
          "Lee"
        ],
        [
          "Hsin-Min",
          "Wang"
        ]
      ],
      "title": "Noise Adaptive Speech Enhancement Using Domain Adversarial Training",
      "original": "1519",
      "page_count": 5,
      "order": 656,
      "p1": "3148",
      "pn": "3152",
      "abstract": [
        "In this study, we propose a novel noise adaptive speech enhancement\n(SE) system, which employs a domain adversarial training (DAT) approach\nto tackle the issue of a noise type mismatch between the training and\ntesting conditions. Such a mismatch is a critical problem in deep-learning-based\nSE systems. A large mismatch may cause a serious performance degradation\nto the SE performance. Because we generally use a well-trained SE system\nto handle various unseen noise types, a noise type mismatch commonly\noccurs in real-world scenarios. The proposed noise adaptive SE system\ncontains an encoder-decoder-based enhancement model and a domain discriminator\nmodel. During adaptation, the DAT approach encourages the encoder to\nproduce noise-invariant features based on the information from the\ndiscriminator model and consequentially increases the robustness of\nthe enhancement model to unseen noise types. Herein, we regard stationary\nnoises as the source domain (with the ground truth of clean speech)\nand non-stationary noises as the target domain (without the ground\ntruth). We evaluated the proposed system on TIMIT sentences. The experiment\nresults show that the proposed noise adaptive SE system successfully\nprovides significant improvements in PESQ (19.0%), SSNR (39.3%), and\nSTOI (27.0%) over the SE system without an adaptation.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1519",
      "author_area_id": 6,
      "author_area_label": "Speech Coding and Enhancement"
    },
    "ge19_interspeech": {
      "authors": [
        [
          "Meng",
          "Ge"
        ],
        [
          "Longbiao",
          "Wang"
        ],
        [
          "Nan",
          "Li"
        ],
        [
          "Hao",
          "Shi"
        ],
        [
          "Jianwu",
          "Dang"
        ],
        [
          "Xiangang",
          "Li"
        ]
      ],
      "title": "Environment-Dependent Attention-Driven Recurrent Convolutional Neural Network for Robust Speech Enhancement",
      "original": "1477",
      "page_count": 5,
      "order": 657,
      "p1": "3153",
      "pn": "3157",
      "abstract": [
        "Speech enhancement aims to keep the real speech signal and reduce noise\nfor building robust communication systems. Under the success of DNN,\nsignificant progress has been made. Nevertheless, accuracy of the speech\nenhancement system is not satisfactory due to insufficient consideration\nof varied environmental and contextual information in complex cases.\nTo address these problems, this research proposes an end-to-end environment-dependent\nattention-driven approach. The local frequency-temporal pattern via\nconvolutional neural network is fully employed without pooling operation.\nIt then integrates an attention mechanism into bidirectional long short-term\nmemory to acquire the weighted dynamic context between consecutive\nframes. Furthermore, dynamic environment estimation and phase correction\nfurther improve the generalization ability. Extensive experimental\nresults on REVERB challenge demonstrated that the proposed approach\noutperformed existing methods, improving PESQ from 2.56 to 2.87 and\nSRMR from 4.95 to 5.50 compared with conventional DNN.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1477",
      "author_area_id": 6,
      "author_area_label": "Speech Coding and Enhancement"
    },
    "pariente19_interspeech": {
      "authors": [
        [
          "Manuel",
          "Pariente"
        ],
        [
          "Antoine",
          "Deleforge"
        ],
        [
          "Emmanuel",
          "Vincent"
        ]
      ],
      "title": "A Statistically Principled and Computationally Efficient Approach to Speech Enhancement Using Variational Autoencoders",
      "original": "1398",
      "page_count": 5,
      "order": 658,
      "p1": "3158",
      "pn": "3162",
      "abstract": [
        "Recent studies have explored the use of deep generative models of speech\nspectra based on variational autoencoders (VAEs), combined with unsupervised\nnoise models, to perform speech enhancement. These studies developed\niterative algorithms involving either Gibbs sampling or gradient descent\nat each step, making them computationally expensive. This paper proposes\na variational inference method to iteratively estimate the power spectrogram\nof the clean speech. Our main contribution is the analytical derivation\nof the variational steps in which the encoder of the pre-learned VAE\ncan be used to estimate the variational approximation of the true posterior\ndistribution, using the very same assumption made to train VAEs. Experiments\nshow that the proposed method produces results on par with the aforementioned\niterative methods using sampling, while decreasing the computational\ncost by a factor 36 to reach a given performance.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1398",
      "author_area_id": 6,
      "author_area_label": "Speech Coding and Enhancement"
    },
    "lin19d_interspeech": {
      "authors": [
        [
          "Ju",
          "Lin"
        ],
        [
          "Sufeng",
          "Niu"
        ],
        [
          "Zice",
          "Wei"
        ],
        [
          "Xiang",
          "Lan"
        ],
        [
          "Adriaan J. van",
          "Wijngaarden"
        ],
        [
          "Melissa C.",
          "Smith"
        ],
        [
          "Kuang-Ching",
          "Wang"
        ]
      ],
      "title": "Speech Enhancement Using Forked Generative Adversarial Networks with Spectral Subtraction",
      "original": "2954",
      "page_count": 5,
      "order": 659,
      "p1": "3163",
      "pn": "3167",
      "abstract": [
        "Speech enhancement techniques that use a generative adversarial network\n(GAN) can effectively suppress noise while allowing models to be trained\nend-to-end. However, such techniques directly operate on time-domain\nwaveforms, which are often highly-dimensional and require extensive\ncomputation. This paper proposes a novel GAN-based speech enhancement\nmethod, referred to as S-ForkGAN, that operates on log-power spectra\nrather than on time-domain speech waveforms, and uses a forked GAN\nstructure to extract both speech and noise information. By operating\non log-power spectra, one can seamlessly include conventional spectral\nsubtraction techniques, and the parameter space typically has a lower\ndimension. The performance of S-ForkGAN is assessed for automatic speech\nrecognition (ASR) using the TIMIT data set and a wide range of noise\nconditions. It is shown that S-ForkGAN outperforms existing GAN-based\ntechniques and that it has a lower complexity.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2954",
      "author_area_id": 6,
      "author_area_label": "Speech Coding and Enhancement"
    },
    "zezario19_interspeech": {
      "authors": [
        [
          "Ryandhimas E.",
          "Zezario"
        ],
        [
          "Szu-Wei",
          "Fu"
        ],
        [
          "Xugang",
          "Lu"
        ],
        [
          "Hsin-Min",
          "Wang"
        ],
        [
          "Yu",
          "Tsao"
        ]
      ],
      "title": "Specialized Speech Enhancement Model Selection Based on Learned Non-Intrusive Quality Assessment Metric",
      "original": "2425",
      "page_count": 5,
      "order": 660,
      "p1": "3168",
      "pn": "3172",
      "abstract": [
        "Previous studies have shown that a specialized speech enhancement model\ncan outperform a general model when the test condition is matched to\nthe training condition. Therefore, choosing the correct (matched) candidate\nmodel from a set of ensemble models is critical to achieve generalizability.\nAlthough the best decision criterion should be based directly on the\nevaluation metric, the need for a clean reference makes it impractical\nfor employment. In this paper, we propose a novel specialized speech\nenhancement model selection (SSEMS) approach that applies a non-intrusive\nquality estimation model, termed Quality-Net, to solve this problem.\nExperimental results first confirm the effectiveness of the proposed\nSSEMS approach. Moreover, we observe that the correctness of Quality-Net\nin choosing the most suitable model increases as input noisy SNR increases,\nand thus the results of the proposed systems outperform another auto-encoder-based\nmodel selection and a general model, particularly under high SNR conditions.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2425",
      "author_area_id": 6,
      "author_area_label": "Speech Coding and Enhancement"
    },
    "chuang19_interspeech": {
      "authors": [
        [
          "Fu-Kai",
          "Chuang"
        ],
        [
          "Syu-Siang",
          "Wang"
        ],
        [
          "Jeih-weih",
          "Hung"
        ],
        [
          "Yu",
          "Tsao"
        ],
        [
          "Shih-Hau",
          "Fang"
        ]
      ],
      "title": "Speaker-Aware Deep Denoising Autoencoder with Embedded Speaker Identity for Speech Enhancement",
      "original": "2108",
      "page_count": 5,
      "order": 661,
      "p1": "3173",
      "pn": "3177",
      "abstract": [
        "Previous studies indicate that noise and speaker variations can degrade\nthe performance of deep-learning-based speech-enhancement systems.\nTo increase the system performance over environmental variations, we\npropose a novel speaker-aware system that integrates a deep denoising\nautoencoder (DDAE) with an embedded speaker identity. The overall system\nfirst extracts embedded speaker identity features using a neural network\nmodel; then the DDAE takes the augmented features as input to generate\nenhanced spectra. With the additional embedded features, the speech-enhancement\nsystem can be guided to generate the optimal output corresponding to\nthe speaker identity. We tested the proposed speech-enhancement system\non the TIMIT dataset. Experimental results showed that the proposed\nspeech-enhancement system could improve the sound quality and intelligibility\nof speech signals from additive noise-corrupted utterances. In addition,\nthe results suggested system robustness for unseen speakers when combined\nwith speaker features.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2108",
      "author_area_id": 6,
      "author_area_label": "Speech Coding and Enhancement"
    },
    "liu19g_interspeech": {
      "authors": [
        [
          "Yun",
          "Liu"
        ],
        [
          "Hui",
          "Zhang"
        ],
        [
          "Xueliang",
          "Zhang"
        ],
        [
          "Yuhang",
          "Cao"
        ]
      ],
      "title": "Investigation of Cost Function for Supervised Monaural Speech Separation",
      "original": "1897",
      "page_count": 5,
      "order": 662,
      "p1": "3178",
      "pn": "3182",
      "abstract": [
        "Speech separation aims to improve the speech quality of noisy speech.\nDeep learning based speech separation methods usually use mean square\nerror (MSE) as the cost function, which measures the distance between\nmodel output and training target. However, the MSE does not match the\nevaluation metrics perfectly. Optimizing the MSE does not directly\nlead to improvement in the commonly used metrics, such as short-time\nobjective intelligibility (STOI), perceptual evaluation of speech quality\n(PESQ), signal-to-noise ratio (SNR) and source-to-distortion ratio\n(SDR). In this study, we inspect some other cost function candidates\nwhich are based on divergence, e.g., Kullback-Leibler and Itakura-Saito\ndivergence. A conjecture about the correlation between cost function\nand evaluation metrics is proposed and examined to explain why these\ncost functions behave differently. On the basis of the proposed conjecture,\nthe optimal cost function candidate is selected. The experimental results\nvalidate our conjecture.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1897"
    },
    "shi19b_interspeech": {
      "authors": [
        [
          "Ziqiang",
          "Shi"
        ],
        [
          "Huibin",
          "Lin"
        ],
        [
          "Liu",
          "Liu"
        ],
        [
          "Rujie",
          "Liu"
        ],
        [
          "Jiqing",
          "Han"
        ],
        [
          "Anyan",
          "Shi"
        ]
      ],
      "title": "Deep Attention Gated Dilated Temporal Convolutional Networks with Intra-Parallel Convolutional Modules for End-to-End Monaural Speech Separation",
      "original": "1373",
      "page_count": 5,
      "order": 663,
      "p1": "3183",
      "pn": "3187",
      "abstract": [
        "Monaural speech separation techniques are far from satisfactory and\nare a challenging task due to interference from multiple sources. Recently\nthe deep dilated temporal convolutional networks (TCN) has proven to\nbe very effective in sequence modeling. This work explores how to extend\nTCN to result a new, state-of-the-art monaural speech separation method.\nFirst, a new gating mechanism is introduced and added to generate a\ngated TCN. The gated activation controls the flow of information. Further\nin order to combine multiple training models to reduce the performance\nvariance and improve the effect of speech separation, we propose to\nuse the principle of ensemble learning in the gated TCN architecture\nby replacing the convolutional modules corresponding to each dilated\nfactor with multiple identical branches of the convolutional components.\nFor the sake of objectivity, we propose to train the network by directly\noptimizing in a permutation invariant training (PIT) style of the utterance\nlevel signal-to-distortion ratio (SDR). Our experiments with the public\nWSJ0-2mix data corpus resulted in an 18.2 dB improvement in SDR, indicating\nthat our proposed network can improve the performance of speaker separation\ntasks.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1373",
      "author_area_id": 6,
      "author_area_label": "Speech Coding and Enhancement"
    },
    "wang19k_interspeech": {
      "authors": [
        [
          "Xianyun",
          "Wang"
        ],
        [
          "Changchun",
          "Bao"
        ]
      ],
      "title": "Masking Estimation with Phase Restoration of Clean Speech for Monaural Speech Enhancement",
      "original": "1141",
      "page_count": 5,
      "order": 664,
      "p1": "3188",
      "pn": "3192",
      "abstract": [
        "Deep neural network (DNN) has become a popular means for separating\ntarget speech from noisy speech due to its good performance for learning\na mapping relationship between the training target and noisy speech.\nFor the DNN-based methods, the time-frequency (T-F) mask commonly used\nas the training target has a significant impact on the performance\nof speech restoration. However, the T-F mask generally modifies magnitude\nspectrum of noisy speech and leaves phase spectrum unchanged in enhancing\nprocess. The recent studies have revealed that incorporating phase\nspectrum information into the T-F mask can effectively improve perceptual\nquality of the enhanced speech. So, in this paper, we present two T-F\nmasks to simultaneously enhance magnitude and phase of speech spectrum\nbased on non-correlation assumption of real part and imaginary part\nabout speech spectrum, and use them as the training target of the DNN\nmodel. Experimental results show that, in comparison with the reference\nmethods, the proposed method can obtain an effective improvement in\nspeech quality for different signal to noise ratio (SNR) conditions.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1141",
      "author_area_id": 6,
      "author_area_label": "Speech Coding and Enhancement"
    },
    "llombart19b_interspeech": {
      "authors": [
        [
          "Jorge",
          "Llombart"
        ],
        [
          "Dayana",
          "Ribas"
        ],
        [
          "Antonio",
          "Miguel"
        ],
        [
          "Luis",
          "Vicente"
        ],
        [
          "Alfonso",
          "Ortega"
        ],
        [
          "Eduardo",
          "Lleida"
        ]
      ],
      "title": "Progressive Speech Enhancement with Residual Connections",
      "original": "1748",
      "page_count": 5,
      "order": 665,
      "p1": "3193",
      "pn": "3197",
      "abstract": [
        "This paper studies the Speech Enhancement based on Deep Neural Networks.\nThe proposed architecture gradually follows the signal transformation\nduring enhancement by means of a visualization probe at each network\nblock. Alongside the process, the enhancement performance is visually\ninspected and evaluated in terms of regression cost. This progressive\nscheme is based on Residual Networks. During the process, we investigate\na residual connection with a constant number of channels, including\ninternal state between blocks, and adding progressive supervision.\nThe insights provided by the interpretation of the network enhancement\nprocess leads us to design an improved architecture for the enhancement\npurpose. Following this strategy, we are able to obtain speech enhancement\nresults beyond the state-of-the-art, achieving a favorable trade-off\nbetween dereverberation and the amount of spectral distortion.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1748",
      "author_area_id": 6,
      "author_area_label": "Speech Coding and Enhancement"
    },
    "chen19k_interspeech": {
      "authors": [
        [
          "Langzhou",
          "Chen"
        ],
        [
          "Volker",
          "Leutnant"
        ]
      ],
      "title": "Acoustic Model Bootstrapping Using Semi-Supervised Learning",
      "original": "2818",
      "page_count": 5,
      "order": 666,
      "p1": "3198",
      "pn": "3202",
      "abstract": [
        "This work aims at bootstrapping acoustic model training for automatic\nspeech recognition with small amounts of human-labeled speech data\nand large amounts of machine-labeled speech data. Semi-supervised learning\nis investigated to select the machine-transcribed training samples.\nTwo semi-supervised learning methods are proposed: one is the local-global\nuncertainty based method which introduces both the local uncertainty\nfrom the current utterance and the global uncertainty from the whole\ndata pool into the data selection; the other is the margin based data\nselection, which selects the utterances near to the decision boundary\nthrough language model tuning. The experimental results based on a\nJapanese far-field automatic speech recognition system indicate that\nthe acoustic model trained by automatically transcribed speech data\nachieve about 17% relative gain when in-domain human annotated data\nwas not available for initialization. While 3.7% relative gain was\nobtained when the initial acoustic model was trained by small amount\nof in-domain data.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2818",
      "author_area_id": 8,
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "mantena19_interspeech": {
      "authors": [
        [
          "Gautam",
          "Mantena"
        ],
        [
          "Ozlem",
          "Kalinli"
        ],
        [
          "Ossama",
          "Abdel-Hamid"
        ],
        [
          "Don",
          "McAllaster"
        ]
      ],
      "title": "Bandwidth Embeddings for Mixed-Bandwidth Speech Recognition",
      "original": "2589",
      "page_count": 5,
      "order": 667,
      "p1": "3203",
      "pn": "3207",
      "abstract": [
        "In this paper, we tackle the problem of handling narrowband and wideband\nspeech by building a single acoustic model (AM), also called mixed\nbandwidth AM. In the proposed approach, an auxiliary input feature\nis used to provide the bandwidth information to the model, and bandwidth\nembeddings are jointly learned as part of acoustic model training.\nExperimental evaluations show that using bandwidth embeddings helps\nthe model to handle the variability of the narrow and wideband speech,\nand makes it possible to train a mixed-bandwidth AM. Furthermore, we\npropose to use parallel convolutional layers to handle the mismatch\nbetween the narrow and wideband speech better, where separate convolution\nlayers are used for each type of input speech signal. Our best system\nachieves 13% relative improvement on narrowband speech, while not degrading\non wideband speech.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2589",
      "author_area_id": 8,
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "khare19_interspeech": {
      "authors": [
        [
          "Shreya",
          "Khare"
        ],
        [
          "Rahul",
          "Aralikatte"
        ],
        [
          "Senthil",
          "Mani"
        ]
      ],
      "title": "Adversarial Black-Box Attacks on Automatic Speech Recognition Systems Using Multi-Objective Evolutionary Optimization",
      "original": "2420",
      "page_count": 5,
      "order": 668,
      "p1": "3208",
      "pn": "3212",
      "abstract": [
        "Fooling deep neural networks with adversarial input have exposed a\nsignificant vulnerability in the current state-of-the-art systems in\nmultiple domains. Both black-box and white-box approaches have been\nused to either replicate the model itself or to craft examples which\ncause the model to fail. In this work, we propose a framework which\nuses multi-objective evolutionary optimization to perform both targeted\nand un-targeted black-box attacks on Automatic Speech Recognition (ASR)\nsystems. We apply this framework on two ASR systems: Deepspeech and\nKaldi-ASR, which increases the Word Error Rates (WER) of these systems\nby upto 980%, indicating the potency of our approach. During both un-targeted\nand targeted attacks, the adversarial samples maintain a high acoustic\nsimilarity of 0.98 and 0.97 with the original audio.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2420",
      "author_area_id": 8,
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "soomro19_interspeech": {
      "authors": [
        [
          "Bilal",
          "Soomro"
        ],
        [
          "Anssi",
          "Kanervisto"
        ],
        [
          "Trung Ngo",
          "Trong"
        ],
        [
          "Ville",
          "Hautam\u00e4ki"
        ]
      ],
      "title": "Towards Debugging Deep Neural Networks by Generating Speech Utterances",
      "original": "2339",
      "page_count": 5,
      "order": 669,
      "p1": "3213",
      "pn": "3217",
      "abstract": [
        "Deep neural networks (DNN) are able to successfully process and classify\nspeech utterances. However, understanding the reason behind a classification\nby DNN is difficult. One such debugging method used with image classification\nDNNs is  activation maximization, which generates example-images that\nare classified as one of the classes. In this work, we evaluate applicability\nof this method to speech utterance classifiers as the means to understanding\nwhat DNN &#8220;listens to&#8221;. We trained a classifier using the\nspeech command corpus and then use activation maximization to pull\nsamples from the trained model. Then we synthesize audio from features\nusing WaveNet vocoder for subjective analysis. We measure the quality\nof generated samples by objective measurements and crowd-sourced human\nevaluations. Results show that when combined with the prior of natural\nspeech, activation maximization can be used to generate examples of\ndifferent classes. Based on these results, activation maximization\ncan be used to start opening up the DNN black-box in speech tasks.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2339",
      "author_area_id": 8,
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "ding19b_interspeech": {
      "authors": [
        [
          "Haisong",
          "Ding"
        ],
        [
          "Kai",
          "Chen"
        ],
        [
          "Qiang",
          "Huo"
        ]
      ],
      "title": "Compression of CTC-Trained Acoustic Models by Dynamic Frame-Wise Distillation or Segment-Wise N-Best Hypotheses Imitation",
      "original": "2182",
      "page_count": 5,
      "order": 670,
      "p1": "3218",
      "pn": "3222",
      "abstract": [
        "Knowledge distillation (KD) has been widely used for model compression\nby learning a simpler student model to imitate the outputs or intermediate\nrepresentations of a more complex teacher model. The most commonly\nused KD technique is to minimize a Kullback-Leibler divergence between\nthe output distributions of the teacher and student models. When it\nis applied to compressing CTC-trained acoustic models, an assumption\nis made that the teacher and student share the same frame-wise feature-transcription\nalignment, which is usually not true due to the topology difference\nof the teacher and student models. In this paper, by making more appropriate\nassumptions, we propose two KD methods, namely dynamic frame-wise distillation\nand segment-wise N-best hypotheses imitation. Experimental results\non Switchboard-I speech recognition task show that the segment-wise\nN-best hypotheses imitation outperforms the frame-level and other sequence-level\ndistillation methods, and achieves a relative word error rate reduction\nof 5%&#8211;8% compared with models trained from scratch.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2182",
      "author_area_id": 8,
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "lopezespejo19_interspeech": {
      "authors": [
        [
          "Iv\u00e1n",
          "L\u00f3pez-Espejo"
        ],
        [
          "Zheng-Hua",
          "Tan"
        ],
        [
          "Jesper",
          "Jensen"
        ]
      ],
      "title": "Keyword Spotting for Hearing Assistive Devices Robust to External Speakers",
      "original": "2010",
      "page_count": 5,
      "order": 671,
      "p1": "3223",
      "pn": "3227",
      "abstract": [
        "Keyword spotting (KWS) is experiencing an upswing due to the pervasiveness\nof small electronic devices that allow interaction with them via speech.\nOften, KWS systems are speaker-independent, which means that any person\n&#8212; user or not &#8212; might trigger them. For applications like\nKWS for hearing assistive devices this is unacceptable, as only the\nuser must be allowed to handle them. In this paper we propose KWS for\nhearing assistive devices that is robust to external speakers. A state-of-the-art\ndeep residual network for small-footprint KWS is regarded as a basis\nto build upon. By following a multi-task learning scheme, this system\nis extended to jointly perform KWS and users&#8217; own-voice/external\nspeaker detection with a negligible increase in the number of parameters.\nFor experiments, we generate from the Google Speech Commands Dataset\na speech corpus emulating hearing aids as a capturing device. Our results\nshow that this multi-task deep residual network is able to achieve\na KWS accuracy relative improvement of around 32% with respect to a\nsystem that does not deal with external speakers.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2010",
      "author_area_id": 8,
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "doulaty19_interspeech": {
      "authors": [
        [
          "Mortaza",
          "Doulaty"
        ],
        [
          "Thomas",
          "Hain"
        ]
      ],
      "title": "Latent Dirichlet Allocation Based Acoustic Data Selection for Automatic Speech Recognition",
      "original": "1797",
      "page_count": 5,
      "order": 672,
      "p1": "3228",
      "pn": "3232",
      "abstract": [
        "Selecting in-domain data from a large pool of diverse and out-of-domain\ndata is a non-trivial problem. In most cases simply using all of the\navailable data will lead to sub-optimal and in some cases even worse\nperformance compared to carefully selecting a matching set. This is\ntrue even for data-inefficient neural models. Acoustic Latent Dirichlet\nAllocation (aLDA) is shown to be useful in a variety of speech technology\nrelated tasks, including domain adaptation of acoustic models for automatic\nspeech recognition and entity labeling for information retrieval. In\nthis paper we propose to use aLDA as a data similarity criterion in\na data selection framework. Given a large pool of out-of-domain and\npotentially mismatched data, the task is to select the best-matching\ntraining data to a set of representative utterances sampled from a\ntarget domain. Our target data consists of around 32 hours of meeting\ndata (both far-field and close-talk) and the pool contains 2k hours\nof meeting, talks, voice search, dictation, command-and-control, audio\nbooks, lectures, generic media and telephony speech data. The proposed\ntechnique for training data selection, significantly outperforms random\nselection, posterior-based selection as well as using all of the available\ndata.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1797",
      "author_area_id": 8,
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "li19o_interspeech": {
      "authors": [
        [
          "Wenjie",
          "Li"
        ],
        [
          "Pengyuan",
          "Zhang"
        ],
        [
          "Yonghong",
          "Yan"
        ]
      ],
      "title": "Target Speaker Recovery and Recognition Network with Average x-Vector and Global Training",
      "original": "1692",
      "page_count": 5,
      "order": 673,
      "p1": "3233",
      "pn": "3237",
      "abstract": [
        "It is very challenging to do multi-talker automatic speech recognition\n(ASR). Some speaker-aware selective methods have been proposed to recover\nthe speech of the target speaker, relying on the auxiliary speaker\ninformation provided by an anchor (a clean audio sample of the target\nspeaker). But the performance is unstable depending on the quality\nof the provided anchors. To address this limitation, we propose to\ntake advantage of the average speaker embeddings to build the target\nspeaker recovery network (TRnet). The TRnet takes the mixed speech\nand the stable average speaker embeddings to produce the TF masks for\nthe target speech. During training of the TRnet, we summarize the speaker\nembeddings on the whole training dataset for each speaker, instead\nof extracting on a randomly picked anchor. On the testing stage, one\nor very few anchors are enough to get decent recovery results. The\nresults of the TRnet trained with average speaker embeddings show 13%\nand 12.5% relative improvements on WER and SDR, compared with the short-anchor\ntrained model. Moreover, to mitigate the mismatch between the TRnet\nand the acoustic model (AM), we adopted two strategies: fine-tuning\nthe AM and training an global TRnet. Both of them bring considerable\nreductions on WER. The results show that the global trained framework\ngets superior performance.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1692",
      "author_area_id": 8,
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "suzuki19b_interspeech": {
      "authors": [
        [
          "Motoyuki",
          "Suzuki"
        ],
        [
          "Sho",
          "Tomita"
        ],
        [
          "Tomoki",
          "Morita"
        ]
      ],
      "title": "Lyrics Recognition from Singing Voice Focused on Correspondence Between Voice and Notes",
      "original": "1318",
      "page_count": 4,
      "order": 674,
      "p1": "3238",
      "pn": "3241",
      "abstract": [
        "Lyrics recognition from singing voice is one of the most important\ntechniques for query-by-singing music information retrieval systems.\nLyrics information realizes a higher retrieval performance than retrieval\nusing only melody information.<br style=\"mso-data-placement:same-cell;\"\n/><br style=\"mso-data-placement:same-cell;\" />\n  However, recognizing\na song lyrics from singing voice is very difficult. In order to improve\nrecognition, a new method focused on correspondence between voice and\nnotes has been proposed. Note boundary scores are calculated for each\nframe, and these values are included in feature vectors by expanding\ntheir dimensions. The marker HMM is defined to correspond to feature\nvectors located at note boundaries, and the marker HMM is inserted\namong all morae in a pronunciation dictionary. As a result, the recognizer\nrestricts an individual mora to correspond to only one note.<br style=\"mso-data-placement:same-cell;\"\n/><br style=\"mso-data-placement:same-cell;\" />\n  We also modified the\nmarker HMM in order to account for short pauses in a particular position.\nA short pause corresponding to a musical rest or breath may occur after\nany morae, even if inside a word. The short pause HMM is concatenated\nto the marker HMM, and a skip transition arc of the short pause HMM\nis also introduced.<br style=\"mso-data-placement:same-cell;\" /><br\nstyle=\"mso-data-placement:same-cell;\" />\n  From experimental results,\nthe proposed model provided higher word accuracy than the baseline\nmodel. It improved word accuracy from 85.71% to 93.18%, which means\nthat 52.3% of the word error rate decreased. Insertion errors, especially,\nwere drastically suppressed.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1318",
      "author_area_id": 8,
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "hsu19b_interspeech": {
      "authors": [
        [
          "Wei-Ning",
          "Hsu"
        ],
        [
          "David",
          "Harwath"
        ],
        [
          "James",
          "Glass"
        ]
      ],
      "title": "Transfer Learning from Audio-Visual Grounding to Speech Recognition",
      "original": "1227",
      "page_count": 5,
      "order": 675,
      "p1": "3242",
      "pn": "3246",
      "abstract": [
        "Transfer learning aims to reduce the amount of data required to excel\nat a new task by re-using the knowledge acquired from learning other\nrelated tasks. This paper proposes a novel transfer learning scenario,\nwhich distills robust phonetic features from grounding models that\nare trained to tell whether a pair of image and speech are semantically\ncorrelated, without using any textual transcripts. As semantics of\nspeech are largely determined by its lexical content, grounding models\nlearn to preserve phonetic information while disregarding uncorrelated\nfactors, such as speaker and channel. To study the properties of features\ndistilled from different layers, we use them as input separately to\ntrain multiple speech recognition models. Empirical results demonstrate\nthat layers closer to input retain more phonetic information, while\nfollowing layers exhibit greater invariance to domain shift. Moreover,\nwhile most previous studies include training data for speech recognition\nfor feature extractor training, our grounding models are not trained\non any of those data, indicating more universal applicability to new\ndomains.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1227",
      "author_area_id": 8,
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "luo19c_interspeech": {
      "authors": [
        [
          "Hui",
          "Luo"
        ],
        [
          "Jiqing",
          "Han"
        ]
      ],
      "title": "Cross-Corpus Speech Emotion Recognition Using Semi-Supervised Transfer Non-Negative Matrix Factorization with Adaptation Regularization",
      "original": "2041",
      "page_count": 5,
      "order": 676,
      "p1": "3247",
      "pn": "3251",
      "abstract": [
        "This paper focuses on a cross-corpus speech emotion recognition (SER)\ntask, in which there are some mismatches between the training corpus\nand the testing corpus. Meanwhile, the label information of the training\ncorpus is known, while the label information of the testing corpus\nis entirely unknown. To alleviate the influence of these mismatches\non the recognition system under this setting, we present a non-negative\nmatrix factorization (NMF) based cross-corpus speech emotion recognition\nmethod, called semi-supervised adaptation regularized transfer NMF\n(SATNMF). The core idea of SATNMF is to incorporate the label information\nof training corpus into NMF, and seek a latent low-rank feature space,\nin which the marginal and conditional distribution differences between\nthe two corpora can be minimized simultaneously. Specifically, in this\ninduced feature space, the maximum mean discrepancy (MMD) criterion\nis used to measure the discrepancies of not only two corpora, but also\neach class within the two corpora. Moreover, to further exploit the\nknowledge of the marginal distributions, their underlying manifold\nstructure is considered by using the manifold regularization. Experiments\non four popular emotional corpora show that the proposed method achieves\nbetter recognition accuracies than state-of-the-art methods.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2041",
      "author_area_id": 3,
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "tammewar19_interspeech": {
      "authors": [
        [
          "Aniruddha",
          "Tammewar"
        ],
        [
          "Alessandra",
          "Cervone"
        ],
        [
          "Eva-Maria",
          "Messner"
        ],
        [
          "Giuseppe",
          "Riccardi"
        ]
      ],
      "title": "Modeling User Context for Valence Prediction from Narratives",
      "original": "2489",
      "page_count": 5,
      "order": 677,
      "p1": "3252",
      "pn": "3256",
      "abstract": [
        "Automated prediction of valence, one key feature of a person&#8217;s\nemotional state, from individuals&#8217; personal narratives may provide\ncrucial information for mental healthcare (e.g. early diagnosis of\nmental diseases, supervision of disease course, etc.). In the Interspeech\n2018 ComParE Self-Assessed Affect challenge, the task of valence prediction\nwas framed as a three-class classification problem using 8 seconds\nfragments from individuals&#8217; narratives. As such, the task did\nnot allow for exploring contextual information of the narratives. In\nthis work, we investigate the intrinsic information from multiple narratives\nrecounted by the same individual in order to predict their current\nstate-of-mind. Furthermore, with generalizability in mind, we decided\nto focus our experiments exclusively on textual information as the\npublic availability of audio narratives is limited compared to text.\nOur hypothesis is that context modeling might provide insights about\nemotion triggering concepts (e.g. events, people, places) mentioned\nin the narratives that are linked to an individual&#8217;s state of\nmind. We explore multiple machine learning techniques to model narratives.\nWe find that the models are able to capture inter-individual differences,\nleading to more accurate predictions of an individual&#8217;s emotional\nstate, as compared to single narratives.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2489",
      "author_area_id": 3,
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "chakraborty19_interspeech": {
      "authors": [
        [
          "Rupayan",
          "Chakraborty"
        ],
        [
          "Ashish",
          "Panda"
        ],
        [
          "Meghna",
          "Pandharipande"
        ],
        [
          "Sonal",
          "Joshi"
        ],
        [
          "Sunil Kumar",
          "Kopparapu"
        ]
      ],
      "title": "Front-End Feature Compensation and Denoising for Noise Robust Speech Emotion Recognition",
      "original": "2243",
      "page_count": 5,
      "order": 678,
      "p1": "3257",
      "pn": "3261",
      "abstract": [
        "Front-end processing is one of the ways to impart noise robustness\nto speech emotion recognition systems in mismatched scenarios. Here,\nwe implement and compare different frontend robustness techniques for\ntheir efficacy in speech emotion recognition. First, we use a feature\ncompensation technique based on the Vector Taylor Series (VTS) expansion\nof noisy Mel-Frequency Cepstral Coefficients (MFCCs). Next, we improve\nupon the feature compensation technique by using the VTS expansion\nwith auditory masking formulation. We have also looked into the applicability\nof 10<SUP>th</SUP>-root compression in MFCC computation. Further, a\nTime Delay Neural Network based Denoising Autoencoder (TDNN-DAE) is\nimplemented to estimate the clean MFCCs from the noisy MFCCs. These\ntechniques have not been investigated yet for their suitability to\nrobust speech emotion recognition task. The performance of these front-end\ntechniques are compared with the Non-Negative Matrix Factorization\n(NMF) based front-end. Relying on extensive experiments done on two\nstandard databases (EmoDB and IEMOCAP), contaminated with 5 types of\nnoise, we show that these techniques provide significant performance\ngain in emotion recognition task. We also show that along with front-end\ncompensation, applying feature selection to non-MFCC high-level descriptors\nresults in better performance.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2243",
      "author_area_id": 3,
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "li19p_interspeech": {
      "authors": [
        [
          "Xingfeng",
          "Li"
        ],
        [
          "Masato",
          "Akagi"
        ]
      ],
      "title": "The Contribution of Acoustic Features Analysis to Model Emotion Perceptual Process for Language Diversity",
      "original": "2229",
      "page_count": 5,
      "order": 679,
      "p1": "3262",
      "pn": "3266",
      "abstract": [
        "The multi-layered perceptual process of emotion in human speech plays\nan essential role in the field of affective computing for underlying\na speaker&#8217;s state. However, a comprehensive process analysis\nof emotion perception is still challenging due to the lack of powerful\nacoustic features allowing accurate inference of emotion across speaker\nand language diversities. Most previous research works study acoustic\nfeatures mostly using Fourier transform, short time Fourier transform\nor linear predictive coding. Even though these features may be useful\nfor stationary signal within short frames, they may not capture the\nlocalized event adequately as speech transmits emotion information\ndynamically over time. This case introduces a set of acoustic features\nvia wavelet transform analysis of the speech signal, and specifically,\nmodels the perceptual process of emotion for language diversity. For\nthis aim, the proposed features are analyzed in a three-layer emotion\nperception model across multiple languages. Experiments show that the\nproposed acoustic features significantly enhance the perceptual process\nof emotion and render a better result in multilingual emotion recognition\nwhen compared it to the widely used prosodic and spectral features,\nas well as their combination in literature.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2229",
      "author_area_id": 3,
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "rajan19_interspeech": {
      "authors": [
        [
          "Rajeev",
          "Rajan"
        ],
        [
          "Haritha",
          "U.G."
        ],
        [
          "Sujitha",
          "A.C."
        ],
        [
          "Rejisha T.",
          "M."
        ]
      ],
      "title": "Design and Development of a Multi-Lingual Speech Corpora (TaMaR-EmoDB) for Emotion Analysis",
      "original": "2034",
      "page_count": 5,
      "order": 680,
      "p1": "3267",
      "pn": "3271",
      "abstract": [
        "This paper presents the design, the development of a new multilingual\nemotional speech corpus, TaMaR- EmoDB (Tamil Malayalam Ravula - Emotion\nDataBase) and its evaluation using a deep neural network (DNN)-baseline\nsystem. The corpus consists of utterances from three languages, namely,\nMalayalam, Tamil and Ravula, a tribal language. The database consists\nof short speech utterances in four emotions - anger, anxiety, happiness,\nand sadness, along with neutral utterances. The subset of the corpus\nis first evaluated using a perception test, in order to understand\nhow well the emotional state in emotional speech is identified by humans.\nLater, machine testing is performed using the fusion of spectral and\nprosodic features with DNN framework. During the classification phase,\nthe system reports an average precision of 0.78, 0.60, 0.61 and recall\nof 0.84, 0.61 and 0.53 for Malayalam, Tamil, and Ravula, respectively.\nThis database can potentially be used as a new linguistic resource\nthat will enable future research in speech emotion detection, corpus-based\nprosody analysis, and speech synthesis.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2034",
      "author_area_id": 3,
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "sridhar19_interspeech": {
      "authors": [
        [
          "Kusha",
          "Sridhar"
        ],
        [
          "Carlos",
          "Busso"
        ]
      ],
      "title": "Speech Emotion Recognition with a Reject Option",
      "original": "1842",
      "page_count": 5,
      "order": 681,
      "p1": "3272",
      "pn": "3276",
      "abstract": [
        " Speech emotion recognition (SER) for categorical descriptors is a\ndifficult task when the recordings come from everyday spontaneous interactions.\nThe boundaries between emotional classes are less clear, resulting\nin complex, mixed emotions. Since the performance of a SER system varies\nacross speech recordings, it is important to understand the reliability\nassociated with its prediction. An intriguing formulation in machine\nlearning related to this problem is the reject option, where a classifier\nonly provides predictions over samples with reliability above a given\nthreshold. This paper proposes a classification technique with a reject\noption using  deep neural networks (DNNs) that increases its performance\nby selectively trading its coverage in the testing set. We use two\ndifferent criteria to develop a SER system with a reject option, where\nit can accept or reject a sample as needed. Using the MSP-Podcast corpus,\nwe evaluate this idea by comparing different classification performance\nas a function of coverage. By selectively defining a coverage of 75%\nof the samples, we obtain relative gains in F1-score of up to 25.71%\nfor a five-class problem and 20.63% for an eight-class problem. The\nsentences that are rejected are analyzed in the evaluation, confirming\nthat they have lower inter-evaluator agreement.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1842",
      "author_area_id": 3,
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "jin19_interspeech": {
      "authors": [
        [
          "Zhenghao",
          "Jin"
        ],
        [
          "Houwei",
          "Cao"
        ]
      ],
      "title": "Development of Emotion Rankers Based on Intended and Perceived Emotion Labels",
      "original": "1831",
      "page_count": 5,
      "order": 682,
      "p1": "3277",
      "pn": "3281",
      "abstract": [
        "In emotion datasets, intended emotion labels and perceived emotion\nlabels both contain valuable information about how human express and\nperceive emotions, and there is a considerable mismatch between the\ntwo. In this paper, we propose a novel method to derive relative labels\nfor preference learning using both the intended labels during emotion\nexpression and the perceived labels given by all raters during perceptual\nevaluation. Based on analyzing the agreement between the intended and\nperceived labels, as well as the consistence among all perceptual ratings,\nwe propose three pairwise ranking rules to generate multi-scale relevant\nscores for preference learning. We further build three sets of rankers\nfor six basic emotions based on the three ranking rules. Through evaluation\non the CREMA-D database, we demonstrate that, by considering both intended\nand perceived labels, our proposed rankers significantly outperform\nthe rankers only relying on the perceptual ratings. We further combine\nthe ranking scores of individual emotions for multi-class classification.\nThrough experiments, we show that the emotion classification systems\nwith ranking information significantly outperform the conventional\nSVM classifiers.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1831",
      "author_area_id": 3,
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "gideon19_interspeech": {
      "authors": [
        [
          "John",
          "Gideon"
        ],
        [
          "Heather T.",
          "Schatten"
        ],
        [
          "Melvin G.",
          "McInnis"
        ],
        [
          "Emily Mower",
          "Provost"
        ]
      ],
      "title": "Emotion Recognition from Natural Phone Conversations in Individuals with and without Recent Suicidal Ideation",
      "original": "1830",
      "page_count": 5,
      "order": 683,
      "p1": "3282",
      "pn": "3286",
      "abstract": [
        "Suicide is a serious public health concern in the U.S., taking the\nlives of over 47,000 people in 2017. Early detection of suicidal ideation\nis key to prevention. One promising approach to symptom monitoring\nis suicidal speech prediction, as speech can be passively collected\nand may indicate changes in risk. However, directly identifying suicidal\nspeech is difficult, as characteristics of speech can vary rapidly\ncompared with suicidal thoughts. Suicidal ideation is also associated\nwith emotion dysregulation. Therefore, in this work, we focus on the\ndetection of emotion from speech and its relation to suicide. We introduce\nthe Ecological Measurement of Affect, Speech, and Suicide (EMASS) dataset,\nwhich contains phone call recordings of individuals recently discharged\nfrom the hospital following admission for suicidal ideation or behavior,\nalong with controls. Participants self-report their emotion periodically\nthroughout the study. However, the dataset is relatively small and\nhas uncertain labels. Because of this, we find that most features traditionally\nused for emotion classification fail. We demonstrate how outside emotion\ndatasets can be used to generate more relevant features, making this\nanalysis possible. Finally, we use emotion predictions to differentiate\nhealthy controls from those with suicidal ideation, providing evidence\nfor suicidal speech detection using emotion.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1830",
      "author_area_id": 3,
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "nazareth19_interspeech": {
      "authors": [
        [
          "Deniece S.",
          "Nazareth"
        ],
        [
          "Ellen",
          "Tournier"
        ],
        [
          "Sarah",
          "Leimk\u00f6tter"
        ],
        [
          "Esther",
          "Janse"
        ],
        [
          "Dirk",
          "Heylen"
        ],
        [
          "Gerben J.",
          "Westerhof"
        ],
        [
          "Khiet P.",
          "Truong"
        ]
      ],
      "title": "An Acoustic and Lexical Analysis of Emotional Valence in Spontaneous Speech: Autobiographical Memory Recall in Older Adults",
      "original": "1823",
      "page_count": 5,
      "order": 684,
      "p1": "3287",
      "pn": "3291",
      "abstract": [
        "Analyzing emotional valence in spontaneous speech remains complex and\nchallenging. We present an acoustic and lexical analysis of emotional\nvalence in spontaneous speech of older adults. Data was collected by\nrecalling autobiographical memories through a word association task.\nDue to the complex and personal nature of memories, we propose a novel\ncoding scheme for emotional valence. We explore acoustic properties\nof speech as well as the use of affective words to predict emotional\nvalence expressed in autobiographical memories. Using mixed-effect\nregression modelling, we compared predictive models based on acoustic\ninformation only, lexical information only, or a combination of both.\nResults show that the combined model accounts for the highest proportion\nof explained variance, with the acoustic features accounting for a\nsmaller share of the total variance than the lexical features. Several\nacoustic and lexical features predicted valence. As a first attempt\nat analyzing spontaneous emotional speech in older adults autobiographical\nmemories, the study provides more insight in which acoustic features\ncan be used to predict valence (automatically) in a more ecologically\nvalid setting.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1823",
      "author_area_id": 3,
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "zhao19g_interspeech": {
      "authors": [
        [
          "Yi",
          "Zhao"
        ],
        [
          "Atsushi",
          "Ando"
        ],
        [
          "Shinji",
          "Takaki"
        ],
        [
          "Junichi",
          "Yamagishi"
        ],
        [
          "Satoshi",
          "Kobashikawa"
        ]
      ],
      "title": "Does the Lombard Effect Improve Emotional Communication in Noise? &#8212; Analysis of Emotional Speech Acted in Noise",
      "original": "1605",
      "page_count": 5,
      "order": 685,
      "p1": "3292",
      "pn": "3296",
      "abstract": [
        "Speakers usually adjust their way of talking in noisy environments\ninvoluntarily for effective communication. This adaptation is known\nas the Lombard effect. Although speech accompanying the Lombard effect\ncan improve the intelligibility of a speaker&#8217;s voice, the changes\nin acoustic features (e.g. fundamental frequency, speech intensity,\nand spectral tilt) caused by the Lombard effect may also affect the\nlistener&#8217;s judgment of emotional content. To the best of our\nknowledge, there is no published study on the influence of the Lombard\neffect in emotional speech. Therefore, we recorded parallel emotional\nspeech waveforms uttered by 12 speakers under both quiet and noisy\nconditions in a professional recording studio in order to explore how\nthe Lombard effect interacts with emotional speech. By analyzing confusion\nmatrices and acoustic features, we aim to answer the following questions:\n1) Can speakers express their emotions correctly even under adverse\nconditions? 2) Can listeners recognize the emotion contained in speech\nsignals even under noise? 3) How does emotional speech uttered in noise\ndiffer from emotional speech uttered in quiet conditions in terms of\nacoustic characteristic?\n"
      ],
      "doi": "10.21437/Interspeech.2019-1605",
      "author_area_id": 3,
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "gharsellaoui19_interspeech": {
      "authors": [
        [
          "Soumaya",
          "Gharsellaoui"
        ],
        [
          "Sid Ahmed",
          "Selouani"
        ],
        [
          "Mohammed Sidi",
          "Yakoub"
        ]
      ],
      "title": "Linear Discriminant Differential Evolution for Feature Selection in Emotional Speech Recognition",
      "original": "1218",
      "page_count": 5,
      "order": 686,
      "p1": "3297",
      "pn": "3301",
      "abstract": [
        "In this paper, an evolutionary algorithm is used to select an optimal\nset of acoustic features for emotional speech recognition. A new algorithm\nthat combines differential evolution (DE) optimization and linear discriminant\nanalysis (LDA) is proposed to design an effective feature selection\nand classification model. An original acoustic feature framework based\non auditory modeling is also presented. The auditory-based features\nare provided as inputs to the DE-LDA based emotional speech recognition\nsystem. To evaluate the effectiveness of the DE-LDA approach, a subset\nof the Emotion Prosody Speech and Transcript corpus covering five emotional\nstates (happiness, anger, panic, sadness, and interest) is used throughout\nthe experiments. The results show that the proposed DE-LDA model performs\nsignificantly better than the baseline systems. It achieves a classification\nrate of 91.6% using only 50 input parameters that are optimally selected\nfrom 128 original acoustic features.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1218",
      "author_area_id": 3,
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "sahu19_interspeech": {
      "authors": [
        [
          "Saurabh",
          "Sahu"
        ],
        [
          "Vikramjit",
          "Mitra"
        ],
        [
          "Nadee",
          "Seneviratne"
        ],
        [
          "Carol",
          "Espy-Wilson"
        ]
      ],
      "title": "Multi-Modal Learning for Speech Emotion Recognition: An Analysis and Comparison of ASR Outputs with Ground Truth Transcription",
      "original": "1149",
      "page_count": 5,
      "order": 687,
      "p1": "3302",
      "pn": "3306",
      "abstract": [
        "In this paper we plan to leverage multi-modal learning and automated\nspeech recognition (ASR) systems toward building a speech-only emotion\nrecognition model. Previous studies have shown that emotion recognition\nmodels using only acoustic features do not perform satisfactorily in\ndetecting valence level. Text analysis has been shown to be helpful\nfor sentiment classification. We compared classification accuracies\nobtained from an audio-only model, a text-only model and a multi-modal\nsystem leveraging both by performing a cross-validation analysis on\nIEMOCAP dataset. Confusion matrices show it&#8217;s the valence level\ndetection that is being improved by incorporating textual information.\nIn the second stage of experiments, we used two ASR application programming\ninterfaces (APIs) to get the transcriptions. We compare the performances\nof multi-modal systems using the ASR transcriptions with each other\nand with that of one using ground truth transcription. We analyze the\nconfusion matrices to determine the effect of using ASR transcriptions\ninstead of ground truth ones on class-wise accuracies. We investigate\nthe generalisability of such a model by performing a cross-corpus study.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1149",
      "author_area_id": 3,
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "spinu19_interspeech": {
      "authors": [
        [
          "Laura",
          "Spinu"
        ],
        [
          "Maida",
          "Percival"
        ],
        [
          "Alexei",
          "Kochetov"
        ]
      ],
      "title": "Articulatory Characteristics of Secondary Palatalization in Romanian Fricatives",
      "original": "3039",
      "page_count": 5,
      "order": 688,
      "p1": "3307",
      "pn": "3311",
      "abstract": [
        "This study explores the articulatory characteristics of plain and palatalized\nfricatives in Romanian. Based on earlier acoustic findings, we hypothesize\nthat there are differences in tongue raising and fronting depending\non the primary place of articulation, with more subtle gestures produced\nin the vicinity of the palatal area. We also predict more individual\nvariation in the realization of secondary palatalization in postalveolars,\nbased on general cross-linguistic patterns.<br style=\"mso-data-placement:same-cell;\"\n/><br style=\"mso-data-placement:same-cell;\" />\n  Ten native speakers\nparticipated in an ultrasound experiment. The stimuli included real\nwords containing labial, dental, and postalveolar fricatives. The fricatives\nat all three places were either plain or palatalized word-finally (the\nonly position available for secondary palatalization in this language).\nTongue contours at the consonant midpoint were compared using Smoothing\nSpline ANOVAs individually with radius distance from the ultrasound\nprobe.<br style=\"mso-data-placement:same-cell;\" /><br style=\"mso-data-placement:same-cell;\"\n/>\n  The findings indicate differences in tongue shape between plain\nand palatalized consonants, with stronger palatalization effects in\nlabials compared to coronals, as well as in dentals compared to postalveolars.\nThe latter also revealed higher individual variation. Our findings\nthus suggest that tongue configurations for secondary palatalization\nin Romanian differ by place of articulation. The contrast is also overall\nless robust in postalveolars, confirming previous reports and explaining\nits rarity cross-linguistically.\n"
      ],
      "doi": "10.21437/Interspeech.2019-3039",
      "author_area_id": 2,
      "author_area_label": "Phonetics, Phonology, and Prosody"
    },
    "ratko19_interspeech": {
      "authors": [
        [
          "Louise",
          "Ratko"
        ],
        [
          "Michael",
          "Proctor"
        ],
        [
          "Felicity",
          "Cox"
        ]
      ],
      "title": "Articulation of Vowel Length Contrasts in Australian English",
      "original": "2995",
      "page_count": 5,
      "order": 689,
      "p1": "3312",
      "pn": "3316",
      "abstract": [
        "The articulatory realisation of phonemic vowel length contrasts is\nstill imperfectly understood. Australian English (AusE) /&#592;&#720;/\nand /&#592;/ differ primarily in duration and therefore provide an\nideal case for examining the articulatory properties of long vs. short\nvowels. Patterns of compression, acceleration ratios and VC coordination\nwere examined using electromagnetic articulography (EMA) in /pV&#720;p/\nand /pVp/ syllables produced by three speakers of AusE at two speech\nrates. Short vowels were less compressible and had higher acceleration\nratios than long vowels. VC rimes had proportionately earlier coda\nonsets than V&#720;C rimes. These findings suggest that long and short\nvowels are characterised by different patterns of both intra- and intergestural\norganisation in AusE.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2995",
      "author_area_id": 2,
      "author_area_label": "Phonetics, Phonology, and Prosody"
    },
    "deme19_interspeech": {
      "authors": [
        [
          "Andrea",
          "Deme"
        ],
        [
          "M\u00e1rton",
          "Bart\u00f3k"
        ],
        [
          "Tekla Etelka",
          "Gr\u00e1czi"
        ],
        [
          "Tam\u00e1s G\u00e1bor",
          "Csap\u00f3"
        ],
        [
          "Alexandra",
          "Mark\u00f3"
        ]
      ],
      "title": "V-to-V Coarticulation Induced Acoustic and Articulatory Variability of Vowels: The Effect of Pitch-Accent",
      "original": "2890",
      "page_count": 5,
      "order": 690,
      "p1": "3317",
      "pn": "3321",
      "abstract": [
        "In the present study we analyzed vowel variation induced by carryover\nV-to-V coarticulation under the effect of pitch-accent as a function\nof vowel quality (using a minimally constrained intervening consonant\nto maximize V-to-V effects). We tested if /i/ is more resistant to\ncoarticulation than /u/, and if both vowels show increased coarticulatory\nresistance in pitch-accented syllables. Our approach was unprecedented\nin the sense that it involved the analysis of parallel acoustic (F<SUB>2</SUB>)\nand articulatory (x-axis dorsum position) data in a great number of\nspeakers (9 speaker), and real words of Hungarian. To analyze the degree\nof coarticulation, we adopted the locus equation approach, and fitted\nlinear models on vowel onset and midpoint data, and calculated the\ndifferences between coarticulated and non-coarticulated vowels in both\ndomains. To measure variability, we calculated standard deviations\nof midpoint F<SUB>2</SUB> values and dorsum positions.<br style=\"mso-data-placement:same-cell;\"\n/><br style=\"mso-data-placement:same-cell;\" />\n  The results showed\nthat accent clearly exerted an effect on the phonetic realization of\nvowels, but the effect we found was dependent on both the vowel quality,\nand the domain (articulation/acoustics) at hand. Observation of the\npatterns we found in parallel acoustic and articulatory data warrants\nfor reconsideration of the term &#8216;coarticulatory resistance&#8217;,\nand how it should be conceptualized.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2890",
      "author_area_id": 2,
      "author_area_label": "Phonetics, Phonology, and Prosody"
    },
    "king19_interspeech": {
      "authors": [
        [
          "Hannah",
          "King"
        ],
        [
          "Emmanuel",
          "Ferragne"
        ]
      ],
      "title": "The Contribution of Lip Protrusion to Anglo-English /r/: Evidence from Hyper- and Non-Hyperarticulated Speech",
      "original": "2851",
      "page_count": 5,
      "order": 691,
      "p1": "3322",
      "pn": "3326",
      "abstract": [
        "Articulatory variation of /r/ has been widely observed in rhotic varieties\nof English, particularly with regards to tongue body shapes, which\nrange from retroflex to bunched. However, little is known about the\nproduction of /r/ in modern non-rhotic varieties, particularly in Anglo-English.\nAlthough it is generally agreed that /r/ may be accompanied by lip\nprotrusion, it is unclear whether there is a relationship between tongue\nshape and the accompanying degree of protrusion. We present acoustic\nand articulatory data (via ultrasound tongue imaging and lip videos)\nfrom Anglo-English /r/ produced in both hyper- and non-hyperarticulated\nspeech. Hyperarticulation was elicited by engaging speakers in error\nresolution with a simulated &#8220;silent speech&#8221; recognition\nprogramme. Our analysis indicates that hyperarticulated /r/ induces\nmore lip protrusion than non-hyperarticulated /r/. However, bunched\n/r/ variants present more protrusion than retroflex variants, regardless\nof hyperarticulation. Despite some methodological limitations, the\nuse of Deep Neural Networks seems to confirm these results. An articulatory\ntrading relation between tongue shape and accompanying lip protrusion\nis proposed.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2851",
      "author_area_id": 2,
      "author_area_label": "Phonetics, Phonology, and Prosody"
    },
    "marko19_interspeech": {
      "authors": [
        [
          "Alexandra",
          "Mark\u00f3"
        ],
        [
          "M\u00e1rton",
          "Bart\u00f3k"
        ],
        [
          "Tam\u00e1s G\u00e1bor",
          "Csap\u00f3"
        ],
        [
          "Tekla Etelka",
          "Gr\u00e1czi"
        ],
        [
          "Andrea",
          "Deme"
        ]
      ],
      "title": "Articulatory Analysis of Transparent Vowel /i&#720;/ in Harmonic and Antiharmonic Hungarian Stems: Is There a Difference?",
      "original": "2352",
      "page_count": 5,
      "order": 692,
      "p1": "3327",
      "pn": "3331",
      "abstract": [
        "The aim of our study is to analyse the articulatory characteristics\nof /i&#720;/ occurring in Hungarian monosyllabic harmonic and antiharmonic\nstems. In their frequently cited work, based on 3 speakers&#8217; data,\nBe&#328;u&#353; and Gafos (2007) [1] claimed that the tongue position\nin transparent vowels of antiharmonic Hungarian stems is less advanced\nthan that of the phonemically identical vowels in harmonic stems. In\ntheir study, the authors compared different harmonic and antiharmonic\nstems (even if the consonantal context was more or less controlled).<br\nstyle=\"mso-data-placement:same-cell;\" /><br style=\"mso-data-placement:same-cell;\"\n/>\n  In the present study, we analysed two homophonous pairs of words\n/si&#720;v/ and /&#x272;i&#720;r/, which are antiharmonic in their\nverbal usage, but are harmonic as nouns. The words were produced by\n4 speakers both (i) in isolation and (ii) in sentence-initial position,\nwhere they were followed by front and back vowels, in a well-controlled\nmanner. The experiment was carried out using electromagnetic articulography.\nWe compared the sequence of the horizontal position of four receiver\ncoils (ttip, tbl, tbo1, tbo2) across the conditions with Generalized\nAdditive Models. The results showed that the horizontal positions of\nthe receivers did not vary as a function of the harmonicity of the\nstem in either the isolated or the coarticulated condition.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2352",
      "author_area_id": 2,
      "author_area_label": "Phonetics, Phonology, and Prosody"
    },
    "cunha19_interspeech": {
      "authors": [
        [
          "Concei\u00e7\u00e3o",
          "Cunha"
        ],
        [
          "Samuel",
          "Silva"
        ],
        [
          "Ant\u00f3nio",
          "Teixeira"
        ],
        [
          "Catarina",
          "Oliveira"
        ],
        [
          "Paula",
          "Martins"
        ],
        [
          "Arun A.",
          "Joseph"
        ],
        [
          "Jens",
          "Frahm"
        ]
      ],
      "title": "On the Role of Oral Configurations in European Portuguese Nasal Vowels",
      "original": "2232",
      "page_count": 5,
      "order": 693,
      "p1": "3332",
      "pn": "3336",
      "abstract": [
        "The characterisation of nasal vowels is not only a question of studying\nvelar aperture. Recent work shows that oropharyngeal articulatory adjustments\nenhance the acoustics of nasal coupling or, at least, magnify differences\nbetween oral/nasal vowel congeners. Despite preliminary studies on\nthe oral configurations of nasal vowels, for European Portuguese, a\nquantitative analysis is missing, particularly one to be applied systematically\nto a desirably large number of speakers. The main objective of this\nstudy is to adapt and extend previous methodological advances for the\nanalysis of MRI data to further investigate: how velar changes affect\noral configurations; the changes to the articulators and constrictions\nwhen compared with oral counterparts; and the closest oral counterpart.\nHigh framerate RT-MRI images (50fps) are automatically processed to\nextract the vocal tract contours and the position/configuration for\nthe different articulators. These data are processed by evolving a\nquantitative articulatory analysis framework, previously proposed by\nthe authors, extended to include information regarding constrictions\n(degree and place) and nasal port. For this study, while the analysis\nof data for more speakers is ongoing, we considered a set of two EP\nnative speakers and addressed the study of oral and nasal vowels mainly\nin the context of stop consonants.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2232",
      "author_area_id": 2,
      "author_area_label": "Phonetics, Phonology, and Prosody"
    },
    "xiong19_interspeech": {
      "authors": [
        [
          "Yan",
          "Xiong"
        ],
        [
          "Visar",
          "Berisha"
        ],
        [
          "Chaitali",
          "Chakrabarti"
        ]
      ],
      "title": "Residual + Capsule Networks (ResCap) for Simultaneous Single-Channel Overlapped Keyword Recognition",
      "original": "2913",
      "page_count": 5,
      "order": 694,
      "p1": "3337",
      "pn": "3341",
      "abstract": [
        "Overlapped speech poses a significant problem in a variety of applications\nin speech processing including speaker identification, speaker diarization,\nand speech recognition among others. To address it, existing systems\ncombine source separation with algorithms for processing non-overlapped\nspeech (e.g. source separation + follow-on speech recognition). In\nthis paper we propose a modified network architecture to simultaneously\nrecognize keywords from overlapped speech without explicitly having\nto perform source separation. We build our network by adding capsule\nlayers to a ResNet architecture that has shown state-of-the-art performance\non a traditional keyword recognition task. We evaluate the model on\na series of 10-word overlapped keyword recognition experiments, using\nspeaker dependent and speaker independent training. Results indicate\nthat Residual + Capsule (ResCap) network shows marked improvement in\nrecognizing overlapped speech, especially in experiments where there\nis a mismatch in the number of overlapped speakers between the training\nset and the test set.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2913",
      "author_area_id": 5,
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "huang19i_interspeech": {
      "authors": [
        [
          "Che-Wei",
          "Huang"
        ],
        [
          "Roland",
          "Maas"
        ],
        [
          "Sri Harish",
          "Mallidi"
        ],
        [
          "Bj\u00f6rn",
          "Hoffmeister"
        ]
      ],
      "title": "A Study for Improving Device-Directed Speech Detection Toward Frictionless Human-Machine Interaction",
      "original": "2840",
      "page_count": 5,
      "order": 695,
      "p1": "3342",
      "pn": "3346",
      "abstract": [
        "In this paper, we extend our previous work on device-directed utterance\ndetection, which aims to distinguish voice queries intended for a smart-home\ndevice from background speech. The task can be phrased as a binary\nutterance-level classification problem that we approach with a DNN-LSTM\nmodel using acoustic features and features from the automatic speech\nrecognition (ASR) decoder as input. In this work, we study the performance\nof the model for different dialog types and for different categories\nof decoder features. To address different dialog types, we found that\na model with a separate output branch for each dialog type outperforms\na model with a shared output branch by a relative 12.5% of equal error\nrate (EER) reduction. We also found the average number of arcs in a\nconfusion network to be one of the most informative ASR decoder features.\nIn addition, we explore different frequencies of backward propagation\nfor training the acoustic embedding for every k frames (k=1,3,5,7),\nand mean and attention pooling methods for generating an utterance\nrepresentation. We found that attention pooling provides the most discriminative\nutterance representation and outperforms mean pooling by a relative\n4.97% of EER reduction.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2840",
      "author_area_id": 5,
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "su19_interspeech": {
      "authors": [
        [
          "Hang",
          "Su"
        ],
        [
          "Borislav",
          "Dzodzo"
        ],
        [
          "Xixin",
          "Wu"
        ],
        [
          "Xunying",
          "Liu"
        ],
        [
          "Helen",
          "Meng"
        ]
      ],
      "title": "Unsupervised Methods for Audio Classification from Lecture Discussion Recordings",
      "original": "2384",
      "page_count": 5,
      "order": 696,
      "p1": "3347",
      "pn": "3351",
      "abstract": [
        "Time allocated for lecturing and student discussions is an important\nindicator of classroom quality assessment. Automated classification\nof lecture and discussion recording segments can serve as an indicator\nof classroom activity in a flipped classroom setting. Segments of lecture\nare primarily the speech of the lecturer, while segments of discussion\ninclude student speech, silence and noise. Multiple audio recorders\nsimultaneously document all class activities. Recordings are coarsely\nsynchronized to a common start time. We note that the lecturer&#8217;s\nspeech tends to be common across recordings, but student discussions\nare captured only in the nearby device(s). Therefore, we window each\nrecording at 0.5 s to 5 s duration and 0.1 s analysis rate. We compute\nthe normalized similarity between a given window and temporally proximate\nwindow segments in other recordings. Histogram plot categorizes higher\nsimilarity windows as lecture and lower ones as discussion. To improve\nthe classification performance, high energy lecture windows and windows\nwith very high and very low similarity are used to train a supervised\nmodel, in order to regenerate the classification results of remaining\nwindows. Experimental results show that binary classification accuracy\nimproves from 96.84% to 97.37%.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2384",
      "author_area_id": 5,
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "ashihara19_interspeech": {
      "authors": [
        [
          "Takanori",
          "Ashihara"
        ],
        [
          "Yusuke",
          "Shinohara"
        ],
        [
          "Hiroshi",
          "Sato"
        ],
        [
          "Takafumi",
          "Moriya"
        ],
        [
          "Kiyoaki",
          "Matsui"
        ],
        [
          "Takaaki",
          "Fukutomi"
        ],
        [
          "Yoshikazu",
          "Yamaguchi"
        ],
        [
          "Yushi",
          "Aono"
        ]
      ],
      "title": "Neural Whispered Speech Detection with Imbalanced Learning",
      "original": "2161",
      "page_count": 5,
      "order": 697,
      "p1": "3352",
      "pn": "3356",
      "abstract": [
        "In this paper, we present a neural whispered-speech detection technique\nthat offers utterance-level classification of whispered and non-whispered\nspeech exhibiting imbalanced data distributions. Previous studies have\nshown that machine learning models trained on a large amount of whispered\nand non-whispered utterances perform remarkably well for whispered\nspeech detection. However, it is often difficult to collect large numbers\nof whispered utterances. In this paper, we propose a method to train\nneural whispered speech detectors from a small amount of whispered\nutterances in combination with a large amount of non-whispered utterances.\nIn doing so, special care is taken to ensure that severely imbalanced\ndatasets can effectively train neural networks. Specifically, we use\na class-aware sampling method for training neural networks. To evaluate\nthe networks, we gather test samples recorded by both condenser and\nsmartphone microphones at different distances from the speakers to\nsimulate practical environments. Experiments show the importance of\nimbalanced learning in enhancing the performance of utterance level\nclassifiers. \n"
      ],
      "doi": "10.21437/Interspeech.2019-2161",
      "author_area_id": 5,
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "bergler19_interspeech": {
      "authors": [
        [
          "Christian",
          "Bergler"
        ],
        [
          "Manuel",
          "Schmitt"
        ],
        [
          "Rachael Xi",
          "Cheng"
        ],
        [
          "Andreas",
          "Maier"
        ],
        [
          "Volker",
          "Barth"
        ],
        [
          "Elmar",
          "N\u00f6th"
        ]
      ],
      "title": "Deep Learning for Orca Call Type Identification &#8212; A Fully Unsupervised Approach",
      "original": "1857",
      "page_count": 5,
      "order": 698,
      "p1": "3357",
      "pn": "3361",
      "abstract": [
        "Call type classification is an important instrument in bioacoustic\nresearch investigating group-specific vocal repertoire, behavioral\npatterns, and cultures of different animal groups. There is a growing\nneed using robust machine-based techniques to replace human classification\ndue to its advantages in handling large datasets, delivering consistent\nresults, removing perceptual-based classification, and minimizing human\nerrors. The current work is the first adopting a two-stage fully unsupervised\napproach on previous machine-segmented orca data to identify orca sound\ntypes using deep learning together with one of the largest bioacoustic\ndatasets &#8212; the Orchive. The proposed methods include: (1) unsupervised\nfeature learning using an undercomplete ResNet18-autoencoder trained\non machine-annotated data, and (2) spectral clustering utilizing compressed\norca feature representations. An existing human-labeled orca dataset\nwas clustered, including 514 signals distributed over 12 classes. This\ntwo-stage fully unsupervised approach is an initial study to (1) examine\nmachine-generated clusters against human-identified orca call type\nclasses, (2) compare supervised call type classification versus unsupervised\ncall type clustering, and (3) verify the general feasibility of a completely\nunsupervised approach based on machine-labeled orca data resulting\nin a major progress within the research field of animal linguistics,\nby deriving a much deeper understanding and facilitating totally new\ninsights and opportunities.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1857",
      "author_area_id": 5,
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "sacchi19_interspeech": {
      "authors": [
        [
          "Niccol\u00f2",
          "Sacchi"
        ],
        [
          "Alexandre",
          "Nanchen"
        ],
        [
          "Martin",
          "Jaggi"
        ],
        [
          "Milos",
          "Cernak"
        ]
      ],
      "title": "Open-Vocabulary Keyword Spotting with Audio and Text Embeddings",
      "original": "1846",
      "page_count": 5,
      "order": 699,
      "p1": "3362",
      "pn": "3366",
      "abstract": [
        "Keyword Spotting (KWS) systems allow detecting a set of spoken (pre-defined)\nkeywords. Open-vocabulary KWS systems search for the keywords in the\nset of word hypotheses generated by an automatic speech recognition\n(ASR) system which is computationally expensive and, therefore, often\nimplemented as a cloud-based service. Besides, KWS systems could use\nalso word classification algorithms that do not allow easily changing\nthe set of words to be recognized, as the classes have to be defined\na priori, even before training the system. In this paper, we propose\nthe implementation of an open-vocabulary ASR-free KWS system based\non speech and text encoders that allow matching the computed embeddings\nin order to spot whether a keyword has been uttered. This approach\nwould allow choosing the set of keywords a posteriori while requiring\nlow computational power. The experiments, performed on two different\ndatasets, show that our method is competitive with other state of the\nart KWS systems while allowing for a flexibility of configuration and\nbeing computationally efficient.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1846",
      "author_area_id": 5,
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "gao19c_interspeech": {
      "authors": [
        [
          "Qiang",
          "Gao"
        ],
        [
          "Shutao",
          "Sun"
        ],
        [
          "Yaping",
          "Yang"
        ]
      ],
      "title": "ToneNet: A CNN Model of Tone Classification of Mandarin Chinese",
      "original": "1483",
      "page_count": 5,
      "order": 700,
      "p1": "3367",
      "pn": "3371",
      "abstract": [
        "In Mandarin Chinese, correct pronunciation is the key to convey word\nmeaning correctly and the correct pronunciation is closely related\nto the tone of text. Therefore, tone classification is a critical part\nof speech evaluation system. Traditional tone classification is based\non F0 and energy or MFCCs. But the extraction of these features is\noften subject to noise and other uncontrollable environmental factors.\nThus, in order to reduce the influence of environment, we designed\na CNN network named ToneNet which adopts mel-spectrogram as a feature\nand uses a customed convolutional neural network and multi-layer perceptron\nto classify Chinese syllables into one of the four tones. We trained\nand tested ToneNet on the Syllable Corpus of Standard Chinese Dataset\n(SCSC). The result shows that the best accuracy and f1-score of our\nmethod have reached 99.16% and 99.11% respectively. Besides, ToneNet\nhas achieved 97.07% of accuracy and 96.83% of f1-score with the condition\nof gaussian noise.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1483",
      "author_area_id": 5,
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "choi19_interspeech": {
      "authors": [
        [
          "Seungwoo",
          "Choi"
        ],
        [
          "Seokjun",
          "Seo"
        ],
        [
          "Beomjun",
          "Shin"
        ],
        [
          "Hyeongmin",
          "Byun"
        ],
        [
          "Martin",
          "Kersner"
        ],
        [
          "Beomsu",
          "Kim"
        ],
        [
          "Dongyoung",
          "Kim"
        ],
        [
          "Sungjoo",
          "Ha"
        ]
      ],
      "title": "Temporal Convolution for Real-Time Keyword Spotting on Mobile Devices",
      "original": "1363",
      "page_count": 5,
      "order": 701,
      "p1": "3372",
      "pn": "3376",
      "abstract": [
        "Keyword spotting (KWS) plays a critical role in enabling speech-based\nuser interactions on smart devices. Recent developments in the field\nof deep learning have led to wide adoption of convolutional neural\nnetworks (CNNs) in KWS systems due to their exceptional accuracy and\nrobustness. The main challenge faced by KWS systems is the trade-off\nbetween high accuracy and low latency. Unfortunately, there has been\nlittle quantitative analysis of the actual latency of KWS models on\nmobile devices. This is especially concerning since conventional convolution-based\nKWS approaches are known to require a large number of operations to\nattain an adequate level of performance.<br style=\"mso-data-placement:same-cell;\"\n/><br style=\"mso-data-placement:same-cell;\" />\n  In this paper, we\npropose a temporal convolution for real-time KWS on mobile devices.\nUnlike most of the 2D convolution-based KWS approaches that require\na deep architecture to fully capture both low- and high-frequency domains,\nwe exploit temporal convolutions with a compact ResNet architecture.\nIn Google Speech Command Dataset, we achieve more than 385&#215; speedup\non Google Pixel 1 and surpass the accuracy compared to the state-of-the-art\nmodel. In addition, we release the implementation of the proposed and\nthe baseline models including an end-to-end pipeline for training models\nand evaluating them on mobile devices.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1363",
      "author_area_id": 5,
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "huang19j_interspeech": {
      "authors": [
        [
          "Zhiying",
          "Huang"
        ],
        [
          "Shiliang",
          "Zhang"
        ],
        [
          "Ming",
          "Lei"
        ]
      ],
      "title": "Audio Tagging with Compact Feedforward Sequential Memory Network and Audio-to-Audio Ratio Based Data Augmentation",
      "original": "1302",
      "page_count": 5,
      "order": 702,
      "p1": "3377",
      "pn": "3381",
      "abstract": [
        "Audio tagging aims to identify the presence or absence of audio events\nin the audio clip. Recently, a lot of researchers have paid attention\nto explore different model structures to improve the performance of\naudio tagging. Convolutional neural network (CNN) is the most popular\nchoice among a wide variety of model structures, and it&#8217;s successfully\napplied to audio events prediction task. However, the model complexity\nof CNN is relatively high, which is not efficient enough to ship in\nreal product. In this paper, compact Feedforward Sequential Memory\nNetwork (cFSMN) is proposed for audio tagging task. Experimental results\nshow that cFSMN-based system yields a comparable performance with the\nCNN-based system. Meanwhile, an audio-to-audio ratio (AAR) based data\naugmentation method is proposed to further improve the classifier performance.\nFinally, with raw waveforms of the balanced training set of  Audio\nSet which is a published standard database, our system can achieve\na state-of-the-art performance with AUC being 0.932. Moreover, cFSMN-based\nmodel has only 1.9 million parameters, which is only about 1/30 of\nthe CNN-based model.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1302",
      "author_area_id": 5,
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "yang19f_interspeech": {
      "authors": [
        [
          "Hansi",
          "Yang"
        ],
        [
          "Wei-Qiang",
          "Zhang"
        ]
      ],
      "title": "Music Genre Classification Using Duplicated Convolutional Layers in Neural Networks",
      "original": "1298",
      "page_count": 5,
      "order": 703,
      "p1": "3382",
      "pn": "3386",
      "abstract": [
        "Music genres are conventional categories that identify some pieces\nof music as belonging to a shared tradition or set of conventions.\nIn this paper, we proposed an approach to improve music genre classification\nwith convolutional neural networks (CNN). Using mel-scale spectrogram\nas the input, we used duplicate convolutional layers whose output will\nbe applied to different pooling layers to provide more statistical\ninformation for classification. Also, we made some modifications on\nresidual learning by taking more outputs from convolutional layers.\nBy comparing two different network topologies, our experimental results\non the GTZAN dataset show that the proposed method can effectively\nimprove the classification accuracy.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1298",
      "author_area_id": 5,
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "carmi19_interspeech": {
      "authors": [
        [
          "Nehory",
          "Carmi"
        ],
        [
          "Azaria",
          "Cohen"
        ],
        [
          "Mireille",
          "Avigal"
        ],
        [
          "Anat",
          "Lerner"
        ]
      ],
      "title": "A Storyteller&#8217;s Tale: Literature Audiobooks Genre Classification Using CNN and RNN Architectures",
      "original": "1154",
      "page_count": 4,
      "order": 704,
      "p1": "3387",
      "pn": "3390",
      "abstract": [
        "Identifying acoustic properties that characterize reading literary\ngenres can assist in giving a more personal and human tone to the speech\nof bots and automatic readings.<br style=\"mso-data-placement:same-cell;\"\n/><br style=\"mso-data-placement:same-cell;\" />\n  In this paper we consider\nthe following question: given speech segments of audiobooks, how well\ncan we classify them according to their literary genres? In this study\nwe consider three different literary genres: children, horror and suspense,\nand humorous audio books, taken from two free audio books sites: Librivox\nand YouTube.<br style=\"mso-data-placement:same-cell;\" /><br style=\"mso-data-placement:same-cell;\"\n/>\n  We ran four classification experiments: three for each pair of\ngenres, and one for all three genres together. We repeated each experiment\ntwice, with two different network architectures: Convolutional Neural\nNetwork (CNN) and Recurrent Neural Network (RNN).<br style=\"mso-data-placement:same-cell;\"\n/><br style=\"mso-data-placement:same-cell;\" />\n  Note that, throughout\nthe reading, there are sections that are more typical to the book&#8217;s\ngenre than others. As the samples were taken sequentially throughout\nthe reading of the books and were short in duration, we did not expect\nhigh classification rates. Nevertheless, the accuracy of all the experiments\nwere at least 72% for all the pair&#8217;s classifications; and at\nleast 57% for both architectures for the three classes classifications.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1154"
    },
    "hwang19_interspeech": {
      "authors": [
        [
          "Min-Jae",
          "Hwang"
        ],
        [
          "Hong-Goo",
          "Kang"
        ]
      ],
      "title": "Parameter Enhancement for MELP Speech Codec in Noisy Communication Environment",
      "original": "3249",
      "page_count": 5,
      "order": 705,
      "p1": "3391",
      "pn": "3395",
      "abstract": [
        "In this paper, we propose a deep learning (DL)-based parameter enhancement\nmethod for a mixed excitation linear prediction (MELP) speech codec\nin noisy communication environment. Unlike conventional speech enhancement\nmodules that are designed to obtain clean speech signal by removing\nnoise components before speech codec processing, the proposed method\ndirectly enhances codec parameters on either the encoder or decoder\nside. As the proposed method has been implemented by a small network\nwithout any additional processes required in conventional enhancement\nsystems, e.g., time-frequency (T-F) analysis/synthesis modules, its\ncomputational complexity is very low. By enhancing the noise-corrupted\ncodec parameters with the proposed DL framework, we achieved an enhancement\nsystem that is much simpler and faster than conventional T-F mask-based\nspeech enhancement methods, while the quality of its performance remains\nsimilar.\n"
      ],
      "doi": "10.21437/Interspeech.2019-3249",
      "author_area_id": 6,
      "author_area_label": "Speech Coding and Enhancement"
    },
    "zhen19_interspeech": {
      "authors": [
        [
          "Kai",
          "Zhen"
        ],
        [
          "Jongmo",
          "Sung"
        ],
        [
          "Mi Suk",
          "Lee"
        ],
        [
          "Seungkwon",
          "Beack"
        ],
        [
          "Minje",
          "Kim"
        ]
      ],
      "title": "Cascaded Cross-Module Residual Learning Towards Lightweight End-to-End Speech Coding",
      "original": "1816",
      "page_count": 5,
      "order": 706,
      "p1": "3396",
      "pn": "3400",
      "abstract": [
        "Speech codecs learn compact representations of speech signals to facilitate\ndata transmission. Many recent deep neural network (DNN) based end-to-end\nspeech codecs achieve low bitrates and high perceptual quality at the\ncost of model complexity. We propose a cross-module residual learning\n(CMRL) pipeline as a module carrier with each module reconstructing\nthe residual from its preceding modules. CMRL differs from other DNN-based\nspeech codecs, in that rather than modeling speech compression problem\nin a single large neural network, it optimizes a series of less-complicated\nmodules in a two-phase training scheme. The proposed method shows better\nobjective performance than AMR-WB and the state-of-the-art DNN-based\nspeech codec with a similar network architecture. As an end-to-end\nmodel, it takes raw PCM signals as an input, but is also compatible\nwith linear predictive coding (LPC), showing better subjective quality\nat high bitrates than AMR-WB and OPUS. The gain is achieved by using\nonly 0.9 million trainable parameters, a significantly less complex\narchitecture than the other DNN-based codecs in the literature.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1816",
      "author_area_id": 6,
      "author_area_label": "Speech Coding and Enhancement"
    },
    "backstrom19_interspeech": {
      "authors": [
        [
          "Tom",
          "B\u00e4ckstr\u00f6m"
        ]
      ],
      "title": "End-to-End Optimization of Source Models for Speech and Audio Coding Using a Machine Learning Framework",
      "original": "1284",
      "page_count": 5,
      "order": 707,
      "p1": "3401",
      "pn": "3405",
      "abstract": [
        "Speech coding is the most commonly used application of speech processing.\nAccumulated layers of improvements have however made codecs so complex\nthat optimization of individual modules becomes increasingly difficult.\nThis work introduces machine learning methodology to speech and audio\ncoding, such that we can optimize quality in terms of overall entropy.\nWe can then use conventional quantization, coding and perceptual models\nwithout modification such that the codec adheres to conventional requirements\non algorithmic complexity, latency and robustness to packet loss. Experiments\ndemonstrate that end-to-end optimization of quantization accuracy of\nthe spectral envelope can be used for a lossless reduction in bitrate\nof 0.4 kbits/s.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1284",
      "author_area_id": 6,
      "author_area_label": "Speech Coding and Enhancement"
    },
    "valin19_interspeech": {
      "authors": [
        [
          "Jean-Marc",
          "Valin"
        ],
        [
          "Jan",
          "Skoglund"
        ]
      ],
      "title": "A Real-Time Wideband Neural Vocoder at 1.6kb/s Using LPCNet",
      "original": "1255",
      "page_count": 5,
      "order": 708,
      "p1": "3406",
      "pn": "3410",
      "abstract": [
        "Neural speech synthesis algorithms are a promising new approach for\ncoding speech at very low bitrate. They have so far demonstrated quality\nthat far exceeds traditional vocoders, at the cost of very high complexity.\nIn this work, we present a low-bitrate neural vocoder based on the\nLPCNet model. The use of linear prediction and sparse recurrent networks\nmakes it possible to achieve real-time operation on general-purpose\nhardware. We demonstrate that LPCNet operating at 1.6 kb/s achieves\nsignificantly higher quality than MELP and that uncompressed LPCNet\ncan exceed the quality of a waveform codec operating at low bitrate.\nThis opens the way for new codec designs based on neural synthesis\nmodels.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1255",
      "author_area_id": 6,
      "author_area_label": "Speech Coding and Enhancement"
    },
    "fuchs19_interspeech": {
      "authors": [
        [
          "Guillaume",
          "Fuchs"
        ],
        [
          "Chamran",
          "Ashour"
        ],
        [
          "Tom",
          "B\u00e4ckstr\u00f6m"
        ]
      ],
      "title": "Super-Wideband Spectral Envelope Modeling for Speech Coding",
      "original": "1620",
      "page_count": 5,
      "order": 709,
      "p1": "3411",
      "pn": "3415",
      "abstract": [
        "Significant improvements in the quality of speech coders have been\nachieved by widening the coded frequency range from narrowband to wideband.\nHowever, existing speech coders still employ a limited band source-filter\nmodel extended by parametric coding of the higher band. In the present\nwork, a superwideband source-filter model running at 32 kHz is considered\nand especially its spectral magnitude envelope modeling. To match super-wideband\noperating mode, we adapted and compared two methods; Linear Predictive\nCoding (LPC) and Distribution Quantization (DQ). LPC uses autoregressive\nmodeling, while DQ quantifies the energy ratios between different parts\nof the spectrum. Parameters of both methods were quantized with a multi-stage\nvector quantization. Objective and subjective evaluations indicate\nthat both methods used in a super-wideband source-filter coding scheme\noffer the same quality range, making them an attractive alternative\nto conventional speech coders that require additional bandwidth extension.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1620",
      "author_area_id": 6,
      "author_area_label": "Speech Coding and Enhancement"
    },
    "li19q_interspeech": {
      "authors": [
        [
          "Xinyu",
          "Li"
        ],
        [
          "Venkata",
          "Chebiyyam"
        ],
        [
          "Katrin",
          "Kirchhoff"
        ]
      ],
      "title": "Speech Audio Super-Resolution for Speech Recognition",
      "original": "3043",
      "page_count": 5,
      "order": 710,
      "p1": "3416",
      "pn": "3420",
      "abstract": [
        "Automatic bandwidth extension (restoring high-frequency information\nfrom low sample rate audio) has a number of applications in speech\nprocessing. We introduce an end-to-end deep learning based system for\nspeech bandwidth extension for use in a downstream automatic speech\nrecognition (ASR) system. Specifically we propose a conditional generative\nadversarial network enriched with ASR-specific loss functions designed\nto upsample the speech audio while maintaining good ASR performance.\nEvaluations on the speech commands dataset and the LibriSpeech corpus\nshow that our approach outperforms a number of traditional bandwidth\nextension methods with respect to word error rate.\n"
      ],
      "doi": "10.21437/Interspeech.2019-3043",
      "author_area_id": 6,
      "author_area_label": "Speech Coding and Enhancement"
    },
    "gupta19e_interspeech": {
      "authors": [
        [
          "Deepika",
          "Gupta"
        ],
        [
          "Hanumant Singh",
          "Shekhawat"
        ]
      ],
      "title": "Artificial Bandwidth Extension Using H&#x221E; Optimization",
      "original": "1580",
      "page_count": 5,
      "order": 711,
      "p1": "3421",
      "pn": "3425",
      "abstract": [
        "This work proposes a new method for artificial bandwidth extension\n(ABE) that aims to extend the bandwidth of speech signals in narrowband\nvoice communications. We extract a signal model which consists of the\nwideband information. Using the signal model, we obtain an infinite\nimpulse response (IIR) interpolation filter with the help of H&#x221E;\noptimization. Interpolation filters are going to be distinct for the\nspeech signals because of their non-stationary (time-variant) nature.\nIn narrowband communications, only narrowband signal is accessible.\nHence, a codebook approach is intended to keep the IIR interpolation\nfilters information (wideband feature) together with their corresponding\nnarrowband signal characteristic (narrowband attribute). For that,\nthe Gaussian mixture modeling (GMM) codebook approach is utilized to\nestimate the wideband feature for a given narrowband attribute of the\nsignal. Performances are assessed for the two sorts of narrowband attributes.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1580"
    },
    "mittag19_interspeech": {
      "authors": [
        [
          "Gabriel",
          "Mittag"
        ],
        [
          "Sebastian",
          "M\u00f6ller"
        ]
      ],
      "title": "Quality Degradation Diagnosis for Voice Networks &#8212; Estimating the Perceived Noisiness, Coloration, and Discontinuity of Transmitted Speech",
      "original": "2636",
      "page_count": 5,
      "order": 712,
      "p1": "3426",
      "pn": "3430",
      "abstract": [
        "We present a single-ended quality diagnosis model for super-wideband\nspeech communication networks, which predicts the perceived  Noisiness,\n Coloration, and  Discontinuity of transmitted speech. The model is\nan extension to the single-ended speech quality prediction model NISQA\nand can additionally indicate the cause of quality degradation. Service\nproviders can use the model independently of the communication system&#8217;s\ntechnology since it is based on universal perceptual quality dimensions.\nThe prediction model consists of a convolutional neural network that\nfirstly calculates per-frame features of a speech signal and subsequently\naggregates the features over time with a recurrent neural network,\nto estimate the speech quality dimensions. The proposed diagnosis model\nachieves promising results with an average RMSE* of 0.24.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2636",
      "author_area_id": 6,
      "author_area_label": "Speech Coding and Enhancement"
    },
    "chai19b_interspeech": {
      "authors": [
        [
          "Li",
          "Chai"
        ],
        [
          "Jun",
          "Du"
        ],
        [
          "Chin-Hui",
          "Lee"
        ]
      ],
      "title": "A Cross-Entropy-Guided (CEG) Measure for Speech Enhancement Front-End Assessing Performances of Back-End Automatic Speech Recognition",
      "original": "2511",
      "page_count": 5,
      "order": 713,
      "p1": "3431",
      "pn": "3435",
      "abstract": [
        "One challenging problem of robust automatic speech recognition (ASR)\nis how to measure the goodness of a speech enhancement algorithm without\ncalculating word error rate (WER) due to the high costs of manual transcriptions,\nlanguage modeling and decoding process. In this study, a novel cross-entropy-guided\n(CEG) measure is proposed for assessing if enhanced speech predicted\nby a speech enhancement algorithm would produce a good performance\nfor robust ASR. CEG consists of three consecutive steps, namely the\nlow-level representations via the feature extraction, high-level representations\nvia the nonlinear mapping with the acoustic model, and the final CEG\ncalculation between the high-level representations of clean and enhanced\nspeech. Specifically, state posterior probabilities from the output\nof the neural network for the acoustic model are adopted as the high-level\nrepresentations and a cross-entropy criterion is used to calculate\nCEG. Experimental results show that CEG could consistently yield the\nhighest correlations with WER and achieve the most accurate assessment\nof the ASR performance when compared to distortion measures based on\nhuman auditory perception and an acoustic confidence measure. Potentially,\nCEG could be adopted to guide the parameter optimization of deep learning\nbased speech enhancement algorithms to further improve the ASR performance.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2511",
      "author_area_id": 6,
      "author_area_label": "Speech Coding and Enhancement"
    },
    "moller19_interspeech": {
      "authors": [
        [
          "Sebastian",
          "M\u00f6ller"
        ],
        [
          "Gabriel",
          "Mittag"
        ],
        [
          "Thilo",
          "Michael"
        ],
        [
          "Vincent",
          "Barriac"
        ],
        [
          "Hitoshi",
          "Aoki"
        ]
      ],
      "title": "Extending the E-Model Towards Super-Wideband and Fullband Speech Communication Scenarios",
      "original": "1340",
      "page_count": 5,
      "order": 714,
      "p1": "3436",
      "pn": "3440",
      "abstract": [
        "In order to plan speech communication services regarding the quality\nexperienced by their users, parametric models have been used since\na long time. These models predict the overall quality experienced by\na communication partner on the basis of parameters describing the elements\nof the transmission channel and the terminal equipment. The mostly\nused model is the E-model which is standardized in ITU-T Rec. G.107\nfor narrowband and in ITU-T Rec. G.107.1 for wideband scenarios. However,\nwith the advent of super-wideband and fullband transmission, the E-model\nneeds to be extended. In this paper, we propose a first version of\nan extended E-model which addresses both super-wideband and fullband\nscenarios, and which predicts the effects of speech codecs, packet\nloss, and delay as the most important degradations to be expected in\nsuch scenarios. Predictions are compared to the results of listening-only\nand conversational tests as well as to signal-based predictions, showing\na reasonable prediction accuracy.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1340",
      "author_area_id": 6,
      "author_area_label": "Speech Coding and Enhancement"
    },
    "sadhu19_interspeech": {
      "authors": [
        [
          "Samik",
          "Sadhu"
        ],
        [
          "Hynek",
          "Hermansky"
        ]
      ],
      "title": "Modulation Vectors as Robust Feature Representation for ASR in Domain Mismatched Conditions",
      "original": "2723",
      "page_count": 5,
      "order": 715,
      "p1": "3441",
      "pn": "3445",
      "abstract": [
        "In this work, we demonstrate the robustness of Modulation Vectors,\nin domain mismatches between the training and test conditions in an\nAutomatic Speech Recognition (ASR) system. Our work focuses on the\nspecific task of dealing with mismatches caused by reverberation. We\nuse simulated data from TIMIT and real reverberant speech from the\nREVERB challenge data to evaluate the performance of our system. The\npaper also describes a multistream system to combine information from\nMel Frequency Cepstral Coefficient (MFCC) and M-vectors to improve\nthe ASR performance in both matched and mismatched datasets. The proposed\nmultistream system achieves a relative improvement of 25% in recognition\naccuracy on the mismatched condition, while a M-vector trained hybrid\nASR system shows a 7&#8211;8% improvement in recognition accuracy,\nboth w.r.t. a MFCC trained hybrid ASR system.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2723",
      "author_area_id": 8,
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "li19r_interspeech": {
      "authors": [
        [
          "Chenda",
          "Li"
        ],
        [
          "Yanmin",
          "Qian"
        ]
      ],
      "title": "Prosody Usage Optimization for Children Speech Recognition with Zero Resource Children Speech",
      "original": "2659",
      "page_count": 5,
      "order": 716,
      "p1": "3446",
      "pn": "3450",
      "abstract": [
        "Children&#8217;s speech recognition remains a big challenge for automatic\nspeech recognition. Due to the more difficult process and higher cost\non data collection, most current ASR systems are optimized only using\nlots of adult speech with limited or even none children&#8217;s speech.\nAccordingly, the acoustic mismatch between children&#8217;s and adult\nspeech is the primary reason for the ASR performance degradation when\nfacing children&#8217;s speech. To overcome this problem, we proposed\nseveral approaches to improve children&#8217;s speech recognition without\nusing any children&#8217;s speech data. A better utilization strategy\non prosody-based features is developed. First, pitch and prosody modification\nis explored in both training and testing respectively, which can significantly\nreduce the mismatch between two types of speech. Furthermore, joint-decoding\nwith both the prosody modified speech and the original speech is designed\nto get a more robust performance on both children&#8217;s and adult\nspeech. Experiments are evaluated on a Mandarin speech recognition\ntask, with only 400-hour adult speech in the training. The results\nshow that our proposed method can obtain a large gain on children&#8217;s\nspeech, with relative &#126;20% WER reduction compared to the baseline,\nand also no obvious degradation is observed on the adult speech for\nthe proposed system.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2659",
      "author_area_id": 8,
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "agrawal19_interspeech": {
      "authors": [
        [
          "Purvi",
          "Agrawal"
        ],
        [
          "Sriram",
          "Ganapathy"
        ]
      ],
      "title": "Unsupervised Raw Waveform Representation Learning for ASR",
      "original": "2652",
      "page_count": 5,
      "order": 717,
      "p1": "3451",
      "pn": "3455",
      "abstract": [
        "In this paper, we propose a deep representation learning approach using\nthe raw speech waveform in an unsupervised learning paradigm. The first\nlayer of the proposed deep model performs acoustic filtering while\nthe subsequent layer performs modulation filtering. The acoustic filterbank\nis implemented using cosine-modulated Gaussian filters whose parameters\nare learned. The modulation filtering is performed on log transformed\noutputs of the first layer and this is achieved using a skip connection\nbased architecture. The outputs from this two layer filtering are fed\nto the variational autoencoder model. All the model parameters including\nthe filtering layers are learned using the VAE cost function. We employ\nthe learned representations (second layer outputs) in a speech recognition\ntask. Experiments are conducted on Aurora-4 (additive noise with channel\nartifact) and CHiME-3 (additive noise with reverberation) databases.\nIn these experiments, the learned representations from the proposed\nframework provide significant improvements in ASR results over the\nbaseline filterbank features and other robust front-ends (average relative\nimprovements of 16% and 6% in word error rate over baseline features\non clean and multi-condition training, respectively on Aurora-4 dataset,\nand 21% over the baseline features on CHiME-3 database).\n"
      ],
      "doi": "10.21437/Interspeech.2019-2652",
      "author_area_id": 8,
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "ramsay19_interspeech": {
      "authors": [
        [
          "David B.",
          "Ramsay"
        ],
        [
          "Kevin",
          "Kilgour"
        ],
        [
          "Dominik",
          "Roblek"
        ],
        [
          "Matthew",
          "Sharifi"
        ]
      ],
      "title": "Low-Dimensional Bottleneck Features for On-Device Continuous Speech Recognition",
      "original": "2193",
      "page_count": 4,
      "order": 718,
      "p1": "3456",
      "pn": "3459",
      "abstract": [
        "Low power digital signal processors (DSPs) typically have a very limited\namount of memory in which to cache data. In this paper we develop efficient\nbottleneck feature (BNF) extractors that can be run on a DSP, and retrain\na baseline large-vocabulary continuous speech recognition (LVCSR) system\nto use these BNFs with only a minimal loss of accuracy. The small BNFs\nallow the DSP chip to cache more audio features while the main application\nprocessor is suspended, thereby reducing the overall battery usage.\nOur presented system is able to reduce the footprint of standard, fixed\npoint DSP spectral features by a factor of 10 without any loss in word\nerror rate (WER) and by a factor of 64 with only a 5.8% relative increase\nin WER.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2193",
      "author_area_id": 9,
      "author_area_label": "Speech Recognition: Architecture, Search, and Linguistic Components"
    },
    "riviello19_interspeech": {
      "authors": [
        [
          "Alexandre",
          "Riviello"
        ],
        [
          "Jean-Pierre",
          "David"
        ]
      ],
      "title": "Binary Speech Features for Keyword Spotting Tasks",
      "original": "1877",
      "page_count": 5,
      "order": 719,
      "p1": "3460",
      "pn": "3464",
      "abstract": [
        "Keyword spotting is a classification task which aims to detect a specific\nset of spoken words. In general, this type of task runs on a power-constrained\ndevice such as a smartphone. One method to reduce the power consumption\nof a keyword spotting algorithm (typically a neural network) is to\nreduce the precision of the network weights and activations. In this\npaper, we propose a new representation of speech features which is\nmore adapted to low-precision networks and compatible with binary/ternary\nneural networks. The new representation is based on the log-Mel spectrogram\nand models the variation of power over time. Tested on a ResNet, this\nrepresentation produces results nearly as accurate as full-precision\nMFCCs, which are traditionally used in speech recognition applications.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1877",
      "author_area_id": 8,
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "schneider19_interspeech": {
      "authors": [
        [
          "Steffen",
          "Schneider"
        ],
        [
          "Alexei",
          "Baevski"
        ],
        [
          "Ronan",
          "Collobert"
        ],
        [
          "Michael",
          "Auli"
        ]
      ],
      "title": "wav2vec: Unsupervised Pre-Training for Speech Recognition",
      "original": "1873",
      "page_count": 5,
      "order": 720,
      "p1": "3465",
      "pn": "3469",
      "abstract": [
        "We explore unsupervised pre-training for speech recognition by learning\nrepresentations of raw audio. wav2vec is trained on large amounts of\nunlabeled audio data and the resulting representations are then used\nto improve acoustic model training. We pre-train a simple multi-layer\nconvolutional neural network optimized via a noise contrastive binary\nclassification task. Our experiments on WSJ reduce WER of a strong\ncharacter-based log-mel filterbank baseline by up to 36%when only a\nfew hours of transcribed data is available. Our approach achieves 2.43%\nWER on the nov92 test set. This outperforms Deep Speech 2, the best\nreported character-based system in the literature while using three\norders of magnitude less labeled training data.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1873",
      "author_area_id": 8,
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "cho19b_interspeech": {
      "authors": [
        [
          "Sunghye",
          "Cho"
        ],
        [
          "Mark",
          "Liberman"
        ],
        [
          "Yong-cheol",
          "Lee"
        ]
      ],
      "title": "Automatic Detection of Prosodic Focus in American English",
      "original": "1668",
      "page_count": 5,
      "order": 721,
      "p1": "3470",
      "pn": "3474",
      "abstract": [
        "Focus, which is usually modulated by prosodic prominence, highlights\na particular element within a sentence for emphasis or contrast. Despite\nits importance in communication, it has received little attention in\nthe field of speech recognition. This paper developed an automatic\ndetection system of prosodic focus in American English, using telephone-number\nstrings. Our data were 100 10-digit phone number strings read by 5\nspeakers (3 females and 2 males). We extracted 18 prosodic features\nfrom each digit within the strings and one categorical variable and\ntrained a Random Forest model to detect where the focused digit is\nwithin a given string. We also compared the model performance to human\njudgment rates from a perception experiment with 67 native speakers\nof American English. Our final model shows 92% of accuracy in detecting\nthe location of prosodic focus, which is slightly lower than the human\nperception (97.2%) but much better than the chance level (10%). We\ndiscuss the predictive features in our model and potential features\nto add in the future study.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1668",
      "author_area_id": 8,
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "menon19_interspeech": {
      "authors": [
        [
          "Raghav",
          "Menon"
        ],
        [
          "Herman",
          "Kamper"
        ],
        [
          "Ewald van der",
          "Westhuizen"
        ],
        [
          "John",
          "Quinn"
        ],
        [
          "Thomas",
          "Niesler"
        ]
      ],
      "title": "Feature Exploration for Almost Zero-Resource ASR-Free Keyword Spotting Using a Multilingual Bottleneck Extractor and Correspondence Autoencoders",
      "original": "1665",
      "page_count": 5,
      "order": 722,
      "p1": "3475",
      "pn": "3479",
      "abstract": [
        "We compare features for dynamic time warping (DTW) when used to bootstrap\nkeyword spotting (KWS) in an almost zero-resource setting. Such quickly-deployable\nsystems aim to support United Nations (UN) humanitarian relief efforts\nin parts of Africa with severely under-resourced languages. Our objective\nis to identify acoustic features that provide acceptable KWS performance\nin such environments. As supervised resource, we restrict ourselves\nto a small, easily acquired and independently compiled set of isolated\nkeywords. For feature extraction, a multilingual bottleneck feature\n(BNF) extractor, trained on well-resourced out-of-domain languages,\nis integrated with a correspondence autoencoder (CAE) trained on extremely\nsparse in-domain data. On their own, BNFs and CAE features are shown\nto achieve a more than 2% absolute performance improvement over baseline\nMFCCs. However, by using BNFs as input to the CAE, even better performance\nis achieved, with a more than 11% absolute improvement in ROC AUC over\nMFCCs and more than twice as many top-10 retrievals for two evaluated\nlanguages, English and Luganda. We conclude that integrating BNFs with\nthe CAE allows both large out-of-domain and sparse in-domain resources\nto be exploited for improved ASR-free keyword spotting.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1665",
      "author_area_id": 8,
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "loweimi19_interspeech": {
      "authors": [
        [
          "Erfan",
          "Loweimi"
        ],
        [
          "Peter",
          "Bell"
        ],
        [
          "Steve",
          "Renals"
        ]
      ],
      "title": "On Learning Interpretable CNNs with Parametric Modulated Kernel-Based Filters",
      "original": "1257",
      "page_count": 5,
      "order": 723,
      "p1": "3480",
      "pn": "3484",
      "abstract": [
        "We investigate the problem of direct waveform modelling using parametric\nkernel-based filters in a convolutional neural network (CNN) framework,\nbuilding on SincNet, a CNN employing the cardinal sine (sinc) function\nto implement learnable bandpass filters. To this end, the general problem\nof learning a filterbank consisting of modulated kernel-based baseband\nfilters is studied. Compared to standard CNNs, such models have fewer\nparameters, learn faster, and require less training data. They are\nalso more amenable to human interpretation, paving the way to embedding\nsome perceptual prior knowledge in the architecture. We have investigated\nthe replacement of the rectangular filters of SincNet with triangular,\ngammatone and Gaussian filters, resulting in higher model flexibility\nand a reduction to the phone error rate. We also explore the properties\nof the learned filters learned for TIMIT phone recognition from both\nperceptual and statistical standpoints. We find that the filters in\nthe first layer, which directly operate on the waveform, are in accord\nwith the prior knowledge utilised in designing and engineering standard\nfilters such as mel-scale triangular filters. That is, the networks\nlearn to pay more attention to perceptually significant spectral neighbourhoods\nwhere the data centroid is located, and the variance and Shannon entropy\nare highest.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1257",
      "author_area_id": 8,
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "verwimp19_interspeech": {
      "authors": [
        [
          "Lyan",
          "Verwimp"
        ],
        [
          "Jerome R.",
          "Bellegarda"
        ]
      ],
      "title": "Reverse Transfer Learning: Can Word Embeddings Trained for Different NLP Tasks Improve Neural Language Models?",
      "original": "1332",
      "page_count": 5,
      "order": 724,
      "p1": "3485",
      "pn": "3489",
      "abstract": [
        "Natural language processing (NLP) tasks tend to suffer from a paucity\nof suitably annotated training data, hence the recent success of transfer\nlearning across a wide variety of them. The typical recipe involves:\n(i) training a deep, possibly bidirectional, neural network with an\nobjective related to language modeling, for which training data is\nplentiful; and (ii) using the trained network to derive contextual\nrepresentations that are far richer than standard linear word embeddings\nsuch as word2vec, and thus result in important gains. In this work,\nwe wonder whether the opposite perspective is also true: can contextual\nrepresentations trained for different NLP tasks improve language modeling\nitself? Since language models (LMs) are predominantly locally optimized,\nother NLP tasks may help them make better predictions based on the\nentire semantic fabric of a document. We test the performance of several\ntypes of pre-trained embeddings in neural LMs, and we investigate whether\nit is possible to make the LM more aware of global semantic information\nthrough embeddings pre-trained with a domain classification model.\nInitial experiments suggest that as long as the proper objective criterion\nis used during training, pre-trained embeddings are likely to be beneficial\nfor neural language modeling.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1332",
      "author_area_id": 9,
      "author_area_label": "Speech Recognition: Architecture, Search, and Linguistic Components"
    },
    "chen19l_interspeech": {
      "authors": [
        [
          "Zhehuai",
          "Chen"
        ],
        [
          "Mahaveer",
          "Jain"
        ],
        [
          "Yongqiang",
          "Wang"
        ],
        [
          "Michael L.",
          "Seltzer"
        ],
        [
          "Christian",
          "Fuegen"
        ]
      ],
      "title": "Joint Grapheme and Phoneme Embeddings for Contextual End-to-End ASR",
      "original": "1434",
      "page_count": 5,
      "order": 725,
      "p1": "3490",
      "pn": "3494",
      "abstract": [
        "End-to-end approaches to automatic speech recognition, such as Listen-Attend-Spell\n(LAS), blend all components of a traditional speech recognizer into\na unified model. Although this simplifies training and decoding pipelines,\na unified model is hard to adapt when mismatch exists between training\nand test data, especially if this information is dynamically changing.\nThe Contextual LAS (CLAS) framework tries to solve this problem by\nencoding contextual entities into fixed-dimensional embeddings and\nutilizing an attention mechanism to model the probabilities of seeing\nthese entities. In this work, we improve the CLAS approach by proposing\nseveral new strategies to extract embeddings for the contextual entities.\nWe compare these embedding extractors based on graphemic and phonetic\ninput and/or output sequences and show that an encoder-decoder model\ntrained jointly towards graphemes and phonemes outperforms other approaches.\nLeveraging phonetic information obtains better discrimination for similarly\nwritten graphemic sequences and also helps the model generalize better\nto graphemic sequences unseen in training. We show significant improvements\nover the original CLAS approach and also demonstrate that the proposed\nmethod scales much better to a large number of contextual entities\nacross multiple domains.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1434",
      "author_area_id": 9,
      "author_area_label": "Speech Recognition: Architecture, Search, and Linguistic Components"
    },
    "liu19h_interspeech": {
      "authors": [
        [
          "Chang",
          "Liu"
        ],
        [
          "Zhen",
          "Zhang"
        ],
        [
          "Pengyuan",
          "Zhang"
        ],
        [
          "Yonghong",
          "Yan"
        ]
      ],
      "title": "Character-Aware Sub-Word Level Language Modeling for Uyghur and Turkish ASR",
      "original": "1484",
      "page_count": 5,
      "order": 726,
      "p1": "3495",
      "pn": "3499",
      "abstract": [
        "Uyghur and Turkish are two typical agglutinative languages, which suffer\nheavily from the data sparsity problem. Due to this, we first apply\na statistical morphological segmentation and change the number of morphs\nto get a better sub-word level automatic speech recognition (ASR) system.\nThe best systems, which yield 2.03% and 1.65% absolute WER reductions\nfrom the word level systems for Uyghur and Turkish respectively, are\nused for further n-best rescoring. To further alleviate the data sparsity\nproblem, we use both convolutional neural network (CNN) based and bi-directional\nlong short-term memory (BLSTM) based character-aware language models\non the two languages. In order to alleviate the information missing\nof the middle steps of the BLSTM based character aware language model,\nwe propose to use the weighted average of each time-steps&#8217; outputs.\nThe proposed weighting methods can be divided into three categories:\ndecay based, position-based and attention-based. Results show that\nthe decay based weighting method leads to the most significant WER\nreductions, which are 2.38% and 1.96%, compared with the sub-word level\n1-pass ASR system for Uyghur and Turkish respectively.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1484"
    },
    "pusateri19_interspeech": {
      "authors": [
        [
          "Ernest",
          "Pusateri"
        ],
        [
          "Christophe Van",
          "Gysel"
        ],
        [
          "Rami",
          "Botros"
        ],
        [
          "Sameer",
          "Badaskar"
        ],
        [
          "Mirko",
          "Hannemann"
        ],
        [
          "Youssef",
          "Oualil"
        ],
        [
          "Ilya",
          "Oparin"
        ]
      ],
      "title": "Connecting and Comparing Language Model Interpolation Techniques",
      "original": "1822",
      "page_count": 5,
      "order": 727,
      "p1": "3500",
      "pn": "3504",
      "abstract": [
        "In this work, we uncover a theoretical connection between two language\nmodel interpolation techniques, count merging and Bayesian interpolation.\nWe compare these techniques as well as linear interpolation in three\nscenarios with abundant training data per component model. Consistent\nwith prior work, we show that both count merging and Bayesian interpolation\noutperform linear interpolation. We include the first (to our knowledge)\npublished comparison of count merging and Bayesian interpolation, showing\nthat the two techniques perform similarly. Finally, we argue that other\nconsiderations will make Bayesian interpolation the preferred approach\nin most circumstances.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1822",
      "author_area_id": 9,
      "author_area_label": "Speech Recognition: Architecture, Search, and Linguistic Components"
    },
    "khassanov19b_interspeech": {
      "authors": [
        [
          "Yerbolat",
          "Khassanov"
        ],
        [
          "Zhiping",
          "Zeng"
        ],
        [
          "Van Tung",
          "Pham"
        ],
        [
          "Haihua",
          "Xu"
        ],
        [
          "Eng Siong",
          "Chng"
        ]
      ],
      "title": "Enriching Rare Word Representations in Neural Language Models by Embedding Matrix Augmentation",
      "original": "1858",
      "page_count": 5,
      "order": 728,
      "p1": "3505",
      "pn": "3509",
      "abstract": [
        "The neural language models (NLM) achieve strong generalization capability\nby learning the dense representation of words and using them to estimate\nprobability distribution function. However, learning the representation\nof rare words is a challenging problem causing the NLM to produce unreliable\nprobability estimates. To address this problem, we propose a method\nto enrich representations of rare words in pre-trained NLM and consequently\nimprove its probability estimation performance. The proposed method\naugments the word embedding matrices of pre-trained NLM while keeping\nother parameters unchanged. Specifically, our method updates the embedding\nvectors of rare words using embedding vectors of other semantically\nand syntactically similar words. To evaluate the proposed method, we\nenrich the rare street names in the pre-trained NLM and use it to rescore\n100-best hypotheses output from the Singapore English speech recognition\nsystem. The enriched NLM reduces the word error rate by 6% relative\nand improves the recognition accuracy of the rare words by 16% absolute\nas compared to the baseline NLM.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1858",
      "author_area_id": 9,
      "author_area_label": "Speech Recognition: Architecture, Search, and Linguistic Components"
    },
    "yu19b_interspeech": {
      "authors": [
        [
          "Jianwei",
          "Yu"
        ],
        [
          "Max W.Y.",
          "Lam"
        ],
        [
          "Shoukang",
          "Hu"
        ],
        [
          "Xixin",
          "Wu"
        ],
        [
          "Xu",
          "Li"
        ],
        [
          "Yuewen",
          "Cao"
        ],
        [
          "Xunying",
          "Liu"
        ],
        [
          "Helen",
          "Meng"
        ]
      ],
      "title": "Comparative Study of Parametric and Representation Uncertainty Modeling for Recurrent Neural Network Language Models",
      "original": "1927",
      "page_count": 5,
      "order": 729,
      "p1": "3510",
      "pn": "3514",
      "abstract": [
        "Recurrent neural network language models (RNNLMs) have shown superior\nperformance across a range of tasks, including speech recognition.\nThe hidden layer of RNNLMs plays a vital role in learning the suitable\nrepresentation of contexts for word prediction. However, the deterministic\nmodel parameters and fixed hidden vectors in conventional RNNLMs have\nlimited power in modeling the uncertainty over hidden representations.\nIn order to address this issue, in this paper, a comparative study\nof parametric and hidden representation uncertainty modeling approaches\nbased on Bayesian gates and variational RNNLMs respectively is investigated\non long short-term memory (LSTM) and gated recurrent units (GRU) LMs.\nExperimental results are presented on two tasks: PennTreebank (PTB)\ncorpus, Switchboard conversational telephone speech (SWBD). Consistent\nperformance improvements were obtained over conventional RNNLMs in\nterms of both perplexity and word error rate.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1927",
      "author_area_id": 9,
      "author_area_label": "Speech Recognition: Architecture, Search, and Linguistic Components"
    },
    "agenbag19_interspeech": {
      "authors": [
        [
          "Wiehan",
          "Agenbag"
        ],
        [
          "Thomas",
          "Niesler"
        ]
      ],
      "title": "Improving Automatically Induced Lexicons for Highly Agglutinating Languages Using Data-Driven Morphological Segmentation",
      "original": "2164",
      "page_count": 5,
      "order": 730,
      "p1": "3515",
      "pn": "3519",
      "abstract": [
        "We present a method of improving the performance of automatically induced\nlexicons for highly agglutinating languages. Our previous work demonstrated\nthe feasibility of using automatic sub-word unit discovery and lexicon\ninduction to enable ASR for under-resourced languages. However, a particularly\nchallenging case for such approaches is found in agglutinating languages,\nwhich have large vocabularies of infrequently used words. In this study,\nwe address the unfavorable vocabulary distribution of such languages\nby performing data-driven morphological segmentation of the orthography\nprior to lexicon induction. We apply this novel step to a corpus of\nrecorded radio broadcasts in Luganda, which is a highly agglutinating\nand severely under-resourced language. The intervention leads to a\n10% (relative) reduction in WER, which puts the resulting ASR performance\non par with an expert lexicon. When context is added to the morphological\nsegments prior to lexicon induction, a further 1% WER reduction is\nachieved. This demonstrates that it is feasible to perform ASR in an\nunder-resourced setting using an automatically induced lexicon even\nin the case of a highly agglutinating language.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2164",
      "author_area_id": 9,
      "author_area_label": "Speech Recognition: Architecture, Search, and Linguistic Components"
    },
    "coucheirolimeres19_interspeech": {
      "authors": [
        [
          "Alejandro",
          "Coucheiro-Limeres"
        ],
        [
          "Fernando",
          "Fern\u00e1ndez-Mart\u00ednez"
        ],
        [
          "Rub\u00e9n",
          "San-Segundo"
        ],
        [
          "Javier",
          "Ferreiros-L\u00f3pez"
        ]
      ],
      "title": "Attention-Based Word Vector Prediction with LSTMs and its Application to the OOV Problem in ASR",
      "original": "2347",
      "page_count": 5,
      "order": 731,
      "p1": "3520",
      "pn": "3524",
      "abstract": [
        "We propose three architectures for a word vector prediction system\n(WVPS) built with LSTMs that consider both past and future contexts\nof a word for predicting a vector in an embedded space where its surrounding\narea is semantically related to the considered word. We introduce an\nattention mechanism in one of the architectures so the system is able\nto assess the specific contribution of each context word to the prediction.\nAll the architectures are trained under the same conditions and the\nsame training material, following a curricular-learning fashion in\nthe presentation of the data. For the inputs, we employ pre-trained\nword embeddings. We evaluate the systems after the same number of training\nsteps, over two different corpora composed of ground-truth speech transcriptions\nin Spanish language from TCSTAR and TV recordings used in the Search\non Speech Challenge of IberSPEECH 2018. The results show that we are\nable to reach significant differences between the architectures, consistently\nacross both corpora. The attention-based architecture achieves the\nbest results, suggesting its adequacy for the task. Also, we illustrate\nthe usefulness of the systems for resolving out-of-vocabulary (OOV)\nregions marked by an ASR system capable of detecting OOV occurrences.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2347",
      "author_area_id": 9,
      "author_area_label": "Speech Recognition: Architecture, Search, and Linguistic Components"
    },
    "gao19d_interspeech": {
      "authors": [
        [
          "Yingying",
          "Gao"
        ],
        [
          "Junlan",
          "Feng"
        ],
        [
          "Ying",
          "Liu"
        ],
        [
          "Leijing",
          "Hou"
        ],
        [
          "Xin",
          "Pan"
        ],
        [
          "Yong",
          "Ma"
        ]
      ],
      "title": "Code-Switching Sentence Generation by Bert and Generative Adversarial Networks",
      "original": "2501",
      "page_count": 5,
      "order": 732,
      "p1": "3525",
      "pn": "3529",
      "abstract": [
        "Code-switching has become a common linguistic phenomenon. Comparing\nto monolingual ASR tasks, insufficient data is a major challenge for\ncode-switching speech recognition. In this paper, we propose an approach\nto compositionally employ the Bidirectional Encoder Representations\nfrom Transformers (Bert) model and Generative Adversarial Net (GAN)\nmodel for code-switching text data generation. It improves upon previous\nwork by (1) applying Bert as a masked language model to predict the\nmixed-in foreign words and (2) basing on the GAN framework with Bert\nfor both the generator and discriminator to further assure the generated\nsentences similar enough to the natural examples. We evaluate the effectiveness\nof the generated data by its contribution to ASR. Experiments show\nour approach can reduce the English word error rate by 1.5% with the\nMandarin-English code-switching spontaneous speech corpus OC16-CE80.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2501",
      "author_area_id": 9,
      "author_area_label": "Speech Recognition: Architecture, Search, and Linguistic Components"
    },
    "ritchie19_interspeech": {
      "authors": [
        [
          "Sandy",
          "Ritchie"
        ],
        [
          "Richard",
          "Sproat"
        ],
        [
          "Kyle",
          "Gorman"
        ],
        [
          "Daan van",
          "Esch"
        ],
        [
          "Christian",
          "Schallhart"
        ],
        [
          "Nikos",
          "Bampounis"
        ],
        [
          "Beno\u00eet",
          "Brard"
        ],
        [
          "Jonas Fromseier",
          "Mortensen"
        ],
        [
          "Millie",
          "Holt"
        ],
        [
          "Eoin",
          "Mahon"
        ]
      ],
      "title": "Unified Verbalization for Speech Recognition &amp; Synthesis Across Languages",
      "original": "2807",
      "page_count": 5,
      "order": 733,
      "p1": "3530",
      "pn": "3534",
      "abstract": [
        "We describe a new approach to converting written tokens to their spoken\nform, which can be shared by automatic speech recognition (ASR) and\ntext-to-speech synthesis (TTS) systems. Both ASR and TTS need to map\nfrom the written to the spoken domain, and we present an approach that\nenables us to share verbalization grammars between the two systems\nwhile exploiting linguistic commonalities to provide simple default\nverbalizations. We also describe improvements to an induction system\nfor number names grammars. Between these shared ASR/TTS verbalizers\nand the improved induction system for number names grammars, we achieve\nsignificant gains in development time and scalability across languages.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2807"
    },
    "sharma19e_interspeech": {
      "authors": [
        [
          "Dravyansh",
          "Sharma"
        ],
        [
          "Melissa",
          "Wilson"
        ],
        [
          "Antoine",
          "Bruguier"
        ]
      ],
      "title": "Better Morphology Prediction for Better Speech Systems",
      "original": "3207",
      "page_count": 5,
      "order": 734,
      "p1": "3535",
      "pn": "3539",
      "abstract": [
        "Prediction of morphological forms is a well-studied problem and can\nlead to better speech systems either directly by rescoring models for\ncorrecting morphology, or indirectly by more accurate dialog systems\nwith improved natural language generation and understanding. This includes\nboth lemmatization, i.e. deriving the lemma or root word from a given\nsurface form as well as morphological inflection, i.e. deriving surface\nforms from the lemma. We train and evaluate various language-agnostic\nend-to-end neural sequence-to-sequence models for these tasks and compare\ntheir effectiveness. We further augment our models with pronunciation\ninformation which is typically available in speech systems to further\nimprove the accuracies of the same tasks. We present the results across\nboth morphologically modest and rich languages to show robustness of\nour approach.\n"
      ],
      "doi": "10.21437/Interspeech.2019-3207",
      "author_area_id": 9,
      "author_area_label": "Speech Recognition: Architecture, Search, and Linguistic Components"
    },
    "sennema19_interspeech": {
      "authors": [
        [
          "Anke",
          "Sennema"
        ],
        [
          "Silke",
          "Hamann"
        ]
      ],
      "title": "Vietnamese Learners Tackling the German /&#643;t/ in Perception",
      "original": "2832",
      "page_count": 4,
      "order": 735,
      "p1": "3540",
      "pn": "3543",
      "abstract": [
        "Previous observations from didactic studies have indicated that Vietnamese\nlearners of German as a foreign language often fail to realize consonantal\nclusters in German [1, 2, 3]. The present study investigated whether\nthis problem occurs already at the level of perception, i.e., whether\nVietnamese learners find it difficult to perceive the difference between\na cluster and a single consonant. We focused on the discrimination\nbetween the German cluster /&#643;t/ and the single consonants /t/\nand /&#643;/, both in onset and coda position. Due to different phonotactic\nrestrictions on coda consonants in Vietnamese, we expected the coda\nposition to pose a bigger challenge for correct discrimination than\nthe onset position. With an AX discrimination task, we tested how 83\nuniversity students from Hanoi perceived these contrasts. Our findings\nshow that only the distinction between /&#643;t/-/&#643;/ in coda position\nposed a real challenge to our listeners. We attribute this difficulty\nto the weak and non-native auditory cues for the plosive in this position.\nFor all other contrasts our participants performed surprisingly well.\nWe propose that this is due to the influence of English as first L2\nthat facilitates the acquisition of phonological contrasts in German\nas an L3.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2832",
      "author_area_id": 1,
      "author_area_label": "Speech Perception, Production and Acquisition"
    },
    "lewis19_interspeech": {
      "authors": [
        [
          "Scott",
          "Lewis"
        ],
        [
          "Adib",
          "Mehrabi"
        ],
        [
          "Esther de",
          "Leeuw"
        ]
      ],
      "title": "An Articulatory-Acoustic Investigation into GOOSE-Fronting in German-English Bilinguals Residing in London, UK",
      "original": "2637",
      "page_count": 5,
      "order": 736,
      "p1": "3544",
      "pn": "3548",
      "abstract": [
        "This study explores L2 acquisition of socially conditioned phonetic\nvariation in 13 German-English sequential bilinguals residing in London,\nUK. The phonetic variable analysed is GOOSE-fronting, i.e. the more\nfront pronunciation of /u/ in words like &#8216;goose&#8217;, acoustically\nmanifested through an increased F2 frequency. In the South of England,\nGOOSE-fronting is a sound change considered to be led by young females.\nWe investigated whether bilinguals adhered to this pattern, e.g. whether\nyounger female German-English bilinguals exhibited a relatively higher\nF2 frequency in words like &#8216;goose&#8217; than other bilinguals.\nThe bilinguals&#8217; English /u/ productions were compared against\ntheir German /u/ (lower F2 as more back) and /y/ (higher F2 as more\nfront) to determine the degree of GOOSE-fronting and whether their\nF2 values were closer to /y/ than /u/. Normalised formant values were\nconsidered in relation to lingual measurements obtained using ultrasound\ntongue imaging. The acoustic and articulatory results revealed that\nfemale bilinguals indeed produced more front English /u/ vowels than\ntheir male counterparts. Within female speakers, age and length of\nresidence in the UK were found to be significant, with younger speakers\nwho had lived in the UK longer than five years displaying the greatest\ndegree of GOOSE-fronting.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2637",
      "author_area_id": 1,
      "author_area_label": "Speech Perception, Production and Acquisition"
    },
    "jenne19_interspeech": {
      "authors": [
        [
          "Sabrina",
          "Jenne"
        ],
        [
          "Ngoc Thang",
          "Vu"
        ]
      ],
      "title": "Multimodal Articulation-Based Pronunciation Error Detection with Spectrogram and Acoustic Features",
      "original": "1677",
      "page_count": 5,
      "order": 737,
      "p1": "3549",
      "pn": "3553",
      "abstract": [
        "Articulation-based pronunciation error detection is concerned with\nthe task of diagnosing mispronounced segments in non-native speech\non the level of broad phonological properties, such as place of articulation\nor voicing. Using acoustic features and visual spectrograms extracted\nfrom native English utterances, we train several neural classifiers\nthat deduce articulatory properties from segments extracted from non-native\nEnglish utterances. Visual cues are thereby processed by convolutional\nneural networks, whereas acoustic cues are processed by recurrent neural\nnetworks.<br style=\"mso-data-placement:same-cell;\" /><br style=\"mso-data-placement:same-cell;\"\n/>\n  We show that combining both modalities increases performance over\nusing models in isolation, with important implications for user satisfaction.\nFurthermore, we test the impact of alignment quality on model performance\nby comparing results on manually corrected segments and force-aligned\nsegments, showing that the proposed pipeline can dispense with manual\ncorrection.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1677",
      "author_area_id": 1,
      "author_area_label": "Speech Perception, Production and Acquisition"
    },
    "foltz19_interspeech": {
      "authors": [
        [
          "Anouschka",
          "Foltz"
        ],
        [
          "Sarah",
          "Cooper"
        ],
        [
          "Tamsin M.",
          "McKelvey"
        ]
      ],
      "title": "Using Prosody to Discover Word Order Alternations in a Novel Language",
      "original": "1183",
      "page_count": 5,
      "order": 738,
      "p1": "3554",
      "pn": "3558",
      "abstract": [
        "Native speakers of a language can use prosodic phrasing to disambiguate\nsyntactically ambiguous sentences [1]. The current paper explores whether\nprosodic phrasing can help learners determine within-clause word order\ndifferences in a new language. Unlike many previous studies, we did\nnot train participants in an artificial language, but exploited word\norder differences that occur in German. Native English speakers with\nno knowledge of German were trained with simple main clause sentences\nas well as complex sentences containing a subordinate clause. During\ntraining, prosodic phrasing of complex sentences either aligned or\ndid not align with the sentences&#8217; clause structure. The results\nfrom two experiments showed that the non-aligned prosodic phrasing\nhelps learners discover clause internal word order differences in German,\nbut only if syntactic variability in the test sessions is low. Overall,\nthe results suggest that learners can exploit prosodic structure to\nlearn word order alternations in certain contexts.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1183",
      "author_area_id": 1,
      "author_area_label": "Speech Perception, Production and Acquisition"
    },
    "bradlow19b_interspeech": {
      "authors": [
        [
          "Ann R.",
          "Bradlow"
        ]
      ],
      "title": "Speaking Rate, Information Density, and Information Rate in First-Language and Second-Language Speech",
      "original": "1150",
      "page_count": 5,
      "order": 739,
      "p1": "3559",
      "pn": "3563",
      "abstract": [
        "Using a corpus of multilingual recordings of a standard text (the North\nWind and the Sun passage, NWS) in 11 languages, speaking rate (SR,\nsyllables/second) and information density (ID, number of syllables\nfor the NWS text) were examined in first-language (L1) and second-language\n(L2) speech. Replicating prior work, cross-language comparison of L1\nspeech showed a trade-off between SR and ID such that relatively low\ndensity languages (many syllables for the NWS text) tended to be produced\nat relatively fast rates, and vice versa. Furthermore, L2 English was\ncharacterized by both slower rate and lower ID than L1 English. That\nis, L2 English involved more syllables than L1 English for the same\nNWS text. A comparison of the number of acoustic syllables (i.e. amplitude\npeaks) with the number of orthographic syllables (i.e. dictionary-based\nsyllable counts for the NWS text) indicated that L1 speech involved\nsubstantial syllable reduction (fewer acoustic than orthographic syllables)\nwhile L2 speech involved substantial syllable epenthesis (more acoustic\nthan orthographic syllables). These findings suggest that L2 speech\nproduction involves temporal restructuring beyond increased segment,\nsyllable and word durations, and that the resultant information rate\n(information bits transmitted/second) of L2 speech diverges substantially\nfrom that of L1 speech.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1150",
      "author_area_id": 1,
      "author_area_label": "Speech Perception, Production and Acquisition"
    },
    "graham19_interspeech": {
      "authors": [
        [
          "Calbert",
          "Graham"
        ],
        [
          "Francis",
          "Nolan"
        ]
      ],
      "title": "Articulation Rate as a Metric in Spoken Language Assessment",
      "original": "2098",
      "page_count": 5,
      "order": 740,
      "p1": "3564",
      "pn": "3568",
      "abstract": [
        "Automated evaluation of non-native pronunciation provides a consistent\nand more cost-efficient alternative to human evaluation. To that end,\nthere is considerable interest in deriving metrics that are based on\nthe cues human listeners use to judge pronunciation. Previous research\nreported the use of phonetic features such as vowel characteristics\nin automated spoken language evaluation. The present study extends\nthis line of work on the significance of phonetic features in automated\nevaluation of L2 speech (both assessment and feedback). Predictive\nmodelling techniques examined the relationship between various articulation\nrate metrics one the one hand, and the proficiency and L1 background\nof non-native English speakers on the other. It was found that the\noptimal predictive model was one in which the phonetic details of phoneme\narticulation were factored in the analysis of articulation rate. Model\nperformance varied also according to the L1 background of speakers.\nThe implications for assessment and feedback are discussed.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2098",
      "author_area_id": 1,
      "author_area_label": "Speech Perception, Production and Acquisition"
    },
    "xu19c_interspeech": {
      "authors": [
        [
          "Haiyang",
          "Xu"
        ],
        [
          "Hui",
          "Zhang"
        ],
        [
          "Kun",
          "Han"
        ],
        [
          "Yun",
          "Wang"
        ],
        [
          "Yiping",
          "Peng"
        ],
        [
          "Xiangang",
          "Li"
        ]
      ],
      "title": "Learning Alignment for Multimodal Emotion Recognition from Speech",
      "original": "3247",
      "page_count": 5,
      "order": 741,
      "p1": "3569",
      "pn": "3573",
      "abstract": [
        "Speech emotion recognition is a challenging problem because human convey\nemotions in subtle and complex ways. For emotion recognition on human\nspeech, one can either extract emotion related features from audio\nsignals or employ speech recognition techniques to generate text from\nspeech and then apply natural language processing to analyze the sentiment.\nFurther, emotion recognition will be beneficial from using audio-textual\nmultimodal information, it is not trivial to build a system to learn\nfrom multimodality. One can build models for two input sources separately\nand combine them in a decision level, but this method ignores the interaction\nbetween speech and text in the temporal domain. In this paper, we propose\nto use an attention mechanism to learn the alignment between speech\nframes and text words, aiming to produce more accurate multimodal feature\nrepresentations. The aligned multimodal features are fed into a sequential\nmodel for emotion recognition. We evaluate the approach on the IEMOCAP\ndataset and the experimental results show the proposed approach achieves\nthe state-of-the-art performance on the dataset.\n"
      ],
      "doi": "10.21437/Interspeech.2019-3247",
      "author_area_id": 1,
      "author_area_label": "Speech Perception, Production and Acquisition"
    },
    "peperkamp19b_interspeech": {
      "authors": [
        [
          "Sharon",
          "Peperkamp"
        ],
        [
          "Monica",
          "Hegde"
        ],
        [
          "Maria Julia",
          "Carbajal"
        ]
      ],
      "title": "Liquid Deletion in French Child-Directed Speech",
      "original": "2838",
      "page_count": 5,
      "order": 742,
      "p1": "3574",
      "pn": "3578",
      "abstract": [
        "In spoken language, words can have different surface realizations due\nto the application of language-specific phonological rules. Young children\nmust acquire these rules in order to be able to undo their effects\nand recognize the intended words during language processing. Evidence\nso far suggests that they achieve this early on, but the learning mechanisms\nthat they exploit are unknown. As a first step in examining this question,\nit is necessary to know to what extent phonological rules occur in\ntheir input. Here, we investigate the occurrence of liquid deletion,\ni.e. the optional deletion of the liquid in word-final obstruent-liquid\nclusters, in French child-directed speech. Analyzing a corpus from\nthe Childes database that contains video recordings, we find that words\nfinishing in obstruent-liquid clusters occur on average once every\n13 utterances, and that in more than half of the cases the liquid is\ndeleted. As in adult-directed speech, deletion applies more often before\nconsonants than before vowels and pauses. Furthermore, pairs of tokens\nof the same word with and without deletion tend to cluster together,\nwith a median distance of 49 seconds of speech. This clustering could\nbe a powerful cue in the process of the acquisition of liquid deletion.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2838",
      "author_area_id": 1,
      "author_area_label": "Speech Perception, Production and Acquisition"
    },
    "seidl19_interspeech": {
      "authors": [
        [
          "Amanda",
          "Seidl"
        ],
        [
          "Anne S.",
          "Warlaumont"
        ],
        [
          "Alejandrina",
          "Cristia"
        ]
      ],
      "title": "Towards Detection of Canonical Babbling by Citizen Scientists: Performance as a Function of Clip Length",
      "original": "1773",
      "page_count": 5,
      "order": 743,
      "p1": "3579",
      "pn": "3583",
      "abstract": [
        "Theoretical, empirical, and intervention research requires access to\na large, unbiased, annotated dataset of infant vocalizations for training\nspeech technology to detect and differentiate consonant-vowel (canonical)\nsyllables in infants&#8217; vocalizations from less mature vocalizations.\nCitizen scientists could help us to achieve the goal of this dataset,\nif classification is accurate regardless of coders&#8217; native language\nand training and can be completed on clips short enough to avoid revealing\npersonal identifying information. Three groups of coders participated\nin an experiment: trained native, semi-trained native, and minimally-trained\nforeign. When vocalizations were presented whole, reliability was highest\nacross the trained coders, with little difference between the semi-trained\nand minimally-trained coders. Among minimally-trained coders, reliability\nfor 400ms-long clips was very similar to that found for full clips,\nwith lower values for 200 and 600ms clips. Finally, error rates were\nminimized when 400ms-long clips were used. In sum, minimally-trained\ncoders can achieve fairly reliable and accurate results, even when\ntheir native language does not match infants&#8217; target language\nand when provided with very short clips. Since shorter clips protect\nthe identity of the child and her family, this manner of data annotation\nmay provide us with a way of building a large, unbiased dataset of\ninfant vocalizations.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1773",
      "author_area_id": 1,
      "author_area_label": "Speech Perception, Production and Acquisition"
    },
    "ludusan19b_interspeech": {
      "authors": [
        [
          "Bogdan",
          "Ludusan"
        ],
        [
          "Annett",
          "Jorschick"
        ],
        [
          "Reiko",
          "Mazuka"
        ]
      ],
      "title": "Nasal Consonant Discrimination in Infant- and Adult-Directed Speech",
      "original": "1737",
      "page_count": 5,
      "order": 744,
      "p1": "3584",
      "pn": "3588",
      "abstract": [
        "Infant-directed speech (IDS) is thought to play a facilitating role\nin language acquisition, by simplifying the input infants receive.\nIn particular, the hypothesis that the acoustic level is enhanced to\nmake the input more clear for infants, has been extensively studied\nin the case of vowels, but less so in the case of consonants. An investigation\ninto how nasal consonants can be discriminated in infant- compared\nto adult-directed speech (ADS) was performed, on a corpus of Japanese\nmother-infant spontaneous conversations, by examining all bilabial\nand alveolar nasals occurring in intervocalic position. The Pearson\ncorrelation between corresponding spectrum slices of nasal consonants,\nin identical vowel contexts, was employed as similarity measure and\na statistical model was fit using this information. It revealed a decrease\nin similarity between the nasal classes, in IDS compared to ADS, although\nthe effect was not statistically significant. We confirmed these results,\nusing an unsupervised machine learning algorithm to discriminate between\nthe two nasal classes, obtaining similar classification performance\nin IDS and ADS. We discuss our findings in the context of the current\nliterature on infant-directed speech.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1737",
      "author_area_id": 1,
      "author_area_label": "Speech Perception, Production and Acquisition"
    },
    "marklund19_interspeech": {
      "authors": [
        [
          "Ellen",
          "Marklund"
        ],
        [
          "Johan",
          "Sjons"
        ],
        [
          "Lisa",
          "Gustavsson"
        ],
        [
          "El\u00edsabet Eir",
          "Cortes"
        ]
      ],
      "title": "No Distributional Learning in Adults from Attended Listening to Non-Speech",
      "original": "1674",
      "page_count": 5,
      "order": 745,
      "p1": "3589",
      "pn": "3593",
      "abstract": [
        "Distributional learning is a perceptual process hypothesized to underlie\nthe phenomena of phonetic recalibration and selective adaptation, as\nwell as infant speech sound category learning. However, in order to\nbe conclusively tied to the earliest stages of speech sound category\ndevelopment, that is, the formation of novel perceptual categories,\ndistributional learning must be shown to operate on stimuli for which\nthere are no pre-existing categories. We investigated this in a previous\nstudy, finding no evidence of distributional learning in adults from\nunattended listening to non-speech. Since attention to stimuli impacts\ndistributional learning, the present study focused on distributional\nlearning from attended listening to non-speech. The same paradigm was\nused as in the previous study, except that participants&#8217; attention\nwas directed towards stimuli by means of a cover task. Non-speech stimuli\nwere spectrally rotated vowels and the mismatch negativity was used\nto measure perceptual categorization. No distributional learning was\nfound, that is, no effect of attention on distributional learning was\ndemonstrated. This could mean that the distributional learning process\ndoes not operate on stimuli where perceptual categories do not already\nexist, or that the mismatch negativity measure does not capture the\nearliest stages of perceptual category development.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1674",
      "author_area_id": 1,
      "author_area_label": "Speech Perception, Production and Acquisition"
    },
    "rasanen19_interspeech": {
      "authors": [
        [
          "Okko",
          "R\u00e4s\u00e4nen"
        ],
        [
          "Khazar",
          "Khorrami"
        ]
      ],
      "title": "A Computational Model of Early Language Acquisition from Audiovisual Experiences of Young Infants",
      "original": "1523",
      "page_count": 5,
      "order": 746,
      "p1": "3594",
      "pn": "3598",
      "abstract": [
        "Earlier research has suggested that human infants might use statistical\ndependencies between speech and non-linguistic multimodal input to\nbootstrap their language learning before they know how to segment words\nfrom running speech. However, feasibility of this hypothesis in terms\nof real-world infant experiences has remained unclear. This paper presents\na step towards a more realistic test of the multimodal bootstrapping\nhypothesis by describing a neural network model that can learn word\nsegments and their meanings from referentially ambiguous acoustic input.\nThe model is tested on recordings of real infant-caregiver interactions\nusing utterance-level labels for concrete visual objects that were\nattended by the infant when caregiver spoke an utterance containing\nthe name of the object, and using random visual labels for utterances\nduring absence of attention. The results show that beginnings of lexical\nknowledge may indeed emerge from individually ambiguous learning scenarios.\nIn addition, the hidden layers of the network show gradually increasing\nselectivity to phonetic categories as a function of layer depth, resembling\nmodels trained for phone recognition in a supervised manner.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1523",
      "author_area_id": 1,
      "author_area_label": "Speech Perception, Production and Acquisition"
    },
    "du19b_interspeech": {
      "authors": [
        [
          "Dan",
          "Du"
        ],
        [
          "Jinsong",
          "Zhang"
        ]
      ],
      "title": "The Production of Chinese Affricates /ts/ and /ts<SUP>h</SUP>/ by Native Urdu Speakers",
      "original": "1638",
      "page_count": 5,
      "order": 747,
      "p1": "3599",
      "pn": "3603",
      "abstract": [
        "Previous studies have shown that learners with different native language\nbackgrounds have common difficulties in learning Chinese affricates\nbut demonstrate in various patterns. While few studies investigated\nthis issue of native Urdu speakers. To address the production of Chinese\naffricates /ts/ and /ts<SUP>h</SUP>/ by native Urdu speakers, speech\nmaterials, produced by two groups of subjects with different Chinese\nproficiency, were selected from the BLCU-SAIT speech corpus. The error\nrate and error types of their production of Chinese affricates /ts/\nand /ts<SUP>h</SUP>/ have been discussed after transcription and data\nanalysis. The results show that though there are no counterparts of\nChinese affricates /ts/ and /ts<SUP>h</SUP>/ in Urdu, the error and\nthe acquisition pattern of these two affricates, to some extent, affected\nby individual differences of their roles in Urdu except universal similarities\nand differences between two languages. The findings of this study shed\nsome light on second language learning and teaching.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1638"
    },
    "li19s_interspeech": {
      "authors": [
        [
          "Xinyu",
          "Li"
        ],
        [
          "Venkata",
          "Chebiyyam"
        ],
        [
          "Katrin",
          "Kirchhoff"
        ]
      ],
      "title": "Multi-Stream Network with Temporal Attention for Environmental Sound Classification",
      "original": "3019",
      "page_count": 5,
      "order": 748,
      "p1": "3604",
      "pn": "3608",
      "abstract": [
        "Environmental sound classification systems often do not perform robustly\nacross different sound classification tasks and audio signals of varying\ntemporal structures. We introduce a multi-stream convolutional neural\nnetwork with temporal attention that addresses these problems. The\nnetwork relies on three input streams consisting of raw audio and spectral\nfeatures and utilizes a temporal attention function computed from energy\nchanges over time. Training and classification utilizes decision fusion\nand data augmentation techniques that incorporate uncertainty. We evaluate\nthis network on three commonly used datasets for environmental sound\nand audio scene classification and achieve new state-of-the-art performance\nwithout any changes in network architecture or front-end preprocessing,\nthus demonstrating better generalizability.\n"
      ],
      "doi": "10.21437/Interspeech.2019-3019",
      "author_area_id": 5,
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "cerutti19_interspeech": {
      "authors": [
        [
          "Gianmarco",
          "Cerutti"
        ],
        [
          "Rahul",
          "Prasad"
        ],
        [
          "Alessio",
          "Brutti"
        ],
        [
          "Elisabetta",
          "Farella"
        ]
      ],
      "title": "Neural Network Distillation on IoT Platforms for Sound Event Detection",
      "original": "2394",
      "page_count": 5,
      "order": 749,
      "p1": "3609",
      "pn": "3613",
      "abstract": [
        "In most classification tasks, wide and deep neural networks perform\nand generalize better than their smaller counterparts, in particular\nwhen they are exposed to large and heterogeneous training sets. However,\nin the emerging field of Internet of Things memory footprint and energy\nbudget pose severe limits on the size and complexity of the neural\nmodels that can be implemented on embedded devices. The Student-Teacher\napproach is an attractive strategy to distill knowledge from a large\nnetwork into smaller ones, that can fit on low-energy low-complexity\nembedded IoT platforms. In this paper, we consider the outdoor sound\nevent detection task as a use case. Building upon the VGGish network,\nwe investigate different distillation strategies to substantially reduce\nthe classifier&#8217;s size and computational cost with minimal performance\nlosses. Experiments on the  UrbanSound8K dataset show that extreme\ncompression factors (up to 4.2 &#8226; 10<SUP>-4</SUP> for parameters\nand 1.2 &#8226; 10<SUP>-3</SUP> for operations with respect to VGGish)\ncan be achieved, limiting the accuracy degradation from 75% to 70%.\nFinally, we compare different embedded platforms to analyze the trade-off\nbetween available resources and achievable accuracy.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2394",
      "author_area_id": 5,
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "lu19d_interspeech": {
      "authors": [
        [
          "Xugang",
          "Lu"
        ],
        [
          "Peng",
          "Shen"
        ],
        [
          "Sheng",
          "Li"
        ],
        [
          "Yu",
          "Tsao"
        ],
        [
          "Hisashi",
          "Kawai"
        ]
      ],
      "title": "Class-Wise Centroid Distance Metric Learning for Acoustic Event Detection",
      "original": "2271",
      "page_count": 5,
      "order": 750,
      "p1": "3614",
      "pn": "3618",
      "abstract": [
        "Designing good feature extraction and classifier models is essential\nfor obtaining high performances of acoustic event detection (AED) systems.\nCurrent state-of-the-art algorithms are based on deep neural network\nmodels that jointly learn the feature representation and classifier\nmodels. As a typical pipeline in these algorithms, several network\nlayers with nonlinear transforms are stacked for feature extraction,\nand a classifier layer with a softmax transform is applied on top of\nthese extracted features to obtain normalized probability outputs.\nThis pipeline is directly connected to a final goal for class discrimination\nwithout explicitly considering how the features should be distributed\nfor inter-class and intra-class samples. In this paper, we explicitly\nadd a distance metric constraint on feature extraction process with\na goal to reduce intra-class sample distances and increase inter-class\nsample distances. Rather than estimating the pair-wise distances of\nsamples, the distances are efficiently calculated between samples and\nclass cluster centroids. With this constraint, the learned features\nhave a good property for improving the generalization of the classification\nmodels. AED experiments on an urban sound classification task were\ncarried out to test the algorithm. Results showed that the proposed\nalgorithm efficiently improved the performance on the current state-of-the-art\ndeep learning algorithms.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2271",
      "author_area_id": 5,
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "bai19b_interspeech": {
      "authors": [
        [
          "Xue",
          "Bai"
        ],
        [
          "Jun",
          "Du"
        ],
        [
          "Zi-Rui",
          "Wang"
        ],
        [
          "Chin-Hui",
          "Lee"
        ]
      ],
      "title": "A Hybrid Approach to Acoustic Scene Classification Based on Universal Acoustic Models",
      "original": "2171",
      "page_count": 5,
      "order": 751,
      "p1": "3619",
      "pn": "3623",
      "abstract": [
        "For the acoustic scenes classification, the main challenge is distinguishing\nsimilar acoustic segments between different scenes. To solve this problem,\nmany deep learning based approaches have been proposed without considering\nthe relevance of different acoustic scenes. In this paper, we propose\na novel acoustic segment model (ASM) for acoustic scene classification.\nASM aims at giving finer segmentation and covering all acoustic scenes\nthrough searching for the underlying phoneme like acoustic units. Furthermore,\nacoustic segments are modeled by Hidden Markov Models (HMMs) and each\naudio is decoded into ASM sequences without prior linguistic knowledge.\nSimilar to the term vector of a text document, these ASM sequences\nare converted into co-occurrence statistics feature vectors and SVM/DNN\nis used as classifier back-end. Validated on the DCASE 2018 task, the\nproposed approach can achieve a competitive performance with single\nmodel and no data augment. By using visualization analysis, we excavate\nthe potential similar units hidden in auditory sense.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2171",
      "author_area_id": 5,
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "he19b_interspeech": {
      "authors": [
        [
          "Ke-Xin",
          "He"
        ],
        [
          "Yu-Han",
          "Shen"
        ],
        [
          "Wei-Qiang",
          "Zhang"
        ]
      ],
      "title": "Hierarchical Pooling Structure for Weakly Labeled Sound Event Detection",
      "original": "2049",
      "page_count": 5,
      "order": 752,
      "p1": "3624",
      "pn": "3628",
      "abstract": [
        "Sound event detection with weakly labeled data is considered as a problem\nof multi-instance learning. And the choice of pooling function is the\nkey to solving this problem. In this paper, we proposed a hierarchical\npooling structure to improve the performance of weakly labeled sound\nevent detection system. Proposed pooling structure has made remarkable\nimprovements on three types of pooling function without adding any\nparameters. Moreover, our system has achieved competitive performance\non Task 4 of Detection and Classification of Acoustic Scenes and Events\n(DCASE) 2017 Challenge using hierarchical pooling structure.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2049",
      "author_area_id": 5,
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "xia19_interspeech": {
      "authors": [
        [
          "Wei",
          "Xia"
        ],
        [
          "Kazuhito",
          "Koishida"
        ]
      ],
      "title": "Sound Event Detection in Multichannel Audio Using Convolutional Time-Frequency-Channel Squeeze and Excitation",
      "original": "1860",
      "page_count": 5,
      "order": 753,
      "p1": "3629",
      "pn": "3633",
      "abstract": [
        "In this study, we introduce a convolutional time-frequency-channel\n&#8220;Squeeze and Excitation&#8221; (tfc-SE) module to explicitly\nmodel inter-dependencies between the time-frequency domain and multiple\nchannels. The tfc-SE module consists of two parts: tf-SE block and\nc-SE block which are designed to provide attention on time-frequency\nand channel domain, respectively, for adaptively recalibrating the\ninput feature map. The proposed tfc-SE module, together with a popular\nConvolutional Recurrent Neural Network (CRNN) model, are evaluated\non a multi-channel sound event detection task with overlapping audio\nsources: the training and test data are synthesized TUT Sound Events\n2018 datasets, recorded with microphone arrays. We show that the tfc-SE\nmodule can be incorporated into the CRNN model at a small additional\ncomputational cost and bring significant improvements on sound event\ndetection accuracy. We also perform detailed ablation studies by analyzing\nvarious factors that may influence the performance of the SE blocks.\nWe show that with the best tfc-SE block, error rate (ER) decreases\nfrom 0.2538 to 0.2026, relative 20.17% reduction of ER, and 5.72% improvement\nof F1 score. The results indicate that the learned acoustic embeddings\nwith the tfc-SE module efficiently strengthen time-frequency and channel-wise\nfeature representations to improve the discriminative performance.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1860",
      "author_area_id": 5,
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "pham19b_interspeech": {
      "authors": [
        [
          "Lam",
          "Pham"
        ],
        [
          "Ian",
          "McLoughlin"
        ],
        [
          "Huy",
          "Phan"
        ],
        [
          "Ramaswamy",
          "Palaniappan"
        ]
      ],
      "title": "A Robust Framework for Acoustic Scene Classification",
      "original": "1841",
      "page_count": 5,
      "order": 754,
      "p1": "3634",
      "pn": "3638",
      "abstract": [
        "Acoustic scene classification (ASC) using front-end time-frequency\nfeatures and back-end neural network classifiers has demonstrated good\nperformance in recent years. However a profusion of systems has arisen\nto suit different tasks and datasets, utilising different feature and\nclassifier types. This paper aims at a robust framework that can explore\nand utilise a range of different time-frequency features and neural\nnetworks, either singly or merged, to achieve good classification performance.\nIn particular, we exploit three different types of front-end time-frequency\nfeature; log energy Mel filter, Gammatone filter and constant Q transform.\nAt the back-end we evaluate effective a two-stage model that exploits\na Convolutional Neural Network for pre-trained feature extraction,\nfollowed by Deep Neural Network classifiers as a post-trained feature\nadaptation model and classifier. We also explore the use of a data\naugmentation technique for these features that effectively generates\na variety of intermediate data, reinforcing model learning abilities,\nparticularly for marginal cases. We assess performance on the DCASE2016\ndataset, demonstrating good classification accuracies exceeding 90%,\nsignificantly outperforming the DCASE2016 baseline and highly competitive\ncompared to state-of-the-art systems.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1841",
      "author_area_id": 5,
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "shi19c_interspeech": {
      "authors": [
        [
          "Bowen",
          "Shi"
        ],
        [
          "Ming",
          "Sun"
        ],
        [
          "Chieh-Chi",
          "Kao"
        ],
        [
          "Viktor",
          "Rozgic"
        ],
        [
          "Spyros",
          "Matsoukas"
        ],
        [
          "Chao",
          "Wang"
        ]
      ],
      "title": "Compression of Acoustic Event Detection Models with Quantized Distillation",
      "original": "1747",
      "page_count": 5,
      "order": 755,
      "p1": "3639",
      "pn": "3643",
      "abstract": [
        "Acoustic Event Detection (AED), aiming at detecting categories of events\nbased on audio signals, has found application in many intelligent systems.\nRecently deep neural network significantly advances this field and\nreduces detection errors to a large scale. However how to efficiently\nexecute deep models in AED has received much less attention. Meanwhile\nstate-of-the-art AED models are based on large deep models, which are\ncomputational demanding and challenging to deploy on devices with constrained\ncomputational resources. In this paper, we present a simple yet effective\ncompression approach which jointly leverages  knowledge distillation\nand  quantization to compress larger network (teacher model) into compact\nnetwork (student model). Experimental results show proposed technique\nnot only lowers error rate of original compact network by 15% through\ndistillation but also further reduces its model size to a large extent\n(2% of teacher, 12% of full-precision student) through quantization.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1747",
      "author_area_id": 5,
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "chen19m_interspeech": {
      "authors": [
        [
          "Jiaxu",
          "Chen"
        ],
        [
          "Jing",
          "Hao"
        ],
        [
          "Kai",
          "Chen"
        ],
        [
          "Di",
          "Xie"
        ],
        [
          "Shicai",
          "Yang"
        ],
        [
          "Shiliang",
          "Pu"
        ]
      ],
      "title": "An End-to-End Audio Classification System Based on Raw Waveforms and Mix-Training Strategy",
      "original": "1579",
      "page_count": 5,
      "order": 756,
      "p1": "3644",
      "pn": "3648",
      "abstract": [
        "Audio classification can distinguish different kinds of sounds, which\nis helpful for intelligent applications in daily life. However, it\nremains a challenging task since the sound events in an audio clip\nis probably multiple, even overlapping. This paper introduces an end-to-end\naudio classification system based on raw waveforms and mix-training\nstrategy. Compared to human-designed features which have been widely\nused in existing research, raw waveforms contain more complete information\nand are more appropriate for multi-label classification. Taking raw\nwaveforms as input, our network consists of two variants of ResNet\nstructure which can learn a discriminative representation. To explore\nthe information in intermediate layers, a multi-level prediction with\nattention structure is applied in our model. Furthermore, we design\na mix-training strategy to break the performance limitation caused\nby the amount of training data. Experiments show that the mean average\nprecision of the proposed audio classification system on Audio Set\ndataset is 37.2%. Without using extra training data, our system exceeds\nthe state-of-the-art multi-level attention model.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1579",
      "author_area_id": 5,
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "zhang19k_interspeech": {
      "authors": [
        [
          "Shilei",
          "Zhang"
        ],
        [
          "Yong",
          "Qin"
        ],
        [
          "Kewei",
          "Sun"
        ],
        [
          "Yonghua",
          "Lin"
        ]
      ],
      "title": "Few-Shot Audio Classification with Attentional Graph Neural Networks",
      "original": "1532",
      "page_count": 5,
      "order": 757,
      "p1": "3649",
      "pn": "3653",
      "abstract": [
        "Few-shot learning is a very promising and challenging field of machine\nlearning as it aims to understand new concepts from very few labeled\nexamples. In this paper, we propose attentional framework to extend\nrecently proposed few-shot learning with graph neural network [1] in\naudio classification scenario. The objective of proposed attentional\nframework is to introduce a flexible framework to implement selectively\nconcentration procedure on support examples for each query process.\nwe also present an empirical study on confidence measure for few-shot\nlearning application by combining posterior probability with normalized\nentropy of the network&#8217;s probability output. The efficiency of\nthe proposed method is demonstrated with experiments on balanced training\nset of Audio set for training and a 5-way test set composed of about\n5-hour audio data for testing.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1532",
      "author_area_id": 5,
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "lu19e_interspeech": {
      "authors": [
        [
          "Kangkang",
          "Lu"
        ],
        [
          "Chuan-Sheng",
          "Foo"
        ],
        [
          "Kah Kuan",
          "Teh"
        ],
        [
          "Huy Dat",
          "Tran"
        ],
        [
          "Vijay Ramaseshan",
          "Chandrasekhar"
        ]
      ],
      "title": "Semi-Supervised Audio Classification with Consistency-Based Regularization",
      "original": "1231",
      "page_count": 5,
      "order": 758,
      "p1": "3654",
      "pn": "3658",
      "abstract": [
        "Consistency-based semi-supervised learning methods such as the Mean\nTeacher method are state-of-the-art on image datasets, but have yet\nto be applied to audio data. Such methods encourage model predictions\nto be consistent on perturbed input data. In this paper, we incorporate\naudio-specific perturbations into the Mean Teacher algorithm and demonstrate\nthe effectiveness of the resulting method on audio classification tasks.\nSpecifically, we perturb audio inputs by mixing in other environmental\naudio clips, and leverage other training examples as sources of noise.\nExperiments on the Google Speech Command Dataset and UrbanSound8K Dataset\nshow that the method can achieve comparable performance to a purely\nsupervised approach while using only a fraction of the labels.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1231",
      "author_area_id": 5,
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "mizgajski19_interspeech": {
      "authors": [
        [
          "Jan",
          "Mizgajski"
        ],
        [
          "Adrian",
          "Szymczak"
        ],
        [
          "Robert",
          "G\u0142owski"
        ],
        [
          "Piotr",
          "Szyma\u0144ski"
        ],
        [
          "Piotr",
          "\u017belasko"
        ],
        [
          "\u0141ukasz",
          "Augustyniak"
        ],
        [
          "Miko\u0142aj",
          "Morzy"
        ],
        [
          "Yishay",
          "Carmiel"
        ],
        [
          "Jeff",
          "Hodson"
        ],
        [
          "\u0141ukasz",
          "W\u00f3jciak"
        ],
        [
          "Daniel",
          "Smoczyk"
        ],
        [
          "Adam",
          "Wr\u00f3bel"
        ],
        [
          "Bartosz",
          "Borowik"
        ],
        [
          "Adam",
          "Artajew"
        ],
        [
          "Marcin",
          "Baran"
        ],
        [
          "Cezary",
          "Kwiatkowski"
        ],
        [
          "Marzena",
          "\u017by\u0142a-Hoppe"
        ]
      ],
      "title": "Avaya Conversational Intelligence: A Real-Time System for Spoken Language Understanding in Human-Human Call Center Conversations",
      "original": "8002",
      "page_count": 2,
      "order": 759,
      "p1": "3659",
      "pn": "3660",
      "abstract": [
        "Avaya Conversational Intelligence (ACI) is an end-to-end, cloud-based\nsolution for real-time Spoken Language Understanding for call centers.\nIt combines large vocabulary, real-time speech recognition, transcript\nrefinement, and entity and intent recognition in order to convert live\naudio into a rich, actionable stream of structured events. These events\ncan be further leveraged with a business rules engine, thus serving\nas a foundation for real-time supervision and assistance applications.\nAfter the ingestion, calls are enriched with unsupervised keyword extraction,\nabstractive summarization, and business-defined attributes, enabling\noffline use cases, such as business intelligence, topic mining, full-text\nsearch, quality assurance, and agent training. ACI comes with a pretrained,\nconfigurable library of hundreds of intents and a robust intent training\nenvironment that allows for efficient, cost-effective creation and\ncustomization of customer-specific intents.\n"
      ]
    },
    "an19b_interspeech": {
      "authors": [
        [
          "Shounan",
          "An"
        ],
        [
          "Youngsoo",
          "Kim"
        ],
        [
          "Hu",
          "Xu"
        ],
        [
          "Jinwoo",
          "Lee"
        ],
        [
          "Myungwoo",
          "Lee"
        ],
        [
          "Insoo",
          "Oh"
        ]
      ],
      "title": "Robust Keyword Spotting via Recycle-Pooling for Mobile Game",
      "original": "8010",
      "page_count": 2,
      "order": 760,
      "p1": "3661",
      "pn": "3662",
      "abstract": [
        "We present an effective method to solve a small-footprint keyword spotting\n(KWS) task via deep neural network for mobile game. Our goal is to\nimprove the accuracy of KWS in various environments. To this end, we\npropose a new neural network layer named recycle-pooling. Extensive\nexperiments indicate that our recycle-pooling based convolutional neural\nnetwork (RP-CNN) indeed improves the performance of KWS in both clean\nand noisy data for mobile game. We will perform live demonstration\nof RP-CNN based KWS integrated into a full-sized, production-quality\nmobile game  A3: Still Alive, which is one of the major games from\nNetmarble this year and will be available on market soon.\n"
      ]
    },
    "chylek19_interspeech": {
      "authors": [
        [
          "Adam",
          "Ch\u00fdlek"
        ],
        [
          "Lubo\u0161",
          "\u0160m\u00eddl"
        ],
        [
          "Jan",
          "\u0160vec"
        ]
      ],
      "title": "Multimodal Dialog with the MALACH Audiovisual Archive",
      "original": "8011",
      "page_count": 2,
      "order": 761,
      "p1": "3663",
      "pn": "3664",
      "abstract": [
        "In this paper, we present a multimodal dialog system capable of information\nretrieval from the large audiovisual archive MALACH of Holocaust testimonies.\nThe users can use spoken natural language queries to search the archive.\nA graphical user interface allows the users to quickly view footage\nwith the answers and explore their context. The dialog was deployed\nin two languages &#8212; English and Czech. The system uses automatic\nspeech recognition and natural language processing for knowledge base\nconstruction and for processing of the user&#8217;s input.\n"
      ]
    },
    "jelil19_interspeech": {
      "authors": [
        [
          "Sarfaraz",
          "Jelil"
        ],
        [
          "Abhishek",
          "Shrivastava"
        ],
        [
          "Rohan Kumar",
          "Das"
        ],
        [
          "S.R. Mahadeva",
          "Prasanna"
        ],
        [
          "Rohit",
          "Sinha"
        ]
      ],
      "title": "SpeechMarker: A Voice Based Multi-Level Attendance Application",
      "original": "8014",
      "page_count": 2,
      "order": 762,
      "p1": "3665",
      "pn": "3666",
      "abstract": [
        "This work describes a multi-level speaker verification (SV) framework\nthat is accessible via a graphical user interface (GUI) with attendance\nas an application. This framework has three different modalities of\nSV system, namely, voice-password, text-dependent and text-independent.\nThe decision for attendance marking can be taken from each of the modalities\nor by fusion. There are two operating modes of the developed GUI, which\nare user and debug modes. The user mode is for general users to mark\nattendance, whereas the debug mode is to study the behavior of the\nthree modalities from deployment point of view. The speech waveforms,\ndifferent plots and scores can be analyzed in the debug mode for analysis.\nThe system has been deployed successfully for regular attendance marking\namong a closed group in a laboratory environment.\n"
      ]
    },
    "wu19h_interspeech": {
      "authors": [
        [
          "Jibin",
          "Wu"
        ],
        [
          "Zihan",
          "Pan"
        ],
        [
          "Malu",
          "Zhang"
        ],
        [
          "Rohan Kumar",
          "Das"
        ],
        [
          "Yansong",
          "Chua"
        ],
        [
          "Haizhou",
          "Li"
        ]
      ],
      "title": "Robust Sound Recognition: A Neuromorphic Approach",
      "original": "8032",
      "page_count": 2,
      "order": 763,
      "p1": "3667",
      "pn": "3668",
      "abstract": [
        "Humans perform remarkably well at sound classification that is used\nas cues to support high-level cognitive functions. Inspired by the\nanatomical structure of human cochlea and auditory attention mechanism,\nwe present a novel neuromorphic sound recognition system that integrates\nan event-driven auditory front-end and a biologically plausible spiking\nneural network classifier (SNN) for robust sound and speech recognition.\nDue to its event-driven nature, the SNN classifier is several orders\nof magnitude more energy efficient than deep learning classifier, therefore,\nit is suitable for many applications in wearable devices.\n"
      ]
    },
    "hu19c_interspeech": {
      "authors": [
        [
          "Shoukang",
          "Hu"
        ],
        [
          "Shansong",
          "Liu"
        ],
        [
          "Heng Fai",
          "Chang"
        ],
        [
          "Mengzhe",
          "Geng"
        ],
        [
          "Jiani",
          "Chen"
        ],
        [
          "Lau Wing",
          "Chung"
        ],
        [
          "To Ka",
          "Hei"
        ],
        [
          "Jianwei",
          "Yu"
        ],
        [
          "Ka Ho",
          "Wong"
        ],
        [
          "Xunying",
          "Liu"
        ],
        [
          "Helen",
          "Meng"
        ]
      ],
      "title": "The CUHK Dysarthric Speech Recognition Systems for English and Cantonese",
      "original": "8047",
      "page_count": 2,
      "order": 764,
      "p1": "3669",
      "pn": "3670",
      "abstract": [
        "Speech disorders affect many people around the world and introduce\na negative impact on their quality of life. Dysarthria is a neural-motor\nspeech disorder that obstructs the normal production of speech. Current\nautomatic speech recognition (ASR) systems are developed for normal\nspeech. They are not suitable for accurate recognition of disordered\nspeech. To the best of our knowledge, the majority of disordered speech\nrecognition systems developed to date are for English. In this paper,\nwe present two disordered speech recognition systems for both English\nand Cantonese. Both systems demonstrate competitive performance when\ncompared with the Google speech recognition API and human recognition\nresults.\n"
      ]
    },
    "schiel19_interspeech": {
      "authors": [
        [
          "Florian",
          "Schiel"
        ],
        [
          "Thomas",
          "Kisler"
        ]
      ],
      "title": "BAS Web Services for Automatic Subtitle Creation and Anonymization",
      "original": "8001",
      "page_count": 2,
      "order": 765,
      "p1": "3671",
      "pn": "3672",
      "abstract": [
        "In this Show&amp;Tell contribution we will demonstrate two new public\nweb services provided by the CLARIN centre Bavarian Archive for Speech\nSignals at the university of Munich. &#8216;Subtitle&#8217; is a service\nthat allows users to automatically create and add a subtitle track\nto video recordings; &#8216;Anonymizer&#8217; can be applied to media\nfiles and their respective annotations in order to mask user-defined\nspoken terms in the signal as well as in the annotation. Both services\nare accessible via a RESTful API or a user-friendly web-interface.\nIn the demo we will demonstrate both services independently and in\ncombination (anonymizing subtitles) using the web interface.\n"
      ]
    },
    "voe19_interspeech": {
      "authors": [
        [
          "Jana",
          "Vo\u00dfe"
        ],
        [
          "Petra",
          "Wagner"
        ]
      ],
      "title": "A User-Friendly and Adaptable Re-Implementation of an Acoustic Prominence Detection and Annotation Tool",
      "original": "8015",
      "page_count": 2,
      "order": 766,
      "p1": "3673",
      "pn": "3674",
      "abstract": [
        "Annotating prominence phenomena in speech corpora is a challenging\ntask, as it requires many resources. Therefore, several approaches\nhave emerged in the past decades to automatise the process of detecting\nand annotating prominence. Among these, [1] propose a fully automatically\noperating acoustic prominence detection and annotation tool that yields\npromising results. The present work aims at making this tool accessible\nto a broader community and more inviting in the manipulation of features.\nTo do so, we re-implemented the prominence annotation approach of [1]\nin the programming language of the software Praat [2], which is commonly\nused for speech analysis purposes within several areas of research.\nBy implementing a user-friendly interface, the Praat-based prominence\ndetection and annotation tool can be controlled without any source\ncode interaction, which makes it accessible to users with differing\nlevels of programming experience. More experienced users have the option\nto directly work with the comprehensively commented and documented\nsource code to manipulate or add features within the prominence detection\nand annotation process. Providing a more accessible and easier to manipulate\nre-implementation of the tool of [1], we want to contribute to further\ndevelopments in the area of automatic prominence detection and annotation.\n"
      ]
    },
    "dominguez19_interspeech": {
      "authors": [
        [
          "M\u00f3nica",
          "Dom\u00ednguez"
        ],
        [
          "Patrick Louis",
          "Rohrer"
        ],
        [
          "Juan",
          "Soler-Company"
        ]
      ],
      "title": "PyToBI: A Toolkit for ToBI Labeling Under Python",
      "original": "8021",
      "page_count": 2,
      "order": 767,
      "p1": "3675",
      "pn": "3676",
      "abstract": [
        "PyToBI is introduced as a user-friendly toolkit for the automatic annotation\nof intonation contours using the Tones and Breaks Indexes convention,\nknown as ToBI.\n"
      ]
    },
    "levy19_interspeech": {
      "authors": [
        [
          "Golan",
          "Levy"
        ],
        [
          "Raquel",
          "Sitman"
        ],
        [
          "Ido",
          "Amir"
        ],
        [
          "Eduard",
          "Golshtein"
        ],
        [
          "Ran",
          "Mochary"
        ],
        [
          "Eilon",
          "Reshef"
        ],
        [
          "Roi",
          "Reichart"
        ],
        [
          "Omri",
          "Allouche"
        ]
      ],
      "title": "GECKO &#8212; A Tool for Effective Annotation of Human Conversations",
      "original": "8025",
      "page_count": 2,
      "order": 768,
      "p1": "3677",
      "pn": "3678",
      "abstract": [
        "With the dramatic improvement in automated speech recognition (ASR)\naccuracy, a variety of machine learning (ML) and natural language processing\n(NLP) algorithms are designed for human conversation data. Supervised\nmachine learning and particularly deep neural networks (DNNs) require\nlarge annotated datasets in order to train high quality models. In\nthis paper we describe Gecko, a tool for annotation of speech and language\nfeatures of conversations. Gecko allows efficient and effective segmentation\nof the voice signal by speaker as well as annotation of the linguistic\ncontent of the conversation. A key feature of Gecko is the presentation\nof the output of automatic segmentation and transcription systems in\nan intuitive user interface for editing. Gecko allows annotation of\nVoice Activity Detection (VAD), Diarization, Speaker Identification\nand ASR outputs on a large scale. Both annotators and data scientists\nhave reported improvement in the speed and accuracy of work. Gecko\nis publicly available for the benefit of the community at https://github.com/gong-io/gecko.\n"
      ]
    },
    "lo19b_interspeech": {
      "authors": [
        [
          "Roger Yu-Hsiang",
          "Lo"
        ],
        [
          "Kathleen Currie",
          "Hall"
        ]
      ],
      "title": "SLP-AA: Tools for Sign Language Phonetic and Phonological Research",
      "original": "8028",
      "page_count": 2,
      "order": 769,
      "p1": "3679",
      "pn": "3680",
      "abstract": [
        "This paper describes the features of a free, open-source software tool,\n Sign Language Phonetic Annotator+Analyzer (SLP-AA), which is designed\nto facilitate phonetic/phonological transcription and analysis on sign\nlanguages.<br style=\"mso-data-placement:same-cell;\" /><br style=\"mso-data-placement:same-cell;\"\n/>\n  The software supports two modes: the Annotator mode allows the\nuser to build phonetically transcribed corpora of sign languages, and\nthe Analyzer mode lets the user perform phonological searches or analyses\non the built corpora. We give a detailed description of one type of\nphonological search &#8212; the extended finger search function &#8212;\nand point out a potential application of this function with respect\nto sign language research.\n"
      ]
    },
    "li19t_interspeech": {
      "authors": [
        [
          "Xinjian",
          "Li"
        ],
        [
          "Zhong",
          "Zhou"
        ],
        [
          "Siddharth",
          "Dalmia"
        ],
        [
          "Alan W.",
          "Black"
        ],
        [
          "Florian",
          "Metze"
        ]
      ],
      "title": "SANTLR: Speech Annotation Toolkit for Low Resource Languages",
      "original": "8040",
      "page_count": 2,
      "order": 770,
      "p1": "3681",
      "pn": "3682",
      "abstract": [
        "While low resource speech recognition has attracted a lot of attention\nfrom the speech community, there are a few tools available to facilitate\nlow resource speech collection. In this work, we present SANTLR: Speech\nAnnotation Toolkit for Low Resource Languages. It is a web-based toolkit\nwhich allows researchers to easily collect and annotate a corpus of\nspeech in a low resource language. Annotators may use this toolkit\nfor two purposes: transcription or recording. In transcription, annotators\nwould transcribe audio files provided by the researchers; in recording,\nannotators would record their voice by reading provided texts. We highlight\ntwo properties of this toolkit. First, SANTLR has a very user-friendly\nUser Interface (UI). Both researchers and annotators may use this simple\nweb interface to interact. There is no requirement for the annotators\nto have any expertise in audio or text processing. The toolkit would\nhandle all preprocessing and postprocessing steps. Second, we employ\na multi-step ranking mechanism facilitate the annotation process. In\nparticular, the toolkit would give higher priority to utterances which\nare easier to annotate and are more beneficial to achieving the goal\nof the annotation, e.g. quickly training an acoustic model.\n"
      ]
    },
    "gruber19_interspeech": {
      "authors": [
        [
          "Martin",
          "Gr\u016fber"
        ],
        [
          "Jakub",
          "V\u00edt"
        ],
        [
          "Jind\u0159ich",
          "Matou\u0161ek"
        ]
      ],
      "title": "Web-Based Speech Synthesis Editor",
      "original": "8013",
      "page_count": 2,
      "order": 771,
      "p1": "3683",
      "pn": "3684",
      "abstract": [
        "This paper presents a web-based GUI frontend for a backend TTS system,\nincluding an editor of the synthesized speech. The tool allows synthesizing\nspeech from general texts using all available synthesis methods with\nboth modifications within the speech synthesis process and subsequent\nmodifications of the synthesized speech targeting for instance speech\nprolongation, shortening, pitch or volume increasing or decreasing,\netc.\n"
      ]
    },
    "perrotin19_interspeech": {
      "authors": [
        [
          "Olivier",
          "Perrotin"
        ],
        [
          "Ian",
          "McLoughlin"
        ]
      ],
      "title": " GFM-Voc: A Real-Time Voice Quality Modification System",
      "original": "8018",
      "page_count": 2,
      "order": 772,
      "p1": "3685",
      "pn": "3686",
      "abstract": [
        "This article introduces  GFM-Voc, a new system that allows high-quality\nand real-time voice modification, including both vocalic formants shifting,\nand voice quality manipulation. In particular, the system is based\non the implementation of a newly developed source-filter decomposition\nmethod, called GFM-IAIF, that allows the extraction of both vocal tract\nand glottis spectral envelopes as a compact set of filter parameters.\nThe latter are then controllable through a GUI, before re-synthesis\nof the speech with the modified parameters. The system requires no\ntraining, and operates on any voice, male or female, without tuning.\nGiven the close link between spectral parameters and speech perception,\nthis system provides an intuitive way to independently manipulate the\nvocalic formants and the spectral shape of the glottal flow that is\nresponsible for voice quality perception. Additionally, rules have\nbeen implemented to link the glottis parameters to high-level voice\nquality parameters such as vocal force and tenseness. Examples of applications\nfor this system include expressive speech synthesis, by adding the\nsystem at the end of a speech synthesiser pipeline, auditory feedback\nperturbation to study a speaker&#8217;s response to modified speech,\nand speech therapy.\n"
      ]
    },
    "szekely19_interspeech": {
      "authors": [
        [
          "\u00c9va",
          "Sz\u00e9kely"
        ],
        [
          "Gustav Eje",
          "Henter"
        ],
        [
          "Jonas",
          "Beskow"
        ],
        [
          "Joakim",
          "Gustafson"
        ]
      ],
      "title": "Off the Cuff: Exploring Extemporaneous Speech Delivery with TTS",
      "original": "8026",
      "page_count": 2,
      "order": 773,
      "p1": "3687",
      "pn": "3688",
      "abstract": [
        "Extemporaneous speech is a delivery type in public speaking which uses\na structured outline but is otherwise delivered conversationally, off\nthe cuff. This demo uses a natural-sounding spontaneous conversational\nspeech synthesiser to simulate this delivery style. We resynthesised\nthe beginnings of two Interspeech keynote speeches with TTS that produces\nmultiple different versions of each utterance that vary in fluency\nand filled-pause placement. The platform allows the user to mark the\nsamples according to any perceptual aspect of interest, such as certainty,\nauthenticity, confidence, etc. During the speech delivery, they can\ndecide on the fly which realisation to play, addressing their audience\nin a connected, conversational fashion. Our aim is to use this platform\nto explore speech synthesis evaluation options from a production perspective\nand in situational contexts.\n"
      ]
    },
    "kessler19_interspeech": {
      "authors": [
        [
          "Lucas",
          "Kessler"
        ],
        [
          "Cecilia Ovesdotter",
          "Alm"
        ],
        [
          "Reynold",
          "Bailey"
        ]
      ],
      "title": "Synthesized Spoken Names: Biases Impacting Perception",
      "original": "8031",
      "page_count": 2,
      "order": 774,
      "p1": "3689",
      "pn": "3690",
      "abstract": [
        "Utilizing a existing neural text-to-speech synthesis architecture to\ngenerate person names and comparing them to reference names read aloud\nin a formal context, we explore how bias resulting from training data\nimpacts the synthesis of person names, focusing on frequency and origin\nof names. Long-term, we aim to apply voice conversion of person names\nto aid the effective reading aloud of such names in celebratory ceremonies.\n"
      ]
    },
    "bernardo19_interspeech": {
      "authors": [
        [
          "Lu\u00eds",
          "Bernardo"
        ],
        [
          "Mathieu",
          "Giquel"
        ],
        [
          "Sebasti\u00e3o",
          "Quintas"
        ],
        [
          "Paulo",
          "Dimas"
        ],
        [
          "Helena",
          "Moniz"
        ],
        [
          "Isabel",
          "Trancoso"
        ]
      ],
      "title": "Unbabel Talk &#8212; Human Verified Translations for Voice Instant Messaging",
      "original": "8034",
      "page_count": 2,
      "order": 775,
      "p1": "3691",
      "pn": "3692",
      "abstract": [
        "Unbabel Talk is a speech-to-speech translation application that provides\nhuman certified translations for voice instant messaging (IM) in multilingual\nscenarios. By combining Unbabel&#8217;s translation pipeline with state-of-the-art\nautomatic speech recognition (ASR) and text-to-speech (TTS) models,\nUnbabel Talk can be used to send a voice message in a language of choice\nthrough popular messaging platforms. The app further ensures that translations\nhave high quality, either by certifying them through Unbabel&#8217;s\nown quality estimation (QE) tool and/or through Unbabel&#8217;s community\nof translators. There are two versions of the app. On version 1, the\napp synthesizes audio that can be delivered with male or female standard\nvoices. Version 2 has features that are currently being developed,\nsuch as voice morphing and transcription correction through Unbabel&#8217;s\ncommunity.\n"
      ]
    },
    "rabiee19_interspeech": {
      "authors": [
        [
          "Azam",
          "Rabiee"
        ],
        [
          "Tae-Ho",
          "Kim"
        ],
        [
          "Soo-Young",
          "Lee"
        ]
      ],
      "title": "Adjusting Pleasure-Arousal-Dominance for Continuous Emotional Text-to-Speech Synthesizer",
      "original": "8045",
      "page_count": 2,
      "order": 776,
      "p1": "3693",
      "pn": "3694",
      "abstract": [
        "Emotion is not limited to discrete categories of happy, sad, angry,\nfear, disgust, surprise, and so on. Instead, each emotion category\nis projected into a set of nearly independent dimensions, named pleasure\n(or valence), arousal, and dominance, known as PAD. The value of each\ndimension varies from -1 to 1, such that the neutral emotion is in\nthe center with all-zero values. Training an emotional continuous text-to-speech\n(TTS) synthesizer on the independent dimensions provides the possibility\nof emotional speech synthesis with unlimited emotion categories. Our\nend-to-end neural speech synthesizer is based on the well-known Tacotron.\nEmpirically, we have found the optimum network architecture for injecting\nthe 3D PADs. Moreover, the PAD values are adjusted for the speech synthesis\npurpose.\n"
      ]
    },
    "lapata19_interspeech": {
      "authors": [
        [
          "Mirella",
          "Lapata"
        ]
      ],
      "title": "Learning Natural Language Interfaces with Neural Models",
      "original": "abs23",
      "page_count": 0,
      "order": 777,
      "p1": "0",
      "pn": "",
      "abstract": [
        "In Spike Jonze&#8217;s futuristic film &#8220;Her&#8221;, Theodore,\na lonely writer, forms a strong emotional bond with Samantha, an operating\nsystem designed to meet his every need. Samantha can carry on seamless\nconversations with Theodore, exhibits a perfect command of language,\nand is able to take on complex tasks. She filters his emails for importance,\nallowing him to deal with information overload, she proactively arranges\nthe publication of Theodore&#8217;s letters, and is able to give advice\nusing common sense and reasoning skills.<br style=\"mso-data-placement:same-cell;\"\n/><br style=\"mso-data-placement:same-cell;\" />\n  In this talk I will\npresent an overview of recent progress on learning natural language\ninterfaces which might not be as clever as Samantha but nevertheless\nallow uses to interact with various devices and services using everyday\nlanguage. I will address the structured prediction problem of mapping\nnatural language utterances onto machine-interpretable representations\nand outline the various challenges it poses. For example, the fact\nthat the translation of natural language to formal language is highly\nnon-isomorphic, data for model training is scarce, and natural language\ncan express the same information need in many different ways. I will\ndescribe a general modeling framework based on neural networks which\ntackles these challenges and improves the robustness of natural language\ninterfaces.\n"
      ]
    },
    "nautsch19c_interspeech": {
      "authors": [
        [
          "Andreas",
          "Nautsch"
        ],
        [
          "Catherine",
          "Jasserand"
        ],
        [
          "Els",
          "Kindt"
        ],
        [
          "Massimiliano",
          "Todisco"
        ],
        [
          "Isabel",
          "Trancoso"
        ],
        [
          "Nicholas",
          "Evans"
        ]
      ],
      "title": "The GDPR &amp; Speech Data: Reflections of Legal and Technology Communities, First Steps Towards a Common Understanding",
      "original": "2647",
      "page_count": 5,
      "order": 778,
      "p1": "3695",
      "pn": "3699",
      "abstract": [
        "Privacy preservation and the protection of speech data is in high demand,\nnot least as a result of recent regulation, e.g. the General Data Protection\nRegulation (GDPR) in the EU. While there has been a period with which\nto prepare for its implementation, its implications for speech data\nis poorly understood. This assertion applies to both the legal and\ntechnology communities, and is hardly surprising since there is no\nuniversal definition of &#8216;privacy&#8217;, let alone a clear understanding\nof when or how the GDPR applies to the capture, storage and processing\nof speech data. In aiming to initiate the discussion that is needed\nto establish a level of harmonisation that is thus far lacking, this\ncontribution presents some reflections of both legal and technology\ncommunities on the implications of the GDPR as regards speech data.\nThe article outlines the need for taxonomies at the intersection of\nspeech technology and data privacy &#8212; a discussion that is still\nvery much in its infancy &#8212; and describes the ways to safeguards\nand priorities for future research. In being agnostic to any specific\napplication, the treatment should be of interest to the speech communication\ncommunity at large.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2647"
    },
    "srivastava19_interspeech": {
      "authors": [
        [
          "Brij Mohan Lal",
          "Srivastava"
        ],
        [
          "Aur\u00e9lien",
          "Bellet"
        ],
        [
          "Marc",
          "Tommasi"
        ],
        [
          "Emmanuel",
          "Vincent"
        ]
      ],
      "title": "Privacy-Preserving Adversarial Representation Learning in ASR: Reality or Illusion?",
      "original": "2415",
      "page_count": 5,
      "order": 779,
      "p1": "3700",
      "pn": "3704",
      "abstract": [
        "Automatic speech recognition (ASR) is a key technology in many services\nand applications. This typically requires user devices to send their\nspeech data to the cloud for ASR decoding. As the speech signal carries\na lot of information about the speaker, this raises serious privacy\nconcerns. As a solution, an encoder may reside on each user device\nwhich performs local computations to anonymize the representation.\nIn this paper, we focus on the protection of speaker identity and study\nthe extent to which users can be recognized based on the encoded representation\nof their speech as obtained by a deep encoder-decoder architecture\ntrained for ASR. Through speaker identification and verification experiments\non the Librispeech corpus with open and closed sets of speakers, we\nshow that the representations obtained from a standard architecture\nstill carry a lot of information about speaker identity. We then propose\nto use adversarial training to learn representations that perform well\nin ASR while hiding speaker identity. Our results demonstrate that\nadversarial training dramatically reduces the closed-set classification\naccuracy, but this does not translate into increased open-set verification\nerror hence into increased protection of the speaker identity in practice.\nWe suggest several possible reasons behind this negative result.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2415",
      "author_area_id": 13,
      "author_area_label": "Special Sessions"
    },
    "nelus19_interspeech": {
      "authors": [
        [
          "Alexandru",
          "Nelus"
        ],
        [
          "Silas",
          "Rech"
        ],
        [
          "Timm",
          "Koppelmann"
        ],
        [
          "Henrik",
          "Biermann"
        ],
        [
          "Rainer",
          "Martin"
        ]
      ],
      "title": "Privacy-Preserving Siamese Feature Extraction for Gender Recognition versus Speaker Identification",
      "original": "1148",
      "page_count": 5,
      "order": 780,
      "p1": "3705",
      "pn": "3709",
      "abstract": [
        "In this paper we propose a deep neural-network-based feature extraction\nscheme with the purpose of reducing the privacy risks encountered in\nspeaker classification tasks. For this we choose a challenging scenario\nwhere we wish to perform gender recognition but at the same time prevent\nan attacker who has intercepted the features to perform speaker identification.\nOur approach is to employ Siamese training in order to obtain a feature\nrepresentation that minimizes the Euclidean distance between same gender\nspeakers while maximizing it for different gender speakers. It is experimentally\nshown that the obtained effect is of anonymizing speakers from the\nsame gender class and thus drastically reducing privacy risks while\nstill permitting class discrimination with a higher accuracy than other\npreviously investigated methods.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1148",
      "author_area_id": 13,
      "author_area_label": "Special Sessions"
    },
    "nelus19b_interspeech": {
      "authors": [
        [
          "Alexandru",
          "Nelus"
        ],
        [
          "Janek",
          "Ebbers"
        ],
        [
          "Reinhold",
          "Haeb-Umbach"
        ],
        [
          "Rainer",
          "Martin"
        ]
      ],
      "title": "Privacy-Preserving Variational Information Feature Extraction for Domestic Activity Monitoring versus Speaker Identification",
      "original": "1703",
      "page_count": 5,
      "order": 781,
      "p1": "3710",
      "pn": "3714",
      "abstract": [
        "In this paper we highlight the privacy risks entailed in deep neural\nnetwork feature extraction for domestic activity monitoring. We employ\nthe baseline system proposed in the Task 5 of the DCASE 2018 challenge\nand simulate a feature interception attack by an eavesdropper who wants\nto perform speaker identification. We then propose to reduce the aforementioned\nprivacy risks by introducing a variational information feature extraction\nscheme that allows for good activity monitoring performance while at\nthe same time minimizing the information of the feature representation,\nthus restricting speaker identification attempts. We analyze the resulting\nmodel&#8217;s composite loss function and the budget scaling factor\nused to control the balance between the performance of the trusted\nand attacker tasks. It is empirically demonstrated that the proposed\nmethod reduces speaker identification privacy risks without significantly\ndeprecating the performance of domestic activity monitoring tasks.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1703",
      "author_area_id": 13,
      "author_area_label": "Special Sessions"
    },
    "thaine19_interspeech": {
      "authors": [
        [
          "Patricia",
          "Thaine"
        ],
        [
          "Gerald",
          "Penn"
        ]
      ],
      "title": "Extracting Mel-Frequency and Bark-Frequency Cepstral Coefficients from Encrypted Signals",
      "original": "1136",
      "page_count": 5,
      "order": 782,
      "p1": "3715",
      "pn": "3719",
      "abstract": [
        "We describe a method for extracting Mel-Frequency and Bark-Frequency\nCepstral Coefficient from an encrypted signal without having to decrypt\nany intermediate values. To do so, we introduce a novel approach for\napproximating the value of logarithms given encrypted input data. This\nmethod works over any interval for which logarithms are defined and\nbounded.<br style=\"mso-data-placement:same-cell;\" /><br style=\"mso-data-placement:same-cell;\"\n/>\n  Extracting spectral features from encrypted signals is the first\nstep towards achieving secure end-to-end automatic speech recognition\nover encrypted data. We experimentally determine the appropriate precision\nthresholds to support accurate WER for ASR over the TIMIT dataset.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1136",
      "author_area_id": 13,
      "author_area_label": "Special Sessions"
    },
    "zarazaga19_interspeech": {
      "authors": [
        [
          "Pablo P\u00e9rez",
          "Zarazaga"
        ],
        [
          "Sneha",
          "Das"
        ],
        [
          "Tom",
          "B\u00e4ckstr\u00f6m"
        ],
        [
          "V. V. Vidyadhara",
          "Raju"
        ],
        [
          "Anil Kumar",
          "Vuppala"
        ]
      ],
      "title": "Sound Privacy: A Conversational Speech Corpus for Quantifying the Experience of Privacy",
      "original": "1172",
      "page_count": 5,
      "order": 783,
      "p1": "3720",
      "pn": "3724",
      "abstract": [
        "With the growing popularity of social networks, cloud services and\nonline applications, people are becoming concerned about the way companies\nstore their data and the ways in which the data can be applied. Privacy\nwith devices and services operated by the voice are of particular interest.\nTo enable studies in privacy, this paper presents a database which\nquantifies the experience of privacy users have in spoken communication.\nWe focus on the effect of the acoustic environment on that perception\nof privacy. Speech signals are recorded in scenarios simulating real-life\nsituations, where the acoustic environment has an effect on the experience\nof privacy. The acoustic data is complemented with measures of the\nspeakers&#8217; experience of privacy, recorded using a questionnaire.\nThe presented corpus enables studies in how acoustic environments affect\npeoples&#8217; experience of privacy, which in turn, can be used to\ndevelop speech operated applications which are respectful of their\nright to privacy.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1172",
      "author_area_id": 13,
      "author_area_label": "Special Sessions"
    },
    "soto19_interspeech": {
      "authors": [
        [
          "Victor",
          "Soto"
        ],
        [
          "Julia",
          "Hirschberg"
        ]
      ],
      "title": "Improving Code-Switched Language Modeling Performance Using Cognate Features",
      "original": "2681",
      "page_count": 5,
      "order": 784,
      "p1": "3725",
      "pn": "3729",
      "abstract": [
        "We have found that  cognate words, defined as sets of words used in\nmultiple languages that share a common etymology, can in fact elicit\ncode-switching or language mixing between the languages. This paper\nfocuses on how information about cognate words can improve language\nmodeling performance of code-switched English-Spanish (EN-ES) language.\nWe have found that the degree of semantic, phonetic or lexical overlap\nbetween a pair of cognate words is a useful feature in identifying\ncode-switching in language. We derive a set of spelling, phonetic and\nsemantic features from a list of of EN-ES cognates and run experiments\non a corpus of conversational code-switched EN-ES. First, we show that\nthere exists a strong statistical relationship between these cognate-based\nfeatures and code-switching in the corpus. Secondly, we demonstrate\nthat language models using these features obtain similar performance\nimprovements as do other manually tagged features including language\nand part-of-speech tags. We conclude that cognate features can be a\nuseful set of automatically-derived features that can be easily obtained\nfor any pair of languages.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2681",
      "author_area_id": 13,
      "author_area_label": "Special Sessions"
    },
    "lee19d_interspeech": {
      "authors": [
        [
          "Grandee",
          "Lee"
        ],
        [
          "Xianghu",
          "Yue"
        ],
        [
          "Haizhou",
          "Li"
        ]
      ],
      "title": "Linguistically Motivated Parallel Data Augmentation for Code-Switch Language Modeling",
      "original": "1382",
      "page_count": 5,
      "order": 785,
      "p1": "3730",
      "pn": "3734",
      "abstract": [
        "Code-switch language modeling is challenging due to data scarcity as\nwell as expanded vocabulary that involves two languages. We present\na novel computational method to generate synthetic code-switch data\nusing the Matrix Language Frame theory to alleviate the issue of data\nscarcity. The proposed method makes use of augmented parallel data\nto supplement the real code-switch data. We use the synthetic data\nto pre-train the language model. We show that the pre-trained language\nmodel can match the performance of vanilla models when it is finetuned\nwith 2.5 times less real code-switch data. We also show that the perplexity\nof a RNN based language model pre-trained on synthetic code-switch\ndata and fine-tuned with real code-switch data is significantly lower\nthan that of the model trained on real code-switch data alone and the\nreduction in perplexity translates into 1.45% absolute reduction in\nWER in a speech recognition experiment.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1382",
      "author_area_id": 13,
      "author_area_label": "Special Sessions"
    },
    "rallabandi19_interspeech": {
      "authors": [
        [
          "SaiKrishna",
          "Rallabandi"
        ],
        [
          "Alan W.",
          "Black"
        ]
      ],
      "title": "Variational Attention Using Articulatory Priors for Generating Code Mixed Speech Using Monolingual Corpora",
      "original": "1103",
      "page_count": 5,
      "order": 786,
      "p1": "3735",
      "pn": "3739",
      "abstract": [
        "Code Mixing &#8212; phenomenon where lexical items from one language\nare embedded in the utterance of another &#8212; is relatively frequent\nin multilingual communities and therefore speech systems should be\nable to process such content. However, building a voice capable of\nsynthesizing such content typically requires bilingual recordings from\nthe speaker which might not always be easy to obtain. In this work,\nwe present an approach for building mixed lingual systems using only\nmonolingual corpora. Specifically we present a way to train multi speaker\ntext to speech system by incorporating stochastic latent variables\ninto the attention mechanism with the objective of synthesizing code\nmixed content. We subject the prior distribution for such latent variables\nto match articulatory constraints. Subjective evaluation shows that\nour systems are capable of generating high quality synthesis in code\nmixed scenarios.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1103"
    },
    "wang19l_interspeech": {
      "authors": [
        [
          "Qinyi",
          "Wang"
        ],
        [
          "Emre",
          "Y\u0131lmaz"
        ],
        [
          "Adem",
          "Derinel"
        ],
        [
          "Haizhou",
          "Li"
        ]
      ],
      "title": "Code-Switching Detection Using ASR-Generated Language Posteriors",
      "original": "1161",
      "page_count": 5,
      "order": 787,
      "p1": "3740",
      "pn": "3744",
      "abstract": [
        "Code-switching (CS) detection refers to the automatic detection of\nlanguage switches in code-mixed utterances. This task can be achieved\nby using a CS automatic speech recognition (ASR) system that can handle\nsuch language switches. In our previous work, we have investigated\nthe code-switching detection performance of the Frisian-Dutch CS ASR\nsystem by using the time alignment of the most likely hypothesis and\nfound that this technique suffers from over-switching due to numerous\nvery short spurious language switches. In this paper, we propose a\nnovel method for CS detection aiming to remedy this shortcoming by\nusing the language posteriors which are the sum of the frame-level\nposteriors of phones belonging to the same language. The CS ASR-generated\nlanguage posteriors contain more complete language-specific information\non frame level compared to the time alignment of the ASR output. Hence,\nit is expected to yield more accurate and robust CS detection. The\nCS detection experiments demonstrate that the proposed language posterior-based\napproach provides higher detection accuracy than the baseline system\nin terms of equal error rate. Moreover, a detailed CS detection error\nanalysis reveals that using language posteriors reduces the false alarms\nand results in more robust CS detection.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1161",
      "author_area_id": 13,
      "author_area_label": "Special Sessions"
    },
    "biswas19b_interspeech": {
      "authors": [
        [
          "Astik",
          "Biswas"
        ],
        [
          "Emre",
          "Y\u0131lmaz"
        ],
        [
          "Febe de",
          "Wet"
        ],
        [
          "Ewald van der",
          "Westhuizen"
        ],
        [
          "Thomas",
          "Niesler"
        ]
      ],
      "title": "Semi-Supervised Acoustic Model Training for Five-Lingual Code-Switched ASR",
      "original": "1325",
      "page_count": 5,
      "order": 788,
      "p1": "3745",
      "pn": "3749",
      "abstract": [
        "This paper presents recent progress in the acoustic modelling of under-resourced\ncode-switched (CS) speech in multiple South African languages. We consider\ntwo approaches. The first constructs separate bilingual acoustic models\ncorresponding to language pairs (English-isiZulu, English-isiXhosa,\nEnglish-Setswana and English-Sesotho). The second constructs a single\nunified five-lingual acoustic model representing all the languages\n(English, isiZulu, isiXhosa, Setswana and Sesotho). For these two approaches\nwe consider the effectiveness of semi-supervised training to increase\nthe size of the very sparse acoustic training sets. Using approximately\n11 hours of untranscribed speech, we show that both approaches benefit\nfrom semi-supervised training. The bilingual TDNN-F acoustic models\nalso benefit from the addition of CNN layers (CNN-TDNN-F), while the\nfive-lingual system does not show any significant improvement. Furthermore,\nbecause English is common to all language pairs in our data, it dominates\nwhen training a unified language model, leading to improved English\nASR performance at the expense of the other languages. Nevertheless,\nthe five-lingual model offers flexibility because it can process more\nthan two languages simultaneously, and is therefore an attractive option\nas an automatic transcription system in a semi-supervised training\npipeline.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1325",
      "author_area_id": 13,
      "author_area_label": "Special Sessions"
    },
    "ylmaz19b_interspeech": {
      "authors": [
        [
          "Emre",
          "Y\u0131lmaz"
        ],
        [
          "Samuel",
          "Cohen"
        ],
        [
          "Xianghu",
          "Yue"
        ],
        [
          "David A. van",
          "Leeuwen"
        ],
        [
          "Haizhou",
          "Li"
        ]
      ],
      "title": "Multi-Graph Decoding for Code-Switching ASR",
      "original": "1125",
      "page_count": 5,
      "order": 789,
      "p1": "3750",
      "pn": "3754",
      "abstract": [
        "In the FAME! Project, a code-switching (CS) automatic speech recognition\n(ASR) system for Frisian-Dutch speech is developed that can accurately\ntranscribe the local broadcaster&#8217;s bilingual archives with CS\nspeech. This archive contains recordings with monolingual Frisian and\nDutch speech segments as well as Frisian-Dutch CS speech, hence the\nrecognition performance on monolingual segments is also vital for accurate\ntranscriptions. In this work, we propose a multi-graph decoding and\nrescoring strategy using bilingual and monolingual graphs together\nwith a unified acoustic model for CS ASR. The proposed decoding scheme\ngives the freedom to design and employ alternative search spaces for\neach (monolingual or bilingual) recognition task and enables the effective\nuse of monolingual resources of the high-resourced mixed language in\nlow-resourced CS scenarios. In our scenario, Dutch is the high-resourced\nand Frisian is the low-resourced language. We therefore use additional\nmonolingual Dutch text resources to improve the Dutch language model\n(LM) and compare the performance of single- and multi-graph CS ASR\nsystems on Dutch segments using larger Dutch LMs. The ASR results show\nthat the proposed approach outperforms baseline single-graph CS ASR\nsystems, providing better performance on the monolingual Dutch segments\nwithout any accuracy loss on monolingual Frisian and code-mixed segments.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1125",
      "author_area_id": 13,
      "author_area_label": "Special Sessions"
    },
    "seki19_interspeech": {
      "authors": [
        [
          "Hiroshi",
          "Seki"
        ],
        [
          "Takaaki",
          "Hori"
        ],
        [
          "Shinji",
          "Watanabe"
        ],
        [
          "Jonathan Le",
          "Roux"
        ],
        [
          "John R.",
          "Hershey"
        ]
      ],
      "title": "End-to-End Multilingual Multi-Speaker Speech Recognition",
      "original": "3038",
      "page_count": 5,
      "order": 790,
      "p1": "3755",
      "pn": "3759",
      "abstract": [
        "The expressive power of end-to-end automatic speech recognition (ASR)\nsystems enables direct estimation of a character or word label sequence\nfrom a sequence of acoustic features. Direct optimization of the whole\nsystem is advantageous because it not only eliminates the internal\nlinkage necessary for hybrid systems, but also extends the scope of\npotential applications by training the model for various objectives.\nIn this paper, we tackle the challenging task of multilingual multi-speaker\nASR using such an all-in-one end-to-end system. Several multilingual\nASR systems were recently proposed based on a monolithic neural network\narchitecture without language-dependent modules, showing that modeling\nof multiple languages is well within the capabilities of an end-to-end\nframework. There has also been growing interest in multi-speaker speech\nrecognition, which enables generation of multiple label sequences from\nsingle-channel mixed speech. In particular, a multi-speaker end-to-end\nASR system that can directly model one-to-many mappings without additional\nauxiliary clues was recently proposed. The proposed model, which integrates\nthe capabilities of these two systems, is evaluated using mixtures\nof two speakers generated by using 10 languages, including code-switching\nutterances.\n"
      ],
      "doi": "10.21437/Interspeech.2019-3038",
      "author_area_id": 13,
      "author_area_label": "Special Sessions"
    },
    "guasch19_interspeech": {
      "authors": [
        [
          "Oriol",
          "Guasch"
        ]
      ],
      "title": "Survey Talk: Realistic Physics-Based Computational Voice Production",
      "original": "abs24",
      "page_count": 0,
      "order": 791,
      "p1": "0",
      "pn": "",
      "abstract": [
        "Simulating the very complex physics of voice on realistic vocal tract\ngeometries looked daunting a few years ago but has recently experienced\na very significant boom. Earlier works mainly dealt with vowel production.\nSolving the wave equation in a three-dimensional vocal tract suffices\nfor that purpose. As we depart from vowels, however, things quickly\nget harder. Simulating a few milliseconds of sibilant /s/ demands high-performance\ncomputers to solve the sound turbulent eddies generate. Producing a\ndiphthong implies dealing with dynamic geometries. A syllable like\n/sa/ seems out of reach of current computation capabilities, though\nsome modelling techniques inspired on one-dimensional approaches may\nlead to more than acceptable results. The shaping of dynamic vocal\ntracts shall be linked to biomechanical models to gain flexibility\nand achieve a more complete representation on how, we humans, generate\nvoice. Besides, including phonation in the computations implies resolving\nthe vocal fold self-oscillations and the very demanding coupling of\nthe mechanical, fluid and acoustic fields. Finally, including naturalness\nin computational voice generation is a newborn and challenging task.\nIn this talk, a general overview on realistic physics-based computational\nvoice production will be given. Current achievements and remaining\nchallenges will be highlighted and discussed.\n"
      ]
    },
    "mohapatra19_interspeech": {
      "authors": [
        [
          "Debasish Ray",
          "Mohapatra"
        ],
        [
          "Victor",
          "Zappi"
        ],
        [
          "Sidney",
          "Fels"
        ]
      ],
      "title": "An Extended Two-Dimensional Vocal Tract Model for Fast Acoustic Simulation of Single-Axis Symmetric Three-Dimensional Tubes",
      "original": "1764",
      "page_count": 5,
      "order": 792,
      "p1": "3760",
      "pn": "3764",
      "abstract": [
        "The simulation of two-dimensional (2D) wave propagation is an affordable\ncomputational task and its use can potentially improve time performance\nin vocal tracts&#8217; acoustic analysis. Several models have been\ndesigned that rely on 2D wave solvers and include 2D representations\nof three-dimensional (3D) vocal tract-like geometries. However, until\nnow, only the acoustics of straight 3D tubes with circular cross-sections\nhave been successfully replicated with this approach. Furthermore,\nthe simulation of the resulting 2D shapes requires extremely high spatiotemporal\nresolutions, dramatically reducing the speed boost deriving from the\nusage of a 2D wave solver. In this paper, we introduce an in-progress\nnovel vocal tract model that extends the 2D Finite-Difference Time-Domain\nwave solver (2.5D FDTD) by adding tube depth, derived from the area\nfunctions, to the acoustic solver. The model combines the speed of\na light 2D numerical scheme with the ability to natively simulate 3D\ntubes that are symmetric in one dimension, hence relaxing previous\nresolution requirements. An implementation of the 2.5D FDTD is presented,\nalong with evaluation of its performance in the case of static vowel\nmodeling. The paper discusses the current features and limits of the\napproach, and the potential impact on computational acoustics applications.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1764",
      "author_area_id": 7,
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "birkholz19_interspeech": {
      "authors": [
        [
          "Peter",
          "Birkholz"
        ],
        [
          "Susanne",
          "Drechsel"
        ],
        [
          "Simon",
          "Stone"
        ]
      ],
      "title": "Perceptual Optimization of an Enhanced Geometric Vocal Fold Model for Articulatory Speech Synthesis",
      "original": "2410",
      "page_count": 5,
      "order": 793,
      "p1": "3765",
      "pn": "3769",
      "abstract": [
        "We present a geometric vocal fold model that describes the glottal\narea between the lower and upper vocal fold edges as a function of\ntime. It is based on a glottis model by Titze [J. Acoust. Soc. Am.,\n75(2), 570&#8211;580 (1984)] and has been enhanced to allow the generation\nof skewed (asymmetric) glottal area waveforms and diplophonic double\npulsing. Embedded in the articulatory speech synthesizer VocalTractLab,\nthe model was used for the synthesis of German words with a range of\nsettings for the vocal fold model parameters to generate different\nmale and female voices. A perception experiment was conducted to determine\nthe parameter settings that generate the most natural-sounding voices.\nThe most natural-sounding male voice was generated with a slightly\ndivergent prephonatory glottal shape, with a phase delay of 70&#176;\nbetween the lower and upper vocal fold edges, symmetric glottal area\npulses, and a little shimmer (double pulsing). The most natural-sounding\nfemale voice was generated with a straight prephonatory glottal channel,\nwith a phase delay of 50&#176; between the vocal fold edges, slightly\nasymmetric glottal area pulses, and a little shimmer.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2410",
      "author_area_id": 7,
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "gao19e_interspeech": {
      "authors": [
        [
          "Yingming",
          "Gao"
        ],
        [
          "Simon",
          "Stone"
        ],
        [
          "Peter",
          "Birkholz"
        ]
      ],
      "title": "Articulatory Copy Synthesis Based on a Genetic Algorithm",
      "original": "1334",
      "page_count": 5,
      "order": 794,
      "p1": "3770",
      "pn": "3774",
      "abstract": [
        "This paper describes a novel approach for copy synthesis of human speech\nwith the articulatory speech synthesizer VocalTractLab (VTL). For a\ngiven natural utterance, an appropriate gestural score (an organized\npattern of articulatory movements) was obtained in two steps: initialization\nand optimization. In the first step, we employed a rule-based method\nto create an initial gestural score. In the second step, this initial\ngestural score was optimized by a genetic algorithm such that the cosine\ndistance of acoustic features between the synthetic and natural utterances\nwas minimized. The optimization was regularized by limiting certain\ngestural score parameters to reasonable values during the analysis-by-synthesis\nprocedure. The experiment results showed that, compared to a baseline\ncoordinate descent algorithm, the genetic algorithm performed better\nin terms of acoustic distance. In addition, a perceptual experiment\nwas conducted to rate the similarity between the optimized synthetic\nspeech and the original human speech. Here, similarity scores of optimized\nutterances with regularization were significantly higher than those\nwithout regularization.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1334",
      "author_area_id": 7,
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "shahrebabaki19_interspeech": {
      "authors": [
        [
          "Abdolreza Sabzi",
          "Shahrebabaki"
        ],
        [
          "Negar",
          "Olfati"
        ],
        [
          "Ali Shariq",
          "Imran"
        ],
        [
          "Sabato Marco",
          "Siniscalchi"
        ],
        [
          "Torbj\u00f8rn",
          "Svendsen"
        ]
      ],
      "title": "A Phonetic-Level Analysis of Different Input Features for Articulatory Inversion",
      "original": "2526",
      "page_count": 5,
      "order": 795,
      "p1": "3775",
      "pn": "3779",
      "abstract": [
        "The challenge of articulatory inversion is to determine the temporal\nmovement of the articulators from the speech waveform, or from acoustic-phonetic\nknowledge, e.g. derived from information about the linguistic content\nof the utterance. The actual position of the articulators is typically\nobtained from measured data, in our case position measurements obtained\nusing EMA (Electromagnetic articulography). In this paper, we investigate\nthe impact on articulatory inversion problem by using features derived\nfrom the acoustic waveform relative to using linguistic features related\nto the time aligned phone sequence of the utterance. Filterbank energies\n(FBE) are used as acoustic features, while phoneme identities and (binary)\nphonetic attributes are used as linguistic features. Experiments are\nperformed on a speech corpus with synchronously recorded EMA measurements\nand employing a bidirectional long short-term memory (BLSTM) that estimates\nthe articulators&#8217; position. Acoustic FBE features performed better\nfor vowel sounds. Phonetic features attained better results for nasal\nand fricative sounds except for /h/. Further improvements were obtained\nby combining FBE and linguistic features, which led to an average relative\nRMSE reduction of 9.8%, and a 3% relative improvement of the Pearson\ncorrelation coefficient.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2526",
      "author_area_id": 7,
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "tuske19_interspeech": {
      "authors": [
        [
          "Zolt\u00e1n",
          "T\u00fcske"
        ],
        [
          "Kartik",
          "Audhkhasi"
        ],
        [
          "George",
          "Saon"
        ]
      ],
      "title": "Advancing Sequence-to-Sequence Based Speech Recognition",
      "original": "3018",
      "page_count": 5,
      "order": 796,
      "p1": "3780",
      "pn": "3784",
      "abstract": [
        "The paper presents our endeavor to improve state-of-the-art speech\nrecognition results using attention based neural network approaches.\nOur test focus was LibriSpeech, a well-known, publicly available, large,\nspeech corpus, but the methodologies are clearly applicable to other\ntasks. After systematic application of standard techniques &#8212;\nsophisticated data augmentation, various dropout schemes, scheduled\nsampling, warm-restart &#8212;, and optimizing search configurations,\nour model achieves 4.0% and 11.7% word error rate (WER) on the test-clean\nand test-other sets,  without any external language model. A powerful\nrecurrent language model drops the error rate further to 2.7% and 8.2%.\nThus, we not only report the lowest sequence-to-sequence model based\nnumbers on this task to date, but our single system even challenges\nthe best result known in the literature, namely a hybrid model together\nwith recurrent language model rescoring. A simple ROVER combination\nof several of our attention based systems achieved 2.5% and 7.3% WER\non the clean and other test sets.\n"
      ],
      "doi": "10.21437/Interspeech.2019-3018",
      "author_area_id": 8,
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "hannun19_interspeech": {
      "authors": [
        [
          "Awni",
          "Hannun"
        ],
        [
          "Ann",
          "Lee"
        ],
        [
          "Qiantong",
          "Xu"
        ],
        [
          "Ronan",
          "Collobert"
        ]
      ],
      "title": "Sequence-to-Sequence Speech Recognition with Time-Depth Separable Convolutions",
      "original": "2460",
      "page_count": 5,
      "order": 797,
      "p1": "3785",
      "pn": "3789",
      "abstract": [
        "We propose a fully convolutional sequence-to-sequence encoder architecture\nwith a simple and efficient decoder. Our model improves WER on LibriSpeech\nwhile being an order of magnitude more efficient than a strong RNN\nbaseline. Key to our approach is a time-depth separable convolution\nblock which dramatically reduces the number of parameters in the model\nwhile keeping the receptive field large. We also give a stable and\nefficient beam search inference procedure which allows us to effectively\nintegrate a language model. Coupled with a convolutional language model,\nour time-depth separable convolution architecture improves by more\nthan 22% relative WER over the best previously reported sequence-to-sequence\nresults on the noisy LibriSpeech test set.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2460",
      "author_area_id": 8,
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "baskar19_interspeech": {
      "authors": [
        [
          "Murali Karthick",
          "Baskar"
        ],
        [
          "Shinji",
          "Watanabe"
        ],
        [
          "Ramon",
          "Astudillo"
        ],
        [
          "Takaaki",
          "Hori"
        ],
        [
          "Luk\u00e1\u0161",
          "Burget"
        ],
        [
          "Jan",
          "\u010cernock\u00fd"
        ]
      ],
      "title": "Semi-Supervised Sequence-to-Sequence ASR Using Unpaired Speech and Text",
      "original": "3167",
      "page_count": 5,
      "order": 798,
      "p1": "3790",
      "pn": "3794",
      "abstract": [
        "Sequence-to-sequence automatic speech recognition (ASR) models require\nlarge quantities of data to attain high performance. For this reason,\nthere has been a recent surge in interest for unsupervised and semi-supervised\ntraining in such models. This work builds upon recent results showing\nnotable improvements in semi-supervised training using cycle-consistency\nand related techniques. Such techniques derive training procedures\nand losses able to leverage unpaired speech and/or text data by combining\nASR with Text-to-Speech (TTS) models. In particular, this work proposes\na new semi-supervised loss combining an end-to-end differentiable ASR&#8594;TTS\nloss with TTS&#8594;ASR loss. The method is able to leverage both unpaired\nspeech and text data to outperform recently proposed related techniques\nin terms of %WER. We provide extensive results analyzing the impact\nof data quantity and speech and text modalities and show consistent\ngains across WSJ and Librispeech corpora. Our code is provided in ESPnet\nto reproduce the experiments.\n"
      ],
      "doi": "10.21437/Interspeech.2019-3167",
      "author_area_id": 8,
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "bai19c_interspeech": {
      "authors": [
        [
          "Ye",
          "Bai"
        ],
        [
          "Jiangyan",
          "Yi"
        ],
        [
          "Jianhua",
          "Tao"
        ],
        [
          "Zhengkun",
          "Tian"
        ],
        [
          "Zhengqi",
          "Wen"
        ]
      ],
      "title": "Learn Spelling from Teachers: Transferring Knowledge from Language Models to Sequence-to-Sequence Speech Recognition",
      "original": "1554",
      "page_count": 5,
      "order": 799,
      "p1": "3795",
      "pn": "3799",
      "abstract": [
        "Integrating an external language model into a sequence-to-sequence\nspeech recognition system is non-trivial. Previous works utilize linear\ninterpolation or a fusion network to integrate external language models.\nHowever, these approaches introduce external components, and increase\ndecoding computation. In this paper, we instead propose a knowledge\ndistillation based training approach to integrating external language\nmodels into a sequence-to-sequence model. A recurrent neural network\nlanguage model, which is trained on large scale external text, generates\nsoft labels to guide the sequence-to-sequence model training. Thus,\nthe language model plays the role of the teacher. This approach does\nnot add any external component to the sequence-to-sequence model during\ntesting. And this approach is flexible to be combined with shallow\nfusion technique together for decoding. The experiments are conducted\non public Chinese datasets AISHELL-1 and CLMAD. Our approach achieves\na character error rate of 9.3%, which is relatively reduced by 18.42%\ncompared with the vanilla sequence-to-sequence model.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1554",
      "author_area_id": 8,
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "irie19_interspeech": {
      "authors": [
        [
          "Kazuki",
          "Irie"
        ],
        [
          "Rohit",
          "Prabhavalkar"
        ],
        [
          "Anjuli",
          "Kannan"
        ],
        [
          "Antoine",
          "Bruguier"
        ],
        [
          "David",
          "Rybach"
        ],
        [
          "Patrick",
          "Nguyen"
        ]
      ],
      "title": "On the Choice of Modeling Unit for Sequence-to-Sequence Speech Recognition",
      "original": "2277",
      "page_count": 5,
      "order": 800,
      "p1": "3800",
      "pn": "3804",
      "abstract": [
        "In conventional speech recognition, phoneme-based models outperform\ngrapheme-based models for non-phonetic languages such as English. The\nperformance gap between the two typically reduces as the amount of\ntraining data is increased. In this work, we examine the impact of\nthe choice of modeling unit for attention-based encoder-decoder models.\nWe conduct experiments on the LibriSpeech 100hr, 460hr, and 960hr tasks,\nusing various target units (phoneme, grapheme, and word-piece); across\nall tasks, we find that grapheme or word-piece models consistently\noutperform phoneme-based models, even though they are evaluated without\na lexicon or an external language model. We also investigate model\ncomplementarity: we find that we can improve WERs by up to 9% relative\nby rescoring N-best lists generated from a strong word-piece based\nbaseline with either the phoneme or the grapheme model. Rescoring an\nN-best list generated by the phonemic system, however, provides limited\nimprovements. Further analysis shows that the word-piece-based models\nproduce more diverse N-best hypotheses, and thus lower oracle WERs,\nthan phonemic models.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2277",
      "author_area_id": 8,
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "weninger19b_interspeech": {
      "authors": [
        [
          "Felix",
          "Weninger"
        ],
        [
          "Jes\u00fas",
          "Andr\u00e9s-Ferrer"
        ],
        [
          "Xinwei",
          "Li"
        ],
        [
          "Puming",
          "Zhan"
        ]
      ],
      "title": "Listen, Attend, Spell and Adapt: Speaker Adapted Sequence-to-Sequence ASR",
      "original": "2719",
      "page_count": 5,
      "order": 801,
      "p1": "3805",
      "pn": "3809",
      "abstract": [
        "Sequence-to-sequence (seq2seq) based ASR systems have shown state-of-the-art\nperformances while having clear advantages in terms of simplicity.\nHowever, comparisons are mostly done on speaker independent (SI) ASR\nsystems, though speaker adapted conventional systems are commonly used\nin practice for improving robustness to speaker and environment variations.\nIn this paper, we apply speaker adaptation to seq2seq models with the\ngoal of matching the performance of conventional ASR adaptation. Specifically,\nwe investigate Kullback-Leibler divergence (KLD) as well as Linear\nHidden Network (LHN) based adaptation for seq2seq ASR, using different\namounts (up to 20 hours) of adaptation data per speaker. Our SI models\nare trained on large amounts of dictation data and achieve state-of-the-art\nresults. We obtained 25% relative word error rate (WER) improvement\nwith KLD adaptation of the seq2seq model vs. 18.7% gain from acoustic\nmodel adaptation in the conventional system. We also show that the\nWER of the seq2seq model decreases log-linearly with the amount of\nadaptation data. Finally, we analyze adaptation based on the minimum\nWER criterion and adapting the language model (LM) for score fusion\nwith the speaker adapted seq2seq model, which result in further improvements\nof the seq2seq system performance.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2719",
      "author_area_id": 8,
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "runarsdottir19_interspeech": {
      "authors": [
        [
          "Anna V.",
          "R\u00fanarsd\u00f3ttir"
        ],
        [
          "Inga R.",
          "Helgad\u00f3ttir"
        ],
        [
          "J\u00f3n",
          "Gu\u00f0nason"
        ]
      ],
      "title": "Lattice Re-Scoring During Manual Editing for Automatic Error Correction of ASR Transcripts",
      "original": "1790",
      "page_count": 5,
      "order": 802,
      "p1": "3810",
      "pn": "3814",
      "abstract": [
        "Automatic speech recognition (ASR) systems are increasingly used to\ntranscribe text for publication or official uses. However, even the\nbest ASR systems make mistakes that can change the meaning of the recognition\nresults. The results from these systems are therefore often reviewed\nby human editors, who fix the errors that arise. Offering automatic\nupdates of utterances, with lattice re-scoring, could decrease the\nmanual labor needed to fix errors from these systems. The research\npresented in this paper is conducted within an ASR-based transcription\nsystem with human post-editing for the Icelandic parliament,  Althingi,\nand aims to automatically correct down-stream errors once the first\nerror of a sentence has been manually corrected. After manually correcting\nthe first error of the utterances, a new path is computed through the\ncorrection, using the lattice created during the ASR decoding process.\nWith re-scoring, the sentence error rate (SER) for utterances containing\ntwo errors (and hence with SER=100%) drops to 82.77% and for utterances\ncontaining three errors drops to 95.88%. This paper demonstrates that\nthe trade-off between automatically fixed errors and new errors introduced\nin the re-scoring heavily favours adding this process to the transcription\nsystem.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1790",
      "author_area_id": 9,
      "author_area_label": "Speech Recognition: Architecture, Search, and Linguistic Components"
    },
    "fukunaga19_interspeech": {
      "authors": [
        [
          "Daisuke",
          "Fukunaga"
        ],
        [
          "Yoshiki",
          "Tanaka"
        ],
        [
          "Yuichi",
          "Kageyama"
        ]
      ],
      "title": "GPU-Based WFST Decoding with Extra Large Language Model",
      "original": "2101",
      "page_count": 5,
      "order": 803,
      "p1": "3815",
      "pn": "3819",
      "abstract": [
        "Weighted finite-state transducer (WFST) decoding in speech recognition\ncan be accelerated by using graphics processing units (GPUs). To obtain\na high recognition accuracy in a WFST-based speech recognition system,\na very large language model (LM) represented as a WFST with more than\n10 GB of data is required. Since a GPU typically has only several GB\nof memory, it is impossible to store such a large LM in GPU memory.\nIn this paper, we propose a new method for WFST decoding on a GPU.\nThe method utilizes the  on-the-fly rescoring algorithm, which performs\nthe Viterbi search on a WFST with a small LM and rescores hypotheses\nusing a large LM during decoding. We solve the problem of insufficient\nGPU memory by storing most of the large LM in a memory on the host\nand copying the data from the host memory to the GPU memory as needed\nduring runtime. Our evaluation of the proposed method on the LibriSpeech\ntest sets using an NVIDIA Tesla V100 GPU shows that it achieves a ten\ntimes faster decoding than an equivalent CPU implementation without\nrecognition accuracy degradation.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2101",
      "author_area_id": 9,
      "author_area_label": "Speech Recognition: Architecture, Search, and Linguistic Components"
    },
    "jorge19_interspeech": {
      "authors": [
        [
          "Javier",
          "Jorge"
        ],
        [
          "Adri\u00e0",
          "Gim\u00e9nez"
        ],
        [
          "Javier",
          "Iranzo-S\u00e1nchez"
        ],
        [
          "Jorge",
          "Civera"
        ],
        [
          "Albert",
          "Sanchis"
        ],
        [
          "Alfons",
          "Juan"
        ]
      ],
      "title": "Real-Time One-Pass Decoder for Speech Recognition Using LSTM Language Models",
      "original": "2798",
      "page_count": 5,
      "order": 804,
      "p1": "3820",
      "pn": "3824",
      "abstract": [
        "Recurrent Neural Networks, in particular Long-Short Term Memory (LSTM)\nnetworks, are widely used in Automatic Speech Recognition for language\nmodelling during decoding, usually as a mechanism for rescoring hypothesis.\nThis paper proposes a new architecture to perform real-time one-pass\ndecoding using LSTM language models. To make decoding efficient, the\nestimation of look-ahead scores was accelerated by precomputing static\nlook-ahead tables. These static tables were precomputed from a pruned\nn-gram model, reducing drastically the computational cost during decoding.\nAdditionally, the LSTM language model evaluation was efficiently performed\nusing Variance Regularization along with a strategy of lazy evaluation.\nThe proposed one-pass decoder architecture was evaluated on the well-known\nLibriSpeech and TED-LIUMv3 datasets. Results showed that the proposed\nalgorithm obtains very competitive WERs with &#126;0.6 RTFs. Finally,\nour one-pass decoder is compared with a decoupled two-pass decoder.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2798",
      "author_area_id": 9,
      "author_area_label": "Speech Recognition: Architecture, Search, and Linguistic Components"
    },
    "seki19b_interspeech": {
      "authors": [
        [
          "Hiroshi",
          "Seki"
        ],
        [
          "Takaaki",
          "Hori"
        ],
        [
          "Shinji",
          "Watanabe"
        ],
        [
          "Niko",
          "Moritz"
        ],
        [
          "Jonathan Le",
          "Roux"
        ]
      ],
      "title": "Vectorized Beam Search for CTC-Attention-Based Speech Recognition",
      "original": "2860",
      "page_count": 5,
      "order": 805,
      "p1": "3825",
      "pn": "3829",
      "abstract": [
        "This paper investigates efficient beam search techniques for end-to-end\nautomatic speech recognition (ASR) with attention-based encoder-decoder\narchitecture. We accelerate the decoding process by vectorizing multiple\nhypotheses during the beam search, where we replace the score accumulation\nsteps for each hypothesis with vector-matrix operations for the vectorized\nhypotheses. This modification allows us to take advantage of the parallel\ncomputing capabilities of multi-core CPUs and GPUs, resulting in significant\nspeedups and also enabling us to process multiple utterances in a batch\nsimultaneously. Moreover, we extend the decoding method to incorporate\na recurrent neural network language model (RNNLM) and connectionist\ntemporal classification (CTC) scores, which typically improve ASR accuracy\nbut have not been investigated for the use of such parallelized decoding\nalgorithms. Experiments with LibriSpeech and Corpus of Spontaneous\nJapanese datasets have demonstrated that the vectorized beam search\nachieves 1.8&#215; speedup on a CPU and 33&#215; speedup on a GPU compared\nwith the original CPU implementation. When using joint CTC/attention\ndecoding with an RNNLM, we also achieved 11&#215; speedup on a GPU\nwhile maintaining the benefits of CTC and RNNLM. With these benefits,\nwe achieved almost real-time processing with a small latency of 0.1&#215;\nreal-time without streaming search process.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2860",
      "author_area_id": 9,
      "author_area_label": "Speech Recognition: Architecture, Search, and Linguistic Components"
    },
    "serrino19_interspeech": {
      "authors": [
        [
          "Jack",
          "Serrino"
        ],
        [
          "Leonid",
          "Velikovich"
        ],
        [
          "Petar",
          "Aleksic"
        ],
        [
          "Cyril",
          "Allauzen"
        ]
      ],
      "title": "Contextual Recovery of Out-of-Lattice Named Entities in Automatic Speech Recognition",
      "original": "2962",
      "page_count": 5,
      "order": 806,
      "p1": "3830",
      "pn": "3834",
      "abstract": [
        "As voice-driven intelligent assistants become commonplace, adaptation\nto user context becomes critical for Automatic Speech Recognition (ASR)\nsystems. For example, ASR systems may be expected to recognize a user&#8217;s\ncontact names containing improbable or out-of-vocabulary (OOV) words.<br\nstyle=\"mso-data-placement:same-cell;\" /><br style=\"mso-data-placement:same-cell;\"\n/>\n  We introduce a method to identify contextual cues in a first-pass\nASR system&#8217;s output and to recover out-of-lattice hypotheses\nthat are contextually relevant. Our proposed module is agnostic to\nthe architecture of the underlying recognizer, provided it generates\na word lattice of hypotheses; it is sufficiently compact for use on\ndevice. The module identifies subgraphs in the lattice likely to contain\nnamed entities (NEs), recovers phoneme hypotheses over corresponding\ntime spans, and inserts NEs that are phonetically close to those hypotheses.\nWe measure a decrease in the mean word error rate (WER) of word lattices\nfrom 11.5% to 4.9% on a test set of NEs.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2962",
      "author_area_id": 9,
      "author_area_label": "Speech Recognition: Architecture, Search, and Linguistic Components"
    },
    "novitasari19_interspeech": {
      "authors": [
        [
          "Sashi",
          "Novitasari"
        ],
        [
          "Andros",
          "Tjandra"
        ],
        [
          "Sakriani",
          "Sakti"
        ],
        [
          "Satoshi",
          "Nakamura"
        ]
      ],
      "title": "Sequence-to-Sequence Learning via Attention Transfer for Incremental Speech Recognition",
      "original": "2985",
      "page_count": 5,
      "order": 807,
      "p1": "3835",
      "pn": "3839",
      "abstract": [
        "Attention-based sequence-to-sequence automatic speech recognition (ASR)\nrequires a significant delay to recognize long utterances because the\noutput is generated after receiving entire input sequences. Although\nseveral studies recently proposed sequence mechanisms for incremental\nspeech recognition (ISR), using different frameworks and learning algorithms\nis more complicated than the standard ASR model. One main reason is\nbecause the model needs to decide the incremental steps and learn the\ntranscription that aligns with the current short speech segment. In\nthis work, we investigate whether it is possible to employ the original\narchitecture of attention-based ASR for ISR tasks by treating a full-utterance\nASR as the teacher model and the ISR as the student model. We design\nan alternative student network that, instead of using a thinner or\na shallower model, keeps the original architecture of the teacher model\nbut with shorter sequences (few encoder and decoder states). Using\nattention transfer, the student network learns to mimic the same alignment\nbetween the current input short speech segments and the transcription.\nOur experiments show that by delaying the starting time of recognition\nprocess with about 1.7 sec, we can achieve comparable performance to\none that needs to wait until the end.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2985",
      "author_area_id": 9,
      "author_area_label": "Speech Recognition: Architecture, Search, and Linguistic Components"
    },
    "lian19b_interspeech": {
      "authors": [
        [
          "Zheng",
          "Lian"
        ],
        [
          "Jianhua",
          "Tao"
        ],
        [
          "Bin",
          "Liu"
        ],
        [
          "Jian",
          "Huang"
        ]
      ],
      "title": "Unsupervised Representation Learning with Future Observation Prediction for Speech Emotion Recognition",
      "original": "1582",
      "page_count": 5,
      "order": 808,
      "p1": "3840",
      "pn": "3844",
      "abstract": [
        "Prior works on speech emotion recognition utilize various unsupervised\nlearning approaches to deal with low-resource samples. However, these\nmethods pay less attention to modeling the long-term dynamic dependency,\nwhich is important for speech emotion recognition. To deal with this\nproblem, this paper combines the unsupervised representation learning\nstrategy &#8212; Future Observation Prediction (FOP), with transfer\nlearning approaches (such as Fine-tuning and Hypercolumns). To verify\nthe effectiveness of the proposed method, we conduct experiments on\nthe IEMOCAP database. Experimental results demonstrate that our method\nis superior to currently advanced unsupervised learning strategies.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1582",
      "author_area_id": 5,
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "phan19_interspeech": {
      "authors": [
        [
          "Huy",
          "Phan"
        ],
        [
          "Oliver Y.",
          "Ch\u00e9n"
        ],
        [
          "Lam",
          "Pham"
        ],
        [
          "Philipp",
          "Koch"
        ],
        [
          "Maarten De",
          "Vos"
        ],
        [
          "Ian",
          "McLoughlin"
        ],
        [
          "Alfred",
          "Mertins"
        ]
      ],
      "title": "Spatio-Temporal Attention Pooling for Audio Scene Classification",
      "original": "3040",
      "page_count": 5,
      "order": 809,
      "p1": "3845",
      "pn": "3849",
      "abstract": [
        "Acoustic scenes are rich and redundant in their content. In this work,\nwe present a spatio-temporal attention pooling layer coupled with a\nconvolutional recurrent neural network to learn from patterns that\nare discriminative while suppressing those that are irrelevant for\nacoustic scene classification. The convolutional layers in this network\nlearn invariant features from time-frequency input. The bidirectional\nrecurrent layers are then able to encode the temporal dynamics of the\nresulting convolutional features. Afterwards, a two-dimensional attention\nmask is formed via the outer product of the spatial and temporal attention\nvectors learned from two designated attention layers to weigh and pool\nthe recurrent output into a final feature vector for classification.\nThe network is trained with  between-class examples generated from\nbetween-class data augmentation. Experiments demonstrate that the proposed\nmethod not only outperforms a strong convolutional neural network baseline\nbut also sets new state-of-the-art performance on the LITIS Rouen dataset.\n"
      ],
      "doi": "10.21437/Interspeech.2019-3040",
      "author_area_id": 5,
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "shi19d_interspeech": {
      "authors": [
        [
          "Qiuying",
          "Shi"
        ],
        [
          "Hui",
          "Luo"
        ],
        [
          "Jiqing",
          "Han"
        ]
      ],
      "title": "Subspace Pooling Based Temporal Features Extraction for Audio Event Recognition",
      "original": "2047",
      "page_count": 5,
      "order": 810,
      "p1": "3850",
      "pn": "3854",
      "abstract": [
        "Currently, most popular methods of Audio Event Recognition (AER) firstly\nsplit audio event signals into multiple short segments, then the features\nof these segments are pooled for recognition. However, the temporal\nfeatures between segments, which highly affect the semantic representation\nof signals, are usually discarded in the above pooling step. Thus,\nhow to introduce the temporal features to the pooling step requires\nfurther investigation. Unfortunately, on the one hand, only a few studies\nhave been conducted towards solving this problem so far. On the other\nhand, the effective temporal features should not only capture the temporal\ndynamics but also have the signal reconstruction ability, while most\nof the above studies mainly focus on the former but ignore the latter.\nIn addition, the effective features of high-dimensional original signals\nusually inhabit a low-dimensional subspace. Therefore, we propose two\nnovel pooling based methods which try to consider both the temporal\ndynamics and signal reconstruction ability of temporal features in\nthe low-dimensional subspace. The proposed methods are evaluated on\nthe AudioEvent database, and experimental results show that our methods\ncan outperform most of the typical methods.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2047",
      "author_area_id": 5,
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "zhang19l_interspeech": {
      "authors": [
        [
          "Jingyang",
          "Zhang"
        ],
        [
          "Wenhao",
          "Ding"
        ],
        [
          "Jintao",
          "Kang"
        ],
        [
          "Liang",
          "He"
        ]
      ],
      "title": "Multi-Scale Time-Frequency Attention for Acoustic Event Detection",
      "original": "1587",
      "page_count": 5,
      "order": 811,
      "p1": "3855",
      "pn": "3859",
      "abstract": [
        "Most attention-based methods only concentrate along the time axis,\nwhich is insufficient for Acoustic Event Detection (AED). Meanwhile,\nprevious methods for AED rarely considered that target events possess\ndistinct temporal and frequential scales. In this work, we propose\na  Multi-Scale Time-Frequency Attention (MTFA) module for AED. MTFA\ngathers information at multiple resolutions to generate a time-frequency\nattention mask which tells the model where to focus along both time\nand frequency axis. With MTFA, the model could capture the characteristics\nof target events with different scales. We demonstrate the proposed\nmethod on Task 2 of Detection and Classification of Acoustic Scenes\nand Events (DCASE) 2017 Challenge. Our method achieves competitive\nresults on both development dataset and evaluation dataset.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1587"
    },
    "song19b_interspeech": {
      "authors": [
        [
          "Hongwei",
          "Song"
        ],
        [
          "Jiqing",
          "Han"
        ],
        [
          "Shiwen",
          "Deng"
        ],
        [
          "Zhihao",
          "Du"
        ]
      ],
      "title": "Acoustic Scene Classification by Implicitly Identifying Distinct Sound Events",
      "original": "2231",
      "page_count": 5,
      "order": 812,
      "p1": "3860",
      "pn": "3864",
      "abstract": [
        "In this paper, we propose a new strategy for acoustic scene classification\n(ASC) , namely recognizing acoustic scenes through identifying distinct\nsound events. This differs from existing strategies, which focus on\ncharacterizing global acoustical distributions of audio or the temporal\nevolution of short-term audio features, without analysis down to the\nlevel of sound events. To identify distinct sound events for each scene,\nwe formulate ASC in a multi-instance learning (MIL) framework, where\neach audio recording is mapped into a bag-of-instances representation.\nHere, instances can be seen as high-level representations for sound\nevents inside a scene. We also propose a MIL neural networks model,\nwhich implicitly identifies distinct instances (i.e., sound events).\nFurthermore, we propose two specially designed modules that model the\nmulti-temporal scale and multi-modal natures of the sound events respectively.\nThe experiments were conducted on the official development set of the\nDCASE2018 Task1 Subtask B, and our best-performing model improves over\nthe official baseline by 9.4% (68.3% vs 58.9%) in terms of classification\naccuracy. This study indicates that recognizing acoustic scenes by\nidentifying distinct sound events is effective and paves the way for\nfuture studies that combine this strategy with previous ones.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2231",
      "author_area_id": 5,
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "qi19_interspeech": {
      "authors": [
        [
          "Xiaoke",
          "Qi"
        ],
        [
          "Lu",
          "Wang"
        ]
      ],
      "title": "Parameter-Transfer Learning for Low-Resource Individualization of Head-Related Transfer Functions",
      "original": "2558",
      "page_count": 5,
      "order": 813,
      "p1": "3865",
      "pn": "3869",
      "abstract": [
        "Individualized head-related transfer functions (HRTFs) play an important\nrole in accurate localization perception. However, it is a great challenge\nto efficiently measure continuous HRTFs for each person in full space.\nIn this paper, we propose a parameter-transfer learning method termed\nPTL to obtain individualized HRTFs based on a small set of HRTF measurements.\nThe key idea behind PTL is to transfer a HRTF generation model from\nother database to a target individual. To this end, PTL first pretrains\na deep neural network (DNN)-based universal model on a large database\nof HRTFs with the assist of domain knowledge. Domain knowledge is used\nto generate the input features derived from the solution to sound wave\npropagation equation at the physical level, and to design the loss\nfunction based on the knowledge of objective evaluation criterion.\nThen, the universal model is transferred to a target individual by\nadapting the parameters of a hidden layer of DNN with a small set of\nHRTF measurements. The adaptation layer is determined by experimental\nverification. We also conduct the objective and subjective experiments,\nand the results show that the proposed method outperforms the state-of-the-arts\nmethods in terms of LSD and localization accuracy.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2558",
      "author_area_id": 5,
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "liu19i_interspeech": {
      "authors": [
        [
          "Lei",
          "Liu"
        ],
        [
          "Meng",
          "Jian"
        ],
        [
          "Wentao",
          "Gu"
        ]
      ],
      "title": "Prosodic Characteristics of Mandarin Declarative and Interrogative Utterances in Parkinson&#8217;s Disease",
      "original": "3276",
      "page_count": 5,
      "order": 814,
      "p1": "3870",
      "pn": "3874",
      "abstract": [
        "This work investigated the prosodic characteristics of declarative\nand interrogative utterances produced by speakers with Parkinson&#8217;s\ndisease (PD), in comparison to healthy controls (HC). Forty native\nspeakers of Mandarin, including 20 PDs and 20 age-matched HCs, recorded\n32 utterances varying in sentence type, sentence length, and sentence-final\ntone. SS-ANOVA was used to show the F0 contours and the global and\nfinal-syllable F0 level, F0 slope, speech rate, and intensity ratio\nwere statistically analyzed using linear mixed-effects models. For\nthe HC group, interrogative utterances showed a significantly higher\nmean F0 than declarative utterances. The PD group exhibited no significant\nF0 difference between declarative and interrogative utterances, coinciding\nwith our subjective impression on PD&#8217;s monotonous voice of tone.\nThis suggests that PD&#8217;s ability to control fundamental frequency\ndegraded in comparison to HC. Also, the PD group produced significantly\nfaster speech, especially final syllable, than the HC group, suggesting\nthat PD&#8217;s articulatory control degraded at the end of an utterance.\n"
      ],
      "doi": "10.21437/Interspeech.2019-3276",
      "author_area_id": 1,
      "author_area_label": "Speech Perception, Production and Acquisition"
    },
    "morovelazquez19_interspeech": {
      "authors": [
        [
          "Laureano",
          "Moro-Velazquez"
        ],
        [
          "JaeJin",
          "Cho"
        ],
        [
          "Shinji",
          "Watanabe"
        ],
        [
          "Mark A.",
          "Hasegawa-Johnson"
        ],
        [
          "Odette",
          "Scharenborg"
        ],
        [
          "Heejin",
          "Kim"
        ],
        [
          "Najim",
          "Dehak"
        ]
      ],
      "title": "Study of the Performance of Automatic Speech Recognition Systems in Speakers with Parkinson&#8217;s Disease",
      "original": "2993",
      "page_count": 5,
      "order": 815,
      "p1": "3875",
      "pn": "3879",
      "abstract": [
        "Parkinson&#8217;s Disease (PD) affects motor capabilities of patients,\nwho in some cases need to use human-computer assistive technologies\nto regain independence. The objective of this work is to study in detail\nthe differences in error patterns from state-of-the-art Automatic Speech\nRecognition (ASR) systems on speech from people with and without PD.\nTwo different speech recognizers (attention-based end-to-end and Deep\nNeural Network - Hidden Markov Models hybrid systems) were trained\non a Spanish language corpus and subsequently tested on speech from\n43 speakers with PD and 46 without PD. The differences related to error\nrates, substitutions, insertions and deletions of characters and phonetic\nunits between the two groups were analyzed, showing that the word error\nrate is 27% higher in speakers with PD than in control speakers, with\na moderated correlation between that rate and the developmental stage\nof the disease. The errors were related to all manner classes, and\nwere more pronounced in the vowel /u/. This study is the first to evaluate\nASR systems&#8217; responses to speech from patients at different stages\nof PD in Spanish. The analyses showed general trends but individual\nspeech deficits must be studied in the future when designing new ASR\nsystems for this population.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2993",
      "author_area_id": 1,
      "author_area_label": "Speech Perception, Production and Acquisition"
    },
    "wang19m_interspeech": {
      "authors": [
        [
          "Tianqi",
          "Wang"
        ],
        [
          "Chongyuan",
          "Lian"
        ],
        [
          "Jingshen",
          "Pan"
        ],
        [
          "Quanlei",
          "Yan"
        ],
        [
          "Feiqi",
          "Zhu"
        ],
        [
          "Manwa L.",
          "Ng"
        ],
        [
          "Lan",
          "Wang"
        ],
        [
          "Nan",
          "Yan"
        ]
      ],
      "title": "Towards the Speech Features of Mild Cognitive Impairment: Universal Evidence from Structured and Unstructured Connected Speech of Chinese",
      "original": "2414",
      "page_count": 5,
      "order": 816,
      "p1": "3880",
      "pn": "3884",
      "abstract": [
        "Language impairment is a sensitive biomarker for the detection of cognitive\ndecline associated with mild cognitive impairment (MCI). Recently,\nknowledge about distinctive linguistic features identifying language\ndeficits in MCI has progressively been enriched and accumulated. However,\nthe employment of a single speech task to elicit connected speech (e.g.,\nstructured vs. spontaneous conversations) might limit the generalization\nof salient linguistic features associated with MCI. Not to mention\nthe scarcity of reports on analysis of extended speech of Chinese.\nThe present study aimed to examine if connected speech production in\nboth situational picture description and spontaneous self-introduction\ntasks could be used to distinguish individuals with psychometric evidence\nof MCI and those who were cognitively intact. Speech samples produced\nby 75 elderly native speakers of Mandarin Chinese, including 19 with\nMCI and 56 healthy controls were obtained. Macrostructural aspects\nof language, including lexico-semantic, syntactic, speech fluency,\nand acoustics were analyzed by applying the linear mixed-effect regression\nmodel. Our study revealed decreasing linear trends in semantic contents\nand syntactic complexity, as well as significantly greater signs of\ndisfluency and reduced speech production in participants with MCI.\nThe findings extended what was reported in the literature, and carry\nimportant implications to the screening and diagnosis of suspected\nMCI.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2414",
      "author_area_id": 1,
      "author_area_label": "Speech Perception, Production and Acquisition"
    },
    "wang19n_interspeech": {
      "authors": [
        [
          "Jiarui",
          "Wang"
        ],
        [
          "Ying",
          "Qin"
        ],
        [
          "Zhiyuan",
          "Peng"
        ],
        [
          "Tan",
          "Lee"
        ]
      ],
      "title": "Child Speech Disorder Detection with Siamese Recurrent Network Using Speech Attribute Features",
      "original": "2320",
      "page_count": 5,
      "order": 817,
      "p1": "3885",
      "pn": "3889",
      "abstract": [
        "Acoustics-based automatic assessment is a highly desirable approach\nto detecting speech sound disorder (SSD) in children. The performance\nof an automatic speech assessment system depends greatly on the availability\nof a good amount of properly annotated disordered speech, which is\na critical problem particularly for child speech. This paper presents\na novel design of child speech disorder detection system that requires\nonly normal speech for model training. The system is based on a Siamese\nrecurrent network, which is trained to learn the similarity and discrepancy\nof pronunciations between a pair of phones in the embedding space.\nFor detection of speech sound disorder, the trained network measures\na distance that contrasts the test phone to the desired phone and the\ndistance is used to train a binary classifier. Speech attribute features\nare incorporated to measure the pronunciation quality and provide diagnostic\nfeedback. Experimental results show that Siamese recurrent network\nwith a combination of speech attribute features and phone posterior\nfeatures could attain an optimal detection accuracy of 0.941.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2320",
      "author_area_id": 1,
      "author_area_label": "Speech Perception, Production and Acquisition"
    },
    "korzekwa19_interspeech": {
      "authors": [
        [
          "Daniel",
          "Korzekwa"
        ],
        [
          "Roberto",
          "Barra-Chicote"
        ],
        [
          "Bozena",
          "Kostek"
        ],
        [
          "Thomas",
          "Drugman"
        ],
        [
          "Mateusz",
          "Lajszczak"
        ]
      ],
      "title": "Interpretable Deep Learning Model for the Detection and Reconstruction of Dysarthric Speech",
      "original": "1206",
      "page_count": 5,
      "order": 818,
      "p1": "3890",
      "pn": "3894",
      "abstract": [
        "We present a novel deep learning model for the detection and reconstruction\nof dysarthric speech. We train the model with a multi-task learning\ntechnique to jointly solve dysarthria detection and speech reconstruction\ntasks. The model key feature is a low-dimensional latent space that\nis meant to encode the properties of dysarthric speech. It is commonly\nbelieved that neural networks are black boxes that solve problems but\ndo not provide interpretable outputs. On the contrary, we show that\nthis latent space successfully encodes interpretable characteristics\nof dysarthria, is effective at detecting dysarthria, and that manipulation\nof the latent space allows the model to reconstruct healthy speech\nfrom dysarthric speech. This work can help patients and speech pathologists\nto improve their understanding of the condition, lead to more accurate\ndiagnoses and aid in reconstructing healthy speech for afflicted patients.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1206",
      "author_area_id": 1,
      "author_area_label": "Speech Perception, Production and Acquisition"
    },
    "noufi19_interspeech": {
      "authors": [
        [
          "Camille",
          "Noufi"
        ],
        [
          "Adam C.",
          "Lammert"
        ],
        [
          "Daryush D.",
          "Mehta"
        ],
        [
          "James R.",
          "Williamson"
        ],
        [
          "Gregory",
          "Ciccarelli"
        ],
        [
          "Douglas",
          "Sturim"
        ],
        [
          "Jordan R.",
          "Green"
        ],
        [
          "Thomas F.",
          "Campbell"
        ],
        [
          "Thomas F.",
          "Quatieri"
        ]
      ],
      "title": "Vocal Biomarker Assessment Following Pediatric Traumatic Brain Injury: A Retrospective Cohort Study",
      "original": "1200",
      "page_count": 5,
      "order": 819,
      "p1": "3895",
      "pn": "3899",
      "abstract": [
        "Recommendations following pediatric traumatic brain injury (TBI) support\nthe integration of instrumental measurement to aid perceptual assessment\nin recovery and treatment plans. A comprehensive set of sensitive,\nrobust and non-invasive measurements is therefore essential in assessing\nvariations in speech characteristics over time following pediatric\nTBI. In this paper, we discuss a method for measuring changes in the\nspeech patterns of a pediatric cohort of ten subjects diagnosed with\nsevere TBI. We apply a diverse set of both well-known and novel feature\nmeasurements to child speech recorded throughout the year following\ndiagnosis. We analyze these features individually and by speech subsystem\nfor each subject as well as for the entire cohort. In children older\nthan 72 months, we find highly significant (p &#60; 0.01) increases\nin pitch variation and number of unique phonemes spoken, shortened\npause length, and steadying articulation rate variability. Younger\nchildren exhibit similar steadied rate variability alongside an increase\nin articulation complexity. Nearly all speech features significantly\nchange (p &#60; 0.05) for the cohort as a whole, confirming that acoustic\nmeasures expanding upon perceptual assessment are needed to identify\nefficacious treatment targets for speech therapy following TBI.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1200",
      "author_area_id": 1,
      "author_area_label": "Speech Perception, Production and Acquisition"
    },
    "scharenborg19b_interspeech": {
      "authors": [
        [
          "Odette",
          "Scharenborg"
        ]
      ],
      "title": "Survey Talk: Reaching Over the Gap: Cross- and Interdisciplinary Research on Human and Automatic Speech Processing",
      "original": "abs25",
      "page_count": 0,
      "order": 820,
      "p1": "0",
      "pn": "",
      "abstract": [
        "The fields of human speech recognition (HSR) and automatic speech recognition\n(ASR) both investigate parts of the speech recognition process and\nhave word recognition as their central issue. Although the research\nfields appear closely related, their aims and research methods are\nquite different. Despite these differences there is, however, in the\npast two decades a growing interest in possible cross-fertilization.\nResearchers from both ASR and HSR are realizing the potential benefit\nof looking at the research field on the other side of the &#8216;gap&#8217;.\nIn this survey talk, I will provide an overview of past and present\nefforts to link human and automatic speech recognition research and\npresent an overview of the literature describing the performance difference\nbetween machines and human listeners. The focus of the talk is on the\nmutual benefits to be derived from establishing closer collaborations\nand knowledge interchange between ASR and HSR.\n"
      ]
    },
    "ogawa19_interspeech": {
      "authors": [
        [
          "Atsunori",
          "Ogawa"
        ],
        [
          "Marc",
          "Delcroix"
        ],
        [
          "Shigeki",
          "Karita"
        ],
        [
          "Tomohiro",
          "Nakatani"
        ]
      ],
      "title": "Improved Deep Duel Model for Rescoring N-Best Speech Recognition List Using Backward LSTMLM and Ensemble Encoders",
      "original": "1949",
      "page_count": 5,
      "order": 821,
      "p1": "3900",
      "pn": "3904",
      "abstract": [
        "We have proposed a neural network (NN) model called a deep duel model\n(DDM) for rescoring N-best speech recognition hypothesis lists. A DDM\nis composed of a long short-term memory (LSTM)-based encoder followed\nby a fully-connected linear layer-based binary-class classifier. Given\nthe feature vector sequences of two hypotheses in an N-best list, the\nDDM encodes the features and selects the hypothesis that has the lower\nword error rate (WER) based on the output binary-class probabilities.\nBy repeating this one-on-one hypothesis comparison (duel) for each\nhypothesis pair in the N-best list, we can find the oracle (lowest\nWER) hypothesis as the survivor of the duels. We showed that the DDM\ncan exploit the score provided by a forward LSTM-based recurrent NN\nlanguage model (LSTMLM) as an additional feature to accurately select\nthe hypotheses. In this study, we further improve the selection performance\nby introducing two modifications, i.e. adding the score provided by\na backward LSTMLM, which uses succeeding words to predict the current\nword, and employing ensemble encoders, which have a high feature encoding\ncapability. By combining these two modifications, our DDM achieves\nan over 10% relative WER reduction from a strong baseline obtained\nusing both the forward and backward LSTMLMs.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1949",
      "author_area_id": 9,
      "author_area_label": "Speech Recognition: Architecture, Search, and Linguistic Components"
    },
    "irie19b_interspeech": {
      "authors": [
        [
          "Kazuki",
          "Irie"
        ],
        [
          "Albert",
          "Zeyer"
        ],
        [
          "Ralf",
          "Schl\u00fcter"
        ],
        [
          "Hermann",
          "Ney"
        ]
      ],
      "title": "Language Modeling with Deep Transformers",
      "original": "2225",
      "page_count": 5,
      "order": 822,
      "p1": "3905",
      "pn": "3909",
      "abstract": [
        "We explore deep autoregressive Transformer models in language modeling\nfor speech recognition. We focus on two aspects. First, we revisit\nTransformer model configurations specifically for language modeling.\nWe show that well configured Transformer models outperform our baseline\nmodels based on the shallow stack of LSTM recurrent neural network\nlayers. We carry out experiments on the open-source LibriSpeech 960hr\ntask, for both 200K vocabulary word-level and 10K byte-pair encoding\nsubword-level language modeling. We apply our word-level models to\nconventional hybrid speech recognition by lattice rescoring, and the\nsubword-level models to attention based encoder-decoder models by shallow\nfusion. Second, we show that deep Transformer language models do not\nrequire positional encoding. The positional encoding is an essential\naugmentation for the self-attention mechanism which is invariant to\nsequence ordering. However, in autoregressive setup, as is the case\nfor language modeling, the amount of information increases along the\nposition dimension, which is a positional signal by its own. The analysis\nof attention weights shows that deep autoregressive self-attention\nmodels can automatically make use of such positional information. We\nfind that removing the positional encoding even slightly improves the\nperformance of these models.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2225",
      "author_area_id": 9,
      "author_area_label": "Speech Recognition: Architecture, Search, and Linguistic Components"
    },
    "raju19_interspeech": {
      "authors": [
        [
          "Anirudh",
          "Raju"
        ],
        [
          "Denis",
          "Filimonov"
        ],
        [
          "Gautam",
          "Tiwari"
        ],
        [
          "Guitang",
          "Lan"
        ],
        [
          "Ariya",
          "Rastrow"
        ]
      ],
      "title": "Scalable Multi Corpora Neural Language Models for ASR",
      "original": "3060",
      "page_count": 5,
      "order": 823,
      "p1": "3910",
      "pn": "3914",
      "abstract": [
        "Neural language models (NLM) have been shown to outperform conventional\nn-gram language models by a substantial margin in Automatic Speech\nRecognition (ASR) and other tasks. There are, however, a number of\nchallenges that need to be addressed for an NLM to be used in a practical\nlarge-scale ASR system. In this paper, we present solutions to some\nof the challenges, including training NLM from heterogenous corpora,\nlimiting latency impact and handling personalized bias in the second-pass\nrescorer. Overall, we show that we can achieve a 6.2% relative WER\nreduction using neural LM in a second-pass n-best rescoring framework\nwith a minimal increase in latency.\n"
      ],
      "doi": "10.21437/Interspeech.2019-3060",
      "author_area_id": 9,
      "author_area_label": "Speech Recognition: Architecture, Search, and Linguistic Components"
    },
    "likhomanenko19_interspeech": {
      "authors": [
        [
          "Tatiana",
          "Likhomanenko"
        ],
        [
          "Gabriel",
          "Synnaeve"
        ],
        [
          "Ronan",
          "Collobert"
        ]
      ],
      "title": "Who Needs Words? Lexicon-Free Speech Recognition",
      "original": "3107",
      "page_count": 5,
      "order": 824,
      "p1": "3915",
      "pn": "3919",
      "abstract": [
        "Lexicon-free speech recognition naturally deals with the problem of\nout-of-vocabulary (OOV) words. In this paper, we show that character-based\nlanguage models (LM) can perform as well as word-based LMs for speech\nrecognition, in word error rates (WER), even without restricting the\ndecoding to a lexicon. We study character-based LMs and show that convolutional\nLMs can effectively leverage large (character) contexts, which is key\nfor good speech recognition performance downstream. We specifically\nshow that the lexicon-free decoding performance (WER) on utterances\nwith OOV words using character-based LMs is better than lexicon-based\ndecoding, both with character or word-based LMs.\n"
      ],
      "doi": "10.21437/Interspeech.2019-3107",
      "author_area_id": 9,
      "author_area_label": "Speech Recognition: Architecture, Search, and Linguistic Components"
    },
    "latif19_interspeech": {
      "authors": [
        [
          "Siddique",
          "Latif"
        ],
        [
          "Rajib",
          "Rana"
        ],
        [
          "Sara",
          "Khalifa"
        ],
        [
          "Raja",
          "Jurdak"
        ],
        [
          "Julien",
          "Epps"
        ]
      ],
      "title": "Direct Modelling of Speech Emotion from Raw Speech",
      "original": "3252",
      "page_count": 5,
      "order": 825,
      "p1": "3920",
      "pn": "3924",
      "abstract": [
        "Speech emotion recognition is a challenging task and heavily depends\non hand-engineered acoustic features, which are typically crafted to\necho human perception of speech signals. However, a filter bank that\nis designed from perceptual evidence is not always guaranteed to be\nthe best in a statistical modelling framework where the end goal is\nfor example emotion classification. This has fuelled the emerging trend\nof learning representations from raw speech especially using deep learning\nneural networks. In particular, a combination of Convolution Neural\nNetworks (CNNs) and Long Short Term Memory (LSTM) have gained great\ntraction for the intrinsic property of LSTM in learning contextual\ninformation crucial for emotion recognition; and CNNs been used for\nits ability to overcome the scalability problem of regular neural networks.\nIn this paper, we show that there are still opportunities to improve\nthe performance of emotion recognition from the raw speech by exploiting\nthe properties of CNN in modelling contextual information. We propose\nthe use of parallel convolutional layers to harness multiple temporal\nresolutions in the feature extraction block that is jointly trained\nwith the LSTM based classification network for the emotion recognition\ntask. Our results suggest that the proposed model can reach the performance\nof CNN trained with hand-engineered features from both IEMOCAP and\nMSP-IMPROV datasets.\n"
      ],
      "doi": "10.21437/Interspeech.2019-3252",
      "author_area_id": 3,
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "sarma19_interspeech": {
      "authors": [
        [
          "Mousmita",
          "Sarma"
        ],
        [
          "Pegah",
          "Ghahremani"
        ],
        [
          "Daniel",
          "Povey"
        ],
        [
          "Nagendra Kumar",
          "Goel"
        ],
        [
          "Kandarpa Kumar",
          "Sarma"
        ],
        [
          "Najim",
          "Dehak"
        ]
      ],
      "title": "Improving Emotion Identification Using Phone Posteriors in Raw Speech Waveform Based DNN",
      "original": "2093",
      "page_count": 5,
      "order": 826,
      "p1": "3925",
      "pn": "3929",
      "abstract": [
        "We propose to exploit phone posteriors as an additional feature in\nDeep Neural Network (DNN) to recognize emotions from raw speech waveform.\nThe proposed DNN setup uses a time domain approach of learning filters\nwithin the network. The frame-level phone posteriors are combined with\nthe learned feature representation through the network. Appended learned\ntime domain features and phone posteriors are used as an input to the\ntemporal context modeling layers which interleaves TDNN-LSTM with time-restricted\nself-attention. We achieve 16.48% relative error rate improvement in\nIEMOCAP categorical problem (with a final weighted accuracy of 75.03%)\nusing phone posteriors compared to DNN setup which uses only learned\ntime domain features for temporal context modeling. Further, we study\nthe effect of learning emotion categories leveraging dimensional primitives\nin multi-task learning DNN model.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2093",
      "author_area_id": 3,
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "cao19_interspeech": {
      "authors": [
        [
          "Miao",
          "Cao"
        ],
        [
          "Chun",
          "Yang"
        ],
        [
          "Fang",
          "Zhou"
        ],
        [
          "Xu-cheng",
          "Yin"
        ]
      ],
      "title": "Pyramid Memory Block and Timestep Attention for Speech Emotion Recognition",
      "original": "3140",
      "page_count": 5,
      "order": 827,
      "p1": "3930",
      "pn": "3934",
      "abstract": [
        "As a sequence model, Deep Feedforward Sequential Memory Network (DFSMN)\nhas shown superior performance on many tasks, such as language modeling\nand speech recognition. Based on this work, we propose an improved\nspeech emotion recognition (SER) end-to-end system. Our model comprises\nboth CNN layers and pyramid FSMN layers, where CNN layers are added\nat the front of the network to extract more sophisticated features.\nA timestep attention mechanism is also integrated into our SER system,\nwhich makes the system learn how to focus on the more robust or informative\nsegments in the input signal. Furthermore, different from traditional\nSER systems, the proposed model is applied directly to spectrograms\nwhich contain more raw speech information, rather than well-established\nhand-crafted speech features such as spectral, cepstral and pitch.\nFinally, we evaluate our system on the Interactive Emotional Motion\nCapture (IEMOCAP) database. The experimental results show that our\nsystem achieves 2.67% improvement compared to the commonly used CNN-biLSTM\nmodel which requires much more computing resource.\n"
      ],
      "doi": "10.21437/Interspeech.2019-3140",
      "author_area_id": 3,
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "oates19_interspeech": {
      "authors": [
        [
          "Christopher",
          "Oates"
        ],
        [
          "Andreas",
          "Triantafyllopoulos"
        ],
        [
          "Ingmar",
          "Steiner"
        ],
        [
          "Bj\u00f6rn W.",
          "Schuller"
        ]
      ],
      "title": "Robust Speech Emotion Recognition Under Different Encoding Conditions",
      "original": "1658",
      "page_count": 5,
      "order": 828,
      "p1": "3935",
      "pn": "3939",
      "abstract": [
        "In an era where large speech corpora annotated for emotion are hard\nto come by, and especially ones where emotion is expressed freely instead\nof being acted, the importance of using free online sources for collecting\nsuch data cannot be overstated. Most of those sources, however, contain\nencoded audio due to storage and bandwidth constraints, often in very\nlow bitrates. In addition, with the increased industry interest on\nvoice-based applications, it is inevitable that speech emotion recognition\n(SER) algorithms will soon find their way into production environments,\nwhere the audio might be encoded in a different bitrate than the one\navailable during training. Our contribution is threefold. First, we\nshow that encoded audio still contains enough relevant information\nfor robust SER. Next, we investigate the effects of mismatched encoding\nconditions in the training and test set both for traditional machine\nlearning algorithms built on hand-crafted features and modern end-to-end\nmethods. Finally, we investigate the robustness of those algorithms\nin the multi-condition scenario, where the training set is augmented\nwith encoded audio, but still differs from the training set. Our results\nindicate that end-to-end methods are more robust even in the more challenging\nscenario of mismatched conditions.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1658",
      "author_area_id": 3,
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "gosztolya19c_interspeech": {
      "authors": [
        [
          "G\u00e1bor",
          "Gosztolya"
        ]
      ],
      "title": "Using the Bag-of-Audio-Word Feature Representation of ASR DNN Posteriors for Paralinguistic Classification",
      "original": "1163",
      "page_count": 5,
      "order": 829,
      "p1": "3940",
      "pn": "3944",
      "abstract": [
        "The Bag-of-Audio-Word (or BoAW) representation is an utterance-level\nfeature representation approach that was successfully applied in the\npast in various computational paralinguistic tasks. Here, we extend\nthe BoAW feature extraction process with the use of Deep Neural Networks:\nfirst we train a DNN acoustic model on an acoustic dataset consisting\nof 22 hours of speech for phoneme identification, then we evaluate\nthis DNN on a standard paralinguistic dataset. To construct utterance-level\nfeatures from the frame-level posterior vectors, we calculate their\nBoAW representation. We found that this approach can be utilized even\non its own, although the results obtained lag behind those of the standard\nparalinguistic approach, and the optimal size of the extracted feature\nvectors tends to be large. Our approach, however, can be easily and\nefficiently combined with the standard paralinguistic one, resulting\nin the highest Unweighted Average Recall (UAR) score achieved so far\nfor a general paralinguistic dataset.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1163",
      "author_area_id": 3,
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "williams19c_interspeech": {
      "authors": [
        [
          "Jennifer",
          "Williams"
        ],
        [
          "Simon",
          "King"
        ]
      ],
      "title": "Disentangling Style Factors from Speaker Representations",
      "original": "1769",
      "page_count": 5,
      "order": 830,
      "p1": "3945",
      "pn": "3949",
      "abstract": [
        "Our goal is to separate out speaking style from speaker identity in\nutterance-level representations of speech such as i-vectors and x-vectors.\nWe first show that both i-vectors and x-vectors contain information\nnot only about speaker but also about speaking style (for one data\nset) or emotion (for another data set), even when projected into a\nlow-dimensional space. To disentangle these factors, we use an autoencoder\nin which the latent space is split into two subspaces. The entangled\ninformation about speaker and style/emotion is pushed apart by the\nuse of auxiliary classifiers that take one of the two latent subspaces\nas input and that are jointly learned with the autoencoder. We evaluate\nhow well the latent subspaces separate the factors by using them as\ninput to separate style/emotion classification tasks. In traditional\nspeaker identification tasks, speaker-invariant characteristics are\nfactorized from channel and then the channel information is ignored.\nOur results suggest that this so-called channel may contain exploitable\ninformation, which we refer to as  style factors. Finally, we propose\nfuture work to use information theory to formalize  style factors in\nthe context of speaker identity.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1769",
      "author_area_id": 3,
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "hsu19c_interspeech": {
      "authors": [
        [
          "Yu-Yin",
          "Hsu"
        ],
        [
          "Anqi",
          "Xu"
        ]
      ],
      "title": "Sentence Prosody and  Wh-Indeterminates in Taiwan Mandarin",
      "original": "2545",
      "page_count": 5,
      "order": 831,
      "p1": "3950",
      "pn": "3954",
      "abstract": [
        "We report results of a speech production experiment about the intonation\nof three sentence types in Taiwan Mandarin, and discuss our results\nwith implications for focus acoustics, and semantic-syntactic theories\nof sentence final particles and  wh-indeterminates.  Wh-indeterminates\nrefer to  wh-phrases that are ambiguous between interrogative and indefinite\nreadings. In Mandarin, different interpretations of  wh-indeterminates\nare not morphologically marked, but can be disambiguated in specific\nsentence contexts marked by sentence final particles. In this study,\nwe systematically examined the intonation of  wh-questions and  yes/no\nquestions by using declarative sentences as the baseline. The results\nshow that both  wh- and  yes/no questions exhibit F0 prominence, and\nlengthening effects on regions containing sentence-final particles\nand  wh-phrases, but the effects were stronger in  wh-questions. Examining\nthe duration and F0 range, we found that  wh-phrases and sentence final\nparticles together formed specific acoustic patterns to distinguish\nquestions from declarative sentences. The findings suggest that the\nprosodic organization interacts with other internal structural organization.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2545",
      "author_area_id": 2,
      "author_area_label": "Phonetics, Phonology, and Prosody"
    },
    "hu19d_interspeech": {
      "authors": [
        [
          "Fang",
          "Hu"
        ],
        [
          "Youjue",
          "He"
        ]
      ],
      "title": "Frication as a Vowel Feature? &#8212; Evidence from the Rui&#8217;an Wu Chinese Dialect",
      "original": "1134",
      "page_count": 5,
      "order": 832,
      "p1": "3955",
      "pn": "3959",
      "abstract": [
        "Frication is not a common feature in characterizing vowels. However,\nChinese dialects are known for having apical vowels. Additionally,\nthere are fricative high vowels in a few dialects. This paper describes\nthe phonetics and phonology of the vowels in the Rui&#8217;an Wu Chinese\ndialect, with an emphasis on vowel features distinguishing the high\nvowels. Rui&#8217;an has 12 monophthongs [i y &#649; e &#248; &#949;\n a &#x27f; &#596; o u &#x26F;]; and half of them [i y &#649; &#x27f;\n u &#x26F;] are high vowels. Formant data from 10 native speakers,\n5 male and 5 female, were analyzed. And acoustic results reveal that\n[&#x27f;] is an apical vowel with significantly higher frication than\nother high vowels, whereas the difference in frication between [&#649;\n&#x26F;] and [y u] respectively is not confirmed. Rather, spectral\ndifference plays a more important role in the distinction between labiodental\nhigh vowels [&#649; &#x26F;] and their plain rounded counterparts [y\nu].\n"
      ],
      "doi": "10.21437/Interspeech.2019-1134",
      "author_area_id": 2,
      "author_area_label": "Phonetics, Phonology, and Prosody"
    },
    "zhang19m_interspeech": {
      "authors": [
        [
          "Zhenrui",
          "Zhang"
        ],
        [
          "Fang",
          "Hu"
        ]
      ],
      "title": "Vowels and Diphthongs in the Xupu Xiang Chinese Dialect",
      "original": "1174",
      "page_count": 5,
      "order": 833,
      "p1": "3960",
      "pn": "3964",
      "abstract": [
        "Based on an acoustic analysis of speech data from 10 speakers, 5 male\nand 5 female, this paper describes the phonetics and phonology of the\nvowels and diphthongs in the Xupu Xiang Chinese dialect. Results suggest\nthat monophthongs and falling diphthongs should be grouped together,\nsince the production of them is a single articulatory event. Falling\ndiphthongs are composed of a dynamic spectral target, while monophthongs\nare composed of a static spectral target. But rising diphthongs are\nsequences of two spectral targets.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1174",
      "author_area_id": 2,
      "author_area_label": "Phonetics, Phonology, and Prosody"
    },
    "albuquerque19_interspeech": {
      "authors": [
        [
          "Luciana",
          "Albuquerque"
        ],
        [
          "Catarina",
          "Oliveira"
        ],
        [
          "Ant\u00f3nio",
          "Teixeira"
        ],
        [
          "Pedro",
          "Sa-Couto"
        ],
        [
          "Daniela",
          "Figueiredo"
        ]
      ],
      "title": "Age-Related Changes in European Portuguese Vowel Acoustics",
      "original": "1818",
      "page_count": 5,
      "order": 834,
      "p1": "3965",
      "pn": "3969",
      "abstract": [
        "This study addresses effects of age and gender on acoustics of European\nPortuguese oral vowels, given to the fact of conflicting findings reported\nin prior research. Fundamental frequency (F0), formant frequencies\n(F1 and F2) and duration of vowels produced by a group of 113 adults,\naged between 35 and 97 years old, were measured. Vowel space area (VSA)\naccording to gender and age was also analysed. The results revealed\nthat the most consistent age-related effect was an increase in vowel\nduration in both genders. F0 decreases above [50&#8211;64] for female\nand for male data suggests a slight drop over the age range [35&#8211;64]\nand then an increase in an older age. That is, F0 tends to be closer\nbetween genders as age increases. In general, there is no evidence\nthat F1 and F2 frequencies were lowering as age increased. Furthermore,\nthere were no changes to VSA with ageing. These results provide a base\nof information to establish vowel acoustics normal patterns of ageing\namong Portuguese adults.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1818",
      "author_area_id": 2,
      "author_area_label": "Phonetics, Phonology, and Prosody"
    },
    "lalhminghlui19_interspeech": {
      "authors": [
        [
          "Wendy",
          "Lalhminghlui"
        ],
        [
          "Viyazonuo",
          "Terhiija"
        ],
        [
          "Priyankoo",
          "Sarmah"
        ]
      ],
      "title": "Vowel-Tone Interaction in Two Tibeto-Burman Languages",
      "original": "2808",
      "page_count": 5,
      "order": 835,
      "p1": "3970",
      "pn": "3974",
      "abstract": [
        "Intrinsic F0 (IF0) has been considered a phonetic phenomenon that has\na physiological basis. However, considering cross linguistic variation\nin IF0, it is also assumed that there is an amount of speaker intended\ncontrol on IF0. This work looks into the two tone languages spoken\nin North East India and confirms the evidence of IF0 in the languages.\nHowever, it also shows that as soon as speakers exert control over\nF0 for tone production, IF0 differences diminish. As previously reported,\nin this study too, IF0 differences were noticed to be more pronounced\nin the higher F0 regions.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2808",
      "author_area_id": 2,
      "author_area_label": "Phonetics, Phonology, and Prosody"
    },
    "rodriguez19_interspeech": {
      "authors": [
        [
          "Jenifer Vega",
          "Rodr\u00edguez"
        ]
      ],
      "title": "The Vowel System of Korebaju",
      "original": "3210",
      "page_count": 5,
      "order": 836,
      "p1": "3975",
      "pn": "3979",
      "abstract": [
        "Korebaju [k&#242;r&#232;&#x3B2;&#225;h&#x1D7C;&#x300;] (ISO 639-3:\ncoe) is a Western Tukanoan language from the South-Western part of\nColombia. A study conducted in 2017 and 2018 with six native speakers\n(3 female and 3 male) shows that Korebaju has an inventory of 17 consonants\n/p, t, k, p<SUP>h</SUP>, t<SUP>h</SUP>, k<SUP>h</SUP>, &#x3B2;, &#x278;,\ns, h, t&#643;&#865;, m, n, &#x272;, <SUP>h</SUP>m, <SUP>h</SUP>&#x014B;,\nr/ and 6 oral vowels /i, e, a, o, u, &#616;/, 6 nasal vowels /&#297;,\n&#x1EBD;, &#227;, &#245;, &#361;, &#x1D7C;&#x303;/ and 3 glottal vowels\n/a&#x2C0;, e&#x2C0;, o&#x2C0;/. Contrary to previous studies, this\npaper shows that Korebaju does not include the vowel [&#x26F;] in its\nphonemic inventory. The vowel [&#x26F;] is an allophone of the high\nback vowel /u/ when it follows a palatal consonant. In the same context,\nthe high central vowel /&#616;/ also has an allophone [&#618;]. This\npaper focuses on an acoustic and articulatory description. Data come\nfrom a set of words recorded with synchronized audio and EGG signals.\n"
      ],
      "doi": "10.21437/Interspeech.2019-3210",
      "author_area_id": 2,
      "author_area_label": "Phonetics, Phonology, and Prosody"
    },
    "ibrahim19_interspeech": {
      "authors": [
        [
          "Omnia",
          "Ibrahim"
        ],
        [
          "Gabriel",
          "Skantze"
        ],
        [
          "Sabine",
          "Stoll"
        ],
        [
          "Volker",
          "Dellwo"
        ]
      ],
      "title": "Fundamental Frequency Accommodation in Multi-Party Human-Robot Game Interactions: The Effect of Winning or Losing",
      "original": "2496",
      "page_count": 5,
      "order": 837,
      "p1": "3980",
      "pn": "3984",
      "abstract": [
        "In human-human interactions, the situational context plays a large\nrole in the degree of speakers&#8217; accommodation. In this paper,\nwe investigate whether the degree of accommodation in a human-robot\ncomputer game is affected by (a) the duration of the interaction and\n(b) the success of the players in the game. 30 teams of two players\nplayed two card games with a conversational robot in which they had\nto find a correct order of five cards. After game 1, the players received\nthe result of the game on a success scale from 1 (lowest success) to\n5 (highest). Speakers&#8217; f<SUB>o</SUB> accommodation was measured\nas the Euclidean distance between the human speakers and each human\nand the robot. Results revealed that (a) the duration of the game had\nno influence on the degree of f<SUB>o</SUB> accommodation and (b) the\nresult of Game 1 correlated with the degree of f<SUB>o</SUB> accommodation\nin Game 2 (higher success equals lower Euclidean distance). We argue\nthat game success is most likely considered as a sign of the success\nof players&#8217; cooperation during the discussion, which leads to\na higher accommodation behavior in speech.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2496",
      "author_area_id": 11,
      "author_area_label": "Spoken dialog systems and conversational analysis"
    },
    "wagner19_interspeech": {
      "authors": [
        [
          "Petra",
          "Wagner"
        ],
        [
          "Nataliya",
          "Bryhadyr"
        ],
        [
          "Marin",
          "Schr\u00f6er"
        ]
      ],
      "title": "Pitch Accent Trajectories Across Different Conditions of Visibility and Information Structure &#8212; Evidence from Spontaneous Dyadic Interaction",
      "original": "1619",
      "page_count": 5,
      "order": 838,
      "p1": "3985",
      "pn": "3989",
      "abstract": [
        "Previous research identified a differential contribution of information\nstructure and the visibility of facial and contextual information to\nthe acoustic-prosodic expression of pitch accents. However, it is unclear\nwhether pitch accent shapes are affected by these conditions as well.\nTo investigate whether varying context cues have a differentiated impact\non pitch accent trajectories produced in conversational interaction,\nwe modified the visibility conditions in a spontaneous dyadic interaction\ntask, i.e. a verbalized version of TicTacToe. Besides varying visibility,\nthe game task allows for measuring the impact of information-structure\non pitch accent trajectories, differentiating important and unpredictable\ngame moves. Using GAMMs on four speaker groups (identified by a cluster\nanalysis), we could isolate varying strategies of prosodic adaptation\nto contextual change. While few speaker groups showed a reaction to\nthe availability of visible context cues (facial prosody or executed\ngame moves), all groups differentiated the verbalization of unpredictable\nand predictable game moves with a group-specific trajectory adaptation.\nThe importance of game moves resulted in differentiated adaptations\nin two out of four speaker groups. The detected strategic trajectory\nadaptations were characterized by different characteristics of boundary\ntones, adaptations of the global f0-level, or the shape of the corresponding\npitch accent.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1619",
      "author_area_id": 11,
      "author_area_label": "Spoken dialog systems and conversational analysis"
    },
    "betz19_interspeech": {
      "authors": [
        [
          "Simon",
          "Betz"
        ],
        [
          "Sina",
          "Zarrie\u00df"
        ],
        [
          "\u00c9va",
          "Sz\u00e9kely"
        ],
        [
          "Petra",
          "Wagner"
        ]
      ],
      "title": "The Greennn Tree &#8212; Lengthening Position Influences Uncertainty Perception",
      "original": "2572",
      "page_count": 5,
      "order": 839,
      "p1": "3990",
      "pn": "3994",
      "abstract": [
        "Synthetic speech can be used to express uncertainty in dialogue systems\nby means of hesitation. If a phrase like &#8220;Next to the green tree&#8221;\nis uttered in a hesitant way, that is, containing lengthening, silences,\nand fillers, the listener can infer that the speaker is not certain\nabout the concepts referred to. However, we do not know anything about\nthe referential domain of the uncertainty; if only a particular word\nin this sentence would be uttered hesitantly, e.g. &#8220;the greee:n\ntree&#8221;, the listener could infer that the uncertainty refers to\nthe color in the statement, but not to the object. In this study, we\nshow that the domain of the uncertainty is controllable. We conducted\nan experiment in which color words in sentences like &#8220;search\nfor the green tree&#8221; were lengthened in two different positions:\nword onsets or final consonants, and participants were asked to rate\nthe uncertainty regarding color and object. The results show that initial\nlengthening is predominantly associated with uncertainty about the\nword itself, whereas final lengthening is primarily associated with\nthe following object. These findings enable dialogue system developers\nto finely control the attitudinal display of uncertainty, adding nuances\nbeyond the lexical content to message delivery.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2572",
      "author_area_id": 11,
      "author_area_label": "Spoken dialog systems and conversational analysis"
    },
    "si19_interspeech": {
      "authors": [
        [
          "Yuke",
          "Si"
        ],
        [
          "Longbiao",
          "Wang"
        ],
        [
          "Jianwu",
          "Dang"
        ],
        [
          "Mengfei",
          "Wu"
        ],
        [
          "Aijun",
          "Li"
        ]
      ],
      "title": "CNN-BLSTM Based Question Detection from Dialogs Considering Phase and Context Information",
      "original": "1701",
      "page_count": 5,
      "order": 840,
      "p1": "3995",
      "pn": "3999",
      "abstract": [
        "Question detection from dialogs is important in human-computer interaction\nsystems. Recent studies on question detection mostly use recurrent\nneural network (RNN) based methods to process low-level descriptors\n(LLD) of the utterance. However, there are three main problems in these\nstudies. Firstly, traditional LLD features are defined based on human\na priori knowledge, some of which are difficult to be extracted accurately.\nSecondly, previous studies of question detection only consider features\nfrom amplitude information and ignored phase information. Thirdly,\nprevious studies show that the context in an utterance is helpful to\ndetect question, while the context between utterances is not well investigated\nin this task. To cope with the aforementioned problems, we propose\na CNN-BLSTM based framework, where amplitude information is obtained\nfrom the combination of spectrogram and LLD, and processed together\nwith the phase information. Our framework also models the context information\nin the dialog. From the experiments on Mandarin dialog corpus, we revealed\nthe effectiveness of the integrated feature with both amplitude and\nphase in question detection. The results indicated that the phase feature\nwas helpful to detect the questions with a short duration, and the\ncontext between utterances was beneficial to detect questions without\nspecial interrogative forms.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1701",
      "author_area_id": 11,
      "author_area_label": "Spoken dialog systems and conversational analysis"
    },
    "metcalf19_interspeech": {
      "authors": [
        [
          "Katherine",
          "Metcalf"
        ],
        [
          "Barry-John",
          "Theobald"
        ],
        [
          "Garrett",
          "Weinberg"
        ],
        [
          "Robert",
          "Lee"
        ],
        [
          "Ing-Marie",
          "Jonsson"
        ],
        [
          "Russ",
          "Webb"
        ],
        [
          "Nicholas",
          "Apostoloff"
        ]
      ],
      "title": "Mirroring to Build Trust in Digital Assistants",
      "original": "1829",
      "page_count": 5,
      "order": 841,
      "p1": "4000",
      "pn": "4004",
      "abstract": [
        "We describe experiments towards building a conversational digital assistant\nthat considers the preferred conversational style of the user. In particular,\nthese experiments are designed to measure whether users prefer and\ntrust an assistant whose conversational style matches their own. To\nthis end we conducted a user study where subjects interacted with a\ndigital assistant whose response either matched their conversational\nstyle, or did not. We found that people strongly prefer a digital assistant\nthat mirrors their &#8220;chattiness&#8221; and that this preference\ncan be reliably detected.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1829",
      "author_area_id": 11,
      "author_area_label": "Spoken dialog systems and conversational analysis"
    },
    "raveh19_interspeech": {
      "authors": [
        [
          "Eran",
          "Raveh"
        ],
        [
          "Ingo",
          "Siegert"
        ],
        [
          "Ingmar",
          "Steiner"
        ],
        [
          "Iona",
          "Gessinger"
        ],
        [
          "Bernd",
          "M\u00f6bius"
        ]
      ],
      "title": "Three&#8217;s a Crowd? Effects of a Second Human on Vocal Accommodation with a Voice Assistant",
      "original": "1825",
      "page_count": 5,
      "order": 842,
      "p1": "4005",
      "pn": "4009",
      "abstract": [
        "This study examines how the presence of other speakers affects the\ninteraction with a spoken dialogue system. We analyze participants&#8217;\nspeech regarding several phonetic features, viz., fundamental frequency,\nintensity, and articulation rate, in two conditions: with and without\nadditional speech input from a human confederate as a third interlocutor.\nThe comparison was made via tasks performed by participants using a\ncommercial voice assistant under both conditions in alternation. We\ncompare the distributions of the features across the two conditions\nto investigate whether speakers behave differently when a confederate\nis involved. Temporal analysis exposes continuous changes in the feature\nproductions. In particular, we measured overall accommodation between\nthe participants and the system throughout the interactions. Results\nshow significant differences in a majority of cases for two of the\nthree features, which are more pronounced in cases where the user first\ninteracted with the device alone. We also analyze factors such as the\ntask performed, participant gender, and task order, providing additional\ninsight into the participants&#8217; behavior.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1825",
      "author_area_id": 11,
      "author_area_label": "Spoken dialog systems and conversational analysis"
    },
    "wang19o_interspeech": {
      "authors": [
        [
          "Qing",
          "Wang"
        ],
        [
          "Pengcheng",
          "Guo"
        ],
        [
          "Sining",
          "Sun"
        ],
        [
          "Lei",
          "Xie"
        ],
        [
          "John H.L.",
          "Hansen"
        ]
      ],
      "title": "Adversarial Regularization for End-to-End Robust Speaker Verification",
      "original": "2983",
      "page_count": 5,
      "order": 843,
      "p1": "4010",
      "pn": "4014",
      "abstract": [
        "Deep learning has been successfully used in speaker verification (SV),\nespecially in end-to-end SV systems which have attracted more interest\nrecently. It has been shown in image as well as speech applications\nthat deep neural networks are vulnerable to adversarial examples. In\nthis study, we explore two methods to generate adversarial examples\nfor advanced SV: (i) fast gradient-sign method (FGSM), and (ii) local\ndistributional smoothness (LDS) method. To explore this issue, we use\nadversarial examples to attack an end-to-end SV system. Experiments\nwill show that the neural network can be easily disturbed by adversarial\nexamples. Next, we propose to train an end-to-end robust SV model using\nthe two proposed adversarial examples for model regularization. Experimental\nresults with the TIMIT dataset indicate that the EER is improved relatively\nby (i) +18.89% and (ii) +5.54% for the original test set using the\nregularized model. In addition, the regularized model improves EER\nof the adversarial example test set by a relative (i) +30.11% and (ii)\n+22.12%, which therefore suggests more consistent performance against\nadversarial example attacks.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2983",
      "author_area_id": 4,
      "author_area_label": "Speaker and Language Identification"
    },
    "monteiro19_interspeech": {
      "authors": [
        [
          "Jo\u00e3o",
          "Monteiro"
        ],
        [
          "Jahangir",
          "Alam"
        ],
        [
          "Tiago H.",
          "Falk"
        ]
      ],
      "title": "Combining Speaker Recognition and Metric Learning for Speaker-Dependent Representation Learning",
      "original": "2974",
      "page_count": 5,
      "order": 844,
      "p1": "4015",
      "pn": "4019",
      "abstract": [
        "In this paper, we tackle automatic speaker verification under a text-independent\nsetting. Speaker modelling is performed by a deep convolutional neural\nnetwork on top of time-frequency speech representations. Convolutions\nperformed over the time dimension provide the means for the model to\ntake both short-term dependencies into account, given the nature of\nthe learned filters which operate over short-windows, as well as long-term\ndependencies, since depth in a convolutional stack implies dependency\nof outputs across large portions of input samples. Additionally, various\npooling strategies across the time dimension are compared so as to\neffectively map varying length recordings into fixed dimensional representations\nwhile simultaneously providing the neural network with an extra mechanism\nto model long-term dependencies. We finally propose a training scheme\nunder which well-known metric learning approaches, namely triplet loss\nminimization, is performed along with speaker recognition in a multi-class\nclassification setting. Evaluation on well-known datasets and comparisons\nwith state-of-the-art benchmarks show that the proposed setting is\neffective in yielding speaker-dependent representations, thus is well-suited\nfor voice biometrics downstream tasks.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2974",
      "author_area_id": 4,
      "author_area_label": "Speaker and Language Identification"
    },
    "zhang19n_interspeech": {
      "authors": [
        [
          "Yang",
          "Zhang"
        ],
        [
          "Lantian",
          "Li"
        ],
        [
          "Dong",
          "Wang"
        ]
      ],
      "title": "VAE-Based Regularization for Deep Speaker Embedding",
      "original": "2486",
      "page_count": 5,
      "order": 845,
      "p1": "4020",
      "pn": "4024",
      "abstract": [
        "Deep speaker embedding has achieved state-of-the-art performance in\nspeaker recognition. A potential problem of these embedded vectors\n(called &#8216;x-vectors&#8217;) are not Gaussian, causing performance\ndegradation with the famous PLDA back-end scoring. In this paper, we\npropose a regularization approach based on Variational Auto-Encoder\n(VAE). This model transforms x-vectors to a latent space where mapped\nlatent codes are more Gaussian, hence more suitable for PLDA scoring.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2486",
      "author_area_id": 4,
      "author_area_label": "Speaker and Language Identification"
    },
    "mingote19b_interspeech": {
      "authors": [
        [
          "Victoria",
          "Mingote"
        ],
        [
          "Diego",
          "Castan"
        ],
        [
          "Mitchell",
          "McLaren"
        ],
        [
          "Mahesh Kumar",
          "Nandwana"
        ],
        [
          "Alfonso",
          "Ortega"
        ],
        [
          "Eduardo",
          "Lleida"
        ],
        [
          "Antonio",
          "Miguel"
        ]
      ],
      "title": "Language Recognition Using Triplet Neural Networks",
      "original": "2437",
      "page_count": 5,
      "order": 846,
      "p1": "4025",
      "pn": "4029",
      "abstract": [
        "In this paper, we propose a novel neural network back-end approach\nbased on triplets for the language recognition task, due to its success\napplication in the related field of text-dependent speaker verification.\nA triplet is a training example constructed of three audio samples;\ntwo from the same class and one from a different class. In presenting\ntwo pairs of samples to the network, the triplet neural network learns\nto discriminate between samples from the same languages and pairs of\ndifferent languages. Triplet-based training optimizes the Area Under\nthe Curve (AUC) in contrast to other triplet loss functions proposed\nin the literature. The optimization of the AUC as cost function is\nappropriate for a detection task as it directly correlates with end-use\nperformance of the system. Moreover, we show the importance of defining\nan appropriate method of triplet selection and how this impacts performance\nof the system. When benchmarked on the LRE09 database, the new triplet\nbackend demonstrated superior performance compared to traditional back-ends\nused for language recognition. In addition, we performed an evaluation\non the LRE15 and LRE17 databases to check the generalization power\nof the proposed systems.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2437",
      "author_area_id": 4,
      "author_area_label": "Speaker and Language Identification"
    },
    "jung19c_interspeech": {
      "authors": [
        [
          "Youngmoon",
          "Jung"
        ],
        [
          "Younggwan",
          "Kim"
        ],
        [
          "Hyungjun",
          "Lim"
        ],
        [
          "Yeunju",
          "Choi"
        ],
        [
          "Hoirin",
          "Kim"
        ]
      ],
      "title": "Spatial Pyramid Encoding with Convex Length Normalization for Text-Independent Speaker Verification",
      "original": "2177",
      "page_count": 5,
      "order": 847,
      "p1": "4030",
      "pn": "4034",
      "abstract": [
        "In this paper, we propose a new pooling method called spatial pyramid\nencoding (SPE) to generate speaker embeddings for text-independent\nspeaker verification. We first partition the output feature maps from\na deep residual network (ResNet) into increasingly fine sub-regions\nand extract speaker embeddings from each sub-region through a learnable\ndictionary encoding layer. These embeddings are concatenated to obtain\nthe final speaker representation. The SPE layer not only generates\na fixed-dimensional speaker embedding for a variable-length speech\nsegment, but also aggregates the information of feature distribution\nfrom multi-level temporal bins. Furthermore, we apply deep length normalization\nby augmenting the loss function with ring loss. By applying ring loss,\nthe network gradually learns to normalize the speaker embeddings using\nmodel weights themselves while preserving convexity, leading to more\nrobust speaker embeddings. Experiments on the VoxCeleb1 dataset show\nthat the proposed system using the SPE layer and ring loss-based deep\nlength normalization outperforms both i-vector and d-vector baselines.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2177",
      "author_area_id": 4,
      "author_area_label": "Speaker and Language Identification"
    },
    "heo19b_interspeech": {
      "authors": [
        [
          "Hee-Soo",
          "Heo"
        ],
        [
          "Jee-weon",
          "Jung"
        ],
        [
          "IL-Ho",
          "Yang"
        ],
        [
          "Sung-Hyun",
          "Yoon"
        ],
        [
          "Hye-jin",
          "Shim"
        ],
        [
          "Ha-Jin",
          "Yu"
        ]
      ],
      "title": "End-to-End Losses Based on Speaker Basis Vectors and All-Speaker Hard Negative Mining for Speaker Verification",
      "original": "1986",
      "page_count": 5,
      "order": 848,
      "p1": "4035",
      "pn": "4039",
      "abstract": [
        "In recent years, speaker verification has primarily performed using\ndeep neural networks that are trained to output embeddings from input\nfeatures such as spectrograms or Mel-filterbank energies. Studies that\ndesign various loss functions, including metric learning have been\nwidely explored. In this study, we propose two end-to-end loss functions\nfor speaker verification using the concept of speaker bases, which\nare trainable parameters. One loss function is designed to further\nincrease the inter-speaker variation, and the other is designed to\nconduct the identical concept with hard negative mining. Each speaker\nbasis is designed to represent the corresponding speaker in the process\nof training deep neural networks. In contrast to the conventional loss\nfunctions that can consider only a limited number of speakers included\nin a mini-batch, the proposed loss functions can consider all the speakers\nin the training set regardless of the mini-batch composition. In particular,\nthe proposed loss functions enable hard negative mining and calculations\nof between-speaker variations with consideration of all speakers. Through\nexperiments on VoxCeleb1 and VoxCeleb2 datasets, we confirmed that\nthe proposed loss functions could supplement conventional softmax and\ncenter loss functions.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1986",
      "author_area_id": 4,
      "author_area_label": "Speaker and Language Identification"
    },
    "jiang19_interspeech": {
      "authors": [
        [
          "Yiheng",
          "Jiang"
        ],
        [
          "Yan",
          "Song"
        ],
        [
          "Ian",
          "McLoughlin"
        ],
        [
          "Zhifu",
          "Gao"
        ],
        [
          "Li-Rong",
          "Dai"
        ]
      ],
      "title": "An Effective Deep Embedding Learning Architecture for Speaker Verification",
      "original": "1606",
      "page_count": 5,
      "order": 849,
      "p1": "4040",
      "pn": "4044",
      "abstract": [
        "In this paper we present an effective deep embedding learning architecture,\nwhich combines a dense connection of dilated convolutional layers with\na gating mechanism, for speaker verification (SV) tasks. Compared with\nthe widely used time-delay neural network (TDNN) based architecture,\ntwo main improvements are proposed: (1) The dilated filters are designed\nto effectively capture time-frequency context information, then the\nconvolutional layer outputs are utilized for effective embedding learning.\nSpecifically, we employ the idea of the successful DenseNet to collect\nthe context information by dense connections from each layer to every\nother layer in a feed-forward fashion. (2) A gating mechanism is further\nintroduced to provide channel-wise attention by exploiting inter-dependencies\nacross channels. Motivated by squeeze-and-excitation networks (SENet),\nthe global time-frequency information is utilized for this feature\ncalibration. To evaluate the proposed network architecture, we conduct\nextensive experiments on noisy and unconstrained SV tasks, i.e., Speaker\nin the Wild (SITW) and Voxceleb1. The results demonstrate state-of-the-art\nSV performance. Specifically, our proposed method reduces equal error\nrate (EER) from TDNN based method by 25% and 27% for SITW and Voxceleb1,\nrespectively.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1606",
      "author_area_id": 4,
      "author_area_label": "Speaker and Language Identification"
    },
    "qin19b_interspeech": {
      "authors": [
        [
          "Xiaoyi",
          "Qin"
        ],
        [
          "Danwei",
          "Cai"
        ],
        [
          "Ming",
          "Li"
        ]
      ],
      "title": "Far-Field End-to-End Text-Dependent Speaker Verification Based on Mixed Training Data with Transfer Learning and Enrollment Data Augmentation",
      "original": "1542",
      "page_count": 5,
      "order": 850,
      "p1": "4045",
      "pn": "4049",
      "abstract": [
        "In this paper, we focus on the far-field end-to-end text-dependent\nspeaker verification task with a small-scale far-field text dependent\ndataset and a large scale close-talking text independent database for\ntraining. First, we show that simulating far-field text independent\ndata from the existing large-scale clean database for data augmentation\ncan reduce the mismatch. Second, using a small far-field text dependent\ndata set to finetune the deep speaker embedding model pre-trained from\nthe simulated far-field as well as original clean text independent\ndata can significantly improve the system performance. Third, in special\napplications when using the close-talking clean utterances for enrollment\nand employing the real far-field noisy utterances for testing, adding\nreverberant noises on the clean enrollment data can further enhance\nthe system performance. We evaluate our methods on AISHELL ASR0009\nand AISHELL 2019B-eval databases and achieve an equal error rate (EER)\nof 5.75% for far-field text-dependent speaker verification under noisy\nenvironments.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1542"
    },
    "ren19_interspeech": {
      "authors": [
        [
          "Zongze",
          "Ren"
        ],
        [
          "Guofu",
          "Yang"
        ],
        [
          "Shugong",
          "Xu"
        ]
      ],
      "title": "Two-Stage Training for Chinese Dialect Recognition",
      "original": "1522",
      "page_count": 5,
      "order": 851,
      "p1": "4050",
      "pn": "4054",
      "abstract": [
        "In this paper, we present a two-stage language identification (LID)\nsystem based on a shallow ResNet14 followed by a simple 2-layer recurrent\nneural network (RNN) architecture, which was used for Xunfei (iFlyTek)\nChinese Dialect Recognition Challenge and won the first place among\n110 teams. The system trains an acoustic model (AM) firstly with connectionist\ntemporal classification (CTC) to recognize the given phonetic sequence\nannotation and then train another RNN to classify dialect category\nby utilizing the intermediate features as inputs from the AM. Compared\nwith a three-stage system we further explore, our results show that\nthe two-stage system can achieve high accuracy for Chinese dialects\nrecognition under both short utterance and long utterance conditions\nwith less training time.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1522",
      "author_area_id": 4,
      "author_area_label": "Speaker and Language Identification"
    },
    "kaminishi19_interspeech": {
      "authors": [
        [
          "Ryota",
          "Kaminishi"
        ],
        [
          "Haruna",
          "Miyamoto"
        ],
        [
          "Sayaka",
          "Shiota"
        ],
        [
          "Hitoshi",
          "Kiya"
        ]
      ],
      "title": "Investigation on Blind Bandwidth Extension with a Non-Linear Function and its Evaluation of x-Vector-Based Speaker Verification",
      "original": "1510",
      "page_count": 5,
      "order": 852,
      "p1": "4055",
      "pn": "4059",
      "abstract": [
        "This study evaluates the effects of some non-learning blind bandwidth\nextension (BWE) methods on automatic speaker verification (ASV) systems\nbased on x-vector. Recently, a non-linear bandwidth extension (N-BWE)\nhas been proposed as a blind, non-learning, and light-weight BWE approach.\nOther non-learning BWEs have also been developed in recent years. For\nASV evaluations, most data available to train ASV systems is narrowband\n(NB) telephone speech. Meanwhile, wideband (WB) data have been used\nto train the state-of-the-art ASV systems, such as i-vector and x-vector.\nThis can cause sampling rate mismatches when all datasets are used.\nIn this paper, we investigate the influence of sampling rate mismatches\nin the x-vector-based ASV systems and how non-learning BWE methods\nperform against them. The results showed that the N-BWE method improved\nthe equal error rate (EER) on ASV systems based on x-vector when the\nmismatches were present. We researched the relationship between objective\nmeasurements and EERs. Consequently, the N-BWE method produced the\nlowest EER and obtained the lower RMS-LSD value and the higher STOI\nscore.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1510"
    },
    "khan19_interspeech": {
      "authors": [
        [
          "Umair",
          "Khan"
        ],
        [
          "Miquel",
          "India"
        ],
        [
          "Javier",
          "Hernando"
        ]
      ],
      "title": "Auto-Encoding Nearest Neighbor i-Vectors for Speaker Verification",
      "original": "1444",
      "page_count": 5,
      "order": 853,
      "p1": "4060",
      "pn": "4064",
      "abstract": [
        "In the last years, i-vectors followed by cosine or PLDA scoring techniques\nwere the state-of-the-art approach in speaker verification. PLDA requires\nlabeled background data, and there exists a significant performance\ngap between the two scoring techniques. In this work, we propose to\nreduce this gap by using an autoencoder to transform i-vector into\na new speaker vector representation, which will be referred to as ae-vector.\nThe autoencoder will be trained to reconstruct neighbor i-vectors instead\nof the same training i-vectors, as usual. These neighbor i-vectors\nwill be selected in an unsupervised manner according to the highest\ncosine scores to the training i-vectors. The evaluation is performed\non the speaker verification trials of VoxCeleb-1 database. The experiments\nshow that our proposed ae-vectors gain a relative improvement of 42%\nin terms of EER compared to the conventional i-vectors using cosine\nscoring, which fills the performance gap between cosine and PLDA scoring\ntechniques by 92%, but without using speaker labels.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1444",
      "author_area_id": 4,
      "author_area_label": "Speaker and Language Identification"
    },
    "zheng19b_interspeech": {
      "authors": [
        [
          "Siqi",
          "Zheng"
        ],
        [
          "Gang",
          "Liu"
        ],
        [
          "Hongbin",
          "Suo"
        ],
        [
          "Yun",
          "Lei"
        ]
      ],
      "title": "Towards a Fault-Tolerant Speaker Verification System: A Regularization Approach to Reduce the Condition Number",
      "original": "1442",
      "page_count": 5,
      "order": 854,
      "p1": "4065",
      "pn": "4069",
      "abstract": [
        "Large-scale deployment of speech interaction devices makes it possible\nto harvest tremendous data quickly, which also introduces the problem\nof wrong labeling during data mining. Mislabeled training data has\na substantial negative effect on the performance of speaker verification\nsystem. This study aims to enhance the generalization ability and robustness\nof the model when the training data is contaminated by wrong labels.\nSeveral regularization approaches are proposed to reduce the condition\nnumber of the speaker verification problem, making the model less sensitive\nto errors in the inputs. They are validated on both NIST SRE corpus\nand far-field smart speaker data. The results suggest that the performance\ndeterioration caused by mislabeled training data can be significantly\nameliorated by proper regularization.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1442",
      "author_area_id": 4,
      "author_area_label": "Speaker and Language Identification"
    },
    "taherian19_interspeech": {
      "authors": [
        [
          "Hassan",
          "Taherian"
        ],
        [
          "Zhong-Qiu",
          "Wang"
        ],
        [
          "DeLiang",
          "Wang"
        ]
      ],
      "title": "Deep Learning Based Multi-Channel Speaker Recognition in Noisy and Reverberant Environments",
      "original": "1428",
      "page_count": 5,
      "order": 855,
      "p1": "4070",
      "pn": "4074",
      "abstract": [
        "Despite successful applications of multi-channel signal processing\nin robust automatic speech recognition (ASR), relatively little research\nhas been conducted on the effectiveness of such techniques in the robust\nspeaker recognition domain. This paper introduces time-frequency (T-F)\nmasking-based beamforming to address text-independent speaker recognition\nin conditions where strong diffuse noise and reverberation are both\npresent. We examine various masking-based beamformers, such as parameterized\nmulti-channel Wiener filter, generalized eigenvalue (GEV) beamformer\nand minimum variance distortion-less response (MVDR) beamformer, and\nevaluate their performance in terms of speaker recognition accuracy\nfor i-vector and x-vector based systems. In addition, we present a\ndifferent formulation for estimating steering vectors from speech covariance\nmatrices. We show that rank-1 approximation of a speech covariance\nmatrix based on generalized eigenvalue decomposition leads to the best\nresults for the masking-based MVDR beamformer. Experiments on the recently\nintroduced NIST SRE 2010 retransmitted corpus show that the MVDR beamformer\nwith rank-1 approximation provides an absolute reduction of 5.55% in\nequal error rate compared to a standard masking-based MVDR beamformer.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1428",
      "author_area_id": 4,
      "author_area_label": "Speaker and Language Identification"
    },
    "yang19g_interspeech": {
      "authors": [
        [
          "Joon-Young",
          "Yang"
        ],
        [
          "Joon-Hyuk",
          "Chang"
        ]
      ],
      "title": "Joint Optimization of Neural Acoustic Beamforming and Dereverberation with x-Vectors for Robust Speaker Verification",
      "original": "1356",
      "page_count": 5,
      "order": 856,
      "p1": "4075",
      "pn": "4079",
      "abstract": [
        "In this paper, we investigate the deep neural network (DNN) supported\nacoustic beamforming and dereverberation as the front-end of the x-vector\nspeaker verification (SV) framework in a noisy and reverberant environment.\nFirstly, a DNN for supporting either the classical beamforming (e.\ng. MVDR) or the dereverberation (e. g. WPE) algorithm is trained on\nmulti-channel speech signals. Next, an x-vector speaker embedding network\nis trained on top of the enhanced speech features to classify the training\nspeakers. Finally, after the separate training stages are over, either\none or both of the DNN supported beamforming and dereverberation modules\nare serially connected to the x-vector network, and jointly trained\nto optimize the common objective of speaker classification. Experiments\non the artificially generated speech dataset using simulated and real\nroom impulse responses (RIRs) with various types of domestic noise\nsamples show that jointly training the supportive neural network models\nalong with the x-vector network within the classical speech enhancement\nframework brings significant performance gain for robust text-independent\n(TI) SV.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1356",
      "author_area_id": 4,
      "author_area_label": "Speaker and Language Identification"
    },
    "miao19b_interspeech": {
      "authors": [
        [
          "Xiaoxiao",
          "Miao"
        ],
        [
          "Ian",
          "McLoughlin"
        ],
        [
          "Yonghong",
          "Yan"
        ]
      ],
      "title": "A New Time-Frequency Attention Mechanism for TDNN and CNN-LSTM-TDNN, with Application to Language Identification",
      "original": "1256",
      "page_count": 5,
      "order": 857,
      "p1": "4080",
      "pn": "4084",
      "abstract": [
        "In this paper, we aim to improve traditional DNN x-vector language\nidentification (LID) performance by employing Convolutional and Long\nShort Term Memory-Recurrent (CLSTM) Neural Networks, as they can strengthen\nfeature extraction and capture longer temporal dependencies. We also\npropose a two-dimensional attention mechanism. Compared with conventional\none-dimensional time attention, our method introduces a frequency attention\nmechanism to give different weights to different frequency bands to\ngenerate weighted means and standard deviations. This mechanism can\ndirect attention to either time or frequency information, and can be\ntrained or fused singly or jointly. Experimental results show firstly\nthat CLSTM can significantly outperform a traditional DNN x-vector\nimplementation. Secondly, the proposed frequency attention method is\nmore effective than time attention, particularly when the number of\nfrequency bands matches the feature size. Furthermore, frequency-time\nscore merging is the best, whereas frequency-time feature merge only\nshows improvements for small frequency dimension.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1256",
      "author_area_id": 4,
      "author_area_label": "Speaker and Language Identification"
    },
    "chen19n_interspeech": {
      "authors": [
        [
          "Jun",
          "Chen"
        ],
        [
          "Ji",
          "Zhu"
        ],
        [
          "Jieping",
          "Ye"
        ]
      ],
      "title": "An Attention-Based Hybrid Network for Automatic Detection of Alzheimer&#8217;s Disease from Narrative Speech",
      "original": "2872",
      "page_count": 5,
      "order": 858,
      "p1": "4085",
      "pn": "4089",
      "abstract": [
        "Alzheimer&#8217;s disease (AD) is one of the leading causes of death\nin the world and affects at least 50 million individuals. Currently,\nthere is no cure for the disease. So a convenient and reliable early\ndetection approach before irreversible brain damage and cognitive decline\nhave occurred is of great importance. One prominent sign of AD is language\ndysfunction. Some aspects of language are affected at the same time\nor even before the memory problems emerge. Therefore, we propose an\nautomatic speech analysis framework to identify AD subjects from short\nnarrative speech transcript elicited with a picture description task.\nThe proposed network is based on attention mechanism and is composed\nof a CNN and a GRU module. We obtained state-of-the-art cross-validation\naccuracy of 97 in distinguishing individuals with AD from elderly normal\ncontrols. The performance of our model makes it reasonable to conclude\nthat our approach reveals a considerable part of the language deficits\nof AD patients and can help with the diagnosis of the disease from\nspontaneous speech.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2872",
      "author_area_id": 10,
      "author_area_label": "Speech Recognition: Technologies and Systems for New Applications"
    },
    "ma19b_interspeech": {
      "authors": [
        [
          "Pingchuan",
          "Ma"
        ],
        [
          "Stavros",
          "Petridis"
        ],
        [
          "Maja",
          "Pantic"
        ]
      ],
      "title": "Investigating the Lombard Effect Influence on End-to-End Audio-Visual Speech Recognition",
      "original": "2726",
      "page_count": 5,
      "order": 859,
      "p1": "4090",
      "pn": "4094",
      "abstract": [
        "Several audio-visual speech recognition models have been recently proposed\nwhich aim to improve the robustness over audio-only models in the presence\nof noise. However, almost all of them ignore the impact of the Lombard\neffect, i.e., the change in speaking style in noisy environments which\naims to make speech more intelligible and affects both the acoustic\ncharacteristics of speech and the lip movements. In this paper, we\ninvestigate the impact of the Lombard effect in audio-visual speech\nrecognition. To the best of our knowledge, this is the first work which\ndoes so using end-to-end deep architectures and presents results on\nunseen speakers. Our results show that properly modelling Lombard speech\nis always beneficial. Even if a relatively small amount of Lombard\nspeech is added to the training set then the performance in a real\nscenario, where noisy Lombard speech is present, can be significantly\nimproved. We also show that the standard approach followed in the literature,\nwhere a model is trained and tested on noisy plain speech, provides\na correct estimate of the video-only performance and slightly underestimates\nthe audio-visual performance. In case of audio-only approaches, performance\nis overestimated for SNRs higher than -3dB and underestimated for lower\nSNRs.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2726",
      "author_area_id": 10,
      "author_area_label": "Speech Recognition: Technologies and Systems for New Applications"
    },
    "ooster19_interspeech": {
      "authors": [
        [
          "Jasper",
          "Ooster"
        ],
        [
          "Pia Nancy Porysek",
          "Moreta"
        ],
        [
          "J\u00f6rg-Hendrik",
          "Bach"
        ],
        [
          "Inga",
          "Holube"
        ],
        [
          "Bernd T.",
          "Meyer"
        ]
      ],
      "title": "&#8220;Computer, Test My Hearing&#8221;: Accurate Speech Audiometry with Smart Speakers",
      "original": "2118",
      "page_count": 5,
      "order": 860,
      "p1": "4095",
      "pn": "4099",
      "abstract": [
        "Speech audiometry based on matrix sentence tests is an important diagnostic\ntool for hearing impairment and fitting of hearing aids. This paper\nintroduces a self-conducted measurement for estimating the speech reception\nthreshold (SRT) of a subject, i.e., the signal-to-noise ratio corresponding\nto 50% intelligibility, based on a smart speaker. While the original\nmeasurement procedure is well-evaluated and provides a very high measurement\naccuracy (&#60;1 dB test-retest standard deviation), the measurement\nusing a smart speaker differs in several aspects from the commercially\navailable implementation, such as missing control over the absolute\npresentation level, mode of presentation (headphones vs. loudspeaker),\npotential errors from the automated response logging, and influence\nfrom room acoustics. The SRT measurement accuracy is evaluated with\nsix normal-hearing subjects conducted with an Amazon Alexa application\non an Echo Plus loudspeaker in a quiet office environment. We found\na significant difference of 0.6 dB in SRT between the proposed and\nthe commercially available testing procedure. However, this bias is\nsmaller than the inter-subject standard deviation, and the measurement\naccuracy is similar to the original test for normal-hearing listeners,\nwhich indicates that smart speakers may become a helpful addition for\nthe screening of hearing deficits.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2118",
      "author_area_id": 10,
      "author_area_label": "Speech Recognition: Technologies and Systems for New Applications"
    },
    "eshky19_interspeech": {
      "authors": [
        [
          "Aciel",
          "Eshky"
        ],
        [
          "Manuel Sam",
          "Ribeiro"
        ],
        [
          "Korin",
          "Richmond"
        ],
        [
          "Steve",
          "Renals"
        ]
      ],
      "title": "Synchronising Audio and Ultrasound by Learning Cross-Modal Embeddings",
      "original": "1804",
      "page_count": 5,
      "order": 861,
      "p1": "4100",
      "pn": "4104",
      "abstract": [
        "Audiovisual synchronisation is the task of determining the time offset\nbetween speech audio and a video recording of the articulators. In\nchild speech therapy, audio and ultrasound videos of the tongue are\ncaptured using instruments which rely on hardware to synchronise the\ntwo modalities at recording time. Hardware synchronisation can fail\nin practice, and no mechanism exists to synchronise the signals post\nhoc. To address this problem, we employ a two-stream neural network\nwhich exploits the correlation between the two modalities to find the\noffset. We train our model on recordings from 69 speakers, and show\nthat it correctly synchronises 82.9% of test utterances from unseen\ntherapy sessions and unseen speakers, thus considerably reducing the\nnumber of utterances to be manually synchronised. An analysis of model\nperformance on the test utterances shows that directed phone articulations\nare more difficult to automatically synchronise compared to utterances\ncontaining natural variation in speech such as words, sentences, or\nconversations.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1804",
      "author_area_id": 10,
      "author_area_label": "Speech Recognition: Technologies and Systems for New Applications"
    },
    "pan19_interspeech": {
      "authors": [
        [
          "Yilin",
          "Pan"
        ],
        [
          "Bahman",
          "Mirheidari"
        ],
        [
          "Markus",
          "Reuber"
        ],
        [
          "Annalena",
          "Venneri"
        ],
        [
          "Daniel",
          "Blackburn"
        ],
        [
          "Heidi",
          "Christensen"
        ]
      ],
      "title": "Automatic Hierarchical Attention Neural Network for Detecting AD",
      "original": "1799",
      "page_count": 5,
      "order": 862,
      "p1": "4105",
      "pn": "4109",
      "abstract": [
        "Picture description tasks are used for the detection of cognitive decline\nassociated with Alzheimer&#8217;s disease (AD). Recent years have seen\nwork on automatic AD detection in picture descriptions based on acoustic\nand word-based analysis of the speech. These methods have shown some\nsuccess but lack an ability to capture any higher level effects of\ncognitive decline on the patient&#8217;s language. In this paper, we\npropose a novel model that encompasses both the hierarchical and sequential\nstructure of the description and detect its informative units by attention\nmechanism. Automatic speech recognition (ASR) and punctuation restoration\nare used to transcribe and segment the data. Using the DementiaBank\ndatabase of people with AD as well as healthy controls (HC), we obtain\nan F-score of 84.43% and 74.37% when using manual and automatic transcripts\nrespectively. We further explore the effect of adding additional data\n(a total of 33 descriptions collected using a &#8216; digital doctor&#8217;\n) during model training, and increase the F-score when using ASR transcripts\nto 76.09%. This outperforms baseline models, including bidirectional\nLSTM and bidirectional hierarchical neural network without an attention\nmechanism, and demonstrate that the use of hierarchical models with\nattention mechanism improves the AD/HC discrimination performance.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1799"
    },
    "nallanthighal19_interspeech": {
      "authors": [
        [
          "Venkata Srikanth",
          "Nallanthighal"
        ],
        [
          "Aki",
          "H\u00e4rm\u00e4"
        ],
        [
          "Helmer",
          "Strik"
        ]
      ],
      "title": "Deep Sensing of Breathing Signal During Conversational Speech",
      "original": "1796",
      "page_count": 5,
      "order": 863,
      "p1": "4110",
      "pn": "4114",
      "abstract": [
        "In this paper, we show the first results on the estimation of breathing\nsignal from conversational speech using deep learning algorithms. Respiratory\ndiseases such as COPD, asthma, and respiratory infections are common\nin the elderly population and patients in health care monitoring and\nmedical alert services in general. In this work, we compare algorithms\nfor the estimation of a known respiratory target signal, measured by\nrespiratory belt transducers positioned across the rib cage and abdomen,\nfrom conversational speech. We demonstrate the estimation of the respiratory\nsignal from speech using convolutional and recurrent neural networks.\nThe estimated breathing pattern gives respiratory rate, breathing capacity\nand thus might provide indications of the pathological condition of\nthe speaker. Evaluation of our model on our database of breathing signal\nand speech yielded a sensitivity of 91.2% for breath event detection\nand a mean absolute error of 1.01 breaths per minute for breathing\nrate estimation.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1796",
      "author_area_id": 10,
      "author_area_label": "Speech Recognition: Technologies and Systems for New Applications"
    },
    "biadsy19_interspeech": {
      "authors": [
        [
          "Fadi",
          "Biadsy"
        ],
        [
          "Ron J.",
          "Weiss"
        ],
        [
          "Pedro J.",
          "Moreno"
        ],
        [
          "Dimitri",
          "Kanvesky"
        ],
        [
          "Ye",
          "Jia"
        ]
      ],
      "title": "Parrotron: An End-to-End Speech-to-Speech Conversion Model and its Applications to Hearing-Impaired Speech and Speech Separation",
      "original": "1789",
      "page_count": 5,
      "order": 864,
      "p1": "4115",
      "pn": "4119",
      "abstract": [
        "We describe Parrotron, an end-to-end-trained speech-to-speech conversion\nmodel that maps an input spectrogram directly to another spectrogram,\nwithout utilizing any intermediate discrete representation. The network\nis composed of an encoder, spectrogram and phoneme decoders, followed\nby a vocoder to synthesize a time-domain waveform. We demonstrate that\nthis model can be trained to normalize speech from any speaker regardless\nof accent, prosody, and background noise, into the voice of a  single\ncanonical target speaker with a fixed accent and consistent articulation\nand prosody. We further show that this normalization model can be adapted\nto normalize highly atypical speech from a deaf speaker, resulting\nin significant improvements in intelligibility and naturalness, measured\nvia a speech recognizer and listening tests. Finally, demonstrating\nthe utility of this model on other speech tasks, we show that the same\nmodel architecture can be trained to perform a speech separation task.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1789",
      "author_area_id": 10,
      "author_area_label": "Speech Recognition: Technologies and Systems for New Applications"
    },
    "liu19j_interspeech": {
      "authors": [
        [
          "Shansong",
          "Liu"
        ],
        [
          "Shoukang",
          "Hu"
        ],
        [
          "Yi",
          "Wang"
        ],
        [
          "Jianwei",
          "Yu"
        ],
        [
          "Rongfeng",
          "Su"
        ],
        [
          "Xunying",
          "Liu"
        ],
        [
          "Helen",
          "Meng"
        ]
      ],
      "title": "Exploiting Visual Features Using Bayesian Gated Neural Networks for Disordered Speech Recognition",
      "original": "1536",
      "page_count": 5,
      "order": 865,
      "p1": "4120",
      "pn": "4124",
      "abstract": [
        "Automatic speech recognition (ASR) for disordered speech is a challenging\ntask. People with speech disorders such as dysarthria often have physical\ndisabilities, leading to severe degradation of speech quality, highly\nvariable voice characteristics and large mismatch against normal speech.\nIt is also difficult to record large amounts of high quality audio-visual\ndata for developing audio-visual speech recognition (AVSR) systems.\nTo address these issues, a novel Bayesian gated neural network (BGNN)\nbased AVSR approach is proposed. Speaker level Bayesian gated control\nof contributions from visual features allows a more robust fusion of\naudio and video modality. A posterior distribution over the gating\nparameters is used to model their uncertainty given limited and variable\ndisordered speech data. Experiments conducted on the UASpeech dysarthric\nspeech corpus suggest the proposed BGNN AVSR system consistently outperforms\nstate-of-the-art deep neural network (DNN) baseline ASR and AVSR systems\nby 4.5% and 4.7% absolute (14.9% and 15.5% relative) in word error\nrate.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1536",
      "author_area_id": 10,
      "author_area_label": "Speech Recognition: Technologies and Systems for New Applications"
    },
    "vougioukas19_interspeech": {
      "authors": [
        [
          "Konstantinos",
          "Vougioukas"
        ],
        [
          "Pingchuan",
          "Ma"
        ],
        [
          "Stavros",
          "Petridis"
        ],
        [
          "Maja",
          "Pantic"
        ]
      ],
      "title": "Video-Driven Speech Reconstruction Using Generative Adversarial Networks",
      "original": "1445",
      "page_count": 5,
      "order": 866,
      "p1": "4125",
      "pn": "4129",
      "abstract": [
        "Speech is a means of communication which relies on both audio and visual\ninformation. The absence of one modality can often lead to confusion\nor misinterpretation of information. In this paper we present an end-to-end\ntemporal model capable of directly synthesising audio from silent video,\nwithout needing to transform to-and-from intermediate features. Our\nproposed approach, based on GANs is capable of producing natural sounding,\nintelligible speech which is synchronised with the video. The performance\nof our model is evaluated on the GRID dataset for both speaker dependent\nand speaker independent scenarios. To the best of our knowledge this\nis the first method that maps video directly to raw audio and the first\nto produce intelligible speech when tested on previously unseen speakers.\nWe evaluate the synthesised audio not only based on the sound quality\nbut also on the accuracy of the spoken words.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1445",
      "author_area_id": 10,
      "author_area_label": "Speech Recognition: Technologies and Systems for New Applications"
    },
    "liu19k_interspeech": {
      "authors": [
        [
          "Shansong",
          "Liu"
        ],
        [
          "Shoukang",
          "Hu"
        ],
        [
          "Xunying",
          "Liu"
        ],
        [
          "Helen",
          "Meng"
        ]
      ],
      "title": "On the Use of Pitch Features for Disordered Speech Recognition",
      "original": "2609",
      "page_count": 5,
      "order": 867,
      "p1": "4130",
      "pn": "4134",
      "abstract": [
        "Pitch features have long been known to be useful for recognition of\nnormal speech. However, for disordered speech, the significant degradation\nof voice quality renders the prosodic features, such as pitch, not\nalways useful, particularly when the underlying conditions, for example,\ndamages to the cerebellum, introduce a large effect on prosody control.\nHence, both acoustic and prosodic information can be distorted. To\nthe best of our knowledge, there has been very limited research on\nusing pitch features for disordered speech recognition. In this paper,\na comparative study of multiple approaches designed to incorporate\npitch features is conducted to improve the performance of two disordered\nspeech recognition tasks: English UASpeech, and Cantonese CUDYS. A\nnovel gated neural network (GNN) based approach is used to improve\nacoustic and pitch feature integration over a conventional concatenation\nbetween the two. Bayesian estimation of GNNs is also investigated to\nfurther improve their robustness.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2609",
      "author_area_id": 10,
      "author_area_label": "Speech Recognition: Technologies and Systems for New Applications"
    },
    "shillingford19_interspeech": {
      "authors": [
        [
          "Brendan",
          "Shillingford"
        ],
        [
          "Yannis",
          "Assael"
        ],
        [
          "Matthew W.",
          "Hoffman"
        ],
        [
          "Thomas",
          "Paine"
        ],
        [
          "C\u00edan",
          "Hughes"
        ],
        [
          "Utsav",
          "Prabhu"
        ],
        [
          "Hank",
          "Liao"
        ],
        [
          "Hasim",
          "Sak"
        ],
        [
          "Kanishka",
          "Rao"
        ],
        [
          "Lorrayne",
          "Bennett"
        ],
        [
          "Marie",
          "Mulville"
        ],
        [
          "Misha",
          "Denil"
        ],
        [
          "Ben",
          "Coppin"
        ],
        [
          "Ben",
          "Laurie"
        ],
        [
          "Andrew",
          "Senior"
        ],
        [
          "Nando de",
          "Freitas"
        ]
      ],
      "title": "Large-Scale Visual Speech Recognition",
      "original": "1669",
      "page_count": 5,
      "order": 868,
      "p1": "4135",
      "pn": "4139",
      "abstract": [
        "This work presents a scalable solution to continuous visual speech\nrecognition. To achieve this, we constructed the largest existing visual\nspeech recognition dataset, consisting of pairs of transcriptions and\nvideo clips of faces speaking (3,886 hours of video). In tandem, we\ndesigned and trained an integrated lipreading system, consisting of\na video processing pipeline that maps raw video to stable videos of\nlips and sequences of phonemes, a scalable deep neural network that\nmaps the lip videos to sequences of phoneme distributions, and a phoneme-to-word\nspeech decoder that outputs sequences of words. The proposed system\nachieves a word error rate (WER) of 40.9% as measured on a held-out\nset. In comparison, professional lipreaders achieve either 86.4% or\n92.9% WER on the same dataset when having access to additional types\nof contextual information. Our approach significantly improves on previous\nlipreading approaches, including variants of  LipNet and of  Watch,\nAttend, and Spell (WAS), which are only capable of 89.8% and 76.8%\nWER respectively.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1669",
      "author_area_id": 10,
      "author_area_label": "Speech Recognition: Technologies and Systems for New Applications"
    },
    "razavi19_interspeech": {
      "authors": [
        [
          "S. Zahra",
          "Razavi"
        ],
        [
          "Benjamin",
          "Kane"
        ],
        [
          "Lenhart K.",
          "Schubert"
        ]
      ],
      "title": "Investigating Linguistic and Semantic Features for Turn-Taking Prediction in Open-Domain Human-Computer Conversation",
      "original": "3152",
      "page_count": 5,
      "order": 869,
      "p1": "4140",
      "pn": "4144",
      "abstract": [
        "In this paper we address the problem of turn-taking prediction in open-ended\ncommunication between humans and dialogue agents. In a non-task-oriented\ninteraction with dialogue agents, user inputs are apt to be grammatically\nand lexically diverse, and at times quite lengthy, with many pauses;\nall of this makes it harder for the system to decide when to jump in.\nAs a result recent turn-taking predictors designed for specific tasks\nor for human-human interactions will scarcely be applicable. In this\npaper we focus primarily on the predictive potential of linguistic\nfeatures, including lexical, syntactic and semantic features, as well\nas timing features, whereas past work has typically placed more emphasis\non prosodic features, sometimes supplemented with non-verbal behaviors\nsuch as gaze and head movements. The basis for our study is a corpus\nof 15 &#8220;friendly&#8221; dialogues between humans and a (Wizard-of-Oz\nenabled) virtual dialogue agent, annotated for pause times and types.\nThe model of turn-taking obtained by supervised learning predicts turn-taking\npoints with increasing accuracy using only prosodic features, only\ntiming and speech rate features, only lexical and syntactic features,\nand achieves state-of-the art performance with a mixture-of-experts\nmodel combining these features along with a semantic criterion.\n"
      ],
      "doi": "10.21437/Interspeech.2019-3152",
      "author_area_id": 11,
      "author_area_label": "Spoken dialog systems and conversational analysis"
    },
    "bechet19_interspeech": {
      "authors": [
        [
          "Fr\u00e9d\u00e9ric",
          "B\u00e9chet"
        ],
        [
          "Christian",
          "Raymond"
        ]
      ],
      "title": "Benchmarking Benchmarks: Introducing New Automatic Indicators for Benchmarking Spoken Language Understanding Corpora",
      "original": "3033",
      "page_count": 5,
      "order": 870,
      "p1": "4145",
      "pn": "4149",
      "abstract": [
        "Empirical evaluation is nowadays the main evaluation paradigm in Natural\nLanguage Processing for assessing the relevance of a new machine-learning\nbased model. If large corpora are available for tasks such as Automatic\nSpeech Recognition, this is not the case for other tasks such as Spoken\nLanguage Understanding (SLU), consisting in translating spoken transcriptions\ninto a formal representation often based on semantic frames. Corpora\nsuch as ATIS or SNIPS are widely used to compare systems, however differences\nin performance among systems are often very small, not statistically\nsignificant, and can be produced by biases in the data collection or\nthe annotation scheme, as we presented on the ATIS corpus (&#8220;Is\nATIS too shallow?, IS2018&#8221;). We propose in this study a new methodology\nfor assessing the relevance of an SLU corpus. We claim that only taking\ninto account systems performance does not provide enough insight about\nwhat is covered by current state-of-the-art models and what is left\nto be done. We apply our methodology on a set of 4 SLU systems and\n5 benchmark corpora (ATIS, SNIPS, M2M, MEDIA) and automatically produce\nseveral indicators assessing the relevance (or not) of each corpus\nfor benchmarking SLU models.\n"
      ],
      "doi": "10.21437/Interspeech.2019-3033",
      "author_area_id": 11,
      "author_area_label": "Spoken dialog systems and conversational analysis"
    },
    "liu19l_interspeech": {
      "authors": [
        [
          "Chaoran",
          "Liu"
        ],
        [
          "Carlos",
          "Ishi"
        ],
        [
          "Hiroshi",
          "Ishiguro"
        ]
      ],
      "title": "A Neural Turn-Taking Model without RNN",
      "original": "2270",
      "page_count": 5,
      "order": 871,
      "p1": "4150",
      "pn": "4154",
      "abstract": [
        "Sequential data such as speech and dialogs are usually modeled by Recurrent\nNeural Networks (RNN) and derivatives since the information can travel\nthrough time with such architecture. However, disadvantages exist with\nthe use of RNNs, including the limited depth of neural networks and\nthe GPU&#8217;s unfriendly training process.<br style=\"mso-data-placement:same-cell;\"\n/><br style=\"mso-data-placement:same-cell;\" />\n  Estimating the timing\nof turn-taking is a critical feature of dialog systems. Such tasks\nrequire knowledge about past dialog contexts and have been modeled\nusing RNNs in several studies. In this paper, we propose a non-RNN\nmodel for the timing estimation of turn-taking in dialogs. The proposed\nmodel takes lexical and acoustic features as its input to predict a\nturn&#8217;s end. We conducted experiments on four types of Japanese\nconversation datasets and show that with proper neural network designs,\nthe long-term information in a dialog could propagate without a recurrent\nstructure. The proposed model outperformed canonical RNN-based architectures\non a turn-taking estimation task.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2270",
      "author_area_id": 11,
      "author_area_label": "Spoken dialog systems and conversational analysis"
    },
    "coman19_interspeech": {
      "authors": [
        [
          "Andrei C.",
          "Coman"
        ],
        [
          "Koichiro",
          "Yoshino"
        ],
        [
          "Yukitoshi",
          "Murase"
        ],
        [
          "Satoshi",
          "Nakamura"
        ],
        [
          "Giuseppe",
          "Riccardi"
        ]
      ],
      "title": "An Incremental Turn-Taking Model for Task-Oriented Dialog Systems",
      "original": "1826",
      "page_count": 5,
      "order": 872,
      "p1": "4155",
      "pn": "4159",
      "abstract": [
        "In a human-machine dialog scenario, deciding the appropriate time for\nthe machine to take the turn is an open research problem. In contrast,\nhumans engaged in conversations are able to timely decide when to interrupt\nthe speaker for competitive or non-competitive reasons. In state-of-the-art\n turn-by-turn dialog systems the decision on the next dialog action\nis taken at the end of the utterance. In this paper, we propose a \ntoken-by-token prediction of the dialog state from incremental transcriptions\nof the user utterance. To identify the point of maximal understanding\nin an ongoing utterance, we a) implement an incremental Dialog State\nTracker which is updated on a token basis (iDST) b) re-label the Dialog\nState Tracking Challenge 2 (DSTC2) dataset and c) adapt it to the incremental\nturn-taking experimental scenario. The re-labeling consists of assigning\na binary value to each token in the user utterance that allows to identify\nthe appropriate point for taking the turn. Finally, we implement an\nincremental Turn Taking Decider (iTTD) that is trained on these new\nlabels for the turn-taking decision. We show that the proposed model\ncan achieve a better performance compared to a deterministic handcrafted\nturn-taking algorithm.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1826",
      "author_area_id": 11,
      "author_area_label": "Spoken dialog systems and conversational analysis"
    },
    "su19b_interspeech": {
      "authors": [
        [
          "Feng-Guang",
          "Su"
        ],
        [
          "Aliyah R.",
          "Hsu"
        ],
        [
          "Yi-Lin",
          "Tuan"
        ],
        [
          "Hung-Yi",
          "Lee"
        ]
      ],
      "title": "Personalized Dialogue Response Generation Learned from Monologues",
      "original": "1696",
      "page_count": 5,
      "order": 873,
      "p1": "4160",
      "pn": "4164",
      "abstract": [
        "Personalized responses are essential for having an informative and\nhuman-like conversation. Because it is difficult to collect a large\namount of dialogues involved with specific speakers, it is desirable\nthat chatbot can learn to generate personalized responses simply from\nmonologues of individuals. In this paper, we propose a novel personalized\ndialogue generation method which reduces the training data requirement\nto dialogues without speaker information and monologues of every target\nspeaker. In the proposed approach, a generative adversarial network\nensures the responses containing recognizable personal characteristics\nof the target speaker, and a backward SEQ2SEQ model reconstructs the\ninput message for keeping the coherence of the generated responses.\nThe proposed model demonstrates its flexibility to respond to open-domain\nconversations, and the experimental results show that the proposed\nmethod performs favorably against prior work in coherence, personality\nclassification, and human evaluation.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1696",
      "author_area_id": 11,
      "author_area_label": "Spoken dialog systems and conversational analysis"
    },
    "heldner19_interspeech": {
      "authors": [
        [
          "Mattias",
          "Heldner"
        ],
        [
          "Marcin",
          "W\u0142odarczak"
        ],
        [
          "\u0160tefan",
          "Be\u0148u\u0161"
        ],
        [
          "Agust\u00edn",
          "Gravano"
        ]
      ],
      "title": "Voice Quality as a Turn-Taking Cue",
      "original": "1592",
      "page_count": 5,
      "order": 874,
      "p1": "4165",
      "pn": "4169",
      "abstract": [
        "This work revisits the idea that voice quality dynamics (VQ) contributes\nto conveying pragmatic distinctions, with two case studies to further\ntest this idea. First, we explore VQ as a turn-taking cue, and then\nas a cue for distinguishing between different functions of affirmative\ncue words. We employ acoustic VQ measures claimed to be better suited\nfor continuous speech than those in own previous work. Both cases indicate\nthat the degree of periodicity (as measured by CPPS) is indeed relevant\nin the production of the different pragmatic functions. In particular,\nturn-yielding is characterized by lower periodicity, sometimes accompanied\nby presence of creaky voice. Periodicity also distinguishes between\nbackchannels, agreements and acknowledgements.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1592",
      "author_area_id": 11,
      "author_area_label": "Spoken dialog systems and conversational analysis"
    },
    "hara19_interspeech": {
      "authors": [
        [
          "Kohei",
          "Hara"
        ],
        [
          "Koji",
          "Inoue"
        ],
        [
          "Katsuya",
          "Takanashi"
        ],
        [
          "Tatsuya",
          "Kawahara"
        ]
      ],
      "title": "Turn-Taking Prediction Based on Detection of Transition Relevance Place",
      "original": "1537",
      "page_count": 5,
      "order": 875,
      "p1": "4170",
      "pn": "4174",
      "abstract": [
        "We address turn-taking prediction in which spoken dialogue systems\npredict when to take the conversational floor. In natural conversations,\nmany turn-taking decisions are arbitrary and subjective. In this study,\nwe propose taking into account the concept of the transition relevance\nplace (TRP) for turn-taking prediction. TRP is defined as a timing\nwhen the current speaking turn can be completed and other participants\nare able to take the turn. We conducted annotation of TRP on a human-robot\ndialogue corpus, ensuring the objectivity of this annotation among\nannotators. The proposed turn-taking prediction model adopts a two-step\napproach that detects TRP at first and then predicts a turn-taking\nevent if TRP is detected. Experimental evaluations demonstrate that\nthe proposed model improves the accuracy of turn-taking prediction\nby incorporating TRP detection.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1537",
      "author_area_id": 11,
      "author_area_label": "Spoken dialog systems and conversational analysis"
    },
    "lala19_interspeech": {
      "authors": [
        [
          "Divesh",
          "Lala"
        ],
        [
          "Shizuka",
          "Nakamura"
        ],
        [
          "Tatsuya",
          "Kawahara"
        ]
      ],
      "title": "Analysis of Effect and Timing of Fillers in Natural Turn-Taking",
      "original": "1527",
      "page_count": 5,
      "order": 876,
      "p1": "4175",
      "pn": "4179",
      "abstract": [
        "Turn-taking for spoken dialogue systems is still below the speed of\nreal human conversation due to latency in speech and natural language\nprocessing, but fillers can be used by the system to take the turn\nmore quickly without sacrificing naturalness. In this work we analyze\nfillers which are used at the start of turns in conversation and determine\na window of appropriate times to use them. We analyze a human-robot\nconversation corpus to obtain an average response time of the fillers,\nand find that this differs according to the filler&#8217;s form. We\nthen conduct a subjective experiment in which participants dynamically\nchange the timing of responses with and without fillers to designate\na window of acceptable response timings. Our results show that the\nmost suitable response time is around 200&#8211;500ms after the previous\nspeaker has finished their turn. We also find differences in timing\nwindows depending on existence of a filler used to begin the turn and\nits particular form. The implications of these results on the design\nof conversational systems are also discussed.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1527",
      "author_area_id": 11,
      "author_area_label": "Spoken dialog systems and conversational analysis"
    },
    "horiguchi19_interspeech": {
      "authors": [
        [
          "Shota",
          "Horiguchi"
        ],
        [
          "Naoyuki",
          "Kanda"
        ],
        [
          "Kenji",
          "Nagamatsu"
        ]
      ],
      "title": "Multimodal Response Obligation Detection with Unsupervised Online Domain Adaptation",
      "original": "1313",
      "page_count": 5,
      "order": 877,
      "p1": "4180",
      "pn": "4184",
      "abstract": [
        "Response obligation detection, which determines whether a dialogue\nrobot has to respond to a detected utterance, is an important function\nfor intelligent dialogue robots. Some studies have tackled this problem;\nhowever, they narrow their applicability by impractical assumptions\nor use of scenario-specific features. Some attempts have been made\nto widen the applicability by avoiding the use of text modality, which\nis said to be highly domain dependent, but it decreases the detection\naccuracy. In this paper, we propose a novel multimodal response obligation\ndetector, which uses visual, audio, and text information for highly-accurate\ndetection, with its unsupervised online domain adaptation to solve\nthe domain dependency problem. Our domain adaptation consists of the\nweights adaptation of the logistic regression for every modality and\nan embedding assignment for new words to cope with the high domain\ndependency of text modality. Experimental results on the dataset collected\nat a station and commercial building showed that our method achieved\nhigh response obligation detection accuracy and was able to handle\ndomain change automatically.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1313",
      "author_area_id": 11,
      "author_area_label": "Spoken dialog systems and conversational analysis"
    },
    "su19c_interspeech": {
      "authors": [
        [
          "Ming-Hsiang",
          "Su"
        ],
        [
          "Chung-Hsien",
          "Wu"
        ],
        [
          "Yi",
          "Chang"
        ]
      ],
      "title": "Follow-Up Question Generation Using Neural Tensor Network-Based Domain Ontology Population in an Interview Coaching System",
      "original": "1300",
      "page_count": 5,
      "order": 878,
      "p1": "4185",
      "pn": "4189",
      "abstract": [
        "This study proposes an approach to follow-up question generation based\non a populated domain ontology in a conversational interview coaching\nsystem. The purpose of this study is to generate the follow-up questions\nwhich are more related to the meaning beyond the literal content in\nthe user&#8217;s answer based on the background knowledge in a populated\ndomain ontology. Firstly, a convolutional neural tensor network (CNTN)\nwas applied for selecting a key sentence from the user answer. Secondly,\nthe neural tensor network (NTN) was used to model the relationship\nbetween the subjects and objects in the resource description framework\n(RDF) triple, defined as (subject, predicate, object), in each predicate\nfrom the ConceptNet for domain ontology population. The words in the\nkey sentence were then used to retrieve relevant triples from the domain\nontology for filling into the slots in the question templates to generate\npotential follow-up questions. Finally, the CNTN-based sentence matching\nmodel was employed to choose the one most related to the answer sentence\nas the final follow-up question. This study used 5-fold cross-validation\nfor performance evaluation. The experimental results showed the generation\nperformance in the proposed model was higher than the traditional method.\nThe performance of key sentence selection model achieved 81.94%, and\nthe sentence matching model achieved 92.28%.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1300",
      "author_area_id": 11,
      "author_area_label": "Spoken dialog systems and conversational analysis"
    },
    "tran19_interspeech": {
      "authors": [
        [
          "Trang",
          "Tran"
        ],
        [
          "Jiahong",
          "Yuan"
        ],
        [
          "Yang",
          "Liu"
        ],
        [
          "Mari",
          "Ostendorf"
        ]
      ],
      "title": "On the Role of Style in Parsing Speech with Neural Models",
      "original": "3122",
      "page_count": 5,
      "order": 879,
      "p1": "4190",
      "pn": "4194",
      "abstract": [
        "The differences in written text and conversational speech are substantial;\nprevious parsers trained on treebanked text have given very poor results\non spontaneous speech. For spoken language, the mismatch in style also\nextends to prosodic cues, though it is less well understood. This paper\nre-examines the use of written text in parsing speech in the context\nof recent advances in neural language processing. We show that neural\napproaches facilitate using written text to improve parsing of spontaneous\nspeech, and that prosody further improves over this state-of-the-art\nresult. Further, we find an asymmetric degradation from read vs. spontaneous\nmismatch, with spontaneous speech more generally useful for training\nparsers.\n"
      ],
      "doi": "10.21437/Interspeech.2019-3122",
      "author_area_id": 12,
      "author_area_label": "Spoken Language Processing: Translation, Information Retrieval, Summarization, Resources and Evaluation"
    },
    "pasad19_interspeech": {
      "authors": [
        [
          "Ankita",
          "Pasad"
        ],
        [
          "Bowen",
          "Shi"
        ],
        [
          "Herman",
          "Kamper"
        ],
        [
          "Karen",
          "Livescu"
        ]
      ],
      "title": "On the Contributions of Visual and Textual Supervision in Low-Resource Semantic Speech Retrieval",
      "original": "3051",
      "page_count": 5,
      "order": 880,
      "p1": "4195",
      "pn": "4199",
      "abstract": [
        "Recent work has shown that speech paired with images can be used to\nlearn semantically meaningful speech representations even without any\ntextual supervision. In real-world low-resource settings, however,\nwe often have access to some transcribed speech. We study whether and\nhow visual grounding is useful in the presence of varying amounts of\ntextual supervision. In particular, we consider the task of semantic\nspeech retrieval in a low-resource setting. We use a previously studied\ndata set and task, where models are trained on images with spoken captions\nand evaluated on human judgments of semantic relevance. We propose\na multitask learning approach to leverage both visual and textual modalities,\nwith visual supervision in the form of keyword probabilities from an\nexternal tagger. We find that visual grounding is helpful even in the\npresence of textual supervision, and we analyze this effect over a\nrange of sizes of transcribed data sets. With &#126;5 hours of transcribed\nspeech, we obtain 23% higher average precision when also using visual\nsupervision.\n"
      ],
      "doi": "10.21437/Interspeech.2019-3051",
      "author_area_id": 12,
      "author_area_label": "Spoken Language Processing: Translation, Information Retrieval, Summarization, Resources and Evaluation"
    },
    "wang19p_interspeech": {
      "authors": [
        [
          "Xinhao",
          "Wang"
        ],
        [
          "Su-Youn",
          "Yoon"
        ],
        [
          "Keelan",
          "Evanini"
        ],
        [
          "Klaus",
          "Zechner"
        ],
        [
          "Yao",
          "Qian"
        ]
      ],
      "title": "Automatic Detection of Off-Topic Spoken Responses Using Very Deep Convolutional Neural Networks",
      "original": "1848",
      "page_count": 5,
      "order": 881,
      "p1": "4200",
      "pn": "4204",
      "abstract": [
        "Test takers in high-stakes speaking assessments may try to inflate\ntheir scores by providing a response to a question that they are more\nfamiliar with instead of the question presented in the test; such a\nresponse is referred to as an off-topic spoken response. The presence\nof these responses can make it difficult to accurately evaluate a test\ntaker&#8217;s speaking proficiency, and thus may reduce the validity\nof assessment scores. This study aims to address this problem by building\nan automatic system to detect off-topic spoken responses which can\ninform the downstream automated scoring pipeline. We propose an innovative\nmethod to interpret the comparison between a test response and the\nquestion used to elicit it as a similarity grid, and then apply very\ndeep convolutional neural networks to determine different degrees of\ntopic relevance. In this study, Inception networks were applied to\nthis task, and the experimental results demonstrate the effectiveness\nof the proposed method. Our system achieves an F1-score of 92.8% on\nthe class of off-topic responses, which significantly outperforms a\nbaseline system using a range of word embedding-based similarity metrics\n(F1-score = 85.5%).\n"
      ],
      "doi": "10.21437/Interspeech.2019-1848",
      "author_area_id": 12,
      "author_area_label": "Spoken Language Processing: Translation, Information Retrieval, Summarization, Resources and Evaluation"
    },
    "piunova19_interspeech": {
      "authors": [
        [
          "Anna",
          "Piunova"
        ],
        [
          "Eugen",
          "Beck"
        ],
        [
          "Ralf",
          "Schl\u00fcter"
        ],
        [
          "Hermann",
          "Ney"
        ]
      ],
      "title": "Rescoring Keyword Search Confidence Estimates with Graph-Based Re-Ranking Using Acoustic Word Embeddings",
      "original": "1817",
      "page_count": 5,
      "order": 882,
      "p1": "4205",
      "pn": "4209",
      "abstract": [
        "Postprocessing of confidence scores in keyword search (KWS) task is\nknown to be an efficient way of improving retrieval performance. In\nthis paper, we extend the existing graph-based re-ranking algorithm\nproposed for KWS score calibration. We replace the originally used\nDynamic TimeWarping (DTW) distance measure between prospective hits\nwith distances between their Acoustic Word Embeddings (AWEs) learned\nfrom Neural Networks. We argue that AWEs trained to discriminate between\nthe same and different words should improve the graph-based re-ranking\nperformance. Experimental results on two languages from IARPA Babel\nprogram show that our approach outperforms the DTW and improves the\nbaseline KWS result between 3.0&#8211;7.5% relative on the Maximum\nTerm Weighted Value (MTWV) measure. It was previously shown, that enhancing\ndetection lists with keyword exemplars given high confidence, improved\nthe algorithm performance. We additionally expanded the detection lists\nwith negative query exemplars and observed further improvements in\nMTWV.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1817",
      "author_area_id": 12,
      "author_area_label": "Spoken Language Processing: Translation, Information Retrieval, Summarization, Resources and Evaluation"
    },
    "segal19_interspeech": {
      "authors": [
        [
          "Yael",
          "Segal"
        ],
        [
          "Tzeviya Sylvia",
          "Fuchs"
        ],
        [
          "Joseph",
          "Keshet"
        ]
      ],
      "title": "SpeechYOLO: Detection and Localization of Speech Objects",
      "original": "1749",
      "page_count": 5,
      "order": 883,
      "p1": "4210",
      "pn": "4214",
      "abstract": [
        "In this paper, we propose to apply object detection methods from the\nvision domain on the speech recognition domain, by treating audio fragments\nas objects. More specifically, we present SpeechYOLO, which is inspired\nby the YOLO algorithm [1] for object detection in images. The goal\nof SpeechYOLO is to localize boundaries of utterances within the input\nsignal, and to correctly classify them. Our system is composed of a\nconvolutional neural network, with a simple least-mean-squares loss\nfunction. We evaluated the system on several keyword spotting tasks,\nthat include corpora of read speech and spontaneous speech. Our system\ncompares favorably with other algorithms trained for both localization\nand classification.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1749",
      "author_area_id": 12,
      "author_area_label": "Spoken Language Processing: Translation, Information Retrieval, Summarization, Resources and Evaluation"
    },
    "oktem19_interspeech": {
      "authors": [
        [
          "Alp",
          "\u00d6ktem"
        ],
        [
          "Mireia",
          "Farr\u00fas"
        ],
        [
          "Antonio",
          "Bonafonte"
        ]
      ],
      "title": "Prosodic Phrase Alignment for Machine Dubbing",
      "original": "1621",
      "page_count": 5,
      "order": 884,
      "p1": "4215",
      "pn": "4219",
      "abstract": [
        "Dubbing is a type of audiovisual translation where dialogues are translated\nand enacted so that they give the impression that the media is in the\ntarget language. It requires a careful alignment of dubbed recordings\nwith the lip movements of performers in order to achieve visual coherence.\nIn this paper, we deal with the specific problem of prosodic phrase\nsynchronization within the framework of machine dubbing. Our methodology\nexploits the attention mechanism output in neural machine translation\nto find plausible phrasing for the translated dialogue lines and then\nuses them to condition their synthesis. Our initial work in this field\nrecords comparable speech rate ratio to professional dubbing translation,\nand improvement in terms of lip-syncing of long dialogue lines.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1621",
      "author_area_id": 12,
      "author_area_label": "Spoken Language Processing: Translation, Information Retrieval, Summarization, Resources and Evaluation"
    },
    "tannander19_interspeech": {
      "authors": [
        [
          "Christina",
          "T\u00e5nnander"
        ],
        [
          "Per",
          "Fallgren"
        ],
        [
          "Jens",
          "Edlund"
        ],
        [
          "Joakim",
          "Gusafsson"
        ]
      ],
      "title": "Spot the Pleasant People! Navigating the Cocktail Party Buzz",
      "original": "1553",
      "page_count": 5,
      "order": 885,
      "p1": "4220",
      "pn": "4224",
      "abstract": [
        "We present an experimental platform for making voice likability assessments\nthat are decoupled from individual voices, and instead capture voice\ncharacteristics over groups of speakers. We employ methods that we\nhave previously used for other purposes to create the Cocktail platform,\nwhere respondents navigate in a voice buzz made up of about 400 voices\non a touch screen. They then choose the location where they find the\nvoice buzz most pleasant. Since there is no image or message on the\nscreen, the platform can be used by visually impaired people, who often\nneed to rely on spoken text, on the same premises as seeing people.\nIn this paper, we describe the platform and its motivation along with\nour analysis method. We conclude by presenting two experiments in which\nwe verify that the platform behaves as expected: one simple sanity\ntest, and one experiment with voices grouped according to their mean\npitch variance.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1553",
      "author_area_id": 12,
      "author_area_label": "Spoken Language Processing: Translation, Information Retrieval, Summarization, Resources and Evaluation"
    },
    "chen19o_interspeech": {
      "authors": [
        [
          "Zhi",
          "Chen"
        ],
        [
          "Wu",
          "Guo"
        ],
        [
          "Li-Rong",
          "Dai"
        ],
        [
          "Zhen-Hua",
          "Ling"
        ],
        [
          "Jun",
          "Du"
        ]
      ],
      "title": "Neural Text Clustering with Document-Level Attention Based on Dynamic Soft Labels",
      "original": "1417",
      "page_count": 5,
      "order": 886,
      "p1": "4225",
      "pn": "4229",
      "abstract": [
        "In this paper, the deep learning framework is applied in text clustering,\nan unsupervised task in natural language processing (NLP). Since there\nare no predefined labels available for text clustering, the deep neural\nnetwork is trained in a pseudo-supervised fashion with labels generated\nfrom pre-clustering step. To address the wrong labelling problem from\npre-clustering step, we adopt soft pseudo-labels instead of hard one-hot\nones, and these labels are dynamically updated during training. Besides,\nwe build a document-level attention over multiple documents based on\ndynamic soft pseudo-labels to further reduce the impact of the wrong\nlabels. Experimental results on three public databases show that our\nmodel outperforms the state-of-the-art systems.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1417",
      "author_area_id": 12,
      "author_area_label": "Spoken Language Processing: Translation, Information Retrieval, Summarization, Resources and Evaluation"
    },
    "bach19_interspeech": {
      "authors": [
        [
          "Nguyen",
          "Bach"
        ],
        [
          "Fei",
          "Huang"
        ]
      ],
      "title": "Noisy BiLSTM-Based Models for Disfluency Detection",
      "original": "1336",
      "page_count": 5,
      "order": 887,
      "p1": "4230",
      "pn": "4234",
      "abstract": [
        "This paper describes BiLSTM-based models to disfluency detection in\nspeech transcripts using residual BiLSTM blocks, self-attention, and\nnoisy training approach. Our best model not only surpasses BERT in\n4 non-Switchboard test sets, but also is 20 times smaller than the\nBERT-based model [1]. Thus, we demonstrate that strong performance\ncan be achieved without extensively use of very large training data.\nIn addition, we show that it is possible to be robust across data sets\nwith noisy training approach in which we found insertion is the most\nuseful noise for augmenting training data.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1336",
      "author_area_id": 12,
      "author_area_label": "Spoken Language Processing: Translation, Information Retrieval, Summarization, Resources and Evaluation"
    },
    "singh19b_interspeech": {
      "authors": [
        [
          "Mittul",
          "Singh"
        ],
        [
          "Sami",
          "Virpioja"
        ],
        [
          "Peter",
          "Smit"
        ],
        [
          "Mikko",
          "Kurimo"
        ]
      ],
      "title": "Subword RNNLM Approximations for Out-Of-Vocabulary Keyword Search",
      "original": "1329",
      "page_count": 5,
      "order": 888,
      "p1": "4235",
      "pn": "4239",
      "abstract": [
        "In spoken Keyword Search, the query may contain out-of-vocabulary (OOV)\nwords not observed when training the speech recognition system. Using\nsubword language models (LMs) in the first-pass recognition makes it\npossible to recognize the OOV words, but even the subword n-gram LMs\nsuffer from data sparsity. Recurrent Neural Network (RNN) LMs alleviate\nthe sparsity problems but are not suitable for first-pass recognition\nas such. One way to solve this is to approximate the RNNLMs by back-off\nn-gram models. In this paper, we propose to interpolate the conventional\nn-gram models and the RNNLM approximation for better OOV recognition.\nFurthermore, we develop a new RNNLM approximation method suitable for\nsubword units: It produces variable-order n-grams to include long-span\napproximations and considers also n-grams that were not originally\nobserved in the training corpus. To evaluate these models on OOVs,\nwe setup Arabic and Finnish Keyword Search tasks concentrating only\non OOV words. On these tasks, interpolating the baseline RNNLM approximation\nand a conventional LM outperforms the conventional LM in terms of the\nMaximum TermWeighted Value for single-character subwords. Moreover,\nreplacing the baseline approximation with the proposed method achieves\nthe best performance on both multi- and single-character subwords.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1329",
      "author_area_id": 12,
      "author_area_label": "Spoken Language Processing: Translation, Information Retrieval, Summarization, Resources and Evaluation"
    },
    "maekaku19_interspeech": {
      "authors": [
        [
          "Takashi",
          "Maekaku"
        ],
        [
          "Yusuke",
          "Kida"
        ],
        [
          "Akihiko",
          "Sugiyama"
        ]
      ],
      "title": "Simultaneous Detection and Localization of a Wake-Up Word Using Multi-Task Learning of the Duration and Endpoint",
      "original": "1180",
      "page_count": 5,
      "order": 889,
      "p1": "4240",
      "pn": "4244",
      "abstract": [
        "This paper proposes a novel method for simultaneous detection and localization\nof a wake-up word using multi-task learning of the duration and endpoint.\nAn onset of the wake-up word is estimated by going back in time by\nan estimated duration of the wake-up word from an estimated endpoint.\nAccurate endpoint estimation is achieved by training the network to\nfire only at the endpoint in contrast to the entire wake-up word. The\naccurate endpoint naturally leads to an accurate onset, when it is\nused as a basis to calculate an onset with an estimated duration that\nreflects the whole acoustic information over the entire wake-up word.\nExperimental results with real-environment data show that a relative\nimprovement in accuracy of 41% for onset estimation and 38% for endpoint\nestimation are achieved compared to a baseline method.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1180",
      "author_area_id": 12,
      "author_area_label": "Spoken Language Processing: Translation, Information Retrieval, Summarization, Resources and Evaluation"
    },
    "lee19e_interspeech": {
      "authors": [
        [
          "Ching-Hua",
          "Lee"
        ],
        [
          "Kuan-Lin",
          "Chen"
        ],
        [
          "Fred",
          "Harris"
        ],
        [
          "Bhaskar D.",
          "Rao"
        ],
        [
          "Harinath",
          "Garudadri"
        ]
      ],
      "title": "On Mitigating Acoustic Feedback in Hearing Aids with Frequency Warping by All-Pass Networks",
      "original": "3195",
      "page_count": 5,
      "order": 890,
      "p1": "4245",
      "pn": "4249",
      "abstract": [
        "Acoustic feedback control continues to be a challenging problem due\nto the emerging form factors in advanced hearing aids (HAs) and hearables.\nIn this paper, we present a novel use of well-known all-pass filters\nin a network to perform frequency warping that we call &#8220;freping.&#8221;\nFreping helps in breaking the Nyquist stability criterion and improves\nadaptive feedback cancellation (AFC). Based on informal subjective\nassessments, distortions due to freping are fairly benign. While common\nobjective metrics like the perceptual evaluation of speech quality\n(PESQ) and the hearing-aid speech quality index (HASQI) may not adequately\ncapture distortions due to freping and acoustic feedback artifacts\nfrom a perceptual perspective, they are still instructive in assessing\nthe proposed method. We demonstrate quality improvements with freping\nfor a basic AFC (PESQ: 2.56 to 3.52 and HASQI: 0.65 to 0.78) at a gain\nsetting of 20; and an advanced AFC (PESQ: 2.75 to 3.17 and HASQI: 0.66\nto 0.73) for a gain of 30. From our investigations, freping provides\nlarger improvement for basic AFC, but still improves overall system\nperformance for many AFC approaches.\n"
      ],
      "doi": "10.21437/Interspeech.2019-3195",
      "author_area_id": 6,
      "author_area_label": "Speech Coding and Enhancement"
    },
    "fazel19_interspeech": {
      "authors": [
        [
          "Amin",
          "Fazel"
        ],
        [
          "Mostafa",
          "El-Khamy"
        ],
        [
          "Jungwon",
          "Lee"
        ]
      ],
      "title": "Deep Multitask Acoustic Echo Cancellation",
      "original": "2908",
      "page_count": 5,
      "order": 891,
      "p1": "4250",
      "pn": "4254",
      "abstract": [
        "Acoustic echo cancellation or suppression methods aim to suppress the\necho originated from acoustic coupling between loudspeakers and microphones.\nConventional approaches estimate echo using adaptive filtering. Due\nto the nonlinearities in the acoustic path of far-end signal, further\npost-processing is needed to attenuate these nonlinear components.\nIn this paper, we propose a novel architecture based on deep gated\nrecurrent neural networks to estimate the near-end signal from the\nmicrophone signal. The proposed architecture is trained using multitask\nlearning to learn the auxiliary task of estimating the echo in order\nto improve the main task of estimating the clean near-end speech signal.\nExperimental results show that our proposed deep learning based method\noutperforms the existing methods for unseen speakers in terms of the\necho return loss enhancement (ERLE) for single-talk periods and the\nperceptual evaluation of speech quality (PESQ) score for double-talk\nperiods.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2908",
      "author_area_id": 6,
      "author_area_label": "Speech Coding and Enhancement"
    },
    "zhang19o_interspeech": {
      "authors": [
        [
          "Hao",
          "Zhang"
        ],
        [
          "Ke",
          "Tan"
        ],
        [
          "DeLiang",
          "Wang"
        ]
      ],
      "title": "Deep Learning for Joint Acoustic Echo and Noise Cancellation with Nonlinear Distortions",
      "original": "2651",
      "page_count": 5,
      "order": 892,
      "p1": "4255",
      "pn": "4259",
      "abstract": [
        "We formulate acoustic echo and noise cancellation jointly as deep learning\nbased speech separation, where near-end speech is separated from a\nsingle microphone recording and sent to the far end. We propose a causal\nsystem to address this problem, which incorporates a convolutional\nrecurrent network (CRN) and a recurrent network with long short-term\nmemory (LSTM). The system is trained to estimate the real and imaginary\nspectrograms of near-end speech and detect the activity of near-end\nspeech from the microphone signal and far-end signal. Subsequently,\nthe estimated real and imaginary spectrograms are used to separate\nthe near-end signal, hence removing echo and noise. The trained near-end\nspeech detector is employed to further suppress residual echo and noise.\nEvaluation results show that the proposed method effectively removes\nacoustic echo and background noise in the presence of nonlinear distortions\nfor both simulated and measured room impulse responses (RIRs). Additionally,\nthe proposed method generalizes well to untrained noises, RIRs and\nspeakers.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2651",
      "author_area_id": 6,
      "author_area_label": "Speech Coding and Enhancement"
    },
    "srensen19_interspeech": {
      "authors": [
        [
          "Charlotte",
          "S\u00f8rensen"
        ],
        [
          "Jesper B.",
          "Boldt"
        ],
        [
          "Mads G.",
          "Christensen"
        ]
      ],
      "title": "Harmonic Beamformers for Non-Intrusive Speech Intelligibility Prediction",
      "original": "2929",
      "page_count": 5,
      "order": 893,
      "p1": "4260",
      "pn": "4264",
      "abstract": [
        "In recent years, research into objective speech intelligibility measures\nhas gained increased interest as a tool to optimize speech enhancement\nalgorithms. While most intelligibility measures are intrusive, i.e.,\nthey require a clean reference signal, this is rarely available in\nreal-time applications. This paper proposes two non-intrusive intelligibility\nmeasures, which allow using the intrusive short-time objective intelligibility\n(STOI) measure without requiring access to the clean signal. Instead,\na reference signal is obtained from the degraded signal using either\na fixed or an adaptive harmonic spatial filter. This reference signal\nis then used as input to STOI. The experimental results show a high\ncorrelation between both proposed non-intrusive speech intelligibility\nmeasures and the original intrusively computed STOI scores.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2929",
      "author_area_id": 6,
      "author_area_label": "Speech Coding and Enhancement"
    },
    "mamun19b_interspeech": {
      "authors": [
        [
          "Nursadul",
          "Mamun"
        ],
        [
          "Soheil",
          "Khorram"
        ],
        [
          "John H.L.",
          "Hansen"
        ]
      ],
      "title": "Convolutional Neural Network-Based Speech Enhancement for Cochlear Implant Recipients",
      "original": "1850",
      "page_count": 5,
      "order": 894,
      "p1": "4265",
      "pn": "4269",
      "abstract": [
        "Attempts to develop speech enhancement algorithms with improved speech\nintelligibility for cochlear implant (CI) users have met with limited\nsuccess. To improve speech enhancement methods for CI users, we propose\nto perform speech enhancement in a cochlear filter-bank feature space,\na feature-set specifically designed for CI users based on CI auditory\nstimuli. We leverage a convolutional neural network (CNN) to extract\nboth stationary and non-stationary components of environmental acoustics\nand speech. We propose three CNN architectures: (1) vanilla CNN that\ndirectly generates the enhanced signal; (2) spectral-subtraction-style\nCNN (SS-CNN) that first predicts noise and then generates the enhanced\nsignal by subtracting noise from the noisy signal; (3) Wiener-style\nCNN (Wiener-CNN) that generates an optimal mask for suppressing noise.\nAn important problem of the proposed networks is that they introduce\nconsiderable delays, which limits their real-time application for CI\nusers. To address this, this study also considers causal variations\nof these networks. Our experiments show that the proposed networks\n(both causal and non-causal forms) achieve significant improvement\nover existing baseline systems. We also found that causal Wiener-CNN\noutperforms other networks, and leads to the best overall envelope\ncoefficient measure (ECM). The proposed algorithms represent a viable\noption for implementation on the CCi-MOBILE research platform as a\npre-processor for CI users in naturalistic environments.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1850",
      "author_area_id": 6,
      "author_area_label": "Speech Coding and Enhancement"
    },
    "srensen19b_interspeech": {
      "authors": [
        [
          "Charlotte",
          "S\u00f8rensen"
        ],
        [
          "Jesper B.",
          "Boldt"
        ],
        [
          "Mads G.",
          "Christensen"
        ]
      ],
      "title": "Validation of the Non-Intrusive Codebook-Based Short Time Objective Intelligibility Metric for Processed Speech",
      "original": "1625",
      "page_count": 5,
      "order": 895,
      "p1": "4270",
      "pn": "4274",
      "abstract": [
        "In recent years, objective measures of speech intelligibility have\ngained increasing interest. However, most speech intelligibility metrics\nrequire a clean reference signal, which is often not available in real-life\napplications. In a recent publication, we proposed a method, the Non-Intrusive\nCodebook-based Short-Time Objective Intelligibility (NIC-STOI) metric,\nwhich allows using an intrusive method without requiring access to\nthe clean signal. The statistics of the reference signal is estimated\nas a combination of predefined codebooks that best fit the degraded\nsignal by modeling the speech and noisy spectra. In this paper, we\nperform additional validation of the NIC-STOI in more diverse noise\ncondition as well as for speech processed non-linearly with binary\nmasks, where it is shown to outperform existing non-intrusive metrics.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1625",
      "author_area_id": 6,
      "author_area_label": "Speech Coding and Enhancement"
    },
    "arai19_interspeech": {
      "authors": [
        [
          "Kenichi",
          "Arai"
        ],
        [
          "Shoko",
          "Araki"
        ],
        [
          "Atsunori",
          "Ogawa"
        ],
        [
          "Keisuke",
          "Kinoshita"
        ],
        [
          "Tomohiro",
          "Nakatani"
        ],
        [
          "Katsuhiko",
          "Yamamoto"
        ],
        [
          "Toshio",
          "Irino"
        ]
      ],
      "title": "Predicting Speech Intelligibility of Enhanced Speech Using Phone Accuracy of DNN-Based ASR System",
      "original": "1381",
      "page_count": 5,
      "order": 896,
      "p1": "4275",
      "pn": "4279",
      "abstract": [
        "The ability of state-of-the-art automatic speech recognition (ASR)\nsystems, which use deep neural networks (DNN), has recently been approaching\nthat of human auditory systems. On the other hand, although measuring\nthe intelligibility of enhanced speech signals is important for developing\nauditory algorithms and devices, the current measurement methods mainly\nrely on subjective experiments. Therefore, it would be preferable to\nemploy an ASR system to predict the subjective speech intelligibility\n(SI) of enhanced speech. In this study, we evaluate the SI prediction\nperformance of DNN-based ASR systems using phone accuracies. We found\nthat an ASR system with multi-condition training achieves the best\nSI prediction accuracy for enhanced speech when compared with conventional\nmethods (STOI, HASPI) and a recently proposed technique (GEDI). In\naddition, since our ASR system uses only a phone language model, it\noffers the advantage of being able to predict intelligibility independently\nof prior knowledge of words.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1381"
    },
    "bu19_interspeech": {
      "authors": [
        [
          "Suliang",
          "Bu"
        ],
        [
          "Yunxin",
          "Zhao"
        ],
        [
          "Mei-Yuh",
          "Hwang"
        ]
      ],
      "title": "A Novel Method to Correct Steering Vectors in MVDR Beamformer for Noise Robust ASR",
      "original": "2944",
      "page_count": 5,
      "order": 897,
      "p1": "4280",
      "pn": "4284",
      "abstract": [
        "Accurate steering vectors (SV) are key to many beamformers. However,\nreliable SV is not easy to obtain. In this work, we investigate a novel\nmethod to identify and correct phase errors in SV for MVDR beamforming.\nOur idea stems from the linear relationship in the phase of a microphone\ncomponent in narrowband SVs across frequency, as modeled by acoustic\ntransfer function. We utilize this property and feedforward neural\nnets to make phase prediction for the microphone components in SVs,\nand use the predicted phase selectively for phase error correction\nand MVDR beamforming. Our method is robust to large fluctuations in\nphase spectrum wrapped within [-&#960;, &#960;]. We have evaluated\nour approach on CHiME-3 and obtained improved performances on both\nword error rate and short-time objective intelligibility in low reverberant\nacoustic environments.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2944",
      "author_area_id": 6,
      "author_area_label": "Speech Coding and Enhancement"
    },
    "lee19f_interspeech": {
      "authors": [
        [
          "Hyeonseung",
          "Lee"
        ],
        [
          "Hyung Yong",
          "Kim"
        ],
        [
          "Woo Hyun",
          "Kang"
        ],
        [
          "Jeunghun",
          "Kim"
        ],
        [
          "Nam Soo",
          "Kim"
        ]
      ],
      "title": "End-to-End Multi-Channel Speech Enhancement Using Inter-Channel Time-Restricted Attention on Raw Waveform",
      "original": "2397",
      "page_count": 5,
      "order": 898,
      "p1": "4285",
      "pn": "4289",
      "abstract": [
        "This paper describes a novel waveform-level end-to-end model for multi-channel\nspeech enhancement. The model first extracts sample-level speech embedding\nusing channel-wise convolutional neural network (CNN) and compensates\ntime-delays between the channels based on the embedding, resulting\nin time-aligned multi-channel signals. Then the signals are given as\ninput of multi-channel enhancement extension of WaveUNet which directly\noutputs estimated clean speech waveform. The whole model is trained\nto minimize modified mean squared error (MSE), signal-to-distortion\nratio (SDR) cost, and senone cross-entropy of back-end acoustic model\nat the same time. Evaluated on the CHiME-4 simulated set, the proposed\nsystem outperformed state-of-the-art generalized eigenvalue (GEV) beamformer\nin terms of perceptual evaluation of speech quality (PESQ) and SDR,\nand showed competitive results in short time objective intelligibility\n(STOI). Word-error-rates (WERs) of the system&#8217;s output on simulated\nsets were comparable to that of bidirectional long short-term memory\n(BLSTM) GEV beamformer. However, the system showed relatively high\nWERs on real sets, achieving relative error rate reduction (RERR) of\n14.3% over noisy signal on real evaluation set.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2397",
      "author_area_id": 6,
      "author_area_label": "Speech Coding and Enhancement"
    },
    "gu19b_interspeech": {
      "authors": [
        [
          "Rongzhi",
          "Gu"
        ],
        [
          "Lianwu",
          "Chen"
        ],
        [
          "Shi-Xiong",
          "Zhang"
        ],
        [
          "Jimeng",
          "Zheng"
        ],
        [
          "Yong",
          "Xu"
        ],
        [
          "Meng",
          "Yu"
        ],
        [
          "Dan",
          "Su"
        ],
        [
          "Yuexian",
          "Zou"
        ],
        [
          "Dong",
          "Yu"
        ]
      ],
      "title": "Neural Spatial Filter: Target Speaker Speech Separation Assisted with Directional Information",
      "original": "2266",
      "page_count": 5,
      "order": 899,
      "p1": "4290",
      "pn": "4294",
      "abstract": [
        "The recent exploration of deep learning for supervised speech separation\nhas significantly accelerated the progress on the multi-talker speech\nseparation problem. The multi-channel approaches have attracted much\nresearch attention due to the benefit of spatial information. In this\npaper, integrated with the power spectra and inter-channel spatial\nfeatures at the input level, we explore to leverage directional features,\nwhich imply the speaker source from the desired target direction, for\ntarget speaker separation. In addition, we incorporate an attention\nmechanism to dynamically tune the model&#8217;s attention to the reliable\ninput features to alleviate spatial ambiguity problem when multiple\nspeakers are closely located. We demonstrate, on the far-field WSJ0\n2-mix dataset, that our proposed approach significantly improves the\nperformance of speech separation against the baseline single-channel\nand multi-channel speech separation methods.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2266",
      "author_area_id": 6,
      "author_area_label": "Speech Coding and Enhancement"
    },
    "afouras19_interspeech": {
      "authors": [
        [
          "Triantafyllos",
          "Afouras"
        ],
        [
          "Joon Son",
          "Chung"
        ],
        [
          "Andrew",
          "Zisserman"
        ]
      ],
      "title": "My Lips Are Concealed: Audio-Visual Speech Enhancement Through Obstructions",
      "original": "3114",
      "page_count": 5,
      "order": 900,
      "p1": "4295",
      "pn": "4299",
      "abstract": [
        "Our objective is an audio-visual model for separating a single speaker\nfrom a mixture of sounds such as other speakers and background noise.\nMoreover, we wish to hear the speaker even when the visual cues are\ntemporarily absent due to occlusion.<br style=\"mso-data-placement:same-cell;\"\n/><br style=\"mso-data-placement:same-cell;\" />\n  To this end we introduce\na deep audio-visual speech enhancement network that is able to separate\na speaker&#8217;s voice by conditioning on both the speaker&#8217;s\nlip movements and/or a representation of their voice. The voice representation\ncan be obtained by either (i) enrollment, or (ii) by self-enrollment\n&#8212; learning the representation on-the-fly given sufficient unobstructed\nvisual input. The model is trained by blending audios, and by introducing\nartificial occlusions around the mouth region that prevent the visual\nmodality from dominating.<br style=\"mso-data-placement:same-cell;\"\n/><br style=\"mso-data-placement:same-cell;\" />\n  The method is speaker-independent,\nand we demonstrate it on real examples of speakers unheard (and unseen)\nduring training. The method also improves over previous models in particular\nfor cases of occlusion in the visual modality.\n"
      ],
      "doi": "10.21437/Interspeech.2019-3114",
      "author_area_id": 6,
      "author_area_label": "Speech Coding and Enhancement"
    },
    "fujita19_interspeech": {
      "authors": [
        [
          "Yusuke",
          "Fujita"
        ],
        [
          "Naoyuki",
          "Kanda"
        ],
        [
          "Shota",
          "Horiguchi"
        ],
        [
          "Kenji",
          "Nagamatsu"
        ],
        [
          "Shinji",
          "Watanabe"
        ]
      ],
      "title": "End-to-End Neural Speaker Diarization with Permutation-Free Objectives",
      "original": "2899",
      "page_count": 5,
      "order": 901,
      "p1": "4300",
      "pn": "4304",
      "abstract": [
        "In this paper, we propose a novel end-to-end neural-network-based speaker\ndiarization method. Unlike most existing methods, our proposed method\ndoes not have separate modules for extraction and clustering of speaker\nrepresentations. Instead, our model has a single neural network that\ndirectly outputs speaker diarization results. To realize such a model,\nwe formulate the speaker diarization problem as a multi-label classification\nproblem, and introduces a permutation-free objective function to directly\nminimize diarization errors without being suffered from the speaker-label\npermutation problem. Besides its end-to-end simplicity, the proposed\nmethod also benefits from being able to explicitly handle overlapping\nspeech during training and inference. Because of the benefit, our model\ncan be easily trained/adapted with real-recorded multi-speaker conversations\njust by feeding the corresponding multi-speaker segment labels. We\nevaluated the proposed method on simulated speech mixtures. The proposed\nmethod achieved diarization error rate of 12.28%, while a conventional\nclustering-based system produced diarization error rate of 28.77%.\nFurthermore, the domain adaptation with real-recorded speech provided\n25.6% relative improvement on the CALLHOME dataset.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2899",
      "author_area_id": 4,
      "author_area_label": "Speaker and Language Identification"
    },
    "india19_interspeech": {
      "authors": [
        [
          "Miquel",
          "India"
        ],
        [
          "Pooyan",
          "Safari"
        ],
        [
          "Javier",
          "Hernando"
        ]
      ],
      "title": "Self Multi-Head Attention for Speaker Recognition",
      "original": "2616",
      "page_count": 5,
      "order": 902,
      "p1": "4305",
      "pn": "4309",
      "abstract": [
        "Most state-of-the-art Deep Learning (DL) approaches for speaker recognition\nwork on a short utterance level. Given the speech signal, these algorithms\nextract a sequence of speaker embeddings from short segments and those\nare averaged to obtain an utterance level speaker representation. In\nthis work we propose the use of an attention mechanism to obtain a\ndiscriminative speaker embedding given non fixed length speech utterances.\nOur system is based on a Convolutional Neural Network (CNN) that encodes\nshort-term speaker features from the spectrogram and a self multi-head\nattention model that maps these representations into a long-term speaker\nembedding. The attention model that we propose produces multiple alignments\nfrom different subsegments of the CNN encoded states over the sequence.\nHence this mechanism works as a pooling layer which decides the most\ndiscriminative features over the sequence to obtain an utterance level\nrepresentation. We have tested this approach for the verification task\nfor the VoxCeleb1 dataset. The results show that self multi-head attention\noutperforms both temporal and statistical pooling methods with a 18%\nof relative EER. Obtained results show a 58% relative improvement in\nEER compared to i-vector+PLDA.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2616",
      "author_area_id": 4,
      "author_area_label": "Speaker and Language Identification"
    },
    "vinals19b_interspeech": {
      "authors": [
        [
          "Ignacio",
          "Vi\u00f1als"
        ],
        [
          "Dayana",
          "Ribas"
        ],
        [
          "Victoria",
          "Mingote"
        ],
        [
          "Jorge",
          "Llombart"
        ],
        [
          "Pablo",
          "Gimeno"
        ],
        [
          "Antonio",
          "Miguel"
        ],
        [
          "Alfonso",
          "Ortega"
        ],
        [
          "Eduardo",
          "Lleida"
        ]
      ],
      "title": "Phonetically-Aware Embeddings, Wide Residual Networks with Time-Delay Neural Networks and Self Attention Models for the 2018 NIST Speaker Recognition Evaluation",
      "original": "2417",
      "page_count": 5,
      "order": 903,
      "p1": "4310",
      "pn": "4314",
      "abstract": [
        "Very often, speaker recognition systems do not take into account phonetic\ninformation explicitly. In order to gain insight along this line of\nresearch, we have studied the use of phonetic information in the embedding\nextraction process for automatic speaker verification systems in two\ndifferent ways: on the one hand using the well-known i-vector paradigm\nand, on the other hand, using Wide Residual Networks (WRN) with Time\nDelay Neural Networks (TDNN) and Self-Attention Mechanisms. The phonetic\ninformation is provided by a WRN with TDNN using 1D convolutional layers\nspecifically trained for this purpose. These two approaches along with\nthe widely used x-vector system based on the Kaldi toolkit were submitted\nto the 2018 NIST speaker recognition evaluation. As back-end, these\nrepresentations used a standard PLDA classifier with ad-hoc configurations\nfor each system and in-domain adaptation. The results obtained in the\nNIST SRE 2018 show that our methods are very promising and it is worth\ncontinuing to work on them to improve their performance.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2417",
      "author_area_id": 4,
      "author_area_label": "Speaker and Language Identification"
    },
    "tu19_interspeech": {
      "authors": [
        [
          "Youzhi",
          "Tu"
        ],
        [
          "Man-Wai",
          "Mak"
        ],
        [
          "Jen-Tzung",
          "Chien"
        ]
      ],
      "title": "Variational Domain Adversarial Learning for Speaker Verification",
      "original": "2168",
      "page_count": 5,
      "order": 904,
      "p1": "4315",
      "pn": "4319",
      "abstract": [
        "Domain mismatch refers to the problem in which the distribution of\ntraining data differs from that of the test data. This paper proposes\na variational domain adversarial neural network (VDANN), which consists\nof a variational autoencoder (VAE) and a domain adversarial neural\nnetwork (DANN), to reduce domain mismatch. The DANN part aims to retain\nspeaker identity information and learn a feature space that is robust\nagainst domain mismatch, while the VAE part is to impose variational\nregularization on the learned features so that they follow a Gaussian\ndistribution. Thus, the representation produced by VDANN is not only\nspeaker discriminative and domain-invariant but also Gaussian distributed,\nwhich is essential for the standard PLDA backend. Experiments on both\nSRE16 and SRE18-CMN2 show that VDANN outperforms the Kaldi baseline\nand the standard DANN. The results also suggest that VAE regularization\nis effective for domain adaptation.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2168",
      "author_area_id": 4,
      "author_area_label": "Speaker and Language Identification"
    },
    "liu19m_interspeech": {
      "authors": [
        [
          "Tianchi",
          "Liu"
        ],
        [
          "Maulik",
          "Madhavi"
        ],
        [
          "Rohan Kumar",
          "Das"
        ],
        [
          "Haizhou",
          "Li"
        ]
      ],
      "title": "A Unified Framework for Speaker and Utterance Verification",
      "original": "1994",
      "page_count": 5,
      "order": 905,
      "p1": "4320",
      "pn": "4324",
      "abstract": [
        "Speaker and utterance verification are two tasks that co-exist in text-dependent\nspeaker verification (SV), where a phrase of the same lexical information\nis spoken during train and test sessions. The conventional approaches\nmostly verify the speaker and the utterance separately using two models.\nWhile there are studies on joint modeling of speaker and utterance,\nit is always desirable to have a common framework that performs both\nspeaker and utterance verification to access the intended service.\nTo this end, we propose a unified framework that deals with both objectives\nand the trade-off between the two. The unified framework is based on\nlong short term memory network trained using both speaker and utterance\ninformation. We use Part I of RSR2015 database for the studies in this\nwork. We show that the unified framework not only demonstrates competitive\nSV performance, but also provides a solution for a system to address\ndifferent levels of security need.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1994",
      "author_area_id": 4,
      "author_area_label": "Speaker and Language Identification"
    },
    "nandwana19c_interspeech": {
      "authors": [
        [
          "Mahesh Kumar",
          "Nandwana"
        ],
        [
          "Luciana",
          "Ferrer"
        ],
        [
          "Mitchell",
          "McLaren"
        ],
        [
          "Diego",
          "Castan"
        ],
        [
          "Aaron",
          "Lawson"
        ]
      ],
      "title": "Analysis of Critical Metadata Factors for the Calibration of Speaker Recognition Systems",
      "original": "1808",
      "page_count": 5,
      "order": 906,
      "p1": "4325",
      "pn": "4329",
      "abstract": [
        "In this paper, we analyze and assess the impact of critical metadata\nfactors on the calibration performance of speaker recognition systems.\nIn particular, we study the effect of duration, distance, language,\nand gender by using a variety of datasets and systematically varying\nthe conditions in the evaluation and calibration sets. For all experiments,\nthe system is based on i-vectors and a probabilistic linear discriminant\nanalysis (PLDA) back-end and linear calibration. We measure system\nperformance in terms of calibration loss. Our experiments reveal (i)\na large degradation when the duration used for calibration is significantly\ndifferent from that in the evaluation set; (ii) no significant degradation\nwhen a different gender is used for calibration than for evaluation;\n(iii) a large degradation when microphone distance is significantly\ndifferent between the sets; and (iv) a small loss for closely related\nlanguages and languages with shared vocabulary. This analysis will\nbe beneficial in the development of speaker recognition systems for\nuse in unseen environments and for forensic speaker recognition analysts\nwhen selecting relevant population data.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1808",
      "author_area_id": 4,
      "author_area_label": "Speaker and Language Identification"
    },
    "novotny19_interspeech": {
      "authors": [
        [
          "Ond\u0159ej",
          "Novotn\u00fd"
        ],
        [
          "Old\u0159ich",
          "Plchot"
        ],
        [
          "Ond\u0159ej",
          "Glembek"
        ],
        [
          "Luk\u00e1\u0161",
          "Burget"
        ]
      ],
      "title": "Factorization of Discriminatively Trained i-Vector Extractor for Speaker Recognition",
      "original": "1757",
      "page_count": 5,
      "order": 907,
      "p1": "4330",
      "pn": "4334",
      "abstract": [
        "In this work, we continue in our research on i-vector extractor for\nspeaker verification (SV) and we optimize its architecture for fast\nand effective discriminative training. We were motivated by computational\nand memory requirements caused by the large number of parameters of\nthe original generative i-vector model. Our aim is to preserve the\npower of the original generative model, and at the same time focus\nthe model towards extraction of speaker-related information. We show\nthat it is possible to represent a standard generative i-vector extractor\nby a model with significantly less parameters and obtain similar performance\non SV tasks. We can further refine this compact model by discriminative\ntraining and obtain i-vectors that lead to better performance on various\nSV benchmarks representing different acoustic domains.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1757",
      "author_area_id": 4,
      "author_area_label": "Speaker and Language Identification"
    },
    "salvati19_interspeech": {
      "authors": [
        [
          "Daniele",
          "Salvati"
        ],
        [
          "Carlo",
          "Drioli"
        ],
        [
          "Gian Luca",
          "Foresti"
        ]
      ],
      "title": "End-to-End Speaker Identification in Noisy and Reverberant Environments Using Raw Waveform Convolutional Neural Networks",
      "original": "2403",
      "page_count": 5,
      "order": 908,
      "p1": "4335",
      "pn": "4339",
      "abstract": [
        "Convolutional neural network (CNN) models are being investigated extensively\nin the field of speech and speaker recognition, and are rapidly gaining\nappreciation due to their performance robustness and effective training\nstrategies. Recently, they are also providing interesting results in\nend-to-end configurations using directly raw waveforms for classification,\nwith the drawback however of being more sensible on the amount of training\ndata. We present a raw waveform (RW) end-to-end computational scheme\nfor speaker identification based on CNNs with noise and reverberation\ndata augmentation (DA). The CNN is designed for a frame-to-frame analysis\nto handle variable-length signals. We analyze the identification performance\nwith simulated experiments in noisy and reverberation conditions comparing\nthe proposed RW-CNN with the mel-frequency cepstral coefficients (MFCCs)\nfeatures. The results show that the method offers robustness to adverse\nconditions. The RW-CNN outperforms the MFCC-CNN in noise conditions,\nand they have similar performance in reverberant environments.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2403",
      "author_area_id": 4,
      "author_area_label": "Speaker and Language Identification"
    },
    "naini19_interspeech": {
      "authors": [
        [
          "Abinay Reddy",
          "Naini"
        ],
        [
          "Achuth Rao",
          "M.V."
        ],
        [
          "Prasanta Kumar",
          "Ghosh"
        ]
      ],
      "title": "Whisper to Neutral Mapping Using Cosine Similarity Maximization in i-Vector Space for Speaker Verification",
      "original": "2280",
      "page_count": 5,
      "order": 909,
      "p1": "4340",
      "pn": "4344",
      "abstract": [
        "In this work, we propose a novel feature mapping (FM) from whispered\nto neutral speech features using a cosine similarity based objective\nfunction for speaker verification (SV) using whispered speech. Typically\nthe performance of an SV system enrolled with neutral speech degrades\nsignificantly when tested using whispered speech, due to the differences\nbetween spectral characteristics of neutral and whispered speech. We\nhypothesize that FM from whispered Mel frequency cepstral coefficients\n(MFCC) to neutral MFCC by maximizing cosine similarity between neutral\nand whisper i-vectors yields better performance than the baseline method,\nwhich typically performs a direct FM between MFCC features by minimizing\nmean squared error (MSE). We also explored an affine transform between\nMFCC features using the proposed objective function. Whisper SV experiments\nwith 1882 speakers reveal that the equal error rate (EER) using the\nproposed method is lower than that using the best baseline by &#126;24%\n(relative). We show that the proposed FM system maintains the neutral\nSV performance, while improving the EER of whispered SV unlike baseline\nmethods. We also show that the bias in the learned affine transform\nis corresponds to the glottal flow information, which is absent in\nthe whispered speech.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2280",
      "author_area_id": 4,
      "author_area_label": "Speaker and Language Identification"
    },
    "zhu19b_interspeech": {
      "authors": [
        [
          "Yingke",
          "Zhu"
        ],
        [
          "Tom",
          "Ko"
        ],
        [
          "Brian",
          "Mak"
        ]
      ],
      "title": "Mixup Learning Strategies for Text-Independent Speaker Verification",
      "original": "2250",
      "page_count": 5,
      "order": 910,
      "p1": "4345",
      "pn": "4349",
      "abstract": [
        "Mixup is a learning strategy that constructs additional virtual training\nsamples from existing training samples by linearly interpolating random\npairs of them. It has been shown that mixup can help avoid data memorization\nand thus improve model generalization. This paper investigates the\nmixup learning strategy in training speaker-discriminative deep neural\nnetwork (DNN) for better text-independent speaker verification.<br\nstyle=\"mso-data-placement:same-cell;\" /><br style=\"mso-data-placement:same-cell;\"\n/>\n  In recent speaker verification systems, a DNN is usually trained\nto classify speakers in the training set. The DNN, at the same time,\nlearns a low-dimensional embedding of speakers so that speaker embeddings\ncan be generated for any speakers during evaluation. We adapted the\nmixup strategy to the speaker-discriminative DNN training procedure,\nand studied different mixup schemes, such as performing mixup on MFCC\nfeatures or raw audio samples. The mixup learning strategy was evaluated\non NIST SRE 2010, 2016 and SITW evaluation sets. Experimental results\nshow consistent performance improvements both in terms of EER and DCF\nof up to 13% relative. We further find that mixup training also improves\nthe DNN&#8217;s speaker classification accuracy consistently without\nrequiring any additional data sources.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2250",
      "author_area_id": 4,
      "author_area_label": "Speaker and Language Identification"
    },
    "ferrer19_interspeech": {
      "authors": [
        [
          "Luciana",
          "Ferrer"
        ],
        [
          "Mitchell",
          "McLaren"
        ]
      ],
      "title": "Optimizing a Speaker Embedding Extractor Through Backend-Driven Regularization",
      "original": "1820",
      "page_count": 5,
      "order": 911,
      "p1": "4350",
      "pn": "4354",
      "abstract": [
        "State-of-the-art speaker verification systems use deep neural networks\n(DNN) to extract highly discriminant representations of the samples,\ncommonly called speaker embeddings. The networks are trained to maximize\nthe cross-entropy between the estimated posteriors and the speaker\nlabels. The pre-activations from one of the last layers in that network\nare used as embeddings. These sample-level vectors are then used as\ninput to a backend that generates the final scores. The most successful\nbackend for speaker verification to date is the probabilistic linear\ndiscriminant analysis (PLDA) backend. The full process consists of\na linear discriminant analysis (LDA) projection of the embeddings,\nfollowed by mean and length normalization, ending with PLDA for score\ncomputation. While this procedure works very well compared to other\napproaches, it seems to be inherently suboptimal since the embeddings\nextractor is not directly trained to optimize the performance of the\nembeddings when using the PLDA backend for scoring. In this work, we\npropose one way to encourage the DNN to generate embeddings that are\noptimized for use in the PLDA backend, by adding a secondary objective\ndesigned to measure the performance of a such backend within the network.\nWe show modest but consistent gains across several speaker recognition\ndatasets.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1820",
      "author_area_id": 4,
      "author_area_label": "Speaker and Language Identification"
    },
    "lee19g_interspeech": {
      "authors": [
        [
          "Kong Aik",
          "Lee"
        ],
        [
          "Hitoshi",
          "Yamamoto"
        ],
        [
          "Koji",
          "Okabe"
        ],
        [
          "Qiongqiong",
          "Wang"
        ],
        [
          "Ling",
          "Guo"
        ],
        [
          "Takafumi",
          "Koshinaka"
        ],
        [
          "Jiacen",
          "Zhang"
        ],
        [
          "Koichi",
          "Shinoda"
        ]
      ],
      "title": "The NEC-TT 2018 Speaker Verification System",
      "original": "1517",
      "page_count": 5,
      "order": 912,
      "p1": "4355",
      "pn": "4359",
      "abstract": [
        "This paper describes the NEC-TT speaker verification system for the\n2018 NIST  speaker recognition evaluation (SRE&#8217;18). We present\nthe details of data partitioning, x-vector speaker embedding, data\naugmentation, speaker diarization, and domain adaptation techniques\nused in NEC-TT SRE&#8217;18 speaker verification system. For the speaker\nembedding front-end, we found that the amount and diversity of training\ndata are essential to improve the robustness of the x-vector extractor.\nThis was achieved with data augmentation and mixed-bandwidth training\nin our submission. For the multi-speaker test scenario, we show that\nx-vector based speaker diarization is promising and holds potential\nfor future research. For the scoring back-end, we used two variants\nof probabilistic linear discriminant analysis (PLDA), namely, the Gaussian\nPLDA and heavy-tailed PLDA. We show that correlation alignment (CORAL)\nand CORAL+ unsupervised PLDA adaptation are effective to deal with\ndomain mismatch.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1517",
      "author_area_id": 4,
      "author_area_label": "Speaker and Language Identification"
    },
    "zheng19c_interspeech": {
      "authors": [
        [
          "Siqi",
          "Zheng"
        ],
        [
          "Gang",
          "Liu"
        ],
        [
          "Hongbin",
          "Suo"
        ],
        [
          "Yun",
          "Lei"
        ]
      ],
      "title": "Autoencoder-Based Semi-Supervised Curriculum Learning for Out-of-Domain Speaker Verification",
      "original": "1440",
      "page_count": 5,
      "order": 913,
      "p1": "4360",
      "pn": "4364",
      "abstract": [
        "This study aims to improve the performance of speaker verification\nsystem when no labeled out-of-domain data is available. An autoencoder-based\nsemi-supervised curriculum learning scheme is proposed to automatically\ncluster unlabeled data and iteratively update the corpus during training.\nThis new training scheme allows us to (1) progressively expand the\nsize of training corpus by utilizing unlabeled data and correcting\nprevious labels at run-time; and (2) improve robustness when generalizing\nto multiple conditions, such as out-of-domain and text-independent\nspeaker verification tasks. It is also discovered that a denoising\nautoencoder can significantly enhance the clustering accuracy when\nit is trained on carefully-selected subset of speakers. Our experimental\nresults show a relative reduction of 30%&#8211;50% in EER compared\nto the baseline.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1440",
      "author_area_id": 4,
      "author_area_label": "Speaker and Language Identification"
    },
    "cai19d_interspeech": {
      "authors": [
        [
          "Danwei",
          "Cai"
        ],
        [
          "Xiaoyi",
          "Qin"
        ],
        [
          "Ming",
          "Li"
        ]
      ],
      "title": "Multi-Channel Training for End-to-End Speaker Recognition Under Reverberant and Noisy Environment",
      "original": "1437",
      "page_count": 5,
      "order": 914,
      "p1": "4365",
      "pn": "4369",
      "abstract": [
        "Despite the significant improvements in speaker recognition enabled\nby deep neural networks, unsatisfactory performance persists under\nfar-field scenarios due to the effects of the long range fading, room\nreverberation, and environmental noises. In this study, we focus on\nfar-field speaker recognition with a microphone array. We propose a\nmulti-channel training framework for the deep speaker embedding neural\nnetwork on noisy and reverberant data. The proposed multi-channel training\nframework simultaneously processes the time-, frequency- and channel-information\nto learn a robust deep speaker embedding. Based on the 2-dimensional\nor 3-dimensional convolution layer, we investigate different multi-channel\ntraining schemes. Experiments on the simulated multi-channel reverberant\nand noisy data show that the proposed method obtains significant improvements\nover the single-channel trained deep speaker embedding system with\nfront end speech enhancement or multi-channel embedding fusion.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1437",
      "author_area_id": 4,
      "author_area_label": "Speaker and Language Identification"
    },
    "cai19e_interspeech": {
      "authors": [
        [
          "Danwei",
          "Cai"
        ],
        [
          "Weicheng",
          "Cai"
        ],
        [
          "Ming",
          "Li"
        ]
      ],
      "title": "The DKU-SMIIP System for NIST 2018 Speaker Recognition Evaluation",
      "original": "1436",
      "page_count": 5,
      "order": 915,
      "p1": "4370",
      "pn": "4374",
      "abstract": [
        "In this paper, we present the system submission for the NIST 2018 Speaker\nRecognition Evaluation by DKU Speech and Multi-Modal Intelligent Information\nProcessing (SMIIP) Lab. We explore various kinds of state-of-the-art\nfront-end extractors as well as back-end modeling for text-independent\nspeaker verifications. Our submitted primary systems employ multiple\nstate-of-the-art front-end extractors, including the MFCC i-vector,\nthe DNN tandem i-vector, the TDNN x-vector, and the deep ResNet. After\nspeaker embedding is extracted, we exploit several kinds of back-end\nmodeling to perform variability compensation and domain adaptation\nfor mismatch training and testing conditions. The final submitted system\non the fixed condition obtains actual detection cost of 0.392 and 0.494\non CMN2 and VAST evaluation data respectively. After the official evaluation,\nwe further extend our experiments by investigating multiple encoding\nlayer designs and loss functions for the deep ResNet system.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1436",
      "author_area_id": 4,
      "author_area_label": "Speaker and Language Identification"
    },
    "wiesner19_interspeech": {
      "authors": [
        [
          "Matthew",
          "Wiesner"
        ],
        [
          "Adithya",
          "Renduchintala"
        ],
        [
          "Shinji",
          "Watanabe"
        ],
        [
          "Chunxi",
          "Liu"
        ],
        [
          "Najim",
          "Dehak"
        ],
        [
          "Sanjeev",
          "Khudanpur"
        ]
      ],
      "title": "Pretraining by Backtranslation for End-to-End ASR in Low-Resource Settings",
      "original": "3254",
      "page_count": 5,
      "order": 916,
      "p1": "4375",
      "pn": "4379",
      "abstract": [
        "We explore training attention-based encoder-decoder ASR in low-resource\nsettings. These models perform poorly when trained on small amounts\nof transcribed speech, in part because they depend on having sufficient\ntarget-side text to train the attention and decoder networks. In this\npaper we address this shortcoming by pretraining our network parameters\nusing only text-based data and transcribed speech from other languages.\nWe analyze the relative contributions of both sources of data. Across\n3 test languages, our text-based approach resulted in a 20% average\nrelative improvement over a text-based augmentation technique without\npretraining. Using transcribed speech from nearby languages gives a\nfurther 20&#8211;30% relative reduction in character error rate.\n"
      ],
      "doi": "10.21437/Interspeech.2019-3254",
      "author_area_id": 8,
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "kim19b_interspeech": {
      "authors": [
        [
          "Suyoun",
          "Kim"
        ],
        [
          "Siddharth",
          "Dalmia"
        ],
        [
          "Florian",
          "Metze"
        ]
      ],
      "title": "Cross-Attention End-to-End ASR for Two-Party Conversations",
      "original": "3173",
      "page_count": 5,
      "order": 917,
      "p1": "4380",
      "pn": "4384",
      "abstract": [
        "We present an end-to-end speech recognition model that learns interaction\nbetween two speakers based on the turn-changing information. Unlike\nconventional speech recognition models, our model exploits two speakers\nhistory of conversational-context information that spans across multiple\nturns within an end-to-end framework. Specifically, we propose a speaker-specific\ncross-attention mechanism that can look at the output of the other\nspeaker side as well as the one of the current speaker for better at\nrecognizing long conversations. We evaluated the models on the Switchboard\nconversational speech corpus and show that our model outperforms standard\nend-to-end speech recognition models.\n"
      ],
      "doi": "10.21437/Interspeech.2019-3173",
      "author_area_id": 8,
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "chorowski19_interspeech": {
      "authors": [
        [
          "Jan",
          "Chorowski"
        ],
        [
          "Adrian",
          "\u0141a\u0144cucki"
        ],
        [
          "Bartosz",
          "Kostka"
        ],
        [
          "Micha\u0142",
          "Zapotoczny"
        ]
      ],
      "title": "Towards Using Context-Dependent Symbols in CTC Without State-Tying Decision Trees",
      "original": "2720",
      "page_count": 5,
      "order": 918,
      "p1": "4385",
      "pn": "4389",
      "abstract": [
        "Deep neural acoustic models benefit from context-dependent (CD) modeling\nof output symbols. We consider direct training of CTC networks with\nCD outputs, and identify two issues. The first one is frame-level normalization\nof probabilities in CTC, which induces strong language modeling behavior\nthat leads to overfitting and interference with external language models.\nThe second one is poor generalization in the presence of numerous lexical\nunits like triphones or tri-chars. We mitigate the former with utterance-level\nnormalization of probabilities. The latter typically requires reducing\nthe CD symbol inventory with state-tying decision trees, which have\nto be transferred from classical GMM-HMM systems. We replace the trees\nwith a CD symbol embedding network, which saves parameters and ensures\ngeneralization to unseen and undersampled CD symbols. The embedding\nnetwork is trained together with the rest of the acoustic model and\nremoves one of the last cases in which neural systems have to be bootstrapped\nfrom GMM-HMM ones.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2720",
      "author_area_id": 8,
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "fan19b_interspeech": {
      "authors": [
        [
          "Ruchao",
          "Fan"
        ],
        [
          "Pan",
          "Zhou"
        ],
        [
          "Wei",
          "Chen"
        ],
        [
          "Jia",
          "Jia"
        ],
        [
          "Gang",
          "Liu"
        ]
      ],
      "title": "An Online Attention-Based Model for Speech Recognition",
      "original": "2218",
      "page_count": 5,
      "order": 919,
      "p1": "4390",
      "pn": "4394",
      "abstract": [
        "Attention-based end-to-end models such as Listen, Attend and Spell\n(LAS), simplify the whole pipeline of traditional automatic speech\nrecognition (ASR) systems and become popular in the field of speech\nrecognition. In previous work, researchers have shown that such architectures\ncan acquire comparable results to state-of-the-art ASR systems, especially\nwhen using a bidirectional encoder and global soft attention (GSA)\nmechanism. However, bidirectional encoder and GSA are two obstacles\nfor real-time speech recognition. In this work, we aim to stream LAS\nbaseline by removing the above two obstacles. On the encoder side,\nwe use a latency-controlled (LC) bidirectional structure to reduce\nthe delay of forward computation. Meanwhile, an adaptive monotonic\nchunk-wise attention (AMoChA) mechanism is proposed to replace GSA\nfor the calculation of attention weight distribution. Furthermore,\nwe propose two methods to alleviate the huge performance degradation\nwhen combining LC and AMoChA. Finally, we successfully acquire an online\nLAS model, LC-AMoChA, which has only 3.5% relative performance reduction\nto LAS baseline on our internal Mandarin corpus.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2218",
      "author_area_id": 8,
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "tian19b_interspeech": {
      "authors": [
        [
          "Zhengkun",
          "Tian"
        ],
        [
          "Jiangyan",
          "Yi"
        ],
        [
          "Jianhua",
          "Tao"
        ],
        [
          "Ye",
          "Bai"
        ],
        [
          "Zhengqi",
          "Wen"
        ]
      ],
      "title": "Self-Attention Transducers for End-to-End Speech Recognition",
      "original": "2203",
      "page_count": 5,
      "order": 920,
      "p1": "4395",
      "pn": "4399",
      "abstract": [
        "Recurrent neural network transducers (RNN-T) have been successfully\napplied in end-to-end speech recognition. However, the recurrent structure\nmakes it difficult for parallelization. In this paper, we propose a\nself-attention transducer (SA-T) for speech recognition. RNNs are replaced\nwith self-attention blocks, which are powerful to model long-term dependencies\ninside sequences and able to be efficiently parallelized. Furthermore,\na path-aware regularization is proposed to assist SA-T to learn alignments\nand improve the performance. Additionally, a chunk-flow mechanism is\nutilized to achieve online decoding. All experiments are conducted\non a Mandarin Chinese dataset AISHELL-1. The results demonstrate that\nour proposed approach achieves a 21.3% relative reduction in character\nerror rate compared with the baseline RNN-T. In addition, the SA-T\nwith chunk-flow mechanism can perform online decoding with only a little\ndegradation of the performance.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2203",
      "author_area_id": 8,
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "li19u_interspeech": {
      "authors": [
        [
          "Sheng",
          "Li"
        ],
        [
          "Dabre",
          "Raj"
        ],
        [
          "Xugang",
          "Lu"
        ],
        [
          "Peng",
          "Shen"
        ],
        [
          "Tatsuya",
          "Kawahara"
        ],
        [
          "Hisashi",
          "Kawai"
        ]
      ],
      "title": "Improving Transformer-Based Speech Recognition Systems with Compressed Structure and Speech Attributes Augmentation",
      "original": "2112",
      "page_count": 5,
      "order": 921,
      "p1": "4400",
      "pn": "4404",
      "abstract": [
        "The end-to-end (E2E) model allows for training of automatic speech\nrecognition (ASR) systems without having to consider the acoustic model,\nlexicon, language model and complicated decoding algorithms, which\nare integral to conventional ASR systems. Recently, the transformer-based\nE2E ASR model (ASR-Transformer) showed promising results in many speech\nrecognition tasks. The most common practice is to stack a number of\nfeed-forward layers in the encoder and decoder. As a result, the addition\nof new layers improves speech recognition performance significantly.\nHowever, this also leads to a large increase in the number of parameters\nand severe decoding latency. In this paper, we propose to reduce the\nmodel complexity by simply reusing parameters across all stacked layers\ninstead of introducing new parameters per layer. In order to address\nthe slight reduction in recognition quality we propose to augment the\nspeech inputs with bags-of-attributes. As a result we obtain a highly\ncompressed, efficient and high quality ASR model.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2112",
      "author_area_id": 8,
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "bang19_interspeech": {
      "authors": [
        [
          "Jeong-Uk",
          "Bang"
        ],
        [
          "Mu-Yeol",
          "Choi"
        ],
        [
          "Sang-Hun",
          "Kim"
        ],
        [
          "Oh-Wook",
          "Kwon"
        ]
      ],
      "title": "Extending an Acoustic Data-Driven Phone Set for Spontaneous Speech Recognition",
      "original": "1979",
      "page_count": 5,
      "order": 922,
      "p1": "4405",
      "pn": "4409",
      "abstract": [
        "In this paper, we propose a method to extend a phone set by using a\nlarge amount of Korean broadcast data to improve the performance of\nspontaneous speech recognition. The proposed method first extracts\nvariable-length phoneme-level segments from broadcast data, and then\nconverts them into fixed-length latent vectors based on an LSTM architecture.\nThen, we used the k-means algorithm to cluster acoustically similar\nlatent vectors and then build a new phone set by gathering the clustered\nvectors. To update the lexicon of a speech recognizer, we choose the\npronunciation sequence of each word with the highest conditional probability.\nTo verify the performance of the proposed unit, we visualize the spectral\npatterns and segment duration for the new phone set. In both spontaneous\nand read speech recognition tasks, the proposed unit is shown to produce\nbetter performance than the phoneme-based and grapheme-based units.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1979",
      "author_area_id": 8,
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "moriya19_interspeech": {
      "authors": [
        [
          "Takafumi",
          "Moriya"
        ],
        [
          "Jian",
          "Wang"
        ],
        [
          "Tomohiro",
          "Tanaka"
        ],
        [
          "Ryo",
          "Masumura"
        ],
        [
          "Yusuke",
          "Shinohara"
        ],
        [
          "Yoshikazu",
          "Yamaguchi"
        ],
        [
          "Yushi",
          "Aono"
        ]
      ],
      "title": "Joint Maximization Decoder with Neural Converters for Fully Neural Network-Based Japanese Speech Recognition",
      "original": "1558",
      "page_count": 5,
      "order": 923,
      "p1": "4410",
      "pn": "4414",
      "abstract": [
        "We present a novel fully neural network (FNN) -based automatic speech\nrecognition (ASR) system that addresses the out-of-vocabulary (OOV)\nproblem. The most common approach to the OOV problem is leveraging\ncharacter/sub-word level units as output symbols. Unfortunately, this\napproach is not suitable for Japanese and Mandarin Chinese since they\nhave many more grapheme sets than English. Our solution is to develop\nFNN-based ASR that uses a pronunciation-based unit set with dictionaries,\ni.e., word-to-pronunciation rules. A previous study proposed, for Mandarin\nChinese, a greedy cascading decoder (GCD) that uses two neural converters,\nacoustic-to-pronunciation (A2P) and pronunciation-to-word (P2W) conversion\nmodels. However, to generate optimal word sequences, the previous work\nconsidered just optimal pronunciation sequences. In this paper, we\npropose a joint maximization decoder (JMD) that considers the joint\nprobability of pronunciation and word in beam-search decoding. Moreover,\nwe introduce a neural network based joint source channel model for\nimproving A2P conversion performance. Experiments on Japanese ASR tasks\ndemonstrate that JMD achieves better performance than GCD. Furthermore,\nwe show the effectiveness of using just language resources to retrain\nthe P2W conversion model.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1558",
      "author_area_id": 8,
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "parcollet19b_interspeech": {
      "authors": [
        [
          "Titouan",
          "Parcollet"
        ],
        [
          "Mohamed",
          "Morchid"
        ],
        [
          "Georges",
          "Linar\u00e8s"
        ],
        [
          "Renato De",
          "Mori"
        ]
      ],
      "title": "Real to H-Space Encoder for Speech Recognition",
      "original": "1539",
      "page_count": 5,
      "order": 924,
      "p1": "4415",
      "pn": "4419",
      "abstract": [
        "Deep neural networks (DNNs) and more precisely recurrent neural networks\n(RNNs) are at the core of modern automatic speech recognition systems,\ndue to their efficiency to process input sequences. Recently, it has\nbeen shown that different input representations, based on multidimensional\nalgebras, such as complex and quaternion numbers, are able to bring\nto neural networks a more natural, compressive and powerful representation\nof the input signal by outperforming common real-valued NNs. Indeed,\nquaternion-valued neural networks (QNNs) better learn both internal\ndependencies, such as the relation between the Mel-filter-bank value\nof a specific time frame and its time derivatives, and global dependencies,\ndescribing the relations that exist between time frames. Nonetheless,\nQNNs are limited to quaternion-valued input signals, and it is difficult\nto benefit from this powerful representation with real-valued input\ndata. This paper proposes to tackle this weakness by introducing a\nreal-to-quaternion encoder that allows QNNs to process any one dimensional\ninput features, such as traditional Mel-filter-banks for automatic\nspeech recognition.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1539",
      "author_area_id": 8,
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "yi19b_interspeech": {
      "authors": [
        [
          "Cheng",
          "Yi"
        ],
        [
          "Feng",
          "Wang"
        ],
        [
          "Bo",
          "Xu"
        ]
      ],
      "title": " Ectc-Docd: An End-to-End Structure with CTC Encoder and OCD Decoder for Speech Recognition",
      "original": "1212",
      "page_count": 5,
      "order": 925,
      "p1": "4420",
      "pn": "4424",
      "abstract": [
        "Real-time streaming speech recognition is required by most applications\nfor a nice interactive experience. To naturally support online recognition,\na common strategy used in recently proposed end-to-end models is to\nintroduce a blank label to the label set and instead output alignments.\nHowever, generating the alignment means decoding much longer than the\nlength of the linguistic sequence. Besides, there exist several blank\nlabels between two output units in the alignment, which hinders models\nfrom learning the adjacent dependency of units in the target sequence.\nIn this work, we propose an innovative encoder-decoder structure, called\n Ectc-Docd, for online speech recognition which directly predicts the\nlinguistic sequence without blank labels. Apart from the encoder and\ndecoder structures,  Ectc-Docd contains an additional shrinking layer\nto drop the redundant acoustic information. This layer serves as a\nbridge connecting acoustic representation and linguistic modelling\nparts. Through experiments, we confirm that  Ectc-Docd can obtain better\nperformance than a strong CTC model in online ASR tasks. We also show\nthat  Ectc-Docd can achieve promising results on both Mandarin and\nEnglish ASR datasets with first and second pass decoding.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1212",
      "author_area_id": 8,
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "denisov19_interspeech": {
      "authors": [
        [
          "Pavel",
          "Denisov"
        ],
        [
          "Ngoc Thang",
          "Vu"
        ]
      ],
      "title": "End-to-End Multi-Speaker Speech Recognition Using Speaker Embeddings and Transfer Learning",
      "original": "1130",
      "page_count": 5,
      "order": 926,
      "p1": "4425",
      "pn": "4429",
      "abstract": [
        "This paper presents our latest investigation on end-to-end automatic\nspeech recognition (ASR) for overlapped speech. We propose to train\nan end-to-end system conditioned on speaker embeddings and further\nimproved by transfer learning from clean speech. This proposed framework\ndoes not require any parallel non-overlapped speech materials and is\nindependent of the number of speakers. Our experimental results on\noverlapped speech datasets show that joint conditioning on speaker\nembeddings and transfer learning significantly improves the ASR performance.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1130",
      "author_area_id": 8,
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "hayashi19_interspeech": {
      "authors": [
        [
          "Tomoki",
          "Hayashi"
        ],
        [
          "Shinji",
          "Watanabe"
        ],
        [
          "Tomoki",
          "Toda"
        ],
        [
          "Kazuya",
          "Takeda"
        ],
        [
          "Shubham",
          "Toshniwal"
        ],
        [
          "Karen",
          "Livescu"
        ]
      ],
      "title": "Pre-Trained Text Embeddings for Enhanced Text-to-Speech Synthesis",
      "original": "3177",
      "page_count": 5,
      "order": 927,
      "p1": "4430",
      "pn": "4434",
      "abstract": [
        "We propose an end-to-end text-to-speech (TTS) synthesis model that\nexplicitly uses information from pre-trained embeddings of the text.\nRecent work in natural language processing has developed self-supervised\nrepresentations of text that have proven very effective as pre-training\nfor language understanding tasks. We propose using one such pre-trained\nrepresentation (BERT) to encode input phrases, as an additional input\nto a Tacotron2-based sequence-to-sequence TTS model. We hypothesize\nthat the text embeddings contain information about the semantics of\nthe phrase and the importance of each word, which should help TTS systems\nproduce more natural prosody and pronunciation. We conduct subjective\nlistening tests of our proposed models using the 24-hour LJSpeech corpus,\nfinding that they improve mean opinion scores modestly but significantly\nover a baseline TTS model without pre-trained text embedding input.\n"
      ],
      "doi": "10.21437/Interspeech.2019-3177",
      "author_area_id": 7,
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "szekely19b_interspeech": {
      "authors": [
        [
          "\u00c9va",
          "Sz\u00e9kely"
        ],
        [
          "Gustav Eje",
          "Henter"
        ],
        [
          "Jonas",
          "Beskow"
        ],
        [
          "Joakim",
          "Gustafson"
        ]
      ],
      "title": "Spontaneous Conversational Speech Synthesis from Found Data",
      "original": "2836",
      "page_count": 5,
      "order": 928,
      "p1": "4435",
      "pn": "4439",
      "abstract": [
        "Synthesising spontaneous speech is a difficult task due to disfluencies,\nhigh variability and syntactic conventions different from those of\nwritten language. Using found data, as opposed to lab-recorded conversations,\nfor speech synthesis adds to these challenges because of overlapping\nspeech and the lack of control over recording conditions. In this paper\nwe address these challenges by using a speaker-dependent CNN-LSTM breath\ndetector to separate continuous recordings into utterances, which we\nhere apply to extract nine hours of clean single-speaker breath groups\nfrom a conversational podcast. The resulting corpus is transcribed\nautomatically (both lexical items and filler tokens) and used to build\nseveral voices on a Tacotron 2 architecture. Listening tests show:\ni) pronunciation accuracy improved with phonetic input and transfer\nlearning; ii) it is possible to create a more fluent conversational\nvoice by training on data without filled pauses; and iii) the presence\nof filled pauses improved perceived speaker authenticity. Another listening\ntest showed the found podcast voice to be more appropriate for prompts\nfrom both public speeches and casual conversations, compared to synthesis\nfrom found read speech and from a manually transcribed lab-recorded\nspontaneous conversation.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2836",
      "author_area_id": 7,
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "klimkov19_interspeech": {
      "authors": [
        [
          "Viacheslav",
          "Klimkov"
        ],
        [
          "Srikanth",
          "Ronanki"
        ],
        [
          "Jonas",
          "Rohnke"
        ],
        [
          "Thomas",
          "Drugman"
        ]
      ],
      "title": "Fine-Grained Robust Prosody Transfer for Single-Speaker Neural Text-To-Speech",
      "original": "2571",
      "page_count": 5,
      "order": 929,
      "p1": "4440",
      "pn": "4444",
      "abstract": [
        "We present a neural text-to-speech system for fine-grained prosody\ntransfer from one speaker to another. Conventional approaches for end-to-end\nprosody transfer typically use either fixed-dimensional or variable-length\nprosody embedding via a secondary attention to encode the reference\nsignal. However, when trained on a single-speaker dataset, the conventional\nprosody transfer systems are not robust enough to speaker variability,\nespecially in the case of a reference signal coming from an unseen\nspeaker. Therefore, we propose decoupling of the reference signal alignment\nfrom the overall system. For this purpose, we pre-compute phoneme-level\ntime stamps and use them to aggregate prosodic features per phoneme,\ninjecting them into a sequence-to-sequence text-to-speech system. We\nincorporate a variational auto-encoder to further enhance the latent\nrepresentation of prosody embeddings. We show that our proposed approach\nis significantly more stable and achieves reliable prosody transplantation\nfrom an unseen speaker. We also propose a solution to the use case\nin which the transcription of the reference signal is absent. We evaluate\nall our proposed methods using both objective and subjective listening\ntests.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2571",
      "author_area_id": 7,
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "hussain19_interspeech": {
      "authors": [
        [
          "Nusrah",
          "Hussain"
        ],
        [
          "Engin",
          "Erzin"
        ],
        [
          "T. Metin",
          "Sezgin"
        ],
        [
          "Y\u00fccel",
          "Yemez"
        ]
      ],
      "title": "Speech Driven Backchannel Generation Using Deep Q-Network for Enhancing Engagement in Human-Robot Interaction",
      "original": "2521",
      "page_count": 5,
      "order": 930,
      "p1": "4445",
      "pn": "4449",
      "abstract": [
        "We present a novel method for training a social robot to generate backchannels\nduring human-robot interaction. We address the problem within an off-policy\nreinforcement learning framework, and show how a robot may learn to\nproduce non-verbal backchannels like laughs, when trained to maximize\nthe engagement and attention of the user. A major contribution of this\nwork is the formulation of the problem as a Markov decision process\n(MDP) with states defined by the speech activity of the user and rewards\ngenerated by quantified engagement levels. The problem that we address\nfalls into the class of applications where unlimited interaction with\nthe environment is not possible (our environment being a human) because\nit may be time-consuming, costly, impracticable or even dangerous in\ncase a bad policy is executed. Therefore, we introduce deep Q-network\n(DQN) in a batch reinforcement learning framework, where an optimal\npolicy is learned from a batch data collected using a more controlled\npolicy. We suggest the use of human-to-human dyadic interaction datasets\nas a batch of trajectories to train an agent for engaging interactions.\nOur experiments demonstrate the potential of our method to train a\nrobot for engaging behaviors in an offline manner.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2521",
      "author_area_id": 7,
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "koriyama19_interspeech": {
      "authors": [
        [
          "Tomoki",
          "Koriyama"
        ],
        [
          "Takao",
          "Kobayashi"
        ]
      ],
      "title": "Semi-Supervised Prosody Modeling Using Deep Gaussian Process Latent Variable Model",
      "original": "2497",
      "page_count": 5,
      "order": 931,
      "p1": "4450",
      "pn": "4454",
      "abstract": [
        "This paper proposes a semi-supervised speech synthesis framework in\nwhich prosodic labels of training data are partially annotated. When\nwe construct a text-to-speech (TTS) system, it is crucial to use appropriately\nannotated prosodic labels. For this purpose, manually annotated ones\nwould provide a good result, but it generally costs much time and patience.\nAlthough recent studies report that end-to-end TTS framework can generate\nnatural-sounding prosody without using prosodic labels, this does not\nalways appear in arbitrary languages such as pitch accent ones. Alternatively,\nwe propose an approach to utilizing a latent variable representation\nof prosodic information. In the latent variable representation, we\nemploy deep Gaussian process (DGP), a deep Bayesian generative model.\nIn the proposed semi-supervised learning framework, the posterior distributions\nof latent variables are inferred from linguistic and acoustic features,\nand the inferred latent variables are utilized to train a DGP-based\nregression model of acoustic features. Experimental results show that\nthe proposed framework can give a comparable performance with the case\nusing fully-annotated speech data in subjective evaluation even if\nthe prosodic information of pitch accent is limited.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2497",
      "author_area_id": 7,
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "nikulasdottir19_interspeech": {
      "authors": [
        [
          "Anna Bj\u00f6rk",
          "Nikul\u00e1sd\u00f3ttir"
        ],
        [
          "J\u00f3n",
          "Gu\u00f0nason"
        ]
      ],
      "title": "Bootstrapping a Text Normalization System for an Inflected Language. Numbers as a Test Case",
      "original": "2367",
      "page_count": 5,
      "order": 932,
      "p1": "4455",
      "pn": "4459",
      "abstract": [
        "Text normalization is an important part of many natural language applications,\nin particular for text-to-speech systems. Text normalization poses\nspecial challenges for highly inflected languages since the correct\nmorphological form for the normalization is not evident from the non-standard\nword, e.g. a digit.<br style=\"mso-data-placement:same-cell;\" /><br\nstyle=\"mso-data-placement:same-cell;\" />\n  In this paper we report\non ongoing work on a text normalization system for Icelandic, a highly\ninflected North Germanic language. We describe experiments on the normalization\nof numbers and address the problem of choosing the correct morphological\nform of number names. We use language models trained on texts containing\nnumber names and inspect effects of different LMs on domain specific\ntexts with a high ratio of digits. A partially class based LM, replacing\nnumber names with their part-of-speech tags, shows the best results\nin all domains. We further show that testing normalization on texts\nwhere number names have been converted to digits does not show representative\nresults for texts originally containing digits: while a test set similar\nto the language model training data shows an error rate of 10.1% on\ninflected cardinals from 1&#8211;99, test sets originally containing\ndigits show 45.3% and 55% error rates.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2367",
      "author_area_id": 7,
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "guo19e_interspeech": {
      "authors": [
        [
          "Haohan",
          "Guo"
        ],
        [
          "Frank K.",
          "Soong"
        ],
        [
          "Lei",
          "He"
        ],
        [
          "Lei",
          "Xie"
        ]
      ],
      "title": "Exploiting Syntactic Features in a Parsed Tree to Improve End-to-End TTS",
      "original": "2167",
      "page_count": 5,
      "order": 933,
      "p1": "4460",
      "pn": "4464",
      "abstract": [
        "The end-to-end TTS, which can predict speech directly from a given\nsequence of graphemes or phonemes, has shown improved performance over\nthe conventional TTS. However, its predicting capability is still limited\nby the acoustic/phonetic coverage of the training data, usually constrained\nby the training set size. To further improve the TTS quality in pronunciation,\nprosody and perceived naturalness, we propose to exploit the information\nembedded in a syntactically parse tree where the inter-phrase/word\ninformation of a sentence is organized in a multilevel tree structure.\nSpecifically, two key features: phrase structure and relations between\nadjacent words are investigated. Experimental results in subjective\nlistening, measured on three test sets, show that the proposed approach\nis effective to improve the pronunciation clarity, prosody and naturalness\nof the synthesized speech of the baseline system.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2167",
      "author_area_id": 7,
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "ni19_interspeech": {
      "authors": [
        [
          "Jinfu",
          "Ni"
        ],
        [
          "Yoshinori",
          "Shiga"
        ],
        [
          "Hisashi",
          "Kawai"
        ]
      ],
      "title": "Duration Modeling with Global Phoneme-Duration Vectors",
      "original": "2126",
      "page_count": 5,
      "order": 934,
      "p1": "4465",
      "pn": "4469",
      "abstract": [
        "A duration model is a major component in every parametric speech synthesis\nsystem. Conventional methods use full contextual labels as features\nto predict phoneme durations that require morphological analysis of\ntext. By contrast, advances in bidirectional recurrent neural networks\n(BRNN) and global space vector models make it possible to perform grapheme-to-phoneme\n(G2P) conversion from plain text. In this paper, we investigate duration\nprediction from plain phonemes instead of using their full contextual\nlabels. We propose a new approach that relies on both BRNN and global\nspace vector representations of phonemes (GPV) and durations (GDV).\nGPVs represent the statistics of phonemes used in a language, whereas\nGDVs capture duration variations beyond linguistic features. They are\nessentially learned from a large-scale text corpus in an unsupervised\nmanner where phonemes are converted by G2P.<br style=\"mso-data-placement:same-cell;\"\n/><br style=\"mso-data-placement:same-cell;\" />\n  We conducted experiments\non two speech corpora in Korean and Chinese to train BRNN-based models\nin a supervised manner. An objective evaluation conducted on a set\nof test sentences demonstrated that the proposed method leads to more\naccurate modeling of phoneme durations than the baselines.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2126",
      "author_area_id": 7,
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "aubin19_interspeech": {
      "authors": [
        [
          "Ad\u00e8le",
          "Aubin"
        ],
        [
          "Alessandra",
          "Cervone"
        ],
        [
          "Oliver",
          "Watts"
        ],
        [
          "Simon",
          "King"
        ]
      ],
      "title": "Improving Speech Synthesis with Discourse Relations",
      "original": "1945",
      "page_count": 5,
      "order": 935,
      "p1": "4470",
      "pn": "4474",
      "abstract": [
        "This paper explores whether adding Discourse Relation (DR) features\nimproves the naturalness of neural statistical parametric speech synthesis\n(SPSS) in English. We hypothesize first &#8212; in the light of several\nprevious studies &#8212; that DRs have a dedicated prosodic encoding.\nSecondly, we hypothesize that encoding DRs in a speech synthesizer&#8217;s\ninput will improve the naturalness of its output. In order to test\nour hypotheses, we prepare a dataset of DR-annotated transcriptions\nof audiobooks in English. We then perform an acoustic analysis of the\ncorpus which supports our first hypothesis that DRs are acoustically\nencoded in speech prosody. The analysis reveals significant correlation\nbetween specific DR categories and acoustic features, such as F0 and\nintensity. Then, we use the corpus to train a neural SPSS system in\ntwo configurations: a baseline configuration making use only of conventional\nlinguistic features, and an experimental one where these are supplemented\nwith DRs. Augmenting the inputs with DR features improves objective\nacoustic scores on a test set and leads to significant preference by\nlisteners in a forced choice AB test for naturalness.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1945",
      "author_area_id": 7,
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "tits19_interspeech": {
      "authors": [
        [
          "No\u00e9",
          "Tits"
        ],
        [
          "Fengna",
          "Wang"
        ],
        [
          "Kevin El",
          "Haddad"
        ],
        [
          "Vincent",
          "Pagel"
        ],
        [
          "Thierry",
          "Dutoit"
        ]
      ],
      "title": "Visualization and Interpretation of Latent Spaces for Controlling Expressive Speech Synthesis Through Audio Analysis",
      "original": "1426",
      "page_count": 5,
      "order": 936,
      "p1": "4475",
      "pn": "4479",
      "abstract": [
        "The field of Text-to-Speech has experienced huge improvements last\nyears benefiting from deep learning techniques. Producing realistic\nspeech becomes possible now. As a consequence, the research on the\ncontrol of the expressiveness, allowing to generate speech in different\nstyles or manners, has attracted increasing attention lately. Systems\nable to control style have been developed and show impressive results.\nHowever the control parameters often consist of latent variables and\nremain complex to interpret.<br style=\"mso-data-placement:same-cell;\"\n/><br style=\"mso-data-placement:same-cell;\" />\n  In this paper, we\nanalyze and compare different latent spaces and obtain an interpretation\nof their influence on expressive speech. This will enable the possibility\nto build controllable speech synthesis systems with an understandable\nbehaviour.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1426",
      "author_area_id": 7,
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "yang19h_interspeech": {
      "authors": [
        [
          "Bing",
          "Yang"
        ],
        [
          "Jiaqi",
          "Zhong"
        ],
        [
          "Shan",
          "Liu"
        ]
      ],
      "title": "Pre-Trained Text Representations for Improving Front-End Text Processing in Mandarin Text-to-Speech Synthesis",
      "original": "1418",
      "page_count": 5,
      "order": 937,
      "p1": "4480",
      "pn": "4484",
      "abstract": [
        "In this paper, we propose a novel method to improve the performance\nand robustness of the front-end text processing modules of Mandarin\ntext-to-speech (TTS) synthesis. We use pre-trained text encoding models,\nsuch as the encoder of a transformer based NMT model and BERT, to extract\nthe latent semantic representations of words or characters and use\nthem as input features for tasks in the front-end of TTS systems. Our\nexperiments on the tasks of Mandarin polyphone disambiguation and prosodic\nstructure prediction show that the proposed method can significantly\nimprove the performances. Specifically, we get an absolute improvement\nof 0.013 and 0.027 in F1 score for prosodic word prediction and prosodic\nphrase prediction respectively, and an absolute improvement of 2.44%\nin polyphone disambiguation compared to previous methods.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1418",
      "author_area_id": 7,
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "pan19b_interspeech": {
      "authors": [
        [
          "Huashan",
          "Pan"
        ],
        [
          "Xiulin",
          "Li"
        ],
        [
          "Zhiqiang",
          "Huang"
        ]
      ],
      "title": "A Mandarin Prosodic Boundary Prediction Model Based on Multi-Task Learning",
      "original": "1400",
      "page_count": 4,
      "order": 938,
      "p1": "4485",
      "pn": "4488",
      "abstract": [
        "In this paper, we propose a mandarin prosodic boundary prediction model\nbased on Multi-Task Learning (MTL) architecture. The prosody structure\nof mandarin is a three-level hierarchical structure, which contains\nthree basic units &#8212; Prosodic Word (PW), Prosodic Phrase (PPH)\nand Intonational Phrase (IPH) [1]. Previous studies usually decompose\nmandarin prosodic boundary prediction task into three independent tasks\non these three unit boundaries [1&#8211;4]. In recent years, with the\ndevelopment of deep learning, MTL has achieved state-of-the-art performance\non many tasks in Natural Language Processing (NLP) field [5&#8211;7].\nInspired by this, this paper implements an MTL framework with Bidirectional\nLong-Short Term Memory and Conditional Random Field (BLSTM-CRF) as\nthe basic model, and takes three independent tasks of mandarin prosodic\nboundary prediction as sub-modules for PW, PPH and IPH individually.\nUnder the MTL architecture, the three independent tasks are unified\nfor overall optimization. The experiment results show that our model\nis effective in solving the task of mandarin prosodic boundary prediction,\nin which the overall prediction performance is improved by 0.8%, and\nthe model size is reduced by about 55%.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1400",
      "author_area_id": 7,
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "gokcen19_interspeech": {
      "authors": [
        [
          "Ajda",
          "Gokcen"
        ],
        [
          "Hao",
          "Zhang"
        ],
        [
          "Richard",
          "Sproat"
        ]
      ],
      "title": "Dual Encoder Classifier Models as Constraints in Neural Text Normalization",
      "original": "1135",
      "page_count": 5,
      "order": 939,
      "p1": "4489",
      "pn": "4493",
      "abstract": [
        "Neural text normalization systems can achieve low error rates; however,\nthe errors they make include not only ones from which the hearer can\nrecover (such as reading  3 as  three dollar) but also  unrecoverable\nerrors, such as reading  3 as  three euros. FST decoding constraints\nhave proven effective at reducing unrecoverable errors. In this paper\nwe explore an alternative approach to error mitigation: using  dual\nencoder classifiers trained with both positive and negative examples\nto implement  soft constraints on acceptability. Since the error rates\nare very low, it is difficult to determine when improvement is significant,\nbut qualitative analysis suggests that soft dual encoder constraints\ncan help reduce the number of unrecoverable errors.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1135",
      "author_area_id": 7,
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "li19v_interspeech": {
      "authors": [
        [
          "Jingbei",
          "Li"
        ],
        [
          "Zhiyong",
          "Wu"
        ],
        [
          "Runnan",
          "Li"
        ],
        [
          "Pengpeng",
          "Zhi"
        ],
        [
          "Song",
          "Yang"
        ],
        [
          "Helen",
          "Meng"
        ]
      ],
      "title": "Knowledge-Based Linguistic Encoding for End-to-End Mandarin Text-to-Speech Synthesis",
      "original": "1118",
      "page_count": 5,
      "order": 940,
      "p1": "4494",
      "pn": "4498",
      "abstract": [
        "Recent researches have shown superior performance of applying end-to-end\narchitecture in text-to-speech (TTS) synthesis. However, considering\nthe complex linguistic structure of Chinese, using Chinese characters\ndirectly for Mandarin TTS may suffer from the poor linguistic encoding\nperformance, resulting in improper word tokenization and pronunciation\nerrors. To ensure the naturalness and intelligibility of synthetic\nspeech, state-of-the-art Mandarin TTS systems employ a list of components,\nsuch as word tokenization, part-of-speech (POS) tagging and grapheme-to-phoneme\n(G2P) conversion, to produce knowledge-enhanced inputs to alleviate\nthe problems caused by linguistic encoding. These components are based\non linguistic expertise and well-designed, but trained individually,\nleading to errors compounding for the TTS system. In this paper, to\nreduce the complexity of Mandarin TTS system and bring further improvement,\nwe proposed a knowledge-based linguistic encoder for the character-based\nend-to-end Mandarin TTS system. Developed with multi-task learning\nstructure, the proposed encoder can learn from linguistic analysis\nsubtasks, providing robust and discriminative linguistic encodings\nfor the following speech generation decoder. Experimental results demonstrate\nthe effectiveness of the proposed framework, with word tokenization\nerror dropped from 12.81% to 1.58%, syllable pronunciation error dropped\nfrom 10.89% to 2.81% compared with state-of-the-art baseline approach,\nproviding mean opinion score (MOS) improvement from 3.76 to 3.87.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1118",
      "author_area_id": 7,
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "shankar19c_interspeech": {
      "authors": [
        [
          "Ravi",
          "Shankar"
        ],
        [
          "Hsi-Wei",
          "Hsieh"
        ],
        [
          "Nicolas",
          "Charon"
        ],
        [
          "Archana",
          "Venkataraman"
        ]
      ],
      "title": "Automated Emotion Morphing in Speech Based on Diffeomorphic Curve Registration and Highway Networks",
      "original": "2386",
      "page_count": 5,
      "order": 941,
      "p1": "4499",
      "pn": "4503",
      "abstract": [
        "We present a novel approach for emotion conversion that bridges the\ndomains of speech analysis and computer vision. Our strategy is to\nwarp the pitch contour of a source emotional utterance using diffeomorphic\ncurve registration. The associated dynamical process pushes the original\nsource contour towards that of a target emotional utterance. Mathematically,\nthis warping process is completely specified by a set of  initial momenta.\nTherefore, we use parallel data to train a highway neural network (HNet)\nto predict these initial momenta directly from the signal characteristics.\nThe input features to the HNet include contextual pitch and spectral\ninformation. Once trained, the HNet is used to obtain the initial momenta\nfor new utterances. From here, the diffeomorphic process takes over\nand warps the pitch contour accordingly. We validate our framework\non the VESUS repository collected at Johns Hopkins University, which\ncontains parallel emotional utterances from 10 actors. The proposed\nwarping is more accurate that three state-of-the-art baselines for\nemotion conversion. We also evaluate the quality of our emotion manipulations\nvia crowd sourcing.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2386",
      "author_area_id": 7,
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "connaghan19_interspeech": {
      "authors": [
        [
          "Kathryn P.",
          "Connaghan"
        ],
        [
          "Jordan R.",
          "Green"
        ],
        [
          "Sabrina",
          "Paganoni"
        ],
        [
          "James",
          "Chan"
        ],
        [
          "Harli",
          "Weber"
        ],
        [
          "Ella",
          "Collins"
        ],
        [
          "Brian",
          "Richburg"
        ],
        [
          "Marziye",
          "Eshghi"
        ],
        [
          "J.P.",
          "Onnela"
        ],
        [
          "James D.",
          "Berry"
        ]
      ],
      "title": "Use of Beiwe Smartphone App to Identify and Track Speech Decline in Amyotrophic Lateral Sclerosis (ALS)",
      "original": "3126",
      "page_count": 5,
      "order": 942,
      "p1": "4504",
      "pn": "4508",
      "abstract": [
        "The capacity for smartphones to remotely capture speech data affords\nsignificant clinical and research opportunities for degenerative neurologic\ndiseases such as amyotrophic lateral sclerosis (ALS). Longitudinal\ndata may inform ALS disease prognosis, facilitate timely intervention,\nand document response to treatment [1]. A recent study established\nthe feasibility of the Beiwe smartphone-based digital phenotyping to\ntrack the clinical progression of ALS across multiple domains [2].\nThe current investigation extends this work to address the utility\nof Beiwe to identify and track speech decline in ALS. Twelve participants\nwith ALS used the Beiwe app weekly to record reading passages and self-report\n(ALSFRS-R) ratings of bulbar (speech) function. Speaking rate and pause\nvariables were automatically extracted from recordings offline [3].\nSpeech function measures at baseline were significantly different for\nparticipants with and without bulbar symptoms. In addition, the rate\nof decline of all measured speech functions was greater for participants\nwith bulbar symptoms. The successful use of Beiwe for speech function\nanalysis suggests that smartphone-based capture of speech has potential\nfor diagnostic screening and disease progress monitoring in ALS. Further\nlarge sample investigation across a comprehensive set of speech variables\nis warranted.\n"
      ],
      "doi": "10.21437/Interspeech.2019-3126"
    },
    "rowe19_interspeech": {
      "authors": [
        [
          "Hannah P.",
          "Rowe"
        ],
        [
          "Jordan R.",
          "Green"
        ]
      ],
      "title": "Profiling Speech Motor Impairments in Persons with Amyotrophic Lateral Sclerosis: An Acoustic-Based Approach",
      "original": "2911",
      "page_count": 5,
      "order": 943,
      "p1": "4509",
      "pn": "4513",
      "abstract": [
        "The goal of this study was to profile the speech motor impairments\nthat characterize dysarthria secondary to amyotrophic lateral sclerosis\n(ALS). This information is important for identifying optimal treatment\nstrategies and guiding speech impairment subtype discovery, which may\nfacilitate ongoing efforts to improve automatic speech recognition\n(ASR) of dysarthric speech. Speech motor impairments were profiled\nby introducing a novel framework that assesses four key components\nof motor control:  coordination,  consistency,  speed, and  precision.\nAn individual acoustic feature was selected to represent each component.\nSpecifically,  coordination was indexed by the proportion of voice\nonset time (VOT) to syllable duration,  consistency by the coefficient\nof variation of VOT between repetitions of /pataka/ within each distinct\nconsonant,  speed by the slope of the second formant (F2), and  precision\nby the standard deviation of F2 slope between distinct consonants within\neach repetition of /pataka/. Acoustic measures were extracted from\naudio recordings of each participant (18 controls and 14 participants\nwith ALS) during a sequential motion rate (SMR) task. Results revealed\nthat the primary underlying speech motor impairments that characterize\nALS are in  coordination,  speed, and  precision. Further research\nis needed to validate the existence of speech-impairment-based subtypes\nacross the continuum of speech motor disorders.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2911",
      "author_area_id": 1,
      "author_area_label": "Speech Perception, Production and Acquisition"
    },
    "mayle19_interspeech": {
      "authors": [
        [
          "Alex",
          "Mayle"
        ],
        [
          "Zhiwei",
          "Mou"
        ],
        [
          "Razvan",
          "Bunescu"
        ],
        [
          "Sadegh",
          "Mirshekarian"
        ],
        [
          "Li",
          "Xu"
        ],
        [
          "Chang",
          "Liu"
        ]
      ],
      "title": "Diagnosing Dysarthria with Long Short-Term Memory Networks",
      "original": "2903",
      "page_count": 5,
      "order": 944,
      "p1": "4514",
      "pn": "4518",
      "abstract": [
        "This paper proposes the use of Recurrent Neural Networks (RNNs) with\nLong Short-Term Memory (LSTM) units for determining whether Mandarin-speaking\nindividuals are afflicted with a form of Dysarthria based on samples\nof syllable pronunciations. Several LSTM network architectures are\nevaluated on this binary classification task, using accuracy and Receiver\nOperating Characteristic (ROC) curves as metrics. The LSTM models are\nshown to significantly improve upon a baseline fully connected network,\nreaching over 90% area under the ROC curve on the task of classifying\nnew speakers, when a sufficient number of cepstrum coefficients are\nused. The results show that the LSTM&#8217;s ability to leverage temporal\ninformation within its input makes for an effective step in the pursuit\nof accessible Dysarthria diagnoses.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2903",
      "author_area_id": 1,
      "author_area_label": "Speech Perception, Production and Acquisition"
    },
    "sudro19_interspeech": {
      "authors": [
        [
          "Protima Nomo",
          "Sudro"
        ],
        [
          "S.R. Mahadeva",
          "Prasanna"
        ]
      ],
      "title": "Modification of Devoicing Error in Cleft Lip and Palate Speech",
      "original": "2604",
      "page_count": 5,
      "order": 945,
      "p1": "4519",
      "pn": "4523",
      "abstract": [
        "The cleft of the lip and palate (CLP) caused by structural and functional\ndeformation leads to various speech-related disorders, which substantially\ndegrades the speech intelligibility. In this work, devoiced stop consonants\nin CLP speech are analyzed, and an approach is proposed for its modification\nin order to enhance the speech intelligibility. The devoicing errors\nare primarily characterized by the absence of voicebar in the closure\ninterval and relatively longer voice onset time (VOT). The proposed\napproach first segments the regions corresponding to the closure interval\nand VOT based on the knowledge of glottal activity, voice onset point,\nvoice offset point, and burst onset point. In the next stage, specific\ntransformations are performed for the modification of closure bar and\nVOT respectively. For transformation, first different transformation\nmatrices are learned for closure bar and VOT from normal and CLP speakers.\nThe transformation matrix is optimized using nonnegative matrix factorization\nmethod. Further, the corresponding transformation matrices are used\nto modify the closure bar and VOT separately. The subjective evaluation\nresults indicate that the devoiced stop consonants tend to exhibit\nthe characteristics of voiced stop consonants.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2604",
      "author_area_id": 1,
      "author_area_label": "Speech Perception, Production and Acquisition"
    },
    "eshghi19_interspeech": {
      "authors": [
        [
          "Marziye",
          "Eshghi"
        ],
        [
          "Panying",
          "Rong"
        ],
        [
          "Antje S.",
          "Mefferd"
        ],
        [
          "Kaila L.",
          "Stipancic"
        ],
        [
          "Yana",
          "Yunusova"
        ],
        [
          "Jordan R.",
          "Green"
        ]
      ],
      "title": "Reduced Task Adaptation in Alternating Motion Rate Tasks as an Early Marker of Bulbar Involvement in Amyotrophic Lateral Sclerosis",
      "original": "2546",
      "page_count": 5,
      "order": 946,
      "p1": "4524",
      "pn": "4528",
      "abstract": [
        "The identification of robust biomarkers to detect the onset of amyotrophic\nlateral sclerosis (ALS) has been an ongoing challenge. Recent evidence\nfrom multiple studies suggests that speech changes are a reliable early\nindicator of ALS particularly during physically demanding speaking\ntasks such as alternating motion rate (AMR). However, it has also been\nfound that individuals make various behavioral adaptations to meet\nthe maximum rate requirement in AMR. In this study, we explored the\nextent to which persons with early-stage ALS are capable of adapting\nto challenging speech-like tasks. Speech motor performance of 14 healthy\ncontrols was compared to that of 18 patients at the early stage of\nALS during standard (unconstrained) and fixed-target (constrained)\nAMR tasks. Fixed-target tasks were designed to impose high demands\non the speech motor system. Although habitual speaking rate was maintained\nwithin normal limits, findings revealed that task adaptation was reduced\nat the early stage of ALS. Furthermore, the difference between the\nnumber of cycles in the fixed-target task and standard task showed\nhigher sensitivity than habitual speaking rate to detect early decline\nin bulbar function. The inability to adapt to the fixed-target task\nwas a good early indicator of bulbar motor involvement due to ALS.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2546",
      "author_area_id": 1,
      "author_area_label": "Speech Perception, Production and Acquisition"
    },
    "wang19q_interspeech": {
      "authors": [
        [
          "Tianqi",
          "Wang"
        ],
        [
          "Quanlei",
          "Yan"
        ],
        [
          "Jingshen",
          "Pan"
        ],
        [
          "Feiqi",
          "Zhu"
        ],
        [
          "Rongfeng",
          "Su"
        ],
        [
          "Yi",
          "Guo"
        ],
        [
          "Lan",
          "Wang"
        ],
        [
          "Nan",
          "Yan"
        ]
      ],
      "title": "Towards the Speech Features of Early-Stage Dementia: Design and Application of the Mandarin Elderly Cognitive Speech Database",
      "original": "2453",
      "page_count": 5,
      "order": 947,
      "p1": "4529",
      "pn": "4533",
      "abstract": [
        "Speech and language features have been proven to be useful for the\ndetection of neurodegenerative diseases, such as Alzheimer&#8217;s\ndisease (AD), and its prodromal stage, mild cognitive impairment (MCI).\nUnfortunately, high-quality speech database remains scarce, which limit\nits application in automatic screening and assessment of early dementia\nin clinical practice. To bridge this gap, the present study aimed to\ndesign a speech database of Chinese elderly with intact cognition and\nMCI, named &#8220;Mandarin Elderly Cognitive Speech Database&#8221;\n(MECSD). The database consists of 110 hours of speech recordings from\n85 native speakers of Mandarin Chinese (age range = 55&#8211;85 years),\nincluding 20 participants with MCI and 65 healthy controls. Manually\ntranscribed materials with temporal information were also included\nin this database. Nine tasks, involving conventional test batteries\nand connected speech productions, were used to obtain speech samples,\nproducing a total of 8563 sentences and 49841 words. Details concerning\nthe design of the database, together with our preliminary findings\napplying automatic speech recognition (ASR), were reported in this\nstudy. The MECSD will provide researchers with access to a large shared\ndatabase that can facilitate hypothesis testing in the study of early-stage\ndementia.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2453",
      "author_area_id": 1,
      "author_area_label": "Speech Perception, Production and Acquisition"
    },
    "chen19p_interspeech": {
      "authors": [
        [
          "Wenjun",
          "Chen"
        ],
        [
          "Jeroen van de",
          "Weijer"
        ],
        [
          "Shuangshuang",
          "Zhu"
        ],
        [
          "Qian",
          "Qian"
        ],
        [
          "Manna",
          "Wang"
        ]
      ],
      "title": "Acoustic Characteristics of Lexical Tone Disruption in Mandarin Speakers After Brain Damage",
      "original": "2432",
      "page_count": 5,
      "order": 948,
      "p1": "4534",
      "pn": "4538",
      "abstract": [
        "This study identifies the acoustic characteristics of tones produced\nby Mandarin brain-damaged patients. We investigate the F0 characteristics\nof the patients&#8217; tone productions and compare them with a control\ngroup of healthy speakers. The results show tone disruption in patients\nwith brain damage in either the left or the right hemisphere. Even\npatients&#8217; tone productions that were correctly identified by\nMandarin native speakers were acoustically different from the ones\nproduced by healthy speakers. The patterns of tone disruption in Mandarin\nbrain-damaged patients might be caused by damage to the motor function\nin the brain.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2432",
      "author_area_id": 1,
      "author_area_label": "Speech Perception, Production and Acquisition"
    },
    "hermes19_interspeech": {
      "authors": [
        [
          "Anne",
          "Hermes"
        ],
        [
          "Doris",
          "M\u00fccke"
        ],
        [
          "Tabea",
          "Thies"
        ],
        [
          "Michael T.",
          "Barbe"
        ]
      ],
      "title": "Intragestural Variation in Natural Sentence Production: Essential Tremor Patients Treated with DBS",
      "original": "2389",
      "page_count": 5,
      "order": 949,
      "p1": "4539",
      "pn": "4543",
      "abstract": [
        "In the present study, we investigate intragestural parameters during\nthe production of CV syllables in natural sentence production of Essential\nTremor (ET) patients treated with Deep Brain Stimulation (DBS). Within\nthe task dynamic approach, we analyzed temporal and spatial parameters\nof consonantal and vocalic movements of the respective target syllables.\nOur analysis revealed that intragestural coordination patterns are\naffected in the patients&#8217; group: While patients with inactivated\nstimulation (DBS-OFF) already showed signs of dysarthria in terms of\nlonger and less stiff movements, there was an additional slowing down\nof the speech motor system under activated stimulation (DBS-ON). When\ncomparing CV production in natural sentence to fast syllable repetition\ntasks (DDK), we find similarities in that there is a slowing down of\nthe system, but also differences in that coordination problems increase\nin DDK leading to an overmodulation of peak velocities and displacements.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2389",
      "author_area_id": 1,
      "author_area_label": "Speech Perception, Production and Acquisition"
    },
    "kalita19_interspeech": {
      "authors": [
        [
          "Sishir",
          "Kalita"
        ],
        [
          "Protima Nomo",
          "Sudro"
        ],
        [
          "S.R. Mahadeva",
          "Prasanna"
        ],
        [
          "S.",
          "Dandapat"
        ]
      ],
      "title": "Nasal Air Emission in Sibilant Fricatives of Cleft Lip and Palate Speech",
      "original": "2345",
      "page_count": 5,
      "order": 950,
      "p1": "4544",
      "pn": "4548",
      "abstract": [
        "Cleft lip and palate (CLP) is a congenital disorder of the orofacial\nregion. Nasal air emission (NAE) in CLP speech occurs due to the presence\nof velopharyngeal dysfunction (VPD), and it mostly occurs in the production\nof fricative sounds. The objective of present work is to study the\nacoustic characteristics of voiceless sibilant fricatives in Kannada\ndistorted by NAE and develop an SVM-based classification to distinguish\nnormal fricatives from the NAE distorted fricatives. Static spectral\nmeasures, such as spectral moments are used to analyze the deviant\nspectral distribution of NAE distorted fricatives. As the aerodynamic\nparameters are deviated due to VPD, the temporal variation of spectral\ncharacteristics might also get deviated in NAE distorted fricatives.\nThis variation is studied using the peak equivalent rectangular bandwidth\n(ERB<SUB>N</SUB>)-number, a psychoacoustic measure to analyze the temporal\nvariation in the spectral properties of fricatives. The analysis of\nNAE distorted fricatives shows that the maximum spectral density is\nconcentrated in the lower frequency range with steep positive skewness\nand more variations in the trajectories of peak ERB<SUB>N</SUB>-number\nas compared to the normal fricatives. The proposed SVM-based classification\nachieves good detection rates in discriminating NAE distorted fricatives\nfrom normal fricatives.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2345",
      "author_area_id": 1,
      "author_area_label": "Speech Perception, Production and Acquisition"
    },
    "serrano19_interspeech": {
      "authors": [
        [
          "Luis",
          "Serrano"
        ],
        [
          "Sneha",
          "Raman"
        ],
        [
          "David",
          "Tavarez"
        ],
        [
          "Eva",
          "Navas"
        ],
        [
          "Inma",
          "Hernaez"
        ]
      ],
      "title": "Parallel vs. Non-Parallel Voice Conversion for Esophageal Speech",
      "original": "2194",
      "page_count": 5,
      "order": 951,
      "p1": "4549",
      "pn": "4553",
      "abstract": [
        "State of the art systems for voice conversion have been shown to generate\nhighly natural sounding converted speech. Voice conversion techniques\nhave also been applied to alaryngeal speech, with the aim of improving\nits quality or its intelligibility. In this paper, we present an attempt\nto apply a voice conversion strategy based on phonetic posteriorgrams\n(PPGs), which produces very high quality converted speech, to improve\nthe characteristics of esophageal speech. The main advantage of this\nPPG based architecture lies in the fact that it is able to convert\nspeech from any source, without the need to previously train the system\nwith a parallel corpus. However, our results show that the PPG approach\ndegrades the intelligibility of the converted speech considerably,\nespecially when the input speech is already poorly intelligible. In\nthis paper two systems are compared, an LSTM based one-to-one conversion\nsystem, which is referred to as the baseline, and the new system using\nphonetic posteriorgrams. Both spectral parameters and f<SUB>0</SUB>\nare converted using DNN (Deep Neural Network) based architectures.\nResults from both objective and subjective evaluations are presented,\nshowing that although ASR (Automated Speech Recognition) errors are\nreduced, original esophageal speech is still preferred by subjects.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2194",
      "author_area_id": 1,
      "author_area_label": "Speech Perception, Production and Acquisition"
    },
    "dubey19b_interspeech": {
      "authors": [
        [
          "Akhilesh Kumar",
          "Dubey"
        ],
        [
          "S.R. Mahadeva",
          "Prasanna"
        ],
        [
          "S.",
          "Dandapat"
        ]
      ],
      "title": "Hypernasality Severity Detection Using Constant Q Cepstral Coefficients",
      "original": "2151",
      "page_count": 5,
      "order": 952,
      "p1": "4554",
      "pn": "4558",
      "abstract": [
        "In this work, detection of hypernasality severity in cleft palate speech\nis attempted using constant Q cepstral coefficients (CQCC) feature.\nThe coupling of nasal tract with the oral tract during the production\nof hypernasal speech adds nasal formants and anti-formants in low frequency\nregion of vowel spectrum mainly around the first formant. The strength\nand position of nasal formants and anti-formants along with the oral\nformants changes as the severity of nasality changes in hypernasal\nspeech. The CQCC feature is extracted from the constant Q transform\n(CQT) spectrum which employs geometrically spaced frequency bins and\nmaintains a constant Q factor for across the entire spectrum. This\nresults in a higher frequency resolution at lower frequencies and higher\ntemporal resolution at higher frequencies. The CQT spectrum resolves\nthe nasal and oral formants in low frequency and captures the spectral\nchanges due to change in nasality severity. The CQCC feature gives\nthe overall classification accuracy of 83.33% and 78.47% for /i/ and\n/u/ vowels corresponding to normal, mild and moderate-severe hypernasal\nspeech, respectively using multiclass support vector classifier.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2151",
      "author_area_id": 1,
      "author_area_label": "Speech Perception, Production and Acquisition"
    },
    "niu19_interspeech": {
      "authors": [
        [
          "Mingyue",
          "Niu"
        ],
        [
          "Jianhua",
          "Tao"
        ],
        [
          "Bin",
          "Liu"
        ],
        [
          "Cunhang",
          "Fan"
        ]
      ],
      "title": "Automatic Depression Level Detection via &#8467;<SUB>p</SUB>-Norm Pooling",
      "original": "1617",
      "page_count": 5,
      "order": 953,
      "p1": "4559",
      "pn": "4563",
      "abstract": [
        "Related physiological studies have shown that Mel-frequency cepstral\ncoefficient (MFCC) is a discriminative acoustic feature for depression\ndetection. This fact has led to some works using MFCCs to identify\nindividual depression degree. However, they rarely adopt neural network\nto capture high-level feature associated with depression detection.\nAnd the suitable feature pooling parameter for depression detection\nhas not been optimized. For these reasons, we propose a hybrid network\nand &#8467;_p-norm pooling combined with least absolute shrinkage and\nselection operator (LASSO) to improve the accuracy of depression detection.\nFirstly, the MFCCs of the original speech are divided into many segments.\nThen, we extract the segment-level feature using the proposed hybrid\nnetwork, which investigates the depression-related information in the\nspatial structure, temporal changes and discriminative representation\nof short-term MFCC segments. Thirdly, &#8467;_p-norm pooling combined\nwith LASSO is adopted to find the optimal pooling parameter for depression\ndetection to generate the utterance-level feature. Finally, depression\nlevel prediction is accomplished using support vector regression (SVR).\nExperiments are conducted on AVEC2013 and AVEC2014. The results demonstrate\nthat our proposed method achieves better performance than the previous\nalgorithms.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1617"
    },
    "bn19_interspeech": {
      "authors": [
        [
          "Suhas",
          "B.N."
        ],
        [
          "Deep",
          "Patel"
        ],
        [
          "Nithin",
          "Rao"
        ],
        [
          "Yamini",
          "Belur"
        ],
        [
          "Pradeep",
          "Reddy"
        ],
        [
          "Nalini",
          "Atchayaram"
        ],
        [
          "Ravi",
          "Yadav"
        ],
        [
          "Dipanjan",
          "Gope"
        ],
        [
          "Prasanta Kumar",
          "Ghosh"
        ]
      ],
      "title": "Comparison of Speech Tasks and Recording Devices for Voice Based Automatic Classification of Healthy Subjects and Patients with Amyotrophic Lateral Sclerosis",
      "original": "1285",
      "page_count": 5,
      "order": 954,
      "p1": "4564",
      "pn": "4568",
      "abstract": [
        "We consider the task of speech based automatic classification of patients\nwith amyotrophic lateral sclerosis (ALS) and healthy subjects. The\nrole of different speech tasks and recording devices on classification\naccuracy is examined. Sustained phoneme production (PHON), diadochokinetic\ntask (DDK) and spontaneous speech (SPON) have been used as speech tasks.\nThe chosen five recording devices include a high quality microphone\nand built-in smartphone microphones at various price ranges. Experiments\nare performed using speech data from 25 ALS patients and 25 healthy\nsubjects using support vector machines and deep neural networks as\nclassifiers and suprasegmental features based on mel frequency cepstral\ncoefficients. Results reveal that DDK consistently performs better\nthan SPON and PHON across all devices for discriminating ALS patients\nand healthy subjects. Considering DDK, the best classification accuracy\nof 92.2% is obtained using a high quality microphone but the accuracy\ndrops if there is a mismatch between the microphones for training and\ntest. However, a classifier trained with recordings from all devices\ntogether performs more uniformly across all devices. The findings from\nthis study could aid in determining the choice of the task and device\nin developing an assistive tool for detection and monitoring of ALS.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1285",
      "author_area_id": 1,
      "author_area_label": "Speech Perception, Production and Acquisition"
    },
    "wang19r_interspeech": {
      "authors": [
        [
          "Dongxiao",
          "Wang"
        ],
        [
          "Hirokazu",
          "Kameoka"
        ],
        [
          "Koichi",
          "Shinoda"
        ]
      ],
      "title": "A Modified Algorithm for Multiple Input Spectrogram Inversion",
      "original": "3242",
      "page_count": 5,
      "order": 955,
      "p1": "4569",
      "pn": "4573",
      "abstract": [
        "We propose a new algorithm to estimate the phase of speech signal in\nthe mixture of audio sources under the assumption that the magnitude\nspectrum of each source is given. The previous method, multiple input\nspectrogram inversion algorithm (MISI), often performs poorly when\nthe magnitude spectrograms estimated are not accurate. This may be\nbecause it imposes a strict constraint that the summation of source\nwaveforms should be exactly the same as the mixture waveform. Our proposing\nalgorithm employs a new objective function in which this constraint\nis relaxed. In this objective function, the difference between the\nsummation of source waveforms and the mixture waveform is the target\nto be minimized. The performance of our method, modified MISI is evaluated\non two different experimental settings. In both settings it improves\nthe audio source separation performance compared to MISI.\n"
      ],
      "doi": "10.21437/Interspeech.2019-3242",
      "author_area_id": 5,
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "bahmaninezhad19_interspeech": {
      "authors": [
        [
          "Fahimeh",
          "Bahmaninezhad"
        ],
        [
          "Jian",
          "Wu"
        ],
        [
          "Rongzhi",
          "Gu"
        ],
        [
          "Shi-Xiong",
          "Zhang"
        ],
        [
          "Yong",
          "Xu"
        ],
        [
          "Meng",
          "Yu"
        ],
        [
          "Dong",
          "Yu"
        ]
      ],
      "title": "A Comprehensive Study of Speech Separation: Spectrogram vs Waveform Separation",
      "original": "3181",
      "page_count": 5,
      "order": 956,
      "p1": "4574",
      "pn": "4578",
      "abstract": [
        "Speech separation has been studied widely for single-channel close-talk\nmicrophone recordings over the past few years; developed solutions\nare mostly in frequency-domain. Recently, a raw audio waveform separation\nnetwork (TasNet) is introduced for single-channel data, with achieving\nhigh Si-SNR (scale-invariant source-to-noise ratio) and SDR (source-to-distortion\nratio) comparing against the state-of-the-art solution in frequency-domain.\nIn this study, we incorporate effective components of the TasNet into\na frequency-domain separation method. We compare both for alternative\nscenarios. We introduce a solution for directly optimizing the separation\ncriterion in frequency-domain networks. In addition to speech separation\nobjective and subjective measurements, we evaluate the separation performance\non a speech recognition task as well. We study the speech separation\nproblem for far-field data (more similar to naturalistic audio streams)\nand develop multi-channel solutions for both frequency and time-domain\nseparators with utilizing spectral, spatial and speaker location information.\nFor our experiments, we simulated multi-channel spatialized reverberate\nWSJ0-2mix dataset. Our experimental results show that spectrogram separation\ncan achieve competitive performance with better network design. Multi-channel\nframework as well is shown to improve the single-channel performance\nrelatively up to +35.5% and +46% in terms of WER and SDR, respectively.\n"
      ],
      "doi": "10.21437/Interspeech.2019-3181",
      "author_area_id": 5,
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "inan19_interspeech": {
      "authors": [
        [
          "Berkay",
          "\u0130nan"
        ],
        [
          "Milos",
          "Cernak"
        ],
        [
          "Helmut",
          "Grabner"
        ],
        [
          "Helena Peic",
          "Tukuljac"
        ],
        [
          "Rodrigo C.G.",
          "Pena"
        ],
        [
          "Benjamin",
          "Ricaud"
        ]
      ],
      "title": "Evaluating Audiovisual Source Separation in the Context of Video Conferencing",
      "original": "2671",
      "page_count": 5,
      "order": 957,
      "p1": "4579",
      "pn": "4583",
      "abstract": [
        "Source separation involving mono-channel audio is a challenging problem,\nin particular for speech separation where source contributions overlap\nboth in time and frequency. This task is of high interest for applications\nsuch as video conferencing. Recent progress in machine learning has\nshown that the combination of visual cues, coming from the video, can\nincrease the source separation performance. Starting from a recently\ndesigned deep neural network, we assess its ability and robustness\nto separate the visible speakers&#8217; speech from other interfering\nspeeches or signals. We test it for different configuration of video\nrecordings where the speaker&#8217;s face may not be fully visible.\nWe also asses the performance of the network with respect to different\nsets of visual features from the speakers&#8217; faces.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2671",
      "author_area_id": 5,
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "ditter19_interspeech": {
      "authors": [
        [
          "David",
          "Ditter"
        ],
        [
          "Timo",
          "Gerkmann"
        ]
      ],
      "title": "Influence of Speaker-Specific Parameters on Speech Separation Systems",
      "original": "2459",
      "page_count": 5,
      "order": 958,
      "p1": "4584",
      "pn": "4588",
      "abstract": [
        "Recent studies have shown that Deep Learning based single-channel speech\nseparation systems perform worse for same-gender mixtures than for\ndifferent-gender mixtures. In this work, we provide for a more detailed\nanalysis of the respective impact of the fundamental frequency and\nthe vocal tract length on the system performance. While both parameters\nare correlated with gender, the vocal tract length is a fixed speaker-specific\nparameter, whereas the fundamental frequency can vary for different\nspeaking styles. We show that the difference of the fundamental frequency\nmedians of two speakers in a mixture is highly correlated with the\nSDR performance while the difference of the vocal tract lengths is\nnot. Our analysis allows us to do performance predictions for given\nspeakers based on measurements of their fundamental frequency. Furthermore\nwe conclude that current systems separate (short-term) speaking styles\nrather than (long-term) speaker characteristics.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2459",
      "author_area_id": 5,
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "zegers19_interspeech": {
      "authors": [
        [
          "Jeroen",
          "Zegers"
        ],
        [
          "Hugo",
          "Van hamme"
        ]
      ],
      "title": "CNN-LSTM Models for Multi-Speaker Source Separation Using Bayesian Hyper Parameter Optimization",
      "original": "2423",
      "page_count": 5,
      "order": 959,
      "p1": "4589",
      "pn": "4593",
      "abstract": [
        "In recent years there have been many deep learning approaches towards\nthe multi-speaker source separation problem. Most use Long Short-Term\nMemory - Recurrent Neural Networks (LSTM-RNN) or Convolutional Neural\nNetworks (CNN) to model the sequential behavior of speech. In this\npaper we propose a novel network for source separation using an encoder-decoder\nCNN and LSTM in parallel. Hyper parameters have to be chosen for both\nparts of the network and they are potentially mutually dependent. Since\nhyper parameter grid search has a high computational burden, random\nsearch is often preferred. However, when sampling a new point in the\nhyper parameter space, it can potentially be very close to a previously\nevaluated point and thus give little additional information. Furthermore,\nrandom sampling is as likely to sample in a promising area as in an\nhyper space area dominated with poor performing models. Therefore,\nwe use a Bayesian hyper parameter optimization technique and find that\nthe parallel CNN-LSTM outperforms the LSTM-only and CNN-only model.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2423",
      "author_area_id": 5,
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "bear19_interspeech": {
      "authors": [
        [
          "Helen L.",
          "Bear"
        ],
        [
          "In\u00eas",
          "Nolasco"
        ],
        [
          "Emmanouil",
          "Benetos"
        ]
      ],
      "title": "Towards Joint Sound Scene and Polyphonic Sound Event Recognition",
      "original": "2169",
      "page_count": 5,
      "order": 960,
      "p1": "4594",
      "pn": "4598",
      "abstract": [
        "Acoustic Scene Classification (ASC) and Sound Event Detection (SED)\nare two separate tasks in the field of computational sound scene analysis.\nIn this work, we present a new dataset with both sound scene and sound\nevent labels and use this to demonstrate a novel method for jointly\nclassifying sound scenes and recognizing sound events. We show that\nby taking a joint approach, learning is more efficient and whilst improvements\nare still needed for sound event detection, SED results are robust\nin a dataset where the sample distribution is skewed towards sound\nscenes.\n"
      ],
      "doi": "10.21437/Interspeech.2019-2169",
      "author_area_id": 5,
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "fan19c_interspeech": {
      "authors": [
        [
          "Cunhang",
          "Fan"
        ],
        [
          "Bin",
          "Liu"
        ],
        [
          "Jianhua",
          "Tao"
        ],
        [
          "Jiangyan",
          "Yi"
        ],
        [
          "Zhengqi",
          "Wen"
        ]
      ],
      "title": "Discriminative Learning for Monaural Speech Separation Using Deep Embedding Features",
      "original": "1940",
      "page_count": 5,
      "order": 961,
      "p1": "4599",
      "pn": "4603",
      "abstract": [
        "Deep clustering (DC) and utterance-level permutation invariant training\n(uPIT) have been demonstrated promising for speaker-independent speech\nseparation. DC is usually formulated as two-step processes: embedding\nlearning and embedding clustering, which results in complex separation\npipelines and a huge obstacle in directly optimizing the actual separation\nobjectives. As for uPIT, it only minimizes the chosen permutation with\nthe lowest mean square error, doesn&#8217;t discriminate it with other\npermutations. In this paper, we propose a discriminative learning method\nfor speaker-independent speech separation using deep embedding features.\nFirstly, a DC network is trained to extract deep embedding features,\nwhich contain each source&#8217;s information and have an advantage\nin discriminating each target speakers. Then these features are used\nas the input for uPIT to directly separate the different sources. Finally,\nuPIT and DC are jointly trained, which directly optimizes the actual\nseparation objectives. Moreover, in order to maximize the distance\nof each permutation, the discriminative learning is applied to fine\ntuning the whole model. Our experiments are conducted on WSJ0-2mix\ndataset. Experimental results show that the proposed models achieve\nbetter performances than DC and uPIT for speaker-independent speech\nseparation.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1940",
      "author_area_id": 5,
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "yousefi19_interspeech": {
      "authors": [
        [
          "Midia",
          "Yousefi"
        ],
        [
          "Soheil",
          "Khorram"
        ],
        [
          "John H.L.",
          "Hansen"
        ]
      ],
      "title": "Probabilistic Permutation Invariant Training for Speech Separation",
      "original": "1827",
      "page_count": 5,
      "order": 962,
      "p1": "4604",
      "pn": "4608",
      "abstract": [
        "Single-microphone, speaker-independent speech separation is normally\nperformed through two steps:  (i) separating the specific speech sources,\nand  (ii) determining the best output-label assignment to find the\nseparation error. The second step is the main obstacle in training\nneural networks for speech separation. Recently proposed  Permutation\nInvariant Training (PIT) addresses this problem by determining the\noutput-label assignment which minimizes the separation error. In this\nstudy, we show that a major drawback of this technique is the overconfident\nchoice of the output-label assignment, especially in the initial steps\nof training when the network generates unreliable outputs. To solve\nthis problem, we propose  Probabilistic PIT (Prob-PIT) which considers\nthe output-label permutation as a discrete latent random variable with\na uniform prior distribution. Prob-PIT defines a log-likelihood function\nbased on the prior distributions and the separation errors of all permutations;\nit trains the speech separation networks by maximizing the log-likelihood\nfunction. Prob-PIT can be easily implemented by replacing the minimum\nfunction of PIT with a soft-minimum function. We evaluate our approach\nfor speech separation on both TIMIT and CHiME datasets. The results\nshow that the proposed method significantly outperforms PIT in terms\nof Signal to Distortion Ratio and Signal to Interference Ratio.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1827",
      "author_area_id": 5,
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "shi19e_interspeech": {
      "authors": [
        [
          "Jing",
          "Shi"
        ],
        [
          "Jiaming",
          "Xu"
        ],
        [
          "Bo",
          "Xu"
        ]
      ],
      "title": "Which Ones Are Speaking? Speaker-Inferred Model for Multi-Talker Speech Separation",
      "original": "1591",
      "page_count": 5,
      "order": 963,
      "p1": "4609",
      "pn": "4613",
      "abstract": [
        "Recent deep learning methods have gained noteworthy success in the\nmulti-talker mixed speech separation task, which is also famous known\nas the Cocktail Party Problem. However, most existing models are well-designed\ntowards some predefined conditions, which make them unable to handle\nthe complex auditory scene automatically, such as a variable and unknown\nnumber of speakers in the mixture. In this paper, we propose a speaker-inferred\nmodel, based on the flexible and efficient Seq2Seq generation model,\nto accurately infer the possible speakers and the speech channel of\neach. Our model is totally end-to-end with several different modules\nto emphasize and better utilize the information from speakers. Without\na priori knowledge about the number of speakers or any additional curriculum\ntraining strategy or man-made rules, our method gets comparable performance\nwith those strong baselines.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1591",
      "author_area_id": 5,
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "shi19f_interspeech": {
      "authors": [
        [
          "Ziqiang",
          "Shi"
        ],
        [
          "Huibin",
          "Lin"
        ],
        [
          "Liu",
          "Liu"
        ],
        [
          "Rujie",
          "Liu"
        ],
        [
          "Shoji",
          "Hayakawa"
        ],
        [
          "Shouji",
          "Harada"
        ],
        [
          "Jiqing",
          "Han"
        ]
      ],
      "title": "End-to-End Monaural Speech Separation with Multi-Scale Dynamic Weighted Gated Dilated Convolutional Pyramid Network",
      "original": "1292",
      "page_count": 5,
      "order": 964,
      "p1": "4614",
      "pn": "4618",
      "abstract": [
        "The monaural speech separation technology is far from satisfactory\nand has been a challenging task due to the interference of multiple\nsound sources. While deep dilated temporal convolutional networks (TCN)\nhave been proved to be very effective in sequence modeling, this work\ninvestigates how to extend TCN to result in a new state-of-the-art\napproach for monaural speech separation. First a novel gating mechanisms\nis introduced and added to result in gated TCN. The gated activation\ncan control the flow of information. Further in order to remedy the\ntemporal scale variation problem caused by word length and pronunciation\ncharacteristics of different people, a multi-scale dynamic weighted\npyramids gated TCNs is proposed, where a &#8220;weightor&#8221; network\nis used to determine the weights of different gated TCNs dynamically\nfor each utterance. Since the strengths of different branches with\ndifferent temporal receipt fields appear complementary, the combination\noutperforms single branch system. For the objective, we propose to\ntrain the network by directly optimizing utterance level signal-to-distortion\nratio (SDR) in a permutation invariant training (PIT) style. Our experiments\non the the WSJ0-2mix data corpus results in 18.4dB SDR improvement,\nwhich shows our proposed networks can leads to performance improvement\non the speaker separation task.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1292",
      "author_area_id": 5,
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "lluis19_interspeech": {
      "authors": [
        [
          "Francesc",
          "Llu\u00eds"
        ],
        [
          "Jordi",
          "Pons"
        ],
        [
          "Xavier",
          "Serra"
        ]
      ],
      "title": "End-to-End Music Source Separation: Is it Possible in the Waveform Domain?",
      "original": "1177",
      "page_count": 5,
      "order": 965,
      "p1": "4619",
      "pn": "4623",
      "abstract": [
        "Most of the currently successful source separation techniques use the\nmagnitude spectrogram as input, and are therefore by default omitting\npart of the signal: the phase. To avoid omitting potentially useful\ninformation, we study the viability of using end-to-end models for\nmusic source separation &#8212; which take into account all the information\navailable in the raw audio signal, including the phase. Although during\nthe last decades end-to-end music source separation has been considered\nalmost unattainable, our results confirm that waveform-based models\ncan perform similarly (if not better) than a spectrogram-based deep\nlearning model. Namely: a Wavenet-based model we propose and Wave-U-Net\ncan outperform DeepConvSep, a recent spectrogram-based deep learning\nmodel.\n"
      ],
      "doi": "10.21437/Interspeech.2019-1177",
      "author_area_id": 5,
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "foley19_interspeech": {
      "authors": [
        [
          "Ben",
          "Foley"
        ],
        [
          "Alina",
          "Rakhi"
        ],
        [
          "Nicholas",
          "Lambourne"
        ],
        [
          "Nicholas",
          "Buckeridge"
        ],
        [
          "Janet",
          "Wiles"
        ]
      ],
      "title": "Elpis, an Accessible Speech-to-Text Tool",
      "original": "8006",
      "page_count": 2,
      "order": 966,
      "p1": "4624",
      "pn": "4625",
      "abstract": [
        "Elpis is a speech-to-text tool which has been designed to give language\nworkers, including linguists and speech scientists, access to cutting-edge\nautomatic speech recognition software, without the specialist training\ntypically required to run these systems. Our presentation would demonstrate\nlocal and server-based versions of Elpis using sample data from the\nAbui (ISO 639: abz) language, about 17,000 speakers in Indonesia. Attendees\nwould gain a sense of the benefits that a first-pass ASR transcription\ncan bring to transcription workflows.\n"
      ]
    },
    "gruber19b_interspeech": {
      "authors": [
        [
          "Martin",
          "Gr\u016fber"
        ],
        [
          "Adam",
          "Ch\u00fdlek"
        ],
        [
          "Jind\u0159ich",
          "Matou\u0161ek"
        ]
      ],
      "title": "Framework for Conducting Tasks Requiring Human Assessment",
      "original": "8009",
      "page_count": 2,
      "order": 967,
      "p1": "4626",
      "pn": "4627",
      "abstract": [
        "This paper presents a web-based framework that improves and simplifies\nthe design and the deployment of tasks that require human input. These\ntasks may include speech, text or image transcription, annotation and\nevaluation. The focus is on listening tests for the purpose of a speech\nsynthesis quality assessment. The framework is quite flexible, i.e.\nmany different types of tasks can be prepared and presented to participants.\nThe participants can then work on the tasks via a user-friendly GUI\nand their responses are recorded in a database. The framework is ready\nto be integrated as an external task for Amazon Mechanical Turk but\nit can also be used as a stand-alone platform.\n"
      ]
    },
    "huang19k_interspeech": {
      "authors": [
        [
          "Shen",
          "Huang"
        ],
        [
          "Bojie",
          "Hu"
        ],
        [
          "Shan",
          "Huang"
        ],
        [
          "Pengfei",
          "Hu"
        ],
        [
          "Jian",
          "Kang"
        ],
        [
          "Zhiqiang",
          "Lv"
        ],
        [
          "Jinghao",
          "Yan"
        ],
        [
          "Qi",
          "Ju"
        ],
        [
          "Shiyin",
          "Kang"
        ],
        [
          "Deyi",
          "Tuo"
        ],
        [
          "Guangzhi",
          "Li"
        ],
        [
          "Nurmemet",
          "Yolwas"
        ]
      ],
      "title": "Multimedia Simultaneous Translation System for Minority Language Communication with Mandarin",
      "original": "8020",
      "page_count": 2,
      "order": 968,
      "p1": "4628",
      "pn": "4629",
      "abstract": [
        "Speech recognition for minority language is always behind main stream\ndue to lack of resources. This work presents a system for simultaneous\ntranslation between Mandarin and major minority languages such as Uyghur,\nTibetan in shape of speech, text and images. The general acoustic model\nis trained via factorized TDNN with lattice free MMI criteria using\nmixed-units based lexicon model. For each specific language, acoustic\nmodel is trained by multi-task mix-lingual modeling with shared bottleneck\nlayers followed by transfer learning. Besides, the system also supports\nstate-of-the-art OCR, TTS, and machine translation, by which language\ninformation will be real-time translated, punctuated and pronounced.\nThe machine translation behind the system gets a high rank in WMT 18\nMandarin-English and CWMT 18 minority language translation task. The\nsystem has integrated into a micro-app at WeChat and can facilitate\ncommunication between Mandarin and Minority languages.\n"
      ]
    },
    "dikici19_interspeech": {
      "authors": [
        [
          "Erinc",
          "Dikici"
        ],
        [
          "Gerhard",
          "Backfried"
        ],
        [
          "J\u00fcrgen",
          "Riedler"
        ]
      ],
      "title": "The SAIL LABS Media Mining Indexer and the CAVA Framework",
      "original": "8029",
      "page_count": 2,
      "order": 969,
      "p1": "4630",
      "pn": "4631",
      "abstract": [
        "In today&#8217;s attention-driven news economy, rapid changes of topics\nand events go hand in hand with rapid changes of vocabulary and of\nlanguage use. ASR systems aimed at transcribing contents pertaining\nto this fluid media landscape need to keep up-to-date in a continuous\nand dynamic manner. Static models, potentially created a long time\nago, are hopelessly outdated within a short period of time. The frequent\nchanges in vocabulary and wording need to be reflected in the models\nemployed for optimal performance of transcription if one does not want\nto risk falling behind. In this demonstration paper we present the\naudio processing capabilities of the SAIL LABS Media Mining Indexer,\nand the CAVA Framework allowing semi-automatic and periodic updates\nof the ASR vocabulary and language model from relevant and new data.\n"
      ]
    },
    "goel19b_interspeech": {
      "authors": [
        [
          "Nagendra Kumar",
          "Goel"
        ],
        [
          "Mousmita",
          "Sarma"
        ],
        [
          "Saikiran",
          "Valluri"
        ],
        [
          "Dharmeshkumar",
          "Agrawal"
        ],
        [
          "Steve",
          "Braich"
        ],
        [
          "Tejendra Singh",
          "Kuswah"
        ],
        [
          "Zikra",
          "Iqbal"
        ],
        [
          "Surbhi",
          "Chauhan"
        ],
        [
          "Raj",
          "Karbar"
        ]
      ],
      "title": "CaptionAI: A Real-Time Multilingual Captioning Application",
      "original": "8039",
      "page_count": 2,
      "order": 970,
      "p1": "4632",
      "pn": "4633",
      "abstract": [
        "We demonstrate CaptionAI, the system that can be used for speech to\ntext transcription, multilingual translation, and real-time closed\ncaptioning. It can also broadcast the audio and translated text to\npersonal devices. There are three components of the application, namely,\nspeech to text conversion, machine translation, and real time broadcast\nof audio and its multilingual text transcription. CaptionAI makes meetings,\nconference, and events accessible to global audience members with its\nreal-time multilingual captioning and broadcast capabilities, improving\ncomprehension and retention. In this application, we support English\nand Spanish real-time speech transcription. It also supports seventeen\npopular languages for real-time Machine Translation of transcribed\nspeech. The front-end is coded on c# and in back-end we use combination\nof python and c++ based software and packages such as Janus, Gstreamer,\nand libwebsockets.\n"
      ]
    }
  },
  "sessions": [
    {
      "title": "ISCA Medal 2019 Keynote Speech",
      "papers": [
        "tokuda19_interspeech"
      ]
    },
    {
      "title": "Spoken Language Processing for Children&#8217;s Speech",
      "papers": [
        "wu19_interspeech",
        "yeung19_interspeech",
        "gale19_interspeech",
        "ribeiro19_interspeech",
        "loukina19_interspeech",
        "lopes19_interspeech"
      ]
    },
    {
      "title": "Dynamics of Emotional Speech Exchanges in Multimodal Communication",
      "papers": [
        "esposito19_interspeech",
        "niebuhr19_interspeech",
        "cohn19_interspeech",
        "lai19_interspeech",
        "sebastian19_interspeech",
        "rajwadi19_interspeech",
        "kleinlein19_interspeech"
      ]
    },
    {
      "title": "End-to-End Speech Recognition",
      "papers": [
        "schluter19_interspeech",
        "pham19_interspeech",
        "li19_interspeech",
        "moritz19_interspeech",
        "belinkov19_interspeech"
      ]
    },
    {
      "title": "Speech Enhancement: Multi-Channel",
      "papers": [
        "tawara19_interspeech",
        "tesch19_interspeech",
        "martindonas19_interspeech",
        "bagheri19_interspeech",
        "togami19_interspeech",
        "nakatani19_interspeech"
      ]
    },
    {
      "title": "Speech Production: Individual Differences and the Brain",
      "papers": [
        "snyder19_interspeech",
        "illa19_interspeech",
        "zhang19_interspeech",
        "yoshinaga19_interspeech",
        "uttam19_interspeech",
        "saha19_interspeech"
      ]
    },
    {
      "title": "Speech Signal Characterization 1",
      "papers": [
        "chung19_interspeech",
        "huang19_interspeech",
        "m19_interspeech",
        "pascual19_interspeech",
        "nellore19_interspeech",
        "chatziagapi19_interspeech"
      ]
    },
    {
      "title": "Neural Waveform Generation",
      "papers": [
        "kons19_interspeech",
        "lorenzotrueba19_interspeech",
        "neekhara19_interspeech",
        "mustafa19_interspeech",
        "wu19b_interspeech",
        "tian19_interspeech"
      ]
    },
    {
      "title": "Attention Mechanism for Speaker State Recognition",
      "papers": [
        "han19_interspeech",
        "zhao19_interspeech",
        "li19b_interspeech",
        "gallardoantolin19_interspeech",
        "mallolragolta19_interspeech"
      ]
    },
    {
      "title": "ASR Neural Network Training &#8212; 1",
      "papers": [
        "carmantini19_interspeech",
        "luscher19_interspeech",
        "kanda19_interspeech",
        "meng19_interspeech",
        "wang19_interspeech",
        "mac19_interspeech"
      ]
    },
    {
      "title": "Zero-Resource ASR",
      "papers": [
        "milde19_interspeech",
        "ondel19_interspeech",
        "higuchi19_interspeech",
        "prasad19_interspeech",
        "azuh19_interspeech",
        "feng19_interspeech"
      ]
    },
    {
      "title": "Sociophonetics",
      "papers": [
        "nissen19_interspeech",
        "ahlers19_interspeech",
        "gubian19_interspeech",
        "gessinger19_interspeech",
        "niebuhr19b_interspeech",
        "michalsky19_interspeech"
      ]
    },
    {
      "title": "Resources &#8211; Annotation &#8211; Evaluation",
      "papers": [
        "sager19_interspeech",
        "koh19_interspeech",
        "picheny19_interspeech",
        "ramteke19_interspeech",
        "ali19_interspeech",
        "fallgren19_interspeech"
      ]
    },
    {
      "title": "Speaker Recognition and Diarization",
      "papers": [
        "diez19_interspeech",
        "vestman19_interspeech",
        "shon19_interspeech",
        "gao19_interspeech",
        "lin19_interspeech",
        "chung19b_interspeech",
        "xie19_interspeech",
        "mccree19_interspeech",
        "ghahabi19_interspeech",
        "park19_interspeech",
        "shafey19_interspeech",
        "cumani19_interspeech",
        "yamamoto19_interspeech",
        "ylmaz19_interspeech",
        "dubey19_interspeech"
      ]
    },
    {
      "title": "ASR for Noisy and Far-Field Speech",
      "papers": [
        "kovacs19_interspeech",
        "soni19_interspeech",
        "wu19c_interspeech",
        "ming19_interspeech",
        "soni19b_interspeech",
        "kumar19_interspeech",
        "delcroix19_interspeech",
        "hsu19_interspeech",
        "suzuki19_interspeech",
        "wu19d_interspeech",
        "wang19b_interspeech",
        "wang19c_interspeech",
        "neekhara19b_interspeech",
        "fujimoto19_interspeech",
        "liu19_interspeech"
      ]
    },
    {
      "title": "Social Signals Detection and Speaker Traits Analysis",
      "papers": [
        "yang19_interspeech",
        "dinkov19_interspeech",
        "an19_interspeech",
        "weninger19_interspeech",
        "gosztolya19_interspeech",
        "mori19_interspeech",
        "ludusan19_interspeech",
        "truong19_interspeech",
        "baird19_interspeech",
        "baird19b_interspeech",
        "niebuhr19c_interspeech",
        "vasquezcorrea19_interspeech"
      ]
    },
    {
      "title": "Applications of Language Technologies",
      "papers": [
        "chang19_interspeech",
        "meier19_interspeech",
        "beeferman19_interspeech",
        "mdhaffar19_interspeech",
        "marinelli19_interspeech",
        "dabike19_interspeech",
        "huang19b_interspeech",
        "vidal19_interspeech",
        "angerbauer19_interspeech",
        "luo19_interspeech"
      ]
    },
    {
      "title": "Speech and Audio Characterization and Segmentation",
      "papers": [
        "gutz19_interspeech",
        "k19_interspeech",
        "heo19_interspeech",
        "chen19_interspeech",
        "sharma19_interspeech",
        "shrem19_interspeech",
        "hui19_interspeech",
        "shah19_interspeech",
        "shankar19_interspeech",
        "mateju19_interspeech",
        "tang19_interspeech"
      ]
    },
    {
      "title": "Neural Techniques for Voice Conversion and Waveform Generation",
      "papers": [
        "paul19_interspeech",
        "chou19_interspeech",
        "lu19_interspeech",
        "tobing19_interspeech",
        "kaneko19_interspeech",
        "kurita19_interspeech",
        "zhao19b_interspeech",
        "juvela19_interspeech",
        "yamamoto19b_interspeech",
        "mohammadi19_interspeech",
        "huang19c_interspeech",
        "liu19b_interspeech",
        "chen19b_interspeech",
        "ding19_interspeech",
        "stephenson19_interspeech"
      ]
    },
    {
      "title": "Model Adaptation for ASR",
      "papers": [
        "dey19_interspeech",
        "kim19_interspeech",
        "zhu19_interspeech",
        "guo19_interspeech",
        "kitza19_interspeech",
        "xie19b_interspeech",
        "tsunoo19_interspeech",
        "sar19_interspeech",
        "sim19_interspeech",
        "jain19_interspeech",
        "shor19_interspeech"
      ]
    },
    {
      "title": "Dialogue Speech Understanding",
      "papers": [
        "peskov19_interspeech",
        "gupta19_interspeech",
        "marzinotto19_interspeech",
        "parcollet19_interspeech",
        "georges19_interspeech",
        "lugosch19_interspeech",
        "shivakumar19_interspeech",
        "tomashenko19_interspeech",
        "song19_interspeech",
        "masumura19_interspeech",
        "chien19_interspeech",
        "williams19_interspeech",
        "korpusik19_interspeech",
        "kobayashi19_interspeech"
      ]
    },
    {
      "title": "Speech Production and Silent Interfaces",
      "papers": [
        "seneviratne19_interspeech",
        "dash19_interspeech",
        "sheth19_interspeech",
        "silva19_interspeech",
        "douros19_interspeech",
        "rasskazova19_interspeech",
        "gubian19b_interspeech",
        "csapo19_interspeech",
        "klein19_interspeech",
        "takemoto19_interspeech",
        "leeuwen19_interspeech",
        "mucke19_interspeech"
      ]
    },
    {
      "title": "Speech Signal Characterization 2",
      "papers": [
        "kleijn19_interspeech",
        "ramanathi19_interspeech",
        "mannem19_interspeech",
        "springenberg19_interspeech",
        "paraskevopoulos19_interspeech",
        "dhiman19_interspeech",
        "xu19_interspeech",
        "sudhakara19_interspeech",
        "saha19b_interspeech"
      ]
    },
    {
      "title": "Applications in Language Learning and Healthcare",
      "papers": [
        "vasquezcorrea19b_interspeech",
        "kiss19_interspeech",
        "yarra19_interspeech",
        "nissen19b_interspeech",
        "miwardelli19_interspeech",
        "annand19_interspeech",
        "radostev19_interspeech"
      ]
    },
    {
      "title": "Keynote 2: Tanja Schultz",
      "papers": [
        "schultz19_interspeech"
      ]
    },
    {
      "title": "The Second DIHARD Speech Diarization Challenge (DIHARD II)",
      "papers": [
        "ryant19_interspeech",
        "singh19_interspeech",
        "vinals19_interspeech",
        "zajic19_interspeech",
        "park19b_interspeech",
        "novoselov19_interspeech"
      ]
    },
    {
      "title": "The 2019 Automatic Speaker Verification Spoofing and Countermeasures Challenge: ASVspoof Challenge &#8212; O",
      "papers": [
        "todisco19_interspeech"
      ]
    },
    {
      "title": "The 2019 Automatic Speaker Verification Spoofing and Countermeasures Challenge: ASVspoof Challenge &#8212; P",
      "papers": [
        "lai19b_interspeech",
        "chettri19_interspeech",
        "cai19_interspeech",
        "biaobrzeski19_interspeech",
        "lavrentyeva19_interspeech",
        "yang19b_interspeech",
        "alluri19_interspeech",
        "li19c_interspeech",
        "williams19b_interspeech",
        "das19_interspeech",
        "chang19b_interspeech",
        "gomezalanis19_interspeech",
        "zeinali19_interspeech",
        "alzantot19_interspeech",
        "jung19_interspeech"
      ]
    },
    {
      "title": "The Zero Resource Speech Challenge 2019: TTS Without T",
      "papers": [
        "dunbar19_interspeech",
        "feng19b_interspeech",
        "yusuf19_interspeech",
        "eloff19_interspeech",
        "liu19c_interspeech",
        "s19_interspeech",
        "tjandra19_interspeech"
      ]
    },
    {
      "title": "Speech Translation",
      "papers": [
        "niehues19_interspeech",
        "jia19_interspeech",
        "liu19d_interspeech",
        "gangi19_interspeech",
        "hillis19_interspeech"
      ]
    },
    {
      "title": "Speaker Recognition 1",
      "papers": [
        "bhattacharya19_interspeech",
        "wang19d_interspeech",
        "ravanelli19_interspeech",
        "you19_interspeech",
        "wu19e_interspeech",
        "you19b_interspeech"
      ]
    },
    {
      "title": "Dialogue Understanding",
      "papers": [
        "bhat19_interspeech",
        "vukotic19_interspeech",
        "ray19_interspeech",
        "bhosale19_interspeech",
        "takatsu19_interspeech",
        "caubriere19_interspeech"
      ]
    },
    {
      "title": "Speech in the Brain",
      "papers": [
        "dash19b_interspeech",
        "nijveld19_interspeech",
        "bosch19_interspeech",
        "kharaman19_interspeech",
        "scharenborg19_interspeech",
        "parmonangan19_interspeech"
      ]
    },
    {
      "title": "Far-Field Speech Recognition",
      "papers": [
        "huang19d_interspeech",
        "zhao19c_interspeech",
        "khokhlov19_interspeech",
        "kanda19b_interspeech",
        "drude19_interspeech",
        "ma19_interspeech"
      ]
    },
    {
      "title": "Speaker and Language Recognition 1",
      "papers": [
        "li19d_interspeech",
        "padi19_interspeech",
        "jung19b_interspeech",
        "rao19_interspeech",
        "mazzawi19_interspeech"
      ]
    },
    {
      "title": "Speech Synthesis: Towards End-to-End",
      "papers": [
        "zheng19_interspeech",
        "guo19b_interspeech",
        "he19_interspeech",
        "zhang19b_interspeech",
        "luong19_interspeech",
        "okamoto19_interspeech"
      ]
    },
    {
      "title": "Semantic Analysis and Classification",
      "papers": [
        "kafle19_interspeech",
        "chien19b_interspeech",
        "sun19_interspeech",
        "shen19_interspeech",
        "tundik19_interspeech",
        "huang19e_interspeech"
      ]
    },
    {
      "title": "Speech and Audio Source Separation and Scene Analysis 1",
      "papers": [
        "narisetty19_interspeech",
        "takahashi19_interspeech",
        "appeltans19_interspeech",
        "gu19_interspeech",
        "yang19c_interspeech",
        "wichern19_interspeech"
      ]
    },
    {
      "title": "Speech Intelligibility",
      "papers": [
        "nautsch19_interspeech",
        "chermaz19_interspeech",
        "edraki19_interspeech",
        "zhang19c_interspeech",
        "dinh19_interspeech"
      ]
    },
    {
      "title": "ASR Neural Network Architectures 1",
      "papers": [
        "platen19_interspeech",
        "merboldt19_interspeech",
        "sun19b_interspeech",
        "karita19_interspeech",
        "zhang19d_interspeech",
        "zhao19d_interspeech"
      ]
    },
    {
      "title": "Speech and Language Analytics for Mental Health",
      "papers": [
        "nasir19_interspeech",
        "du19_interspeech",
        "voleti19_interspeech",
        "matton19_interspeech",
        "rohanian19_interspeech",
        "espywilson19_interspeech"
      ]
    },
    {
      "title": "Dialogue Modelling",
      "papers": [
        "paul19b_interspeech",
        "goel19_interspeech",
        "martinek19_interspeech",
        "chao19_interspeech",
        "griol19_interspeech",
        "chen19c_interspeech"
      ]
    },
    {
      "title": "Speaker Recognition Evaluation",
      "papers": [
        "sadjadi19_interspeech",
        "villalba19_interspeech",
        "garciaromero19_interspeech",
        "lee19_interspeech",
        "khoury19_interspeech",
        "garciaromero19b_interspeech"
      ]
    },
    {
      "title": "Speech Synthesis: Data and Evaluation",
      "papers": [
        "ayllon19_interspeech",
        "kuo19_interspeech",
        "braude19_interspeech",
        "zen19_interspeech",
        "shamsi19_interspeech",
        "hojo19_interspeech",
        "lo19_interspeech",
        "fong19_interspeech",
        "govender19_interspeech",
        "douros19b_interspeech",
        "chen19d_interspeech",
        "park19c_interspeech"
      ]
    },
    {
      "title": "Model Training for ASR",
      "papers": [
        "karaulov19_interspeech",
        "tong19_interspeech",
        "cui19_interspeech",
        "shah19b_interspeech",
        "goyal19_interspeech",
        "fainberg19_interspeech",
        "michel19_interspeech",
        "masumura19b_interspeech",
        "heba19_interspeech",
        "kurata19_interspeech",
        "fukuda19_interspeech",
        "ladkat19_interspeech",
        "huang19f_interspeech",
        "kurata19b_interspeech",
        "li19e_interspeech"
      ]
    },
    {
      "title": "Network Architectures for Emotion and Paralinguistics Recognition",
      "papers": [
        "georgiou19_interspeech",
        "mitra19_interspeech",
        "parry19_interspeech",
        "wang19e_interspeech",
        "egorow19_interspeech",
        "zhao19e_interspeech",
        "zhong19_interspeech",
        "chao19b_interspeech",
        "mao19_interspeech",
        "triantafyllopoulos19_interspeech",
        "li19f_interspeech",
        "jalal19_interspeech"
      ]
    },
    {
      "title": "Acoustic Phonetics",
      "papers": [
        "dapolito19_interspeech",
        "jamakovic19_interspeech",
        "arantes19_interspeech",
        "kelly19_interspeech",
        "jatteau19_interspeech",
        "huang19g_interspeech",
        "schuppler19_interspeech",
        "johny19_interspeech",
        "guitardivent19_interspeech",
        "wei19_interspeech",
        "moczanow19_interspeech",
        "berger19_interspeech",
        "luo19b_interspeech",
        "zarka19_interspeech",
        "alowonou19_interspeech"
      ]
    },
    {
      "title": "Speech Enhancement: Noise Attenuation",
      "papers": [
        "guo19c_interspeech",
        "hao19_interspeech",
        "pascual19b_interspeech",
        "li19g_interspeech",
        "lin19b_interspeech",
        "chai19_interspeech",
        "llombart19_interspeech",
        "reddy19_interspeech",
        "adiga19_interspeech",
        "pv19_interspeech",
        "braithwaite19_interspeech"
      ]
    },
    {
      "title": "Language Learning and Databases",
      "papers": [
        "kyriakopoulos19_interspeech",
        "merkx19_interspeech",
        "skidmore19_interspeech",
        "hansen19_interspeech",
        "chen19e_interspeech",
        "trisitichoke19_interspeech",
        "karhila19_interspeech",
        "yoon19_interspeech",
        "lu19b_interspeech",
        "yang19d_interspeech"
      ]
    },
    {
      "title": "Emotion and Personality in Conversation",
      "papers": [
        "hori19_interspeech",
        "gopalakrishnan19_interspeech",
        "kubasova19_interspeech",
        "martinez19_interspeech",
        "haake19_interspeech",
        "li19h_interspeech",
        "gjoreski19_interspeech",
        "yu19_interspeech",
        "aldeneh19_interspeech",
        "lubold19_interspeech",
        "lian19_interspeech"
      ]
    },
    {
      "title": "Voice Quality, Speech Perception, and Prosody",
      "papers": [
        "oneill19_interspeech",
        "kakouros19_interspeech",
        "peperkamp19_interspeech",
        "kocharov19_interspeech",
        "gobl19_interspeech",
        "chodroff19_interspeech",
        "park19d_interspeech",
        "sturm19_interspeech",
        "kelterer19_interspeech",
        "chien19c_interspeech",
        "tavi19_interspeech"
      ]
    },
    {
      "title": "Speech Signal Characterization 3",
      "papers": [
        "xu19b_interspeech",
        "sharma19b_interspeech",
        "ardaillon19_interspeech",
        "dong19_interspeech",
        "sharma19c_interspeech",
        "sharma19d_interspeech",
        "terasawa19_interspeech",
        "lin19c_interspeech",
        "yamamoto19c_interspeech",
        "gupta19b_interspeech",
        "vafeiadis19_interspeech",
        "kaburagi19_interspeech"
      ]
    },
    {
      "title": "Speech Synthesis: Pronunciation, Multilingual, and Low Resource",
      "papers": [
        "zou19_interspeech",
        "xue19_interspeech",
        "sokolov19_interspeech",
        "taylor19_interspeech",
        "chen19f_interspeech",
        "zhang19e_interspeech",
        "juzova19_interspeech",
        "dai19_interspeech",
        "yolchuyeva19_interspeech",
        "bleyan19_interspeech",
        "chen19g_interspeech",
        "cai19b_interspeech",
        "sun19c_interspeech"
      ]
    },
    {
      "title": "Cross-Lingual and Multilingual ASR",
      "papers": [
        "li19i_interspeech",
        "arsikere19_interspeech",
        "kannan19_interspeech",
        "mendes19_interspeech",
        "viglino19_interspeech",
        "li19j_interspeech",
        "taneja19_interspeech",
        "hu19_interspeech",
        "khassanov19_interspeech",
        "zeng19_interspeech",
        "zhang19f_interspeech"
      ]
    },
    {
      "title": "Spoken Term Detection, Confidence Measure, and End-to-End Speech Recognition",
      "papers": [
        "swarup19_interspeech",
        "zhang19g_interspeech",
        "peyser19_interspeech",
        "bai19_interspeech",
        "kao19_interspeech",
        "li19k_interspeech",
        "guo19d_interspeech",
        "tanaka19_interspeech",
        "malhotra19_interspeech",
        "karafiat19_interspeech",
        "zapotoczny19_interspeech",
        "jansche19_interspeech",
        "dudziak19_interspeech",
        "gaur19_interspeech",
        "li19l_interspeech"
      ]
    },
    {
      "title": "Speech Perception",
      "papers": [
        "cohn19b_interspeech",
        "lewandowski19_interspeech",
        "lalonde19_interspeech",
        "bentum19_interspeech",
        "bentum19b_interspeech",
        "turner19_interspeech",
        "bosch19b_interspeech",
        "liu19e_interspeech",
        "yokoe19_interspeech",
        "meyer19_interspeech",
        "hsieh19_interspeech",
        "levari19_interspeech",
        "segedin19_interspeech",
        "papadimitriou19_interspeech"
      ]
    },
    {
      "title": "Topics in Speech and Audio Signal Processing",
      "papers": [
        "somandepalli19_interspeech",
        "belitz19_interspeech",
        "nguyen19_interspeech",
        "ahmed19_interspeech",
        "moore19_interspeech",
        "muckenhirn19_interspeech",
        "kilgour19_interspeech",
        "gong19_interspeech",
        "bt19_interspeech"
      ]
    },
    {
      "title": "Speech Processing and Analysis",
      "papers": [
        "lee19b_interspeech",
        "pienaar19_interspeech",
        "maurer19_interspeech",
        "noll19_interspeech",
        "eldesouki19_interspeech",
        "haider19_interspeech",
        "gupta19c_interspeech"
      ]
    },
    {
      "title": "Keynote 3: Manfred Kaltenbacher",
      "papers": [
        "kaltenbacher19_interspeech"
      ]
    },
    {
      "title": "The Interspeech 2019 Computational Paralinguistics Challenge (ComParE)",
      "papers": [
        "schuller19_interspeech",
        "dubagunta19_interspeech",
        "elsner19_interspeech",
        "kisler19_interspeech",
        "yeh19_interspeech",
        "wu19f_interspeech",
        "ravi19_interspeech",
        "gosztolya19b_interspeech",
        "das19b_interspeech",
        "schiller19_interspeech",
        "caraty19_interspeech",
        "wu19g_interspeech"
      ]
    },
    {
      "title": "The VOiCES from a Distance Challenge &#8212; O",
      "papers": [
        "nandwana19_interspeech",
        "novoselov19b_interspeech",
        "matejka19_interspeech",
        "medennikov19_interspeech",
        "chong19_interspeech"
      ]
    },
    {
      "title": "The VOiCES from a Distance Challenge &#8212; P",
      "papers": [
        "nandwana19b_interspeech",
        "novoselov19c_interspeech",
        "matejka19b_interspeech",
        "medennikov19b_interspeech",
        "chong19b_interspeech",
        "jati19_interspeech",
        "snyder19b_interspeech",
        "huang19h_interspeech",
        "sun19d_interspeech",
        "liang19_interspeech",
        "wang19f_interspeech",
        "cai19c_interspeech"
      ]
    },
    {
      "title": "Voice Quality Characterization for Clinical Voice Assessment: Voice Production, Acoustics, and Auditory Perception",
      "papers": [
        "hauptman19_interspeech",
        "drioli19_interspeech",
        "kadiri19_interspeech",
        "cho19_interspeech",
        "schoentgen19_interspeech",
        "schaeffler19_interspeech",
        "moore19b_interspeech"
      ]
    },
    {
      "title": "Prosody",
      "papers": [
        "ward19_interspeech",
        "roessig19_interspeech",
        "suni19_interspeech",
        "murphy19_interspeech",
        "albar19_interspeech"
      ]
    },
    {
      "title": "Speech and Audio Classification 1",
      "papers": [
        "okawa19_interspeech",
        "mulimani19_interspeech",
        "shen19b_interspeech",
        "ford19_interspeech",
        "reddy19b_interspeech",
        "tarantino19_interspeech"
      ]
    },
    {
      "title": "Singing and Multimodal Synthesis",
      "papers": [
        "nachmani19_interspeech",
        "lee19c_interspeech",
        "yi19_interspeech",
        "dahmani19_interspeech",
        "ayllon19b_interspeech",
        "biasuttolervat19_interspeech"
      ]
    },
    {
      "title": "ASR Neural Network Training &#8212; 2",
      "papers": [
        "park19e_interspeech",
        "audhkhasi19_interspeech",
        "miao19_interspeech",
        "zhang19h_interspeech",
        "zhang19i_interspeech",
        "menne19_interspeech"
      ]
    },
    {
      "title": "Bilingualism, L2, and Non-Nativeness",
      "papers": [
        "bradlow19_interspeech",
        "novak19_interspeech",
        "shi19_interspeech",
        "chen19h_interspeech",
        "tremblay19_interspeech"
      ]
    },
    {
      "title": "Spoken Term Detection",
      "papers": [
        "abujabal19_interspeech",
        "bhati19_interspeech",
        "yusuf19b_interspeech",
        "yang19e_interspeech",
        "wang19g_interspeech",
        "boito19_interspeech"
      ]
    },
    {
      "title": "Speech and Audio Source Separation and Scene Analysis 2",
      "papers": [
        "xue19b_interspeech",
        "grondin19_interspeech",
        "zhang19j_interspeech",
        "masuyama19_interspeech",
        "li19m_interspeech",
        "ochiai19_interspeech"
      ]
    },
    {
      "title": "Speech Enhancement: Single Channel 2",
      "papers": [
        "germain19_interspeech",
        "wang19h_interspeech",
        "liao19_interspeech",
        "mowlaee19_interspeech",
        "yao19_interspeech",
        "hui19b_interspeech"
      ]
    },
    {
      "title": "Multimodal ASR",
      "papers": [
        "metze19_interspeech",
        "shrivastava19_interspeech",
        "kandala19_interspeech",
        "koumparoulis19_interspeech",
        "qu19_interspeech"
      ]
    },
    {
      "title": "ASR Neural Network Architectures 2",
      "papers": [
        "sainath19_interspeech",
        "lam19_interspeech",
        "gowda19_interspeech",
        "han19b_interspeech",
        "hu19b_interspeech",
        "lu19c_interspeech"
      ]
    },
    {
      "title": "Training Strategy for Speech Emotion Recognition",
      "papers": [
        "li19n_interspeech",
        "schmitt19_interspeech",
        "ouyang19_interspeech",
        "ando19_interspeech",
        "gorrostieta19_interspeech",
        "bao19_interspeech"
      ]
    },
    {
      "title": "Voice Conversion for Style, Accent, and Emotion",
      "papers": [
        "bollepalli19_interspeech",
        "seshadri19_interspeech",
        "zhao19f_interspeech",
        "shankar19b_interspeech",
        "lapidot19_interspeech",
        "gao19b_interspeech"
      ]
    },
    {
      "title": "Speaker Recognition 2",
      "papers": [
        "stafylakis19_interspeech",
        "nautsch19b_interspeech",
        "liu19f_interspeech",
        "hajavi19_interspeech",
        "zhou19_interspeech",
        "shon19b_interspeech"
      ]
    },
    {
      "title": "Speaker Recognition and Anti-Spoofing",
      "papers": [
        "avila19_interspeech",
        "patil19_interspeech",
        "mingote19_interspeech",
        "fan19_interspeech",
        "marras19_interspeech",
        "gunendradasan19_interspeech",
        "yun19_interspeech",
        "seo19_interspeech",
        "you19c_interspeech",
        "wang19i_interspeech",
        "kanagasundaram19_interspeech",
        "chen19i_interspeech",
        "wickramasinghe19_interspeech",
        "bousquet19_interspeech",
        "shon19c_interspeech"
      ]
    },
    {
      "title": "Rich Transcription and ASR Systems",
      "papers": [
        "yoshioka19_interspeech",
        "thomas19_interspeech",
        "farooq19_interspeech",
        "tang19b_interspeech",
        "szaszak19_interspeech",
        "pellegrini19_interspeech",
        "oneata19_interspeech",
        "wang19j_interspeech",
        "biswas19_interspeech",
        "helgadottir19_interspeech",
        "gupta19d_interspeech"
      ]
    },
    {
      "title": "Speech and Language Analytics for Medical Applications",
      "papers": [
        "rutowski19_interspeech",
        "pietrowicz19_interspeech",
        "jeancolas19_interspeech",
        "janbakhshi19_interspeech",
        "pasquale19_interspeech",
        "rueda19_interspeech",
        "onu19_interspeech",
        "hong19_interspeech",
        "lopez19_interspeech",
        "klumpp19_interspeech",
        "chakravarthula19_interspeech",
        "qin19_interspeech"
      ]
    },
    {
      "title": "Speech Perception in Adverse Listening Conditions",
      "papers": [
        "fu19_interspeech",
        "zayats19_interspeech",
        "parhammer19_interspeech",
        "hazan19_interspeech",
        "davis19_interspeech",
        "ariasvergara19_interspeech",
        "hodoshima19_interspeech",
        "mamun19_interspeech",
        "felker19_interspeech",
        "paulus19_interspeech",
        "ward19b_interspeech",
        "chen19j_interspeech"
      ]
    },
    {
      "title": "Speech Enhancement: Single Channel 1",
      "papers": [
        "pirhosseinloo19_interspeech",
        "liao19b_interspeech",
        "ge19_interspeech",
        "pariente19_interspeech",
        "lin19d_interspeech",
        "zezario19_interspeech",
        "chuang19_interspeech",
        "liu19g_interspeech",
        "shi19b_interspeech",
        "wang19k_interspeech",
        "llombart19b_interspeech"
      ]
    },
    {
      "title": "Speech Recognition and Beyond",
      "papers": [
        "chen19k_interspeech",
        "mantena19_interspeech",
        "khare19_interspeech",
        "soomro19_interspeech",
        "ding19b_interspeech",
        "lopezespejo19_interspeech",
        "doulaty19_interspeech",
        "li19o_interspeech",
        "suzuki19b_interspeech",
        "hsu19b_interspeech"
      ]
    },
    {
      "title": "Emotion Modeling and Analysis",
      "papers": [
        "luo19c_interspeech",
        "tammewar19_interspeech",
        "chakraborty19_interspeech",
        "li19p_interspeech",
        "rajan19_interspeech",
        "sridhar19_interspeech",
        "jin19_interspeech",
        "gideon19_interspeech",
        "nazareth19_interspeech",
        "zhao19g_interspeech",
        "gharsellaoui19_interspeech",
        "sahu19_interspeech"
      ]
    },
    {
      "title": "Articulatory Phonetics",
      "papers": [
        "spinu19_interspeech",
        "ratko19_interspeech",
        "deme19_interspeech",
        "king19_interspeech",
        "marko19_interspeech",
        "cunha19_interspeech"
      ]
    },
    {
      "title": "Speech and Audio Classification 2",
      "papers": [
        "xiong19_interspeech",
        "huang19i_interspeech",
        "su19_interspeech",
        "ashihara19_interspeech",
        "bergler19_interspeech",
        "sacchi19_interspeech",
        "gao19c_interspeech",
        "choi19_interspeech",
        "huang19j_interspeech",
        "yang19f_interspeech",
        "carmi19_interspeech"
      ]
    },
    {
      "title": "Speech Coding and Evaluation",
      "papers": [
        "hwang19_interspeech",
        "zhen19_interspeech",
        "backstrom19_interspeech",
        "valin19_interspeech",
        "fuchs19_interspeech",
        "li19q_interspeech",
        "gupta19e_interspeech",
        "mittag19_interspeech",
        "chai19b_interspeech",
        "moller19_interspeech"
      ]
    },
    {
      "title": "Feature Extraction for ASR",
      "papers": [
        "sadhu19_interspeech",
        "li19r_interspeech",
        "agrawal19_interspeech",
        "ramsay19_interspeech",
        "riviello19_interspeech",
        "schneider19_interspeech",
        "cho19b_interspeech",
        "menon19_interspeech",
        "loweimi19_interspeech"
      ]
    },
    {
      "title": "Lexicon and Language Model for Speech Recognition",
      "papers": [
        "verwimp19_interspeech",
        "chen19l_interspeech",
        "liu19h_interspeech",
        "pusateri19_interspeech",
        "khassanov19b_interspeech",
        "yu19b_interspeech",
        "agenbag19_interspeech",
        "coucheirolimeres19_interspeech",
        "gao19d_interspeech",
        "ritchie19_interspeech",
        "sharma19e_interspeech"
      ]
    },
    {
      "title": "First and Second Language Acquisition",
      "papers": [
        "sennema19_interspeech",
        "lewis19_interspeech",
        "jenne19_interspeech",
        "foltz19_interspeech",
        "bradlow19b_interspeech",
        "graham19_interspeech",
        "xu19c_interspeech",
        "peperkamp19b_interspeech",
        "seidl19_interspeech",
        "ludusan19b_interspeech",
        "marklund19_interspeech",
        "rasanen19_interspeech",
        "du19b_interspeech"
      ]
    },
    {
      "title": "Speech and Audio Classification 3",
      "papers": [
        "li19s_interspeech",
        "cerutti19_interspeech",
        "lu19d_interspeech",
        "bai19b_interspeech",
        "he19b_interspeech",
        "xia19_interspeech",
        "pham19b_interspeech",
        "shi19c_interspeech",
        "chen19m_interspeech",
        "zhang19k_interspeech",
        "lu19e_interspeech"
      ]
    },
    {
      "title": "Speech and Speaker Recognition",
      "papers": [
        "mizgajski19_interspeech",
        "an19b_interspeech",
        "chylek19_interspeech",
        "jelil19_interspeech",
        "wu19h_interspeech",
        "hu19c_interspeech"
      ]
    },
    {
      "title": "Speech Annotation and Labelling",
      "papers": [
        "schiel19_interspeech",
        "voe19_interspeech",
        "dominguez19_interspeech",
        "levy19_interspeech",
        "lo19b_interspeech",
        "li19t_interspeech"
      ]
    },
    {
      "title": "Speech Synthesis",
      "papers": [
        "gruber19_interspeech",
        "perrotin19_interspeech",
        "szekely19_interspeech",
        "kessler19_interspeech",
        "bernardo19_interspeech",
        "rabiee19_interspeech"
      ]
    },
    {
      "title": "Keynote 4: Mirella Lapata",
      "papers": [
        "lapata19_interspeech"
      ]
    },
    {
      "title": "Privacy in Speech and Audio Interfaces",
      "papers": [
        "nautsch19c_interspeech",
        "srivastava19_interspeech",
        "nelus19_interspeech",
        "nelus19b_interspeech",
        "thaine19_interspeech",
        "zarazaga19_interspeech"
      ]
    },
    {
      "title": "Speech Technologies for Code-Switching in Multilingual Communities",
      "papers": [
        "soto19_interspeech",
        "lee19d_interspeech",
        "rallabandi19_interspeech",
        "wang19l_interspeech",
        "biswas19b_interspeech",
        "ylmaz19b_interspeech",
        "seki19_interspeech"
      ]
    },
    {
      "title": "Speech Synthesis: Articulatory and Physical Approaches",
      "papers": [
        "guasch19_interspeech",
        "mohapatra19_interspeech",
        "birkholz19_interspeech",
        "gao19e_interspeech",
        "shahrebabaki19_interspeech"
      ]
    },
    {
      "title": "Sequence-to-Sequence Speech Recognition",
      "papers": [
        "tuske19_interspeech",
        "hannun19_interspeech",
        "baskar19_interspeech",
        "bai19c_interspeech",
        "irie19_interspeech",
        "weninger19b_interspeech"
      ]
    },
    {
      "title": "Search Methods for Speech Recognition",
      "papers": [
        "runarsdottir19_interspeech",
        "fukunaga19_interspeech",
        "jorge19_interspeech",
        "seki19b_interspeech",
        "serrino19_interspeech",
        "novitasari19_interspeech"
      ]
    },
    {
      "title": "Audio Signal Characterization",
      "papers": [
        "lian19b_interspeech",
        "phan19_interspeech",
        "shi19d_interspeech",
        "zhang19l_interspeech",
        "song19b_interspeech",
        "qi19_interspeech"
      ]
    },
    {
      "title": "Speech and Voice Disorders 1",
      "papers": [
        "liu19i_interspeech",
        "morovelazquez19_interspeech",
        "wang19m_interspeech",
        "wang19n_interspeech",
        "korzekwa19_interspeech",
        "noufi19_interspeech"
      ]
    },
    {
      "title": "Neural Networks for Language Modeling",
      "papers": [
        "scharenborg19b_interspeech",
        "ogawa19_interspeech",
        "irie19b_interspeech",
        "raju19_interspeech",
        "likhomanenko19_interspeech"
      ]
    },
    {
      "title": "Representation Learning of Emotion and Paralinguistics",
      "papers": [
        "latif19_interspeech",
        "sarma19_interspeech",
        "cao19_interspeech",
        "oates19_interspeech",
        "gosztolya19c_interspeech",
        "williams19c_interspeech"
      ]
    },
    {
      "title": "World&#8217;s Languages and Varieties",
      "papers": [
        "hsu19c_interspeech",
        "hu19d_interspeech",
        "zhang19m_interspeech",
        "albuquerque19_interspeech",
        "lalhminghlui19_interspeech",
        "rodriguez19_interspeech"
      ]
    },
    {
      "title": "Adaptation and Accommodation in Conversation",
      "papers": [
        "ibrahim19_interspeech",
        "wagner19_interspeech",
        "betz19_interspeech",
        "si19_interspeech",
        "metcalf19_interspeech",
        "raveh19_interspeech"
      ]
    },
    {
      "title": "Speaker and Language Recognition 2",
      "papers": [
        "wang19o_interspeech",
        "monteiro19_interspeech",
        "zhang19n_interspeech",
        "mingote19b_interspeech",
        "jung19c_interspeech",
        "heo19b_interspeech",
        "jiang19_interspeech",
        "qin19b_interspeech",
        "ren19_interspeech",
        "kaminishi19_interspeech",
        "khan19_interspeech",
        "zheng19b_interspeech",
        "taherian19_interspeech",
        "yang19g_interspeech",
        "miao19b_interspeech"
      ]
    },
    {
      "title": "Medical Applications and Visual ASR",
      "papers": [
        "chen19n_interspeech",
        "ma19b_interspeech",
        "ooster19_interspeech",
        "eshky19_interspeech",
        "pan19_interspeech",
        "nallanthighal19_interspeech",
        "biadsy19_interspeech",
        "liu19j_interspeech",
        "vougioukas19_interspeech",
        "liu19k_interspeech",
        "shillingford19_interspeech"
      ]
    },
    {
      "title": "Turn Management in Dialogue",
      "papers": [
        "razavi19_interspeech",
        "bechet19_interspeech",
        "liu19l_interspeech",
        "coman19_interspeech",
        "su19b_interspeech",
        "heldner19_interspeech",
        "hara19_interspeech",
        "lala19_interspeech",
        "horiguchi19_interspeech",
        "su19c_interspeech"
      ]
    },
    {
      "title": "Corpus Annotation and Evaluation",
      "papers": [
        "tran19_interspeech",
        "pasad19_interspeech",
        "wang19p_interspeech",
        "piunova19_interspeech",
        "segal19_interspeech",
        "oktem19_interspeech",
        "tannander19_interspeech",
        "chen19o_interspeech",
        "bach19_interspeech",
        "singh19b_interspeech",
        "maekaku19_interspeech"
      ]
    },
    {
      "title": "Speech Enhancement: Multi-Channel and Intelligibility",
      "papers": [
        "lee19e_interspeech",
        "fazel19_interspeech",
        "zhang19o_interspeech",
        "srensen19_interspeech",
        "mamun19b_interspeech",
        "srensen19b_interspeech",
        "arai19_interspeech",
        "bu19_interspeech",
        "lee19f_interspeech",
        "gu19b_interspeech",
        "afouras19_interspeech"
      ]
    },
    {
      "title": "Speaker Recognition 3",
      "papers": [
        "fujita19_interspeech",
        "india19_interspeech",
        "vinals19b_interspeech",
        "tu19_interspeech",
        "liu19m_interspeech",
        "nandwana19c_interspeech",
        "novotny19_interspeech",
        "salvati19_interspeech",
        "naini19_interspeech",
        "zhu19b_interspeech",
        "ferrer19_interspeech",
        "lee19g_interspeech",
        "zheng19c_interspeech",
        "cai19d_interspeech",
        "cai19e_interspeech"
      ]
    },
    {
      "title": "NN Architectures for ASR",
      "papers": [
        "wiesner19_interspeech",
        "kim19b_interspeech",
        "chorowski19_interspeech",
        "fan19b_interspeech",
        "tian19b_interspeech",
        "li19u_interspeech",
        "bang19_interspeech",
        "moriya19_interspeech",
        "parcollet19b_interspeech",
        "yi19b_interspeech",
        "denisov19_interspeech"
      ]
    },
    {
      "title": "Speech Synthesis: Text Processing, Prosody, and Emotion",
      "papers": [
        "hayashi19_interspeech",
        "szekely19b_interspeech",
        "klimkov19_interspeech",
        "hussain19_interspeech",
        "koriyama19_interspeech",
        "nikulasdottir19_interspeech",
        "guo19e_interspeech",
        "ni19_interspeech",
        "aubin19_interspeech",
        "tits19_interspeech",
        "yang19h_interspeech",
        "pan19b_interspeech",
        "gokcen19_interspeech",
        "li19v_interspeech",
        "shankar19c_interspeech"
      ]
    },
    {
      "title": "Speech and Voice Disorders 2",
      "papers": [
        "connaghan19_interspeech",
        "rowe19_interspeech",
        "mayle19_interspeech",
        "sudro19_interspeech",
        "eshghi19_interspeech",
        "wang19q_interspeech",
        "chen19p_interspeech",
        "hermes19_interspeech",
        "kalita19_interspeech",
        "serrano19_interspeech",
        "dubey19b_interspeech",
        "niu19_interspeech",
        "bn19_interspeech"
      ]
    },
    {
      "title": "Speech and Audio Source Separation and Scene Analysis 3",
      "papers": [
        "wang19r_interspeech",
        "bahmaninezhad19_interspeech",
        "inan19_interspeech",
        "ditter19_interspeech",
        "zegers19_interspeech",
        "bear19_interspeech",
        "fan19c_interspeech",
        "yousefi19_interspeech",
        "shi19e_interspeech",
        "shi19f_interspeech",
        "lluis19_interspeech"
      ]
    },
    {
      "title": "Speech-to-Text and Speech Assessment",
      "papers": [
        "foley19_interspeech",
        "gruber19b_interspeech",
        "huang19k_interspeech",
        "dikici19_interspeech",
        "goel19b_interspeech"
      ]
    }
  ],
  "doi": "10.21437/Interspeech.2019"
}