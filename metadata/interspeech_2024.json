{
 "series": "Interspeech",
 "title": "Interspeech 2024",
 "location": "Kos, Greece",
 "startDate": "01/09/2024",
 "endDate": "05/09/2024",
 "URL": "https://interspeech2024.org/",
 "chair": "Chairs: Itshak Lapidot, Sharon Gannot",
 "conf": "Interspeech",
 "name": "interspeech_2024",
 "year": "2024",
 "SIG": "",
 "title1": "Interspeech 2024",
 "booklet": "intro.pdf",
 "date": "1-5 September 2024",
 "month": 9,
 "day": 1,
 "now": 1725173628965026,
 "papers": {
  "wang24_interspeech": {
   "authors": [
    [
     "Ruizhe",
     "Wang"
    ]
   ],
   "title": "SA-MF: A Novel Self-Attention Mechanism for Multifeature Fusion in Speech Enhancement Networks",
   "original": "4",
   "order": 689,
   "page_count": 5,
   "abstract": [
    "Multichannel speech enhancement in complex noise environments remains a formidable challenge due to the necessity of adapting to multiple noise types, addressing nonlinear distortions, and other influencing factors. Current methods using utilizing time and frequency domains often result in inaccurate predictions or distortions. In this paper, we propose a self-attentive multi-feature fusion network to enhance the quality of multichannel speech in noisy environments. The method simultaneously extracts spatial and spectral features, performing feature fusion through the self-attention mechanism. Experimental results demonstrate that, compared with existing methods, our proposed approach achieves significant improvements, effectively enhancing speech quality in noisy environments. SDR show improvements of 0.4, respectively, when compared with other state-of-the-art multi-channel enhancement models."
   ],
   "p1": 3365,
   "pn": 3369,
   "doi": "10.21437/Interspeech.2024-4",
   "url": "interspeech_2024/wang24_interspeech.html"
  },
  "ai24_interspeech": {
   "authors": [
    [
     "Zhiqi",
     "Ai"
    ],
    [
     "Zhiyong",
     "Chen"
    ],
    [
     "Shugong",
     "Xu"
    ]
   ],
   "title": "MM-KWS: Multi-modal Prompts for Multilingual User-defined Keyword Spotting",
   "original": "10",
   "order": 498,
   "page_count": 5,
   "abstract": [
    "In this paper, we propose MM-KWS, a novel approach to user-defined keyword spotting leveraging multi-modal enrollments of text and speech templates. Unlike previous methods that focus solely on either text or speech features, MM-KWS extracts phoneme, text, and speech embeddings from both modalities. These embeddings are then compared with the query speech embedding to detect the target keywords. To ensure the applicability of MM-KWS across diverse languages, we utilize a feature extractor incorporating several multilingual pre-trained models. Subsequently, we validate its effectiveness on Mandarin and English tasks. In addition, we have integrated advanced data augmentation tools for hard case mining to enhance MM-KWS in distinguishing confusable words. Experimental results on the LibriPhrase and WenetPhrase datasets demonstrate that MM-KWS outperforms prior methods significantly."
   ],
   "p1": 2415,
   "pn": 2419,
   "doi": "10.21437/Interspeech.2024-10",
   "url": "interspeech_2024/ai24_interspeech.html"
  },
  "lu24_interspeech": {
   "authors": [
    [
     "Ye-Xin",
     "Lu"
    ],
    [
     "Yang",
     "Ai"
    ],
    [
     "Zheng-Yan",
     "Sheng"
    ],
    [
     "Zhen-Hua",
     "Ling"
    ]
   ],
   "title": "MultiStage Speech Bandwidth Extension with Flexible Sampling Rate Control",
   "original": "11",
   "order": 469,
   "page_count": 5,
   "abstract": [
    "The majority of existing speech bandwidth extension (BWE) methods operate under the constraint of fixed source and target sampling rates, which limits their flexibility in practical applications. In this paper, we propose a multi-stage speech BWE model named MS-BWE, which can handle a set of source and target sampling rate pairs and achieve flexible extensions of frequency bandwidth. The proposed MS-BWE model comprises a cascade of BWE blocks, with each block featuring a dual-stream architecture to realize amplitude and phase extension, progressively painting the speech frequency bands stage by stage. The teacher-forcing strategy is employed to mitigate the discrepancy between training and inference. Experimental results demonstrate that our proposed MS-BWE is comparable to state-of-the-art speech BWE methods in speech quality. Regarding generation efficiency, the one-stage generation of MS-BWE can achieve over one thousand times real-time on GPU and about sixty times on CPU."
   ],
   "p1": 2270,
   "pn": 2274,
   "doi": "10.21437/Interspeech.2024-11",
   "url": "interspeech_2024/lu24_interspeech.html"
  },
  "simantiraki24_interspeech": {
   "authors": [
    [
     "Olympia",
     "Simantiraki"
    ],
    [
     "Martin",
     "Cooke"
    ]
   ],
   "title": "Listeners' F0 preferences in quiet and stationary noise",
   "original": "14",
   "order": 869,
   "page_count": 5,
   "abstract": [
    "Talkers - and increasingly speech output technology - typically alter speech characteristics when faced with challenging communicative conditions, but the impact of these changes on interlocutors is not fully understood. One such characteristic is fundamental frequency (F0), whose mean and range tend to increase when talking in noise or when communicating with inexperienced listeners. However, speech perception experiments have yet to demonstrate any intelligibility advantage for F0 modifications. The current study asked listeners to alter mean F0 or F0 variation with real-time feedback, in order to maximise comprehensibility in quiet and noise. Listeners chose a lower mean F0 than that of the original, and F0 variation similar to the original. Masker level had no effect on preference, suggesting that while listeners express clear choices, adjustment of F0 has no impact on intelligibility, and may instead reflect considerations such as naturalness or listening effort. "
   ],
   "p1": 4224,
   "pn": 4228,
   "doi": "10.21437/Interspeech.2024-14",
   "url": "interspeech_2024/simantiraki24_interspeech.html"
  },
  "williams24_interspeech": {
   "authors": [
    [
     "Jennifer",
     "Williams"
    ],
    [
     "Eike",
     "Schneiders"
    ],
    [
     "Henry",
     "Card"
    ],
    [
     "Tina",
     "Seabrooke"
    ],
    [
     "Beatrice",
     "Pakenham-Walsh"
    ],
    [
     "Tayyaba",
     "Azim"
    ],
    [
     "Lucy",
     "Valls-Reed"
    ],
    [
     "Ganesh",
     "Vigneswaran"
    ],
    [
     "John Robert",
     "Bautista"
    ],
    [
     "Rohan",
     "Chandra"
    ],
    [
     "Arya",
     "Farahi"
    ]
   ],
   "title": "Predicting Acute Pain Levels Implicitly from Vocal Features",
   "original": "15",
   "order": 301,
   "page_count": 5,
   "abstract": [
    "Evaluating pain in speech represents a critical challenge in high-stakes clinical scenarios, from analgesia delivery to emergency triage. Clinicians have predominantly relied on direct verbal communication of pain which is difficult for patients with communication barriers, such as those affected by stroke, autism, and learning difficulties. Many previous efforts have focused on multimodal data which does not suit all clinical applications. Our work is the first to collect a new English speech dataset wherein we have induced acute pain in adults using a cold pressor task protocol and recorded subjects reading sentences out loud. We report pain discrimination performance as F1 scores from binary (pain vs. no pain) and three-class (mild, moderate, severe) prediction tasks, and support our results with explainable feature analysis. Our work is a step towards providing medical decision support for pain evaluation from speech to improve care across diverse and remote healthcare settings."
   ],
   "p1": 1460,
   "pn": 1464,
   "doi": "10.21437/Interspeech.2024-15",
   "url": "interspeech_2024/williams24_interspeech.html"
  },
  "yang24_interspeech": {
   "authors": [
    [
     "Xingxing",
     "Yang"
    ]
   ],
   "title": "G2PA: G2P with Aligned Audio for Mandarin Chinese",
   "original": "19",
   "resource": "https://doi.org/10.5281/zenodo.12734573",
   "order": 576,
   "page_count": 5,
   "abstract": [
    "During the training of a Mandarin Chinese Text-To-Speech (TTS) system, it is necessary to preprocess the training speech data, which mainly consists of audio and text pairs. One crucial preprocessing step involves converting Chinese graphemes to phonemes (G2P) to obtain phoneme representations when using phonemes as input. However, relying solely on the text for G2P conversion may lead to inaccurate results due to the pronunciation ambiguity of polyphones - characters with multiple pronunciations. Although previous research has attempted to address this issue, most approaches solely rely on text-based methods, disregarding the valuable audio information that can only be captured from the raw audio. To overcome this limitation, we  propose a G2P pipeline that leverages both audio and text inputs to resolve pronunciation ambiguity. The code and model weights for our approach are publicly available at the following GitHub repository: https://github.com/iooops/G2PA."
   ],
   "p1": 2800,
   "pn": 2804,
   "doi": "10.21437/Interspeech.2024-19",
   "url": "interspeech_2024/yang24_interspeech.html"
  },
  "chen24_interspeech": {
   "authors": [
    [
     "Sizhou",
     "Chen"
    ],
    [
     "Yibo",
     "Bai"
    ],
    [
     "Jiadi",
     "Yao"
    ],
    [
     "Xiao-Lei",
     "Zhang"
    ],
    [
     "Xuelong",
     "Li"
    ]
   ],
   "title": "Textual-Driven Adversarial Purification for Speaker Verification",
   "original": "21",
   "order": 107,
   "page_count": 5,
   "abstract": [
    "Adversarial attacks introduce subtle perturbations to audio signals for causing automatic speaker verification (ASV) systems to make mistakes. To address this challenge, adversarial purification techniques have emerged, where diffusion models have been proven effective. However, the latest development with the diffusion models caused a negative effect that the audio generation quality is not high enough. Moreover, these approaches tend to focus solely on audio features, while often neglecting textual information. To overcome these limitations, we propose a textual-driven adversarial purification (TDAP) framework, which integrates diffusion models with pretrained large audio language models for comprehensive defense. TDAP employs textual data extracted from audio to guide the diffusion-based purification process. Extensive experimental results show that TDAP significantly enhances the defense robustness against adversarial attacks."
   ],
   "p1": 527,
   "pn": 531,
   "doi": "10.21437/Interspeech.2024-21",
   "url": "interspeech_2024/chen24_interspeech.html"
  },
  "doan24_interspeech": {
   "authors": [
    [
     "Thien-Phuc",
     "Doan"
    ],
    [
     "Long",
     "Nguyen-Vu"
    ],
    [
     "Kihun",
     "Hong"
    ],
    [
     "Souhwan",
     "Jung"
    ]
   ],
   "title": "Balance, Multiple Augmentation, and Re-synthesis: A Triad Training Strategy for Enhanced Audio Deepfake Detection",
   "original": "24",
   "resource": "https://doi.org/10.5281/zenodo.12734217",
   "order": 436,
   "page_count": 5,
   "abstract": [
    "The detection of deepfake voices has become increasingly challenging. Finding the boundary that separates real and synthetic voices requires a good training set and an effective strategy. In this study, we introduce a novel training strategy designed to improve detection performance by actively assembling training mini-batches within the framework of Supervised Contrastive Learning. We argue that enhancing model robustness is achievable through balancing samples between classes, applying multiple speech augmentation methods, and re-synthesized samples training. By carefully fine-tuning the mini-batch settings, we have surpassed the performance and model generalization of existing methods on various audio deepfake benchmark sets. These include the ASVspoof DF evaluation and in-the-wild benchmarking sets, where we achieved Equal Error Rates of 2.17% and 4.51%, respectively. The experiment of this work is available on Github."
   ],
   "p1": 2105,
   "pn": 2109,
   "doi": "10.21437/Interspeech.2024-24",
   "url": "interspeech_2024/doan24_interspeech.html"
  },
  "li24_interspeech": {
   "authors": [
    [
     "Mohan",
     "Li"
    ],
    [
     "Simon",
     "Keizer"
    ],
    [
     "Rama",
     "Doddipatla"
    ]
   ],
   "title": "Prompting Whisper for QA-driven Zero-shot End-to-end Spoken Language Understanding",
   "original": "26",
   "order": 275,
   "page_count": 5,
   "abstract": [
    "Zero-shot spoken language understanding (SLU) enables systems to comprehend user utterances in new domains without prior exposure to training data. Recent studies often rely on large language models (LLMs), leading to excessive footprints and complexity. This paper proposes the use of Whisper, a standalone speech processing model, for zero-shot end-to-end (E2E) SLU. To handle unseen semantic labels, SLU tasks are integrated into a question-answering (QA) framework, which prompts the Whisper decoder for semantics deduction. The system is efficiently trained with prefix-tuning, optimising a minimal set of parameters rather than the entire Whisper model. We show that the proposed system achieves a 40.7% absolute gain for slot filling (SLU-F1) on SLURP compared to a recently introduced zero-shot benchmark. Furthermore, it performs comparably to a Whisper-GPT-2 modular system under both in-corpus and cross-corpus evaluation settings, but with a relative 34.8% reduction in model parameters."
   ],
   "p1": 1330,
   "pn": 1334,
   "doi": "10.21437/Interspeech.2024-26",
   "url": "interspeech_2024/li24_interspeech.html"
  },
  "niizumi24_interspeech": {
   "authors": [
    [
     "Daisuke",
     "Niizumi"
    ],
    [
     "Daiki",
     "Takeuchi"
    ],
    [
     "Yasunori",
     "Ohishi"
    ],
    [
     "Noboru",
     "Harada"
    ],
    [
     "Masahiro",
     "Yasuda"
    ],
    [
     "Shunsuke",
     "Tsubaki"
    ],
    [
     "Keisuke",
     "Imoto"
    ]
   ],
   "title": "M2D-CLAP: Masked Modeling Duo Meets CLAP for Learning General-purpose Audio-Language Representation",
   "original": "29",
   "order": 13,
   "page_count": 5,
   "abstract": [
    "Contrastive language-audio pre-training (CLAP) enables zero-shot (ZS) inference of audio and exhibits promising performance in several classification tasks. However, conventional audio representations are still crucial for many tasks where ZS is not applicable (e.g., regression problems). Here, we explore a new representation, a general-purpose audio-language representation, that performs well in both ZS and transfer learning. To do so, we propose a new method, M2D-CLAP, which combines self-supervised learning Masked Modeling Duo (M2D) and CLAP. M2D learns an effective representation to model audio signals, and CLAP aligns the representation with text embedding. As a result, M2D-CLAP learns a versatile representation that allows for both ZS and transfer learning. Experiments show that M2D-CLAP performs well on linear evaluation, fine-tuning, and ZS classification with a GTZAN state-of-the-art of 75.17%, thus achieving a general-purpose audio-language representation."
   ],
   "p1": 57,
   "pn": 61,
   "doi": "10.21437/Interspeech.2024-29",
   "url": "interspeech_2024/niizumi24_interspeech.html"
  },
  "muller24_interspeech": {
   "authors": [
    [
     "Nicolas M.",
     "Müller"
    ],
    [
     "Piotr",
     "Kawa"
    ],
    [
     "Shen",
     "Hu"
    ],
    [
     "Matthias",
     "Neu"
    ],
    [
     "Jennifer",
     "Williams"
    ],
    [
     "Philip",
     "Sperl"
    ],
    [
     "Konstantin",
     "Böttinger"
    ]
   ],
   "title": "A New Approach to Voice Authenticity",
   "original": "31",
   "order": 464,
   "page_count": 5,
   "abstract": [
    "Voice faking poses significant societal challenges. Currently, the prevailing assumption is that unaltered human speech can always be considered genuine, while fake speech usually comes from text-to-speech (TTS) synthesis. We argue that this type of binary distinction is oversimplified. For instance, altered playback speeds can maliciously deceive listeners, as in the `Drunken Nancy Pelosi' incident. Similarly, editing of audio clips can be done ethically, e.g. for brevity or summarization in news reporting or podcasts, but editing can also create misleading narratives. In this paper, we propose a conceptual shift away from the longstanding binary paradigm of speech audio being either `fake' or `real'. Instead, we focus on pinpointing `voice edits', which encompass traditional modifications like filters and cuts, as well as neural synthesis. We delineate six categories of voice edits and curate a new challenge dataset, for which we present baseline voice edit detection systems."
   ],
   "p1": 2245,
   "pn": 2249,
   "doi": "10.21437/Interspeech.2024-31",
   "url": "interspeech_2024/muller24_interspeech.html"
  },
  "kuhn24_interspeech": {
   "authors": [
    [
     "Korbinian",
     "Kuhn"
    ],
    [
     "Verena",
     "Kersken"
    ],
    [
     "Gottfried",
     "Zimmermann"
    ]
   ],
   "title": "Beyond Levenshtein: Leveraging Multiple Algorithms for Robust Word Error Rate Computations And Granular Error Classifications",
   "original": "32",
   "resource": "https://doi.org/10.5281/zenodo.12748507",
   "order": 933,
   "page_count": 5,
   "abstract": [
    "The Word Error Rate (WER) is the common measure of accuracy for Automatic Speech Recognition (ASR). Transcripts are usually pre-processed by substituting specific characters to account for non-semantic differences. As a result of this normalisation, information on the accuracy of punctuation or capitalisation is lost. We present a non-destructive, token-based approach using an extended Levenshtein distance algorithm to compute a robust WER and additional orthographic metrics. Transcription errors are also classified more granularly by existing string similarity and phonetic algorithms. An evaluation on several datasets demonstrates the practical equivalence of our approach compared to common WER computations. We also provide an exemplary analysis of derived use cases, such as a punctuation error rate, and a web application for interactive use and visualisation of our implementation. The code is available open-source."
   ],
   "p1": 4543,
   "pn": 4547,
   "doi": "10.21437/Interspeech.2024-32",
   "url": "interspeech_2024/kuhn24_interspeech.html"
  },
  "shi24_interspeech": {
   "authors": [
    [
     "Jiatong",
     "Shi"
    ],
    [
     "Yueqian",
     "Lin"
    ],
    [
     "Xinyi",
     "Bai"
    ],
    [
     "Keyi",
     "Zhang"
    ],
    [
     "Yuning",
     "Wu"
    ],
    [
     "Yuxun",
     "Tang"
    ],
    [
     "Yifeng",
     "Yu"
    ],
    [
     "Qin",
     "Jin"
    ],
    [
     "Shinji",
     "Watanabe"
    ]
   ],
   "title": "Singing Voice Data Scaling-up: An Introduction to ACE-Opencpop and ACE-KiSing",
   "original": "33",
   "order": 385,
   "page_count": 5,
   "abstract": [
    "In singing voice synthesis (SVS), generating singing voices from musical scores faces challenges due to limited data availability. This study proposes a unique strategy to address the data scarcity in SVS. We employ an existing singing voice synthesizer for data augmentation, complemented by detailed manual tuning, an approach not previously explored in data curation, to reduce instances of unnatural voice synthesis. This innovative method has led to the creation of two expansive singing voice datasets, ACE-Opencpop and ACE-KiSing, which are instrumental for large-scale, multi-singer voice synthesis. Through thorough experimentation, we establish that these datasets not only serve as new benchmarks for SVS but also enhance SVS performance on other singing voice datasets when used as supplementary resources. The corpora, pre-trained models, and their related training recipes are publicly available at ESPnet-Muskits (https://github.com/espnet/espnet)."
   ],
   "p1": 1880,
   "pn": 1884,
   "doi": "10.21437/Interspeech.2024-33",
   "url": "interspeech_2024/shi24_interspeech.html"
  },
  "cappellazzo24_interspeech": {
   "authors": [
    [
     "Umberto",
     "Cappellazzo"
    ],
    [
     "Daniele",
     "Falavigna"
    ],
    [
     "Alessio",
     "Brutti"
    ]
   ],
   "title": "Efficient Fine-tuning of Audio Spectrogram Transformers via Soft Mixture of Adapters",
   "original": "38",
   "order": 679,
   "page_count": 5,
   "abstract": [
    "Mixture of Experts (MoE) architectures have recently started burgeoning due to their ability to scale model’s capacity while maintaining the computational cost affordable, leading to state-of-the-art results in numerous fields. While MoE has been mostly investigated for the pre-training stage, its use in parameter-efficient transfer learning (PETL) settings is underexplored. To narrow this gap, this paper attempts to demystify the use of MoE for PETL of Audio Spectrogram Transformers to audio and speech downstream tasks. Specifically, we propose Soft Mixture of Adapters (Soft-MoA). It exploits adapters as the experts and, leveraging the recent Soft MoE method, it relies on a soft assignment between the input tokens and experts to keep the computational time limited. Extensive experiments across 4 benchmarks demonstrate that Soft-MoA outperforms the single adapter method and performs on par with the dense MoA counterpart. We finally present ablation studies on key elements of Soft-MoA. Our code is available at https://github.com/umbertocappellazzo/PETL_AST."
   ],
   "p1": 3315,
   "pn": 3319,
   "doi": "10.21437/Interspeech.2024-38",
   "url": "interspeech_2024/cappellazzo24_interspeech.html"
  },
  "parcollet24_interspeech": {
   "authors": [
    [
     "Titouan",
     "Parcollet"
    ],
    [
     "Rogier",
     "van Dalen"
    ],
    [
     "Shucong",
     "Zhang"
    ],
    [
     "Sourav",
     "Bhattacharya"
    ]
   ],
   "title": "SummaryMixing: A Linear-Complexity Alternative to Self-Attention for Speech Recognition and Understanding",
   "original": "40",
   "order": 708,
   "page_count": 5,
   "abstract": [
    "Modern speech processing systems rely on self-attention. Unfortunately, token mixing with self-attention takes quadratic time in the length of the speech utterance, slowing down inference and training and increasing memory consumption. Cheaper alternatives to self-attention for ASR have been developed, but they fail to consistently reach the same level of accuracy. This paper, therefore, proposes a novel linear-time alternative to self-attention. It summarises an utterance with the mean over vectors for all time steps. This single summary is then combined with time-specific information. We call this method \"SummaryMixing\". Introducing SummaryMixing in state-of-the-art ASR models makes it feasible to preserve or exceed previous speech recognition performance while making training and inference up to 28 % faster and reducing memory use by half."
   ],
   "p1": 3460,
   "pn": 3464,
   "doi": "10.21437/Interspeech.2024-40",
   "url": "interspeech_2024/parcollet24_interspeech.html"
  },
  "deshmukh24_interspeech": {
   "authors": [
    [
     "Soham",
     "Deshmukh"
    ],
    [
     "Rita",
     "Singh"
    ],
    [
     "Bhiksha",
     "Raj"
    ]
   ],
   "title": "Domain Adaptation for Contrastive Audio-Language Models",
   "original": "41",
   "order": 345,
   "page_count": 5,
   "abstract": [
    "Audio-Language Models (ALM) aim to be general-purpose audio models by providing zero-shot capabilities at test time. The zero-shot performance of ALM improves by using suitable text prompts for each domain. The text prompts are usually hand-crafted through an ad-hoc process and lead to a drop in ALM generalization and out-of-distribution performance. Existing approaches to improve domain performance, like few-shot learning or fine-tuning, require access to annotated data and iterations of training. Therefore, we propose a test-time domain adaptation method for ALMs that does not require access to annotations. Our method learns a domain vector by enforcing consistency across augmented views of the testing audio. We extensively evaluate our approach on 12 downstream tasks across domains. With just one example, our domain adaptation method leads to 3.2% (max 8.4%) average zero-shot performance improvement. After adaptation, the model still retains the generalization property of ALMs."
   ],
   "p1": 1680,
   "pn": 1684,
   "doi": "10.21437/Interspeech.2024-41",
   "url": "interspeech_2024/deshmukh24_interspeech.html"
  },
  "pesan24_interspeech": {
   "authors": [
    [
     "Jan",
     "Pešán"
    ],
    [
     "Vojtěch",
     "Juřík"
    ],
    [
     "Martin",
     "Karafiát"
    ],
    [
     "Jan",
     "Černocký"
    ]
   ],
   "title": "BESST Dataset: A Multimodal Resource for Speech-based Stress Detection and Analysis",
   "original": "42",
   "order": 280,
   "page_count": 5,
   "abstract": [
    "The Brno Extended Stress and Speech Test (BESST) dataset is a new resource for the speech research community, offering multimodal audiovisual, physiological and psychological data that enable investigations into the interplay between stress and speech. In this paper, we introduce the BESST dataset and provide a details of its design, collection protocols, and technical aspects. The dataset comprises speech samples, physiological signals (including electrocardiogram, electrodermal activity, skin temperature, and acceleration data), and video recordings from 90 subjects performing stress-inducing tasks. It comprises 16.9 hours of clean Czech speech data, averaging 15 minutes of clean speech per participant. The data collection procedure involves the induction of cognitive and physical stress induced by Reading Span task (RSPAN) and Hand Immersion (HIT) task respectively. The BESST dataset was collected under stringent ethical standards and is accessible for research and development."
   ],
   "p1": 1355,
   "pn": 1359,
   "doi": "10.21437/Interspeech.2024-42",
   "url": "interspeech_2024/pesan24_interspeech.html"
  },
  "bentum24_interspeech": {
   "authors": [
    [
     "Martijn",
     "Bentum"
    ],
    [
     "Louis ten",
     "Bosch"
    ],
    [
     "Tom",
     "Lentz"
    ]
   ],
   "title": "The Processing of Stress in End-to-End Automatic Speech Recognition Models",
   "original": "44",
   "order": 485,
   "page_count": 5,
   "abstract": [
    "Listeners use stress to facilitate word recognition and speech segmentation. Classical ASR systems did not incorporate stress in their recognition process. In contrast, end-to-end ASR systems may use the information carried by stress. The present study shows that Wav2vec 2.0 is indeed sensitive to stress, and that this sensitivity is not a mere reflection of acoustic correlates of stress. Diagnostic classifiers of the CNN output reveal vowel-specific stress representations, that perform on par with acoustic features.  Stress classifiers trained on transformer layers outperform classifiers based on acoustic correlates, but degrade when context is removed, showing that higher layers take the relative nature of stress into account. Results obtained by testing a stress classifier on a vowel it is not trained on, show that stress processing is to some extent abstract, i.e., the classifier does not simply detect a set of stressed vowel representations but rather, their common denominator."
   ],
   "p1": 2350,
   "pn": 2354,
   "doi": "10.21437/Interspeech.2024-44",
   "url": "interspeech_2024/bentum24_interspeech.html"
  },
  "huo24_interspeech": {
   "authors": [
    [
     "Zhouyuan",
     "Huo"
    ],
    [
     "Dongseong",
     "Hwang"
    ],
    [
     "Gan",
     "Song"
    ],
    [
     "Khe Chai",
     "Sim"
    ],
    [
     "Weiran",
     "Wang"
    ]
   ],
   "title": "AdaRA: Adaptive Rank Allocation of Residual Adapters for Speech Foundation Model",
   "original": "45",
   "order": 490,
   "page_count": 5,
   "abstract": [
    "A recent paradigm shift in artificial intelligence has witnessed the emergence of foundation models. These foundation models, possessing billions of parameters and trained on extensive datasets, are anticipated to demonstrate superior generalization across diverse downstream tasks. Residual Adapters represent a broadly employed methodology for efficient adaptation, achieved by updating a limited set of additive parameters while maintaining a fixed bottleneck dimension. However, when the parameter budget is constrained, allocating additive parameters uniformly across layers proves sub-optimal. In this paper, we propose a novel adaptive efficient adaptation method that automatically determines the optimal number of bottleneck dimensions for Residual Adapters at different layers. Experimental results confirm that the proposed method effectively learns an optimal additive parameter allocation, surpassing the performance of comparable methods in speech recognition domain adaptation."
   ],
   "p1": 2375,
   "pn": 2379,
   "doi": "10.21437/Interspeech.2024-45",
   "url": "interspeech_2024/huo24_interspeech.html"
  },
  "niu24_interspeech": {
   "authors": [
    [
     "Xinlei",
     "Niu"
    ],
    [
     "Jing",
     "Zhang"
    ],
    [
     "Charles Patrick",
     "Martin"
    ]
   ],
   "title": "HybridVC: Efficient Voice Style Conversion with Text and Audio Prompts",
   "original": "46",
   "order": 898,
   "page_count": 5,
   "abstract": [
    "We introduce HybridVC, a voice conversion (VC) framework built upon a pre-trained conditional variational autoencoder (CVAE) that combines the strengths of a latent model with contrastive learning. HybridVC supports text and audio prompts, enabling more flexible voice style conversion. HybridVC models a latent distribution conditioned on speaker embeddings acquired by a pretrained speaker encoder and optimises style text embeddings to align with the speaker style information through contrastive learning in parallel. Therefore, HybridVC can be efficiently trained under limited computational resources. Our experiments demonstrate HybridVC's superior training efficiency and its capability for advanced multi-modal voice style conversion. This underscores its potential for widespread applications such as user-defined personalised voice in various social media platforms. A comprehensive ablation study further validates the effectiveness of our method."
   ],
   "p1": 4368,
   "pn": 4372,
   "doi": "10.21437/Interspeech.2024-46",
   "url": "interspeech_2024/niu24_interspeech.html"
  },
  "rybakov24_interspeech": {
   "authors": [
    [
     "Oleg",
     "Rybakov"
    ],
    [
     "Dmitriy",
     "Serdyuk"
    ],
    [
     "Chengjian",
     "Zheng"
    ]
   ],
   "title": "USM RNN-T model weights binarization",
   "original": "47",
   "order": 926,
   "page_count": 5,
   "abstract": [
    "Large-scale universal speech models (USM) are already used in production. However, as the model size grows, the serving cost grows too. Serving cost of large models is dominated by model size that is why model size reduction is an important research topic. In this work we are focused on model size reduction using weights only quantization. We present the weights binarization of USM Recurrent Neural Network Transducer (RNN-T) and show that its model size can be reduced by 15.9x times at cost of word error rate (WER) increase by only 1.9% in comparison to the float32 model. It makes it attractive for practical applications."
   ],
   "p1": 4508,
   "pn": 4512,
   "doi": "10.21437/Interspeech.2024-47",
   "url": "interspeech_2024/rybakov24_interspeech.html"
  },
  "hwang24_interspeech": {
   "authors": [
    [
     "Ji-Sang",
     "Hwang"
    ],
    [
     "Hyeongrae",
     "Noh"
    ],
    [
     "Yoonseok",
     "Hong"
    ],
    [
     "Insoo",
     "Oh"
    ]
   ],
   "title": "X-Singer: Code-Mixed Singing Voice Synthesis via Cross-Lingual Learning",
   "original": "49",
   "order": 386,
   "page_count": 5,
   "abstract": [
    "Singing voice synthesis (SVS) systems have exhibited a remarkable ability to synthesize natural singing voices. However, existing methods still depend on the phoneme annotation in a musical score (MS) and are limited in their ability to generate a code-mixed singing voice. Therefore, we propose X-Singer, a code-mixed SVS system that uses cross-lingual language learning. First, we introduce a MS encoder to handle a realistic MS comprising code-mixed lyrics without phoneme annotation. The MS encoder adopts language code-switching to encode code-mixed lyrics, and mixture alignment to reduce dependency on the phoneme annotation. Furthermore, we use a conditional flow matching-based decoder to achieve high-quality SVS in a few sampling steps. We observe that X-Singer outperformed the baseline models in terms of naturalness for intra- and cross-lingual SVS. Moreover, the proposed model can synthesize code-mixed SVS through cross-lingual learning using a mixture of monolingual SVS datasets."
   ],
   "p1": 1885,
   "pn": 1889,
   "doi": "10.21437/Interspeech.2024-49",
   "url": "interspeech_2024/hwang24_interspeech.html"
  },
  "gao24_interspeech": {
   "authors": [
    [
     "Yingying",
     "Gao"
    ],
    [
     "Shilei",
     "Zhang"
    ],
    [
     "Chao",
     "Deng"
    ],
    [
     "Junlan",
     "Feng"
    ]
   ],
   "title": "GenDistiller: Distilling Pre-trained Language Models based on an Autoregressive Generative Model",
   "original": "51",
   "order": 681,
   "page_count": 5,
   "abstract": [
    "Pre-trained speech language models such as HuBERT and WavLM leverage unlabeled speech data for self-supervised learning and offer powerful representations for numerous downstream tasks. Despite the success of these models, their high requirements for memory and computing resource hinder their application on resource restricted devices. Therefore, this paper introduces GenDistiller, a novel knowledge distillation framework which generates the hidden representations of the pretrained teacher model directly by a much smaller student network. The proposed method takes the previous hidden layer as history and implements a layer-by-layer prediction of the teacher model autoregressively. Experiments on SUPERB reveal the advantage of GenDistiller over the baseline distilling method without an autoregressive framework, with 33% fewer parameters, similar time consumption and better performance on most of the SUPERB tasks. Ultimately, the proposed GenDistiller reduces the size of WavLM by 82%."
   ],
   "p1": 3325,
   "pn": 3329,
   "doi": "10.21437/Interspeech.2024-51",
   "url": "interspeech_2024/gao24_interspeech.html"
  },
  "cao24_interspeech": {
   "authors": [
    [
     "Rui",
     "Cao"
    ],
    [
     "Tianrui",
     "Wang"
    ],
    [
     "Meng",
     "Ge"
    ],
    [
     "Andong",
     "Li"
    ],
    [
     "Longbiao",
     "Wang"
    ],
    [
     "Jianwu",
     "Dang"
    ],
    [
     "Yungang",
     "Jia"
    ]
   ],
   "title": "VoiCor: A Residual Iterative Voice Correction Framework for Monaural Speech Enhancement",
   "original": "53",
   "order": 996,
   "page_count": 5,
   "abstract": [
    "Supervised speech enhancement has gained significant progress from recent advancements in neural networks, especially due to their ability to non-linearly fit the diverse representations of target speech, such as waveform or spectrum. However, most of them continue to face issues with degraded speech and residual noise in challenging acoustic scenarios. In this paper, to alleviate the above issues, we propose a residual iterative voice correction framework to further correct the acoustic structure of the speech pre-enhanced by existing enhancement solutions. Concretely, inspired by the chain rule for information, we perform multiple corrections in a residual iterative manner to refine the spectral structure details of pre-enhanced speech. Experimental results on the DNS-Challenge dataset show that our solution consistently improves 0.3+ PESQ score over baselines, with only an additional 1.18 M parameters."
   ],
   "p1": 4858,
   "pn": 4862,
   "doi": "10.21437/Interspeech.2024-53",
   "url": "interspeech_2024/cao24_interspeech.html"
  },
  "kim24_interspeech": {
   "authors": [
    [
     "Heeseung",
     "Kim"
    ],
    [
     "Sang-gil",
     "Lee"
    ],
    [
     "Jiheum",
     "Yeom"
    ],
    [
     "Che Hyun",
     "Lee"
    ],
    [
     "Sungwon",
     "Kim"
    ],
    [
     "Sungroh",
     "Yoon"
    ]
   ],
   "title": "VoiceTailor: Lightweight Plug-In Adapter for Diffusion-Based Personalized Text-to-Speech",
   "original": "63",
   "order": 907,
   "page_count": 5,
   "abstract": [
    "We propose VoiceTailor, a parameter-efficient speaker-adaptive text-to-speech (TTS) system, by equipping a pre-trained diffusion-based TTS model with a personalized adapter. VoiceTailor identifies pivotal modules that benefit from the adapter based on a weight change ratio analysis. We utilize Low-Rank Adaptation (LoRA) as a parameter-efficient adaptation method and incorporate the adapter into pivotal modules of the pre-trained diffusion decoder. To achieve powerful adaptation performance with few parameters, we explore various guidance techniques for speaker adaptation and investigate the best strategies to strengthen speaker information. VoiceTailor demonstrates comparable speaker adaptation performance to existing adaptive TTS models by fine-tuning only 0.25% of the total parameters. VoiceTailor shows strong robustness when adapting to a wide range of real-world speakers, as shown in the demo."
   ],
   "p1": 4413,
   "pn": 4417,
   "doi": "10.21437/Interspeech.2024-63",
   "url": "interspeech_2024/kim24_interspeech.html"
  },
  "liu24_interspeech": {
   "authors": [
    [
     "Jizhong",
     "Liu"
    ],
    [
     "Gang",
     "Li"
    ],
    [
     "Junbo",
     "Zhang"
    ],
    [
     "Heinrich",
     "Dinkel"
    ],
    [
     "Yongqing",
     "Wang"
    ],
    [
     "Zhiyong",
     "Yan"
    ],
    [
     "Yujun",
     "Wang"
    ],
    [
     "Bin",
     "Wang"
    ]
   ],
   "title": "Enhancing Automated Audio Captioning via Large Language Models with Optimized Audio Encoding",
   "original": "65",
   "order": 236,
   "page_count": 5,
   "abstract": [
    "Automated audio captioning (AAC) is an audio-to-text task to describe audio contents in natural language. Recently, the advancements in large language models (LLMs), with improvements in training approaches for audio encoders, have opened up possibilities for improving AAC. Thus, we explore enhancing AAC from three aspects: 1) a pre-trained audio encoder via consistent ensemble distillation (CED) is used to improve the effectivity of acoustic tokens, with a querying transformer (Q-Former) bridging the modality gap to LLM and compress acoustic tokens; 2) we investigate the advantages of using a LLAMA 2 with 7B parameters as the decoder; 3) another pre-trained LLM corrects text errors caused by insufficient training data and annotation ambiguities. Both the audio encoder and text decoder are optimized by low-rank adaptation (LoRA). Experiments show that each of these enhancements is effective. Our method obtains a 33.0 SPIDEr-FL score, outperforming the winner of DCASE 2023 Task 6A."
   ],
   "p1": 1135,
   "pn": 1139,
   "doi": "10.21437/Interspeech.2024-65",
   "url": "interspeech_2024/liu24_interspeech.html"
  },
  "sun24_interspeech": {
   "authors": [
    [
     "Siqi",
     "Sun"
    ],
    [
     "Korin",
     "Richmond"
    ]
   ],
   "title": "Learning Pronunciation from Other Accents via Pronunciation Knowledge Transfer",
   "original": "66",
   "resource": "https://doi.org/10.5281/zenodo.12775334",
   "order": 577,
   "page_count": 5,
   "abstract": [
    "Bootstrapping has proven to be effective in transforming a conventional pipeline-based linguistic frontend to an integrated Sequence-to-Sequence (Seq2Seq) frontend for text-to-speech (TTS). However, for target accents with limited lexical coverage, the performance of bootstrapped Seq2Seq frontends would be greatly limited. In this work, we utilize multi-accent bootstrapping for rich-resource source accents and low-resource target accents to enable pronunciation knowledge transfer between them, effectively enlarging the lexical coverage of target accent. We formally analyze the effect of transfer between 3 English accents (word accuracy increase of 12%-17% absolute for transferred words) and how it scales with the number of annotated unique word types in the target accent. When annotating as few as 1k word types for the target accent, the transfer achieves a word accuracy of 81% for transferred words, approaching the generalisation ability of a baseline annotating 51k word types."
   ],
   "p1": 2805,
   "pn": 2809,
   "doi": "10.21437/Interspeech.2024-66",
   "url": "interspeech_2024/sun24_interspeech.html"
  },
  "ohagi24_interspeech": {
   "authors": [
    [
     "Masaya",
     "Ohagi"
    ],
    [
     "Tomoya",
     "Mizumoto"
    ],
    [
     "Katsumasa",
     "Yoshikawa"
    ]
   ],
   "title": "Investigation of look-ahead techniques to improve response time in spoken dialogue system",
   "original": "67",
   "order": 732,
   "page_count": 5,
   "abstract": [
    "This paper reports a new method that improves the response speed in spoken dialogue systems that use large language models. In existing systems, the start of the chatbot’s response after the user utterance is delayed by the time required to generate that response. In contrast, our system predicts what the user may say next and pre-generates the bot’s response before the user finishes speaking. This look-ahead technique allows the response to be returned by simply matching the predicted user utterance with the actual user utterance. Evaluation results show that our method has high look-ahead accuracy in task-oriented dialogue, contributing to improved response speeds."
   ],
   "p1": 3580,
   "pn": 3584,
   "doi": "10.21437/Interspeech.2024-67",
   "url": "interspeech_2024/ohagi24_interspeech.html"
  },
  "chen24b_interspeech": {
   "authors": [
    [
     "Si",
     "Chen"
    ],
    [
     "Bruce Xiao",
     "Wang"
    ],
    [
     "Yitian",
     "Hong"
    ],
    [
     "Fang",
     "Zhou"
    ],
    [
     "Angel",
     "Chan"
    ],
    [
     "Po-yi",
     "Tang"
    ],
    [
     "Bin",
     "Li"
    ],
    [
     "Chunyi",
     "Wen"
    ],
    [
     "James",
     "Cheung"
    ],
    [
     "Yan",
     "Liu"
    ],
    [
     "Zhuoming",
     "Chen"
    ]
   ],
   "title": "Acoustic changes in speech prosody produced by children with autism after robot-assisted speech training",
   "original": "68",
   "order": 511,
   "page_count": 5,
   "abstract": [
    "Children with Autism exhibit distinct speech prosody, perceived as monotone and they are reported to show deficits in focus marking. The current study designed a robot-assisted training with controlled social interactions aiming to enhance the prosody of children with Autism speaking a tonal language, Cantonese, specifically on focus marking. 20 autistic and 23 typically-developing (TD) children participated in this study. Only the autistic group received training. Stimuli were designed for training, pre- and post-training production. Acoustics of target words were extracted and analysed using linear mixed-effects models examining effects of training and clinical status. Children with Autism improved in signalling sentence prominence using duration but not f0 and intensity. Variability suggests that certain acoustic cues are more challenging. Comparing to TD children's focus marking patterns, autistic children's variability may also stem from their ongoing prosodic profile development."
   ],
   "p1": 2480,
   "pn": 2484,
   "doi": "10.21437/Interspeech.2024-68",
   "url": "interspeech_2024/chen24b_interspeech.html"
  },
  "wang24b_interspeech": {
   "authors": [
    [
     "Wenbin",
     "Wang"
    ],
    [
     "Yang",
     "Song"
    ],
    [
     "Sanjay",
     "Jha"
    ]
   ],
   "title": "GLOBE: A High-quality English Corpus with Global Accents for Zero-shot Speaker Adaptive Text-to-Speech",
   "original": "70",
   "resource": "https://doi.org/10.57967/hf/2715",
   "order": 282,
   "page_count": 5,
   "abstract": [
    "This paper introduces GLOBE, a high-quality English corpus with worldwide accents, specifically designed to address the limitations of current zero-shot speaker adaptive Text-to-Speech (TTS) systems that exhibit poor generalizability in adapting to speakers with accents. Compared to commonly used English corpora, such as LibriTTS and VCTK, GLOBE is unique in its inclusion of utterances from 23,519 speakers and covers 164 accents worldwide, along with detailed metadata for these speakers. Compared to its original corpus, i.e., Common Voice, GLOBE significantly improves the quality of the speech data through rigorous filtering and enhancement processes, while also populating all missing speaker metadata. The final curated GLOBE corpus includes 535 hours of speech data at a 24 kHz sampling rate. Our benchmark results indicate that the speaker adaptive TTS model trained on the GLOBE corpus can synthesize speech with better speaker similarity and comparable naturalness than that trained on other popular corpora. We will release GLOBE publicly after acceptance. The GLOBE dataset is available at https://globecorpus.github.io/."
   ],
   "p1": 1365,
   "pn": 1369,
   "doi": "10.21437/Interspeech.2024-70",
   "url": "interspeech_2024/wang24b_interspeech.html"
  },
  "pieper24_interspeech": {
   "authors": [
    [
     "Jaden",
     "Pieper"
    ],
    [
     "Stephen",
     "Voran"
    ]
   ],
   "title": "AlignNet: Learning dataset score alignment functions to enable better training of speech quality estimators",
   "original": "74",
   "resource": "https://doi.org/10.5281/zenodo.12734153",
   "order": 18,
   "page_count": 5,
   "abstract": [
    "We develop two complementary advances for training no-reference (NR) speech quality estimators with independent datasets. Multi-dataset finetuning (MDF) pretrains an NR estimator on a single dataset and then finetunes it on multiple datasets at once, including the dataset used for pretraining. AlignNet uses an AudioNet to generate intermediate score estimates before using the Aligner to map intermediate estimates to the appropriate score range. AlignNet is agnostic to the choice of AudioNet so any successful NR speech quality estimator can benefit from its Aligner. The methods can be used in tandem, and we use two studies to show that they improve on current solutions: one study uses nine smaller datasets and the other uses four larger datasets. AlignNet with MDF improves on other solutions because it efficiently and effectively removes misalignments that impair the learning process, and thus enables successful training with larger amounts of more diverse data."
   ],
   "p1": 82,
   "pn": 86,
   "doi": "10.21437/Interspeech.2024-74",
   "url": "interspeech_2024/pieper24_interspeech.html"
  },
  "yoon24_interspeech": {
   "authors": [
    [
     "Ji Won",
     "Yoon"
    ],
    [
     "Beom Jun",
     "Woo"
    ],
    [
     "Nam Soo",
     "Kim"
    ]
   ],
   "title": "HuBERT-EE: Early Exiting HuBERT for Efficient Speech Recognition",
   "original": "80",
   "order": 495,
   "page_count": 5,
   "abstract": [
    "Pre-training with self-supervised models, such as Hidden-unit BERT (HuBERT) and wav2vec 2.0, has brought significant improvements in automatic speech recognition (ASR). However, these models usually require an expensive computational cost to achieve outstanding performance, slowing down the inference speed. To improve the model efficiency, we introduce an early exit scheme for ASR, namely HuBERT-EE, that allows the model to stop the inference dynamically. In HuBERT-EE, multiple early exit branches are added at the intermediate layers. When the intermediate prediction of the early exit branch is confident, the model stops the inference, and the corresponding result can be returned early. We investigate the proper early exiting criterion and fine-tuning strategy to effectively perform early exiting. Experimental results on the LibriSpeech show that HuBERT-EE can accelerate the inference of the HuBERT while simultaneously balancing the trade-off between the performance and the latency."
   ],
   "p1": 2400,
   "pn": 2404,
   "doi": "10.21437/Interspeech.2024-80",
   "url": "interspeech_2024/yoon24_interspeech.html"
  },
  "oiso24_interspeech": {
   "authors": [
    [
     "Hideyuki",
     "Oiso"
    ],
    [
     "Yuto",
     "Matsunaga"
    ],
    [
     "Kazuya",
     "Kakizaki"
    ],
    [
     "Taiki",
     "Miyagawa"
    ]
   ],
   "title": "Prompt Tuning for Audio Deepfake Detection: Computationally Efficient Test-time Domain Adaptation with Limited Target Dataset",
   "original": "81",
   "order": 558,
   "page_count": 5,
   "abstract": [
    "We study test-time domain adaptation for audio deepfake detection (ADD), addressing three challenges: (i) source-target domain gaps, (ii) limited target dataset size, and (iii) high computational costs. We propose an ADD method using prompt tuning in a plug-in style. It bridges domain gaps by integrating it seamlessly with state-of-the-art transformer models and/or with other fine-tuning methods, boosting their performance on target data (challenge (i)). In addition, our method can fit small target datasets because it does not require a large number of extra parameters (challenge (ii)). This feature also contributes to computational efficiency, countering the high computational costs typically associated with large-scale pre-trained models in ADD (challenge (iii)). We conclude that prompt tuning for ADD under domain gaps presents a promising avenue for enhancing accuracy with minimal target data and negligible extra computational burden."
   ],
   "p1": 2710,
   "pn": 2714,
   "doi": "10.21437/Interspeech.2024-81",
   "url": "interspeech_2024/oiso24_interspeech.html"
  },
  "lay24_interspeech": {
   "authors": [
    [
     "Bunlong",
     "Lay"
    ],
    [
     "Timo",
     "Gerkmann"
    ]
   ],
   "title": "An Analysis of the Variance of Diffusion-based Speech Enhancement",
   "original": "85",
   "order": 456,
   "page_count": 5,
   "abstract": [
    "Diffusion models proved to be powerful models for generative speech enhancement. In recent SGMSE+ approaches, training involves a stochastic differential equation for the diffusion process, adding both Gaussian and environmental noise to the clean speech signal gradually. The speech enhancement performance varies depending on the choice of the stochastic differential equation that controls the evolution of the mean and the variance along the diffusion processes when adding environmental and Gaussian noise. In this work, we highlight that the scale of the variance is a dominant parameter for speech enhancement performance and show that it controls the tradeoff between noise attenuation and speech distortions. More concretely, we show that a larger variance increases the noise attenuation and allows for reducing the computational footprint, as fewer function evaluations for generating the estimate are required."
   ],
   "p1": 2205,
   "pn": 2209,
   "doi": "10.21437/Interspeech.2024-85",
   "url": "interspeech_2024/lay24_interspeech.html"
  },
  "christ24_interspeech": {
   "authors": [
    [
     "Lukas",
     "Christ"
    ],
    [
     "Shahin",
     "Amiriparian"
    ],
    [
     "Friederike",
     "Hawighorst"
    ],
    [
     "Ann-Kathrin",
     "Schill"
    ],
    [
     "Angelo",
     "Boutalikakis"
    ],
    [
     "Lorenz",
     "Graf-Vlachy"
    ],
    [
     "Andreas",
     "König"
    ],
    [
     "Björn",
     "Schuller"
    ]
   ],
   "title": "This Paper Had the Smartest Reviewers - Flattery Detection Utilising an Audio-Textual Transformer-Based Approach",
   "original": "87",
   "resource": "https://doi.org/10.5281/zenodo.11561487",
   "order": 722,
   "page_count": 5,
   "abstract": [
    "Flattery is an important aspect of human communication that facilitates social bonding, shapes perceptions, and influences behaviour through strategic compliments and praise, leveraging the power of speech to build rapport effectively. Its automatic detection can thus enhance the naturalness of human-AI interactions. To meet this need, we present a novel audio textual dataset comprising 20 hours of speech and train machine learning models for automatic flattery detection. In particular, we employ pretrained AST, Wav2Vec2, and Whisper models for the speech modality, and Whisper TTS models combined with a RoBERTa text classifier for the textual modality. Subsequently, we build a multimodal classifier by combining text and audio representations. Evaluation on unseen test data demonstrates promising results, with Unweighted Average Recall scores reaching 82.46% in audio-only experiments, 85.97% in text-only experiments, and 87.16% using a multimodal approach."
   ],
   "p1": 3530,
   "pn": 3534,
   "doi": "10.21437/Interspeech.2024-87",
   "url": "interspeech_2024/christ24_interspeech.html"
  },
  "jin24_interspeech": {
   "authors": [
    [
     "Zengrui",
     "Jin"
    ],
    [
     "Yifan",
     "Yang"
    ],
    [
     "Mohan",
     "Shi"
    ],
    [
     "Wei",
     "Kang"
    ],
    [
     "Xiaoyu",
     "Yang"
    ],
    [
     "Zengwei",
     "Yao"
    ],
    [
     "Fangjun",
     "Kuang"
    ],
    [
     "Liyong",
     "Guo"
    ],
    [
     "Lingwei",
     "Meng"
    ],
    [
     "Long",
     "Lin"
    ],
    [
     "Yong",
     "Xu"
    ],
    [
     "Shi-Xiong",
     "Zhang"
    ],
    [
     "Daniel",
     "Povey"
    ]
   ],
   "title": "LibriheavyMix: A 20,000-Hour Dataset for Single-Channel Reverberant Multi-Talker Speech Separation, ASR and Speaker Diarization",
   "original": "90",
   "order": 142,
   "page_count": 5,
   "abstract": [
    "The evolving speech processing landscape is increasingly focused on complex scenarios like meetings or cocktail parties with multiple simultaneous speakers and far-field conditions. Existing methodologies for addressing these challenges fall into two categories: multi-channel and single-channel solutions. Single-channel approaches, notable for their generality and convenience, do not require specific information about microphone arrays. This paper presents a large-scale far-field overlapping speech dataset, crafted to advance research in speech separation, recognition, and speaker diarization. This dataset is a critical resource for decoding “Who said What and When” in multitalker, reverberant environments, a daunting challenge in the field. Additionally, we introduce a pipeline system encompassing speech separation, recognition, and diarization as a foundational benchmark. Evaluations on the WHAMR! dataset validate the broad applicability of the proposed data."
   ],
   "p1": 702,
   "pn": 706,
   "doi": "10.21437/Interspeech.2024-90",
   "url": "interspeech_2024/jin24_interspeech.html"
  },
  "triantafyllopoulos24_interspeech": {
   "authors": [
    [
     "Andreas",
     "Triantafyllopoulos"
    ],
    [
     "Anton",
     "Batliner"
    ],
    [
     "Wolfgang",
     "Mayr"
    ],
    [
     "Markus",
     "Fendler"
    ],
    [
     "Florian",
     "Pokorny"
    ],
    [
     "Maurice",
     "Gerczuk"
    ],
    [
     "Shahin",
     "Amiriparian"
    ],
    [
     "Thomas",
     "Berghaus"
    ],
    [
     "Björn",
     "Schuller"
    ]
   ],
   "title": "Sustained Vowels for Pre- vs Post-Treatment COPD Classification",
   "original": "96",
   "order": 291,
   "page_count": 5,
   "abstract": [
    "Chronic obstructive pulmonary disease (COPD) is a serious inflammatory lung disease affecting millions of people around the world. Due to an obstructed airflow from the lungs, it also becomes manifest in patients' vocal behaviour. Of particular importance is the detection of an exacerbation episode, which marks an acute phase  and often requires hospitalisation and treatment. Previous work has shown that it is possible to distinguish between a pre- and a post-treatment state using automatic analysis of read speech. In this contribution, we examine whether sustained vowels can provide a complementary lens for telling apart these two states. Using a cohort of 50 patients, we show that the inclusion of sustained vowels can improve performance to up to 79% unweighted average recall, from a 71% baseline using read speech. We further identify and interpret the most important acoustic features that characterise the manifestation of COPD in sustained vowels."
   ],
   "p1": 1410,
   "pn": 1414,
   "doi": "10.21437/Interspeech.2024-96",
   "url": "interspeech_2024/triantafyllopoulos24_interspeech.html"
  },
  "triantafyllopoulos24b_interspeech": {
   "authors": [
    [
     "Andreas",
     "Triantafyllopoulos"
    ],
    [
     "Anton",
     "Batliner"
    ],
    [
     "Simon",
     "Rampp"
    ],
    [
     "Manuel",
     "Milling"
    ],
    [
     "Björn",
     "Schuller"
    ]
   ],
   "title": "INTERSPEECH 2009 Emotion Challenge Revisited: Benchmarking 15 Years of Progress in Speech Emotion Recognition",
   "original": "97",
   "order": 326,
   "page_count": 5,
   "abstract": [
    "We revisit the INTERSPEECH 2009 Emotion Challenge -- the first ever speech emotion recognition (SER) challenge -- and evaluate a series of deep learning models that are representative of the major advances in SER research in the time since then. We start by training each model using a fixed set of hyperparameters, and further fine-tune the best-performing models of that initial setup with a grid search. Results are always reported on the official test set with a separate validation set only used for early stopping. Most models score below or close to the official baseline, while they marginally outperform the original challenge winners after hyperparameter tuning. Our work illustrates that, despite recent progress, FAU-AIBO remains a very challenging benchmark. An interesting corollary is that newer methods do not consistently outperform older ones, showing that progress towards `solving' SER is not necessarily monotonic."
   ],
   "p1": 1585,
   "pn": 1589,
   "doi": "10.21437/Interspeech.2024-97",
   "url": "interspeech_2024/triantafyllopoulos24b_interspeech.html"
  },
  "triantafyllopoulos24c_interspeech": {
   "authors": [
    [
     "Andreas",
     "Triantafyllopoulos"
    ],
    [
     "Björn",
     "Schuller"
    ]
   ],
   "title": "Enrolment-based personalisation for improving individual-level fairness in speech emotion recognition",
   "original": "98",
   "order": 769,
   "page_count": 5,
   "abstract": [
    "The expression of emotion is highly individualistic. However, contemporary speech emotion recognition (SER) systems typically rely on population-level models that adopt a `one-size-fits-all' approach for predicting emotion. Moreover, standard evaluation practices measure performance also on the population level, thus failing to characterise how models work across different speakers. In the present contribution, we present a new method for capitalising on individual differences to adapt an SER model to each new speaker using a minimal set of enrolment utterances. In addition, we present novel evaluation schemes for measuring fairness across different speakers. Our findings show that aggregated evaluation metrics may obfuscate fairness issues on the individual-level, which are uncovered by our evaluation, and that our proposed method can improve performance both in aggregated and disaggregated terms."
   ],
   "p1": 3729,
   "pn": 3733,
   "doi": "10.21437/Interspeech.2024-98",
   "url": "interspeech_2024/triantafyllopoulos24c_interspeech.html"
  },
  "wang24c_interspeech": {
   "authors": [
    [
     "Zhenyu",
     "Wang"
    ],
    [
     "Shuyu",
     "Kong"
    ],
    [
     "Li",
     "Wan"
    ],
    [
     "Biqiao",
     "Zhang"
    ],
    [
     "Yiteng",
     "Huang"
    ],
    [
     "Mumin",
     "Jin"
    ],
    [
     "Ming",
     "Sun"
    ],
    [
     "Xin",
     "Lei"
    ],
    [
     "Zhaojun",
     "Yang"
    ]
   ],
   "title": "Query-by-Example Keyword Spotting Using Spectral-Temporal Graph Attentive Pooling and Multi-Task Learning",
   "original": "100",
   "order": 66,
   "page_count": 5,
   "abstract": [
    "Existing keyword spotting (KWS) systems primarily rely on predefined keyword phrases. However, the ability to recognize customized keywords is crucial for tailoring interactions with intelligent devices. In this paper, we present a novel Query-by-Example (QbyE) KWS system that employs spectral-temporal graph attentive pooling and multi-task learning. This framework aims to effectively learn speaker-invariant and linguistic-informative embeddings for QbyE KWS tasks. Within this framework, we investigate three distinct network architectures for encoder modeling: LiCoNet, Conformer and ECAPA_TDNN. The experimental results on a substantial internal dataset of 629 speakers have demonstrated the effectiveness of the proposed QbyE framework in maximizing the potential of simpler models such as LiCoNet. Particularly, LiCoNet, which is 13x more efficient, achieves comparable performance to the computationally intensive Conformer model (1.98% vs. 1.63% FRR at 0.3 FAs/Hr)."
   ],
   "p1": 322,
   "pn": 326,
   "doi": "10.21437/Interspeech.2024-100",
   "url": "interspeech_2024/wang24c_interspeech.html"
  },
  "wiepert24_interspeech": {
   "authors": [
    [
     "Daniela A.",
     "Wiepert"
    ],
    [
     "Rene L.",
     "Utianski"
    ],
    [
     "Joseph R.",
     "Duffy"
    ],
    [
     "John L.",
     "Stricker"
    ],
    [
     "Leland R.",
     "Barnard"
    ],
    [
     "David T.",
     "Jones"
    ],
    [
     "Hugo",
     "Botha"
    ]
   ],
   "title": "Speech foundation models in healthcare: Effect of layer selection on pathological speech feature prediction",
   "original": "102",
   "order": 948,
   "page_count": 5,
   "abstract": [
    "Accurately extracting clinical information from speech is critical to the diagnosis and treatment of many neurological conditions. As such, there is interest in leveraging AI for automatic, objective assessments of clinical speech to facilitate diagnosis and treatment of speech disorders. We explore transfer learning using foundation models, focusing on the impact of layer selection for the downstream task of predicting pathological speech features. We find that selecting an optimal layer can greatly improve performance (15.8% increase in balanced accuracy per feature as compared to worst layer,  13.6% increase as compared to final layer), though the best layer varies by predicted feature and does not always generalize well to unseen data. A learned weighted sum offers comparable performance to the average best layer in-distribution (only  1.2% lower) and had strong generalization for out-of-distribution data (only 1.5% lower than the average best layer). "
   ],
   "p1": 4618,
   "pn": 4622,
   "doi": "10.21437/Interspeech.2024-102",
   "url": "interspeech_2024/wiepert24_interspeech.html"
  },
  "li24b_interspeech": {
   "authors": [
    [
     "Yuang",
     "Li"
    ],
    [
     "Jiawei",
     "Yu"
    ],
    [
     "Min",
     "Zhang"
    ],
    [
     "Mengxin",
     "Ren"
    ],
    [
     "Yanqing",
     "Zhao"
    ],
    [
     "Xiaofeng",
     "Zhao"
    ],
    [
     "Shimin",
     "Tao"
    ],
    [
     "Jinsong",
     "Su"
    ],
    [
     "Hao",
     "Yang"
    ]
   ],
   "title": "Using Large Language Model for End-to-End Chinese ASR and NER",
   "original": "103",
   "order": 166,
   "page_count": 5,
   "abstract": [
    "Mapping speech tokens to the same feature space as text tokens has become the paradigm for integrating speech modality into decoder-only large language models (LLMs). An alternative is to use an encoder-decoder architecture that incorporates speech features through cross-attention. In this work, we connect the Whisper encoder with ChatGLM3 and provide in-depth comparisons of these two approaches using Chinese automatic speech recognition (ASR) and named entity recognition (NER) tasks. We evaluate their performance using the F1 score and a fine-grained taxonomy of ASR-NER errors. Our experiments reveal that the encoder-decoder model outperforms the decoder-only model if the context is short, while the decoder-only model benefits from a long context as it fully exploits all layers of the LLM. Additionally, we obtain a state-of-the-art F1 score of 0.805 on the AISHELL-NER test set by using chain-of-thought NER which first infers long-form ASR transcriptions and then predicts NER labels."
   ],
   "p1": 822,
   "pn": 826,
   "doi": "10.21437/Interspeech.2024-103",
   "url": "interspeech_2024/li24b_interspeech.html"
  },
  "li24c_interspeech": {
   "authors": [
    [
     "Yuang",
     "Li"
    ],
    [
     "Min",
     "Zhang"
    ],
    [
     "Chang",
     "Su"
    ],
    [
     "Yinglu",
     "Li"
    ],
    [
     "Xiaosong",
     "Qiao"
    ],
    [
     "Mengxin",
     "Ren"
    ],
    [
     "Miaomiao",
     "Ma"
    ],
    [
     "Daimeng",
     "Wei"
    ],
    [
     "Shimin",
     "Tao"
    ],
    [
     "Hao",
     "Yang"
    ]
   ],
   "title": "A Multitask Training Approach to Enhance Whisper with Open-Vocabulary Keyword Spotting",
   "original": "104",
   "order": 261,
   "page_count": 5,
   "abstract": [
    "The recognition of rare named entities, such as personal names and terminologies, is challenging for automatic speech recognition (ASR) systems, especially when they are not frequently observed in the training data. In this paper, we introduce keyword spotting enhanced Whisper (KWS-Whisper), a novel ASR system that leverages the Whisper model and performs open-vocabulary keyword spotting (OV-KWS) on the hidden states of the Whisper encoder to recognize user-defined named entities. These entities serve as prompts for the Whisper decoder. To optimize the model, we propose a multitask training approach that learns OV-KWS and contextual-ASR tasks. We evaluate our approach on Chinese Aishell hot word subsets and two internal code-switching test sets and show that it significantly improves the entity recall compared to the original Whisper model. Moreover, we demonstrate that the OV-KWS can be a plug-and-play module to enhance the ASR error correction methods and frozen Whisper models."
   ],
   "p1": 1260,
   "pn": 1264,
   "doi": "10.21437/Interspeech.2024-104",
   "url": "interspeech_2024/li24c_interspeech.html"
  },
  "xiao24_interspeech": {
   "authors": [
    [
     "Li",
     "Xiao"
    ],
    [
     "Lucheng",
     "Fang"
    ],
    [
     "Yuhong",
     "Yang"
    ],
    [
     "Weiping",
     "Tu"
    ]
   ],
   "title": "LungAdapter: Efficient Adapting Audio Spectrogram Transformer for Lung Sound Classification",
   "original": "106",
   "order": 972,
   "page_count": 5,
   "abstract": [
    "Recently, fine-tuning the pre-trained large-scale Transformer models in lung sound classification tasks has yielded remarkable outcomes. However, the predominant method for fine-tuning is still full fine-tuning, which entails updating all parameters of large-scale models during training.  Given the recent advancements in large-scale models, this approach requires significant computational resources and time. To tackle this issue, we introduce an efficient fine-tuning approach based on Adapter tuning, namely LungAdapter.  This method can incorporate trainable blocks into a pre-trained audio Transformer model, allowing extraction of crucial information on lung sound classification from the model, while preserving the frozen parameters of large-scale pre-trained models. Experiments have shown that our method achieves performance comparable to or even superior to full fine-tuning while optimizing only 2.83% of the parameters."
   ],
   "p1": 4738,
   "pn": 4742,
   "doi": "10.21437/Interspeech.2024-106",
   "url": "interspeech_2024/xiao24_interspeech.html"
  },
  "gao24b_interspeech": {
   "authors": [
    [
     "Yi",
     "Gao"
    ],
    [
     "Xiang",
     "Su"
    ]
   ],
   "title": "Low Complexity Echo Delay Estimator Based on Binarized Feature Matching",
   "original": "107",
   "order": 33,
   "page_count": 5,
   "abstract": [
    "Echo delay estimation (EDE) serves as a preprocessing component within an acoustic echo canceller (AEC). Despite some progress over the past few decades, there is a dearth of literature on efficient algorithms. This paper introduces a binarized feature-matching (BFM) framework, encompassing a set of feature extraction methods, which are compared with traditional methods such as the GCC-Phat-based method, the adaptive-filter-based method, and popular methods published in WebRTC projects, in terms of both complexity and performance. The computational loads of the BFM methods are significantly lower, and a hybrid BFM method further enhances performance in terms of convergence speed and robustness. This method, characterized by its low complexity, benefits both traditional and NN-based AEC."
   ],
   "p1": 157,
   "pn": 161,
   "doi": "10.21437/Interspeech.2024-107",
   "url": "interspeech_2024/gao24b_interspeech.html"
  },
  "ai24b_interspeech": {
   "authors": [
    [
     "Yang",
     "Ai"
    ],
    [
     "Ye-Xin",
     "Lu"
    ],
    [
     "Xiao-Hang",
     "Jiang"
    ],
    [
     "Zheng-Yan",
     "Sheng"
    ],
    [
     "Rui-Chen",
     "Zheng"
    ],
    [
     "Zhen-Hua",
     "Ling"
    ]
   ],
   "title": "A Low-Bitrate Neural Audio Codec Framework with Bandwidth Reduction and Recovery for High-Sampling-Rate Waveforms",
   "original": "108",
   "order": 362,
   "page_count": 5,
   "abstract": [
    "This paper proposes a novel neural audio codec framework which incorporates bandwidth reduction and recovery, facilitating its application in scenarios with high sampling rates and low bitrates. The proposed framework consists of a two-stage-downsampling-based encoder, a quantizer, and a two-stage-upsampling-based decoder. The encoder initially reduces the bandwidth of the high-sampling-rate waveform before encoding it. Therefore, the discrete tokens outputted by the quantizer are derived from the low-sampling-rate waveform, resulting in a low bitrate. The decoder decodes the low-sampling-rate waveform and ultimately recovers the original high-sampling-rate waveform by bandwidth recovery. Experiments confirm that our proposed framework achieves high-quality audio coding at a sampling rate of 48 kHz and a bitrate of only 1 kbps. The bitrate savings amount to 6 times compared to baseline codecs without bandwidth reduction and recovery."
   ],
   "p1": 1765,
   "pn": 1769,
   "doi": "10.21437/Interspeech.2024-108",
   "url": "interspeech_2024/ai24b_interspeech.html"
  },
  "irino24_interspeech": {
   "authors": [
    [
     "Toshio",
     "Irino"
    ],
    [
     "Shintaro",
     "Doan"
    ],
    [
     "Minami",
     "Ishikawa"
    ]
   ],
   "title": "Signal processing algorithm effective for sound quality of hearing loss simulators ",
   "original": "111",
   "order": 178,
   "page_count": 5,
   "abstract": [
    "Hearing loss (HL) simulators, which allow normal hearing (NH) listeners to experience HL, have been used in speech intelligibility experiments, but not in sound quality experiments due to perceptible distortion. If they produced less distortion, they might be useful for NH listeners to evaluate the sound quality of, for example, hearing aids. We conducted perceptual sound quality experiments to compare the Cambridge version of HL simulator (CamHLS) and the Wakayama version of the HL simulator (WHIS), which has the two algorithms of filterbank analysis synthesis (FBAS) and direct time-varying filter (DTVF). The experimental results showed that WHIS with DTVF produces less perceptible distortion in speech sounds than CamHLS and WHIS with FBAS, even when the nonlinear process is working. This advantage is mainly due to the use of the DTVF algorithm, which could be applied to various signal synthesis applications with filterbank analysis."
   ],
   "p1": 882,
   "pn": 886,
   "doi": "10.21437/Interspeech.2024-111",
   "url": "interspeech_2024/irino24_interspeech.html"
  },
  "wang24d_interspeech": {
   "authors": [
    [
     "Hualei",
     "Wang"
    ],
    [
     "Jianguo",
     "Mao"
    ],
    [
     "Zhifang",
     "Guo"
    ],
    [
     "Jiarui",
     "Wan"
    ],
    [
     "Hong",
     "Liu"
    ],
    [
     "Xiangdong",
     "Wang"
    ]
   ],
   "title": "Leveraging Language Model Capabilities for Sound Event Detection",
   "original": "112",
   "order": 985,
   "page_count": 5,
   "abstract": [
    "Large language models reveal deep comprehension and fluent generation in the field of multi-modality. Although significant advancements have been achieved in audio multi-modality, existing methods are rarely leverage language model for sound event detection (SED). In this work, we propose an end-to-end framework for understanding audio features while simultaneously generating sound event and their temporal location. Specifically, we employ pretrained acoustic models to capture discriminative features across different categories and language models for autoregressive text generation. Conventional methods generally struggle to obtain features in pure audio domain for classification. In contrast, our framework utilizes the language model to flexibly understand abundant semantic context aligned with the acoustic representation. The experimental results showcase the effectiveness of proposed method in enhancing timestamps precision and event classification."
   ],
   "p1": 4803,
   "pn": 4807,
   "doi": "10.21437/Interspeech.2024-112",
   "url": "interspeech_2024/wang24d_interspeech.html"
  },
  "vali24_interspeech": {
   "authors": [
    [
     "Mohammad Hassan",
     "Vali"
    ],
    [
     "Tom",
     "Bäckström"
    ]
   ],
   "title": "Privacy PORCUPINE: Anonymization of Speaker Attributes Using Occurrence Normalization for Space-Filling Vector Quantization",
   "original": "117",
   "resource": "https://doi.org/10.5281/zenodo.13132890",
   "order": 461,
   "page_count": 5,
   "abstract": [
    "Speech signals contain a vast range of private information such as its text, speaker identity, emotions, and state of health. Privacy-preserving speech processing seeks to filter out any private information that is not needed for downstream tasks, for example with an information bottleneck, sufficiently tight that only the desired information can pass through. We however demonstrate that the occurrence frequency of codebook elements in bottlenecks using vector quantization have an uneven information rate, threatening privacy. We thus propose to use space-filling vector quantization (SFVQ) together with occurrence normalization, balancing the information rate and thus protecting privacy. Our experiments with speaker identification validate the proposed method. This approach thus provides a generic tool for quantizing information bottlenecks in any speech applications such that their privacy disclosure is predictable and quantifiable."
   ],
   "p1": 2230,
   "pn": 2234,
   "doi": "10.21437/Interspeech.2024-117",
   "url": "interspeech_2024/vali24_interspeech.html"
  },
  "wu24_interspeech": {
   "authors": [
    [
     "Jingyao",
     "Wu"
    ],
    [
     "Ting",
     "Dang"
    ],
    [
     "Vidhyasaharan",
     "Sethu"
    ],
    [
     "Eliathamby",
     "Ambikairajah"
    ]
   ],
   "title": "Dual-Constrained Dynamical Neural ODEs for Ambiguity-aware Continuous Emotion Prediction",
   "original": "119",
   "resource": "https://doi.org/10.5281/zenodo.12791873",
   "order": 653,
   "page_count": 5,
   "abstract": [
    "There has been a significant focus on modelling emotion ambiguity in recent years, with advancements made in representing emotions as distributions to capture ambiguity. However, there has been comparatively less effort devoted to the consideration of temporal dependencies in emotion distributions which encodes ambiguity in perceived emotions that evolve smoothly over time. Recognizing the benefits of using constrained dynamical neural ordinary differential equations (CD-NODE) to model time series as dynamic processes, we propose an ambiguity-aware dual-constrained Neural ODE approach to model the dynamics of emotion distributions on arousal and valence. In our approach, we utilize ODEs parameterised by neural networks to estimate the distribution parameters, and we integrate additional constraints to restrict the range of the system outputs to ensure the validity of predicted distributions. We evaluated our proposed system on the publicly available RECOLA dataset and observed very promising performance across a range of evaluation metrics."
   ],
   "p1": 3185,
   "pn": 3189,
   "doi": "10.21437/Interspeech.2024-119",
   "url": "interspeech_2024/wu24_interspeech.html"
  },
  "bn24_interspeech": {
   "authors": [
    [
     "Suhas",
     "BN"
    ],
    [
     "Amanda",
     "Rebar"
    ],
    [
     "Saeed",
     "Abdullah"
    ]
   ],
   "title": "Speaking of Health: Leveraging Large Language Models to assess Exercise Motivation and Behavior of Rehabilitation Patients",
   "original": "121",
   "order": 647,
   "page_count": 5,
   "abstract": [
    "This paper aims to establish relationships between conversational markers and health outcomes using data from cardio-pulmonary rehabilitation sessions. Specifically, we used speech and text data from conversations between patients and researchers to assess exercise compliance and psychological wellbeing. We trained a Multimodal Transformer (MMT) on speech, transcript, and ground-truth labels. We further evaluate MMT's predictive performance by using session summaries generated by three Large Language Models (LLMs), which focused on dialogue characteristics (e.g., sentiment, thematic content, and future planning). Our findings establish the feasibility of augmenting speech and language processing of clinical sessions to improve decision-making and health outcomes."
   ],
   "p1": 3155,
   "pn": 3159,
   "doi": "10.21437/Interspeech.2024-121",
   "url": "interspeech_2024/bn24_interspeech.html"
  },
  "chen24c_interspeech": {
   "authors": [
    [
     "Yu-Wen",
     "Chen"
    ],
    [
     "Zhou",
     "Yu"
    ],
    [
     "Julia",
     "Hirschberg"
    ]
   ],
   "title": "MultiPA: A Multi-task Speech Pronunciation Assessment Model for Open Response Scenarios",
   "original": "123",
   "order": 61,
   "page_count": 5,
   "abstract": [
    "Pronunciation assessment models designed for open response scenarios enable users to practice language skills in a manner similar to real-life communication. However, previous open-response pronunciation assessment models have predominantly focused on a single pronunciation task, such as sentence-level accuracy, rather than offering a comprehensive assessment in various aspects. We propose MultiPA, a Multitask Pronunciation Assessment model that provides sentence-level accuracy, fluency, prosody, and word-level accuracy assessment for open responses. We examined the correlation between different pronunciation tasks and showed the benefits of multi-task learning. Our model reached the state-of-the-art performance on existing in-domain data sets and effectively generalized to an out-of-domain dataset that we newly collected. The experimental results demonstrate the practical utility of our model in real-world applications."
   ],
   "p1": 297,
   "pn": 301,
   "doi": "10.21437/Interspeech.2024-123",
   "url": "interspeech_2024/chen24c_interspeech.html"
  },
  "mawalim24_interspeech": {
   "authors": [
    [
     "Candy Olivia",
     "Mawalim"
    ],
    [
     "Shogo",
     "Okada"
    ],
    [
     "Masashi",
     "Unoki"
    ]
   ],
   "title": "Are Recent Deep Learning-Based Speech Enhancement Methods Ready to Confront Real-World Noisy Environments?",
   "original": "129",
   "order": 356,
   "page_count": 5,
   "abstract": [
    "Recent advancements in speech enhancement techniques have ignited interest in improving speech quality and intelligibility. However, the effectiveness of recently proposed methods is unclear. In this paper, a comprehensive analysis of modern deep learning-based speech enhancement approaches is presented. Through evaluations using the Deep Suppression Noise and Clarity Enhancement Challenge datasets, we assess the performances of three methods: Denoiser, DeepFilterNet3, and FullSubNet+. Our findings reveal nuanced performance differences among these methods, with varying efficacy across datasets. While objective metrics offer valuable insights, they struggle to represent complex scenarios with multiple noise sources. Leveraging ASR-based methods for these scenarios shows promise but may induce critical hallucination effects. Our study emphasizes the need for ongoing research to refine techniques for diverse real-world environments."
   ],
   "p1": 1735,
   "pn": 1739,
   "doi": "10.21437/Interspeech.2024-129",
   "url": "interspeech_2024/mawalim24_interspeech.html"
  },
  "mun24_interspeech": {
   "authors": [
    [
     "Jihyun",
     "Mun"
    ],
    [
     "Sunhee",
     "Kim"
    ],
    [
     "Minhwa",
     "Chung"
    ]
   ],
   "title": "Developing an End-to-End Framework for Predicting the Social Communication Severity Scores of Children with Autism Spectrum Disorder",
   "original": "131",
   "order": 295,
   "page_count": 5,
   "abstract": [
    "Autism Spectrum Disorder (ASD) is a lifelong condition that significantly influencing an individual's communication abilities and their social interactions. Early diagnosis and intervention are critical due to the profound impact of ASD's characteristic behaviors on foundational developmental stages. However, limitations of standardized diagnostic tools necessitate the development of objective and precise diagnostic methodologies. This paper proposes an end-to-end framework for automatically predicting the social communication severity of children with ASD from raw speech data. This framework incorporates an automatic speech recognition model, fine-tuned with speech data from children with ASD, followed by the application of fine-tuned pre-trained language models to generate a final prediction score. Achieving a Pearson Correlation Coefficient of 0.6566 with human-rated scores, the proposed method showcases its potential as an accessible and objective tool for the assessment of ASD."
   ],
   "p1": 1430,
   "pn": 1434,
   "doi": "10.21437/Interspeech.2024-131",
   "url": "interspeech_2024/mun24_interspeech.html"
  },
  "nijat24_interspeech": {
   "authors": [
    [
     "Mewlude",
     "Nijat"
    ],
    [
     "Chen",
     "Chen"
    ],
    [
     "Dong",
     "Wang"
    ],
    [
     "Askar",
     "Hamdulla"
    ]
   ],
   "title": "UY/CH-CHILD -- A Public Chinese L2 Speech Database of Uyghur Children",
   "original": "135",
   "order": 533,
   "page_count": 5,
   "abstract": [
    "Exploring the progression of pronunciation skills in second language (L2) acquisition among children presents an intriguing research avenue. Yet, the comprehension of this process for Uyghur children learning Chinese as their L2 has been constrained by a scarcity of speech data. To bridge this gap, we have developed the UY/CH-CHILD speech database, comprising 29,061 samples of Chinese words articulated by 106 Uyghur children from both kindergartens and primary schools. The database includes carefully labelled syllables and tones by native Chinese speakers. To showcase the utility of this novel resource, we conducted a comparative analysis of pronunciation errors between the kindergarten and primary school groups, unveiling interesting insights into the evolution of pronunciation proficiency in Uyghur children as they mature. \nThe database can be downloaded online at http://child.cslt.org."
   ],
   "p1": 2585,
   "pn": 2589,
   "doi": "10.21437/Interspeech.2024-135",
   "url": "interspeech_2024/nijat24_interspeech.html"
  },
  "vandereeckt24_interspeech": {
   "authors": [
    [
     "Steven",
     "Vander Eeckt"
    ],
    [
     "Hugo",
     "Van hamme"
    ]
   ],
   "title": "Unsupervised Online Continual Learning for Automatic Speech Recognition",
   "original": "136",
   "order": 585,
   "page_count": 5,
   "abstract": [
    "Adapting Automatic Speech Recognition (ASR) models to new domains leads to Catastrophic Forgetting (CF) of previously learned information. This paper addresses CF in the challenging context of Online Continual Learning (OCL), with tasks presented as a continuous data stream with unknown boundaries. We extend OCL for ASR into the unsupervised realm, by leveraging self-training (ST) to facilitate unsupervised adaptation, enabling models to adapt continually without label dependency and without forgetting previous knowledge. Through comparative analysis of various OCL and ST methods across two domain adaptation experiments, we show that UOCL suffers from significantly less forgetting compared to supervised OCL, allowing UOCL methods to approach the performance levels of supervised OCL. Our proposed UOCL extensions further boosts UOCL's efficacy. Our findings represent a significant step towards continually adaptable ASR systems, capable of leveraging unlabeled data across diverse domains."
   ],
   "p1": 2845,
   "pn": 2849,
   "doi": "10.21437/Interspeech.2024-136",
   "url": "interspeech_2024/vandereeckt24_interspeech.html"
  },
  "scheibler24_interspeech": {
   "authors": [
    [
     "Robin",
     "Scheibler"
    ],
    [
     "Yusuke",
     "Fujita"
    ],
    [
     "Yuma",
     "Shirahata"
    ],
    [
     "Tatsuya",
     "Komatsu"
    ]
   ],
   "title": "Universal Score-based Speech Enhancement with High Content Preservation",
   "original": "138",
   "order": 242,
   "page_count": 5,
   "abstract": [
    "We propose UNIVERSE++, a universal speech enhancement method based on score-based diffusion and adversarial training. Specifically, we improve the existing UNIVERSE model that decouples clean speech feature extraction and diffusion. Our contributions are three-fold. First, we make several modifications to the network architecture, improving training stability and final performance. Second, we introduce an adversarial loss to promote learning high quality speech features. Third, we propose a low-rank adaptation scheme with a phoneme fidelity loss to improve content preservation in the enhanced speech. In the experiments, we train a universal enhancement model on a large scale dataset of speech degraded by noise, reverberation, and various distortions. The results on multiple public benchmark datasets demonstrate that UNIVERSE++ compares favorably to both discriminative and generative baselines for a wide range of qualitative and intelligibility metrics."
   ],
   "p1": 1165,
   "pn": 1169,
   "doi": "10.21437/Interspeech.2024-138",
   "url": "interspeech_2024/scheibler24_interspeech.html"
  },
  "lee24_interspeech": {
   "authors": [
    [
     "Minyoung",
     "Lee"
    ],
    [
     "Eunil",
     "Park"
    ],
    [
     "Sungeun",
     "Hong"
    ]
   ],
   "title": "FVTTS : Face Based Voice Synthesis for Text-to-Speech",
   "original": "140",
   "resource": "https://doi.org/10.5281/zenodo.12741666",
   "order": 1015,
   "page_count": 5,
   "abstract": [
    "A face is expressive of individual identity and used in various studies such as identification, authentication, and personalization. Similarly, a voice is a means of expressing individuals, and personalized voice synthesis based on voice reference is active. However, the voice-based method confronts voice sample dependency limitations. We propose Face-based Voice synthesis for Text-To-Speech (FVTTS) to synthesize voice from face images that are more expressive of personal identity than voice samples. A major challenge in face-based TTS methods is extracting distinct voice features highly related to voice from the face image. Our face encoder is designed to tackle this by integrating global facial attributes with voice-related features to represent personalized characteristics. FVTTS has shown superiority in various metrics and adaptability across different data domains. We establish a new standard in face-based TTS, leading the way in personalized voice synthesis."
   ],
   "p1": 4953,
   "pn": 4957,
   "doi": "10.21437/Interspeech.2024-140",
   "url": "interspeech_2024/lee24_interspeech.html"
  },
  "tao24_interspeech": {
   "authors": [
    [
     "Liang",
     "Tao"
    ],
    [
     "Maoshen",
     "Jia"
    ],
    [
     "Yonggang",
     "Hu"
    ],
    [
     "Changchun",
     "Bao"
    ]
   ],
   "title": "Spatial Acoustic Enhancement Using Unbiased Relative Harmonic Coefficients",
   "original": "141",
   "order": 669,
   "page_count": 5,
   "abstract": [
    "This paper targets at enhancing the noisy soundfield over the entire recording area and all the individual channels, while preserving the spatial clues of the original soundfield. For the goal, we utilize a recently proposed spherical harmonics (SH) domain feature denoted relative harmonic coefficients (RHC) as it compactly contains the source’s spatial information. Specifically, we (i) propose an unbiased estimator of RHC in noisy environments; (ii) estimate the source signal in noisy environments using a SH domain beamformer; (iii) enhance the SH coefficients by multiplying the estimated RHC and source signal; and (iv) reconstruct the entire soundfield based on the enhanced SH coefficients. Finally, we evaluate and validate the performance of the enhancement algorithm using extensive simulations."
   ],
   "p1": 3265,
   "pn": 3269,
   "doi": "10.21437/Interspeech.2024-141",
   "url": "interspeech_2024/tao24_interspeech.html"
  },
  "jing24_interspeech": {
   "authors": [
    [
     "Xin",
     "Jing"
    ],
    [
     "Luyang",
     "Zhang"
    ],
    [
     "Jiangjian",
     "Xie"
    ],
    [
     "Alexander",
     "Gebhard"
    ],
    [
     "Alice",
     "Baird"
    ],
    [
     "Björn",
     "Schuller"
    ]
   ],
   "title": "DB3V: A Dialect Dominated Dataset of Bird Vocalisation for Cross-corpus Bird Species Recognition",
   "original": "143",
   "order": 27,
   "page_count": 5,
   "abstract": [
    "In ornithology, bird species are known to have variedit’s widely acknowledged that bird species display diverse dialects in their calls across different regions. Consequently, computational methods to identify bird species onsolely through their calls face critsignificalnt challenges. There is growing interest in understanding the impact of species-specific dialects on the effectiveness of bird species recognition methods. Despite potential mitigation through the expansion of dialect datasets, the absence of publicly available testing data currently impedes robust benchmarking efforts. This paper presents the Dialect Dominated Dataset of Bird Vocalisation (D3BV), the first cross-corpus dataset that focuses on dialects in bird vocalisations. The D3BV comprises more than 25 hours of audio recordings from 10 bird species distributed across three distinct regions in the contiguous United States (CONUS). In addition to presenting the dataset, we conduct analyses and establish baseline models for cross-corpus bird recognition. The data and code are publicly available online: https://zenodo.org/records/11544734"
   ],
   "p1": 127,
   "pn": 131,
   "doi": "10.21437/Interspeech.2024-143",
   "url": "interspeech_2024/jing24_interspeech.html"
  },
  "mu24_interspeech": {
   "authors": [
    [
     "Da",
     "Mu"
    ],
    [
     "Zhicheng",
     "Zhang"
    ],
    [
     "Haobo",
     "Yue"
    ]
   ],
   "title": "MFF-EINV2: Multi-scale Feature Fusion across Spectral-Spatial-Temporal Domains for Sound Event Localization and Detection",
   "original": "145",
   "resource": "https://doi.org/10.5281/zenodo.12753731",
   "order": 20,
   "page_count": 5,
   "abstract": [
    "Sound Event Localization and Detection (SELD) involves detecting and localizing sound events using multichannel sound recordings. Previously proposed Event-Independent Network V2 (EINV2) has achieved outstanding performance on SELD. However, it still faces challenges in effectively extracting features across spectral, spatial, and temporal domains. This paper proposes a three-stage network structure named Multi-scale Feature Fusion (MFF) module to fully extract multi-scale features across spectral, spatial, and temporal domains. The MFF module utilizes parallel subnetworks architecture to generate multi-scale spectral and spatial features. The TF-Convolution Module is employed to provide multi-scale temporal features. We incorporated MFF into EINV2 and term the proposed method as MFF-EINV2. Experimental results in 2022 and 2023 DCASE challenge task3 datasets show the effectiveness of our MFF-EINV2, which achieves state-of-the-art (SOTA) performance compared to published methods."
   ],
   "p1": 92,
   "pn": 96,
   "doi": "10.21437/Interspeech.2024-145",
   "url": "interspeech_2024/mu24_interspeech.html"
  },
  "yang24b_interspeech": {
   "authors": [
    [
     "Hao",
     "Yang"
    ],
    [
     "Min",
     "Zhang"
    ],
    [
     "Minghan",
     "Wang"
    ],
    [
     "Jiaxin",
     "Guo"
    ]
   ],
   "title": "RASU: Retrieval Augmented Speech Understanding through Generative Modeling",
   "original": "148",
   "order": 718,
   "page_count": 5,
   "abstract": [
    "Large language models have benefited from retrieval augmented generation (RAG) techniques, which allow relevant knowledge to be retrieved and provided as prompts to enhance natural language understanding capabilities. Extending this promising approach to spoken language understanding (SLU) tasks represents  an important area of study. This paper introduces a novel RAG framework tailored for SLU called Retrieval Augmented Speech Understanding(RASU). The proposed model first employs the encoder from a pre-trained automatic speech recognition (ASR) model to retrieve relevant speech segments and transcripts from the training data given a new spoken utterance. The retrieved text transcripts and their corresponding intent labels are then formulated as prompts to conditionally guide the SLU decoder during generation. Additionally, a prompt attention mechanism is incorporated to strengthen the interaction between the generated outputs and the retrieved prompts. Empirical evaluations demonstrate that RASU substantially outperforms conventional end-to-end and cascaded SLU models on intent prediction from speech data. These results highlight the efficacy of leveraging retrieval-based prompting and external knowledge sources to markedly improve spoken language understanding performance. The RASU approach presents a promising direction for advancing SLU capabilities by bridging speech retrieval and generative language modeling."
   ],
   "p1": 3510,
   "pn": 3514,
   "doi": "10.21437/Interspeech.2024-148",
   "url": "interspeech_2024/yang24b_interspeech.html"
  },
  "richter24_interspeech": {
   "authors": [
    [
     "Julius",
     "Richter"
    ],
    [
     "Yi-Chiao",
     "Wu"
    ],
    [
     "Steven",
     "Krenn"
    ],
    [
     "Simon",
     "Welker"
    ],
    [
     "Bunlong",
     "Lay"
    ],
    [
     "Shinji",
     "Watanabe"
    ],
    [
     "Alexander",
     "Richard"
    ],
    [
     "Timo",
     "Gerkmann"
    ]
   ],
   "title": "EARS: An Anechoic Fullband Speech Dataset Benchmarked for Speech Enhancement and Dereverberation",
   "original": "153",
   "order": 999,
   "page_count": 5,
   "abstract": [
    "We release the EARS (Expressive Anechoic Recordings of Speech) dataset, a high-quality speech dataset comprising 107 speakers from diverse backgrounds, totalling in 100 hours of clean, anechoic speech data. The dataset covers a large range of different speaking styles, including emotional speech, different reading styles, non-verbal sounds, and conversational freeform speech. We benchmark various methods for speech enhancement and dereverberation on the dataset and evaluate their performance through a set of instrumental metrics. In addition, we conduct a listening test with 20 participants for the speech enhancement task, where a generative method is preferred. We introduce a blind test set that allows for automatic online evaluation of uploaded data. Dataset download links and automatic evaluation server can be found online."
   ],
   "p1": 4873,
   "pn": 4877,
   "doi": "10.21437/Interspeech.2024-153",
   "url": "interspeech_2024/richter24_interspeech.html"
  },
  "despotovic24_interspeech": {
   "authors": [
    [
     "Vladimir",
     "Despotovic"
    ],
    [
     "Abir",
     "Elbéji"
    ],
    [
     "Petr V.",
     "Nazarov"
    ],
    [
     "Guy",
     "Fagherazzi"
    ]
   ],
   "title": "Multimodal Fusion for Vocal Biomarkers Using Vector Cross-Attention",
   "original": "156",
   "order": 296,
   "page_count": 5,
   "abstract": [
    "Vocal biomarkers are measurable characteristics of person's voice that provide valuable insights into various aspects of their physiological and psychological state, or health status. The use of standardized voice tasks, such as reading, counting, or sustained vowel phonation are common in vocal biomarker research, but semi-spontaneous tasks where the person is instructed to talk about a particular topic, or spontaneous speech are also increasingly used. However, limited efforts were made to combine multiple voice modalities. In this paper, we propose a simple, yet efficient approach of fusing multiple standardized voice tasks based on vector cross-attention, showing improved predictive capacity for derived vocal biomarkers in comparison to single modalities. The multimodal approach is tested on the assessment of respiratory quality of life from reading and sustained vowel phonation recordings, outperforming single modalities up to 4.2% in terms of accuracy (relative increase of 7%)."
   ],
   "p1": 1435,
   "pn": 1439,
   "doi": "10.21437/Interspeech.2024-156",
   "url": "interspeech_2024/despotovic24_interspeech.html"
  },
  "wang24e_interspeech": {
   "authors": [
    [
     "Liming",
     "Wang"
    ],
    [
     "Yuan",
     "Gong"
    ],
    [
     "Nauman",
     "Dawalatabad"
    ],
    [
     "Marco",
     "Vilela"
    ],
    [
     "Katerina",
     "Placek"
    ],
    [
     "Brian",
     "Tracey"
    ],
    [
     "Yishu",
     "Gong"
    ],
    [
     "Alan",
     "Premasiri"
    ],
    [
     "Fernando",
     "Vieira"
    ],
    [
     "James",
     "Glass"
    ]
   ],
   "title": "Automatic Prediction of Amyotrophic Lateral Sclerosis Progression using Longitudinal Speech Transformer",
   "original": "158",
   "order": 409,
   "page_count": 5,
   "abstract": [
    "Automatic prediction of amyotrophic lateral sclerosis (ALS) disease progression provides a more efficient and objective alternative than manual approaches. We propose ALS longitudinal speech transformer (ALST), a neural network-based automatic predictor of ALS disease progression from longitudinal speech recordings of ALS patients. By taking advantage of high-quality pretrained speech features and longitudinal information in the recordings, our best model achieves 91.0%AUC, improving upon the previous best model by 5.6% relative on the ALS TDI dataset. Careful analysis reveals that ALST is capable of fine-grained, interpretable predictions of ALS progression."
   ],
   "p1": 2000,
   "pn": 2004,
   "doi": "10.21437/Interspeech.2024-158",
   "url": "interspeech_2024/wang24e_interspeech.html"
  },
  "wang24f_interspeech": {
   "authors": [
    [
     "Yujia",
     "Wang"
    ],
    [
     "Hexin",
     "Liu"
    ],
    [
     "Leibny Paola",
     "Garcia"
    ]
   ],
   "title": "Bridging Child-Centered Speech Language Identification and Language Diarization via Phonetics",
   "original": "159",
   "order": 1054,
   "page_count": 5,
   "abstract": [
    "Language Diarization (LD) can be viewed as an expansion of Language Identification (LID) that removes the monolingual input assumption. Taking inspiration from this connection and the challenges inherent in Code-Switching (CS) child-centered speech, we extended PHO-LID, an LID model that incorporates acoustic and phonotactic information without needing phoneme annotation, to LD. Our method explores three avenues to adapt PHO-LID into LD: a temporal slicing scheme bridging LID and LD, an embedding modification enriching LD message, and a back-end scoring facilitating fine-tuning. Compared to the baseline, trained on a simulated out-of-domain dataset, SEAME_sim, our method shows a 15.82% relative accuracy improvement on MERLIon, a child-centered CS speech corpus. The back-end scoring preserves pre-trained knowledge in fine-tuning, with a 16.93% relative accuracy improvement on pre-trained SEAME_sim test set without compromising the fine-tuning test set performance."
   ],
   "p1": 5148,
   "pn": 5152,
   "doi": "10.21437/Interspeech.2024-159",
   "url": "interspeech_2024/wang24f_interspeech.html"
  },
  "loweimi24_interspeech": {
   "authors": [
    [
     "Erfan",
     "Loweimi"
    ],
    [
     "Mengjie",
     "Qian"
    ],
    [
     "Kate",
     "Knill"
    ],
    [
     "Mark",
     "Gales"
    ]
   ],
   "title": "On the Usefulness of Speaker Embeddings for Speaker Retrieval in the Wild: A Comparative Study of x-vector and ECAPA-TDNN Models",
   "original": "161",
   "order": 778,
   "page_count": 5,
   "abstract": [
    "In this paper, we investigate the efficacy of the widely-used x-vector and ECAPA-TDNN speaker embeddings for speaker retrieval on the BBC Rewind corpus. In this archival collection each file is briefly described by a synopsis. Our objective is to develop a speaker retrieval system, treating the names mentioned in the synopses as speakers. However, the provided labels exhibit significant noise, posing challenges for model training. Further, the dataset encompasses diverse acoustic conditions, ranging from clean to highly noisy environments. To address these challenges, we develop a speaker retrieval system ``in the wild'' via leveraging pre-trained x-vector and ECAPA-TDNN embeddings models from the SpeechBrain and NeMo toolkits. We assess the effectiveness of these embeddings and explore the usefulness of their combination. Additionally, we evaluate the models' robustness against additive noise and reverberation as well as variations in bit-depth and sampling rate."
   ],
   "p1": 3774,
   "pn": 3778,
   "doi": "10.21437/Interspeech.2024-161",
   "url": "interspeech_2024/loweimi24_interspeech.html"
  },
  "li24d_interspeech": {
   "authors": [
    [
     "Zhengxiao",
     "Li"
    ],
    [
     "Nakamasa",
     "Inoue"
    ]
   ],
   "title": "Locally Aligned Rectified Flow Model for Speech Enhancement Towards Single-Step Diffusion",
   "original": "162",
   "order": 454,
   "page_count": 5,
   "abstract": [
    "Diffusion models based on stochastic differential equations have been shown to be effective in speech enhancement, a task of recovering clean speech signals from noisy speech signals. However, these models are limited by computational complexity, mainly due to the large number of function evaluations required in the reverse diffusion process. To address this limitation, we propose the locally aligned rectified flow (LARF) model, a diffusion model based on ordinary differential equations that learns a transport mapping between the distributions of clean and noisy speech features. By introducing global and local flow matching losses, LARF restricts the transport mapping to be as straight as possible, resulting in a reduction in the number of function evaluations. In experiments, we demonstrate the effectiveness of LARF on the two speech enhancement datasets: WSJ0-CHiME3 and VoiceBank-DEMAND. On WSJ0-CHiME3, LARF achieved a PESQ of 2.95 and an SI-SDR of 19.3 with a single step."
   ],
   "p1": 2195,
   "pn": 2199,
   "doi": "10.21437/Interspeech.2024-162",
   "url": "interspeech_2024/li24d_interspeech.html"
  },
  "lee24b_interspeech": {
   "authors": [
    [
     "Jeehye",
     "Lee"
    ],
    [
     "Hyeji",
     "Seo"
    ]
   ],
   "title": "Online Knowledge Distillation of Decoder-Only Large Language Models for Efficient Speech Recognition",
   "original": "163",
   "order": 594,
   "page_count": 5,
   "abstract": [
    "Large language models (LLMs), which show promising performance in generation tasks, have proven their capabilities to be applied in a wide range of tasks. Although there are several approaches to adapt LLMs as decoder in speech recognition tasks, these can slow down inference speed, which is an important issue for the product-level systems. To address this problem, we introduce online knowledge distillation methods to transfer information from the decoder-only LLMs to a more compact Transformer decoder during the training phase. Implementing our proposed methods on a multilingual low-resource dataset, we achieved a 8.2% relative character error rate (CER) reduction compared to the LLM decoder model with much lower inference cost and a 34.7% relative CER reduction compared to the attention-based encoder-decoder (AED) model. Furthermore, we obtained a 14.9% relative CER reduction along with the same inference cost on a general Korean dataset."
   ],
   "p1": 2890,
   "pn": 2894,
   "doi": "10.21437/Interspeech.2024-163",
   "url": "interspeech_2024/lee24b_interspeech.html"
  },
  "zhang24_interspeech": {
   "authors": [
    [
     "Yixuan",
     "Zhang"
    ],
    [
     "Hao",
     "Zhang"
    ],
    [
     "Meng",
     "Yu"
    ],
    [
     "Dong",
     "Yu"
    ]
   ],
   "title": "Neural Network Augmented Kalman Filter for Robust Acoustic Howling Suppression",
   "original": "166",
   "order": 352,
   "page_count": 5,
   "abstract": [
    "Acoustic howling suppression (AHS) is a critical challenge in audio communication systems. In this paper, we propose a novel approach that leverages the power of neural networks (NN) to enhance the performance of traditional Kalman filter algorithms for AHS. Specifically, our method involves the integration of NN modules into the Kalman filter, enabling refining reference signal, a key factor in effective adaptive filtering, and estimating covariance metrics for the filter which are crucial for adaptability in dynamic conditions, thereby obtaining improved AHS performance. As a result, the proposed method achieves improved AHS performance compared to both standalone NN and Kalman filter methods. Experimental evaluations validate the effectiveness of our approach."
   ],
   "p1": 1715,
   "pn": 1719,
   "doi": "10.21437/Interspeech.2024-166",
   "url": "interspeech_2024/zhang24_interspeech.html"
  },
  "yang24c_interspeech": {
   "authors": [
    [
     "Dong",
     "Yang"
    ],
    [
     "Tomoki",
     "Koriyama"
    ],
    [
     "Yuki",
     "Saito"
    ]
   ],
   "title": "Frame-Wise Breath Detection with Self-Training: An Exploration of Enhancing Breath Naturalness in Text-to-Speech",
   "original": "168",
   "order": 1010,
   "page_count": 5,
   "abstract": [
    "Developing Text-to-Speech (TTS) systems that can synthesize natural breath is essential for human-like voice agents but requires extensive manual annotation of breath positions in training data. To this end, we propose a self-training method for training a breath detection model that can automatically detect breath positions in speech. Our method trains the model using a large speech corpus and involves: 1) annotation of limited breath sounds utilizing a rule-based approach, and 2) iterative augmentation of these annotations through pseudo-labeling based on the model's predictions. Our detection model employs Conformer blocks with down-/up-sampling layers, enabling accurate frame-wise breath detection. We investigate its effectiveness in multi-speaker TTS using text transcripts with detected breath marks. The results indicate that using our proposed model for breath detection and breath mark insertion synthesizes breath-contained speech more naturally than a baseline model."
   ],
   "p1": 4928,
   "pn": 4932,
   "doi": "10.21437/Interspeech.2024-168",
   "url": "interspeech_2024/yang24c_interspeech.html"
  },
  "suda24_interspeech": {
   "authors": [
    [
     "Hitoshi",
     "Suda"
    ],
    [
     "Aya",
     "Watanabe"
    ],
    [
     "Shinnosuke",
     "Takamichi"
    ]
   ],
   "title": "Who Finds This Voice Attractive? A Large-Scale Experiment Using In-the-Wild Data",
   "original": "173",
   "order": 649,
   "page_count": 5,
   "abstract": [
    "This paper introduces CocoNut-Humoresque, an open-source large-scale speech likability corpus that includes speech segments and their per-listener likability scores. Evaluating voice likability is essential to designing preferable voices for speech systems, such as dialogue or announcement systems. In this study, we let 885 listeners rate 1800 speech segments of a wide range of speakers regarding their likability. When constructing the corpus, we also collected the multiple speaker attributes: genders, ages, and favorite YouTube videos. Therefore, the corpus enables the large-scale statistical analysis of voice likability regarding both speaker and listener factors. This paper describes the construction methodology and preliminary data analysis to reveal the gender and age biases in voice likability. In addition, the relationship between the likability and two acoustic features, the fundamental frequencies and the x-vectors of given utterances, is also investigated."
   ],
   "p1": 3165,
   "pn": 3169,
   "doi": "10.21437/Interspeech.2024-173",
   "url": "interspeech_2024/suda24_interspeech.html"
  },
  "singh24_interspeech": {
   "authors": [
    [
     "Mayank Kumar",
     "Singh"
    ],
    [
     "Naoya",
     "Takahashi"
    ],
    [
     "Weihsiang",
     "Liao"
    ],
    [
     "Yuki",
     "Mitsufuji"
    ]
   ],
   "title": "SilentCipher: Deep Audio Watermarking",
   "original": "174",
   "resource": "https://doi.org/10.5281/zenodo.12792806",
   "order": 462,
   "page_count": 5,
   "abstract": [
    "In the realm of audio watermarking, it is challenging to simultaneously encode imperceptible messages while enhancing the message capacity and robustness. Although recent advancements in deep learning-based methods bolster the message capacity and robustness over traditional methods, the encoded messages introduce audible artefacts that restricts their usage in professional settings. In this study, we introduce three key innovations. Firstly, our work is the first deep learning-based model to integrate psychoacoustic model based thresholding to achieve imperceptible watermarks. Secondly, we introduce psuedo-differentiable compression layers, enhancing the robustness of our watermarking algorithm. Lastly, we introduce a method to eliminate the need for perceptual losses, enabling us to achieve SOTA in both robustness as well as imperceptible watermarking. Our contributions lead us to SilentCipher, a model enabling users to encode messages within audio signals sampled at 44.1kHz."
   ],
   "p1": 2235,
   "pn": 2239,
   "doi": "10.21437/Interspeech.2024-174",
   "url": "interspeech_2024/singh24_interspeech.html"
  },
  "kim24b_interspeech": {
   "authors": [
    [
     "Hyun Myung",
     "Kim"
    ],
    [
     "Kangwook",
     "Jang"
    ],
    [
     "Hoirin",
     "Kim"
    ]
   ],
   "title": "One-class learning with adaptive centroid shift for audio deepfake detection",
   "original": "177",
   "order": 995,
   "page_count": 5,
   "abstract": [
    "As speech synthesis systems continue to make remarkable advances in recent years, the importance of robust deepfake detection systems that perform well in unseen systems has grown. In this paper, we propose a novel adaptive centroid shift (ACS) method that updates the centroid representation by continually shifting as the weighted average of bonafide representations. Our approach uses only bonafide samples to define their centroid, which can yield a specialized centroid for one-class learning. Integrating our ACS with one-class learning gathers bonafide representations into a single cluster, forming well-separated embeddings robust to unseen spoofing attacks. Our proposed method achieves an equal error rate (EER) of 2.19% on the ASVspoof 2021 deepfake dataset, outperforming all existing systems. Furthermore, the t-SNE visualization illustrates that our method effectively maps the bonafide embeddings into a single cluster and successfully disentangles the bonafide and spoof classes."
   ],
   "p1": 4853,
   "pn": 4857,
   "doi": "10.21437/Interspeech.2024-177",
   "url": "interspeech_2024/kim24b_interspeech.html"
  },
  "kaland24_interspeech": {
   "authors": [
    [
     "Constantijn",
     "Kaland"
    ],
    [
     "Jeremy",
     "Steffman"
    ],
    [
     "Jennifer",
     "Cole"
    ]
   ],
   "title": "K-means and hierarchical clustering of f0 contours",
   "original": "181",
   "order": 313,
   "page_count": 5,
   "abstract": [
    "Cluster analysis on time-series f0 data is an increasingly popular method in intonation research. There are a number of methodological decisions to take when applying cluster analysis. Crucially, these decisions may affect the clustering results, potentially also the conclusions of the research. This paper investigates the extent to which the choice for either K-means or hierarchical clustering, two of the most popular clustering methods, leads to grouping differences that are potentially relevant for intonation research. This is tested using a dataset of f0 measures taken from imitated intonation patterns in American English. The analysis concerns a generic correlation test between K-means and hierarchical clustering outcomes as well as a number of specific measures assessing partitioning quality and f0 contour differences. The results show that both cluster methods generally show very similar outcomes, although considerable differences for specific clusterings might occur."
   ],
   "p1": 1520,
   "pn": 1524,
   "doi": "10.21437/Interspeech.2024-181",
   "url": "interspeech_2024/kaland24_interspeech.html"
  },
  "kaland24b_interspeech": {
   "authors": [
    [
     "Constantijn",
     "Kaland"
    ],
    [
     "Maria",
     "Lialiou"
    ]
   ],
   "title": "Quantity-sensitivity affects recall performance of word stress",
   "original": "182",
   "order": 872,
   "page_count": 5,
   "abstract": [
    "Previous studies showed that word stress patterns need to be lexically stored when they are highly variable. Hence, listeners are better at memorizing stress patterns in recall tasks when their native language has variable stress patterns than when their native language has fixed stress patterns. The current study hypothesizes that in quantity-sensitive stress languages, segmental information facilitates stress perception and therefore makes smaller demands on lexical storage than in languages with quantity-insensitive stress patterns. This prediction is tested in two stress recall tasks with German (var. stress, q-sensitive) and Greek (var. stress, q-insensitive) listeners. Results show that German listeners indeed perform worse than Greek listeners. The outcomes provide an important novel perspective on the interaction between segmental and suprasegmental information in speech perception and nuance lexical statistics accounts of word stress."
   ],
   "p1": 4238,
   "pn": 4242,
   "doi": "10.21437/Interspeech.2024-182",
   "url": "interspeech_2024/kaland24b_interspeech.html"
  },
  "ilias24_interspeech": {
   "authors": [
    [
     "Loukas",
     "Ilias"
    ],
    [
     "Dimitris",
     "Askounis"
    ]
   ],
   "title": "A Cross-Attention Layer coupled with Multimodal Fusion Methods for Recognizing Depression from Spontaneous Speech",
   "original": "188",
   "order": 184,
   "page_count": 5,
   "abstract": [
    "Depression is a serious mood disorder, which affects the way people feel and perform daily activities. Speech is a reliable biomarker for diagnosing depression, since depressed people present decreased verbal activity productivity and “lifeless” sounding speech. Existing methods employ unimodal models, use early, intermediate, or late fusion strategies to fuse the different modalities, rely on feature extraction, and perform their approaches only in the English language. This study presents a new method for identifying depression from spontaneous speech in the Italian language, which uses a cross-attention layer for capturing the cross-modal interactions, followed by a variety of multimodal fusion methods. We also perform a multi-task learning framework to explore whether the prediction of age, education level, and gender help in recognizing depression. Findings show that our approach yields multiple advantages over existing approaches reaching Accuracy up to 95.29%."
   ],
   "p1": 912,
   "pn": 916,
   "doi": "10.21437/Interspeech.2024-188",
   "url": "interspeech_2024/ilias24_interspeech.html"
  },
  "paissan24_interspeech": {
   "authors": [
    [
     "Francesco",
     "Paissan"
    ],
    [
     "Elisabetta",
     "Farella"
    ]
   ],
   "title": "tinyCLAP: Distilling Constrastive Language-Audio Pretrained Models",
   "original": "193",
   "order": 346,
   "page_count": 5,
   "abstract": [
    "Contrastive Language-Audio Pretraining (CLAP) became of crucial importance in the field of audio and speech processing. Its employment ranges from sound event detection to text-to-audio generation. However, one of the main limitations is the considerable amount of data required in the training process and the overall computational complexity during inference. This paper investigates how we can reduce the complexity of contrastive language-audio pre-trained models, yielding an efficient model that we call tinyCLAP. We derive an unimodal distillation loss from first principles and explore how the dimensionality of the shared, multimodal latent space can be reduced via pruning. tinyCLAP uses only 6% of the original Microsoft CLAP parameters with a minimal reduction (less than 5%) in zero-shot classification performance across the three sound event detection datasets on which it was tested."
   ],
   "p1": 1685,
   "pn": 1689,
   "doi": "10.21437/Interspeech.2024-193",
   "url": "interspeech_2024/paissan24_interspeech.html"
  },
  "wang24g_interspeech": {
   "authors": [
    [
     "Yiwen",
     "Wang"
    ],
    [
     "Xihong",
     "Wu"
    ]
   ],
   "title": "TSE-PI: Target Sound Extraction under Reverberant Environments with Pitch Information",
   "original": "197",
   "order": 122,
   "page_count": 5,
   "abstract": [
    "Target sound extraction (TSE) separates the target sound from the mixture signals based on provided clues. However, the performance of existing models significantly degrades under reverberant conditions. Inspired by auditory scene analysis (ASA), this work proposes a TSE model provided with pitch information named TSE-PI. Conditional pitch extraction is achieved through the Feature-wise Linearly Modulated layer with the sound-class label. A modified Waveformer model combined with pitch information, employing a learnable Gammatone filterbank in place of the convolutional encoder, is used for target sound extraction. The inclusion of pitch information is aimed at improving the model's performance. The experimental results on the FSD50K dataset illustrate 2.4 dB improvements of target sound extraction under reverberant environments when incorporating pitch information and Gammatone filterbank."
   ],
   "p1": 602,
   "pn": 606,
   "doi": "10.21437/Interspeech.2024-197",
   "url": "interspeech_2024/wang24g_interspeech.html"
  },
  "kwok24_interspeech": {
   "authors": [
    [
     "Chin Yuen",
     "Kwok"
    ],
    [
     "Jia Qi",
     "Yip"
    ],
    [
     "Eng Siong",
     "Chng"
    ]
   ],
   "title": "Continual Learning Optimizations for Auto-regressive Decoder of Multilingual ASR systems",
   "original": "205",
   "order": 254,
   "page_count": 5,
   "abstract": [
    "Continual Learning (CL) involves fine-tuning pre-trained models with new data while maintaining the performance on the pre-trained data. This is particularly relevant for expanding multilingual ASR (MASR) capabilities. However, existing CL methods, mainly designed for computer vision and reinforcement learning tasks, often yield sub-optimal results when directly applied to MASR. We hypothesise that this is because CL of the auto-regressive decoder in the MASR model is difficult. To verify this, we propose four optimizations on the decoder. They include decoder-layer gradient surgery, freezing unused token embeddings, suppressing output of newly added tokens, and learning rate re-scaling. Our experiments on adapting Whisper to 10 unseen languages from the Common Voice dataset demonstrate that these optimizations reduce the Average Word Error Rate (AWER) of pretrained languages from 14.2% to 12.4% compared with Experience Replay, without compromising the AWER of new languages."
   ],
   "p1": 1225,
   "pn": 1229,
   "doi": "10.21437/Interspeech.2024-205",
   "url": "interspeech_2024/kwok24_interspeech.html"
  },
  "gengembre24_interspeech": {
   "authors": [
    [
     "Nicolas",
     "Gengembre"
    ],
    [
     "Olivier",
     "Le Blouch"
    ],
    [
     "Cédric",
     "Gendrot"
    ]
   ],
   "title": "Disentangling prosody and timbre embeddings via voice conversion",
   "original": "207",
   "order": 569,
   "page_count": 5,
   "abstract": [
    "Modern voice conversion and anonymization architectures generally share a design preserving source linguistic content and expressivity while modifying speaker timbre characteristics. This approach leads to a converted signal quite perfectly synchronized with the source signal. In this paper, we hypothesize that this paradigm can help us to quantify the amount of speaker identity preserved in converted voice, refered here as prosody (including speech melody and rhythm). Based on this observation, we propose a method to split and disentangle speaker representation into complementary embeddings conveying respectively prosodic and timbre information. Additionally, we propose a method to evaluate prosody preservation in standard voice privacy architectures and we validate the power of prosodic and timbre embeddings to detect related voice attributes."
   ],
   "p1": 2765,
   "pn": 2769,
   "doi": "10.21437/Interspeech.2024-207",
   "url": "interspeech_2024/gengembre24_interspeech.html"
  },
  "murata24_interspeech": {
   "authors": [
    [
     "Masato",
     "Murata"
    ],
    [
     "Koichi",
     "Miyazaki"
    ],
    [
     "Tomoki",
     "Koriyama"
    ]
   ],
   "title": "An Attribute Interpolation Method in Speech Synthesis by Model Merging",
   "original": "208",
   "resource": "https://doi.org/10.5281/zenodo.12786814",
   "order": 692,
   "page_count": 5,
   "abstract": [
    "With the development of speech synthesis, recent research has focused on challenging tasks, such as speaker generation and emotion intensity control. Attribute interpolation is a common approach to these tasks. However, most previous methods for attribute interpolation require specific modules or training methods. We propose an attribute interpolation method in speech synthesis by model merging. Model merging is a method that creates new parameters by only averaging the parameters of base models. The merged model can generate an output with an intermediate feature of the base models. This method is easily applicable without specific modules or training methods, as it uses only existing trained base models. We merged two text-to-speech models to achieve attribute interpolation and evaluated its performance on speaker generation and emotion intensity control tasks. As a result, our proposed method achieved smooth attribute interpolation while keeping the linguistic content in both tasks."
   ],
   "p1": 3380,
   "pn": 3384,
   "doi": "10.21437/Interspeech.2024-208",
   "url": "interspeech_2024/murata24_interspeech.html"
  },
  "wang24h_interspeech": {
   "authors": [
    [
     "Quan",
     "Wang"
    ],
    [
     "Yiling",
     "Huang"
    ],
    [
     "Guanlong",
     "Zhao"
    ],
    [
     "Evan",
     "Clark"
    ],
    [
     "Wei",
     "Xia"
    ],
    [
     "Hank",
     "Liao"
    ]
   ],
   "title": "DiarizationLM: Speaker Diarization Post-Processing with Large Language Models",
   "original": "209",
   "order": 774,
   "page_count": 5,
   "abstract": [
    "In this paper, we introduce DiarizationLM, a framework to leverage large language models (LLM) to post-process the outputs from a speaker diarization system. In this framework, the outputs of the automatic speech recognition (ASR) and speaker diarization systems are represented as a compact textual format, which is included in the prompt to an optionally finetuned LLM. The outputs of the LLM can be used as the refined diarization results with the desired enhancement. As a post-processing step, this framework can be easily applied to any off-the-shelf ASR and speaker diarization systems without retraining existing components. Our experiments show that a finetuned PaLM 2-S model can reduce the WDER by rel. 55.5% on the Fisher telephone conversation dataset, and rel. 44.9% on the Callhome English dataset."
   ],
   "p1": 3754,
   "pn": 3758,
   "doi": "10.21437/Interspeech.2024-209",
   "url": "interspeech_2024/wang24h_interspeech.html"
  },
  "bujnowski24_interspeech": {
   "authors": [
    [
     "Pawel",
     "Bujnowski"
    ],
    [
     "Bartlomiej",
     "Kuzma"
    ],
    [
     "Bartlomiej",
     "Paziewski"
    ],
    [
     "Jacek",
     "Rutkowski"
    ],
    [
     "Joanna",
     "Marhula"
    ],
    [
     "Zuzanna",
     "Bordzicka"
    ],
    [
     "Piotr",
     "Andruszkiewicz"
    ]
   ],
   "title": "SAMSEMO: New dataset for multilingual and multimodal emotion recognition",
   "original": "212",
   "resource": "https://doi.org/10.5281/zenodo.12773193",
   "order": 601,
   "page_count": 5,
   "abstract": [
    "The task of emotion recognition using image, audio and text modalities has recently attained popularity due to its various potential applications. However, the list of large-scale multimodal datasets is very short and all available datasets have significant limitations. We present SAMSEMO, a novel dataset for multimodal and multilingual emotion recognition. Our collection of over 23k video scenes is multilingual as it includes video scenes in 5 languages (EN, DE, ES, PL and KO). Video scenes are heterogeneous, they come from diverse sources and are accompanied with rich manually collected metadata and emotion annotations. In the paper, we also study the valence and arousal of audio features of our data for the most important emotion classes and compare them with the features of CMU-MOSEI data. Moreover, we perform multimodal experiments for emotion recognition with SAMSEMO and show how to use a multilingual model to improve the detection of imbalanced classes."
   ],
   "p1": 2925,
   "pn": 2929,
   "doi": "10.21437/Interspeech.2024-212",
   "url": "interspeech_2024/bujnowski24_interspeech.html"
  },
  "nam24_interspeech": {
   "authors": [
    [
     "Hyeonuk",
     "Nam"
    ],
    [
     "Seong-Hu",
     "Kim"
    ],
    [
     "Deokki",
     "Min"
    ],
    [
     "Junhyeok",
     "Lee"
    ],
    [
     "Yong-Hwa",
     "Park"
    ]
   ],
   "title": "Diversifying and Expanding Frequency-Adaptive Convolution Kernels for Sound Event Detection",
   "original": "216",
   "order": 21,
   "page_count": 5,
   "abstract": [
    "Frequency dynamic convolution (FDY conv) has shown the state-of-the-art performance in sound event detection (SED) using frequency-adaptive kernels obtained by frequency-varying combination of basis kernels. However, FDY conv lacks an explicit mean to diversify frequency-adaptive kernels, potentially limiting the performance. In addition, size of basis kernels is limited while time-frequency patterns span larger spectro-temporal range. Therefore, we propose dilated frequency dynamic convolution (DFD conv) which diversifies and expands frequency-adaptive kernels by introducing different dilation sizes to basis kernels. Experiments showed advantages of varying dilation sizes along frequency dimension, and analysis on attention weight variance proved dilated basis kernels are effectively diversified. By adapting class-wise median filter with intersection-based F1 score, proposed DFD-CRNN outperforms FDY-CRNN by 3.12% in terms of polyphonic sound detection score (PSDS)."
   ],
   "p1": 97,
   "pn": 101,
   "doi": "10.21437/Interspeech.2024-216",
   "url": "interspeech_2024/nam24_interspeech.html"
  },
  "lee24c_interspeech": {
   "authors": [
    [
     "Jihwan",
     "Lee"
    ],
    [
     "Aditya",
     "Kommineni"
    ],
    [
     "Tiantian",
     "Feng"
    ],
    [
     "Kleanthis",
     "Avramidis"
    ],
    [
     "Xuan",
     "Shi"
    ],
    [
     "Sudarsana Reddy",
     "Kadiri"
    ],
    [
     "Shrikanth",
     "Narayanan"
    ]
   ],
   "title": "Toward Fully-End-to-End Listened Speech Decoding from EEG Signals",
   "original": "223",
   "order": 309,
   "page_count": 5,
   "abstract": [
    "Speech decoding from EEG signals is a challenging task, where brain activity is modeled to estimate salient characteristics of acoustic stimuli. We propose FESDE, a novel framework for Fully-End-to-end Speech Decoding from EEG signals. Our approach aims to directly reconstruct listened speech waveforms given EEG signals, where no intermediate acoustic feature processing step is required. The proposed method consists of an EEG module and a speech module along with a connector. The EEG module learns to better represent EEG signals, while the speech module generates speech waveforms from model representations. The connector learns to bridge the distributions of the latent spaces of EEG and speech. The proposed framework is both simple and efficient, by allowing single-step inference, and outperforms prior works on objective metrics. A fine-grained phoneme analysis is conducted to unveil model characteristics of speech decoding. The source code is available here: github.com/lee-jhwn/fesde."
   ],
   "p1": 1500,
   "pn": 1504,
   "doi": "10.21437/Interspeech.2024-223",
   "url": "interspeech_2024/lee24c_interspeech.html"
  },
  "chen24d_interspeech": {
   "authors": [
    [
     "Tuochao",
     "Chen"
    ],
    [
     "Qirui",
     "Wang"
    ],
    [
     "Bohan",
     "Wu"
    ],
    [
     "Malek",
     "Itani"
    ],
    [
     "Emre Sefik",
     "Eskimez"
    ],
    [
     "Takuya",
     "Yoshioka"
    ],
    [
     "Shyamnath",
     "Gollakota"
    ]
   ],
   "title": "Target conversation extraction: Source separation using turn-taking dynamics",
   "original": "225",
   "order": 726,
   "page_count": 5,
   "abstract": [
    "Extracting the speech of participants in a conversation amidst interfering speakers and noise presents a challenging problem. In this paper, we introduce the novel task of target conversation extraction, where the goal is to extract the audio of a target conversation based on the speaker embedding of one of its participants. To accomplish this, we propose leveraging temporal patterns inherent in human conversations, particularly turn-taking dynamics, which uniquely characterize speakers engaged in conversation and distinguish them from interfering speakers and noise. Using neural networks, we show  the feasibility of our approach on English and Mandarin conversation datasets. In the presence of interfering speakers, our results show an 8.19 dB improvement in signal-to-noise ratio for 2-speaker conversations and a 7.92 dB improvement for 2-4-speaker conversations."
   ],
   "p1": 3550,
   "pn": 3554,
   "doi": "10.21437/Interspeech.2024-225",
   "url": "interspeech_2024/chen24d_interspeech.html"
  },
  "jung24_interspeech": {
   "authors": [
    [
     "Youngmoon",
     "Jung"
    ],
    [
     "Seungjin",
     "Lee"
    ],
    [
     "Joon-Young",
     "Yang"
    ],
    [
     "Jaeyoung",
     "Roh"
    ],
    [
     "Chang Woo",
     "Han"
    ],
    [
     "Hoon-Young",
     "Cho"
    ]
   ],
   "title": "Relational Proxy Loss for Audio-Text based Keyword Spotting",
   "original": "229",
   "order": 67,
   "page_count": 5,
   "abstract": [
    "In recent years, there has been an increasing focus on user convenience, leading to increased interest in text-based keyword enrollment systems for keyword spotting (KWS). Since the system utilizes text input during the enrollment phase and audio input during actual usage, we call this task audio-text based KWS. To enable this task, both acoustic and text encoders are typically trained using deep metric learning loss functions, such as triplet- and proxy-based losses. This study aims to improve existing methods by leveraging the structural relations within acoustic embeddings and within text embeddings. Unlike previous studies that only compare acoustic and text embeddings on a point-to-point basis, our approach focuses on the relational structures within the embedding space by introducing the concept of Relational Proxy Loss (RPL). By incorporating RPL, we demonstrated improved performance on the Wall Street Journal (WSJ) corpus."
   ],
   "p1": 327,
   "pn": 331,
   "doi": "10.21437/Interspeech.2024-229",
   "url": "interspeech_2024/jung24_interspeech.html"
  },
  "lee24d_interspeech": {
   "authors": [
    [
     "Jaejun",
     "Lee"
    ],
    [
     "Yoori",
     "Oh"
    ],
    [
     "Injune",
     "Hwang"
    ],
    [
     "Kyogu",
     "Lee"
    ]
   ],
   "title": "Hear Your Face: Face-based voice conversion with F0 estimation",
   "original": "232",
   "order": 900,
   "page_count": 5,
   "abstract": [
    "This paper delves into the emerging field of face-based voice conversion, leveraging the unique relationship between an individual's facial features and their vocal characteristics. We present a novel face-based voice conversion framework that particularly utilizes the average fundamental frequency of the target speaker, derived solely from their facial images. Through extensive analysis, our framework demonstrates superior speech generation quality and the ability to align facial features with voice characteristics, including tracking of the target speaker's fundamental frequency."
   ],
   "p1": 4378,
   "pn": 4382,
   "doi": "10.21437/Interspeech.2024-232",
   "url": "interspeech_2024/lee24d_interspeech.html"
  },
  "park24_interspeech": {
   "authors": [
    [
     "Eunik",
     "Park"
    ],
    [
     "Daehyun",
     "Ahn"
    ],
    [
     "Hyungjun",
     "Kim"
    ]
   ],
   "title": "RepTor: Re-parameterizable Temporal Convolution for Keyword Spotting via Differentiable Kernel Search",
   "original": "233",
   "order": 928,
   "page_count": 5,
   "abstract": [
    "Keyword spotting (KWS) is an essential in speech recognition, particularly for smart device interfaces. However, it is challenging to design KWS models that are both efficient and accurate, especially for mobile devices with limited resources. In this paper, we propose RepTor-family models, where a structural re-parameterization technique was applied to temporal convolutions in order to improve the efficiency of KWS systems in various environments. The re-parameterized RepTor models have a single-branch plain feed-forward architecture without any residual connections and, therefore, can be efficiently computed on mobile devices. We further propose a latency-aware differentiable kernel search method to find the optimal kernel size for each convolution layer. The proposed RepTor-A~E models achieve 97.9% top-1 accuracy on the Google Speech Command dataset v2 with a latency of 183ms on the Galaxy S10. They outperform state-of-the-art KWS models regarding the accuracy-latency trade-off."
   ],
   "p1": 4518,
   "pn": 4522,
   "doi": "10.21437/Interspeech.2024-233",
   "url": "interspeech_2024/park24_interspeech.html"
  },
  "lee24e_interspeech": {
   "authors": [
    [
     "Seonwoo",
     "Lee"
    ],
    [
     "Sunhee",
     "Kim"
    ],
    [
     "Minhwa",
     "Chung"
    ]
   ],
   "title": "Automatic Assessment of Speech Production Skills for Children with Cochlear Implants Using Wav2Vec2.0 Acoustic Embeddings",
   "original": "234",
   "order": 174,
   "page_count": 5,
   "abstract": [
    "This study introduces an automatic assessment model for speech production skills of children with cochlear implants (CIs) to support home-based speech therapy. The model employs acoustic embeddings from self-supervised models and considers speech traits of both normal hearing (NH) adults and children, which is a novel method for evaluating speech of children with disorders. It combines phoneme embeddings and two acoustic embeddings from Wav2Vec2.0 models, each trained on the speech of NH adults and children, via multi-head attention. Using a speech corpus of Korean-speaking children with CIs, our model outperforms single-embedding methods in a Pearson correlation coefficient between predicted and expert-rated scores, with a relative improvement of 51%. The results highlight the effectiveness of Wav2Vec2.0 acoustic embeddings and the importance of incorporating both of typical speech patterns of NH adults and children in assessing speech production skills in children with CIs."
   ],
   "p1": 862,
   "pn": 866,
   "doi": "10.21437/Interspeech.2024-234",
   "url": "interspeech_2024/lee24e_interspeech.html"
  },
  "munakata24_interspeech": {
   "authors": [
    [
     "Hokuto",
     "Munakata"
    ],
    [
     "Ryo",
     "Terashima"
    ],
    [
     "Yusuke",
     "Fujita"
    ]
   ],
   "title": "Song Data Cleansing for End-to-End Neural Singer Diarization Using Neural Analysis and Synthesis Framework",
   "original": "235",
   "order": 342,
   "page_count": 5,
   "abstract": [
    "We propose a data cleansing method that utilizes a neural analysis and synthesis (NANSY++) framework to train an end-to-end neural diarization model (EEND) for singer diarization. Our proposed model converts song data with choral singing commonly contained in popular music and unsuitable for generating a simulated dataset to the solo singing data. This cleansing is based on NANSY++, which is a framework trained to reconstruct an input non-overlapped audio signal. We exploit the pre-trained NANSY++ to convert choral singing into clean, non-overlapped audio. This cleansing process mitigates the mislabeling of choral singing to solo singing and helps the effective training of EEND models even when the majority of available song data contains choral singing sections. We experimentally evaluated the EEND model trained with a dataset using our proposed method using annotated popular duet songs. As a result, our proposed method improved 14.8 points in diarization error rate."
   ],
   "p1": 1665,
   "pn": 1669,
   "doi": "10.21437/Interspeech.2024-235",
   "url": "interspeech_2024/munakata24_interspeech.html"
  },
  "dinkel24_interspeech": {
   "authors": [
    [
     "Heinrich",
     "Dinkel"
    ],
    [
     "Zhiyong",
     "Yan"
    ],
    [
     "Yongqing",
     "Wang"
    ],
    [
     "Junbo",
     "Zhang"
    ],
    [
     "Yujun",
     "Wang"
    ],
    [
     "Bin",
     "Wang"
    ]
   ],
   "title": "Streaming Audio Transformers for Online Audio Tagging",
   "original": "242",
   "order": 238,
   "page_count": 5,
   "abstract": [
    "Transformers have emerged as a prominent model framework for audio tagging (AT), boasting state-of-the-art (SOTA) performance on the widely-used Audioset dataset. However, their impressive performance often comes at the cost of high memory usage, slow inference speed, and considerable model delay, rendering them impractical for real-world AT applications. In this study, we introduce streaming audio transformers (SAT) that combine the vision transformer (ViT) architecture with Transformer-Xl-like chunk processing, enabling efficient processing of long-range audio signals. Our proposed SAT is benchmarked against other transformer-based SOTA methods, achieving significant improvements in terms of mean average precision (mAP) at a delay of 2s and 1s, while also exhibiting significantly lower memory usage and computational overhead."
   ],
   "p1": 1145,
   "pn": 1149,
   "doi": "10.21437/Interspeech.2024-242",
   "url": "interspeech_2024/dinkel24_interspeech.html"
  },
  "meng24_interspeech": {
   "authors": [
    [
     "Qinglin",
     "Meng"
    ],
    [
     "Min",
     "Liu"
    ],
    [
     "Kaixun",
     "Huang"
    ],
    [
     "Kun",
     "Wei"
    ],
    [
     "Lei",
     "Xie"
    ],
    [
     "Zongfeng",
     "Quan"
    ],
    [
     "Weihong",
     "Deng"
    ],
    [
     "Quan",
     "Lu"
    ],
    [
     "Ning",
     "Jiang"
    ],
    [
     "Guoqing",
     "Zhao"
    ]
   ],
   "title": "SEQ-former: A context-enhanced and efficient automatic speech recognition framework",
   "original": "243",
   "order": 44,
   "page_count": 5,
   "abstract": [
    "Contextual information is crucial for automatic speech recognition (ASR). Effective utilization of contextual information can improve the accuracy of ASR systems. To improve the model's ability to capture this information, we propose a novel ASR framework called SEQ-former, emphasizing simplicity, efficiency, and quickness. We incorporate a Prediction Decoder Network and a Shared Prediction Decoder Network to enhance contextual capabilities. To further increase efficiency, we use intermediate CTC and CTC Spike Reduce Methods to guide attention masks and reduce redundant peaks. Our approach demonstrates state-of-the-art performance on the AiShell-1 dataset, improves decoding efficiency, and delivers competitive results on LibriSpeech. Additionally, it optimizes 6.3% over 11,000 hours of private data compared to Efficient Conformer."
   ],
   "p1": 212,
   "pn": 216,
   "doi": "10.21437/Interspeech.2024-243",
   "url": "interspeech_2024/meng24_interspeech.html"
  },
  "fujita24_interspeech": {
   "authors": [
    [
     "Yusuke",
     "Fujita"
    ],
    [
     "Tatsuya",
     "Komatsu"
    ]
   ],
   "title": "Audio Fingerprinting with Holographic Reduced Representations",
   "original": "245",
   "order": 14,
   "page_count": 5,
   "abstract": [
    "This paper proposes an audio fingerprinting model with holographic reduced representation (HRR). The proposed method reduces the number of stored fingerprints, whereas conventional neural audio fingerprinting requires many fingerprints for each audio track to achieve high accuracy and time resolution. We utilize HRR to aggregate multiple fingerprints into a composite fingerprint via circular convolution and summation, resulting in fewer fingerprints with the same dimensional space as the original. Our search method efficiently finds a combined fingerprint in which a query fingerprint exists. Using HRR's inverse operation, it can recover the relative position within a combined fingerprint, retaining the original time resolution. Experiments show that our method can reduce the number of fingerprints with modest accuracy degradation while maintaining the time resolution, outperforming simple decimation and summation-based aggregation methods."
   ],
   "p1": 62,
   "pn": 66,
   "doi": "10.21437/Interspeech.2024-245",
   "url": "interspeech_2024/fujita24_interspeech.html"
  },
  "dinkel24b_interspeech": {
   "authors": [
    [
     "Heinrich",
     "Dinkel"
    ],
    [
     "Zhiyong",
     "Yan"
    ],
    [
     "Yongqing",
     "Wang"
    ],
    [
     "Junbo",
     "Zhang"
    ],
    [
     "Yujun",
     "Wang"
    ],
    [
     "Bin",
     "Wang"
    ]
   ],
   "title": "Scaling up masked audio encoder learning for general audio classification",
   "original": "246",
   "order": 111,
   "page_count": 5,
   "abstract": [
    "Despite progress in audio classification, a generalization gap remains between speech and other sound domains, such as environmental sounds and music. Models trained for speech tasks often fail to perform well on environmental or musical audio tasks, and vice versa. While self-supervised (SSL) audio representations offer an alternative, there has been limited exploration of scaling both model and dataset sizes for SSL-based general audio classification. We introduce Dasheng, a simple SSL audio encoder, based on the efficient masked autoencoder framework.  Trained with 1.2 billion parameters on 272,356 hours of diverse audio, Dasheng obtains significant performance gains on the HEAR benchmark. It outperforms previous works on CREMA-D, LibriCount, Speech Commands, VoxLingua, and competes well in music and environment classification. Dasheng features inherently contain rich speech, music, and environmental information, as shown in nearest-neighbor classification experiments."
   ],
   "p1": 547,
   "pn": 551,
   "doi": "10.21437/Interspeech.2024-246",
   "url": "interspeech_2024/dinkel24b_interspeech.html"
  },
  "muller24b_interspeech": {
   "authors": [
    [
     "Nicolas M.",
     "Müller"
    ],
    [
     "Nicholas",
     "Evans"
    ],
    [
     "Hemlata",
     "Tak"
    ],
    [
     "Philip",
     "Sperl"
    ],
    [
     "Konstantin",
     "Böttinger"
    ]
   ],
   "title": "Harder or Different? Understanding Generalization of Audio Deepfake Detection",
   "original": "247",
   "order": 557,
   "page_count": 5,
   "abstract": [
    "Recent research has highlighted a key issue in speech deepfake detection: models trained on one set of deepfakes perform poorly on others. The question arises: is this due to the continuously improving quality of text-to-speech (TTS) models, i.e., are newer DeepFakes just ‘harder’ to detect? Or, is it because deepfakes generated with one model are fundamentally different to those generated using another model? We answer this question by decomposing the performance gap between in-domain and out-of-domain test data into ‘hardness’ and ‘difference’ components. Experiments performed using ASVspoof databases indicate that the hardness component is practically negligible, with the performance gap being attributed primarily to the difference component. This has direct implications for real-world deepfake detection, highlighting that merely increasing model capacity, the currently-dominant research trend, may not effectively address the generalization challenge."
   ],
   "p1": 2705,
   "pn": 2709,
   "doi": "10.21437/Interspeech.2024-247",
   "url": "interspeech_2024/muller24b_interspeech.html"
  },
  "chen24e_interspeech": {
   "authors": [
    [
     "Shihao",
     "Chen"
    ],
    [
     "Yu",
     "Gu"
    ],
    [
     "Jie",
     "Zhang"
    ],
    [
     "Na",
     "Li"
    ],
    [
     "Rilin",
     "Chen"
    ],
    [
     "Liping",
     "Chen"
    ],
    [
     "Lirong",
     "Dai"
    ]
   ],
   "title": "LDM-SVC: Latent Diffusion Model Based Zero-Shot Any-to-Any Singing Voice Conversion with Singer Guidance",
   "original": "250",
   "order": 570,
   "page_count": 5,
   "abstract": [
    "Any-to-any singing voice conversion (SVC) is an interesting audio editing technique, aiming to convert the singing voice of one singer into that of another, given only a few seconds of singing data. However, during the conversion process, the issue of timbre leakage is inevitable: the converted singing voice still sounds like the original singer's voice. To tackle this, we propose a latent diffusion model for SVC (LDM-SVC)  in this work, which attempts to perform SVC in the latent space using an LDM. We pretrain a variational autoencoder structure using the noted open-source So-VITS-SVC project based on the VITS framework, which is then used for the LDM training. Besides, we propose a singer guidance training method based on  classifier-free guidance to further suppress the timbre of the original singer. Experimental results show the superiority of  the proposed method over previous works in both subjective and objective evaluations of timbre similarity."
   ],
   "p1": 2770,
   "pn": 2774,
   "doi": "10.21437/Interspeech.2024-250",
   "url": "interspeech_2024/chen24e_interspeech.html"
  },
  "wu24b_interspeech": {
   "authors": [
    [
     "Haochen",
     "Wu"
    ],
    [
     "Wu",
     "Guo"
    ],
    [
     "Zhentao",
     "Zhang"
    ],
    [
     "Wenting",
     "Zhao"
    ],
    [
     "Shengyu",
     "Peng"
    ],
    [
     "Jie",
     "Zhang"
    ]
   ],
   "title": "Spoofing Speech Detection by Modeling Local Spectro-Temporal and Long-term Dependency",
   "original": "251",
   "order": 103,
   "page_count": 5,
   "abstract": [
    "In this work, a dual-branch network is proposed to exploit both local and global information of utterances for spoofing speech detection (SSD). The local artifacts of spoofing speech can reside in specific temporal or spectral regions, which are the primary objectives for SSD systems. We propose a spectro-temporal graph attention network to jointly capture the temporal and spectral differences of the spoofing speech. It is different from existing methods that the proposed method exploits the cross attention mechanism to bridge the spectro-temporal dependency. As the global artifacts can also provide complimentary information for SSD, we use a BiLSTM-based branch to modeling temporal long-term discriminative clues. These two branches are then separately optimized with the weighted cross-entropy loss, and the scores are fused at equal weights. Results  on three benchmark datasets (i.e., ASVspoof 2019, 2021 LA and 2021 DF) reveal the superiority of the proposed method over advanced systems."
   ],
   "p1": 507,
   "pn": 511,
   "doi": "10.21437/Interspeech.2024-251",
   "url": "interspeech_2024/wu24b_interspeech.html"
  },
  "wu24c_interspeech": {
   "authors": [
    [
     "Haochen",
     "Wu"
    ],
    [
     "Wu",
     "Guo"
    ],
    [
     "Shengyu",
     "Peng"
    ],
    [
     "Zhuhai",
     "Li"
    ],
    [
     "Jie",
     "Zhang"
    ]
   ],
   "title": "Adapter Learning from Pre-trained Model for Robust Spoof Speech Detection",
   "original": "253",
   "order": 434,
   "page_count": 5,
   "abstract": [
    "Speech anti-spoofing models can be improved by using large pre-trained model as front-end, e.g., Wav2vec2 or WavLM. However, apart from the heavy computation overhead, fine-tuning of pre-trained model is prone to over-fitting and catastrophic forgetting due to limited training data. In this paper, we propose an novel adapter learning framework based on pre-trained model for robust spoof speech detection. We consider two adapter cases, i.e., intra-block adapters and cross-block adapters, which are inserted or appended to the backbone Wav2vec2. The parameters of the adapters are updated by freezing the backbone during training. The local-global task-dependent information for spoof speech detection is obtained via the proposed adapter learning with a marginal increase of parameters. Results on three benchmark datasets validate the superiority over the baseline and existing SOTA systems."
   ],
   "p1": 2095,
   "pn": 2099,
   "doi": "10.21437/Interspeech.2024-253",
   "url": "interspeech_2024/wu24c_interspeech.html"
  },
  "xie24_interspeech": {
   "authors": [
    [
     "Yuankun",
     "Xie"
    ],
    [
     "Ruibo",
     "Fu"
    ],
    [
     "Zhengqi",
     "Wen"
    ],
    [
     "Zhiyong",
     "Wang"
    ],
    [
     "Xiaopeng",
     "Wang"
    ],
    [
     "Haonnan",
     "Cheng"
    ],
    [
     "Long",
     "Ye"
    ],
    [
     "Jianhua",
     "Tao"
    ]
   ],
   "title": "Generalized Source Tracing: Detecting Novel Audio Deepfake Algorithm with Real Emphasis and Fake Dispersion Strategy",
   "original": "254",
   "order": 991,
   "page_count": 5,
   "abstract": [
    "With the proliferation of deepfake audio, there is an urgent need to investigate their attribution. Current source tracing methods can effectively distinguish in-distribution (ID) categories. However, the rapid evolution of deepfake algorithms poses a critical challenge in the accurate identification of outof-distribution (OOD) novel deepfake algorithms. In this paper, we propose Real Emphasis and Fake Dispersion (REFD) strategy for audio deepfake algorithm recognition, demonstrating its effectiveness in discriminating ID samples while identifying OOD samples. For effective OOD detection, we first explore current post-hoc OOD methods and propose NSD, a novel OOD approach in identifying novel deepfake algorithms through the similarity consideration of both feature and logits scores. REFD achieves 86.83% F1-score as a single system in Audio Deepfake Detection Challenge 2023 Track3, showcasing its state-of-the-art performance."
   ],
   "p1": 4833,
   "pn": 4837,
   "doi": "10.21437/Interspeech.2024-254",
   "url": "interspeech_2024/xie24_interspeech.html"
  },
  "du24_interspeech": {
   "authors": [
    [
     "Hui-Peng",
     "Du"
    ],
    [
     "Ye-Xin",
     "Lu"
    ],
    [
     "Yang",
     "Ai"
    ],
    [
     "Zhen-Hua",
     "Ling"
    ]
   ],
   "title": "BiVocoder: A Bidirectional Neural Vocoder Integrating Feature Extraction and Waveform Generation",
   "original": "255",
   "order": 802,
   "page_count": 5,
   "abstract": [
    "This paper proposes a novel bidirectional neural vocoder, named BiVocoder, capable both of feature extraction and reverse waveform generation within the short-time Fourier transform (STFT) domain. For feature extraction, the BiVocoder takes amplitude and phase spectra derived from STFT as inputs, transforms them into long-frame-shift and low-dimensional features through convolutional neural networks. The extracted features are demonstrated suitable for direct prediction by acoustic models, supporting its application in text-to-speech (TTS) task. For waveform generation, the BiVocoder restores amplitude and phase spectra from the features by a symmetric network, followed by inverse STFT to reconstruct the speech waveform. Experimental results show that our proposed BiVocoder achieves better performance compared to some baseline vocoders, by comprehensively considering both synthesized speech quality and inference speed for both analysis-synthesis and TTS tasks. "
   ],
   "p1": 3894,
   "pn": 3898,
   "doi": "10.21437/Interspeech.2024-255",
   "url": "interspeech_2024/du24_interspeech.html"
  },
  "honda24_interspeech": {
   "authors": [
    [
     "Tomoki",
     "Honda"
    ],
    [
     "Shinsuke",
     "Sakai"
    ],
    [
     "Tatsuya",
     "Kawahara"
    ]
   ],
   "title": "Efficient and Robust Long-Form Speech Recognition with Hybrid H3-Conformer",
   "original": "258",
   "order": 595,
   "page_count": 5,
   "abstract": [
    "Recently, Conformer has achieved state-of-the-art performance in many speech recognition tasks. However, the Transformer-based models show significant deterioration for long-form speech, such as lectures, because the self-attention mechanism becomes unreliable with the computation of the square order of the input length. To solve the problem, we incorporate a kind of state-space model, Hungry Hungry Hippos (H3), to replace or complement the multi-head self-attention (MHSA). H3 allows for efficient modeling of long-form sequences with a linear-order computation. In experiments using two datasets of CSJ and LibriSpeech, our proposed H3-Conformer model performs efficient and robust recognition of long-form speech. Moreover, we propose a hybrid of H3 and MHSA and show that using H3 in higher layers and MHSA in lower layers provides significant improvement in online recognition. We also investigate a parallel use of H3 and MHSA in all layers, resulting in the best performance."
   ],
   "p1": 2895,
   "pn": 2899,
   "doi": "10.21437/Interspeech.2024-258",
   "url": "interspeech_2024/honda24_interspeech.html"
  },
  "ye24_interspeech": {
   "authors": [
    [
     "Shuaishuai",
     "Ye"
    ],
    [
     "Shunfei",
     "Chen"
    ],
    [
     "Xinhui",
     "Hu"
    ],
    [
     "Xinkang",
     "Xu"
    ]
   ],
   "title": "SC-MoE: Switch Conformer Mixture of Experts for Unified Streaming and Non-streaming Code-Switching ASR",
   "original": "259",
   "order": 823,
   "page_count": 5,
   "abstract": [
    "In this work, we propose a Switch-Conformer-based MoE system named SC-MoE for unified streaming and non-streaming code-switching (CS) automatic speech recognition (ASR), where we design a streaming MoE layer consisting of three language experts, which correspond to Mandarin, English, and blank, respectively, and equipped with a language identification (LID) network with a Connectionist Temporal Classification (CTC) loss as a router in the encoder of SC-MoE to achieve a real-time streaming CS ASR system. To further utilize the language information embedded in text, we also incorporate MoE layers into the decoder of SC-MoE. In addition, we introduce routers into every MoE layer of the encoder and the decoder and achieve better recognition performance. Experimental results show that the SC-MoE significantly improves CS ASR performances over baseline with comparable computational efficiency."
   ],
   "p1": 3999,
   "pn": 4003,
   "doi": "10.21437/Interspeech.2024-259",
   "url": "interspeech_2024/ye24_interspeech.html"
  },
  "peng24_interspeech": {
   "authors": [
    [
     "Shengyu",
     "Peng"
    ],
    [
     "Wu",
     "Guo"
    ],
    [
     "Haochen",
     "Wu"
    ],
    [
     "Zuoliang",
     "Li"
    ],
    [
     "Jie",
     "Zhang"
    ]
   ],
   "title": "Fine-tune Pre-Trained Models with Multi-Level Feature Fusion for Speaker Verification",
   "original": "260",
   "order": 437,
   "page_count": 5,
   "abstract": [
    "In this paper, we consider speaker verification by fine-tuning the pre-trained model (PTM) with multi-level features, including multi-layer features from PTM and hand-crafted features. The proposed framework contains a PTM as front-end and a Dual-Branch ECAPA-TDNN (DBE) back-end. For the front-end, we propose an attention-based fusion module (AFM) to merge deep and shallow layers of the PTM features. To enhance the performance, we add an auxiliary speaker loss after the last layer of the PTM. For the DBE back-end, each branch of DBE takes aggregated features from PTM and FBank features as input. The AFM is further used to merge the dual-branch features to provide complementary information. Experimental results on VoxCeleb datasets confirm the effectiveness of our proposed method across different PTMs."
   ],
   "p1": 2110,
   "pn": 2114,
   "doi": "10.21437/Interspeech.2024-260",
   "url": "interspeech_2024/peng24_interspeech.html"
  },
  "ling24_interspeech": {
   "authors": [
    [
     "Tongtao",
     "Ling"
    ],
    [
     "Yutao",
     "Lai"
    ],
    [
     "Lei",
     "Chen"
    ],
    [
     "Shilei",
     "Huang"
    ],
    [
     "Yi",
     "Liu"
    ]
   ],
   "title": "A Small and Fast BERT for Chinese Medical Punctuation Restoration",
   "original": "263",
   "order": 931,
   "page_count": 5,
   "abstract": [
    "In clinical dictation, utterances after automatic speech recognition (ASR) without explicit punctuation marks may lead to the misunderstanding of dictated reports. To provide a precise and understandable clinical report with ASR, automatic punctuation restoration (APR) is required. Considering a practical scenario, we propose a fast and lightweight pre-trained model for Chinese medical punctuation restoration based on the ‘pre-training and fine-tuning’ paradigm. In this work, we distill pre-trained models by incorporating supervised contrastive learning and a novel auxiliary pre-training task (Punctuation Mark Prediction) to make it well-suited for punctuation restoration. We then reformulate APR as a slot tagging problem in the fine-tuning stage to bridge the gap between pre-training and fine-tuning. Our experiments on various distilled models reveal that our model can achieve 95% performance with a 10% model size relative to the state-of-the-art Chinese RoBERTa. "
   ],
   "p1": 4533,
   "pn": 4537,
   "doi": "10.21437/Interspeech.2024-263",
   "url": "interspeech_2024/ling24_interspeech.html"
  },
  "yang24d_interspeech": {
   "authors": [
    [
     "Qian",
     "Yang"
    ],
    [
     "Jialong",
     "Zuo"
    ],
    [
     "Zhe",
     "Su"
    ],
    [
     "Ziyue",
     "Jiang"
    ],
    [
     "Mingze",
     "Li"
    ],
    [
     "Zhou",
     "Zhao"
    ],
    [
     "Feiyang",
     "Chen"
    ],
    [
     "Zhefeng",
     "Wang"
    ],
    [
     "Baoxing",
     "Huai"
    ]
   ],
   "title": "MSceneSpeech: A Multi-Scene Speech Dataset For Expressive Speech Synthesis",
   "original": "266",
   "order": 378,
   "page_count": 5,
   "abstract": [
    "We introduce an open source high-quality Mandarin TTS dataset MSceneSpeech (Multiple Scene Speech Dataset), which is intended to provide resources for expressive speech synthesis. MSceneSpeech comprises numerous audio recordings and texts performed and recorded according to daily life scenarios. Each scenario includes multiple speakers and a diverse range of prosodic styles, making it suitable for speech synthesis that entails multi-speaker style and prosody modeling. We have established a robust baseline, through the prompting mechanism, that can effectively synthesize speech characterized by both user-specific timbre and scene-specific prosody with arbitrary text input. The open source MSceneSpeech Dataset and audio samples of our baseline are available at https://speechai-demo.github.io/MSceneSpeech/."
   ],
   "p1": 1845,
   "pn": 1849,
   "doi": "10.21437/Interspeech.2024-266",
   "url": "interspeech_2024/yang24d_interspeech.html"
  },
  "maisonneuve24_interspeech": {
   "authors": [
    [
     "Malo",
     "Maisonneuve"
    ],
    [
     "Corinne",
     "Fredouille"
    ],
    [
     "Muriel",
     "Lalain"
    ],
    [
     "Alain",
     "Ghio"
    ],
    [
     "Virginie",
     "Woisard"
    ]
   ],
   "title": "Towards objective and interpretable speech disorder assessment: a comparative analysis of CNN and transformer-based models",
   "original": "267",
   "order": 403,
   "page_count": 5,
   "abstract": [
    "Head and Neck Cancers (HNC) significantly impact patients' ability to speak, affecting their quality of life. Commonly used metrics for assessing pathological speech are subjective, prompting the need for automated and unbiased evaluation methods. This study proposes a self-supervised Wav2Vec2-based model for phone classification with HNC patients, to enhance accuracy and improve the discrimination of phonetic features for subsequent interpretability purpose. The impact of pre-training datasets, model size, and fine-tuning datasets and parameters are explored. Evaluation on diverse corpora reveals the effectiveness of the Wav2Vec2 architecture, outperforming a CNN-based approach, used in previous work. Correlation with perceptual measures also affirms the model relevance for impaired speech analysis. This work paves the way for better understanding of pathological speech with interpretable approaches for clinicians, by leveraging complex self-learnt speech representations."
   ],
   "p1": 1970,
   "pn": 1974,
   "doi": "10.21437/Interspeech.2024-267",
   "url": "interspeech_2024/maisonneuve24_interspeech.html"
  },
  "setoguchi24_interspeech": {
   "authors": [
    [
     "Ryo",
     "Setoguchi"
    ],
    [
     "Yoshiko",
     "Arimoto"
    ]
   ],
   "title": "Acoustical analysis of the initial phones in speech-laugh",
   "original": "268",
   "order": 650,
   "page_count": 5,
   "abstract": [
    "To elucidate the mechanism underlying the occurrence of speech-laugh, the acoustical difference between an initial phone of speech-laugh and phones during nonlaughing speech was examined by conducting acoustic analyses using two features representing vocal tract and voice source characteristics. First, a two-way analysis of variance (ANOVA) for the first and second formant frequencies (F1 and F2) was performed based on the factors of speech type (speech-laugh vs. speech) and five vowels (/a/, /e/, /i/, /o/, /u/). Second, using vowels and consonants, generalized linear mixed models (GLMMs) were developed with speech type as the objective variable and 12th-order mel-cepstral coefficients as explanatory variables. The ANOVA results revealed that the F1 values of the vowels /a/ and /o/ were greater at the beginning of the speech-laugh than during speech. The GLMM analysis showed that lower-order coefficients contributed to differentiating speech-laugh and speech."
   ],
   "p1": 3170,
   "pn": 3174,
   "doi": "10.21437/Interspeech.2024-268",
   "url": "interspeech_2024/setoguchi24_interspeech.html"
  },
  "shi24b_interspeech": {
   "authors": [
    [
     "Hao",
     "Shi"
    ],
    [
     "Tatsuya",
     "Kawahara"
    ]
   ],
   "title": "Dual-path Adaptation of Pretrained Feature Extraction Module for Robust Automatic Speech Recognition",
   "original": "270",
   "order": 586,
   "page_count": 5,
   "abstract": [
    "Self-supervised learning (SSL)-based pretrained models have significantly improved automatic speech recognition (ASR) performance. As the feature extraction (FE) module has also been well-trained with a large amount of training data, freezing the FE during finetuning for downstream ASR tasks is common. When there is a severe mismatch between the simulated noisy data for pretraining and real noisy data, however, finetuning the FE with the real noisy data should be done without losing the effective information of the pretrained FE. In this paper, we propose a dual-path adaptation of the FE to address this problem. It combines the frozen pretrained FE path and the finetuned-adapted FE path with convolutional fusion layers. Moreover, adapters are inserted into the Transformer encoder. The experimental results using the CHiME–4 dataset show that using adapters for the FE or the Transformer encoder is effective, but achieving synergy of these two is challenging. Finetuning of the FE combined with adapters in the encoder realizes effective model adaptation. Moreover, the proposed method utilizes the complementarity between the pretrained and the finetuned FE paths, achieving significant improvements even with noise-robust WavLM models."
   ],
   "p1": 2850,
   "pn": 2854,
   "doi": "10.21437/Interspeech.2024-270",
   "url": "interspeech_2024/shi24b_interspeech.html"
  },
  "truong24_interspeech": {
   "authors": [
    [
     "Thanh Lan",
     "Truong"
    ],
    [
     "Andrea",
     "Weber"
    ]
   ],
   "title": "Ethnolinguistic Identification of Vietnamese-German Heritage Speech",
   "original": "274",
   "order": 866,
   "page_count": 5,
   "abstract": [
    "We examined whether listeners can identify the heritage of speakers based on their voices. In Experiment 1, native German listeners with and without an Asian heritage listened to sentences that had either been produced by native German speakers with a Vietnamese heritage or by Germans without any Asian heritage. Asian heritage listeners performed more accurately when speakers matched their heritage background, but they could identify German speakers with no Asian heritage with above chance accuracy. Listeners with no Asian heritage performed more accurately when the speakers had no Vietnamese background than when they did, but they could not identify speakers with a Vietnamese heritage with above chance accuracy. In Experiment 2, the pattern for German listeners with no Asian heritage was replicated with monotonized stimuli. Thus, matching heritage background facilitates ethnolinguistic identification for Vietnamese-German heritage speech, which persists even when pitch is absent."
   ],
   "p1": 4209,
   "pn": 4213,
   "doi": "10.21437/Interspeech.2024-274",
   "url": "interspeech_2024/truong24_interspeech.html"
  },
  "niebuhr24_interspeech": {
   "authors": [
    [
     "Oliver",
     "Niebuhr"
    ],
    [
     "Nafiseh",
     "Taghva"
    ]
   ],
   "title": "How rhythm metrics are linked to produced and perceived speaker charisma",
   "original": "277",
   "order": 222,
   "page_count": 5,
   "abstract": [
    "Based on a medium-sized sample of English investor-oriented business-idea presentations (so-called “investor pitches”), the present paper investigates the links between speech rhythm and perceived speaker charisma. Eight trained public speakers are recorded while performing the same investor pitch twice, once in an emotionally-neutral matter-of-fact fashion and once charismatically, i.e. in an expressive, committed onstage presentation style. The recorded presentations were rated by 21 listeners for their degree of perceived speaker charisma – and additionally acoustically analyzed in terms of established duration-based rhythm measures such as ∅, Δ, and PVI. We find significant rhythmic differences between the matter-of-fact and charismatic presentation performances and, in conjunction with the perception results, we show that consonantal rhythmic elements play a bigger role in the perception than in the production of a rhythmic charisma, and that especially the duration variation of larger rhythm elements correlates positively and gender-independently with charisma ratings. The findings are discussed in light of previous studies with their practical implications."
   ],
   "p1": 1065,
   "pn": 1069,
   "doi": "10.21437/Interspeech.2024-277",
   "url": "interspeech_2024/niebuhr24_interspeech.html"
  },
  "amiriparian24_interspeech": {
   "authors": [
    [
     "Shahin",
     "Amiriparian"
    ],
    [
     "Filip",
     "Packań"
    ],
    [
     "Maurice",
     "Gerczuk"
    ],
    [
     "Björn W.",
     "Schuller"
    ]
   ],
   "title": "ExHuBERT: Enhancing HuBERT Through Block Extension and Fine-Tuning on 37 Emotion Datasets",
   "original": "280",
   "resource": "https://doi.org/10.57967/hf/2747",
   "order": 543,
   "page_count": 5,
   "abstract": [
    "Foundation models have shown great promise in speech emotion recognition (SER) by leveraging their pre-trained representations to capture emotion patterns in speech signals. To further enhance SER performance across various languages and domains, we propose a novel twofold approach. First, we gather EmoSet++, a comprehensive multi-lingual, multi-cultural speech emotion corpus with 37 datasets, 150,907 samples, and a total duration of 119.5 hours. Second, we introduce ExHuBERT, an enhanced version of HuBERT achieved by backbone extension and fine-tuning on EmoSet++. We duplicate each encoder layer and its weights, then freeze the first duplicate, integrating an extra zero-initialized linear layer and skip connections to preserve functionality and ensure its adaptability for subsequent fine-tuning. Our evaluation on unseen datasets shows the efficacy of ExHuBERT, setting a new benchmark for various SER tasks. Model and details on EmoSet++: https://huggingface.co/amiriparian/ExHuBERT."
   ],
   "p1": 2635,
   "pn": 2639,
   "doi": "10.21437/Interspeech.2024-280",
   "url": "interspeech_2024/amiriparian24_interspeech.html"
  },
  "lin24_interspeech": {
   "authors": [
    [
     "Jie",
     "Lin"
    ],
    [
     "Xiuping",
     "Yang"
    ],
    [
     "Li",
     "Xiao"
    ],
    [
     "Xinhong",
     "Li"
    ],
    [
     "Weiyan",
     "Yi"
    ],
    [
     "Yuhong",
     "Yang"
    ],
    [
     "Weiping",
     "Tu"
    ],
    [
     "Xiong",
     "Chen"
    ]
   ],
   "title": "SimuSOE: A Simulated Snoring Dataset for Obstructive Sleep Apnea-Hypopnea Syndrome Evaluation during Wakefulness",
   "original": "283",
   "order": 30,
   "page_count": 5,
   "abstract": [
    "Obstructive Sleep Apnea-Hypopnea Syndrome (OSAHS) is a prevalent chronic breathing disorder caused by upper airway obstruction. Previous studies advanced OSAHS evaluation through machine learning-based systems trained on sleep snoring or speech signal datasets. However, constructing datasets for training a precise and rapid OSAHS evaluation system poses a challenge, since 1) it is time-consuming to collect sleep snores and 2) the speech signal is limited in reflecting upper airway obstruction. In this paper, we propose a new snoring dataset for OSAHS evaluation, named SimuSOE, in which a novel and time-effective snoring collection method is introduced for tackling the above problems. In particular, we adopt simulated snoring which is a type of snore intentionally emitted by patients to replace natural snoring. Experimental results indicate that the simulated snoring signal during wakefulness can serve as an effective feature in OSAHS preliminary screening."
   ],
   "p1": 142,
   "pn": 146,
   "doi": "10.21437/Interspeech.2024-283",
   "url": "interspeech_2024/lin24_interspeech.html"
  },
  "messica24_interspeech": {
   "authors": [
    [
     "Shoval",
     "Messica"
    ],
    [
     "Yossi",
     "Adi"
    ]
   ],
   "title": "NAST: Noise Aware Speech Tokenization for Speech Language Models",
   "original": "288",
   "order": 857,
   "page_count": 5,
   "abstract": [
    "Speech tokenization is the task of representing speech signals as a sequence of discrete units. Such representations can be later used for various downstream tasks including automatic speech recognition, text-to-speech, etc. More relevant to this study, such representation serves as the basis of Speech Language Models. In this work, we tackle the task of speech tokenization under the noisy setup and present NAST: Noise Aware Speech Tokenization for Speech Language Models. NAST is composed of three main components: (i) a predictor; (ii) a residual encoder; and (iii) a decoder. We evaluate the efficiency of NAST considering several spoken language modeling tasks and show that NAST is superior to the evaluated baselines across all setups. Lastly, we analyze NAST and show its disentanglement properties and robustness to signal variations in the form of noise, reverberation, pitch-shift, and time-stretch. Code and pre-trained models are available at https://github.com/ShovalMessica/NAST."
   ],
   "p1": 4169,
   "pn": 4173,
   "doi": "10.21437/Interspeech.2024-288",
   "url": "interspeech_2024/messica24_interspeech.html"
  },
  "riegger24_interspeech": {
   "authors": [
    [
     "Chiara",
     "Riegger"
    ],
    [
     "Tina",
     "Bögel"
    ],
    [
     "George",
     "Walkden"
    ]
   ],
   "title": "The prosody of the verbal prefix ge-: historical and experimental evidence",
   "original": "289",
   "order": 428,
   "page_count": 5,
   "abstract": [
    "This study investigates whether prosodic words in West Germanic languages are determined by morphological structure or rhythmic principles, i.e., the trochaic foot. To investigate this question, we looked at the unstressed verbal prefix ge- diachronically and synchronically. A corpus study of three Old English, Old Saxon, and Old High German manuscripts showed that ge- often attaches to preceding auxiliaries and negation particles but not content words. The findings of the corpus study were verified through a production study in Modern German, measuring the closure duration of [g] as an indication of boundary strength between the prefix and the preceding word. Results showed that closure duration is reduced if the verb follows a monosyllabic auxiliary. Furthermore, we found no indicators that the prefix interacts with preceding content words or negation. Both results taken together support the trochaic foot structure based on rhythmic principles, but only in very restricted contexts."
   ],
   "p1": 2065,
   "pn": 2069,
   "doi": "10.21437/Interspeech.2024-289",
   "url": "interspeech_2024/riegger24_interspeech.html"
  },
  "liu24b_interspeech": {
   "authors": [
    [
     "Kexu",
     "Liu"
    ],
    [
     "Yuanxin",
     "Wang"
    ],
    [
     "Shengchen",
     "Li"
    ],
    [
     "Xi",
     "Shao"
    ]
   ],
   "title": "Speech Formants Integration for Generalized Detection of Synthetic Speech Spoofing Attacks",
   "original": "292",
   "order": 435,
   "page_count": 5,
   "abstract": [
    "Existing synthetic speech detection systems struggle with high variance of performance in different spoofing attacks. This is due to the diversity of unseen synthesis algorithms, making it challenging for the system to generalize unseen spoofing attacks. To address this, we propose multi-view features with one-class learning for synthetic speech detection. The key idea is to capture bona-fide speech features from dynamic information of formants and XLS-R dimensions, aiming to compactly represent bona-fide speech in the embedding space without the need to fit various unseen spoofing attacks. To leverage multi-view features, the dynamic information of formants is integrated with XLS-R features using a parallel attention mechanism and gating modulation. Our system achieves an equal error rate (EER) of 0.39% in the ASVspoof 2019 logical access scenario, demonstrating a low performance variance of 0.069 across all 13 attacks, outperforming most mainstream single-systems."
   ],
   "p1": 2100,
   "pn": 2104,
   "doi": "10.21437/Interspeech.2024-292",
   "url": "interspeech_2024/liu24b_interspeech.html"
  },
  "li24e_interspeech": {
   "authors": [
    [
     "Zhe",
     "Li"
    ],
    [
     "Man-wai",
     "Mak"
    ],
    [
     "Hung-yi",
     "Lee"
    ],
    [
     "Helen",
     "Meng"
    ]
   ],
   "title": "Parameter-efficient Fine-tuning of Speaker-Aware Dynamic Prompts for Speaker Verification",
   "original": "295",
   "order": 551,
   "page_count": 5,
   "abstract": [
    "Prompt tuning can effectively reduce tunable parameters in pre-trained Transformers. However, it is weak at capturing speaker traits because the prompts can easily overfit the adaptation utterances, resulting in poor generalization to unseen speakers. This paper introduces a prompt pool comprising learnable prompts to tackle this issue. Unlike the traditional method that learns a fixed set of prompts for each training utterance, our method uses a dynamic selection strategy to select the best matching prompts in a pool for tuning, resulting in each prompt being tuned by its closely matched speaker. The objective is to make the prompts in the pool form speaker clusters, enhancing speaker prediction in the downstream classifier while maintaining the plasticity of the pre-trained Transformers. Our experiments on language mismatch in speaker verification demonstrate that the dynamic prompt pool provides a memory- and computation-efficient solution to fine-tune pre-trained Transformers. "
   ],
   "p1": 2675,
   "pn": 2679,
   "doi": "10.21437/Interspeech.2024-295",
   "url": "interspeech_2024/li24e_interspeech.html"
  },
  "yusuf24_interspeech": {
   "authors": [
    [
     "Bolaji",
     "Yusuf"
    ],
    [
     "Murali Karthick",
     "Baskar"
    ],
    [
     "Andrew",
     "Rosenberg"
    ],
    [
     "Bhuvana",
     "Ramabhadran"
    ]
   ],
   "title": "Speculative Speech Recognition by Audio-Prefixed Low-Rank Adaptation of Language Models",
   "original": "298",
   "order": 160,
   "page_count": 5,
   "abstract": [
    "This paper explores speculative speech recognition (SSR), where we empower conventional automatic speech recognition (ASR) with speculation capabilities, allowing the recognizer to run ahead of audio. We introduce a metric for measuring SSR performance and we propose a model which does SSR by combining a RNN-Transducer-based ASR system with an audio-prefixed language model (LM). The ASR system transcribes ongoing audio and feeds the resulting transcripts, along with an audio-dependent prefix, to the LM, which speculates likely completions for the transcriptions. We experiment with a variety of ASR datasets on which show the efficacy our method and the feasibility of SSR as a method of reducing ASR latency."
   ],
   "p1": 792,
   "pn": 796,
   "doi": "10.21437/Interspeech.2024-298",
   "url": "interspeech_2024/yusuf24_interspeech.html"
  },
  "abel24_interspeech": {
   "authors": [
    [
     "Louis",
     "Abel"
    ],
    [
     "Vincent",
     "Colotte"
    ],
    [
     "Slim",
     "Ouni"
    ]
   ],
   "title": "Towards realtime co-speech gestures synthesis using STARGATE",
   "original": "302",
   "order": 1003,
   "page_count": 5,
   "abstract": [
    "The field of co-speech gestures synthesis is gaining more and more interest. However, many new systems utilize complex or resource-intensive architectures, making them impractical for integration into Embodied Conversational Agents (ECAs) or for exploration in fields like linguistics, where understanding the connection between speech and gestures is challenging. This paper introduces STARGATE, a novel architecture for Spatio-Temporal Autoregressive Graph from Audio-Text Embeddings. The model leverages autoregression for fast gestures generation, alongside graph convolutions and attention to integrate explicit structural knowledge and facilitate efficient spatial and temporal processing. Through both subjective and objective assessments against state-of-the-art models, our research demonstrates our model capabilities of generating convincing gestures fast. It also achieves slightly better scores in terms of credibility and coherence of generated gestures in relation to speech."
   ],
   "p1": 4893,
   "pn": 4897,
   "doi": "10.21437/Interspeech.2024-302",
   "url": "interspeech_2024/abel24_interspeech.html"
  },
  "li24f_interspeech": {
   "authors": [
    [
     "Chenyu",
     "Li"
    ],
    [
     "Jalal",
     "Al-Tamimi"
    ]
   ],
   "title": "Impact of the tonal factor on diphthong realizations in Standard Mandarin with Generalized Additive Mixed Models",
   "original": "303",
   "order": 320,
   "page_count": 5,
   "abstract": [
    "This study examines the effects of lexical tones on diphthong realizations in Standard Mandarin. We investigated the two falling diphthongs /ai/ and /au/ from a Standard Mandarin reading text corpus. A set of GAMMs models was employed to test whether and how tones and f0 influence the diphthong realizations. The results show the vowel height (reflected by F1) to differ with respect to tones: with a high tone, the diphthongs tend to be realized as more closed, and as more open with a low tone; with a rising tone, they tend to have a typical diphthongized realization, with a dynamical pattern of F1 contour; and to be monophthongized with a falling tone. The interaction between f0 and F1 extensively confirmed in monophthongs across languages, is equally applicable to /ai/ and /au/: f0 is negatively correlated with F1. The results show the universality of the tonal effect on vowel realization in different diphthongs and imply physiological factors behind it."
   ],
   "p1": 1555,
   "pn": 1559,
   "doi": "10.21437/Interspeech.2024-303",
   "url": "interspeech_2024/li24f_interspeech.html"
  },
  "dissen24_interspeech": {
   "authors": [
    [
     "Yehoshua",
     "Dissen"
    ],
    [
     "Shiry",
     "Yonash"
    ],
    [
     "Israel",
     "Cohen"
    ],
    [
     "Joseph",
     "Keshet"
    ]
   ],
   "title": "Enhanced ASR Robustness to Packet Loss with a Front-End Adaptation Network",
   "original": "306",
   "order": 1026,
   "page_count": 5,
   "abstract": [
    "In the realm of automatic speech recognition (ASR), robustness in noisy environments remains a significant challenge. Recent ASR models, such as Whisper, have shown promise, but their efficacy in noisy conditions can be further enhanced. This study is focused on recovering from packet loss to improve the word error rate (WER) of ASR models. We propose using a front-end adaptation network connected to a frozen ASR model. The adaptation network is trained to modify the corrupted input spectrum by minimizing the criteria of the ASR model in addition to an enhancement loss function. Our experiments demonstrate that the adaptation network, trained on Whisper's criteria, notably reduces word error rates across domains and languages in packet-loss scenarios. This improvement is achieved with minimal affect to Whisper model's foundational performance, underscoring our method's practicality and potential in enhancing ASR models in challenging acoustic environments."
   ],
   "p1": 5008,
   "pn": 5012,
   "doi": "10.21437/Interspeech.2024-306",
   "url": "interspeech_2024/dissen24_interspeech.html"
  },
  "audibert24_interspeech": {
   "authors": [
    [
     "Nicolas",
     "Audibert"
    ],
    [
     "Cecile",
     "Fougeron"
    ],
    [
     "Christine",
     "Meunier"
    ]
   ],
   "title": "Do Speaker-dependent Vowel Characteristics depend on Speech Style?",
   "original": "310",
   "order": 757,
   "page_count": 5,
   "abstract": [
    "Based on the examination of 385K vowels pertaining to 6 oral and 3 nasal vowel categories produced by twenty-three French speakers in both read and spontaneous speech, the present study questions the interplay between intra-speaker style-dependent variability in vowel production and speaker-specific vowel properties. Acoustic properties of the speakers’ vowels in the 12-D MFCC space are compared to that of other speakers in two styles. Results show that vowels do index speaker-distinctiveness better in read vs. spontaneous speech. Furthermore, in both speech styles, distinctions between speakers are the largest for the nasal vowels. Intra-speaker variability in vowel production is also examined between speech styles and is found to depend on the speaker and on the vowel category. However, for most speakers and most vowels, the variation between styles is smaller than the distinction between speakers in both styles. Implications of these results for speaker identification are discussed."
   ],
   "p1": 3669,
   "pn": 3673,
   "doi": "10.21437/Interspeech.2024-310",
   "url": "interspeech_2024/audibert24_interspeech.html"
  },
  "singh24b_interspeech": {
   "authors": [
    [
     "Riyansha",
     "Singh"
    ],
    [
     "Parinita",
     "Nema"
    ],
    [
     "Vinod K",
     "Kurmi"
    ]
   ],
   "title": "Towards Robust Few-shot Class Incremental Learning in Audio Classification using Contrastive Representation",
   "original": "312",
   "order": 1029,
   "page_count": 5,
   "abstract": [
    "In machine learning applications, gradual data ingress is common, especially in audio processing where incremental learning is vital for real-time analytics. Few-shot class-incremental learning addresses challenges arising from limited incoming data. Existing methods often integrate additional trainable components or rely on a fixed embedding extractor post-training on base sessions to mitigate concerns related to catastrophic forgetting and the dangers of model overfitting. However, using cross-entropy loss alone during base session training is suboptimal for audio data. To address this, we propose incorporating supervised contrastive learning to refine the representation space, enhancing discriminative power and leading to better generalization since it facilitates seamless integration of incremental classes, upon arrival. Experimental results on NSynth and LibriSpeech datasets with 100 classes, as well as ESC dataset with 50 and 10 classes, demonstrate state-of-the-art performance. Project page: https://visdomlab.github.io/FsACLearning/."
   ],
   "p1": 5023,
   "pn": 5027,
   "doi": "10.21437/Interspeech.2024-312",
   "url": "interspeech_2024/singh24b_interspeech.html"
  },
  "roth24_interspeech": {
   "authors": [
    [
     "Amit",
     "Roth"
    ],
    [
     "Arnon",
     "Turetzky"
    ],
    [
     "Yossi",
     "Adi"
    ]
   ],
   "title": "A Language Modeling Approach to Diacritic-Free Hebrew TTS",
   "original": "313",
   "order": 571,
   "page_count": 5,
   "abstract": [
    "We tackle the task of text-to-speech (TTS) in Hebrew. Traditional Hebrew contains Diacritics, which dictate the way individuals should pronounce given words, however, modern Hebrew rarely uses them. The lack of diacritics in modern Hebrew results in readers expected to conclude the correct pronunciation and understand which phonemes to use based on the context. This imposes a fundamental challenge on TTS systems to accurately map between text-to-speech. In this work, we propose to adopt a language modeling Diacritics-Free approach, for the task of Hebrew TTS. The model operates on discrete speech representations and is conditioned on a word-piece tokenizer. We optimize the proposed method using in-the-wild weakly supervised data and compare it to several diacritic-based TTS systems. Results suggest the proposed method is superior to the evaluated baselines considering both content preservation and naturalness of the generated speech. Samples can be found under the following link: pages.cs.huji.ac.il/adiyoss-lab/HebTTS/"
   ],
   "p1": 2775,
   "pn": 2779,
   "doi": "10.21437/Interspeech.2024-313",
   "url": "interspeech_2024/roth24_interspeech.html"
  },
  "boukun24_interspeech": {
   "authors": [
    [
     "Veranika",
     "Boukun"
    ],
    [
     "Jakob",
     "Drefs"
    ],
    [
     "Jörg",
     "Lücke"
    ]
   ],
   "title": "Blind Zero-Shot Audio Restoration: A Variational Autoencoder Approach for Denoising and Inpainting",
   "original": "314",
   "resource": "https://doi.org/10.5281/zenodo.12793258",
   "order": 989,
   "page_count": 5,
   "abstract": [
    "We address the task of blind 'zero-shot' audio signal denoising and inpainting. In the blind zero-shot setting, only the corrupted audio signal is used for signal restoration (no other signals are available to train the model). For this challenging setting, we apply a recent variational autoencoder that can leverage advanced probabilistic variational optimization in addition to flexible data modeling enabled by deep neural networks (DNNs). The investigated approach uses a non-amortized encoder and truncated posteriors as variational distributions. This way, the posterior correlations can be approximated, and a theoretically grounded treatment of missing values is directly available. In benchmarks for denoising and inpainting and in comparison with other zero-shot approaches, we observe competitive performance. Our results suggest that combining high-quality probabilistic optimization with DNN optimization is a very promising strategy for challenging audio restoration tasks."
   ],
   "p1": 4823,
   "pn": 4827,
   "doi": "10.21437/Interspeech.2024-314",
   "url": "interspeech_2024/boukun24_interspeech.html"
  },
  "jones24_interspeech": {
   "authors": [
    [
     "Austin",
     "Jones"
    ],
    [
     "Margaret E. L.",
     "Renwick"
    ]
   ],
   "title": "Evaluating Italian Vowel Variation with the Recurrent Neural Network Phonet",
   "original": "317",
   "resource": "https://doi.org/10.5281/zenodo.12786176",
   "order": 759,
   "page_count": 5,
   "abstract": [
    "Linguistic applications of speech technology require phone labels that accurately correspond to the phonetic signal. Mismatches between an ASR system's lexicon and speech acoustics can occur in situations of sociophonetic variation or phonological marginal contrast. We apply the recurrent neural network tool Phonet to Italian mid vowels, which are both sociophonetically variable and marginally contrastive. Phonet represents natural classes of sounds via interpretable vectors of phonological posteriors. Phonet was trained on 10 hours of Italian speech. Results show significant negative correlations between posterior probabilities for theoretically opposed classes like [open] and [close], alongside significant correlations of phonological posteriors to acoustics, e.g. values for [close] and vowels' first formant (F1). We demonstrate that Phonet’s phoneme recognition model generalizes to provide a better fit to vowel characteristics than phone labels derived from a G2P lexicon."
   ],
   "p1": 3679,
   "pn": 3683,
   "doi": "10.21437/Interspeech.2024-317",
   "url": "interspeech_2024/jones24_interspeech.html"
  },
  "tadavarthy24_interspeech": {
   "authors": [
    [
     "Harsha Veena",
     "Tadavarthy"
    ],
    [
     "Austin",
     "Jones"
    ],
    [
     "Margaret E. L.",
     "Renwick"
    ]
   ],
   "title": "Phonological Feature Detection for US English using the Phonet Library",
   "original": "318",
   "order": 312,
   "page_count": 5,
   "abstract": [
    "This paper details adaptation of the Phonet Library, a speech technology that uses distinctive features to calculate posterior probabilities for phonological classes, by training it on US English. Phonet's posterior probabilities provide a statistical basis for understanding patterned variability in speech, thus bridging the gap between acoustic data and phonological structures. We train Phonet on an English corpus, and we investigate both its precision in phoneme recognition at designated timestamps and the relationship of its distinctive feature probabilities to linguistic expectations for a subset of vowels and consonants. Phonet shows accuracy of over 90\\% in predicting English phonological classes. Our analysis demonstrates Phonet's robustness in capturing basic relationships between theoretical natural classes of sounds in English, highlighting its utility in the broader context of speech analysis and phonetic research. "
   ],
   "p1": 1515,
   "pn": 1519,
   "doi": "10.21437/Interspeech.2024-318",
   "url": "interspeech_2024/tadavarthy24_interspeech.html"
  },
  "rouditchenko24_interspeech": {
   "authors": [
    [
     "Andrew",
     "Rouditchenko"
    ],
    [
     "Yuan",
     "Gong"
    ],
    [
     "Samuel",
     "Thomas"
    ],
    [
     "Leonid",
     "Karlinsky"
    ],
    [
     "Hilde",
     "Kuehne"
    ],
    [
     "Rogerio",
     "Feris"
    ],
    [
     "James",
     "Glass"
    ]
   ],
   "title": "Whisper-Flamingo: Integrating Visual Features into Whisper for Audio-Visual Speech Recognition and Translation",
   "original": "322",
   "order": 499,
   "page_count": 5,
   "abstract": [
    "Audio-Visual Speech Recognition (AVSR) uses lip-based video to improve performance in noise. Since videos are harder to obtain than audio, the video training data of AVSR models is usually limited to a few thousand hours. In contrast, speech models such as Whisper are trained with hundreds of thousands of hours of data, and thus learn a better speech-to-text decoder. The huge training data difference motivates us to adapt Whisper to handle video inputs. Inspired by Flamingo which injects visual features into language models, we propose Whisper-Flamingo which integrates visual features into the Whisper speech recognition and translation model with gated cross attention. Our audio-visual Whisper-Flamingo outperforms audio-only Whisper on English speech recognition and En-X translation for 6 languages in noisy conditions. Moreover, Whisper-Flamingo is a versatile model and conducts all of these tasks using one set of parameters, while prior methods are trained separately on each language."
   ],
   "p1": 2420,
   "pn": 2424,
   "doi": "10.21437/Interspeech.2024-322",
   "url": "interspeech_2024/rouditchenko24_interspeech.html"
  },
  "deshmukh24b_interspeech": {
   "authors": [
    [
     "Soham",
     "Deshmukh"
    ],
    [
     "Dareen",
     "Alharthi"
    ],
    [
     "Benjamin",
     "Elizalde"
    ],
    [
     "Hannes",
     "Gamper"
    ],
    [
     "Mahmoud",
     "Al Ismail"
    ],
    [
     "Rita",
     "Singh"
    ],
    [
     "Bhiksha",
     "Raj"
    ],
    [
     "Huaming",
     "Wang"
    ]
   ],
   "title": "PAM: Prompting Audio-Language Models for Audio Quality Assessment",
   "original": "325",
   "order": 680,
   "page_count": 5,
   "abstract": [
    "Audio quality is a key performance metric for various audio processing tasks, including generative modeling, however its objective measurement remains a challenge. Audio-Language Models (ALM) are pre-trained on millions of audio-text pairs that may contain information about audio quality, the presence of artifacts or noise. Given an audio input and a text prompt about quality, an ALM can calculate a similarity score between the two. We exploit this capability and introduce PAM, a truly reference-free metric for assessing audio quality for different audio processing tasks. Contrary to other “reference-free” metrics, PAM does not require computing embeddings on a reference dataset nor training a task-specific model on a costly set of human listening scores. We extensively evaluate PAM against established metrics and newly collected human listening scores on four tasks: text- to-audio (TTA), text-to-music generation (TTM), text-to-speech (TTS), and deep noise suppression (DNS). We perform multiple ablation studies with controlled audio distortions, in-the-wild setups, and prompt choices. Our evaluation shows that overall, PAM correlates strongly with human listening scores and performs better than existing metrics. These results demonstrate the potential of ALM for computing a general-purpose audio quality metric. Code and human listening scores will be released."
   ],
   "p1": 3320,
   "pn": 3324,
   "doi": "10.21437/Interspeech.2024-325",
   "url": "interspeech_2024/deshmukh24b_interspeech.html"
  },
  "shih24_interspeech": {
   "authors": [
    [
     "Yi-Jen",
     "Shih"
    ],
    [
     "David",
     "Harwath"
    ]
   ],
   "title": "Interface Design for Self-Supervised Speech Models",
   "original": "326",
   "order": 516,
   "page_count": 5,
   "abstract": [
    "Self-supervised speech (SSL) models have recently become widely adopted for many downstream speech processing tasks. The general usage pattern is to employ SSL models as feature extractors, and then train a downstream prediction head to solve a specific task. However, different layers of SSL models have been shown to capture different types of information, and the methods of combining them are not well studied. To this end, we extend the general framework for SSL model utilization by proposing the interface that connects the upstream and downstream. Under this view, the dominant technique of combining features via a layerwise weighted sum can be regarded as a specific interface. We propose several alternative interface designs and demonstrate that the weighted sum interface is suboptimal for many tasks. In particular, we show that a convolutional interface whose depth scales logarithmically with the depth of the upstream model consistently outperforms many other interface designs."
   ],
   "p1": 2504,
   "pn": 2508,
   "doi": "10.21437/Interspeech.2024-326",
   "url": "interspeech_2024/shih24_interspeech.html"
  },
  "wang24i_interspeech": {
   "authors": [
    [
     "Helin",
     "Wang"
    ],
    [
     "Jesús",
     "Villalba"
    ],
    [
     "Laureano",
     "Moro-Velazquez"
    ],
    [
     "Jiarui",
     "Hai"
    ],
    [
     "Thomas",
     "Thebaud"
    ],
    [
     "Najim",
     "Dehak"
    ]
   ],
   "title": "Noise-robust Speech Separation with Fast Generative Correction",
   "original": "327",
   "order": 448,
   "page_count": 5,
   "abstract": [
    "Speech separation, the task of isolating multiple speech sources from a mixed audio signal, remains challenging in noisy environments. In this paper, we propose a generative correction method to enhance the output of a discriminative separator. By leveraging a generative corrector based on a diffusion model, we refine the separation process for single-channel mixture speech by removing noises and perceptually unnatural distortions. Furthermore, we optimize the generative model using a predictive loss to streamline the diffusion model’s reverse process into a single step and rectify any associated errors by the reverse process. Our method achieves state-of-the-art performance on the in-domain Libri2Mix noisy dataset, and out-of-domain WSJ with a variety of noises, improving SI-SNR by 22-35% relative to SepFormer, demonstrating robustness and strong generalization capabilities."
   ],
   "p1": 2165,
   "pn": 2169,
   "doi": "10.21437/Interspeech.2024-327",
   "url": "interspeech_2024/wang24i_interspeech.html"
  },
  "ghosh24_interspeech": {
   "authors": [
    [
     "Suhita",
     "Ghosh"
    ],
    [
     "Melanie",
     "Jouaiti"
    ],
    [
     "Arnab",
     "Das"
    ],
    [
     "Yamini",
     "Sinha"
    ],
    [
     "Tim",
     "Polzehl"
    ],
    [
     "Ingo",
     "Siegert"
    ],
    [
     "Sebastian",
     "Stober"
    ]
   ],
   "title": "Anonymising Elderly and Pathological Speech: Voice Conversion Using DDSP and Query-by-Example",
   "original": "328",
   "order": 912,
   "page_count": 5,
   "abstract": [
    "Speech anonymisation aims to protect speaker identity by changing personal identifiers in speech while retaining linguistic content. Current methods fail to retain prosody and unique speech patterns found in elderly and pathological speech domains, which is essential for remote health monitoring. To address this gap, we propose a voice conversion-based method (DDSP-QbE) using differentiable digital signal processing and query-by-example. The proposed method, trained with novel losses, aids in disentangling linguistic, prosodic, and domain representations, enabling the model to adapt to uncommon speech patterns. Objective and subjective evaluations show that DDSP-QbE significantly outperforms the voice conversion state-of-the-art concerning intelligibility, prosody, and domain preservation across diverse datasets, pathologies, and speakers while maintaining quality and speaker anonymity. Experts validate domain preservation by analysing twelve clinically pertinent domain attributes."
   ],
   "p1": 4438,
   "pn": 4442,
   "doi": "10.21437/Interspeech.2024-328",
   "url": "interspeech_2024/ghosh24_interspeech.html"
  },
  "dhawan24_interspeech": {
   "authors": [
    [
     "Kunal",
     "Dhawan"
    ],
    [
     "Nithin Rao",
     "Koluguri"
    ],
    [
     "Ante",
     "Jukić"
    ],
    [
     "Ryan",
     "Langman"
    ],
    [
     "Jagadeesh",
     "Balam"
    ],
    [
     "Boris",
     "Ginsburg"
    ]
   ],
   "title": "Codec-ASR: Training Performant Automatic Speech Recognition Systems with Discrete Speech Representations",
   "original": "330",
   "order": 530,
   "page_count": 5,
   "abstract": [
    "Discrete speech representations have garnered recent attention for their efficacy in training transformer-based models for various speech-related tasks such as automatic speech recognition (ASR), translation, speaker verification, and joint speech-text foundational models. In this work, we present a comprehensive analysis on building ASR systems with discrete codes. We investigate different methods for codec training such as quantization schemes and time-domain vs spectral feature encodings. We further explore ASR training techniques aimed at enhancing performance, training efficiency, and noise robustness. Drawing upon our findings, we introduce a codec ASR pipeline that outperforms Encodec at similar bit-rate. Remarkably, it also surpasses the state-of-the-art results achieved by strong self-supervised models on the 143 languages ML-SUPERB benchmark despite being smaller in size and pretrained on significantly less data."
   ],
   "p1": 2574,
   "pn": 2578,
   "doi": "10.21437/Interspeech.2024-330",
   "url": "interspeech_2024/dhawan24_interspeech.html"
  },
  "srinivas24_interspeech": {
   "authors": [
    [
     "Vidya",
     "Srinivas"
    ],
    [
     "Malek",
     "Itani"
    ],
    [
     "Tuochao",
     "Chen"
    ],
    [
     "Emre Sefik",
     "Eskimez"
    ],
    [
     "Takuya",
     "Yoshioka"
    ],
    [
     "Shyamnath",
     "Gollakota"
    ]
   ],
   "title": "Knowledge boosting during low-latency inference",
   "original": "331",
   "resource": "https://doi.org/10.5281/zenodo.12575452",
   "order": 892,
   "page_count": 5,
   "abstract": [
    "Models for low-latency, streaming applications could benefit from the knowledge capacity of larger models, but edge devices cannot run these models due to resource constraints. A possible solution is to transfer hints during inference from a large model running remotely to a small model running on-device. However, this incurs a communication delay that breaks real-time requirements and does not guarantee that both models will operate on the same data at the same time. We propose knowledge boosting, a novel technique that allows a large model to operate on time-delayed input during inference, while still boosting small model performance. Using a streaming neural network that processes 8 ms chunks, we evaluate different speech separation and enhancement tasks with communication delays of up to six chunks or 48 ms. Our results show larger gains where the performance gap between the small and large models is wide, demonstrating a promising method for large-small model collaboration for low-latency applications."
   ],
   "p1": 4338,
   "pn": 4342,
   "doi": "10.21437/Interspeech.2024-331",
   "url": "interspeech_2024/srinivas24_interspeech.html"
  },
  "shim24_interspeech": {
   "authors": [
    [
     "Kyuhong",
     "Shim"
    ],
    [
     "Jinkyu",
     "Lee"
    ],
    [
     "Hyunjae",
     "Kim"
    ]
   ],
   "title": "Leveraging Adapter for Parameter-Efficient ASR Encoder",
   "original": "334",
   "order": 491,
   "page_count": 5,
   "abstract": [
    "The expansion of speech models emphasizes the importance of parameter efficiency in practical automatic speech recognition (ASR) systems. Parameter sharing, which reuses the same parameter multiple times, has emerged as a promising solution to reduce storage requirements. However, previous studies have often faced challenges in balancing the number of parameters with performance. In this paper, we propose a novel architecture that effectively reduces the number of parameters while minimizing performance degradation. The key idea is to insert a lightweight adapter module that adjusts the features generated by shared parameters, thereby enhancing the diversity of representations. We introduce a unique adapter module and parameter-sharing configuration tailored for Conformer-based ASR encoders. Experimental results demonstrate that the proposed architecture reduces approximately 50% of parameters and 20% of computations without compromising speech recognition performance."
   ],
   "p1": 2380,
   "pn": 2384,
   "doi": "10.21437/Interspeech.2024-334",
   "url": "interspeech_2024/shim24_interspeech.html"
  },
  "neekhara24_interspeech": {
   "authors": [
    [
     "Paarth",
     "Neekhara"
    ],
    [
     "Shehzeen",
     "Hussain"
    ],
    [
     "Subhankar",
     "Ghosh"
    ],
    [
     "Jason",
     "Li"
    ],
    [
     "Boris",
     "Ginsburg"
    ]
   ],
   "title": "Improving Robustness of LLM-based Speech Synthesis by Learning Monotonic Alignment",
   "original": "335",
   "order": 701,
   "page_count": 5,
   "abstract": [
    "Large Language Model (LLM) based text-to-speech (TTS) systems have demonstrated remarkable capabilities in handling large speech datasets and generating natural speech for new speakers. However, LLM-based TTS models are not robust as the generated output can contain repeating words, missing words and mis-aligned speech (referred to as hallucinations or attention errors), especially when the text contains multiple occurrences of the same token. We examine these challenges in an encoder-decoder transformer model and find that certain cross-attention heads in such models implicitly learn the text and speech alignment when trained for predicting speech tokens for a given text. To make the alignment more robust, we propose techniques utilizing CTC loss and attention priors that encourage monotonic cross-attention over the text tokens. Our guided attention training technique does not introduce any new learnable parameters and significantly improves robustness of LLM-based TTS models."
   ],
   "p1": 3425,
   "pn": 3429,
   "doi": "10.21437/Interspeech.2024-335",
   "url": "interspeech_2024/neekhara24_interspeech.html"
  },
  "guo24_interspeech": {
   "authors": [
    [
     "Hongmei",
     "Guo"
    ],
    [
     "Yijiang",
     "Chen"
    ],
    [
     "Xiao-Lei",
     "Zhang"
    ],
    [
     "Xuelong",
     "Li"
    ]
   ],
   "title": "Graph Attention Based Multi-Channel U-Net for Speech Dereverberation With Ad-Hoc Microphone Arrays",
   "original": "336",
   "order": 125,
   "page_count": 5,
   "abstract": [
    "Speech dereverberation with ad-hoc microphone arrays seems not studied sufficiently, particularly in the scenario where the reverberation time is large. In this paper, we propose a novel multi-channel U-Net model for speech dereverberation with ad-hoc microphone arrays, where an attention module is integrated into the model in an end-to-end training manner to do channel selection and fusion. Specifically, we first train a single-channel U-Net model. Then, we replicate the U-Net model to each channel. Finally, we train the attention module for aggregating the information of the channels, where the parameters of the U-Net model are fixed at this stage. To our knowledge, this is the first work that U-Net was used for dereverberation with ad-hoc microphone arrays. We studied two attention mechanism, which are the self-attention and graph-attention; moreover, we integrated the attention module into either the bottleneck layer or the output layer of the multi-channel U-Net, which results in four implementations. Experimental results demonstrate that the proposed method achieves the state-of-the-art performance, and the attention module is very important in channel selection and fusion for improving the performance against long reverberation time."
   ],
   "p1": 617,
   "pn": 621,
   "doi": "10.21437/Interspeech.2024-336",
   "url": "interspeech_2024/guo24_interspeech.html"
  },
  "yip24_interspeech": {
   "authors": [
    [
     "Jia Qi",
     "Yip"
    ],
    [
     "Shengkui",
     "Zhao"
    ],
    [
     "Dianwen",
     "Ng"
    ],
    [
     "Eng Siong",
     "Chng"
    ],
    [
     "Bin",
     "Ma"
    ]
   ],
   "title": "Towards Audio Codec-based Speech Separation",
   "original": "337",
   "resource": "https://doi.org/10.5281/zenodo.12741928",
   "order": 453,
   "page_count": 5,
   "abstract": [
    "Recent improvements in neural audio codec (NAC) models have generated interest in adopting pre-trained codecs for a variety of speech processing applications to take advantage of the efficiencies gained from high compression, but these have yet been applied to the speech separation (SS) task. SS can benefit from high compression because the compute required for traditional SS models makes them impractical for many edge computing use cases. However, SS is a waveform-masking task where compression tends to introduce distortions that severely impact performance. Here we propose a novel task of Audio Codec-based SS, where SS is performed within the embedding space of a NAC, and propose a new model, Codecformer, to address this task. At inference, Codecformer achieves a 52x reduction in MAC while producing separation performance comparable to a cloud deployment of Sepformer. This method charts a new direction for performing efficient SS in practical scenarios."
   ],
   "p1": 2190,
   "pn": 2194,
   "doi": "10.21437/Interspeech.2024-337",
   "url": "interspeech_2024/yip24_interspeech.html"
  },
  "ward24_interspeech": {
   "authors": [
    [
     "Nigel G.",
     "Ward"
    ],
    [
     "Andres",
     "Segura"
    ],
    [
     "Alejandro",
     "Ceballos"
    ],
    [
     "Divette",
     "Marco"
    ]
   ],
   "title": "Towards a General-Purpose Model of Perceived Pragmatic Similarity",
   "original": "339",
   "order": 1008,
   "page_count": 5,
   "abstract": [
    "Models for estimating the similarity between two utterances are fundamental in speech technology.  While fairly good automatic measures exist for semantic similarity, pragmatic similarity has not been previously explored. Using a new collection of thousands of human judgments of the pragmatic similarity between utterance pairs, we train and evaluate various predictive models. The best performing model, which uses 103 features selected from HuBert's 24th layer, correlates on average 0.74 with human judges for the highest-quality data subset, and it sometimes approaches human inter-annotator agreement. We also find evidence for some degree of generality across languages."
   ],
   "p1": 4918,
   "pn": 4922,
   "doi": "10.21437/Interspeech.2024-339",
   "url": "interspeech_2024/ward24_interspeech.html"
  },
  "shirahata24_interspeech": {
   "authors": [
    [
     "Yuma",
     "Shirahata"
    ],
    [
     "Byeongseon",
     "Park"
    ],
    [
     "Ryuichi",
     "Yamamoto"
    ],
    [
     "Kentaro",
     "Tachibana"
    ]
   ],
   "title": "Audio-conditioned phonemic and prosodic annotation for building text-to-speech models from unlabeled speech data",
   "original": "342",
   "order": 575,
   "page_count": 5,
   "abstract": [
    "This paper proposes an audio-conditioned phonemic and prosodic annotation model for building text-to-speech (TTS) datasets from unlabeled speech samples. For creating a TTS dataset that consists of label-speech paired data, the proposed annotation model leverages an automatic speech recognition (ASR) model to obtain phonemic and prosodic labels from unlabeled speech samples. By fine-tuning a large-scale pretrained ASR model, we can construct the annotation model using a limited amount of label-speech paired data within an existing TTS dataset. To alleviate the shortage of label-speech paired data for training the annotation model, we generate pseudo label-speech paired data using text-only corpora and an auxiliary TTS model. This TTS model is also trained with the existing TTS dataset. Experimental results show that the TTS model trained with the dataset created by the proposed annotation method can synthesize speech as naturally as the one trained with a fully-labeled dataset."
   ],
   "p1": 2795,
   "pn": 2799,
   "doi": "10.21437/Interspeech.2024-342",
   "url": "interspeech_2024/shirahata24_interspeech.html"
  },
  "xu24_interspeech": {
   "authors": [
    [
     "Jingjing",
     "Xu"
    ],
    [
     "Wei",
     "Zhou"
    ],
    [
     "Zijian",
     "Yang"
    ],
    [
     "Eugen",
     "Beck"
    ],
    [
     "Ralf",
     "Schlüter"
    ]
   ],
   "title": "Dynamic Encoder Size Based on Data-Driven Layer-wise Pruning for Speech Recognition",
   "original": "343",
   "order": 937,
   "page_count": 5,
   "abstract": [
    "Varying-size models are often required to deploy ASR systems under different hardware and/or application constraints such as memory and latency. To avoid redundant training and optimization efforts for individual models of different sizes, we present the dynamic encoder size approach, which jointly trains multiple performant models within one supernet from scratch. These subnets of various sizes are layer-wise pruned from the supernet, and thus, enjoy full parameter sharing. By combining score-based pruning with supernet training, we propose two novel methods, Simple-Top-k and Iterative-Zero-Out, to automatically select the best-performing subnets in a data-driven manner, avoiding resource-intensive search efforts. Our experiments using CTC on both Librispeech and TED-LIUM-v2 corpora show that our methods can achieve on-par performance as individually trained models of each size category. Also, our approach consistently brings small performance improvements for the full-size supernet."
   ],
   "p1": 4563,
   "pn": 4567,
   "doi": "10.21437/Interspeech.2024-343",
   "url": "interspeech_2024/xu24_interspeech.html"
  },
  "li24g_interspeech": {
   "authors": [
    [
     "Zhuhai",
     "Li"
    ],
    [
     "Jie",
     "Zhang"
    ],
    [
     "Wu",
     "Guo"
    ],
    [
     "Haochen",
     "Wu"
    ]
   ],
   "title": "Boosting the Transferability of Adversarial Examples with Gradient-Aligned Ensemble Attack for Speaker Recognition",
   "original": "346",
   "order": 108,
   "page_count": 5,
   "abstract": [
    "In the black-box attack for speaker recognition systems, the adversarial examples can exhibit better transferability for unseen victim system if they can consistently spoof an ensemble of substitute models. In this work, we propose a gradient-aligned ensemble attack method to find the optimal gradient direction to update the adversarial example using a set of substitute models. Specifically, we first calculate the overfitting-reduced gradient for each substitute model by randomly masking some regions of the input acoustic features. Then we obtain the weight of the gradient for each substitute model based on the consistency of its gradient with respect to others. The final update gradient is calculated by the weighted sum of the gradients over all substitute models. Experimental results on the VoxCeleb dataset verify the effectiveness of the proposed approach for the speaker identification and speaker verification tasks."
   ],
   "p1": 532,
   "pn": 536,
   "doi": "10.21437/Interspeech.2024-346",
   "url": "interspeech_2024/li24g_interspeech.html"
  },
  "chen24f_interspeech": {
   "authors": [
    [
     "Run",
     "Chen"
    ],
    [
     "Haozhe",
     "Chen"
    ],
    [
     "Anushka",
     "Kulkarni"
    ],
    [
     "Eleanor",
     "Lin"
    ],
    [
     "Linda",
     "Pang"
    ],
    [
     "Divya",
     "Tadimeti"
    ],
    [
     "Jun",
     "Shin"
    ],
    [
     "Julia",
     "Hirschberg"
    ]
   ],
   "title": "Detecting Empathy in Speech",
   "original": "347",
   "order": 225,
   "page_count": 5,
   "abstract": [
    "Empathy is the ability to understand another’s feelings as if we were having those feelings ourselves. It has been shown to increase to people’s trust and likability. Much research has been done on creating empathetic responses in text in conversational systems, yet little work has been done to identify the acoustic-prosodic speech features that can create an empathetic-sounding voice. Our contributions include 1) collection of a new empathy speech dataset, 2) identifying interpretable acoustic-prosodic features that contribute to empathy expression and 3) benchmarking the empathy detection task."
   ],
   "p1": 1080,
   "pn": 1084,
   "doi": "10.21437/Interspeech.2024-347",
   "url": "interspeech_2024/chen24f_interspeech.html"
  },
  "khassanov24_interspeech": {
   "authors": [
    [
     "Yerbolat",
     "Khassanov"
    ],
    [
     "Zhipeng",
     "Chen"
    ],
    [
     "Tianfeng",
     "Chen"
    ],
    [
     "Tze Yuang",
     "Chong"
    ],
    [
     "Wei",
     "Li"
    ],
    [
     "Jun",
     "Zhang"
    ],
    [
     "Lu",
     "Lu"
    ],
    [
     "Yuxuan",
     "Wang"
    ]
   ],
   "title": "Dual-Pipeline with Low-Rank Adaptation for New Language Integration in Multilingual ASR",
   "original": "348",
   "order": 159,
   "page_count": 5,
   "abstract": [
    "This paper addresses challenges in integrating new languages into a pre-trained multilingual automatic speech recognition (mASR) system, particularly in scenarios where training data for existing languages is limited or unavailable. The proposed method employs a dual-pipeline with low-rank adaptation (LoRA). It maintains two data flow pipelines—one for existing languages and another for new languages. The primary pipeline follows the standard flow through the pre-trained parameters of mASR, while the secondary pipeline additionally utilizes language-specific parameters represented by LoRA and a separate output decoder module. Importantly, the proposed approach minimizes the performance degradation of existing languages and enables a language-agnostic operation mode, facilitated by a decoder selection strategy. We validate the effectiveness of the proposed method by extending the pre-trained Whisper model to 19 new languages from the FLEURS dataset."
   ],
   "p1": 787,
   "pn": 791,
   "doi": "10.21437/Interspeech.2024-348",
   "url": "interspeech_2024/khassanov24_interspeech.html"
  },
  "matsuura24_interspeech": {
   "authors": [
    [
     "Kohei",
     "Matsuura"
    ],
    [
     "Takanori",
     "Ashihara"
    ],
    [
     "Takafumi",
     "Moriya"
    ],
    [
     "Masato",
     "Mimura"
    ],
    [
     "Takatomo",
     "Kano"
    ],
    [
     "Atsunori",
     "Ogawa"
    ],
    [
     "Marc",
     "Delcroix"
    ]
   ],
   "title": "Sentence-wise Speech Summarization: Task, Datasets, and End-to-End Modeling with LM Knowledge Distillation",
   "original": "349",
   "order": 398,
   "page_count": 5,
   "abstract": [
    "This paper introduces a novel approach called sentence-wise speech summarization (Sen-SSum), which generates text summaries from a spoken document in a sentence-by-sentence manner. Sen-SSum combines the real-time processing of automatic speech recognition (ASR) with the conciseness of speech summarization. To explore this approach, we present two datasets for Sen-SSum: Mega-SSum and CSJ-SSum. Using these datasets, our study evaluates two types of Transformer-based models: 1) cascade models that combine ASR and strong text summarization models, and 2) end-to-end (E2E) models that directly convert speech into a text summary. While E2E models are appealing to develop compute-efficient models, they perform worse than cascade models. Therefore, we propose knowledge distillation for E2E models using pseudo-summaries generated by the cascade models. Our experiments show that this proposed knowledge distillation effectively improves the performance of the E2E model on both datasets."
   ],
   "p1": 1945,
   "pn": 1949,
   "doi": "10.21437/Interspeech.2024-349",
   "url": "interspeech_2024/matsuura24_interspeech.html"
  },
  "jin24b_interspeech": {
   "authors": [
    [
     "Zezhong",
     "Jin"
    ],
    [
     "Youzhi",
     "Tu"
    ],
    [
     "Man-Wai",
     "Mak"
    ]
   ],
   "title": "W-GVKT: Within-Global-View Knowledge Transfer for Speaker Verification",
   "original": "354",
   "order": 779,
   "page_count": 5,
   "abstract": [
    "Contrastive self-supervised learning has played an important role in speaker verification (SV). However, such approaches suffer from false-negative issues. To address this problem, we enhance the non-contrastive DINO framework by enabling knowledge transfer from the teacher network to the student network through diversified versions of global views and call the method Within-Global-View Knowledge Transfer (W-GVKT) DINO. We discovered that given the global view of the entire utterance, creating discrepancies in the student’s output through applying spectral augmentation and feature diversification to the global view can facilitate the transfer of knowledge from the teacher to the student. With negligible computational resource increases, W-GVKT achieves an impressive EER of 4.11% without utilizing speaker labels on Voxceleb1. When combined with the RDNIO framework, W-GVKT achieved an EER of 2.89%."
   ],
   "p1": 3779,
   "pn": 3783,
   "doi": "10.21437/Interspeech.2024-354",
   "url": "interspeech_2024/jin24b_interspeech.html"
  },
  "lambropoulos24_interspeech": {
   "authors": [
    [
     "Michael",
     "Lambropoulos"
    ],
    [
     "Frantz",
     "Clermont"
    ],
    [
     "Shunichi",
     "Ishihara"
    ]
   ],
   "title": "The sub-band cepstrum as a tool for locating local spectral regions of phonetic sensitivity: A first attempt with multi-speaker vowel data",
   "original": "357",
   "order": 316,
   "page_count": 5,
   "abstract": [
    "Phonetic information is well-known to be unevenly encoded throughout vowel spectra, implying the existence of sub-band regions sensitive to that information. This work exploits band-limited cepstral coefficients (BLCCs) to locate such regions and quantify their sensitivity through vowel classification. BLCCs are acoustic parameters representing sub-band spectra; their extraction involves a linear transformation of full-band CCs with flexible sub-band selection. Here, 18 sub-bands spanning the full band [0-4 kHz] and their respective BLCCs are used to classify Japanese vowels from 306 native male speakers. Classification accuracy is high in sub-bands where phonetic differences between vowels are the most significant. Such sub-bands are mainly in the low frequency range as expected, but do not exclusively align with formant regions. These findings suggest that BLCCs are potentially very useful for gaining detailed phonetic insights with flexible sub-band focus and efficient computation."
   ],
   "p1": 1535,
   "pn": 1539,
   "doi": "10.21437/Interspeech.2024-357",
   "url": "interspeech_2024/lambropoulos24_interspeech.html"
  },
  "zhou24_interspeech": {
   "authors": [
    [
     "Kun",
     "Zhou"
    ],
    [
     "Shengkui",
     "Zhao"
    ],
    [
     "Yukun",
     "Ma"
    ],
    [
     "Chong",
     "Zhang"
    ],
    [
     "Hao",
     "Wang"
    ],
    [
     "Dianwen",
     "Ng"
    ],
    [
     "Chongjia",
     "Ni"
    ],
    [
     "Trung Hieu",
     "Nguyen"
    ],
    [
     "Jia Qi",
     "Yip"
    ],
    [
     "Bin",
     "Ma"
    ]
   ],
   "title": "Phonetic Enhanced Language Modeling for Text-to-Speech Synthesis",
   "original": "359",
   "order": 704,
   "page_count": 5,
   "abstract": [
    "Recent language model-based text-to-speech (TTS) frameworks demonstrate scalability and in-context learning capabilities. However, they suffer from robustness issues due to the accumulation of errors in speech unit predictions during autoregressive language modeling. In this paper, we propose a phonetic enhanced language modeling method to improve the performance of TTS models. We leverage self-supervised representations that are phonetically rich as the training target for the autoregressive language model. Subsequently, a non-autoregressive model is employed to predict discrete acoustic codecs that contain fine-grained acoustic details. The TTS model focuses solely on linguistic modeling during autoregressive training, thereby reducing the error propagation that occurs in non-autoregressive training. Both objective and subjective evaluations validate the effectiveness of our proposed method."
   ],
   "p1": 3440,
   "pn": 3444,
   "doi": "10.21437/Interspeech.2024-359",
   "url": "interspeech_2024/zhou24_interspeech.html"
  },
  "jin24c_interspeech": {
   "authors": [
    [
     "Zezhong",
     "Jin"
    ],
    [
     "Youzhi",
     "Tu"
    ],
    [
     "Man-Wai",
     "Mak"
    ]
   ],
   "title": "Self-Supervised Learning with Multi-Head Multi-Mode Knowledge Distillation for Speaker Verification",
   "original": "360",
   "order": 969,
   "page_count": 5,
   "abstract": [
    "Training speaker verification (SV) systems without labeled data is challenging. To tackle the challenge, we propose Multi-Head, Multi-Mode (MeMo) self-supervised learning based on knowledge distillation. Unlike DINO, the teacher in MeMo uses two distinct architectures to learn collaboratively, and so does the student. MeMo employs two distillation modes: self- and cross-distillations, with the teacher and student having the same and different architectures, respectively. To reduce the output discrepancy caused by different architectures, we divide the projection head into self- and cross-heads so that each head is responsible for distillation in its respective mode. We also discover that contrastive learning at the embedding level is supportive only in early training stages. To address this issue, we propose dynamically stopping the contrastive learning while continuing knowledge distillation. MeMo achieves an impressive EER of 3.10% on Voxceleb1 using a small ECAPA-TDNN backbone."
   ],
   "p1": 4723,
   "pn": 4727,
   "doi": "10.21437/Interspeech.2024-360",
   "url": "interspeech_2024/jin24c_interspeech.html"
  },
  "kim24c_interspeech": {
   "authors": [
    [
     "Ju-ho",
     "Kim"
    ],
    [
     "Hee-Soo",
     "Heo"
    ],
    [
     "Bong-Jin",
     "Lee"
    ],
    [
     "Youngki",
     "Kwon"
    ],
    [
     "Minjae",
     "Lee"
    ],
    [
     "Ha-Jin",
     "Yu"
    ]
   ],
   "title": "Self-supervised speaker verification with relational mask prediction",
   "original": "362",
   "order": 547,
   "page_count": 5,
   "abstract": [
    "Recently, self-supervised learning (SSL) has emerged as a promising strategy for constructing speaker verification (SV) systems, effectively mitigating the cost and privacy issues associated with the labeling process. The majority of SSL-based SV systems tend to focus on utterance-level features, potentially overlooking the inherent inter-frame structure of speech. To bridge this gap, we propose the relational mask prediction (RMP), a novel loss function that encourages models to understand the relationships between frames. Additionally, we introduce a block aggregation Transformer (BA-Transformer) to enrich frame-level features. Models were trained without labels using the VoxCeleb2 development set and comprehensively evaluated using various test sets. Experimental results demonstrate that the proposed framework outperforms recent SSL-based SV systems, achieving an average performance improvement of 22.39% over the baseline across the entire evaluation dataset."
   ],
   "p1": 2655,
   "pn": 2659,
   "doi": "10.21437/Interspeech.2024-362",
   "url": "interspeech_2024/kim24c_interspeech.html"
  },
  "kim24d_interspeech": {
   "authors": [
    [
     "Eungbeom",
     "Kim"
    ],
    [
     "Hantae",
     "Kim"
    ],
    [
     "Kyogu",
     "Lee"
    ]
   ],
   "title": "Guiding Frame-Level CTC Alignments Using Self-knowledge Distillation",
   "original": "363",
   "order": 942,
   "page_count": 5,
   "abstract": [
    "Transformer encoder with connectionist temporal classification (CTC) framework is widely used for automatic speech recognition (ASR). However, knowledge distillation (KD) for ASR displays a problem of disagreement between teacher-student models in frame-level alignment which ultimately hinders it from improving the student model’s performance. In order to resolve this problem, this paper introduces a self-knowledge distillation (SKD) method that guides the frame-level alignment during the training time. In contrast to the conventional method using separate teacher and student models, this study introduces a simple and effective method sharing encoder layers and applying the sub-model as the student model. Overall, our approach is effective in improving both the resource efficiency as well as performance. We also conducted an experimental analysis of the spike timings to illustrate that the proposed method improves performance by reducing the alignment disagreement."
   ],
   "p1": 4588,
   "pn": 4592,
   "doi": "10.21437/Interspeech.2024-363",
   "url": "interspeech_2024/kim24d_interspeech.html"
  },
  "shen24_interspeech": {
   "authors": [
    [
     "Yao",
     "Shen"
    ],
    [
     "Yingying",
     "Gao"
    ],
    [
     "Yaqian",
     "Hao"
    ],
    [
     "Chenguang",
     "Hu"
    ],
    [
     "Fulin",
     "Zhang"
    ],
    [
     "Junlan",
     "Feng"
    ],
    [
     "Shilei",
     "Zhang"
    ]
   ],
   "title": "CEC: A Noisy Label Detection Method for Speaker Recognition",
   "original": "364",
   "order": 780,
   "page_count": 5,
   "abstract": [
    "Noisy labels are inevitable, even in well-annotated datasets. The detection of noisy labels is of significant importance to enhance the robustness of speaker recognition models. In this paper, we propose a novel noisy label detection approach based on two new statistical metrics: Continuous Inconsistent Counting (CIC) and Total Inconsistent Counting (TIC). These metrics are calculated through Cross-Epoch Counting (CEC) and correspond to the early and late stages of training, respectively. Additionally, we categorize samples based on their prediction results into three categories: inconsistent samples, hard samples, and easy samples. During training, we gradually increase the difficulty of hard samples to update model parameters, preventing noisy labels from being overfitted. Compared to contrastive schemes, our approach not only achieves the best performance in speaker verification but also excels in noisy label detection."
   ],
   "p1": 3784,
   "pn": 3788,
   "doi": "10.21437/Interspeech.2024-364",
   "url": "interspeech_2024/shen24_interspeech.html"
  },
  "yu24_interspeech": {
   "authors": [
    [
     "En-Lun",
     "Yu"
    ],
    [
     "Kuan-Hsun",
     "Ho"
    ],
    [
     "Jeih-weih",
     "Hung"
    ],
    [
     "Shih-Chieh",
     "Huang"
    ],
    [
     "Berlin",
     "Chen"
    ]
   ],
   "title": "Speaker Conditional Sinc-Extractor for Personal VAD",
   "original": "365",
   "order": 438,
   "page_count": 5,
   "abstract": [
    "This study explores Sinc-convolution's novel application in Personal Voice Activity Detection (PVAD). The Sinc-Extractor (SE) network, developed for PVAD, learns cutoff frequencies and band gains of sinc functions to extract acoustic features. Additionally, the speaker conditional SE (SCSE) module incorporates speaker information from high-dimensional d-vectors into low-dimensional acoustic features. SE-PVAD and Vanilla PVAD have similar model size and computing load, while SCSE-PVAD is more compact with shorter inference time as it excludes speaker embedding. Evaluated with concatenated utterances from the LibriSpeech corpus, SE-PVAD outperforms Vanilla PVAD significantly. SCSE-PVAD matches Vanilla PVAD's performance but reduces input feature dimensionality and network complexity. Thus, SCSE-PVAD can function like a typical VAD, accepting only acoustic features, making it suitable for low-resource wearable devices."
   ],
   "p1": 2115,
   "pn": 2119,
   "doi": "10.21437/Interspeech.2024-365",
   "url": "interspeech_2024/yu24_interspeech.html"
  },
  "um24_interspeech": {
   "authors": [
    [
     "Seyun",
     "Um"
    ],
    [
     "Doyeon",
     "Kim"
    ],
    [
     "Hong-Goo",
     "Kang"
    ]
   ],
   "title": "PARAN: Variational Autoencoder-based End-to-End Articulation-to-Speech System for Speech Intelligibility",
   "original": "366",
   "order": 510,
   "page_count": 5,
   "abstract": [
    "Deep learning-based articulation-to-speech (ATS) systems designed for individuals with speech disorders have been extensively researched in recent years. However, conventional methods have faced challenges in representing the transformation in latent space across speech and electromagnetic articulography (EMA) domains, resulting in low speech quality. In this paper, we propose a variational autoencoder (VAE)-based end-to-end ATS model called PARAN that efficiently produces high-fidelity speech from EMA signals. Our model adjusts a prior distribution of latent representations from EMA signals to match a posterior distribution derived from speech utilizing a normalizing flow mechanism. To further enhance the clarity and intelligibility of the synthesized speech, we incorporate an additional loss function aimed at predicting phonetic information from EMA signals. Experimental results demonstrate that our model outperforms previous methods in terms of speech quality and intelligibility."
   ],
   "p1": 2475,
   "pn": 2479,
   "doi": "10.21437/Interspeech.2024-366",
   "url": "interspeech_2024/um24_interspeech.html"
  },
  "lu24b_interspeech": {
   "authors": [
    [
     "Jingze",
     "Lu"
    ],
    [
     "Yuxiang",
     "Zhang"
    ],
    [
     "Zhuo",
     "Li"
    ],
    [
     "Zengqiang",
     "Shang"
    ],
    [
     "Wenchao",
     "Wang"
    ],
    [
     "Pengyuan",
     "Zhang"
    ]
   ],
   "title": "Improving Copy-Synthesis Anti-Spoofing Training Method with Rhythm and Speaker Perturbation",
   "original": "367",
   "order": 104,
   "page_count": 5,
   "abstract": [
    "The rapid development of speech synthesis algorithms poses a challenge in constructing corresponding training datasets for speech anti-spoofing systems in real-world scenarios. The copy-synthesis method offers a simple yet effective solution to this problem. However, the limitation of this method is that it only utilizes the artifacts generated by vocoders, neglecting those from acoustic models. This paper aims to locate the artifacts introduced by the acoustic models of Text-to-Speech (TTS) and Voice Conversion (VC) algorithms, and optimize the copy-synthesis pipeline. The proposed rhythm and speaker perturbation modules successfully boost anti-spoofing models to leverage the artifacts introduced by acoustic models, thereby enhancing their generalization ability when facing various TTS and VC algorithms."
   ],
   "p1": 512,
   "pn": 516,
   "doi": "10.21437/Interspeech.2024-367",
   "url": "interspeech_2024/lu24b_interspeech.html"
  },
  "li24h_interspeech": {
   "authors": [
    [
     "Sheng",
     "Li"
    ],
    [
     "Chen",
     "Chen"
    ],
    [
     "Chin Yuen",
     "Kwok"
    ],
    [
     "Chenhui",
     "Chu"
    ],
    [
     "Eng Siong",
     "Chng"
    ],
    [
     "Hisashi",
     "Kawai"
    ]
   ],
   "title": "Investigating ASR Error Correction with Large Language Model and Multilingual 1-best Hypotheses",
   "original": "368",
   "order": 272,
   "page_count": 5,
   "abstract": [
    "This paper investigates using pre-trained large language models (LLMs) to improve multilingual automatic speech recognition (ASR) outputs. Current popular methods involve feeding the N-best ASR output into LLMs. Although this approach demonstrates improved results, obtaining N-best hypotheses is time-consuming and unavailable sometimes. To develop a more general method, this paper investigates LLM-based ASR error correction with 1-best hypotheses. We fine-tuned a multilingual LLM covering more than 100 languages and let it correct 1-best hypotheses errors from different speech foundation models. The experiment shows that the proposed method effectively enhances the ASR result only using 1-best hypotheses. Moreover, we also noticed that knowledge-transferring between the languages using the same writing system in the LLM can effectively correct low-resourced languages' hypotheses."
   ],
   "p1": 1315,
   "pn": 1319,
   "doi": "10.21437/Interspeech.2024-368",
   "url": "interspeech_2024/li24h_interspeech.html"
  },
  "kan24_interspeech": {
   "authors": [
    [
     "Yip Keng",
     "Kan"
    ],
    [
     "Ke",
     "Xu"
    ],
    [
     "Hao",
     "Li"
    ],
    [
     "Jie",
     "Shi"
    ]
   ],
   "title": "VoiceDefense: Protecting Automatic Speaker Verification Models Against Black-box Adversarial Attacks",
   "original": "372",
   "order": 105,
   "page_count": 5,
   "abstract": [
    "Automatic Speaker Verification (ASV) is extensively used in many security-sensitive domains, but the increasing prevalence of adversarial attacks has seriously compromised the trustworthiness of these systems. Targeted black-box attacks emerge as the most formidable threat, proving incredibly challenging to counteract. However, existing defenses exhibit limitations when applied in real-world scenarios. We propose VoiceDefense - a novel adversarial sample detection method that slices an audio sample into multiple segments and captures their local audio features with segment-specific ASV scores. These scores present distributions that vary distinctly between genuine and adversarial samples, which VoiceDefense leverages for detection. VoiceDefense outperforms the state of the art with a best AUC of 0.9624 and is consistently effective against various attacks and perturbation budgets, all while maintaining remarkably low computational overhead."
   ],
   "p1": 517,
   "pn": 521,
   "doi": "10.21437/Interspeech.2024-372",
   "url": "interspeech_2024/kan24_interspeech.html"
  },
  "choi24_interspeech": {
   "authors": [
    [
     "HyunJung",
     "Choi"
    ],
    [
     "Muyeol",
     "Choi"
    ],
    [
     "Yohan",
     "Lim"
    ],
    [
     "Minkyu",
     "Lee"
    ],
    [
     "Seonhui",
     "Kim"
    ],
    [
     "Seung",
     "Yun"
    ],
    [
     "Donghyun",
     "Kim"
    ],
    [
     "SangHun",
     "Kim"
    ]
   ],
   "title": "Spoken-to-written text conversion with Large Language Model",
   "original": "376",
   "order": 497,
   "page_count": 5,
   "abstract": [
    "The improvement in end-to-end speech recognition systems has enhanced the readability of results, making it easier for users to understand texts and reducing translation errors. Korean uses both written and spoken forms, making it crucial to standardize pronunciation notation for high readability. Inverse Text Normalization (ITN) technology, which converts pronunciation into readable written form, can be applied in preprocessing training corpora or post-processing speech recognition outcomes. Recent Korean ITN research utilizes transformer models based on training data with both notations, facing performance degradation due to data scarcity. This paper proposes using Large Language Models for ITN to address this issue, overcoming the performance decline from limited data. The proposed method showed an 12.6% reduction in Error Reduction Rate (ERR)."
   ],
   "p1": 2410,
   "pn": 2414,
   "doi": "10.21437/Interspeech.2024-376",
   "url": "interspeech_2024/choi24_interspeech.html"
  },
  "suh24_interspeech": {
   "authors": [
    [
     "Jiwon",
     "Suh"
    ],
    [
     "Injae",
     "Na"
    ],
    [
     "Woohwan",
     "Jung"
    ]
   ],
   "title": "Improving Domain-Specific ASR with LLM-Generated Contextual Descriptions",
   "original": "377",
   "order": 260,
   "page_count": 5,
   "abstract": [
    "End-to-end automatic speech recognition (E2E ASR) systems have significantly improved speech recognition through training on extensive datasets. Despite these advancements, they still struggle to accurately recognize domain specific words, such as proper nouns and technical terminologies. To address this problem, we propose a method to utilize the state-of-the-art Whisper without modifying its architecture, preserving its generalization performance while enabling it to leverage descriptions effectively. Moreover, we propose two additional training techniques to improve the domain specific ASR: decoder fine-tuning, and context perturbation. We also propose a method to use a Large Language Model (LLM) to generate descriptions with simple metadata, when descriptions are unavailable. Our experiments demonstrate that proposed methods notably enhance domain-specific ASR accuracy on real-life datasets, with LLM-generated descriptions outperforming human-crafted ones in effectiveness."
   ],
   "p1": 1255,
   "pn": 1259,
   "doi": "10.21437/Interspeech.2024-377",
   "url": "interspeech_2024/suh24_interspeech.html"
  },
  "shen24b_interspeech": {
   "authors": [
    [
     "Rubing",
     "Shen"
    ],
    [
     "Yanzhen",
     "Ren"
    ],
    [
     "Zongkun",
     "Sun"
    ]
   ],
   "title": "FA-GAN: Artifacts-free and Phase-aware High-fidelity GAN-based Vocoder",
   "original": "380",
   "order": 800,
   "page_count": 5,
   "abstract": [
    "Generative adversarial network (GAN) based vocoders have achieved significant attention in speech synthesis with high quality and fast inference speed. However, there still exist many noticeable spectral artifacts, resulting in the quality decline of synthesized speech. In this work, we adopt a novel GAN-based vocoder designed for few artifacts and high fidelity, called FA-GAN. To suppress the aliasing artifacts caused by non-ideal upsampling layers in high-frequency components, we introduce the anti-aliased twin deconvolution module in the generator. To alleviate blurring artifacts and enrich the reconstruction of spectral details, we propose a novel fine-grained multi-resolution real and imaginary loss to assist in the modeling of phase information. Experimental results reveal that FA-GAN outperforms the compared approaches in promoting audio quality and alleviating spectral artifacts, and exhibits superior performance when applied to unseen speaker scenarios."
   ],
   "p1": 3884,
   "pn": 3888,
   "doi": "10.21437/Interspeech.2024-380",
   "url": "interspeech_2024/shen24b_interspeech.html"
  },
  "feng24_interspeech": {
   "authors": [
    [
     "Sheng",
     "Feng"
    ],
    [
     "Heyang",
     "Liu"
    ],
    [
     "Yu",
     "Wang"
    ],
    [
     "Yanfeng",
     "Wang"
    ]
   ],
   "title": "Towards an End-to-End Framework for Invasive Brain Signal Decoding with Large Language Models",
   "original": "382",
   "order": 308,
   "page_count": 5,
   "abstract": [
    "In this paper, we introduce a groundbreaking end-to-end (E2E) framework for decoding invasive brain signals, marking a significant advancement in the field of speech neuroprosthesis. Our methodology leverages the comprehensive reasoning abilities of large language models (LLMs) to facilitate direct decoding. By fully integrating LLMs, we achieve results comparable to the state-of-the-art cascade models. Our findings underscore the immense potential of E2E frameworks in speech neuroprosthesis, particularly as the technology behind brain-computer interfaces (BCIs) and the availability of relevant datasets continue to evolve. This work not only showcases the efficacy of combining LLMs with E2E decoding for enhancing speech neuroprosthesis but also sets a new direction for future research in BCI applications, underscoring the impact of LLMs in decoding complex neural signals for communication restoration. Code will be made available at https://github.com/FsFrancis15/BrainLLM."
   ],
   "p1": 1495,
   "pn": 1499,
   "doi": "10.21437/Interspeech.2024-382",
   "url": "interspeech_2024/feng24_interspeech.html"
  },
  "wang24j_interspeech": {
   "authors": [
    [
     "Yi-Wei",
     "Wang"
    ],
    [
     "Ke-Han",
     "Lu"
    ],
    [
     "Kuan-Yu",
     "Chen"
    ]
   ],
   "title": "HypR: A comprehensive study for ASR hypothesis revising with a reference corpus",
   "original": "385",
   "order": 715,
   "page_count": 5,
   "abstract": [
    "With the development of deep learning, automatic speech recognition (ASR) has made significant progress. To further enhance the performance of ASR, revising recognition results is one of the lightweight but efficient manners. Various methods can be roughly classified into N -best reranking modeling and error correction modeling. The former aims to select the hypothesis with the lowest error rate from a set of candidates generated by ASR for a given input speech. The latter focuses on detecting recognition errors in a given hypothesis and correcting these errors to obtain an enhanced result. However, we observe that these studies are hardly comparable to each other, as they are usually evaluated on different corpora, paired with different ASR models, and even use different datasets to train the models. Accordingly, we first concentrate on providing an ASR hypothesis revising (HypR) dataset in this study. HypR contains several commonly used corpora (AISHELL-1, TED-LIUM 2, and LibriSpeech) and provides 50 recognition hypotheses for each speech utterance. The checkpoint models of ASR are also published. In addition, we implement and compare several classic and representative methods, showing the recent research progress in revising speech recognition results. We hope that the publicly available HypR dataset can become a reference benchmark for subsequent research and promote this field of research to an advanced level."
   ],
   "p1": 3495,
   "pn": 3499,
   "doi": "10.21437/Interspeech.2024-385",
   "url": "interspeech_2024/wang24j_interspeech.html"
  },
  "saito24_interspeech": {
   "authors": [
    [
     "Yuki",
     "Saito"
    ],
    [
     "Takuto",
     "Igarashi"
    ],
    [
     "Kentaro",
     "Seki"
    ],
    [
     "Shinnosuke",
     "Takamichi"
    ],
    [
     "Ryuichi",
     "Yamamoto"
    ],
    [
     "Kentaro",
     "Tachibana"
    ],
    [
     "Hiroshi",
     "Saruwatari"
    ]
   ],
   "title": "SRC4VC: Smartphone-Recorded Corpus for Voice Conversion Benchmark",
   "original": "388",
   "order": 374,
   "page_count": 5,
   "abstract": [
    "We present SRC4VC, a new corpus containing 11 hours of speech recorded on smartphones by 100 Japanese speakers. Although high-quality multi-speaker corpora can advance voice conversion (VC) technologies, they are not always suitable for testing VC when low-quality speech recording is given as the input. To this end, we first asked 100 crowdworkers to record their voice samples using smartphones. Then, we annotated the recorded samples with speaker-wise recording-quality scores and utterance-wise perceived emotion labels. We also benchmark SRC4VC on any-to-any VC, in which we trained a multi-speaker VC model on high-quality speech and used the SRC4VC speakers' voice samples as the source in VC. The results show that the recording quality mismatch between the training and evaluation data significantly degrades the VC performance, which can be improved by applying speech enhancement to the low-quality source speech samples."
   ],
   "p1": 1825,
   "pn": 1829,
   "doi": "10.21437/Interspeech.2024-388",
   "url": "interspeech_2024/saito24_interspeech.html"
  },
  "svirsky24_interspeech": {
   "authors": [
    [
     "Jonathan",
     "Svirsky"
    ],
    [
     "Uri",
     "Shaham"
    ],
    [
     "Ofir",
     "Lindenbaum"
    ]
   ],
   "title": "Sparse Binarization for Fast Keyword Spotting",
   "original": "389",
   "order": 618,
   "page_count": 5,
   "abstract": [
    "With the increasing prevalence of voice-activated devices and applications, keyword spotting (KWS) models enable users to interact with technology hands-free, enhancing convenience and accessibility in various contexts. Deploying KWS models on edge devices, such as smartphones and embedded systems, offers significant benefits for real-time applications, privacy, and bandwidth efficiency. However, these devices often possess limited computational power and memory. This necessitates optimizing neural network models for efficiency without significantly compromising their accuracy. To address these challenges, we propose a novel keyword-spotting model based on sparse input representation followed by a linear classifier. The model is four times faster than the previous state-of-the-art edge device-compatible model with better accuracy. We show that our method is also more robust in noisy environments while being fast. Our code is available at: https://github.com/jsvir/sparknet."
   ],
   "p1": 3010,
   "pn": 3014,
   "doi": "10.21437/Interspeech.2024-389",
   "url": "interspeech_2024/svirsky24_interspeech.html"
  },
  "shamsian24_interspeech": {
   "authors": [
    [
     "Aviv",
     "Shamsian"
    ],
    [
     "Aviv",
     "Navon"
    ],
    [
     "Neta",
     "Glazer"
    ],
    [
     "Gill",
     "Hetz"
    ],
    [
     "Joseph",
     "Keshet"
    ]
   ],
   "title": "Keyword-Guided Adaptation of Automatic Speech Recognition",
   "original": "391",
   "order": 148,
   "page_count": 5,
   "abstract": [
    "Automatic Speech Recognition (ASR) technology has made significant progress in recent years, providing accurate transcription across various domains. However, some challenges remain, especially in noisy environments and specialized jargon. In this paper, we propose a novel approach for improved jargon word recognition by contextual biasing Whisper-based models. We employ a keyword spotting model that leverages the Whisper encoder representation to dynamically generate prompts for guiding the decoder during the transcription process. We introduce two approaches to effectively steer the decoder towards these prompts: KG-Whisper, which is aimed at fine-tuning the Whisper decoder, and  KG-Whisper-PT, which learns a prompt prefix. Our results show a significant improvement in the recognition accuracy of specified keywords and in reducing the overall word error rates. Specifically, in unseen language generalization, we demonstrate an average WER improvement of 5.1% over Whisper."
   ],
   "p1": 732,
   "pn": 736,
   "doi": "10.21437/Interspeech.2024-391",
   "url": "interspeech_2024/shamsian24_interspeech.html"
  },
  "bonafos24_interspeech": {
   "authors": [
    [
     "Guillem",
     "Bonafos"
    ],
    [
     "Clara",
     "Bourot"
    ],
    [
     "Pierre",
     "Pudlo"
    ],
    [
     "Jean-Marc",
     "Freyermuth"
    ],
    [
     "Laurence",
     "Reboul"
    ],
    [
     "Samuel",
     "Tronçon"
    ],
    [
     "Arnaud",
     "Rey"
    ]
   ],
   "title": "Dirichlet process mixture model based on topologically augmented signal representation for clustering infant vocalizations",
   "original": "394",
   "order": 737,
   "page_count": 5,
   "abstract": [
    "Based on audio recordings made once a month during the first 12 months of a child's life, we propose a new method for clustering this set of vocalizations. We use a topologically augmented representation of the vocalizations, employing two persistence diagrams for each vocalization: one computed on the surface of its spectrogram and one on the Takens' embeddings of the vocalization. A synthetic persistent variable is derived for each diagram and added to the MFCCs (Mel-frequency cepstral coefficients). Using this representation, we fit a non-parametric Bayesian mixture model with a Dirichlet process prior to model the number of components. This procedure leads to a novel data-driven categorization of vocal productions. Our findings reveal the presence of 8 clusters of vocalizations, allowing us to compare their temporal distribution and acoustic profiles in the first 12 months of life. "
   ],
   "p1": 3605,
   "pn": 3609,
   "doi": "10.21437/Interspeech.2024-394",
   "url": "interspeech_2024/bonafos24_interspeech.html"
  },
  "ullah24_interspeech": {
   "authors": [
    [
     "Asad",
     "Ullah"
    ],
    [
     "Alessandro",
     "Ragano"
    ],
    [
     "Andrew",
     "Hines"
    ]
   ],
   "title": "Reduce, Reuse, Recycle: Is Perturbed Data Better than Other Language Augmentation for Low Resource Self-Supervised Speech Models",
   "original": "396",
   "order": 17,
   "page_count": 5,
   "abstract": [
    "Self-supervised representation learning (SSRL) has demonstrated superior performance than supervised models for tasks including phoneme recognition. Training SSRL models poses a challenge for low-resource languages where sufficient pre-training data may not be available. A common approach is cross-lingual pre-training. Instead, we propose to use audio augmentation techniques, namely: pitch variation, noise addition, accented target language and other language speech to pre-train SSRL models in a low resource condition and evaluate phoneme recognition. Our comparisons found that a combined synthetic augmentations (noise/pitch) strategy outperformed accent and language knowledge transfer. Furthermore, we examined the scaling factor of augmented data to achieve equivalent performance to model pre-trained with target domain speech. Our findings suggest that for resource-constrained languages, combined augmentations can be a viable option than other augmentations."
   ],
   "p1": 77,
   "pn": 81,
   "doi": "10.21437/Interspeech.2024-396",
   "url": "interspeech_2024/ullah24_interspeech.html"
  },
  "cho24_interspeech": {
   "authors": [
    [
     "Deok-Hyeon",
     "Cho"
    ],
    [
     "Hyung-Seok",
     "Oh"
    ],
    [
     "Seung-Bin",
     "Kim"
    ],
    [
     "Sang-Hoon",
     "Lee"
    ],
    [
     "Seong-Whan",
     "Lee"
    ]
   ],
   "title": "EmoSphere-TTS: Emotional Style and Intensity Modeling via Spherical Emotion Vector for Controllable Emotional Text-to-Speech",
   "original": "398",
   "order": 371,
   "page_count": 5,
   "abstract": [
    "Despite rapid advances in the field of emotional text-to-speech (TTS), recent studies primarily focus on mimicking the average style of a particular emotion. As a result, the ability to manipulate speech emotion remains constrained to several predefined labels, compromising the ability to reflect the nuanced variations of emotion. In this paper, we propose EmoSphere-TTS, which synthesizes expressive emotional speech by using a spherical emotion vector to control the emotional style and intensity of the synthetic speech. Without any human annotation, we use the arousal, valence, and dominance pseudo-labels to model the complex nature of emotion via a Cartesian-spherical transformation. Furthermore, we propose a dual conditional adversarial network to improve the quality of generated speech by reflecting the multi-aspect characteristics. The experimental results demonstrate the model’s ability to control emotional style and intensity with high-quality expressive speech."
   ],
   "p1": 1810,
   "pn": 1814,
   "doi": "10.21437/Interspeech.2024-398",
   "url": "interspeech_2024/cho24_interspeech.html"
  },
  "zhang24b_interspeech": {
   "authors": [
    [
     "Chenyuan",
     "Zhang"
    ],
    [
     "Linkai",
     "Luo"
    ],
    [
     "Hong",
     "Peng"
    ],
    [
     "Wei",
     "Wen"
    ]
   ],
   "title": "Variable Segment Length and Domain-Adapted Feature Optimization for Speaker Diarization",
   "original": "399",
   "resource": "https://doi.org/10.5281/zenodo.12738767",
   "order": 772,
   "page_count": 5,
   "abstract": [
    "In speaker diarization, a suitable segment length is still a challenge. Long segments may contain multiple speakers, leading to unreliable embeddings, while short segments may lack sufficient information. We propose an approach of variable segment length using a mixed segment recognition (MSR) network to address this. The MSR module distinguishes between segments with multiple speakers and those with a single speaker. Identified mixed segments are re-cut until pure or reaching the minimum length. In addition, we propose a scheme of domain-adapted feature optimization to fine-tune the pre-trained speaker embedding extractor, where both a specific data augmentation and a distance loss function are used to improve embeddings of the remaining segments still with speaker alternation and overlap. The results demonstrate the effectiveness of our method. It achieves a relative improvement of 25.5% in diarization error rate over the baseline and surpasses the recent state-of-the-art methods."
   ],
   "p1": 3744,
   "pn": 3748,
   "doi": "10.21437/Interspeech.2024-399",
   "url": "interspeech_2024/zhang24b_interspeech.html"
  },
  "kunesova24_interspeech": {
   "authors": [
    [
     "Marie",
     "Kunešová"
    ],
    [
     "Jan",
     "Lehečka"
    ],
    [
     "Josef",
     "Michálek"
    ],
    [
     "Jindrich",
     "Matousek"
    ],
    [
     "Jan",
     "Švec"
    ]
   ],
   "title": "Zero-shot Out-of-domain is No Joke: Lessons Learned in the VoiceMOS 2023 MOS Prediction Challenge",
   "original": "400",
   "order": 1007,
   "page_count": 5,
   "abstract": [
    "This paper describes our team’s experiences in the VoiceMOS Challenge 2023 - a challenge centered around the evaluation of the quality of synthetic or noisy speech. Inspired by our success with an ensemble approach in the first VoiceMOS Challenge in 2022, we submitted an ensemble of four models this time, based on wav2vec 2.0, QuartzNet, CNN-RNN, and LDNet. This was enough to win one of the two tracks we participated in (Track 1b). However, post-challenge analysis shows that only two of the models offer a meaningful contribution in any of the VoiceMOS 2023 tracks, while the other two only degrade the ensemble’s overall performance. On the other hand, post-challenge results on Track 2 (singing voice conversion data) surpassed all our expectations. In the paper, we explain how we tried to deal with the new zero-shot out-of-domain scenarios, analyze the results, and discuss the lessons learned."
   ],
   "p1": 4913,
   "pn": 4917,
   "doi": "10.21437/Interspeech.2024-400",
   "url": "interspeech_2024/kunesova24_interspeech.html"
  },
  "rosello24_interspeech": {
   "authors": [
    [
     "Eros",
     "Rosello"
    ],
    [
     "Angel M.",
     "Gomez"
    ],
    [
     "Iván",
     "López-Espejo"
    ],
    [
     "Antonio M.",
     "Peinado"
    ],
    [
     "Juan M.",
     "Martín-Doñas"
    ]
   ],
   "title": "Anti-spoofing Ensembling Model: Dynamic Weight Allocation in Ensemble Models for Improved Voice Biometrics Security",
   "original": "403",
   "order": 101,
   "page_count": 5,
   "abstract": [
    "This paper proposes an ensembling model as spoofed speech countermeasure, with a particular focus on synthetic voice. Despite the recent advances in speaker verification based on deep neural networks, this technology is still susceptible to various malicious attacks, so that some kind of countermeasures are needed. While an increasing number of anti-spoofing techniques can be found in the literature, the combination of multiple models, or ensemble models, still proves to be one of the best approaches. However, current iterations often rely on fixed weight assignments, potentially neglecting the unique strengths of each individual model. In response, we propose a novel ensembling model, an adaptive neural network-based approach that dynamically adjusts weights based on input utterances. Our experimental findings show that this approach outperforms traditional weighted score averaging techniques, showcasing its ability to adapt to diverse audio characteristics effectively."
   ],
   "p1": 497,
   "pn": 501,
   "doi": "10.21437/Interspeech.2024-403",
   "url": "interspeech_2024/rosello24_interspeech.html"
  },
  "wang24k_interspeech": {
   "authors": [
    [
     "Tianzi",
     "Wang"
    ],
    [
     "Xurong",
     "Xie"
    ],
    [
     "Zhaoqing",
     "Li"
    ],
    [
     "Shoukang",
     "Hu"
    ],
    [
     "Zengrui",
     "Jin"
    ],
    [
     "Jiajun",
     "Deng"
    ],
    [
     "Mingyu",
     "Cui"
    ],
    [
     "Shujie",
     "Hu"
    ],
    [
     "Mengzhe",
     "Geng"
    ],
    [
     "Guinan",
     "Li"
    ],
    [
     "Helen",
     "Meng"
    ],
    [
     "Xunying",
     "Liu"
    ]
   ],
   "title": "Towards Effective and Efficient Non-autoregressive Decoding Using Block-based Attention Mask",
   "original": "404",
   "order": 54,
   "page_count": 5,
   "abstract": [
    "This paper proposes a novel non-autoregressive (NAR) block-based Attention Mask Decoder (AMD) that flexibly balances performance-efficiency trade-offs for Conformer ASR systems. AMD performs parallel NAR inference within contiguous blocks of output labels that are concealed using attention masks, while conducting left-to-right AR prediction and history context amalgamation between blocks. A beam search algorithm is designed to leverage a dynamic fusion of CTC, AR Decoder, and AMD probabilities. Experiments on the LibriSpeech-100hr corpus suggest the tripartite Decoder incorporating the AMD module produces a maximum decoding speed-up ratio of 1.73x over the baseline CTC+AR decoding, while incurring no statistically significant word error rate (WER) increase on the test sets. When operating with the same decoding real time factors,  statistically significant WER reductions of up to 0.7% and 0.3% absolute (5.3% and 6.1% relative) were obtained over the CTC+AR baseline."
   ],
   "p1": 262,
   "pn": 266,
   "doi": "10.21437/Interspeech.2024-404",
   "url": "interspeech_2024/wang24k_interspeech.html"
  },
  "xin24_interspeech": {
   "authors": [
    [
     "Yifei",
     "Xin"
    ],
    [
     "Xuxin",
     "Cheng"
    ],
    [
     "Zhihong",
     "Zhu"
    ],
    [
     "Xusheng",
     "Yang"
    ],
    [
     "Yuexian",
     "Zou"
    ]
   ],
   "title": "DiffATR: Diffusion-based Generative Modeling for Audio-Text Retrieval",
   "original": "405",
   "order": 343,
   "page_count": 5,
   "abstract": [
    "Existing audio-text retrieval (ATR) methods are essentially discriminative models that aim to maximize the conditional likelihood, represented as p(candidates|query). Nevertheless, this methodology fails to consider the intrinsic data distribution p(query), leading to difficulties in discerning out-of-distribution data. In this work, we attempt to tackle this constraint through a generative perspective and model the relationship between audio and text as their joint probability p(candidates, query). To this end, we present a diffusion-based ATR framework (DiffATR), which models ATR as an iterative procedure that progressively generates joint distribution from noise. Throughout its training phase, DiffATR is optimized from both generative and discriminative viewpoints: the generator is refined through a generation loss, while the feature extractor benefits from a contrastive loss, thus combining the merits of both methodologies. Experiments on the AudioCaps and Clotho datasets with superior performances, verify the effectiveness of our approach. Notably, without any alterations, our DiffATR consistently exhibits strong performance in out-of-domain retrieval settings."
   ],
   "p1": 1670,
   "pn": 1674,
   "doi": "10.21437/Interspeech.2024-405",
   "url": "interspeech_2024/xin24_interspeech.html"
  },
  "sato24_interspeech": {
   "authors": [
    [
     "Hiroshi",
     "Sato"
    ],
    [
     "Takafumi",
     "Moriya"
    ],
    [
     "Masato",
     "Mimura"
    ],
    [
     "Shota",
     "Horiguchi"
    ],
    [
     "Tsubasa",
     "Ochiai"
    ],
    [
     "Takanori",
     "Ashihara"
    ],
    [
     "Atsushi",
     "Ando"
    ],
    [
     "Kentaro",
     "Shinayama"
    ],
    [
     "Marc",
     "Delcroix"
    ]
   ],
   "title": "SpeakerBeam-SS: Real-time Target Speaker Extraction with Lightweight Conv-TasNet and State Space Modeling",
   "original": "413",
   "order": 1031,
   "page_count": 5,
   "abstract": [
    "Real-time target speaker extraction (TSE) is intended to extract the desired speaker's voice from the observed mixture of multiple speakers in a streaming manner. Implementing real-time TSE is challenging as the computational complexity must be reduced to provide real-time operation. This work introduces to Conv-TasNet-based TSE a new architecture based on state space modeling (SSM) that has been shown to model long-term dependency effectively. Owing to SSM, fewer dilated convolutional layers are required to capture temporal dependency in Conv-TasNet, resulting in the reduction of model complexity. We also enlarge the window length and shift of the convolutional (TasNet) frontend encoder to reduce the computational cost further; the performance decline is compensated by over-parameterization of the frontend encoder. The proposed method reduces the real-time factor by 78% from the conventional causal Conv-TasNet-based TSE while matching its performance."
   ],
   "p1": 5033,
   "pn": 5037,
   "doi": "10.21437/Interspeech.2024-413",
   "url": "interspeech_2024/sato24_interspeech.html"
  },
  "monaghan24_interspeech": {
   "authors": [
    [
     "Jessica",
     "Monaghan"
    ],
    [
     "Arun",
     "Sebastian"
    ],
    [
     "Nicky",
     "Chong-White"
    ],
    [
     "Vicky",
     "Zhang"
    ],
    [
     "Vijayalakshmi",
     "Easwar"
    ],
    [
     "Padraig",
     "Kitterick"
    ]
   ],
   "title": "Automatic Detection of Hearing Loss from Children's Speech using wav2vec 2.0 Features",
   "original": "414",
   "order": 180,
   "page_count": 5,
   "abstract": [
    "This study explores the feasibility of employing machine learning to classify acoustic features of speech for detecting hearing loss in preschool children. Acknowledging the critical developmental impacts of early hearing loss identification and the challenges associated with traditional testing methods for this age group, we propose a novel, scalable approach leveraging automatic speech analysis. Using speech recordings from children with and without hearing loss, we used wav2vec 2.0 and ComParE feature sets to capture speech characteristics and compared LSTM, DNN, and XGBoost classifiers. Our findings reveal that these models can accurately distinguish between the speech of children with hearing loss and those with normal hearing, achieving up to 96.4% accuracy. This proof-of-concept study indicates the potential of using speech for early hearing loss detection, and a path toward non-intrusive, scalable screening tools that could significantly benefit early developmental outcomes."
   ],
   "p1": 892,
   "pn": 896,
   "doi": "10.21437/Interspeech.2024-414",
   "url": "interspeech_2024/monaghan24_interspeech.html"
  },
  "zaitova24_interspeech": {
   "authors": [
    [
     "Iuliia",
     "Zaitova"
    ],
    [
     "Irina",
     "Stenger"
    ],
    [
     "Wei",
     "Xue"
    ],
    [
     "Tania",
     "Avgustinova"
    ],
    [
     "Bernd",
     "Möbius"
    ],
    [
     "Dietrich",
     "Klakow"
    ]
   ],
   "title": "Cross-Linguistic Intelligibility of Non-Compositional Expressions in Spoken Context",
   "original": "416",
   "resource": "https://doi.org/10.5281/zenodo.12800247",
   "order": 862,
   "page_count": 5,
   "abstract": [
    "This study investigates intelligibility of non-compositional expressions in spoken context for five closely related Slavic languages (Belarusian, Bulgarian, Czech, Polish, and Ukrainian) by native Russian speakers. Our investigation employs a web-based experiment involving free-response and multiple-choice translation tasks. Drawing on prior research, two factors were examined: (1) linguistic similarities (orthographic and phonological distances), and (2) surprisal scores obtained from two multilingual speech representation (SR) models fine-tuned for Russian (Wav2Vec2-Large-Ru-Golos-With-LM and Whisper Medium Russian).  According to the results of Pearson correlation and regression analyses, phonological distance appears to be a better predictor of intelligibility scores than SR surprisal."
   ],
   "p1": 4189,
   "pn": 4193,
   "doi": "10.21437/Interspeech.2024-416",
   "url": "interspeech_2024/zaitova24_interspeech.html"
  },
  "turetzky24_interspeech": {
   "authors": [
    [
     "Arnon",
     "Turetzky"
    ],
    [
     "Or",
     "Tal"
    ],
    [
     "Yael",
     "Segal"
    ],
    [
     "Yehoshua",
     "Dissen"
    ],
    [
     "Ella",
     "Zeldes"
    ],
    [
     "Amit",
     "Roth"
    ],
    [
     "Eyal",
     "Cohen"
    ],
    [
     "Yosi",
     "Shrem"
    ],
    [
     "Bronya R.",
     "Chernyak"
    ],
    [
     "Olga",
     "Seleznova"
    ],
    [
     "Joseph",
     "Keshet"
    ],
    [
     "Yossi",
     "Adi"
    ]
   ],
   "title": "HebDB: a Weakly Supervised Dataset for Hebrew Speech Processing",
   "original": "417",
   "order": 281,
   "page_count": 5,
   "abstract": [
    "We present HebDB, a weakly supervised dataset for spoken language processing in the Hebrew language. HebDB offers roughly 2500 hours of natural and spontaneous speech recordings in the Hebrew language, consisting of a large variety of speakers and topics. We provide raw recordings together with a pre-processed, weakly supervised, and filtered version. The goal of HebDB is to further enhance research and development of spoken language processing tools for the Hebrew language. Hence, we additionally provide two baseline systems for Automatic Speech Recognition (ASR): (i) a self-supervised model; and (ii) a fully supervised model. We present the performance of these two methods optimized on HebDB and compare them to current multi-lingual ASR alternatives. Results suggest the proposed method reaches better results than the evaluated baselines considering similar model sizes. Dataset, code, and models are publicly available under https://pages.cs.huji.ac.il/adiyoss-lab/HebDB/."
   ],
   "p1": 1360,
   "pn": 1364,
   "doi": "10.21437/Interspeech.2024-417",
   "url": "interspeech_2024/turetzky24_interspeech.html"
  },
  "xue24_interspeech": {
   "authors": [
    [
     "Wei",
     "Xue"
    ],
    [
     "Ivan",
     "Yuen"
    ],
    [
     "Bernd",
     "Möbius"
    ]
   ],
   "title": "Towards a better understanding of receptive multilingualism: listening conditions and priming effects",
   "original": "418",
   "order": 4,
   "page_count": 5,
   "abstract": [
    "Receptive multilingualism is a form of communication where speakers can comprehend an utterance of a foreign language (Lx) using their native language (L1) when L1 and Lx share similarities in, e.g., vocabulary and pronunciation. The success of receptive multilingualism can be tested by examining accuracy and reaction time of auditory word recognition (AWR) of target words in lexical decision tasks. AWR in such tasks can be affected by adverse listening conditions due to environmental noises and by the presence of a preceding prime word. This study explores whether AWR of L1 in Lx-L1 pairs (Lx = Dutch; L1 = German or English) will be affected by different degrees of similarities in their phonology and semantics and whether such an influence will differ as a function of listening condition. We observed less accurate and slower responses without semantic similarity but a null effect on accuracy without phonological overlap. The interaction with listening conditions is language-dependent."
   ],
   "p1": 12,
   "pn": 16,
   "doi": "10.21437/Interspeech.2024-418",
   "url": "interspeech_2024/xue24_interspeech.html"
  },
  "yan24_interspeech": {
   "authors": [
    [
     "Zhiyong",
     "Yan"
    ],
    [
     "Heinrich",
     "Dinkel"
    ],
    [
     "Yongqing",
     "Wang"
    ],
    [
     "Jizhong",
     "Liu"
    ],
    [
     "Junbo",
     "Zhang"
    ],
    [
     "Yujun",
     "Wang"
    ],
    [
     "Bin",
     "Wang"
    ]
   ],
   "title": "Bridging Language Gaps in Audio-Text Retrieval",
   "original": "420",
   "order": 344,
   "page_count": 5,
   "abstract": [
    "Audio-text retrieval is a challenging task, requiring the search for an audio clip or a text caption within a database.  The predominant focus of existing research on English descriptions poses a limitation on the applicability of such models, given the abundance of non-English content in real-world data. To address these linguistic disparities, we propose a language enhancement (LE), using a multilingual text encoder (SONAR) to encode the text data with language-specific information. Additionally, we optimize the audio encoder through the application of consistent ensemble distillation (CED), enhancing support for variable-length audio-text retrieval. Our methodology excels in English audio-text retrieval, demonstrating state-of-the-art (SOTA) performance on commonly used datasets such as AudioCaps and Clotho. Simultaneously, the approach exhibits proficiency in retrieving content in seven other languages with only 10\\% of additional language-enhanced training data, yielding promising results."
   ],
   "p1": 1675,
   "pn": 1679,
   "doi": "10.21437/Interspeech.2024-420",
   "url": "interspeech_2024/yan24_interspeech.html"
  },
  "wang24l_interspeech": {
   "authors": [
    [
     "Xin",
     "Wang"
    ],
    [
     "Tomi",
     "Kinnunen"
    ],
    [
     "Kong Aik",
     "Lee"
    ],
    [
     "Paul-Gauthier",
     "Noé"
    ],
    [
     "Junichi",
     "Yamagishi"
    ]
   ],
   "title": "Revisiting and Improving Scoring Fusion for Spoofing-aware Speaker Verification Using Compositional Data Analysis",
   "original": "422",
   "order": 231,
   "page_count": 5,
   "abstract": [
    "Fusing outputs from automatic speaker verification (ASV) and spoofing countermeasure (CM) is expected to make an integrated system robust to zero-effort imposters and synthesized spoofing attacks. Many score-level fusion methods have been proposed, but many remain heuristic. This paper revisits score-level fusion using tools from decision theory and presents three main findings. First, fusion by summing the ASV and CM scores can be interpreted on the basis of compositional data analysis, and score calibration before fusion is essential. Second, the interpretation leads to an improved fusion method that linearly combines the log-likelihood ratios of ASV and CM. However, as the third finding reveals, this linear combination is inferior to a non-linear one in making optimal decisions. The outcomes of these findings, namely, the score calibration before fusion, improved linear fusion, and better non-linear fusion, were found to be effective on the SASV challenge database."
   ],
   "p1": 1110,
   "pn": 1114,
   "doi": "10.21437/Interspeech.2024-422",
   "url": "interspeech_2024/wang24l_interspeech.html"
  },
  "kim24e_interspeech": {
   "authors": [
    [
     "Miseul",
     "Kim"
    ],
    [
     "Soo-Whan",
     "Chung"
    ],
    [
     "Youna",
     "Ji"
    ],
    [
     "Hong-Goo",
     "Kang"
    ],
    [
     "Min-Seok",
     "Choi"
    ]
   ],
   "title": "Speak in the Scene: Diffusion-based Acoustic Scene Transfer toward Immersive Speech Generation",
   "original": "425",
   "order": 1001,
   "page_count": 5,
   "abstract": [
    "This paper introduces a novel task in generative speech processing, Acoustic Scene Transfer (AST), which aims to transfer acoustic scenes of speech signals to diverse environments. AST promises an immersive experience in speech perception by adapting the acoustic scene behind speech signals to desired environments. We propose AST-LDM for the AST task, which generates speech signals accompanied by the target acoustic scene of the reference prompt. Specifically, AST-LDM is a latent diffusion model conditioned by CLAP embeddings that describe target acoustic scenes in either audio or text modalities. The contributions of this paper include introducing the AST task and implementing its baseline model. For AST-LDM, we emphasize its core framework, which is to preserve the input speech and generate audio consistently with both the given speech and the target acoustic environment. Experiments, including objective and subjective tests, validate the feasibility and efficacy of our approach."
   ],
   "p1": 4883,
   "pn": 4887,
   "doi": "10.21437/Interspeech.2024-425",
   "url": "interspeech_2024/kim24e_interspeech.html"
  },
  "sun24b_interspeech": {
   "authors": [
    [
     "Haiyang",
     "Sun"
    ],
    [
     "Fulin",
     "Zhang"
    ],
    [
     "Yingying",
     "Gao"
    ],
    [
     "Shilei",
     "Zhang"
    ],
    [
     "Zheng",
     "Lian"
    ],
    [
     "Junlan",
     "Feng"
    ]
   ],
   "title": "MFSN: Multi-perspective Fusion Search Network For Pre-training Knowledge in Speech Emotion Recognition",
   "original": "427",
   "order": 965,
   "page_count": 5,
   "abstract": [
    "Speech Emotion Recognition (SER) is an important research topic in human-computer interaction. Many recent works focus on directly extracting emotional cues through pre-trained knowledge, frequently overlooking considerations of appropriateness and comprehensiveness. Therefore, we propose a novel framework for pre-training knowledge in SER, called Multi-perspective Fusion Search Network (MFSN). Considering comprehensiveness, we partition speech knowledge into Textual-related Emotional Content (TEC) and Speech-related Emotional Content (SEC), capturing cues from both semantic and acoustic perspectives, and we design a new architecture search space to fully leverage them. Considering appropriateness, we verifies the efficacy of different modeling approaches in capturing SEC and fills the gap in current research. Experimental results on multiple datasets demonstrate the superiority of MFSN."
   ],
   "p1": 4703,
   "pn": 4707,
   "doi": "10.21437/Interspeech.2024-427",
   "url": "interspeech_2024/sun24b_interspeech.html"
  },
  "hwang24b_interspeech": {
   "authors": [
    [
     "Hyun Kyung",
     "Hwang"
    ],
    [
     "Manami",
     "Hirayama"
    ]
   ],
   "title": "Acquisition of high vowel devoicing in Japanese: A production experiment with three and four year olds",
   "original": "428",
   "order": 860,
   "page_count": 4,
   "abstract": [
    "The purpose of this study is to investigate the developmental path of high vowel devoicing (HVD) in Japanese. A picture-naming task was conducted with Japanese-learning preschoolers of three and four years old. The empirical data presented in this study allow us not only to make comparisons with the data from 4 year-olds in a previous study, but also to address the devoicing patterns for the 3-year old children, which have received little attention in the developmental literature on HVD. The results of twenty children reveal distinct patterns depending on position; Word-medially, the overall occurrence of HVD increases if we compare the average HVD rates between the ages of 3 and 4, but their rate is not yet reached at the adult-like level at the age of four. Word-finally, on the other hand, the rates are overall lower than the word-medial devoicing. Further, unlike the incremental pattern observed in the rates for the word-medial devoicing, no clear developmental advancement is found in this position. The presence or absence of the developmental advancement appears to support the qualitative differences between two types of HVDs in distinct positions."
   ],
   "p1": 4180,
   "pn": 4183,
   "doi": "10.21437/Interspeech.2024-428",
   "url": "interspeech_2024/hwang24b_interspeech.html"
  },
  "rousso24_interspeech": {
   "authors": [
    [
     "Rotem",
     "Rousso"
    ],
    [
     "Eyal",
     "Cohen"
    ],
    [
     "Joseph",
     "Keshet"
    ],
    [
     "Eleanor",
     "Chodroff"
    ]
   ],
   "title": "Tradition or Innovation: A Comparison of Modern ASR Methods for Forced Alignment",
   "original": "429",
   "order": 314,
   "page_count": 5,
   "abstract": [
    "Forced alignment (FA) plays a key role in speech research through the automatic time alignment of speech signals with corresponding text transcriptions. Despite the move towards end-to-end architectures for speech technology, FA is still dominantly achieved through a classic GMM-HMM acoustic model. This work directly compares alignment performance from leading automatic speech recognition (ASR) methods, WhisperX and Massively Multilingual Speech Recognition (MMS), against a Kaldi-based GMM-HMM system, the Montreal Forced Aligner (MFA). Performance was assessed on the manually aligned TIMIT and Buckeye datasets, with comparisons conducted only on words correctly recognized by WhisperX and MMS. The MFA outperformed both WhisperX and MMS, revealing a shortcoming of modern ASR systems. These findings highlight the need for advancements in forced alignment and emphasize the importance of integrating traditional expertise with modern innovation to foster progress."
   ],
   "p1": 1525,
   "pn": 1529,
   "doi": "10.21437/Interspeech.2024-429",
   "url": "interspeech_2024/rousso24_interspeech.html"
  },
  "ahn24_interspeech": {
   "authors": [
    [
     "Young Jin",
     "Ahn"
    ],
    [
     "Jungwoo",
     "Park"
    ],
    [
     "Sangha",
     "Park"
    ],
    [
     "Jonghyun",
     "Choi"
    ],
    [
     "Kee-Eung",
     "Kim"
    ]
   ],
   "title": "SyncVSR: Data-Efficient Visual Speech Recognition with End-to-End Crossmodal Audio Token Synchronization",
   "original": "432",
   "order": 175,
   "page_count": 5,
   "abstract": [
    "Visual Speech Recognition (VSR) stands at the intersection of computer vision and speech recognition, aiming to interpret spoken content from visual cues. A prominent challenge in VSR is the presence of homophenes—visually similar lip gestures that represent different phonemes. Prior approaches have sought to distinguish fine-grained visemes by aligning visual and auditory semantics, but often fell short of full synchronization. To address this, we present SyncVSR, an end-to-end learning framework that leverages quantized audio for frame-level crossmodal supervision. By integrating a projection layer that synchronizes visual representation with acoustic data, our encoder learns to generate discrete audio tokens from a video sequence in a non-autoregressive manner. SyncVSR shows versatility across tasks, languages, and modalities at the cost of a forward pass. Our empirical evaluations show that it not only achieves state-of-the-art results but also reduces data usage by up to ninefold."
   ],
   "p1": 867,
   "pn": 871,
   "doi": "10.21437/Interspeech.2024-432",
   "url": "interspeech_2024/ahn24_interspeech.html"
  },
  "zhang24c_interspeech": {
   "authors": [
    [
     "Fengrun",
     "Zhang"
    ],
    [
     "Wangjin",
     "Zhou"
    ],
    [
     "Yiming",
     "Liu"
    ],
    [
     "Wang",
     "Geng"
    ],
    [
     "Yahui",
     "Shan"
    ],
    [
     "Chen",
     "Zhang"
    ]
   ],
   "title": "Disentangling Age and Identity with a Mutual Information Minimization for Cross-Age Speaker Verification",
   "original": "434",
   "order": 781,
   "page_count": 5,
   "abstract": [
    "There has been an increasing research interest in cross-age speaker verification (CASV). However, existing speaker verification systems perform poorly in CASV due to the great individual differences in voice caused by aging. In this paper, we propose a disentangled representation learning framework for CASV based on mutual information (MI) minimization. In our method, a backbone model is trained to disentangle the identity- and age-related embeddings from speaker information, and an MI estimator is trained to minimize the correlation between age- and identity-related embeddings via MI minimization, resulting in age-invariant speaker embeddings. Furthermore, by using the age gaps between positive and negative samples, we propose an aging-aware MI minimization loss function that allows the backbone model to focus more on the vocal changes with large age gaps. Experimental results show that the proposed method outperforms other methods on multiple Cross-Age test sets of Vox-CA."
   ],
   "p1": 3789,
   "pn": 3793,
   "doi": "10.21437/Interspeech.2024-434",
   "url": "interspeech_2024/zhang24c_interspeech.html"
  },
  "makishima24_interspeech": {
   "authors": [
    [
     "Naoki",
     "Makishima"
    ],
    [
     "Naotaka",
     "Kawata"
    ],
    [
     "Mana",
     "Ihori"
    ],
    [
     "Tomohiro",
     "Tanaka"
    ],
    [
     "Shota",
     "Orihashi"
    ],
    [
     "Atsushi",
     "Ando"
    ],
    [
     "Ryo",
     "Masumura"
    ]
   ],
   "title": "SOMSRED: Sequential Output Modeling for Joint Multi-talker Overlapped Speech Recognition and Speaker Diarization",
   "original": "436",
   "order": 341,
   "page_count": 5,
   "abstract": [
    "This paper proposes SOMSRED, which jointly models the multi-talker automatic speech recognition (ASR) and speaker diarization (SD) for fully overlapped speech of unknown speakers. The conventional method that jointly estimates ASR and SD requires non-overlapping speech and a  separate clustering-based SD component for accurately identifying speakers. However, the speech is often overlapped, which deteriorates speaker identification performance, and the separate model makes the whole system sub-optimal. To address this problem, our idea is to build a sequential output model that outputs transcriptions, timestamps, and newly introduced speaker identifiers recursively from overlapped speech. Since speaker identifier do not fully represent the speaker characteristics of unknown speakers, SOMSRED utilizes the intermediate feature as speaker embeddings. Experimental results show the efficacy of the proposed method in speaker recognition, SD, and multi-talker ASR."
   ],
   "p1": 1660,
   "pn": 1664,
   "doi": "10.21437/Interspeech.2024-436",
   "url": "interspeech_2024/makishima24_interspeech.html"
  },
  "qiu24_interspeech": {
   "authors": [
    [
     "Xihang",
     "Qiu"
    ],
    [
     "Lixian",
     "Zhu"
    ],
    [
     "Zikai",
     "Song"
    ],
    [
     "Zeyu",
     "Chen"
    ],
    [
     "Haojie",
     "Zhang"
    ],
    [
     "Kun",
     "Qian"
    ],
    [
     "Ye",
     "Zhang"
    ],
    [
     "Bin",
     "Hu"
    ],
    [
     "Yoshiharu",
     "Yamamoto"
    ],
    [
     "Björn W.",
     "Schuller"
    ]
   ],
   "title": "Study Selectively: An Adaptive Knowledge Distillation based on a Voting Network for Heart Sound Classification",
   "original": "439",
   "order": 29,
   "page_count": 5,
   "abstract": [
    "Phonocardiogram classification methods using deep neural networks have been widely applied to the early detection of cardiovascular diseases recently. Despite their excellent recognition rate, the sizeable computational complexity limits their further development. Nowadays, knowledge distillation (KD) is an established paradigm for model compression. While current research on multi-teacher KD has shown potential to impart more comprehensive knowledge to the student than single-teacher KD, this approach is not suitable for all scenarios. This paper proposes a novel KD strategy to realise an adaptive multi-teacher instruction mechanism. We design a teacher selection strategy called voting network to tell the contribution of different teachers on each distillation points, so that the student can choose the useful information and renounce the redundant one. An evaluation demonstrates that our method reaches excellent accuracy (92.8%) while maintaining a low computational complexity (0.7M)."
   ],
   "p1": 137,
   "pn": 141,
   "doi": "10.21437/Interspeech.2024-439",
   "url": "interspeech_2024/qiu24_interspeech.html"
  },
  "moriya24_interspeech": {
   "authors": [
    [
     "Takafumi",
     "Moriya"
    ],
    [
     "Takanori",
     "Ashihara"
    ],
    [
     "Masato",
     "Mimura"
    ],
    [
     "Hiroshi",
     "Sato"
    ],
    [
     "Kohei",
     "Matsuura"
    ],
    [
     "Ryo",
     "Masumura"
    ],
    [
     "Taichi",
     "Asami"
    ]
   ],
   "title": "Boosting Hybrid Autoregressive Transducer-based ASR with Internal Acoustic Model Training and Dual Blank Thresholding",
   "original": "442",
   "order": 709,
   "page_count": 5,
   "abstract": [
    "A hybrid autoregressive transducer (HAT) is a variant of neural transducer that models blank and non-blank posterior distributions separately. In this paper, we propose a novel internal acoustic model (IAM) training strategy to enhance HAT-based speech recognition. IAM consists of encoder and joint networks, which are fully shared and jointly trained with HAT. This joint training not only enhances the HAT training efficiency but also encourages IAM and HAT to emit blanks synchronously which skips the more expensive non-blank computation, resulting in more effective blank thresholding for faster decoding. Experiments demonstrate that the relative error reductions of the HAT with IAM compared to the vanilla HAT are statistically significant. Moreover, we introduce dual blank thresholding, which combines both HAT- and IAM-blank thresholding and a compatible decoding algorithm. This results in a 42-75% decoding speed-up with no major performance degradation."
   ],
   "p1": 3465,
   "pn": 3469,
   "doi": "10.21437/Interspeech.2024-442",
   "url": "interspeech_2024/moriya24_interspeech.html"
  },
  "hsieh24_interspeech": {
   "authors": [
    [
     "I-Ting",
     "Hsieh"
    ],
    [
     "Chung-Hsien",
     "Wu"
    ]
   ],
   "title": "Dysarthric Speech Recognition Using Curriculum Learning and Articulatory Feature Embedding",
   "original": "444",
   "order": 269,
   "page_count": 5,
   "abstract": [
    "Recognizing speech in individuals with articulation disorders is a challenging task due to limited resources and diverse speaker characteristics. Domain adaptation is commonly employed to address these issues, and in this paper, we apply curriculum learning, a method within this approach, to Automatic Speech Recognition (ASR). To enhance the efficiency of curriculum learning, we reorganize the dataset. Additionally, we incorporate speaker and articulatory features to capture the pronunciation characteristics of patients. Experimental results demonstrate that our proposed method achieves an 11.37% improvement compared to the baseline."
   ],
   "p1": 1300,
   "pn": 1304,
   "doi": "10.21437/Interspeech.2024-444",
   "url": "interspeech_2024/hsieh24_interspeech.html"
  },
  "vankeirsbilck24_interspeech": {
   "authors": [
    [
     "Matthijs",
     "Van keirsbilck"
    ],
    [
     "Alexander",
     "Keller"
    ]
   ],
   "title": "Conformer without Convolutions",
   "original": "445",
   "order": 711,
   "page_count": 5,
   "abstract": [
    "We analyze the weights of a trained speech-to-text neural network and discover a surprising amount of structure in the temporal convolutions. Based on our observations we propose to completely remove learnable temporal convolutions, and replace them with fixed averaging and shift operations which have no learnable parameters and open the way for significantly faster implementations. In the state-of-the-art models Conformer, Squeezeformer and FastConformer, this improves WER by 0.12%, 0.62%, and 0.20% respectively, while reducing the computational cost."
   ],
   "p1": 3475,
   "pn": 3479,
   "doi": "10.21437/Interspeech.2024-445",
   "url": "interspeech_2024/vankeirsbilck24_interspeech.html"
  },
  "lin24b_interspeech": {
   "authors": [
    [
     "Yi-Cheng",
     "Lin"
    ],
    [
     "Tzu-Quan",
     "Lin"
    ],
    [
     "Hsi-Che",
     "Lin"
    ],
    [
     "Andy T.",
     "Liu"
    ],
    [
     "Hung-yi",
     "Lee"
    ]
   ],
   "title": "On the social bias of speech self-supervised models",
   "original": "454",
   "order": 952,
   "page_count": 5,
   "abstract": [
    "Self-supervised learning (SSL) speech models have achieved remarkable performance in various tasks, yet the biased outcomes, especially affecting marginalized groups, raise significant concerns. Social bias refers to the phenomenon where algorithms potentially amplify disparate properties between social groups present in the data used for training. Bias in SSL models can perpetuate injustice by automating discriminatory patterns and reinforcing inequitable systems. This work reveals that prevalent SSL models inadvertently acquire biased associations. We probe how various factors, such as model architecture, size, and training methodologies, influence the propagation of social bias within these models. Finally, we explore the efficacy of debiasing SSL models through regularization techniques, specifically via model compression. Our findings reveal that employing techniques such as row-pruning and training wider, shallower models can effectively mitigate social bias within SSL model."
   ],
   "p1": 4638,
   "pn": 4642,
   "doi": "10.21437/Interspeech.2024-454",
   "url": "interspeech_2024/lin24b_interspeech.html"
  },
  "lu24c_interspeech": {
   "authors": [
    [
     "Ke-Han",
     "Lu"
    ],
    [
     "Zhehuai",
     "Chen"
    ],
    [
     "Szu-Wei",
     "Fu"
    ],
    [
     "He",
     "Huang"
    ],
    [
     "Boris",
     "Ginsburg"
    ],
    [
     "Yu-Chiang Frank",
     "Wang"
    ],
    [
     "Hung-yi",
     "Lee"
    ]
   ],
   "title": "DeSTA: Enhancing Speech Language Models through Descriptive Speech-Text Alignment",
   "original": "457",
   "order": 855,
   "page_count": 5,
   "abstract": [
    "Recent speech language models (SLMs) typically incorporate pre-trained speech models to extend the capabilities from large language models (LLMs). In this paper, we propose a Descriptive Speech-Text Alignment approach that leverages speech captioning to bridge the gap between speech and text modalities, enabling SLMs to interpret and generate comprehensive natural language descriptions, thereby facilitating the capability to understand both linguistic and non-linguistic features in speech. Enhanced with the proposed approach, our model demonstrates superior performance on the Dynamic-SUPERB benchmark, particularly in generalizing to unseen tasks. Moreover, we discover that the aligned model exhibits a zero-shot instruction-following capability without explicit speech instruction tuning. These findings highlight the potential to reshape instruction-following SLMs by incorporating rich, descriptive speech captions."
   ],
   "p1": 4159,
   "pn": 4163,
   "doi": "10.21437/Interspeech.2024-457",
   "url": "interspeech_2024/lu24c_interspeech.html"
  },
  "yang24e_interspeech": {
   "authors": [
    [
     "Ching-Yu",
     "Yang"
    ],
    [
     "Shreya G.",
     "Upadhyay"
    ],
    [
     "Ya-Tse",
     "Wu"
    ],
    [
     "Bo-Hao",
     "Su"
    ],
    [
     "Chi-Chun",
     "Lee"
    ]
   ],
   "title": "RW-VoiceShield: Raw Waveform-based Adversarial Attack on One-shot Voice Conversion",
   "original": "458",
   "order": 562,
   "page_count": 5,
   "abstract": [
    "In recent years, there have been significant advancements in one-shot voice conversion (VC), enabling the alteration of speaker traits with just a single sentence. However, as this technology matures and generates increasingly realistic utterances, it becomes vulnerable to privacy concerns. In this paper, we propose RW-VoiceShield to shield voice from replication. This is achieved by effectively attacking one-shot VC models through the application of imperceptible noise generated from a raw waveform-based generative model. Our method undergoes testing using the latest one-shot VC model, conducting subjective and objective evaluations under both black-box and white-box scenarios. Our results indicate significant disparities in speaker characteristics between the utterances generated by the VC model and those of the protected speaker. Furthermore, even with adversarial noise introduced to protected utterances, the speaker’s distinct characteristics remain recognizable."
   ],
   "p1": 2730,
   "pn": 2734,
   "doi": "10.21437/Interspeech.2024-458",
   "url": "interspeech_2024/yang24e_interspeech.html"
  },
  "cao24b_interspeech": {
   "authors": [
    [
     "Xinwei",
     "Cao"
    ],
    [
     "Zijian",
     "Fan"
    ],
    [
     "Torbjørn",
     "Svendsen"
    ],
    [
     "Giampiero",
     "Salvi"
    ]
   ],
   "title": "A Framework for Phoneme-Level Pronunciation Assessment Using CTC",
   "original": "459",
   "order": 62,
   "page_count": 5,
   "abstract": [
    "Traditional phoneme-level goodness of pronunciation (GOP) methods require phoneme to speech alignment. The drawback is that these methods, by their definitions, are prone to alignment errors and preclude the possibility of deletion and insertion errors in pronunciation. We produce experimental evidence that CTC-based methods can be used in traditional GOP estimation in spite of their “peaky” output behaviour and may be less prone to alignment errors than traditional methods. We also propose a new framework for GOP estimation based on CTC-trained model that is independent of speech-phoneme alignment. By accounting for deletion and insertions as well as substitution errors, we show that our framework outperform alignment-based method. Our experimental results are based on the CMU-kids dataset for child speech and on the Speechocean762 containing both child and adult speech speakers. Our best method achieves 29.02% relative improvement over the baseline GOP methods."
   ],
   "p1": 302,
   "pn": 306,
   "doi": "10.21437/Interspeech.2024-459",
   "url": "interspeech_2024/cao24b_interspeech.html"
  },
  "chien24_interspeech": {
   "authors": [
    [
     "Woan-Shiuan",
     "Chien"
    ],
    [
     "Chi-Chun",
     "Lee"
    ]
   ],
   "title": "An Investigation of Group versus Individual Fairness in Perceptually Fair Speech Emotion Recognition",
   "original": "461",
   "order": 657,
   "page_count": 5,
   "abstract": [
    "Speech emotion recognition (SER) has been extensively integrated into voice-centric applications. A unique fairness issue of SER stems from the naturally biased labels given by raters as ground truth. While existing efforts primarily aim to advance SER fairness through a group (i.e., gender) fairness standpoint, our analysis reveals that label biases arising from individual raters also persist and require equal attention. Our work presents a systematic analysis to determine the effect of enhanced group (gender) fairness on individual fairness. Specifically, by evaluating two datasets we demonstrate that there exists a trade-off between group and individual fairness when removing group information. Moreover, our results indicate that achieving group fairness results in diminished individual fairness, particularly when the attribute distributions of the two groups are significantly distant. This work brings initial insights into issues of group and individual fairness in the SER systems."
   ],
   "p1": 3205,
   "pn": 3209,
   "doi": "10.21437/Interspeech.2024-461",
   "url": "interspeech_2024/chien24_interspeech.html"
  },
  "lenglet24_interspeech": {
   "authors": [
    [
     "Martin",
     "Lenglet"
    ],
    [
     "Olivier",
     "Perrotin"
    ],
    [
     "Gerard",
     "Bailly"
    ]
   ],
   "title": "FastLips: an End-to-End Audiovisual Text-to-Speech System with Lip Features Prediction for Virtual Avatars",
   "original": "462",
   "order": 706,
   "page_count": 5,
   "abstract": [
    "In this paper, we introduce FastLips, an end-to-end neural model designed to generate speech and co-verbal facial movements from text, animating a virtual avatar. Based on the FastSpeech2 Text-to-Speech model, FastLips integrates an audiovisual Transformer-based encoder with distinct audio and visual neural decoders. This model combines audiovisual representations computed by the shared encoder with asynchronous generation of audio and visual features. Furthermore, we enhance the model with explicit predictors of lip aperture and spreading, adapted from prosodic FastSpeech2's variance adaptor. The proposed model generates mel-spectrograms and facial features (head, eyes, jaw and lip movements) to drive the virtual avatar's action units. In our evaluation, we compare FastLips with a baseline audiovisual-Tacotron2, demonstrating the advantages of the FastSpeech2 architecture for lip generation. This benefit becomes particularly prominent when implementing explicit lip prediction."
   ],
   "p1": 3450,
   "pn": 3454,
   "doi": "10.21437/Interspeech.2024-462",
   "url": "interspeech_2024/lenglet24_interspeech.html"
  },
  "lin24c_interspeech": {
   "authors": [
    [
     "Yin-Tse",
     "Lin"
    ],
    [
     "Shreya G.",
     "Upadhyay"
    ],
    [
     "Bo-Hao",
     "Su"
    ],
    [
     "Chi-Chun",
     "Lee"
    ]
   ],
   "title": "SWiBE: A Parameterized Stochastic Diffusion Process for Noise-Robust Bandwidth Expansion",
   "original": "463",
   "order": 468,
   "page_count": 5,
   "abstract": [
    "Speech recordings frequently encounter a variety of distortions, making the task of eliminating them essential yet challenging. In this study, leveraging the current success of score-based generative modeling (SGM), we propose a novel noise-robust bandwidth expansion (BWE) framework based on an innovative parameterized stochastic diffusion process, achieved through stepwise bandwidth expansion in the spectrogram. Our proposed Step-Wised Bandwidth Expansion (SWiBE) method outperforms baseline approaches over considered metrics, including the current state-of-the-art noise-robust BWE model and various diffusion and GAN-based models. Moreover, we analyze the interaction between the hyperparameters and performance across different aspects including perceptual quality and spectral reconstruction. Our findings reveal that the score-based model manifests distinct characteristics under varying parameterizations."
   ],
   "p1": 2265,
   "pn": 2269,
   "doi": "10.21437/Interspeech.2024-463",
   "url": "interspeech_2024/lin24c_interspeech.html"
  },
  "lee24f_interspeech": {
   "authors": [
    [
     "Joun Yeop",
     "Lee"
    ],
    [
     "Myeonghun",
     "Jeong"
    ],
    [
     "Minchan",
     "Kim"
    ],
    [
     "Ji-Hyun",
     "Lee"
    ],
    [
     "Hoon-Young",
     "Cho"
    ],
    [
     "Nam Soo",
     "Kim"
    ]
   ],
   "title": "High Fidelity Text-to-Speech Via Discrete Tokens Using Token Transducer and Group Masked Language Model",
   "original": "465",
   "order": 705,
   "page_count": 5,
   "abstract": [
    "We propose a novel two-stage text-to-speech (TTS) framework with two types of discrete tokens, i.e., semantic and acoustic tokens, for high-fidelity speech synthesis. It features two core components: the Interpreting module, which processes text and a speech prompt into semantic tokens focusing on linguistic contents and alignment, and the Speaking module, which captures the timbre of the target voice to generate acoustic tokens from semantic tokens, enriching speech reconstruction. The Interpreting stage employs a transducer for its robustness in aligning text to speech. In contrast, the Speaking stage utilizes a Conformer-based architecture integrated with a Grouped Masked Language Model (G-MLM) to boost computational efficiency. Our experiments verify that this innovative structure surpasses the conventional models in the zero-shot scenario in terms of speech quality and speaker similarity."
   ],
   "p1": 3445,
   "pn": 3449,
   "doi": "10.21437/Interspeech.2024-465",
   "url": "interspeech_2024/lee24f_interspeech.html"
  },
  "upadhyay24_interspeech": {
   "authors": [
    [
     "Shreya G.",
     "Upadhyay"
    ],
    [
     "Carlos",
     "Busso"
    ],
    [
     "Chi-Chun",
     "Lee"
    ]
   ],
   "title": "A Layer-Anchoring Strategy for Enhancing Cross-Lingual Speech Emotion Recognition",
   "original": "469",
   "order": 963,
   "page_count": 5,
   "abstract": [
    "Cross-lingual speech emotion recognition (SER) is important for a wide range of everyday applications. While recent SER research relies heavily on large pretrained models for emotion training, existing studies often concentrate solely on the final transformer layer of these models. However, given the task-specific nature and hierarchical architecture of these models, each transformer layer encapsulates different levels of information. Leveraging this hierarchical structure, our study focuses on the information embedded  across different layers. Through an examination of layer feature similarity across different languages, we propose a novel strategy called a layer-anchoring mechanism to facilitate emotion transfer in cross-lingual SER tasks. Our approach is evaluated using two distinct language affective corpora (MSP-Podcast and BIIC-Podcast), achieving a best UAR performance of 60.21% on the BIIC podcast corpus. The analysis uncovers interesting insights into the behavior of popular pretrained models."
   ],
   "p1": 4693,
   "pn": 4697,
   "doi": "10.21437/Interspeech.2024-469",
   "url": "interspeech_2024/upadhyay24_interspeech.html"
  },
  "hsu24_interspeech": {
   "authors": [
    [
     "Wei-Tung",
     "Hsu"
    ],
    [
     "Chin-Po",
     "Chen"
    ],
    [
     "Yun-Shao",
     "Lin"
    ],
    [
     "Chi-Chun",
     "Lee"
    ]
   ],
   "title": "A Cluster-based Personalized Federated Learning Strategy for End-to-End ASR of Dementia Patients",
   "original": "470",
   "order": 505,
   "page_count": 5,
   "abstract": [
    "Automatic speech recognition (ASR) is crucial for all users, but adapting it for Alzheimer’s disease (AD) faces challenges due to irregular speech patterns and privacy concerns. Federated learning (FL), a privacy-preserving algorithm, is a solution. However, FL ASR suffers from acoustic and text heterogeneities. While advanced model-based and cluster-based FL methods aim to address the issue, they lack a direct mechanism for high intra-speaker heterogeneity exhibited by AD individuals and ASR-related properties. This study presents cluster-based personalized federated learning (CPFL), a strategy mitigating heterogeneity by clustering ASR output token using the proposed CharDiv, a metric for pause and word usage distributions. Evaluation on the ADReSS challenge dataset shows a 3.6% improvement in word error rate (WER). Analysis of per-cluster WER improvements and CharDiv distributions indicates reduced heterogeneity, emphasizing pause usage as a potential key factor in AD-oriented ASR."
   ],
   "p1": 2450,
   "pn": 2454,
   "doi": "10.21437/Interspeech.2024-470",
   "url": "interspeech_2024/hsu24_interspeech.html"
  },
  "chou24_interspeech": {
   "authors": [
    [
     "Hsing-Hang",
     "Chou"
    ],
    [
     "Woan-Shiuan",
     "Chien"
    ],
    [
     "Ya-Tse",
     "Wu"
    ],
    [
     "Chi-Chun",
     "Lee"
    ]
   ],
   "title": "An Inter-Speaker Fairness-Aware Speech Emotion Regression Framework",
   "original": "471",
   "order": 654,
   "page_count": 5,
   "abstract": [
    "Speech emotion recognition (SER) helps to achieve better human-to-machine interactions in voice technologies. Recent studies have pointed out critical fairness issues in the SER. While there are efforts in building fair SER, most of the works focus on fairness between demographic groups and rely on these broad categorical attributes to build a fair SER. In this paper, we instead focus on the fairness learning among individual speakers, which is rarely discussed yet much more intuitively appealing in constructing a fair SER model. To reduce the reliance on knowing speaker IDs, we perform unsupervised clustering on the utterance embeddings from a pretrained speaker verification model that puts utterances with different characteristics into clusters that roughly represent the true speaker index. Our evaluation demonstrates that with these cluster IDs, we can construct a fairness-aware SER model at an individual speaker-level without knowing speaker IDs upfront."
   ],
   "p1": 3190,
   "pn": 3194,
   "doi": "10.21437/Interspeech.2024-471",
   "url": "interspeech_2024/chou24_interspeech.html"
  },
  "lehecka24_interspeech": {
   "authors": [
    [
     "Jan",
     "Lehečka"
    ],
    [
     "Josef V.",
     "Psutka"
    ],
    [
     "Lubos",
     "Smidl"
    ],
    [
     "Pavel",
     "Ircing"
    ],
    [
     "Josef",
     "Psutka"
    ]
   ],
   "title": "A Comparative Analysis of Bilingual and Trilingual Wav2Vec Models for Automatic Speech Recognition in Multilingual Oral History Archives",
   "original": "472",
   "order": 266,
   "page_count": 5,
   "abstract": [
    "In this paper, we are comparing monolingual Wav2Vec 2.0 models with various multilingual models to see whether we could improve speech recognition performance on a unique oral history archive containing a lot of mixed-language sentences. Our main goal is to push forward research on this unique dataset, which is an extremely valuable part of our cultural heritage. Our results suggest that monolingual speech recognition models are, in most cases, superior to multilingual models, even when processing the oral history archive full of mixed-language sentences from non-native speakers. We also performed the same experiments on the public CommonVoice dataset to verify our results. We are contributing to the research community by releasing our pre-trained models to the public."
   ],
   "p1": 1285,
   "pn": 1289,
   "doi": "10.21437/Interspeech.2024-472",
   "url": "interspeech_2024/lehecka24_interspeech.html"
  },
  "hoffner24_interspeech": {
   "authors": [
    [
     "Dirk Eike",
     "Hoffner"
    ],
    [
     "Jana",
     "Roßbach"
    ],
    [
     "Bernd T.",
     "Meyer"
    ]
   ],
   "title": "Joint prediction of subjective listening effort and speech intelligibility based on end-to-end learning",
   "original": "473",
   "order": 867,
   "page_count": 5,
   "abstract": [
    "Subjective listening effort and speech intelligibility are crucial aspects in human communication. Models that can predict these metrics are important tools to develop speech enhancement or compression algorithms. To make predictions in real-life situations, non-intrusive models are required which do not use a clean reference signal as additional input. This paper explores a non-intrusive model for joint prediction of listening effort and speech intelligibility, which is based on character probabilities obtained from an end-to-end automatic speech recognition system. The uncertainty of the character classification is quantified using an entropy-based measure and compared to subjective data from normal-hearing and hearing-impaired listeners. The proposed model achieves correlation values of at least 0.9 and a root-mean-square error at or below 5 percentage points for speech intelligibility, and outperforms an intrusive baseline in four out of six conditions."
   ],
   "p1": 4214,
   "pn": 4218,
   "doi": "10.21437/Interspeech.2024-473",
   "url": "interspeech_2024/hoffner24_interspeech.html"
  },
  "getman24_interspeech": {
   "authors": [
    [
     "Yaroslav",
     "Getman"
    ],
    [
     "Tamas",
     "Grosz"
    ],
    [
     "Mikko",
     "Kurimo"
    ]
   ],
   "title": "What happens in continued pre-training? Analysis of self-supervised speech models with continued pre-training for colloquial Finnish ASR",
   "original": "476",
   "order": 1033,
   "page_count": 5,
   "abstract": [
    "The advancement of self-supervised learning has enabled the rapid development of highly accurate speech recognition models, such as wav2vec 2.0, for many languages. While high-resourced languages like English benefit from purely monolingual models, other, less-resourced ones must build upon multilingual foundations. In this work, we investigate various strategies to specialize models for the colloquial Finnish language and demonstrate that continued pre-training of available multilingual models is the best solution. Furthermore, we investigate the success of the pre-training procedure by examining the learned quantized representations and show how the continued pre-training improved the discovered latent codeword groups."
   ],
   "p1": 5043,
   "pn": 5047,
   "doi": "10.21437/Interspeech.2024-476",
   "url": "interspeech_2024/getman24_interspeech.html"
  },
  "cumlin24_interspeech": {
   "authors": [
    [
     "Fredrik",
     "Cumlin"
    ],
    [
     "Xinyu",
     "Liang"
    ],
    [
     "Victor",
     "Ungureanu"
    ],
    [
     "Chandan",
     "K. A. Reddy"
    ],
    [
     "Christian",
     "Schüldt"
    ],
    [
     "Saikat",
     "Chatterjee"
    ]
   ],
   "title": "DNSMOS Pro: A Reduced-Size DNN for Probabilistic MOS of Speech",
   "original": "478",
   "order": 988,
   "page_count": 5,
   "abstract": [
    "We propose a deep neural network-based architecture and training design for objective non-intrusive speech quality assessment. The proposed method builds on DNSMOS, and we call the proposed model DNSMOS Pro. DNSMOS Pro has a reduced-size architecture suitable for VoIP, a relatively simple training design using only the mean opinion score (MOS) as the target label, and predicts the posterior distribution of MOS given an input speech clip. This means DNSMOS Pro can be trained when only the MOS is reported on a subjectively rated dataset. Furthermore, we implement several non-intrusive speech quality methods and compare them to DNSMOS Pro when training and testing on different subjectively rated datasets. DNSMOS Pro has significantly better performance on these benchmark datasets compared to similar DNN-based non-intrusive speech quality methods, and competitive results to methods assuming auxiliary information in the datasets."
   ],
   "p1": 4818,
   "pn": 4822,
   "doi": "10.21437/Interspeech.2024-478",
   "url": "interspeech_2024/cumlin24_interspeech.html"
  },
  "getman24b_interspeech": {
   "authors": [
    [
     "Yaroslav",
     "Getman"
    ],
    [
     "Tamas",
     "Grosz"
    ],
    [
     "Katri",
     "Hiovain-Asikainen"
    ],
    [
     "Mikko",
     "Kurimo"
    ]
   ],
   "title": "Exploring adaptation techniques of large speech foundation models for low-resource ASR: a case study on Northern Sámi",
   "original": "479",
   "order": 523,
   "page_count": 5,
   "abstract": [
    "Speech foundation models such as wav2vec 2.0 have made it possible to develop highly accurate models for low-resourced languages using a limited amount of speech data. For optimal results, the pre-training should already include data from the target language, but unfortunately, none of the available foundation models include Northern Sámi. In this work, we explore various ways of preparing the foundation model for the Northern Sámi, including continued pre-training with a small untranscribed corpus and our new extended fine-tuning method. The extended fine-tuning starts from an already fine-tuned ASR model and augments it with new output units for the unique Sámi characters before new fine-tuning with transcribed Sámi data. Our results demonstrate the benefits of these advanced adaptation techniques, as both approaches lead to better performance than the direct fine-tuning-based adaptation."
   ],
   "p1": 2539,
   "pn": 2543,
   "doi": "10.21437/Interspeech.2024-479",
   "url": "interspeech_2024/getman24b_interspeech.html"
  },
  "tran24_interspeech": {
   "authors": [
    [
     "Hoan My",
     "Tran"
    ],
    [
     "David",
     "Guennec"
    ],
    [
     "Philippe",
     "Martin"
    ],
    [
     "Aghilas",
     "Sini"
    ],
    [
     "Damien",
     "Lolive"
    ],
    [
     "Arnaud",
     "Delhay"
    ],
    [
     "Pierre-François",
     "Marteau"
    ]
   ],
   "title": "Spoofed Speech Detection with a Focus on Speaker Embedding",
   "original": "481",
   "order": 431,
   "page_count": 5,
   "abstract": [
    "Self-Supervised Learning (SSL) models excel as feature extractors in downstream speech tasks, including the increasingly crucial area of spoof speech detection due to the rise of audio deepfakes using Text-To-Speech (TTS) and Voice Conversion (VC) technologies.  To address this issue, we propose a novel approach  that  relies  on  speaker  embedding  using  a  finetuned WavLM model with layer-wise attentive statistics pooling combined  to  a  supervised  contrastive  learning  and  cross-entropy loss.  Evaluation on Logical Access (LA) and DeepFake (DF) tasks  on  ASVspoof  2019  and  2021  highlights  its  potential  in detecting audio deepfakes, with the contrastive loss producing more stable results among test sets."
   ],
   "p1": 2080,
   "pn": 2084,
   "doi": "10.21437/Interspeech.2024-481",
   "url": "interspeech_2024/tran24_interspeech.html"
  },
  "wu24d_interspeech": {
   "authors": [
    [
     "Ya-Tse",
     "Wu"
    ],
    [
     "Jingyao",
     "Wu"
    ],
    [
     "Vidhyasaharan",
     "Sethu"
    ],
    [
     "Chi-Chun",
     "Lee"
    ]
   ],
   "title": "Can Modelling Inter-Rater Ambiguity Lead To Noise-Robust Continuous Emotion Predictions?",
   "original": "482",
   "order": 766,
   "page_count": 5,
   "abstract": [
    "There has been increasing attention drawn to modelling inter-rater ambiguity in Continuous Emotion Recognition (CER) systems using probability distributions for arousal and valence. However, the relationship between modelling label ambiguity and robustness to noise, and more broadly, the impact of real-world noise on CER systems remains insufficiently explored. In this study, we argue that incorporating inter-rater ambiguity during training can regularize the noise response, leading to noise robustness. To this end, we propose a novel loss function that incorporates inter-rater ambiguity into model training. Experiments conducted on the RECOLA dataset demonstrate that our proposed method achieves a maximum Concordance Correlation Coefficient (CCC) improvement of 0.117 and 0.077 for mean and standard deviation predictions, respectively, across all noise conditions. We further integrate traditional noisy augmentation strategies with our proposed method and observe promising results."
   ],
   "p1": 3714,
   "pn": 3718,
   "doi": "10.21437/Interspeech.2024-482",
   "url": "interspeech_2024/wu24d_interspeech.html"
  },
  "zhang24d_interspeech": {
   "authors": [
    [
     "Yuanyuan",
     "Zhang"
    ],
    [
     "Zhengjun",
     "Yue"
    ],
    [
     "Tanvina",
     "Patel"
    ],
    [
     "Odette",
     "Scharenborg"
    ]
   ],
   "title": "Improving child speech recognition with augmented child-like speech",
   "original": "485",
   "order": 1061,
   "page_count": 5,
   "abstract": [
    "State-of-the-art ASRs show suboptimal performance for child speech. The scarcity of child speech limits the development of child speech recognition (CSR). Therefore, we studied child-to-child voice conversion (VC) from existing child speakers in the dataset and additional (new) child speakers via monolingual and cross-lingual (Dutch-to-German) VC, respectively. The results showed that cross-lingual child-to-child VC significantly improved child ASR performance. Experiments on the impact of the quantity of child-to-child cross-lingual VC-generated data on fine-tuning (FT) ASR models gave the best results with two-fold augmentation for our FT-Conformer model and FT-Whisper model which reduced WERs with ∼3% absolute compared to the baseline, and with six-fold augmentation for the model trained from scratch, which improved by an absolute 3.6% WER. Moreover, using a small amount of “high-quality” VC-generated data achieved similar results to those of our best-FT models."
   ],
   "p1": 5183,
   "pn": 5187,
   "doi": "10.21437/Interspeech.2024-485",
   "url": "interspeech_2024/zhang24d_interspeech.html"
  },
  "miara24_interspeech": {
   "authors": [
    [
     "Victor",
     "Miara"
    ],
    [
     "Theo",
     "Lepage"
    ],
    [
     "Reda",
     "Dehak"
    ]
   ],
   "title": "Towards Supervised Performance on Speaker Verification with Self-Supervised Learning by Leveraging Large-Scale ASR Models",
   "original": "486",
   "order": 548,
   "page_count": 5,
   "abstract": [
    "Recent advancements in Self-Supervised Learning (SSL) have shown promising results in Speaker Verification (SV). However, narrowing the performance gap with supervised systems remains an ongoing challenge. Several studies have observed that speech representations from large-scale ASR models contain valuable speaker information. This work explores the limitations of fine-tuning these models for SV using an SSL contrastive objective in an end-to-end approach. Then, we propose a framework to learn speaker representations in an SSL context by fine-tuning a pre-trained WavLM with a supervised loss using pseudo-labels. Initial pseudo-labels are derived from an SSL DINO-based model and are iteratively refined by clustering the model embeddings. Our method achieves 0.99% EER on VoxCeleb1-O, establishing the new state-of-the-art on self-supervised SV. As this performance is close to our supervised baseline of 0.94% EER, this contribution is a step towards supervised performance on SV with SSL."
   ],
   "p1": 2660,
   "pn": 2664,
   "doi": "10.21437/Interspeech.2024-486",
   "url": "interspeech_2024/miara24_interspeech.html"
  },
  "yang24f_interspeech": {
   "authors": [
    [
     "Guanrou",
     "Yang"
    ],
    [
     "Ziyang",
     "Ma"
    ],
    [
     "Fan",
     "Yu"
    ],
    [
     "Zhifu",
     "Gao"
    ],
    [
     "Shiliang",
     "Zhang"
    ],
    [
     "Xie",
     "Chen"
    ]
   ],
   "title": "MaLa-ASR: Multimedia-Assisted LLM-Based ASR",
   "original": "488",
   "order": 496,
   "page_count": 5,
   "abstract": [
    "As more and more information-rich data like video become available, utilizing multi-modal auxiliary information to enhance audio tasks has sparked widespread research interest. The recent surge in research on LLM-based audio models provides fresh perspectives for tackling audio tasks. Given that LLM can flexibly ingest multiple inputs, we propose MaLa-ASR, an LLM-based ASR model that can integrate textual keywords extracted from presentation slides to improve recognition of conference content. MaLa-ASR yields average WERs of 9.4% and 11.7% on the L95 and S95 subsets of the SlideSpeech corpus, representing a significant relative WER drop of 27.9% and 44.7% over the baseline model reported in SlideSpeech. MaLa-ASR underscores LLM's strong performance in speech tasks and the capability to integrate auxiliary information conveniently. By adding keywords to the input prompt, the biased word error rate (B-WER) reduces relatively by 46.0% and 44.2%, establishing a new SOTA on this dataset."
   ],
   "p1": 2405,
   "pn": 2409,
   "doi": "10.21437/Interspeech.2024-488",
   "url": "interspeech_2024/yang24f_interspeech.html"
  },
  "macaire24_interspeech": {
   "authors": [
    [
     "Cécile",
     "Macaire"
    ],
    [
     "Chloé",
     "Dion"
    ],
    [
     "Didier",
     "Schwab"
    ],
    [
     "Benjamin",
     "Lecouteux"
    ],
    [
     "Emmanuelle",
     "Esperança-Rodier"
    ]
   ],
   "title": "Towards Speech-to-Pictograms Translation",
   "original": "490",
   "order": 173,
   "page_count": 5,
   "abstract": [
    "The automatic translation of speech into pictogram terms (Speech-to-Pictos) represents a novel NLP task with the potential to enhance communication for individuals with language impairments. Recent research has not explored the adaptation of state-of-the-art methods to this task, despite its significance. In this work, we investigate two approaches: (1) the cascade approach, which combines a speech recognition system with a machine translation system, and (2) the end-to-end approach, which tailors a speech translation system. We compare state-of-the-art architectures trained on an aligned speech-to-pictogram dataset, specially created and released for this study. We conduct an in-depth automatic and human evaluation to analyze their behavior on pictogram translation. The results highlight the cascade approach’s ability to generate relevant translations from everyday read speech, while the end-to-end approach achieves competitive results with challenging acoustic data."
   ],
   "p1": 857,
   "pn": 861,
   "doi": "10.21437/Interspeech.2024-490",
   "url": "interspeech_2024/macaire24_interspeech.html"
  },
  "kim24f_interspeech": {
   "authors": [
    [
     "June-Woo",
     "Kim"
    ],
    [
     "Miika",
     "Toikkanen"
    ],
    [
     "Yera",
     "Choi"
    ],
    [
     "Seoung-Eun",
     "Moon"
    ],
    [
     "Ho-Young",
     "Jung"
    ]
   ],
   "title": "BTS: Bridging Text and Sound Modalities for Metadata-Aided Respiratory Sound Classification",
   "original": "492",
   "order": 347,
   "page_count": 5,
   "abstract": [
    "Respiratory sound classification (RSC) is challenging due to varied acoustic signatures, primarily influenced by patient demographics and recording environments. To address this issue, we introduce a text-audio multimodal model that utilizes metadata of respiratory sounds, which provides useful complementary information for RSC. Specifically, we fine-tune a pretrained text-audio multimodal model using free-text descriptions derived from the sound samples' metadata which includes the gender and age of patients, type of recording devices, and recording location on the patient's body. Our method achieves state-of-the-art performance on the ICBHI dataset, surpassing the previous best result by a notable margin of 1.17%. This result validates the effectiveness of leveraging metadata and respiratory sound samples in enhancing RSC performance. Additionally, we investigate the model performance in the case where metadata is partially unavailable, which may occur in real-world clinical setting."
   ],
   "p1": 1690,
   "pn": 1694,
   "doi": "10.21437/Interspeech.2024-492",
   "url": "interspeech_2024/kim24f_interspeech.html"
  },
  "kathan24_interspeech": {
   "authors": [
    [
     "Alexander",
     "Kathan"
    ],
    [
     "Martin",
     "Bürger"
    ],
    [
     "Andreas",
     "Triantafyllopoulos"
    ],
    [
     "Sabrina",
     "Milkus"
    ],
    [
     "Jonas",
     "Hohmann"
    ],
    [
     "Pauline",
     "Muderlak"
    ],
    [
     "Jürgen",
     "Schottdorf"
    ],
    [
     "Richard",
     "Musil"
    ],
    [
     "Björn",
     "Schuller"
    ],
    [
     "Shahin",
     "Amiriparian"
    ]
   ],
   "title": "Real-world PTSD Recognition: A Cross-corpus and Cross-linguistic Evaluation",
   "original": "493",
   "order": 99,
   "page_count": 5,
   "abstract": [
    "Post-traumatic Stress Disorder (PTSD) is a mental condition that develops as a result of catastrophic events. Triggers for this may include experiences, such as military combat, natural disasters, or sexual abuse, having a great influence on the mental wellbeing. Due to the severity of this condition, early detection and professional treatment is crucial. For this reason, previous research explored prediction models for recognising PTSD at an early stage. However, when these models are transferred from research to real-world applications, they face heterogeneous environments (e.g., different recording settings, various dialects or languages). To analyse this effect, we develop a speech-based PTSD recognition model and subsequently analyse its cross-corpus and cross-linguistic performance. Our experiments indicate that there are cross-cultural factors incluencing PTSD and leading to a best area under the ROC curve (AUC) of 70.1% evaluated on the cross-corpus."
   ],
   "p1": 487,
   "pn": 491,
   "doi": "10.21437/Interspeech.2024-493",
   "url": "interspeech_2024/kathan24_interspeech.html"
  },
  "wang24m_interspeech": {
   "authors": [
    [
     "Pu",
     "Wang"
    ],
    [
     "Junhui",
     "Li"
    ],
    [
     "Jialu",
     "Li"
    ],
    [
     "Liangdong",
     "Guo"
    ],
    [
     "Youshan",
     "Zhang"
    ]
   ],
   "title": "Diffusion Gaussian Mixture Audio Denoise",
   "original": "494",
   "order": 455,
   "page_count": 5,
   "abstract": [
    "Recent diffusion models have achieved promising performances in audio-denoising tasks. The unique property of the reverse process could recover clean signals. However, the distribution of real-world noises does not comply with a single Gaussian distribution and is even unknown. The sampling of Gaussian noise conditions limits its application scenarios. To overcome these challenges, we propose a DiffGMM model, a denoising model based on the diffusion and Gaussian mixture models. We employ the reverse process to estimate parameters for the Gaussian mixture model. Given a noisy audio signal, we first apply a 1D-U-Net to extract features and train linear layers to estimate parameters for the Gaussian mixture model, and we approximate the real noise distributions. The noisy signal is continuously subtracted from the estimated noise to output clean audio signals. Extensive experimental results demonstrate that the proposed DiffGMM model achieves state-of-the-art performance."
   ],
   "p1": 2200,
   "pn": 2204,
   "doi": "10.21437/Interspeech.2024-494",
   "url": "interspeech_2024/wang24m_interspeech.html"
  },
  "graave24_interspeech": {
   "authors": [
    [
     "Thomas",
     "Graave"
    ],
    [
     "Zhengyang",
     "Li"
    ],
    [
     "Timo",
     "Lohrenz"
    ],
    [
     "Tim",
     "Fingscheidt"
    ]
   ],
   "title": "Mixed Children/Adult/Childrenized Fine-Tuning for Children’s ASR: How to Reduce Age Mismatch and Speaking Style Mismatch",
   "original": "499",
   "order": 1062,
   "page_count": 5,
   "abstract": [
    "Today’s end-to-end (E2E) ASR models achieve strong performance when applied to adult speech, but deteriorate on children’s speech. Most E2E ASR models are pre-trained on adult speech, which introduces an age mismatch that can be addressed by finetuning on child data. However, due to limited availability of child datasets, fine-tuning on children’s speech may introduce new domain shifts such as speaking style mismatch. In this work, we explore mixed fine-tuning on partially matched data, namely read adult speech and spontaneous children’s speech, to improve the performance of E2E ASR on read children’s speech. We isolate the individual impact of age mismatch and speaking style mismatch and investigate the use of childrenization of read adult speech. Our proposed method reduces the WER by up to 5% absolute (21% relative) compared to the pre-trained E2E ASR and by roughly 3% absolute (15% relative) compared to individual fine-tuning on partially matched datasets."
   ],
   "p1": 5188,
   "pn": 5192,
   "doi": "10.21437/Interspeech.2024-499",
   "url": "interspeech_2024/graave24_interspeech.html"
  },
  "zhang24e_interspeech": {
   "authors": [
    [
     "Shucong",
     "Zhang"
    ],
    [
     "Titouan",
     "Parcollet"
    ],
    [
     "Rogier",
     "van Dalen"
    ],
    [
     "Sourav",
     "Bhattacharya"
    ]
   ],
   "title": "Linear-Complexity Self-Supervised Learning for Speech Processing",
   "original": "500",
   "order": 712,
   "page_count": 5,
   "abstract": [
    "Self-supervised learning (SSL) models usually require weeks of pre-training with dozens of high-end GPUs. These models typically have a multi-headed self-attention (MHSA) context encoder. However, MHSA takes quadratic time and space in the input length, contributing to the high pre-training cost. Linear-complexity alternatives to MHSA have been proposed. For instance, in supervised training, the SummaryMixing model is the first to outperform MHSA across multiple speech processing tasks. However, these cheaper alternatives have not been explored for SSL yet. This paper studies a linear-complexity context encoder for SSL for the first time. With better or equivalent performance for the downstream tasks of the MP3S benchmark, SummaryMixing reduces the pre-training time and peak VRAM of wav2vec 2.0 model by 18% and  by 23%, respectively, leading to the pre-training of a 155M wav2vec 2.0 model finished within one week with 4 Tesla A100 GPUs. Code is available."
   ],
   "p1": 3480,
   "pn": 3484,
   "doi": "10.21437/Interspeech.2024-500",
   "url": "interspeech_2024/zhang24e_interspeech.html"
  },
  "huang24_interspeech": {
   "authors": [
    [
     "Fan",
     "Huang"
    ],
    [
     "Kun",
     "Zeng"
    ],
    [
     "Wei",
     "Zhu"
    ]
   ],
   "title": "DiffVC+: Improving Diffusion-based Voice Conversion for Speaker Anonymization",
   "original": "502",
   "resource": "https://doi.org/10.5281/zenodo.12795032",
   "order": 915,
   "page_count": 5,
   "abstract": [
    "The increasing risks of speech data leakage prompt growing concerns about voice privacy. This paper proposes DiffVC+, a speaker anonymization model designed to preserve speech privacy. It operates as a diffusion-based voice conversion model that suppresses identity information by converting the speaker's voice through flexible approaches. DiffVC+ comprises a self-supervised learning (SSL) content encoder that effectively extracts the source speech content, a speaker encoder and an embedding generator that both supply the target speaker embedding, and a diffusion-based decoder generating the converted speech. Furthermore, we propose DiffVC+ light and DiffVC+ decoupled for edge-side and server-side deployments, respectively. Experimental results demonstrate that our models significantly outperform the baseline in terms of the intelligibility and naturalness of the converted speech, while achieving competitive anonymization performance."
   ],
   "p1": 4453,
   "pn": 4457,
   "doi": "10.21437/Interspeech.2024-502",
   "url": "interspeech_2024/huang24_interspeech.html"
  },
  "li24i_interspeech": {
   "authors": [
    [
     "Zhengyang",
     "Li"
    ],
    [
     "Patrick",
     "Blumenberg"
    ],
    [
     "Jing",
     "Liu"
    ],
    [
     "Thomas",
     "Graave"
    ],
    [
     "Timo",
     "Lohrenz"
    ],
    [
     "Siegfried",
     "Kunzmann"
    ],
    [
     "Tim",
     "Fingscheidt"
    ]
   ],
   "title": "Interleaved Audio/Audiovisual Transfer Learning for AV-ASR in Low-Resourced Languages",
   "original": "503",
   "order": 520,
   "page_count": 5,
   "abstract": [
    "Cross-language transfer learning from English to a target language has shown effectiveness in low-resourced audiovisual speech recognition (AV-ASR). We first investigate a 2-stage protocol, which performs fine-tuning of the English pre-trained AV encoder on a large audio corpus in the target language (1st stage), and then carries out cross-modality transfer learning from audio to AV in the target language for AV-ASR (2nd stage). Second, we propose an alternative interleaved audio/audiovisual transfer learning to avoid catastrophic forgetting of the video modality and to overcome 2nd stage overfitting to the small AV corpus. We use only 10h AV training data in either German or French target language. Our proposed interleaved method outperforms the 2-stage method in all low-resource conditions and both languages. It also excels the former state of the art both in the noisy benchmark (babble 0dB, 53.9% vs. 65.9%) and in clean condition (34.9% vs. 48.1%) on the German MuAVIC test set."
   ],
   "p1": 2524,
   "pn": 2528,
   "doi": "10.21437/Interspeech.2024-503",
   "url": "interspeech_2024/li24i_interspeech.html"
  },
  "kundu24_interspeech": {
   "authors": [
    [
     "Arnav",
     "Kundu"
    ],
    [
     "Prateeth",
     "Nayak"
    ],
    [
     "Priyanka",
     "Padmanabhan"
    ],
    [
     "Devang",
     "Naik"
    ]
   ],
   "title": "RepCNN: Micro-sized, Mighty Models for Wakeword Detection",
   "original": "505",
   "order": 710,
   "page_count": 5,
   "abstract": [
    "Always-on machine learning models require a very low memory and compute footprint. Their restricted parameter count limits the model’s capacity to learn, and the effectiveness of the usual training algorithms to find the best parameters. Here we show that a small convolutional model can be better trained by first refactoring its computation into a larger redundant multi-branched architecture. Then, for inference, we algebraically re-parameterize the trained model into the single-branched form with fewer parameters for a lower memory footprint and compute cost. Using this technique, we show that our always-on wake-word detector model, RepCNN, provides a good trade-off between latency and accuracy during inference. RepCNN re-parameterized models are 43% more accurate than a uni-branch convolutional model while having the same runtime. RepCNN also meets the accuracy of complex architectures like BC-ResNet, while having 2x lesser peak memory usage and 10x faster runtime."
   ],
   "p1": 3470,
   "pn": 3474,
   "doi": "10.21437/Interspeech.2024-505",
   "url": "interspeech_2024/kundu24_interspeech.html"
  },
  "lemerle24_interspeech": {
   "authors": [
    [
     "Théodor",
     "Lemerle"
    ],
    [
     "Nicolas",
     "Obin"
    ],
    [
     "Axel",
     "Roebel"
    ]
   ],
   "title": "Small-E: Small Language Model with Linear Attention for Efficient Speech Synthesis",
   "original": "508",
   "order": 700,
   "page_count": 5,
   "abstract": [
    "Recent advancements in text-to-speech (TTS) powered by language models have showcased remarkable capabilities in achieving naturalness and zero-shot voice cloning. Notably, the decoder-only transformer is the prominent architecture in this domain. However, transformers face challenges stemming from their quadratic complexity in sequence length, impeding training on lengthy sequences and resource-constrained hardware. Moreover they lack specific inductive bias with regards to the monotonic nature of TTS alignments. In response, we propose to replace transformers with emerging recurrent architectures and introduce specialized cross-attention mechanisms for reducing repeating and skipping issues. Consequently our architecture can be efficiently trained on long samples and achieve state-of-the-art zero-shot voice cloning against baselines of comparable size. Our implementation and demos are available at https://github.com/theodorblackbird/lina-speech."
   ],
   "p1": 3420,
   "pn": 3424,
   "doi": "10.21437/Interspeech.2024-508",
   "url": "interspeech_2024/lemerle24_interspeech.html"
  },
  "kang24_interspeech": {
   "authors": [
    [
     "Ji-Hun",
     "Kang"
    ],
    [
     "Jae-Hong",
     "Lee"
    ],
    [
     "Mun-Hak",
     "Lee"
    ],
    [
     "Joon-Hyuk",
     "Chang"
    ]
   ],
   "title": "Whisper Multilingual Downstream Task Tuning Using Task Vectors",
   "original": "513",
   "order": 492,
   "page_count": 5,
   "abstract": [
    "Recently, the size of automatic speech recognition (ASR) models has been increasing, similar to large language models (LLMs), and efficient tuning to enhance the performance of downstream tasks with limited resources remains a challenge. In this paper, we propose a simple and effective downstream task tuning method using task vectors. We utilize task vectors to orient the pre-trained Whisper model in the weight space, moving in that direction to achieve downstream task adaptation. We demonstrate that the model can be adjusted through arithmetic operations of the task vector, and this adjustment is reflected in the Whisper. Furthermore, we can efficiently construct a generalized model by summing vectors. We set the direction of the model weight space for each multilingual language as the task vector to evaluate its effectiveness. We confirm that the task vector serves as a simple and effective approach for tuning downstream tasks in ASR using the Common Voice multilingual dataset."
   ],
   "p1": 2385,
   "pn": 2389,
   "doi": "10.21437/Interspeech.2024-513",
   "url": "interspeech_2024/kang24_interspeech.html"
  },
  "parragallego24_interspeech": {
   "authors": [
    [
     "Luis Felipe",
     "Parra-Gallego"
    ],
    [
     "Tilak",
     "Purohit"
    ],
    [
     "Bogdan",
     "Vlasenko"
    ],
    [
     "Juan Rafael",
     "Orozco-Arroyave"
    ],
    [
     "Mathew",
     "Magimai.-Doss"
    ]
   ],
   "title": "Cross-transfer Knowledge between Speech and Text Encoders to Evaluate Customer Satisfaction",
   "original": "514",
   "order": 97,
   "page_count": 5,
   "abstract": [
    "Customer Satisfaction (CS) in call centers influences customer loyalty and the company's reputation. Traditionally, CS evaluations were conducted manually or with classical machine learning algorithms; however, advancements in deep learning have led to automated systems that evaluate CS using speech and text analyses. Previous studies have shown the text approach to be more accurate but relies on an external ASR for transcription. This study introduces a cross-transfer knowledge technique, distilling knowledge from the BERT model into speech encoders like Wav2Vec2, WavLM, and Whisper. By enriching these encoders with BERT’s linguistic information, we improve speech analysis performance and eliminate the need for an ASR. In evaluations on a dataset of customer opinions, our methods achieve over 92\\% accuracy in identifying CS categories, providing a faster and cost-effective solution compared to traditional text approaches."
   ],
   "p1": 477,
   "pn": 481,
   "doi": "10.21437/Interspeech.2024-514",
   "url": "interspeech_2024/parragallego24_interspeech.html"
  },
  "chen24g_interspeech": {
   "authors": [
    [
     "Honglie",
     "Chen"
    ],
    [
     "Rodrigo",
     "Mira"
    ],
    [
     "Stavros",
     "Petridis"
    ],
    [
     "Maja",
     "Pantic"
    ]
   ],
   "title": "RT-LA-VocE: Real-Time Low-SNR Audio-Visual Speech Enhancement",
   "original": "516",
   "order": 458,
   "page_count": 5,
   "abstract": [
    "In this paper, we aim to generate clean speech frame by frame from a live video stream and a noisy audio stream without relying on future inputs. To this end, we propose RT-LA-VocE, which completely re-designs every component of LA-VocE, a state-of-the-art non-causal audio-visual speech enhancement model, to perform causal real-time inference with a 40 ms input frame. We do so by devising new visual and audio encoders that rely solely on past frames, replacing the Transformer encoder with the Emformer, and designing a new causal neural vocoder C-HiFi-GAN. On the popular AVSpeech dataset, we show that our algorithm achieves state-of-the-art results in all real-time scenarios. More importantly, each component is carefully tuned to minimize the algorithm latency to the theoretical minimum (40 ms) while maintaining a low end-to-end processing latency of 28.15 ms per frame, enabling real-time frame-by-frame enhancement with minimal delay."
   ],
   "p1": 2215,
   "pn": 2219,
   "doi": "10.21437/Interspeech.2024-516",
   "url": "interspeech_2024/chen24g_interspeech.html"
  },
  "barberis24_interspeech": {
   "authors": [
    [
     "Mara",
     "Barberis"
    ],
    [
     "Pieter",
     "De Clercq"
    ],
    [
     "Bastiaan",
     "Tamm"
    ],
    [
     "Hugo",
     "Van hamme"
    ],
    [
     "Maaike",
     "Vandermosten"
    ]
   ],
   "title": "Automatic recognition and detection of aphasic natural speech",
   "original": "517",
   "order": 407,
   "page_count": 5,
   "abstract": [
    "Aphasia is a language disorder affecting one third of stroke patients. Current aphasia assessment does not consider natural speech due to the time consuming nature of manual transcriptions and a lack of knowledge on how to analyze such data. Here, we evaluate the potential of automatic speech recognition (ASR) to transcribe Dutch aphasic speech and the ability of natural speech features to detect aphasia. A picture-description task was administered and automatically transcribed in 62 persons with aphasia and 57 controls. Acoustic and linguistic features were semi-automatically extracted and provided as input to a support vector machine (SVM) classifier. Our ASR model obtained a WER of 24.5%, outperforming earlier ASR models for aphasia. The SVM shows high accuracy (86.6%) at the individual level, with fluency features as most dominant to detect aphasia. ASR and semi-automatic feature extraction can thus facilitate natural speech analysis in a time efficient manner in clinical practice."
   ],
   "p1": 1990,
   "pn": 1994,
   "doi": "10.21437/Interspeech.2024-517",
   "url": "interspeech_2024/barberis24_interspeech.html"
  },
  "laquatra24_interspeech": {
   "authors": [
    [
     "Moreno",
     "La Quatra"
    ],
    [
     "Maria Francesca",
     "Turco"
    ],
    [
     "Torbjørn",
     "Svendsen"
    ],
    [
     "Giampiero",
     "Salvi"
    ],
    [
     "Juan Rafael",
     "Orozco-Arroyave"
    ],
    [
     "Sabato Marco",
     "Siniscalchi"
    ]
   ],
   "title": "Exploiting Foundation Models and Speech Enhancement for Parkinson's Disease Detection from Speech in Real-World Operative Conditions",
   "original": "522",
   "order": 290,
   "page_count": 5,
   "abstract": [
    "This work is concerned with devising a robust Parkinson's (PD) disease detector from speech in real-world operating conditions using (i) foundational models, and (ii) speech enhancement (SE) methods. To this end, we first fine-tune several foundational-based models on the standard PC-GITA (s-PC-GITA) clean data. Our results demonstrate superior performance to previously proposed models. Second, we assess the generalization capability of the PD models on the extended PC-GITA (e-PC-GITA) recordings, collected in real-world operative conditions, and observe a severe drop in performance moving from ideal to real-world conditions. Third, we align training and testing conditions applaying off-the-shelf SE techniques on e-PC-GITA, and a significant boost in performance is observed only  for the foundational-based models. Finally, combining the two best foundational-based models trained on  s-PC-GITA, namely WavLM Base and Hubert Base, yielded top performance on the enhanced e-PC-GITA."
   ],
   "p1": 1405,
   "pn": 1409,
   "doi": "10.21437/Interspeech.2024-522",
   "url": "interspeech_2024/laquatra24_interspeech.html"
  },
  "zhang24f_interspeech": {
   "authors": [
    [
     "Cong",
     "Zhang"
    ],
    [
     "Tong",
     "Li"
    ],
    [
     "Gayle",
     "DeDe"
    ],
    [
     "Christos",
     "Salis"
    ]
   ],
   "title": "Prosody of speech production in latent post-stroke aphasia",
   "original": "524",
   "order": 1046,
   "page_count": 5,
   "abstract": [
    "This study explores prosodic production in latent aphasia, a mild form of aphasia associated with left-hemisphere brain damage (e.g. stroke). Unlike prior research on moderate to severe aphasia, we investigated latent aphasia, which can seem to have very similar speech production with neurotypical speech. We analysed the f0, intensity and duration of utterance-initial and utterance-final words of ten speakers with latent aphasia and ten matching controls. Regression models were fitted to improve our understanding of this understudied type of very mild aphasia. The results highlighted varying degrees of differences in all three prosodic measures between groups. We also investigated the diagnostic classification of latent aphasia versus neurotypical control using random forest, aiming to build a fast and reliable tool to assist with the identification of latent aphasia. The random forest analysis also reinforced the significance of prosodic features in distinguishing latent aphasia."
   ],
   "p1": 5108,
   "pn": 5112,
   "doi": "10.21437/Interspeech.2024-524",
   "url": "interspeech_2024/zhang24f_interspeech.html"
  },
  "maselli24_interspeech": {
   "authors": [
    [
     "Lorenzo",
     "Maselli"
    ],
    [
     "Véronique",
     "Delvaux"
    ]
   ],
   "title": "Aerodynamics of Sakata labial-velar oral stops",
   "original": "525",
   "resource": "https://doi.org/10.17605/OSF.IO/CDFK8",
   "order": 644,
   "page_count": 5,
   "abstract": [
    "The present contribution represents the first in-detail exploratory account of the aerodynamics of labial-velar oral stops in Sakata, a Bantu dialect cluster of southwestern Congo. Data collection took place at the phonetics laboratory facilities of Université de Mons with three speakers of central Sakata. Comparative data of labial-velar and plain bilabial oral stops are presented and analysed. Descriptive statistics of the relevant variables are discussed. Given each group of variables, MANOVA results are presented for specially tailored subsets of the whole dataset to investigate variance in the corpus. Sakata labial-velar stops are shown to differ from plain bilabials for duration, airflow, and pressure patterns. Voiceless labial-velar stops exhibit pressure and airflow values consistent with a more prominent lowering of the tongue root / larynx than their voiced counter- parts. Matches and mismatches with the available typological literature are also delineated and discussed."
   ],
   "p1": 3140,
   "pn": 3144,
   "doi": "10.21437/Interspeech.2024-525",
   "url": "interspeech_2024/maselli24_interspeech.html"
  },
  "kim24g_interspeech": {
   "authors": [
    [
     "Jongsuk",
     "Kim"
    ],
    [
     "Jiwon",
     "Shin"
    ],
    [
     "Junmo",
     "Kim"
    ]
   ],
   "title": "AVCap: Leveraging Audio-Visual Features as Text Tokens for Captioning",
   "original": "526",
   "resource": "https://doi.org/10.5281/zenodo.12737969",
   "order": 392,
   "page_count": 5,
   "abstract": [
    "In recent years, advancements in representation learning and language models have propelled Automated Captioning (AC) to new heights, enabling the generation of human-level descriptions. Leveraging these advancements, we propose AVCap, an Audio-Visual Captioning framework, a simple yet powerful baseline approach applicable to audio-visual captioning. AVCap utilizes audio-visual features as text tokens, which has many advantages not only in performance but also in the extensibility and scalability of the model. AVCap is designed around three pivotal dimensions: the exploration of optimal audio-visual encoder architectures, the adaptation of pre-trained models according to the characteristics of generated text, and the investigation into the efficacy of modality fusion in cap- tioning. Our method outperforms existing audio-visual captioning methods across all metrics and the code is available on https://github.com/JongSuk1/AVCap."
   ],
   "p1": 1915,
   "pn": 1919,
   "doi": "10.21437/Interspeech.2024-526",
   "url": "interspeech_2024/kim24g_interspeech.html"
  },
  "taherian24_interspeech": {
   "authors": [
    [
     "Hassan",
     "Taherian"
    ],
    [
     "Vahid",
     "Ahmadi Kalkhorani"
    ],
    [
     "Ashutosh",
     "Pandey"
    ],
    [
     "Daniel",
     "Wong"
    ],
    [
     "Buye",
     "Xu"
    ],
    [
     "DeLiang",
     "Wang"
    ]
   ],
   "title": "Towards Explainable Monaural Speaker Separation with Auditory-based Training",
   "original": "530",
   "order": 116,
   "page_count": 5,
   "abstract": [
    "Permutation ambiguity is a major challenge in training monaural talker-independent speaker separation. While permutation invariant training (PIT) is a widely used technique, it functions as a `black box', providing little insight into which auditory cues lead to successful training. We introduce a new approach to speaker separation by leveraging differences in pitch and onset, which are both prominent cues for auditory scene analysis. We propose pitch-based and onset-based training to resolve permutation ambiguity, assigning speakers by their pitch frequencies and onset times, respectively. This approach offers a more explainable training strategy than PIT. We also propose a hybrid criterion combining these cues to improve separation performance in challenging conditions such as same-gender speakers or close utterance onsets. Evaluation results show that pitch and onset criteria each perform competitively to PIT and the hybrid criterion surpasses PIT in separating two-speaker mixtures."
   ],
   "p1": 572,
   "pn": 576,
   "doi": "10.21437/Interspeech.2024-530",
   "url": "interspeech_2024/taherian24_interspeech.html"
  },
  "selvakumar24_interspeech": {
   "authors": [
    [
     "Anith",
     "Selvakumar"
    ],
    [
     "Homa",
     "Fashandi"
    ]
   ],
   "title": "Getting More for Less: Using Weak Labels and AV-Mixup for Robust Audio-Visual Speaker Verification",
   "original": "531",
   "order": 970,
   "page_count": 5,
   "abstract": [
    "Distance Metric Learning (DML) has typically dominated the audio-visual speaker verification problem space, owing to strong performance in new and unseen classes. In our work, we explored multitask learning techniques to further enhance DML, and show that an auxiliary task with even weak labels can increase the quality of the learned speaker representation without increasing model complexity during inference. We also extend the Generalized End-to-End Loss (GE2E) to multimodal inputs and demonstrate that it can achieve competitive performance in an audio-visual space. Finally, we introduce AV-Mixup, a multimodal augmentation technique during training time that has shown to reduce speaker overfit. Our network achieves state of the art performance for speaker verification, reporting 0.244%, 0.252%, 0.441% Equal Error Rate (EER) on the VoxCeleb1-O/E/H test sets, which is to our knowledge, the best published results on VoxCeleb1-E and VoxCeleb1-H."
   ],
   "p1": 4728,
   "pn": 4732,
   "doi": "10.21437/Interspeech.2024-531",
   "url": "interspeech_2024/selvakumar24_interspeech.html"
  },
  "dekel24_interspeech": {
   "authors": [
    [
     "Avihu",
     "Dekel"
    ],
    [
     "Raul",
     "Fernandez"
    ]
   ],
   "title": "Exploring the Benefits of Tokenization of Discrete Acoustic Units",
   "original": "533",
   "order": 572,
   "page_count": 5,
   "abstract": [
    "Tokenization algorithms that merge the units of a base vocabulary into larger, variable-rate units have become standard in natural language processing tasks. This idea, however, has been mostly overlooked when the vocabulary consists of phonemes or Discrete Acoustic Units (DAUs), an audio-based representation that is playing an increasingly important role due to the success of discrete language-modeling techniques. In this paper, we showcase the advantages of tokenization of  phonetic units and of DAUs on three prediction tasks: grapheme-to-phoneme, grapheme-to-DAUs, and unsupervised speech generation using DAU language modeling. We demonstrate that tokenization yields significant improvements  in terms of performance, as well as training and inference speed, across all three tasks. We also offer theoretical insights to provide some explanation for the superior performance observed."
   ],
   "p1": 2780,
   "pn": 2784,
   "doi": "10.21437/Interspeech.2024-533",
   "url": "interspeech_2024/dekel24_interspeech.html"
  },
  "zhou24b_interspeech": {
   "authors": [
    [
     "Junzuo",
     "Zhou"
    ],
    [
     "Jiangyan",
     "Yi"
    ],
    [
     "Tao",
     "Wang"
    ],
    [
     "Jianhua",
     "Tao"
    ],
    [
     "Ye",
     "Bai"
    ],
    [
     "Chu Yuan",
     "Zhang"
    ],
    [
     "Yong",
     "Ren"
    ],
    [
     "Zhengqi",
     "Wen"
    ]
   ],
   "title": "TraceableSpeech: Towards Proactively Traceable Text-to-Speech with Watermarking",
   "original": "534",
   "order": 465,
   "page_count": 5,
   "abstract": [
    "Various threats posed by the progress in text-to-speech (TTS) have prompted the need to reliably trace synthesized speech. However, contemporary approaches to this task involve adding watermarks to the audio separately after generation, a process that hurts both speech quality and watermark imperceptibility. In addition, these approaches are limited in robustness and flexibility. To address these problems, we propose TraceableSpeech, a novel TTS model that directly generates watermarked speech, improving watermark imperceptibility and speech quality. Furthermore, We design the frame-wise imprinting and extraction of watermarks, achieving higher robustness against resplicing attacks and temporal flexibility in operation. Experimental results show that TraceableSpeech outperforms the strong baseline where VALL-E or HiFicodec individually uses WavMark in watermark imperceptibility, speech quality and resilience against resplicing attacks. It also can apply to speech of various durations."
   ],
   "p1": 2250,
   "pn": 2254,
   "doi": "10.21437/Interspeech.2024-534",
   "url": "interspeech_2024/zhou24b_interspeech.html"
  },
  "chochlakis24_interspeech": {
   "authors": [
    [
     "Georgios",
     "Chochlakis"
    ],
    [
     "Chandrashekhar",
     "Lavania"
    ],
    [
     "Prashant",
     "Mathur"
    ],
    [
     "Kyu J.",
     "Han"
    ]
   ],
   "title": "Tackling Missing Modalities in Audio-Visual Representation Learning Using Masked Autoencoders",
   "original": "535",
   "order": 960,
   "page_count": 5,
   "abstract": [
    "Audio-visual representations leverage information from both modalities to produce joint representations. Such representations have demonstrated their usefulness in a variety of tasks. However, both modalities incorporated in the learned model might not necessarily be present all the time during inference. In this work, we study whether and how we can make existing models, trained under pristine conditions, robust to partial modality loss without retraining them. We propose to use a curriculum trained Masked AutoEncoder, to impute features of missing input segments. We show that fine-tuning of classification heads with the imputed features make the base models robust on multiple downstream tasks like emotion recognition and Lombard speech recognition. Among the 12 cases evaluated, our method outperforms strong baselines in 10 instances."
   ],
   "p1": 4678,
   "pn": 4682,
   "doi": "10.21437/Interspeech.2024-535",
   "url": "interspeech_2024/chochlakis24_interspeech.html"
  },
  "wang24n_interspeech": {
   "authors": [
    [
     "Lun",
     "Wang"
    ],
    [
     "Om",
     "Thakkar"
    ],
    [
     "Zhong",
     "Meng"
    ],
    [
     "Nicole",
     "Rafidi"
    ],
    [
     "Rohit",
     "Prabhavalkar"
    ],
    [
     "Arun",
     "Narayanan"
    ]
   ],
   "title": "Efficiently Train ASR Models that Memorize Less and Perform Better with Per-core Clipping",
   "original": "536",
   "order": 273,
   "page_count": 5,
   "abstract": [
    "Gradient clipping plays a vital role in training large-scale automatic speech recognition (ASR) models. It is typically applied to minibatch gradients to prevent gradient explosion, and to the individual sample gradients to mitigate unintended memorization. This work systematically investigates the impact of a specific granularity of gradient clipping, namely per-core clipping (PCC), across training a wide range of ASR models. We empirically demonstrate that PCC can effectively mitigate unintended memorization in ASR models. Surprisingly, we find that PCC positively influences ASR performance metrics, leading to improved convergence rates and reduced word error rates. To avoid tuning the additional hyperparameter introduced by PCC, we further propose a novel variant, adaptive per-core clipping (APCC), for streamlined optimization. Our findings highlight the multifaceted benefits of PCC as a strategy for robust, privacy-forward ASR model training."
   ],
   "p1": 1320,
   "pn": 1324,
   "doi": "10.21437/Interspeech.2024-536",
   "url": "interspeech_2024/wang24n_interspeech.html"
  },
  "huang24b_interspeech": {
   "authors": [
    [
     "Zilong",
     "Huang"
    ],
    [
     "Man-Wai",
     "Mak"
    ],
    [
     "Kong Aik",
     "Lee"
    ]
   ],
   "title": "MM-NodeFormer: Node Transformer Multimodal Fusion for Emotion Recognition in Conversation",
   "original": "538",
   "order": 837,
   "page_count": 5,
   "abstract": [
    "Emotion Recognition in Conversation (ERC) has great prospects in human-computer interaction and medical consultation. Existing ERC approaches mainly focus on information in the text and speech modalities and often concatenate multimodal features without considering the richness of emotional information in individual modalities. We propose a multimodal network called MM-NodeFormer for ERC to address this issue. The network leverages the characteristics of different Transformer encoding stages to fuse the emotional features from the text, audio, and visual modalities according to their emotional richness. The module considers text as the main modality and audio and visual as auxiliary modalities, leveraging the complementarity between the main and auxiliary modalities. We conducted extensive experiments on two public benchmark datasets, IEMOCAP and MELD, achieving an accuracy of 74.24% and 67.86%, respectively, significantly higher than many state-of-the-art approaches."
   ],
   "p1": 4069,
   "pn": 4073,
   "doi": "10.21437/Interspeech.2024-538",
   "url": "interspeech_2024/huang24b_interspeech.html"
  },
  "um24b_interspeech": {
   "authors": [
    [
     "Ji Sub",
     "Um"
    ],
    [
     "Hoirin",
     "Kim"
    ]
   ],
   "title": "Utilizing Adaptive Global Response Normalization and Cluster-Based Pseudo Labels for Zero-Shot Voice Conversion",
   "original": "539",
   "order": 564,
   "page_count": 5,
   "abstract": [
    "Recently, there has been an increase in research on zero-shot voice conversion. Many conventional studies use dynamic layers to conduct conversion for unseen speakers. Our aim is to extend dynamic methods to transmit content information as well. To achieve this, we propose AGRN-VC, which utilizes ConvNeXt V2 modules with adaptive global response normalization (AGRN) layers to convey content information. When conveying this information, it is crucial to ensure that the source speaker's information is not transmitted. So we adopt auxiliary learning with cluster-based pseudo labels. It helps the content encoder to focus on content information while excluding speaker information by performing a pseudo label classification task using its output. We conduct comparative experiments between various baseline models and the proposed model using subjective and objective metrics. Our proposed approach achieves better converted speech quality in terms of speaker similarity and naturalness."
   ],
   "p1": 2740,
   "pn": 2744,
   "doi": "10.21437/Interspeech.2024-539",
   "url": "interspeech_2024/um24b_interspeech.html"
  },
  "li24j_interspeech": {
   "authors": [
    [
     "Jialu",
     "Li"
    ],
    [
     "Mark",
     "Hasegawa-Johnson"
    ],
    [
     "Karrie",
     "Karahalios"
    ]
   ],
   "title": "Enhancing Child Vocalization Classification with  Phonetically-Tuned Embeddings for Assisting Autism Diagnosis",
   "original": "540",
   "resource": "https://doi.org/10.5281/zenodo.12795770",
   "order": 1057,
   "page_count": 5,
   "abstract": [
    "The assessment of children at risk of autism typically involves a clinician observing, taking notes, and rating children’s behaviors. A machine learning model that can label adult and child audio may largely save labor in coding children's behaviors, helping clinicians capture critical events and better communicate with parents. In this study, we leverage Wav2Vec 2.0 (W2V2), pre-trained on 4300-hour of home audio of children under 5 years old, to build a unified system for tasks of clinician-child speaker diarization and vocalization classification (VC). To enhance children’s VC, we build a W2V2 phoneme recognition system for children under 4 years old, and we incorporate its phonetically-tuned embeddings as auxiliary features or recognize pseudo phonetic transcripts as an auxiliary task. We test our method on two corpora (Rapid-ABC and BabbleCor) and obtain consistent improvements. Additionally, we outperform the state-of-the-art performance on the reproducible subset of BabbleCor."
   ],
   "p1": 5163,
   "pn": 5167,
   "doi": "10.21437/Interspeech.2024-540",
   "url": "interspeech_2024/li24j_interspeech.html"
  },
  "johnson24_interspeech": {
   "authors": [
    [
     "Alexander",
     "Johnson"
    ],
    [
     "Peter",
     "Plantinga"
    ],
    [
     "Pheobe",
     "Sun"
    ],
    [
     "Swaroop",
     "Gadiyaram"
    ],
    [
     "Abenezer",
     "Girma"
    ],
    [
     "Ahmad",
     "Emami"
    ]
   ],
   "title": "Efficient SQA from Long Audio Contexts: A Policy-driven Approach",
   "original": "543",
   "order": 279,
   "page_count": 5,
   "abstract": [
    "Spoken question answering (SQA), or the process of retrieving relevant information from speech recordings, has become a topic of great interest due in part to the recent growth of large language models. However, many state-of-the-art SQA have memory and compute costs that scale linearly with the length of the input audio context, making them infeasible to apply on long speech recordings (meetings, podcasts, etc.). This paper proposes a method which uses deep Q-learning to learn a policy for skipping over irrelevant audio segments in a longer audio file without analyzing them for more efficient SQA. In this framework, an agent model is trained on BERT sentence embeddings extracted from lightweight ASR transcripts to make decisions on how far it can safely skip through an audio file in order to move closer to the target answer span.  Applied on the CORAAL QA database, this work approaches the performance of SOTA SQA systems while using less than half of the compute to analyze audio."
   ],
   "p1": 1350,
   "pn": 1354,
   "doi": "10.21437/Interspeech.2024-543",
   "url": "interspeech_2024/johnson24_interspeech.html"
  },
  "lai24_interspeech": {
   "authors": [
    [
     "Li-Fang",
     "Lai"
    ],
    [
     "Nicole",
     "Holliday"
    ]
   ],
   "title": "Voice Quality Variation in AAE: An Additional Challenge for Addressing Bias in ASR Models?",
   "original": "544",
   "order": 632,
   "page_count": 5,
   "abstract": [
    "Creaky voice, a non-modal phonation type often stigmatized in the U.S. media, has become increasingly prevalent in the speech of young Americans across ethnic and regional groups. This paper aims to add to our knowledge of voice quality variation and how it interacts with ASR, by conducting three analyses using a new African American English (AAE) dataset.  Acoustic analyses show robust differences between creaky voice and modal voice, suggesting cross-ethnic similarity in vocal fold articulation between AAE and Mainstream American English (MAE) speakers. In addition, we observed gender differences in creaky production both quantitatively (women > men) and qualitatively (women: medial partial creaks vs. men: final full creaks). This indicates that young AAE female speakers are participating in the phonation change taking place in MAE. We also found that the creakier the speech, the more errors in ASR output, suggesting the importance of incorporating voice quality into ASR systems. "
   ],
   "p1": 3080,
   "pn": 3084,
   "doi": "10.21437/Interspeech.2024-544",
   "url": "interspeech_2024/lai24_interspeech.html"
  },
  "aziz24_interspeech": {
   "authors": [
    [
     "Shiran",
     "Aziz"
    ],
    [
     "Yossi",
     "Adi"
    ],
    [
     "Shmuel",
     "Peleg"
    ]
   ],
   "title": "Audio Enhancement from Multiple Crowdsourced Recordings: A Simple and Effective Baseline",
   "original": "545",
   "order": 687,
   "page_count": 5,
   "abstract": [
    "With the popularity of cellular phones, events are often recorded by multiple devices from different locations and shared on social media. Several different recordings could be found for many events. Such recordings are usually noisy, where noise for each device is local and unrelated to others. This case of multiple microphones at unknown locations, capturing local, uncorrelated noise, was rarely treated in the literature. In this work we propose a simple and effective crowdsourced audio enhancement method to remove local noises at each input audio signal. Then, averaging all cleaned source signals gives an improved audio of the event. We demonstrate the effectiveness of our method using synthetic audio signals, together with real-world recordings. This simple approach can set a new baseline for crowdsourced audio enhancement for more sophisticated methods which we hope will be developed by the research community. Code, dataset, and models are available.  "
   ],
   "p1": 3355,
   "pn": 3359,
   "doi": "10.21437/Interspeech.2024-545",
   "url": "interspeech_2024/aziz24_interspeech.html"
  },
  "wu24e_interspeech": {
   "authors": [
    [
     "Wen",
     "Wu"
    ],
    [
     "Chao",
     "Zhang"
    ],
    [
     "Philip C.",
     "Woodland"
    ]
   ],
   "title": "Confidence Estimation for Automatic Detection of Depression and Alzheimer’s Disease Based on Clinical Interviews",
   "original": "546",
   "order": 648,
   "page_count": 5,
   "abstract": [
    "Speech-based automatic detection of Alzheimer's disease (AD) and depression has attracted increased attention. Confidence estimation is crucial for a trust-worthy automatic diagnostic system which informs the clinician about the confidence of model predictions and helps reduce the risk of misdiagnosis. This paper investigates confidence estimation for automatic detection of AD and depression based on clinical interviews. A novel Bayesian approach is proposed which uses a dynamic Dirichlet prior distribution to model the second-order probability of the predictive distribution. Experimental results on the publicly available ADReSS and DAIC-WOZ datasets demonstrate that the proposed method outperforms a range of baselines for both classification accuracy and confidence estimation."
   ],
   "p1": 3160,
   "pn": 3164,
   "doi": "10.21437/Interspeech.2024-546",
   "url": "interspeech_2024/wu24e_interspeech.html"
  },
  "pankov24_interspeech": {
   "authors": [
    [
     "Vikentii",
     "Pankov"
    ],
    [
     "Valeria",
     "Pronina"
    ],
    [
     "Alexander",
     "Kuzmin"
    ],
    [
     "Maksim",
     "Borisov"
    ],
    [
     "Nikita",
     "Usoltsev"
    ],
    [
     "Xingshan",
     "Zeng"
    ],
    [
     "Alexander",
     "Golubkov"
    ],
    [
     "Nikolai",
     "Ermolenko"
    ],
    [
     "Aleksandra",
     "Shirshova"
    ],
    [
     "Yulia",
     "Matveeva"
    ]
   ],
   "title": "DINO-VITS: Data-Efficient Zero-Shot TTS with Self-Supervised Speaker Verification Loss for Noise Robustness",
   "original": "549",
   "order": 141,
   "page_count": 5,
   "abstract": [
    "We address zero-shot TTS systems' noise-robustness problem by proposing a dual-objective training for the speaker encoder using self-supervised DINO loss. This approach enhances the speaker encoder with the speech synthesis objective, capturing a wider range of speech characteristics beneficial for voice cloning. At the same time, the DINO objective improves speaker representation learning, ensuring robustness to noise and speaker discriminability. Experiments demonstrate significant improvements in subjective metrics under both clean and noisy conditions, outperforming traditional speaker-encoder-based TTS systems. Additionally, we explore training zero-shot TTS on noisy, unlabeled data. Our two-stage training strategy, leveraging self-supervised speech models to distinguish between noisy and clean speech, shows notable advances in similarity and naturalness, especially with noisy training datasets, compared to the ASR-transcription-based approach."
   ],
   "p1": 697,
   "pn": 701,
   "doi": "10.21437/Interspeech.2024-549",
   "url": "interspeech_2024/pankov24_interspeech.html"
  },
  "chi24_interspeech": {
   "authors": [
    [
     "Jie",
     "Chi"
    ],
    [
     "Electra",
     "Wallington"
    ],
    [
     "Peter",
     "Bell"
    ]
   ],
   "title": "Characterizing code-switching: Applying Linguistic Principles for Metric Assessment and Development",
   "original": "551",
   "order": 3,
   "page_count": 5,
   "abstract": [
    "With handling code-switching becoming an increasingly important topic in speech technology, driven by the expansion of low-resource and multilingual methodologies, it is vital that we recognize the diversity of code-switching as a phenomenon. We propose a framework that leverages linguistic findings as makeshift ground-truths to assess the quality and sufficiency of existing metrics designed to capture data-sets' differing code-switching styles.  We also introduce a new metric, T-index, which leverages machine translation systems to capture properties of code-switched words in relation to the participating language pair. Through analysis of diverse Hindi-English and Mandarin-English datasets, we systematically explore how well these metrics align with linguistic intuition regarding code-switching richness levels in conversational versus technical domains."
   ],
   "p1": 7,
   "pn": 11,
   "doi": "10.21437/Interspeech.2024-551",
   "url": "interspeech_2024/chi24_interspeech.html"
  },
  "robertson24_interspeech": {
   "authors": [
    [
     "Sean",
     "Robertson"
    ],
    [
     "Gerald",
     "Penn"
    ],
    [
     "Ewan",
     "Dunbar"
    ]
   ],
   "title": "Quantifying the Role of Textual Predictability in Automatic Speech Recognition",
   "original": "552",
   "resource": "https://doi.org/10.5281/zenodo.12797911",
   "order": 829,
   "page_count": 5,
   "abstract": [
    "A long-standing question in automatic speech recognition research is how to attribute errors to the ability of a model to model the acoustics, versus its ability to leverage higher-order context (lexicon, morphology, syntax, semantics). We validate a novel approach which models error rates as a function of relative textual predictability, and yields a single number, k, which measures the effect of textual predictability on the recognizer. We use this method to demonstrate that a Wav2Vec 2.0-based model makes greater stronger use of textual context than a hybrid ASR model, in spite of not using an explicit language model, and also use it to shed light on recent results demonstrating poor performance of standard ASR systems on African-American English. We demonstrate that these mostly represent failures of  acoustic-phonetic modelling. We show how this approach can be used straightforwardly in diagnosing and improving ASR."
   ],
   "p1": 4029,
   "pn": 4033,
   "doi": "10.21437/Interspeech.2024-552",
   "url": "interspeech_2024/robertson24_interspeech.html"
  },
  "han24_interspeech": {
   "authors": [
    [
     "Shiyi",
     "Han"
    ],
    [
     "Mingbin",
     "Xu"
    ],
    [
     "Zhihong",
     "Lei"
    ],
    [
     "Zhen",
     "Huang"
    ],
    [
     "Xingyu",
     "Na"
    ]
   ],
   "title": "Enhancing CTC-based speech recognition with diverse modeling units",
   "original": "555",
   "order": 941,
   "page_count": 5,
   "abstract": [
    "In recent years, the evolution of end-to-end (E2E) automatic speech recognition (ASR) models has been remarkable, largely due to advances in deep learning architectures like transformer.  On top of E2E systems, researchers have achieved substantial accuracy improvement by rescoring E2E model's N-best hypotheses with a phoneme-based model. This raises an interesting question about where the improvements come from other than the system combination effect. We examine the underlying mechanisms driving these gains and propose an efficient joint training approach, where E2E models are trained jointly with diverse modeling units. This methodology does not only align the strengths of both phoneme and grapheme-based models but also reveals that using these diverse modeling units in a synergistic way can significantly enhance model accuracy. Our findings offer new insights into the optimal integration of heterogeneous modeling units in the development of more robust and accurate ASR systems."
   ],
   "p1": 4583,
   "pn": 4587,
   "doi": "10.21437/Interspeech.2024-555",
   "url": "interspeech_2024/han24_interspeech.html"
  },
  "huang24c_interspeech": {
   "authors": [
    [
     "Zhiqi",
     "Huang"
    ],
    [
     "Diamantino",
     "Caseiro"
    ],
    [
     "Kandarp",
     "Joshi"
    ],
    [
     "Christopher",
     "Li"
    ],
    [
     "Pat",
     "Rondon"
    ],
    [
     "Zelin",
     "Wu"
    ],
    [
     "Petr",
     "Zadrazil"
    ],
    [
     "Lillian",
     "Zhou"
    ]
   ],
   "title": "Optimizing Large-Scale Context Retrieval for End-to-End ASR",
   "original": "558",
   "order": 939,
   "page_count": 5,
   "abstract": [
    "Contextual Automatic Speech Recognition (ASR) requires scalable and accurate retrieval of content relevant to the user’s context. This paper presents a comparative study of two independent context retrieval methods: sequence and segment level scoring. Evaluated on datasets with up to 100k phrases, all methods exhibit excellent retrieval recall. Notably, the segment-level scoring achieves an outstanding 75.6% recall over 100k entities. When each method is further integrated with ASR through joint training, significant improvements over nonbiased ASR are observed, with WER reduction of up to 36% with 2k entities and 28% with 100k entities. This comparative analysis provides valuable insights for selecting the optimal context retrieval technique to achieve scalable and accurate performance in contextual ASR applications."
   ],
   "p1": 4573,
   "pn": 4577,
   "doi": "10.21437/Interspeech.2024-558",
   "url": "interspeech_2024/huang24c_interspeech.html"
  },
  "huang24d_interspeech": {
   "authors": [
    [
     "Yiling",
     "Huang"
    ],
    [
     "Weiran",
     "Wang"
    ],
    [
     "Guanlong",
     "Zhao"
    ],
    [
     "Hank",
     "Liao"
    ],
    [
     "Wei",
     "Xia"
    ],
    [
     "Quan",
     "Wang"
    ]
   ],
   "title": "On the Success and Limitations of Auxiliary Network Based Word-Level End-to-End Neural Speaker Diarization",
   "original": "561",
   "order": 8,
   "page_count": 5,
   "abstract": [
    "While standard speaker diarization attempts to answer the question \"who spoke when\", many realistic applications are interested in determining \"who spoke what\". In both the conventional modularized approach and the more recent end-to-end neural diarization (EEND), an additional automatic speech recognition (ASR) model and an orchestration algorithm are required to associate speakers with recognized words. In this paper, we propose Word-level End-to-End Neural Diarization (WEEND) with auxiliary network, a multi-task learning algorithm that performs end-to-end ASR and speaker diarization in the same architecture by sharing blank logits. Such a framework allows easily adding diarization capabilities to any existing RNN-T based ASR models without Word Error Rate (WER) regressions. Experimental results demonstrate that WEEND outperforms a strong turn-based diarization baseline system on all 2-speaker short-form scenarios, with the capability to generalize to audio lengths of 5 minutes."
   ],
   "p1": 32,
   "pn": 36,
   "doi": "10.21437/Interspeech.2024-561",
   "url": "interspeech_2024/huang24d_interspeech.html"
  },
  "chien24b_interspeech": {
   "authors": [
    [
     "Chung-Ming",
     "Chien"
    ],
    [
     "Andros",
     "Tjandra"
    ],
    [
     "Apoorv",
     "Vyas"
    ],
    [
     "Matt",
     "Le"
    ],
    [
     "Bowen",
     "Shi"
    ],
    [
     "Wei-Ning",
     "Hsu"
    ]
   ],
   "title": "Learning Fine-Grained Controllability on Speech Generation via Efficient Fine-Tuning",
   "original": "562",
   "order": 698,
   "page_count": 5,
   "abstract": [
    "As the scale of generative models continues to grow, efficient reuse and adaptation of pre-trained models have become crucial considerations. In this work, we propose Voicebox Adapter, a novel approach that integrates fine-grained conditions into a pre-trained Voicebox speech generation model using a crossattention module. To ensure a smooth integration of newly added modules with pre-trained ones, we explore various efficient fine-tuning approaches. Our experiment shows that the LoRA with bias-tuning configuration yields the best performance, enhancing controllability without compromising speech quality. Across three fine-grained conditional generation tasks, we demonstrate the effectiveness and resource efficiency of Voicebox Adapter. Follow-up experiments further highlight the robustness of Voicebox Adapter across diverse data setups."
   ],
   "p1": 3410,
   "pn": 3414,
   "doi": "10.21437/Interspeech.2024-562",
   "url": "interspeech_2024/chien24b_interspeech.html"
  },
  "gupta24_interspeech": {
   "authors": [
    [
     "Ankit",
     "Gupta"
    ],
    [
     "George",
     "Saon"
    ],
    [
     "Brian",
     "Kingsbury"
    ]
   ],
   "title": "Exploring the limits of decoder-only models trained on public speech recognition corpora",
   "original": "565",
   "order": 52,
   "page_count": 5,
   "abstract": [
    "The emergence of industrial-scale automatic speech recognition (ASR) models such as Whisper and USM, trained on 1M hours of weakly labelled and 12M hours of audio only proprietary data respectively, has led to a stronger need for large scale public ASR corpora and competitive open source pipelines. Unlike the said models, large language models are typically based on Transformer decoders, and it remains unclear if decoder-only models trained on public data alone can deliver competitive performance. In this work, we investigate factors such as choice of training datasets and modeling components necessary for obtaining the best performance using only public English ASR corpora. Our Decoder-Only Transformer for ASR (DOTA) model comprehensively outperforms the encoder-decoder open source replication of Whisper (OWSM) on nearly all English ASR benchmarks and outperforms Whisper large-v3 on 6 out of 15 test sets. We release our codebase and model checkpoints under permissive license."
   ],
   "p1": 252,
   "pn": 256,
   "doi": "10.21437/Interspeech.2024-565",
   "url": "interspeech_2024/gupta24_interspeech.html"
  },
  "kang24b_interspeech": {
   "authors": [
    [
     "Woo Hyun",
     "Kang"
    ],
    [
     "Srikanth",
     "Vishnubhotla"
    ],
    [
     "Rudolf",
     "Braun"
    ],
    [
     "Yogesh",
     "Virkar"
    ],
    [
     "Raghuveer",
     "Peri"
    ],
    [
     "Kyu J.",
     "Han"
    ]
   ],
   "title": "SWAN: SubWord Alignment Network for HMM-free word timing estimation in end-to-end automatic speech recognition",
   "original": "569",
   "order": 598,
   "page_count": 5,
   "abstract": [
    "End-to-end (E2E) automatic speech recognition (ASR) systems often exploited pre-trained hidden Markov model (HMM) systems for word timing estimation (WTE), due to their inability to predict word boundaries. However, training an HMM is difficult for low-resource languages due to the lack of phonetic transcriptions, leading to a high demand for HMM-free WTE methods, particularly for multilingual ASR systems. In this paper, we propose a novel framework for performing WTE without the need for any HMM or phonetic labels. Specifically, the proposed method trains an alignment network using the outputs of the E2E ASR encoder and a voice activity detection module to generate the frame-level subword labels. In our experiments, the proposed method outperforms previous HMM-free WTE methods in a multilingual scenario. Notably, in the Fleurs dataset, we obtain a relative improvement of 57% over previous work in terms of accumulated averaging shift across 5 languages."
   ],
   "p1": 2910,
   "pn": 2914,
   "doi": "10.21437/Interspeech.2024-569",
   "url": "interspeech_2024/kang24b_interspeech.html"
  },
  "seide24_interspeech": {
   "authors": [
    [
     "Frank",
     "Seide"
    ],
    [
     "Yangyang",
     "Shi"
    ],
    [
     "Morrie",
     "Doulaty"
    ],
    [
     "Yashesh",
     "Gaur"
    ],
    [
     "Junteng",
     "Jia"
    ],
    [
     "Chunyang",
     "Wu"
    ]
   ],
   "title": "Speech ReaLLM – Real-time Speech Recognition with Multimodal Language Models by Teaching the Flow of Time",
   "original": "571",
   "order": 389,
   "page_count": 5,
   "abstract": [
    "We introduce Speech ReaLLM, a new ASR architecture that marries “decoder-only” ASR with the RNN-T to make multi-modal LLM architectures capable of real-time streaming. This is the first “decoder-only” ASR architecture designed to handle continuous audio without explicit end-pointing. Speech ReaLLM is a special case of the more general ReaLLM (“real-time LLM”) approach, also introduced here for the first time. The idea is inspired by RNN-T: Instead of generating a response only at the end of a user prompt, generate after every input token received in real time (it is often empty). On Librispeech “test,” an 80M Speech ReaLLM achieves WERs of 3.0% and 7.4% in real time (without an external LM or auxiliary loss). This is only slightly above a 3x larger Attention-Encoder-Decoder baseline. We also show that this way, an LLM architecture can learn to represent and reproduce the flow of time; and that a pre-trained 7B LLM can be fine-tuned to do reasonably well on this task."
   ],
   "p1": 1900,
   "pn": 1904,
   "doi": "10.21437/Interspeech.2024-571",
   "url": "interspeech_2024/seide24_interspeech.html"
  },
  "ho24_interspeech": {
   "authors": [
    [
     "Tuan Vu",
     "Ho"
    ],
    [
     "Kota",
     "Dohi"
    ],
    [
     "Yohei",
     "Kawaguchi"
    ]
   ],
   "title": "Stream-based Active Learning for Anomalous Sound Detection in Machine Condition Monitoring",
   "original": "573",
   "order": 22,
   "page_count": 5,
   "abstract": [
    "This paper introduces an active learning (AL) framework for anomalous sound detection (ASD) in machine condition monitoring system. Typically, ASD models are trained solely on normal samples due to the scarcity of anomalous data, leading to decreased accuracy for unseen samples during inference. AL is a promising solution to solve this problem by enabling the model to learn new concepts more effectively with fewer labeled examples, thus reducing manual annotation efforts. However, its effectiveness in ASD remains unexplored. To minimize update costs and time, our proposed method focuses on updating the scoring backend of ASD system without retraining the neural network model. Experimental results on the DCASE 2023 Challenge Task 2 dataset confirm that our AL framework significantly improves ASD performance even with low labeling budgets. Moreover, our proposed sampling strategy outperforms other baselines in terms of the partial area under the receiver operating characteristic score."
   ],
   "p1": 102,
   "pn": 106,
   "doi": "10.21437/Interspeech.2024-573",
   "url": "interspeech_2024/ho24_interspeech.html"
  },
  "shejwalkar24_interspeech": {
   "authors": [
    [
     "Virat",
     "Shejwalkar"
    ],
    [
     "Om",
     "Thakkar"
    ],
    [
     "Arun",
     "Narayanan"
    ]
   ],
   "title": "Quantifying Unintended Memorization in BEST-RQ ASR Encoders",
   "original": "574",
   "order": 597,
   "page_count": 5,
   "abstract": [
    "Self-supervised ASR encoders are increasingly being adopted in real-world applications as they enable downstream ASR tasks with impressive performances. This raises concerns around privacy of the data used to train such encoders, especially since neural networks are known to unintentionally memorize rare/unique samples from their training data. To this end, we perform the first systematic auditing of unintended memorization in ASR encoders. Specifically, we focus on a state-of-the-art Conformer-based ASR encoder pre-trained using the BEST-RQ technique, which forms the foundation of many real-world ASR applications. We propose a novel auditing method that can successfully demonstrate such memorization in ASR encoders, even for  samples occurring just once in their training data. Finally, we show the promise of pre-training with per-sample gradient clipping towards mitigating such memorization in ASR encoders without significantly impacting downstream model quality."
   ],
   "p1": 2905,
   "pn": 2909,
   "doi": "10.21437/Interspeech.2024-574",
   "url": "interspeech_2024/shejwalkar24_interspeech.html"
  },
  "chen24h_interspeech": {
   "authors": [
    [
     "Ke",
     "Chen"
    ],
    [
     "Jiaqi",
     "Su"
    ],
    [
     "Taylor",
     "Berg-Kirkpatrick"
    ],
    [
     "Shlomo",
     "Dubnov"
    ],
    [
     "Zeyu",
     "Jin"
    ]
   ],
   "title": "Improving Generalization of Speech Separation in Real-World Scenarios: Strategies in Simulation, Optimization, and Evaluation",
   "original": "576",
   "order": 451,
   "page_count": 5,
   "abstract": [
    "Achieving robust speech separation for overlapping speakers in various acoustic environments with noise and reverberation re-mains an open challenge. Although existing datasets are available to train separators for specific scenarios, they do not effectively generalize across diverse real-world scenarios. In this paper, we present a novel data simulation pipeline that produces diverse training data from a range of acoustic environments and content, and propose new training paradigms to improve quality of a general speech separation model. Specifically, we first introduce AC-SIM, a data simulation pipeline that incorporates broad variations in both content and acoustics. Then we integrate multiple training objectives into the permutation invariant training (PIT) to enhance separation quality and generalization of the trained model. Finally, we conduct comprehensive ob- jective and human listening experiments across separation architectures and benchmarks to validate our methods, demonstrating substantial improvement of generalization on both non-homologous and real-world test sets."
   ],
   "p1": 2180,
   "pn": 2184,
   "doi": "10.21437/Interspeech.2024-576",
   "url": "interspeech_2024/chen24h_interspeech.html"
  },
  "yang24g_interspeech": {
   "authors": [
    [
     "Tien-Ju",
     "Yang"
    ],
    [
     "Andrew",
     "Rosenberg"
    ],
    [
     "Bhuvana",
     "Ramabhadran"
    ]
   ],
   "title": "Contemplative Mechanism for Speech Recognition: Speech Encoders can Think",
   "original": "577",
   "order": 707,
   "page_count": 5,
   "abstract": [
    "Encoders are crucial for speech recognition, and boosting their computation enhances feature quality. However, the common practice of scaling model size to achieve this is becoming increasingly costly with large models. This paper proposes a novel \"contemplative mechanism\" designed to enhance encoder quality without increasing model size. Our core innovation lies in strategically interleaving special \"think tokens\" within the input sequence of speech tokens during both training and inference. This mechanism encourages deeper processing of the original input and leads to improved feature representations. We demonstrate the effectiveness of this mechanism on various speech recognition datasets and encoder architectures. Experiments show that inserting a single think token per speech token can yield accuracy gains equivalent to doubling the model size. While focused on the speech domain, our method holds promise for improving encoders in other modalities as well."
   ],
   "p1": 3455,
   "pn": 3459,
   "doi": "10.21437/Interspeech.2024-577",
   "url": "interspeech_2024/yang24g_interspeech.html"
  },
  "jiang24_interspeech": {
   "authors": [
    [
     "Pan-Pan",
     "Jiang"
    ],
    [
     "Jimmy",
     "Tobin"
    ],
    [
     "Katrin",
     "Tomanek"
    ],
    [
     "Robert",
     "MacDonald"
    ],
    [
     "Katie",
     "Seaver"
    ],
    [
     "Richard",
     "Cave"
    ],
    [
     "Marilyn",
     "Ladewig"
    ],
    [
     "Rus",
     "Heywood"
    ],
    [
     "Jordan",
     "Green"
    ]
   ],
   "title": "Learnings from curating a trustworthy, well-annotated, and useful dataset of disordered English speech",
   "original": "578",
   "order": 513,
   "page_count": 4,
   "abstract": [
    "Project Euphonia, a Google initiative, is dedicated to improving automatic speech recognition (ASR) of disordered speech. A central objective of the project is to create a large, high-quality, and diverse speech corpus. This report describes the project’s latest advancements in data collection and annotation methodologies, such as expanding speaker diversity in the database, adding human-reviewed transcript corrections and audio quality tags to 350K (of the 1.2M total) audio recordings, and amassing a comprehensive set of metadata (including more than 40 speech characteristic labels) for over 75% of the speakers in the database. We report on the impact of transcript corrections on our machine-learning (ML) research, inter-rater variability of assessments of disordered speech patterns, and our rationale for gathering speech metadata. We also consider the limitations of using automated off-the-shelf annotation methods for assessing disordered speech."
   ],
   "p1": 2490,
   "pn": 2493,
   "doi": "10.21437/Interspeech.2024-578",
   "url": "interspeech_2024/jiang24_interspeech.html"
  },
  "jukic24_interspeech": {
   "authors": [
    [
     "Ante",
     "Jukić"
    ],
    [
     "Roman",
     "Korostik"
    ],
    [
     "Jagadeesh",
     "Balam"
    ],
    [
     "Boris",
     "Ginsburg"
    ]
   ],
   "title": "Schrödinger Bridge for Generative Speech Enhancement",
   "original": "579",
   "order": 244,
   "page_count": 5,
   "abstract": [
    "This paper proposes a generative speech enhancement model based on Schrodinger bridge (SB). The proposed model is employinga tractable SB to formulate a data-to-data process between the clean speech distribution and the observed noisy speech distribution. The model is trained with a data prediction loss, aiming to recover the complex-valued clean speech coefficients, and an auxiliary time-domain loss is used to improve training of the model. The effectiveness of the proposed SB-based model is evaluated in two different speech enhancement tasks: speech denoising and speech dereverberation. The experimental results demonstrate that the proposed SB-based outperforms diffusion-based models in terms of speech quality metrics and ASR performance, e.g., resulting in relative word error rate reduction of 20% for denoising and 6% for dereverberation compared to the best baseline model. The proposed model also demonstrates improved efficiency, achieving better quality than the baselines for the same number of sampling steps and with a reduced computational cost."
   ],
   "p1": 1175,
   "pn": 1179,
   "doi": "10.21437/Interspeech.2024-579",
   "url": "interspeech_2024/jukic24_interspeech.html"
  },
  "zheng24_interspeech": {
   "authors": [
    [
     "Naijun",
     "Zheng"
    ],
    [
     "Xucheng",
     "Wan"
    ],
    [
     "Kai",
     "Liu"
    ],
    [
     "Ziqing",
     "Du"
    ],
    [
     "Zhou",
     "Huan"
    ]
   ],
   "title": "An efficient text augmentation approach for contextualized Mandarin speech recognition",
   "original": "583",
   "order": 271,
   "page_count": 5,
   "abstract": [
    "Although contextualized automatic speech recognition (ASR) systems are commonly used to improve the recognition of uncommon words, their effectiveness is hindered by the inherent limitations of speech-text data availability. To address this challenge, our study proposes to leverage extensive text-only datasets and contextualize pre-trained ASR models using a straightforward text-augmentation (TA) technique, all while keeping computational costs minimal. In particular, to contextualize a pre-trained CIF-based ASR, we construct a codebook using limited speech-text data. By utilizing a simple codebook lookup process, we convert available text-only data into latent text embeddings. These embeddings then enhance the inputs for the contextualized ASR. Our experiments on diverse Mandarin test sets demonstrate that our TA approach significantly boosts recognition performance. The top-performing system shows relative CER improvements of up to 30% on rare words and 15% across all words in general."
   ],
   "p1": 1310,
   "pn": 1314,
   "doi": "10.21437/Interspeech.2024-583",
   "url": "interspeech_2024/zheng24_interspeech.html"
  },
  "zhong24_interspeech": {
   "authors": [
    [
     "Jiafeng",
     "Zhong"
    ],
    [
     "Bin",
     "Li"
    ],
    [
     "Jiangyan",
     "Yi"
    ]
   ],
   "title": "Enhancing Partially Spoofed Audio Localization with Boundary-aware Attention Mechanism",
   "original": "587",
   "resource": "https://doi.org/10.5281/zenodo.12747417",
   "order": 992,
   "page_count": 5,
   "abstract": [
    "The task of partially spoofed audio localization aims to accurately determine audio authenticity at a frame level. Although some works have achieved encouraging results, utilizing boundary information within a single model remains an unexplored research topic. In this work, we propose a novel method called Boundary-aware Attention Mechanism (BAM). Specifically, it consists of two core modules: Boundary Enhancement and Boundary Frame-wise Attention. The former assembles the intra-frame and inter-frame information to extract discriminative boundary features that are subsequently used for boundary position detection and authenticity decision, while the latter leverages boundary prediction results to explicitly control the feature interaction between frames, which achieves effective discrimination between real and fake frames. Experimental results on PartialSpoof database demonstrate our proposed method achieves the best performance. The code is available at https://github.com/mediaseclab/BAM."
   ],
   "p1": 4838,
   "pn": 4842,
   "doi": "10.21437/Interspeech.2024-587",
   "url": "interspeech_2024/zhong24_interspeech.html"
  },
  "hou24_interspeech": {
   "authors": [
    [
     "Zejiang",
     "Hou"
    ],
    [
     "Goeric",
     "Huybrechts"
    ],
    [
     "Anshu",
     "Bhatia"
    ],
    [
     "Daniel",
     "Garcia-Romero"
    ],
    [
     "Kyu J.",
     "Han"
    ],
    [
     "Katrin",
     "Kirchhoff"
    ]
   ],
   "title": "Revisiting Convolution-free Transformer for Speech Recognition",
   "original": "588",
   "order": 938,
   "page_count": 5,
   "abstract": [
    "Convolution augmented Transformer architectures have dominated the field of automatic speech recognition by showing better WER results when the models are trained on relatively smaller training data. In this work, we revisit the necessity of convolution modules in the ASR encoder architecture, given that the inductive bias brought by the convolution modules may only boost performance in a low training data regime. We show that with architectural improvements to the Transformer block, a convolution-free Transformer architecture (namely, Transformer++) can catch up with the best Conformer WER results as we scale up the training data. Moreover, we demonstrate that with large scale unsupervised pre-training, the proposed Transformer++ can achieve even better WER than the best Conformer results. Importantly, Transformer++ achieves state-of- the-art performance with top efficiency, where we show 40% CPU inference realtime factor (RTF) improvement and 25% GPU training speedup compared to Conformer."
   ],
   "p1": 4568,
   "pn": 4572,
   "doi": "10.21437/Interspeech.2024-588",
   "url": "interspeech_2024/hou24_interspeech.html"
  },
  "guo24b_interspeech": {
   "authors": [
    [
     "Houjian",
     "Guo"
    ],
    [
     "Chaoran",
     "Liu"
    ],
    [
     "Carlos Toshinori",
     "Ishi"
    ],
    [
     "Hiroshi",
     "Ishiguro"
    ]
   ],
   "title": "X-E-Speech: Joint Training Framework of Non-Autoregressive Cross-lingual Emotional Text-to-Speech and Voice Conversion",
   "original": "589",
   "order": 1021,
   "page_count": 5,
   "abstract": [
    "Large language models (LLMs) have been widely used in cross-lingual and emotional speech synthesis, but they require extensive data and retain the drawbacks of previous autoregressive (AR) speech models, such as slow inference speed and lack of robustness and interpretation. In this paper, we propose a cross-lingual emotional speech generation model, X-E-Speech, which achieves the disentanglement of speaker style and cross-lingual content features by jointly training non-autoregressive (NAR) voice conversion (VC) and text-to-speech (TTS) models. For TTS, we freeze the style-related model components and fine-tune the content-related structures to enable cross-lingual emotional speech synthesis. For VC, we improve the emotion similarity between the generated results and the reference speech by introducing the similarity loss between content features for VC and text for TTS."
   ],
   "p1": 4983,
   "pn": 4987,
   "doi": "10.21437/Interspeech.2024-589",
   "url": "interspeech_2024/guo24b_interspeech.html"
  },
  "yang24h_interspeech": {
   "authors": [
    [
     "Haici",
     "Yang"
    ],
    [
     "Jiaqi",
     "Su"
    ],
    [
     "Minje",
     "Kim"
    ],
    [
     "Zeyu",
     "Jin"
    ]
   ],
   "title": "Genhancer: High-Fidelity Speech Enhancement via Generative Modeling on Discrete Codec Tokens",
   "original": "590",
   "order": 243,
   "page_count": 5,
   "abstract": [
    "We present a high-fidelity generative speech enhancement model, Genhancer, which generates clean speech as discrete codec tokens while conditioning on the input speech features. Discrete codec tokens provide an efficient latent domain in place of the conventional time or time-frequency domain of signals, so as to enable complex modeling of speech and allow generative modeling to enforce speaker consistency and content continuity. We provide insights into the best-fit generation scheme for enhancement among parallel prediction, auto-regression, and masking to demonstrate the benefits of conditioning on both pre-trained and jointly learned speech features. Subjective and objective tests show that Genhancer significantly improves audio quality and speaker-identity retention over the SOTA baselines, including conventional and generative ones while preserving content accuracy. Audio samples and supplement materials are available at https://minjekim.com/research-projects/genhancer"
   ],
   "p1": 1170,
   "pn": 1174,
   "doi": "10.21437/Interspeech.2024-590",
   "url": "interspeech_2024/yang24h_interspeech.html"
  },
  "li24k_interspeech": {
   "authors": [
    [
     "Yanxiong",
     "Li"
    ],
    [
     "Jiaxin",
     "Tan"
    ],
    [
     "Guoqing",
     "Chen"
    ],
    [
     "Jialong",
     "Li"
    ],
    [
     "Yongjie",
     "Si"
    ],
    [
     "Qianhua",
     "He"
    ]
   ],
   "title": "Low-Complexity Acoustic Scene Classification Using Parallel Attention-Convolution Network",
   "original": "591",
   "order": 115,
   "page_count": 5,
   "abstract": [
    "This work is an improved system that we submitted to task 1 of DCASE2023 challenge. We propose a method of low-complexity acoustic scene classification by a parallel attention-convolution network which consists of four modules, including pre-processing, fusion, global and local contextual information extraction. The proposed network is computationally efficient to capture global and local contextual information from each audio clip. In addition, we integrate other techniques into our method, such as knowledge distillation, data augmentation, and adaptive residual normalization. When evaluated on the official dataset of DCASE2023 challenge, our method obtains the highest accuracy of 56.10% with parameter number of 5.21 kilo and multiply-accumulate operations of 1.44 million. It exceeds the top two systems of DCASE2023 challenge in accuracy and complexity, and obtains state-of-the-art result. Code is at: https://github.com/Jessytan/Low-complexity-ASC."
   ],
   "p1": 567,
   "pn": 571,
   "doi": "10.21437/Interspeech.2024-591",
   "url": "interspeech_2024/li24k_interspeech.html"
  },
  "xin24b_interspeech": {
   "authors": [
    [
     "Yifei",
     "Xin"
    ],
    [
     "Zhihong",
     "Zhu"
    ],
    [
     "Xuxin",
     "Cheng"
    ],
    [
     "Xusheng",
     "Yang"
    ],
    [
     "Yuexian",
     "Zou"
    ]
   ],
   "title": "Audio-text Retrieval with Transformer-based Hierarchical Alignment and Disentangled Cross-modal Representation",
   "original": "593",
   "order": 237,
   "page_count": 5,
   "abstract": [
    "Most existing audio-text retrieval (ATR) approaches typically rely on a single-level interaction to associate audio and text, limiting their ability to align different modalities and leading to suboptimal matches. To tackle this issue, we present a novel ATR framework that leverages two-stream Transformers in conjunction with a Hierarchical Alignment (THA) module to identify multi-level correspondences of different Transformer blocks between audio and text. Moreover, current ATR methods mainly focus on learning a global-level representation, missing out on intricate details to capture audio occurrences that correspond to textual semantics. To bridge this gap, we introduce a Disentangled Cross-modal Representation (DCR) approach that disentangles high-dimensional features into compact latent factors to grasp finegrained audio-text semantic correlations. Additionally, we develop a confidence-aware (CA) module to estimate the confidence of each latent factor pair and adaptively aggregate cross-modal latent factors to achieve local semantic alignment. Experiments show that our THA effectively boosts ATR performance, with the DCR approach further contributing to consistent performance gains."
   ],
   "p1": 1140,
   "pn": 1144,
   "doi": "10.21437/Interspeech.2024-593",
   "url": "interspeech_2024/xin24b_interspeech.html"
  },
  "yang24i_interspeech": {
   "authors": [
    [
     "Huai-Zhe",
     "Yang"
    ],
    [
     "Chia-Ping",
     "Chen"
    ],
    [
     "Shan-Yun",
     "He"
    ],
    [
     "Cheng-Ruei",
     "Li"
    ]
   ],
   "title": "Bilingual and Code-switching TTS Enhanced with Denoising Diffusion Model and GAN",
   "original": "600",
   "order": 1012,
   "page_count": 5,
   "abstract": [
    "In this paper, we propose a Mandarin-English bilingual and code-switching text-to-speech (TTS) system featuring a diffusion model and generative adversarial network (GAN) to improve the output speech. To address speaker consistency, we employ a feature separation architecture that converts language and speaker IDs into embeddings as input to the encoder. Subsequently, we employ two adversarial classifiers and two classifiers to separate language and speaker features. We integrate a modified diffusion model and discriminators to push for better speech quality and speaker consistency, especially for code-swtiching scenarios. On the MOS measure, the performance of the proposed TTS system differs only slightly from the ground truth data in monolingual speech and achieves MOS of 3.83 in the synthesis of code-switching speech."
   ],
   "p1": 4938,
   "pn": 4942,
   "doi": "10.21437/Interspeech.2024-600",
   "url": "interspeech_2024/yang24i_interspeech.html"
  },
  "liou24_interspeech": {
   "authors": [
    [
     "Shiu-Hsiang",
     "Liou"
    ],
    [
     "Po-Cheng",
     "Chan"
    ],
    [
     "Chia-Ping",
     "Chen"
    ],
    [
     "Tzu-Chieh",
     "Lin"
    ],
    [
     "Chung-Li",
     "Lu"
    ],
    [
     "Yu-Han",
     "Cheng"
    ],
    [
     "Hsiang-Feng",
     "Chuang"
    ],
    [
     "Wei-Yu",
     "Chen"
    ]
   ],
   "title": "Enhancing ECAPA-TDNN with Feature Processing Module and Attention Mechanism for Speaker Verification",
   "original": "601",
   "order": 439,
   "page_count": 5,
   "abstract": [
    "In this paper, we introduce three methods to enhance the state-of-the-art ECAPA-TDNN model for speaker verification, namely self-calibration (SC), simple attention mechanism (SimAM), and a modified temporal dynamic convolution (MTDY) based front-end module. The SC module expands the model’s receptive field and improves spatial attention for better capture of contextual information. The SimAM attention mechanism assigns unique weights to individual neurons, so it can place greater emphasis on more informative ones. The MDTY-based front-end module adapts itself to diverse temporal speech features with adaptive convolutional kernels, and aggregates these kernels to capture temporal variations with attention weights. Our proposed model, IM ECAPA MTDY-TDNN SimAM, demonstrates improved performance and complexity trade-offs compared to recent research works. On the VoxCeleb1-H test set, it achieves a 1.655% EER and 0.157 minDCF with 9.71M parameters and 1.97G FLOPs."
   ],
   "p1": 2120,
   "pn": 2124,
   "doi": "10.21437/Interspeech.2024-601",
   "url": "interspeech_2024/liou24_interspeech.html"
  },
  "li24l_interspeech": {
   "authors": [
    [
     "HengYu",
     "Li"
    ],
    [
     "Kangdi",
     "Mei"
    ],
    [
     "Zhaoci",
     "Liu"
    ],
    [
     "Yang",
     "Ai"
    ],
    [
     "Liping",
     "Chen"
    ],
    [
     "Jie",
     "Zhang"
    ],
    [
     "Zhenhua",
     "Ling"
    ]
   ],
   "title": "Refining Self-supervised Learnt Speech Representation using Brain Activations",
   "original": "604",
   "order": 305,
   "page_count": 5,
   "abstract": [
    "It was shown in literature that speech representations extracted by self-supervised pre-trained models exhibit similarities with brain activations of human for speech perception and fine-tuning speech representation models on downstream tasks can further improve the similarity. However, it still remains unclear if this similarity can be used to optimize the pre-trained speech models. In this work, we therefore propose to use the brain activations recorded by fMRI to refine the often-used wav2vec2.0 model by aligning model representations toward human neural responses. Experimental results on SUPERB reveal that this operation is beneficial for several downstream tasks, e.g., speaker verification, automatic speech recognition, intent classification. One can then consider the proposed method as a new alternative to improve self-supervised speech models."
   ],
   "p1": 1480,
   "pn": 1484,
   "doi": "10.21437/Interspeech.2024-604",
   "url": "interspeech_2024/li24l_interspeech.html"
  },
  "gu24_interspeech": {
   "authors": [
    [
     "Tianteng",
     "Gu"
    ],
    [
     "Bei",
     "Liu"
    ],
    [
     "Hang",
     "Shao"
    ],
    [
     "Yanmin",
     "Qian"
    ]
   ],
   "title": "SparseWAV: Fast and Accurate One-Shot Unstructured Pruning for Large Speech Foundation Models",
   "original": "607",
   "order": 924,
   "page_count": 5,
   "abstract": [
    "Self-supervised speech representation learning has shown remarkable capability in automatic speech recognition. However, it requires substantial computations and storage capacity. Pruning is an effective method for model compression. In this work, we propose SparseWAV, a fast and accurate unstructured pruning framework designed for large speech foundation models, which can efficiently remove unimportant parameters without sacrificing performance. It adaptively determines the sparsity ratio for each weight matrix within pre-trained models and updates the remaining parameters to compensate for the eliminated ones. Experiments on Librispeech demonstrate the proposed method can remove 80% of the parameters of pre-trained large speech foundation models with negligible performance loss. Compared to previous works, our resulting models achieves up to 30% improvement in performance under similar parameters. Meanwhile, the compression algorithm's time consumption is reduced by up to 1080x."
   ],
   "p1": 4498,
   "pn": 4502,
   "doi": "10.21437/Interspeech.2024-607",
   "url": "interspeech_2024/gu24_interspeech.html"
  },
  "kim24h_interspeech": {
   "authors": [
    [
     "Changhwan",
     "Kim"
    ]
   ],
   "title": "ClariTTS: Feature-ratio Normalization and Duration Stabilization for Code-mixed Multi-speaker Speech Synthesis",
   "original": "608",
   "order": 696,
   "page_count": 5,
   "abstract": [
    "Recent text-to-speech (TTS) models have synthesized remarkably natural speech for code-mixed TTS as well as cross-lingual TTS. However, code-mixed texts are synthesized with unnatural accents for each word because speaker-related features can include linguistic features from the speaker's source language. To solve the problems, we propose ClariTTS, which synthesizes speech with appropriate accents for the language of each word in code-mixed texts. Specifically, we propose feature-ratio normalized affine coupling layer in the flow-based TTS model, which disentangles speaker and linguistic features to prevent the accent of the target speaker's source language from being included in the target language. Furthermore, we introduce a duration stabilization training objectives to ensure stable duration prediction in code-mixed TTS. From the experimental results, we demonstrate that ClariTTS reliably generates code-mixed speech with clear pronunciation while preserving speaker identity. "
   ],
   "p1": 3400,
   "pn": 3404,
   "doi": "10.21437/Interspeech.2024-608",
   "url": "interspeech_2024/kim24h_interspeech.html"
  },
  "li24m_interspeech": {
   "authors": [
    [
     "Junhui",
     "Li"
    ],
    [
     "Pu",
     "Wang"
    ],
    [
     "Jialu",
     "Li"
    ],
    [
     "Youshan",
     "Zhang"
    ]
   ],
   "title": "Complex Image-Generative Diffusion Transformer for Audio Denoising",
   "original": "611",
   "order": 459,
   "page_count": 5,
   "abstract": [
    "The audio denoising technique has captured widespread attention in the deep neural network field. Recently, the audio denoising problem has been converted into an image generation task, and deep learning-based approaches have been applied to tackle this problem. However, its performance is still limited, leaving room for further improvement. In order to enhance audio denoising performance, this paper introduces a complex image-generative diffusion transformer that captures more information from the complex Fourier domain. We explore a novel diffusion transformer by integrating the transformer with a diffusion model. Our proposed model demonstrates the scalability of the transformer and expands the receptive field of sparse attention using attention diffusion. Our work is among the first to utilize diffusion transformers to deal with the image generation task for audio denoising. Extensive experiments on two benchmark datasets demonstrate that our proposed model outperforms state-of-the-art models."
   ],
   "p1": 2220,
   "pn": 2224,
   "doi": "10.21437/Interspeech.2024-611",
   "url": "interspeech_2024/li24m_interspeech.html"
  },
  "li24n_interspeech": {
   "authors": [
    [
     "Aijun",
     "Li"
    ],
    [
     "Jun",
     "Gao"
    ],
    [
     "Zhiwei",
     "Wang"
    ]
   ],
   "title": "Effect of Complex Boundary Tones on Tone Identification: An Experimental Study with Mandarin-speaking Preschool Children",
   "original": "614",
   "order": 865,
   "page_count": 5,
   "abstract": [
    "In Chinese, boundary tones can be categorized into simultaneous addition boundary tone (SiABT) and successive addition boundary tone (SuABT). This study focuses on the use of falling SuABT in child-directed speech for preschoolers' word learning scenarios and examines whether SuABT affects children's decoding of lexical tone. Forty-eight Mandarin-speaking preschoolers participated in an identification experiment. A total of 90 familiar monosyllabic words were designed, comprising two types of word pairs: with or without tonal contrast. The target words were embedded in the two types of boundary tones. The results indicate that complex SuABT does not impede the decoding of tonal categories but rather assists preschoolers in acquiring lexical information. Age demonstrates a correlation with the ability to decode tones under both boundary tones, with a more pronounced correlation observed under the SuABT condition. As age increases, this ability gradually stabilizes after 32 months."
   ],
   "p1": 4204,
   "pn": 4208,
   "doi": "10.21437/Interspeech.2024-614",
   "url": "interspeech_2024/li24n_interspeech.html"
  },
  "hirschkind24_interspeech": {
   "authors": [
    [
     "Nameer",
     "Hirschkind"
    ],
    [
     "Xiao",
     "Yu"
    ],
    [
     "Mahesh Kumar",
     "Nandwana"
    ],
    [
     "Joseph",
     "Liu"
    ],
    [
     "Eloi",
     "DuBois"
    ],
    [
     "Dao",
     "Le"
    ],
    [
     "Nicolas",
     "Thiebaut"
    ],
    [
     "Colin",
     "Sinclair"
    ],
    [
     "Kyle",
     "Spence"
    ],
    [
     "Charles",
     "Shang"
    ],
    [
     "Zoe",
     "Abrams"
    ],
    [
     "Morgan",
     "McGuire"
    ]
   ],
   "title": "Diffusion Synthesizer for Efficient Multilingual Speech to Speech Translation",
   "original": "616",
   "order": 169,
   "page_count": 5,
   "abstract": [
    "We introduce DiffuseST, a low-latency, direct speech-to-speech translation system capable of preserving the input speaker's voice zero-shot while translating from multiple source languages into English. We experiment with the synthesizer component of the architecture, comparing a Tacotron-based synthesizer to a novel diffusion-based synthesizer. We find the diffusion-based synthesizer to improve MOS and PESQ audio quality metrics by 23% each and speaker similarity by 5% while maintaining comparable BLEU scores. Despite having more than double the parameter count, the diffusion synthesizer has lower latency, allowing the entire model to run more than 5 times faster than real-time."
   ],
   "p1": 837,
   "pn": 841,
   "doi": "10.21437/Interspeech.2024-616",
   "url": "interspeech_2024/hirschkind24_interspeech.html"
  },
  "nakagome24_interspeech": {
   "authors": [
    [
     "Yu",
     "Nakagome"
    ],
    [
     "Michael",
     "Hentschel"
    ]
   ],
   "title": "InterBiasing: Boost Unseen Word Recognition through Biasing Intermediate Predictions",
   "original": "619",
   "order": 43,
   "page_count": 5,
   "abstract": [
    "Despite recent advances in end-to-end speech recognition methods, their output is biased to the training data’s vocabulary, resulting in inaccurate recognition of unknown terms or proper nouns. To improve the recognition accuracy for a given set of such terms, we propose an adaptation parameter-free approach based on Self-conditioned CTC. Our method improves the recognition accuracy of misrecognized target keywords by substituting their intermediate CTC predictions with corrected labels, which are then passed on to the subsequent layers. First, we create pairs of correct labels and recognition error instances for a keyword list using Text-to-Speech and a recognition model. We use these pairs to replace intermediate prediction errors by the labels. Conditioning the subsequent layers of the encoder on the labels, it is possible to acoustically evaluate the target keywords. Experiments conducted in Japanese demonstrated that our method successfully improved the F1 score for unknown words."
   ],
   "p1": 207,
   "pn": 211,
   "doi": "10.21437/Interspeech.2024-619",
   "url": "interspeech_2024/nakagome24_interspeech.html"
  },
  "guan24_interspeech": {
   "authors": [
    [
     "Haixin",
     "Guan"
    ],
    [
     "Wei",
     "Dai"
    ],
    [
     "Guangyong",
     "Wang"
    ],
    [
     "Xiaobin",
     "Tan"
    ],
    [
     "Peng",
     "Li"
    ],
    [
     "Jiaen",
     "Liang"
    ]
   ],
   "title": "Reducing Speech Distortion and Artifacts for Speech Enhancement by Loss Function",
   "original": "620",
   "order": 355,
   "page_count": 5,
   "abstract": [
    "Deep learning-based speech enhancement has made significant strides. However, challenges such as speech distortion and artifacts persist. These issues can diminish perceived auditory quality and the accuracy of speech recognition systems, particularly when employing lightweight models. Therefore, this paper investigates the underlying principles governing the formation of speech distortion and artifacts, and introduces a novel combined loss function that integrates Voice Activity Detection (VAD) information and speech continuity to solve the problem. Additionally, a new training strategy is designed based on the proposed loss function to address the difficulty of training this combined loss on extremely small models. Experiments validate the effectiveness of our approach on the DNS2020 dataset and real meeting data in enhancing both subjective and objective speech metrics, as well as Automatic Speech Recognition (ASR) performance."
   ],
   "p1": 1730,
   "pn": 1734,
   "doi": "10.21437/Interspeech.2024-620",
   "url": "interspeech_2024/guan24_interspeech.html"
  },
  "lin24d_interspeech": {
   "authors": [
    [
     "Tzu-Quan",
     "Lin"
    ],
    [
     "Hung-yi",
     "Lee"
    ],
    [
     "Hao",
     "Tang"
    ]
   ],
   "title": "DAISY: Data Adaptive Self-Supervised Early Exit for Speech Representation Models",
   "original": "626",
   "order": 927,
   "page_count": 5,
   "abstract": [
    "Self-supervised speech models have shown to be useful for various tasks, but their large size limits the use in devices with low computing power and memory. In this work, we explore early exit, an approach for reducing latency by exiting the forward process of a network early. Most approaches of early exit need a separate early exit model for each task, with some even requiring fine-tuning of the entire pretrained model. We introduce Data Adaptive Self-Supervised Early Exit (DAISY), an approach that decides when to exit based on the self-supervised loss, eliminating the need for multiple round of training and fine-tuning. DAISY matches the performance of HuBERT on the MiniSUPERB benchmark, but with much faster inference times. Our analysis on the adaptivity of DAISY shows that the model exits early (using fewer layers) on clean data while exits late (using more layers) on noisy data, dynamically adjusting the computational cost of inference based on the noise level of each sample."
   ],
   "p1": 4513,
   "pn": 4517,
   "doi": "10.21437/Interspeech.2024-626",
   "url": "interspeech_2024/lin24d_interspeech.html"
  },
  "chen24i_interspeech": {
   "authors": [
    [
     "Guanlin",
     "Chen"
    ],
    [
     "Yun",
     "Jin"
    ]
   ],
   "title": "Cascaded Transfer Learning Strategy for Cross-Domain Alzheimer's  Disease Recognition through Spontaneous Speech",
   "original": "627",
   "order": 183,
   "page_count": 5,
   "abstract": [
    "In our work, we propose a cascaded transfer learning strategy for cross-domain Alzheimer’s disease (AD) recognition through spontaneous speech. This strategy cascaded a pre-trained GPT-3 model as the first-level and a Random Forest Multi-Source Discriminant Subspace Alignment (RF-MDSA) Algorithm as the second-level. The goal of this strategy is to align feature spaces extracted from different corpora to derive a common projection subspace. We conduct experiments on three corpora in Chinese, English, and Spanish. On single corpus experiments, we achieve accuracy rates of 74.6%, 82.4%, and 62.8%, respectively. In cross-domain experiments, we achieve accuracy rates of 71.8%, 72.9%, and 56.1%, respectively. These results suggest that our cascaded strategy improves the model’s generalization capabilities, making it effective for cross-domain AD recognition."
   ],
   "p1": 907,
   "pn": 911,
   "doi": "10.21437/Interspeech.2024-627",
   "url": "interspeech_2024/chen24i_interspeech.html"
  },
  "ochi24_interspeech": {
   "authors": [
    [
     "Keiko",
     "Ochi"
    ],
    [
     "Koji",
     "Inoue"
    ],
    [
     "Divesh",
     "Lala"
    ],
    [
     "Tatsuya",
     "Kawahara"
    ]
   ],
   "title": "Entrainment Analysis and Prosody Prediction of Subsequent Interlocutor’s Backchannels in Dialogue",
   "original": "628",
   "order": 94,
   "page_count": 5,
   "abstract": [
    "This study investigates the characteristics of backchannels showing the entrainment to the interlocutor’s speech. The prosodic features of the dialogues of attentive listening are analyzed to describe how the prosody of Japanese backchannels is affected by the preceding interlocutor’s utterance. We adopt a support vector regression (SVR) to model the relationships between the prosodic features of backchannels and those of the preceding utterances. As a result, we found an interrelationship between the different types of features; in particular, the F0 of backchannels is highly correlated with the power of the preceding utterance. The regression analyses show that the combination of prosodic features of the preceding utterances achieves good prediction of both the F0 and power of backchannels. The findings of this study can be applied to the automatic generation of backchannels for spoken dialogue systems to show empathy and facilitate user’s speech."
   ],
   "p1": 462,
   "pn": 466,
   "doi": "10.21437/Interspeech.2024-628",
   "url": "interspeech_2024/ochi24_interspeech.html"
  },
  "gupta24b_interspeech": {
   "authors": [
    [
     "Shubham",
     "Gupta"
    ],
    [
     "Mirco",
     "Ravanelli"
    ],
    [
     "Pascal",
     "Germain"
    ],
    [
     "Cem",
     "Subakan"
    ]
   ],
   "title": "Phoneme Discretized Saliency Maps for Explainable Detection of AI-Generated Voice",
   "original": "632",
   "order": 675,
   "page_count": 5,
   "abstract": [
    "In this paper, we propose Phoneme Discretized Saliency Maps (PDSM), a discretization algorithm for saliency maps that takes advantage of phoneme boundaries for explainable detection of AI-generated voice. We experimentally show with two different Text-to-Speech systems (i.e., Tacotron2 and Fastspeech2) that the proposed algorithm produces saliency maps that result in more faithful explanations compared to standard posthoc explanation methods. Moreover, by associating the saliency maps to the phoneme representations, this methodology generates explanations that tend to be more understandable than standard saliency maps on magnitude spectrograms."
   ],
   "p1": 3295,
   "pn": 3299,
   "doi": "10.21437/Interspeech.2024-632",
   "url": "interspeech_2024/gupta24b_interspeech.html"
  },
  "ando24_interspeech": {
   "authors": [
    [
     "Atsushi",
     "Ando"
    ],
    [
     "Takafumi",
     "Moriya"
    ],
    [
     "Shota",
     "Horiguchi"
    ],
    [
     "Ryo",
     "Masumura"
    ]
   ],
   "title": "Factor-Conditioned Speaking-Style Captioning",
   "original": "633",
   "order": 158,
   "page_count": 5,
   "abstract": [
    "This paper presents a novel speaking-style captioning method that generates diverse descriptions while accurately predicting speaking-style information. Conventional learning criteria directly use original captions that contain not only speaking-style factor terms but also syntax words, which disturbs learning speaking-style information. To solve this problem, we introduce factor-conditioned captioning (FCC), which first outputs a phrase representing speaking-style factors (e.g., gender, pitch, etc.), and then generates a caption to ensure the model explicitly learns speaking-style factors. We also propose greedy-then-sampling (GtS) decoding, which first predicts speaking-style factors deterministically to guarantee semantic accuracy, and then generates a caption based on factor-conditioned sampling to ensure diversity. Experiments show that FCC outperforms the original caption-based training, and with GtS, it generates more diverse captions while keeping style prediction performance."
   ],
   "p1": 782,
   "pn": 786,
   "doi": "10.21437/Interspeech.2024-633",
   "url": "interspeech_2024/ando24_interspeech.html"
  },
  "liu24c_interspeech": {
   "authors": [
    [
     "Zhe",
     "Liu"
    ],
    [
     "Suyoun",
     "Kim"
    ],
    [
     "Ozlem",
     "Kalinli"
    ]
   ],
   "title": "Evaluating Speech Recognition Performance Towards Large Language Model Based Voice Assistants",
   "original": "635",
   "order": 843,
   "page_count": 5,
   "abstract": [
    "In recent years, there has been a rise in the popularity of large language model (LLM) based voice assistants. A practical question being raised in the evaluation of cascaded automatic speech recognition (ASR) systems in LLM-powered voice assistants is how to determine whether any errors in ASR transcriptions will result in task failures for the downstream assistants. Thus, measuring ASR systems that can reflect voice assistants' perception and judgement becomes increasingly important. In this paper, we propose novel evaluation metrics by leveraging the same assistant LLM to project ASR hypotheses into a vector space and compute their semantic distances with respect to the references. We perform experiments on a curated OpenAssistant test set and demonstrate that our presented methods with semantic embeddings calculated from LLMs are superior to conventional metrics on evaluating ASR performance towards LLM based voice assistants."
   ],
   "p1": 4099,
   "pn": 4103,
   "doi": "10.21437/Interspeech.2024-635",
   "url": "interspeech_2024/liu24c_interspeech.html"
  },
  "paissan24b_interspeech": {
   "authors": [
    [
     "Francesco",
     "Paissan"
    ],
    [
     "Luca",
     "Della Libera"
    ],
    [
     "Zhepei",
     "Wang"
    ],
    [
     "Paris",
     "Smaragdis"
    ],
    [
     "Mirco",
     "Ravanelli"
    ],
    [
     "Cem",
     "Subakan"
    ]
   ],
   "title": "Audio Editing with Non-Rigid Text Prompts",
   "original": "636",
   "order": 674,
   "page_count": 5,
   "abstract": [
    "In this paper, we explore audio editing with non-rigid text prompts via Latent Diffusion Models. Our methodology is based on carrying out a fine-tuning step on the latent diffusion model, which increases the overall faithfulness of the generated edits to the input audio. We quantitatively and qualitatively show that our pipeline obtains results which outperform current state-of-the-art neural audio editing pipelines for addition, style transfer, and inpainting. Through a user study, we show that our method results in higher user preference compared to several baselines. We also show that the produced edits obtain better trade-offs in terms of fidelity to the text prompt and to the input audio compared to the baselines. Finally, we benchmark the impact of LoRA to improve editing speed while maintaining edits quality."
   ],
   "p1": 3290,
   "pn": 3294,
   "doi": "10.21437/Interspeech.2024-636",
   "url": "interspeech_2024/paissan24b_interspeech.html"
  },
  "wang24o_interspeech": {
   "authors": [
    [
     "Shilin",
     "Wang"
    ],
    [
     "Haixin",
     "Guan"
    ],
    [
     "Yanhua",
     "Long"
    ]
   ],
   "title": "QMixCAT: Unsupervised Speech Enhancement Using Quality-guided Signal Mixing and Competitive Alternating Model Training",
   "original": "639",
   "order": 130,
   "page_count": 5,
   "abstract": [
    "Most deep learning-based speech enhancement (SE) models are supervised, requiring pairs of mixture and clean speech for training. This poses great challenges for real-world SE applications. Addressing this limitation is very crucial. In this paper, we introduce QMixCAT, an innovative unsupervised SE approach that enables unsupervised mixtures to be trained in a supervised manner within a teacher-student framework. Specifically, we propose a quality-guided signal mixing (QMix) approach to generate noisy-mixture-based supervised training data. Then, the model is trained using these data in a teacher-student framework, iteratively incorporating the QMix process during each epoch. In addition, a competitive alternating model training (CAT) mechanism is further proposed to enhance the quality of both teacher and student models. Experimental results demonstrate that QMixCAT significantly outperforms the strong baselines across multiple evaluation metrics."
   ],
   "p1": 642,
   "pn": 646,
   "doi": "10.21437/Interspeech.2024-639",
   "url": "interspeech_2024/wang24o_interspeech.html"
  },
  "wang24p_interspeech": {
   "authors": [
    [
     "Shuai",
     "Wang"
    ],
    [
     "Dehao",
     "Zhang"
    ],
    [
     "Kexin",
     "Shi"
    ],
    [
     "Yuchen",
     "Wang"
    ],
    [
     "Wenjie",
     "Wei"
    ],
    [
     "Jibin",
     "Wu"
    ],
    [
     "Malu",
     "Zhang"
    ]
   ],
   "title": "Global-Local Convolution with Spiking Neural Networks for Energy-efficient Keyword Spotting",
   "original": "642",
   "order": 929,
   "page_count": 5,
   "abstract": [
    "Thanks to Deep Neural Networks (DNNs), the accuracy of Keyword Spotting (KWS) has made substantial progress. However, as KWS systems are usually implemented on edge devices, energy efficiency becomes a critical requirement besides performance. Here, we take advantage of spiking neural networks' energy efficiency and propose an end-to-end lightweight KWS model. The model consists of two innovative modules: 1) Global-Local Spiking Convolution (GLSC) module and 2) Bottleneck-PLIF module. Compared to the hand-crafted feature extraction methods, the GLSC module achieves speech feature extraction that is sparser, more energy-efficient, and yields better performance. The Bottleneck-PLIF module further processes the signals from GLSC with the aim to achieve higher accuracy with fewer parameters. Extensive experiments are conducted on the Google Speech Commands Dataset (V1 and V2). The results show our method achieves competitive performance among SNN-based KWS models with fewer parameters."
   ],
   "p1": 4523,
   "pn": 4527,
   "doi": "10.21437/Interspeech.2024-642",
   "url": "interspeech_2024/wang24p_interspeech.html"
  },
  "kong24_interspeech": {
   "authors": [
    [
     "Yuexuan",
     "Kong"
    ],
    [
     "Viet-Anh",
     "Tran"
    ],
    [
     "Romain",
     "Hennequin"
    ]
   ],
   "title": "STraDa: A Singer Traits Dataset",
   "original": "644",
   "resource": "https://doi.org/10.5281/zenodo.10057434",
   "order": 283,
   "page_count": 5,
   "abstract": [
    "There is a limited amount of large-scale public datasets that contain downloadable music audio files and rich lead singer metadata. To provide such a dataset to benefit research in singing voices, we created Singer Traits Dataset (STraDa) with two subsets: automatic-strada and annotated-strada. The automatic-strada contains twenty-five thousand tracks across numerous genres and languages of more than five thousand unique lead singers, which includes cross-validated lead singer metadata as well as other track metadata. The annotated-strada consists of two hundred tracks that are balanced in terms of 2 genders, 5 languages, and 4 age groups. To show its use for model training and bias analysis thanks to its metadata's richness and downloadable audio files, we benchmarked singer sex classification (SSC) and conducted bias analysis."
   ],
   "p1": 1370,
   "pn": 1374,
   "doi": "10.21437/Interspeech.2024-644",
   "url": "interspeech_2024/kong24_interspeech.html"
  },
  "ahn24b_interspeech": {
   "authors": [
    [
     "Junseok",
     "Ahn"
    ],
    [
     "Youkyum",
     "Kim"
    ],
    [
     "Yeunju",
     "Choi"
    ],
    [
     "Doyeop",
     "Kwak"
    ],
    [
     "Ji-Hoon",
     "Kim"
    ],
    [
     "Seongkyu",
     "Mun"
    ],
    [
     "Joon Son",
     "Chung"
    ]
   ],
   "title": "VoxSim: A perceptual voice similarity dataset",
   "original": "646",
   "order": 532,
   "page_count": 5,
   "abstract": [
    "This paper introduces VoxSim, a dataset of perceptual voice similarity ratings. Recent efforts to automate the assessment of speech synthesis technologies have primarily focused on predicting mean opinion score of naturalness, leaving speaker voice similarity relatively unexplored due to a lack of extensive training data. To address this, we generate about 41k utterance pairs from the VoxCeleb dataset, a widely utilised speech dataset for speaker recognition, and collect nearly 70k speaker similarity scores through a listening test. VoxSim offers a valuable resource for the development and benchmarking of speaker similarity prediction models. We provide baseline results of speaker similarity prediction models on the VoxSim test set and further demonstrate that the model trained on our dataset gener-alises to the out-of-domain VCC2018 dataset."
   ],
   "p1": 2580,
   "pn": 2584,
   "doi": "10.21437/Interspeech.2024-646",
   "url": "interspeech_2024/ahn24b_interspeech.html"
  },
  "mai24_interspeech": {
   "authors": [
    [
     "Jialong",
     "Mai"
    ],
    [
     "Xiaofen",
     "Xing"
    ],
    [
     "Weidong",
     "Chen"
    ],
    [
     "Xiangmin",
     "Xu"
    ]
   ],
   "title": "DropFormer: A Dynamic Noise-Dropping Transformer for Speech Emotion Recognition",
   "original": "651",
   "order": 545,
   "page_count": 5,
   "abstract": [
    "Speech Emotion Recognition (SER) is an important component for human-computer interaction. Recently, various optimized Transformer variants have been applied to SER. However, most of studies use all the information in the sample and tend to overlook local details, making it difficult to perceive emotional information that is present locally in speech. While there are studies exploring how to utilize local information, their approaches are not flexible enough or are overly complex. To address the issues, we propose DropFormer, a new architecture that focuses only on the emotional segments by dynamically dropping non-emotional information. DropFormer consists of two main components: (1) Drop Attention, proficient in capturing local emotion and highlighting emotion-related segments, (2) Token Dropout Module, capable of dropping tokens lacking emotional information. Experimental results show that our DropFormer achieves state-of-the-art performance on the IEMOCAP and MELD benchmarks."
   ],
   "p1": 2645,
   "pn": 2649,
   "doi": "10.21437/Interspeech.2024-651",
   "url": "interspeech_2024/mai24_interspeech.html"
  },
  "wang24q_interspeech": {
   "authors": [
    [
     "Peng",
     "Wang"
    ],
    [
     "Yifan",
     "Yang"
    ],
    [
     "Zheng",
     "Liang"
    ],
    [
     "Tian",
     "Tan"
    ],
    [
     "Shiliang",
     "Zhang"
    ],
    [
     "Xie",
     "Chen"
    ]
   ],
   "title": "Incorporating Class-based Language Model for Named Entity Recognition in Factorized Neural Transducer",
   "original": "653",
   "order": 150,
   "page_count": 5,
   "abstract": [
    "Despite advancements of end-to-end (E2E) models in speech recognition, named entity recognition (NER) is still challenging but critical for semantic understanding. Previous studies mainly focus on various rule-based or attention-based contextual biasing algorithms. However, their performance might be sensitive to the biasing weight or degraded by excessive attention to the named entity list, along with a risk of false triggering. Inspired by the success of the class-based language model (LM) in NER in conventional hybrid systems and the effective decoupling of acoustic and linguistic information in the factorized neural Transducer (FNT), we propose C-FNT, a novel E2E model that incorporates class-based LMs into FNT. In C-FNT, the LM score of named entities can be associated with the name class instead of its surface form. The experimental results show that our proposed C-FNT significantly reduces error in named entities without hurting performance in general word recognition."
   ],
   "p1": 742,
   "pn": 746,
   "doi": "10.21437/Interspeech.2024-653",
   "url": "interspeech_2024/wang24q_interspeech.html"
  },
  "truong24b_interspeech": {
   "authors": [
    [
     "Duc-Tuan",
     "Truong"
    ],
    [
     "Ruijie",
     "Tao"
    ],
    [
     "Tuan",
     "Nguyen"
    ],
    [
     "Hieu-Thi",
     "Luong"
    ],
    [
     "Kong Aik",
     "Lee"
    ],
    [
     "Eng Siong",
     "Chng"
    ]
   ],
   "title": "Temporal-Channel Modeling in Multi-head Self-Attention for Synthetic Speech Detection",
   "original": "659",
   "order": 109,
   "page_count": 5,
   "abstract": [
    "Recent synthetic speech detectors leveraging the Transformer model have superior performance compared to the convolutional neural network counterparts. This improvement could be due to the powerful modeling ability of the multi-head self-attention (MHSA) in the Transformer model, which learns the temporal relationship of each input token. However, artifacts of synthetic speech can be located in specific regions of both frequency channels and temporal segments, while MHSA neglects this temporal-channel dependency of the input sequence. In this work, we proposed a Temporal-Channel Modeling (TCM) module to enhance MHSA’s capability for capturing temporal-channel dependencies. Experimental results on the ASVspoof 2021 show that with only 0.03M additional parameters, the TCM module can outperform the state-of-the-art system by 9.25% in EER. Further ablation study reveals that utilizing both temporal and channel information yields the most improvement for detecting synthetic speech."
   ],
   "p1": 537,
   "pn": 541,
   "doi": "10.21437/Interspeech.2024-659",
   "url": "interspeech_2024/truong24b_interspeech.html"
  },
  "zhao24_interspeech": {
   "authors": [
    [
     "Fei",
     "Zhao"
    ],
    [
     "Chenggang",
     "Zhang"
    ],
    [
     "Shulin",
     "He"
    ],
    [
     "Jinjiang",
     "Liu"
    ],
    [
     "Xueliang",
     "Zhang"
    ]
   ],
   "title": "Deep Echo Path Modeling for Acoustic Echo Cancellation",
   "original": "662",
   "resource": "https://doi.org/10.5281/zenodo.12734097",
   "order": 124,
   "page_count": 5,
   "abstract": [
    "Acoustic echo cancellation (AEC) is a key audio processing technology that removes echoes from microphone inputs to enable natural-sounding full-duplex communication. In recent years, deep learning has shown great potential for advancing AEC. However, deep learning methods face challenges in generalizing to complex environments, especially unseen conditions not represented in training. In this paper, we propose a deep learning-based method to predict the echo path in the time-frequency domain. Specifically, we first estimate the echo path under single-talk scenario without near-end signal and then utilize these predicted echo paths as auxiliary labels to train the model on double-talk scenario with near-end signal. Experimental results show that our method outperforms the strong baselines and exhibits good generalization capabilities for unseen acoustic scenarios. By estimating the echo path using deep learning, this work advances AEC performance in the presence of complex conditions. "
   ],
   "p1": 612,
   "pn": 616,
   "doi": "10.21437/Interspeech.2024-662",
   "url": "interspeech_2024/zhao24_interspeech.html"
  },
  "huang24e_interspeech": {
   "authors": [
    [
     "Jen-Hung",
     "Huang"
    ],
    [
     "Wei-Tsung",
     "Lee"
    ],
    [
     "Chung-Hsien",
     "Wu"
    ]
   ],
   "title": "USD-AC: Unsupervised Speech Disentanglement for Accent Conversion",
   "original": "664",
   "order": 902,
   "page_count": 5,
   "abstract": [
    "This study proposes USD-AC, an innovative Unsupervised Speech Disentanglement Accent Conversion that does not require parallel data and text transcription for training, solving challenges such as limited labeled data and generalizability issues. USD-AC, grounded in speech decomposition, aims to separate accent features from linguistic content, enhancing its adaptability across various accent conversion tasks. It utilizes a pre-trained ASR model to extract linguistic content and incorporates accent embedding for accent representation. Adversarial training effectively disentangles accent information from other attributes, boosting conversion performance. USD-AC achieves remarkable outcomes for known speakers and accents and exhibits exceptional generalization to unseen speakers, accents, and content. Through experimental comparison, USDAC based on unsupervised learning has shown superiority and generalization ability compared to supervised learning methods."
   ],
   "p1": 4388,
   "pn": 4392,
   "doi": "10.21437/Interspeech.2024-664",
   "url": "interspeech_2024/huang24e_interspeech.html"
  },
  "lin24e_interspeech": {
   "authors": [
    [
     "Yuqin",
     "Lin"
    ],
    [
     "Longbiao",
     "Wang"
    ],
    [
     "Jianwu",
     "Dang"
    ],
    [
     "Nobuaki",
     "Minematsu"
    ]
   ],
   "title": "Exploring Pre-trained Speech Model for Articulatory Feature Extraction in Dysarthric Speech Using ASR",
   "original": "665",
   "order": 944,
   "page_count": 5,
   "abstract": [
    "Most speech technologies are beneficial for normal speakers, but less effective for speakers with dysphonia. Dysarthria is a motor speech disorder, involving some impairments in the process of speech production. Therefore, articulatory information is important for speech techniques for this special group.However, articulatory features are difficult to extract due to challenges in annotating articulation. Recent studies explored phonemic features in Wav2vec 2.0 pretrained speech models and found they carries some articulatory-related information. Based on this investigation, this paper proposes DS-AAFE to extract more accurate articulatory features from the pretrained speech model based phonemic features. In DS-AAFE, partial articulatory features are isolated from phonemic features by being jointly optimized with ASR. Articulatory attribute detection is employed to evaluate the articulatory information in the proposed features, demonstrating a notable enhancement in the accuracy of articulatory attribute detection. Furthermore, experiments on the UASpeech and TORGO dysarthria datasets showed that the proposed features improved the ASR performance for dysarthric speech."
   ],
   "p1": 4598,
   "pn": 4602,
   "doi": "10.21437/Interspeech.2024-665",
   "url": "interspeech_2024/lin24e_interspeech.html"
  },
  "hu24_interspeech": {
   "authors": [
    [
     "Yiying",
     "Hu"
    ],
    [
     "Hui",
     "Feng"
    ]
   ],
   "title": "Key Acoustic Cues for the Realization of Metrical Prominence in Tone Languages: A Cross-Dialect Study",
   "original": "666",
   "order": 642,
   "page_count": 5,
   "abstract": [
    "Few studies have quantitatively discussed the acoustic realization of metrical prominence across tone languages. This study addresses this issue in word-level prosodic units, aiming to identify effective acoustic cues for cross-language analysis based on the realization of metrical prominence. Findings include: (1) Within-dialect analyses reveal robust acoustic cues for realizing metrically strong units: duration, pitch height, and pitch slope for both left- and right-dominant prominence, and intensity, pitch range and cumulative pitch dynamics, an often-overlooked cue, for right-dominant one; (2) Cross-dialect analyses by Boruta algorithm identify that all these six acoustic cues are significant for the classification of Chinese dialects according to their realization of metrical prominence. Linear Discriminant Analysis emphasizes duration, intensity, and cumulative pitch dynamics as key discriminators across dialects. "
   ],
   "p1": 3130,
   "pn": 3134,
   "doi": "10.21437/Interspeech.2024-666",
   "url": "interspeech_2024/hu24_interspeech.html"
  },
  "harkonen24_interspeech": {
   "authors": [
    [
     "Marc",
     "Härkönen"
    ],
    [
     "Samuel J.",
     "Broughton"
    ],
    [
     "Lahiru",
     "Samarakoon"
    ]
   ],
   "title": "EEND-M2F: Masked-attention mask transformers for speaker diarization",
   "original": "668",
   "order": 9,
   "page_count": 5,
   "abstract": [
    "In this paper, we make the explicit connection between image segmentation methods and end-to-end diarization methods. From these insights, we propose a novel, fully end-to-end diarization model, EEND-M2F, based on the Mask2Former architecture. Speaker representations are computed in parallel using a stack of transformer decoders, in which irrelevant frames are explicitly masked from the cross attention using predictions from previous layers. EEND-M2F is efficient, and truly end-to-end, eliminating the need for additional segmentation models or clustering algorithms. Our model achieves state-of-the-art performance on several public datasets, such as AMI, AliMeeting and RAMC. Most notably our DER of 16.07% on DIHARD-III is the first major improvement upon the challenge winning system."
   ],
   "p1": 37,
   "pn": 41,
   "doi": "10.21437/Interspeech.2024-668",
   "url": "interspeech_2024/harkonen24_interspeech.html"
  },
  "lee24g_interspeech": {
   "authors": [
    [
     "Dongheon",
     "Lee"
    ],
    [
     "Jung-Woo",
     "Choi"
    ]
   ],
   "title": "DeFTAN-AA: Array Geometry Agnostic Multichannel Speech Enhancement",
   "original": "669",
   "order": 688,
   "page_count": 5,
   "abstract": [
    "We propose an array geometry agnostic multichannel speech enhancement model, which is trained on a single microphone array but can enhance speech in various arrays with different shapes and numbers of microphones. To enable array-agnostic processing, the model employs a gated split dense block (GSDB) that separates foreground speech and background noise regardless of array geometry. Furthermore, to design an array-agnostic encoder compatible with different numbers of microphones, we introduce the spatial transformer (ST) that aggregates spatial information by channel-wise self-attention. The proposed space-object cross-attention (SOCA) block alleviates overfitting to a specific array configuration through cross-attention between spatial features and object features. Experimental results demonstrate the efficacy of the proposed model across various array geometries in both simulated and real-world datasets."
   ],
   "p1": 3360,
   "pn": 3364,
   "doi": "10.21437/Interspeech.2024-669",
   "url": "interspeech_2024/lee24g_interspeech.html"
  },
  "shah24_interspeech": {
   "authors": [
    [
     "Neil",
     "Shah"
    ],
    [
     "Shirish",
     "Karande"
    ],
    [
     "Vineet",
     "Gandhi"
    ]
   ],
   "title": "Towards Improving NAM-to-Speech Synthesis Intelligibility using Self-Supervised Speech Models",
   "original": "672",
   "order": 509,
   "page_count": 5,
   "abstract": [
    "We propose a novel approach to significantly improve the intelligibility in the Non-Audible Murmur (NAM)-to-speech conversion task, leveraging self-supervision and sequence-to-sequence (Seq2Seq) learning techniques. Unlike conventional methods that explicitly record ground-truth speech, our methodology relies on self-supervision and speech-to-speech synthesis to simulate ground-truth speech. Despite utilizing simulated speech, our method surpasses the current state-of-the-art (SOTA) by 29.08% improvement in the Mel-Cepstral Distortion (MCD) metric. Additionally, we present error rates and demonstrate our model’s proficiency to synthesize speech in novel voices of interest. Moreover, we present a methodology for augmenting the existing CSTR NAM TIMIT Plus corpus, setting a benchmark with a Word Error Rate (WER) of 42.57% to gauge the intelligibility of the synthesized speech. Speech samples can be found at https://nam2speech.github.io/NAM2Speech/"
   ],
   "p1": 2470,
   "pn": 2474,
   "doi": "10.21437/Interspeech.2024-672",
   "url": "interspeech_2024/shah24_interspeech.html"
  },
  "liu24d_interspeech": {
   "authors": [
    [
     "Pin-Yen",
     "Liu"
    ],
    [
     "Jen-Tzung",
     "Chien"
    ]
   ],
   "title": "Modality Translation Learning for Joint Speech-Text Model",
   "original": "675",
   "order": 156,
   "page_count": 5,
   "abstract": [
    "Recent research on speech models, which are jointly pre-trained with text, has unveiled its promising potential to enhance speech representations by encoding both speech and text within a shared space. However, these models often struggle with the interference between speech and text modalities that hardly achieves cross-modality alignment. Furthermore, the previous focus of evaluation for these models has been on neutral speech scenarios. Their effectiveness in addressing domain-shift speech, notably in the context of emotional speech, has remained largely unexplored in the existing works. In this study, a modality translation model is proposed to align speech and text modalities based on a shared space for speech-to-text translation, and aims to harness such a shared representation to address the challenge of emotional speech recognition. Experiment results show that the proposed method achieves about 3% absolute improvement in word error rate when compared with speech models."
   ],
   "p1": 772,
   "pn": 776,
   "doi": "10.21437/Interspeech.2024-675",
   "url": "interspeech_2024/liu24d_interspeech.html"
  },
  "kim24i_interspeech": {
   "authors": [
    [
     "Semin",
     "Kim"
    ],
    [
     "Myeonghun",
     "Jeong"
    ],
    [
     "Hyeonseung",
     "Lee"
    ],
    [
     "Minchan",
     "Kim"
    ],
    [
     "Byoung Jin",
     "Choi"
    ],
    [
     "Nam Soo",
     "Kim"
    ]
   ],
   "title": "MakeSinger: A Semi-Supervised Training Method for Data-Efficient Singing Voice Synthesis via Classifier-free Diffusion Guidance",
   "original": "678",
   "order": 382,
   "page_count": 5,
   "abstract": [
    "In this paper, we propose MakeSinger, a semi-supervised training method for singing voice synthesis (SVS) via classifierfree diffusion guidance. The challenge in SVS lies in the costly process of gathering aligned sets of text, pitch, and audio data. MakeSinger enables the training of the diffusion-based SVS model from any speech and singing voice data regardless of its labeling, thereby enhancing the quality of generated voices with large amount of unlabeled data. At inference, our novel dual guiding mechanism gives text and pitch guidance on the reverse diffusion step by estimating the score of masked input. Experimental results show that the model trained in a semisupervised manner outperforms other baselines trained only onthe labeled data in terms of pronunciation, pitch accuracy and overall quality. Furthermore, we demonstrate that by adding Text-to-Speech (TTS) data in training, the model can synthesize the singing voices of TTS speakers even without their singing voices."
   ],
   "p1": 1865,
   "pn": 1869,
   "doi": "10.21437/Interspeech.2024-678",
   "url": "interspeech_2024/kim24i_interspeech.html"
  },
  "meng24b_interspeech": {
   "authors": [
    [
     "Hanyu",
     "Meng"
    ],
    [
     "Qiquan",
     "Zhang"
    ],
    [
     "Xiangyu",
     "Zhang"
    ],
    [
     "Vidhyasaharan",
     "Sethu"
    ],
    [
     "Eliathamby",
     "Ambikairajah"
    ]
   ],
   "title": "Binaural Selective Attention Model for Target Speaker Extraction",
   "original": "683",
   "order": 889,
   "page_count": 5,
   "abstract": [
    "The remarkable ability of humans to selectively focus on a target speaker in cocktail party scenarios is facilitated by binaural audio processing. In this paper, we present a binaural time-domain Target Speaker Extraction model based on the Filter-and-Sum Network (FaSNet). Inspired by human selective hearing, our proposed model introduces target speaker embedding into separators using a multi-head attention-based selective attention block. We also compared two binaural interaction approaches – the cosine similarity of time-domain signals and inter-channel correlation in learned spectral representations. Our experimental results show that our proposed model outperforms monaural configurations and state-of-the-art multichannel target speaker extraction models, achieving best-inclass performance with 18.52 dB SI-SDR, 19.12 dB SDR, and 3.05 PESQ scores under anechoic two-speaker test configurations."
   ],
   "p1": 4323,
   "pn": 4327,
   "doi": "10.21437/Interspeech.2024-683",
   "url": "interspeech_2024/meng24b_interspeech.html"
  },
  "ma24_interspeech": {
   "authors": [
    [
     "Liuxian",
     "Ma"
    ],
    [
     "Lin",
     "Shen"
    ],
    [
     "Ruobing",
     "Li"
    ],
    [
     "Haojie",
     "Zhang"
    ],
    [
     "Kun",
     "Qian"
    ],
    [
     "Bin",
     "Hu"
    ],
    [
     "Björn W.",
     "Schuller"
    ],
    [
     "Yoshiharu",
     "Yamamoto"
    ]
   ],
   "title": "E-ODN: An Emotion Open Deep Network for Generalised and Adaptive Speech Emotion Recognition",
   "original": "685",
   "order": 883,
   "page_count": 5,
   "abstract": [
    "Recognising the widest range of emotions possible is a major challenge in the task of Speech Emotion Recognition(SER), especially for complex and mixed emotions. However, due to the limited number of emotional types and uneven distribution of data within existing datasets, current SER models are typically trained and used in a narrow range of emotional types. In this paper, we propose the Emotion Open Deep Network(E-ODN) model to address this issue. Besides, we introduce a novel Open-Set Recognition method that maps sample emotional features into a three-dimensional emotional space. The method can infer unknown emotions and initialise new type weights, enabling the model to dynamically learn and infer emerging emotional types. The empirical results show that our recognition model outperforms the state-of-the-art(SOTA) models in dealing with multi-type unbalanced data, and it can also perform finer-grained emotion recognition."
   ],
   "p1": 4293,
   "pn": 4297,
   "doi": "10.21437/Interspeech.2024-685",
   "url": "interspeech_2024/ma24_interspeech.html"
  },
  "chen24j_interspeech": {
   "authors": [
    [
     "Hongyang",
     "Chen"
    ],
    [
     "Yuhong",
     "Yang"
    ],
    [
     "Zhongyuan",
     "Wang"
    ],
    [
     "Weiping",
     "Tu"
    ],
    [
     "Haojun",
     "Ai"
    ],
    [
     "Cedar",
     "Lin"
    ]
   ],
   "title": "Exploring Sentence Type Effects on the Lombard Effect and Intelligibility Enhancement: A Comparative Study of Natural and Grid Sentences",
   "original": "691",
   "order": 796,
   "page_count": 5,
   "abstract": [
    "This study explores how sentence types affect the Lombard effect and intelligibility enhancement, focusing on comparisons between natural and grid sentences. Using the Lombard Chinese-TIMIT (LCT) corpus and the Enhanced MAndarin Lombard Grid (EMALG) corpus, we analyze changes in phonetic and acoustic features across different noise levels. Our results show that grid sentences produce more pronounced Lombard effects than natural sentences. Then, we develop and test a normal-to-Lombard conversion model, trained separately on LCT and EMALG corpora. Through subjective and objective evaluations, natural sentences are superior in maintaining speech quality in intelligibility enhancement. In contrast, grid sentences could provide superior intelligibility due to the more pronounced Lombard effect. This study provides a valuable perspective on enhancing speech communication in noisy environments."
   ],
   "p1": 3864,
   "pn": 3868,
   "doi": "10.21437/Interspeech.2024-691",
   "url": "interspeech_2024/chen24j_interspeech.html"
  },
  "kawamura24_interspeech": {
   "authors": [
    [
     "Masaya",
     "Kawamura"
    ],
    [
     "Ryuichi",
     "Yamamoto"
    ],
    [
     "Yuma",
     "Shirahata"
    ],
    [
     "Takuya",
     "Hasumi"
    ],
    [
     "Kentaro",
     "Tachibana"
    ]
   ],
   "title": "LibriTTS-P: A Corpus with Speaking Style and Speaker Identity Prompts for Text-to-Speech and Style Captioning",
   "original": "692",
   "order": 379,
   "page_count": 5,
   "abstract": [
    "We introduce LibriTTS-P, a new corpus based on LibriTTS-R that includes utterance-level descriptions (i.e., prompts) of speaking style and speaker-level prompts of speaker characteristics. We employ a hybrid approach to construct prompt annotations: (1) manual annotations that capture human perceptions of speaker characteristics and (2) synthetic annotations on speaking style. Compared to existing English prompt datasets, our corpus provides more diverse prompt annotations for all speakers of LibriTTS-R. Experimental results for prompt-based controllable TTS demonstrate that the TTS model trained with LibriTTS-P achieves higher naturalness than the model using the conventional dataset. Furthermore, the results for style captioning tasks show that the model utilizing LibriTTS-P generates 2.5 times more accurate words than the model using a conventional dataset. Our corpus, LibriTTS-P, is available at https://github.com/line/LibriTTS-P."
   ],
   "p1": 1850,
   "pn": 1854,
   "doi": "10.21437/Interspeech.2024-692",
   "url": "interspeech_2024/kawamura24_interspeech.html"
  },
  "chen24k_interspeech": {
   "authors": [
    [
     "Yujie",
     "Chen"
    ],
    [
     "Jiangyan",
     "Yi"
    ],
    [
     "Jun",
     "Xue"
    ],
    [
     "Chenglong",
     "Wang"
    ],
    [
     "Xiaohui",
     "Zhang"
    ],
    [
     "Shunbo",
     "Dong"
    ],
    [
     "Siding",
     "Zeng"
    ],
    [
     "Jianhua",
     "Tao"
    ],
    [
     "Zhao",
     "Lv"
    ],
    [
     "Cunhang",
     "Fan"
    ]
   ],
   "title": "RawBMamba: End-to-End Bidirectional State Space Model for Audio Deepfake Detection",
   "original": "698",
   "resource": "https://doi.org/10.5281/zenodo.12743966",
   "order": 560,
   "page_count": 5,
   "abstract": [
    "Fake artefacts for discriminating between bonafide and fake audio can exist in both short-  and long-range segments. Therefore, combining local and global feature information can effectively discriminate between bonafide and fake audio. This paper proposes an end-to-end bidirectional state space model, named RawBMamba, to capture both short- and long-range discriminative information for audio deepfake detection. Specifically, we use sinc Layer and multiple convolutional layers to capture short-range features, and then design a bidirectional Mamba to address Mamba's unidirectional modelling problem and further capture long-range feature information. Moreover, we develop a bidirectional fusion module to integrate embeddings, enhancing audio context representation and combining short- and long-range information. The results show that our proposed RawBMamba achieves a 34.1% improvement over Rawformer on ASVspoof2021 LA dataset, and demonstrates competitive performance on other datasets. Codes will be released on https://github.com/cyjie429/RawBMamba."
   ],
   "p1": 2720,
   "pn": 2724,
   "doi": "10.21437/Interspeech.2024-698",
   "url": "interspeech_2024/chen24k_interspeech.html"
  },
  "xing24_interspeech": {
   "authors": [
    [
     "Xujiang",
     "Xing"
    ],
    [
     "Mingxing",
     "Xu"
    ],
    [
     "Thomas Fang",
     "Zheng"
    ]
   ],
   "title": "A Joint Noise Disentanglement and Adversarial Training Framework for Robust Speaker Verification",
   "original": "700",
   "order": 143,
   "page_count": 5,
   "abstract": [
    "Automatic Speaker Verification (ASV) suffers from performance degradation in noisy conditions. To address this issue, we propose a novel adversarial learning framework that incorporates noise-disentanglement to establish a noise-independent speaker invariant embedding space. Specifically, the disentanglement module includes two encoders for separating speaker related and irrelevant information, respectively. The reconstruction module serves as a regularization term to constrain the noise.  A feature-robust loss is also used to supervise the speaker encoder to learn noise-independent speaker embeddings without losing speaker information. In addition, adversarial training is introduced to discourage the speaker encoder from encoding acoustic condition information for achieving a speaker-invariant embedding space. Experiments on Voxceleb1 indicate that the proposed method improves the performance of the speaker verification system under both clean and noisy conditions."
   ],
   "p1": 707,
   "pn": 711,
   "doi": "10.21437/Interspeech.2024-700",
   "url": "interspeech_2024/xing24_interspeech.html"
  },
  "jung24b_interspeech": {
   "authors": [
    [
     "Chaeyoung",
     "Jung"
    ],
    [
     "Suyeon",
     "Lee"
    ],
    [
     "Ji-Hoon",
     "Kim"
    ],
    [
     "Joon Son",
     "Chung"
    ]
   ],
   "title": "FlowAVSE: Efficient Audio-Visual Speech Enhancement with Conditional Flow Matching",
   "original": "701",
   "order": 457,
   "page_count": 5,
   "abstract": [
    "This work proposes an efficient method to enhance the quality of corrupted speech signals by leveraging both acoustic and visual cues. While existing diffusion-based approaches have demonstrated remarkable quality, their applicability is limited by slow inference speeds and computational complexity. To address this issue, we present FlowAVSE which enhances the inference speed and reduces the number of learnable parameters without degrading the output quality. In particular, we employ a conditional flow matching algorithm that enables the generation of high-quality speech in a single sampling step. Moreover, we increase efficiency by optimizing the underlying U-net architecture of diffusion-based systems. Our experiments demonstrate that FlowAVSE achieves 22 times faster inference speed and reduces the model size by half while maintaining the output quality. The demo page is available at:  https://cyongong.github.io/FlowAVSE.github.io/."
   ],
   "p1": 2210,
   "pn": 2214,
   "doi": "10.21437/Interspeech.2024-701",
   "url": "interspeech_2024/jung24b_interspeech.html"
  },
  "kashiwagi24_interspeech": {
   "authors": [
    [
     "Yosuke",
     "Kashiwagi"
    ],
    [
     "Hayato",
     "Futami"
    ],
    [
     "Emiru",
     "Tsunoo"
    ],
    [
     "Siddhant",
     "Arora"
    ],
    [
     "Shinji",
     "Watanabe"
    ]
   ],
   "title": "Rapid Language Adaptation for Multilingual E2E Speech Recognition Using Encoder Prompting",
   "original": "702",
   "order": 596,
   "page_count": 5,
   "abstract": [
    "End-to-end multilingual speech recognition models handle multiple languages through a single model, often incorporating language identification to automatically detect the language of incoming speech. Since the common scenario is where the language is already known, these models can perform as language-specific by using language information as prompts, which is particularly beneficial for attention-based encoder-decoder architectures. However, the Connectionist Temporal Classification (CTC) approach, which enhances recognition via joint decoding and multi-task training, does not normally incorporate language prompts due to its conditionally independent output tokens. To overcome this, we introduce an encoder prompting technique within the self-conditioned CTC framework, enabling language-specific adaptation of the CTC model in a zero-shot manner. Our method has shown to significantly reduce errors by 28% on average and by 41% on low-resource languages."
   ],
   "p1": 2900,
   "pn": 2904,
   "doi": "10.21437/Interspeech.2024-702",
   "url": "interspeech_2024/kashiwagi24_interspeech.html"
  },
  "li24o_interspeech": {
   "authors": [
    [
     "Zhaoqing",
     "Li"
    ],
    [
     "Haoning",
     "Xu"
    ],
    [
     "Tianzi",
     "Wang"
    ],
    [
     "Shoukang",
     "Hu"
    ],
    [
     "Zengrui",
     "Jin"
    ],
    [
     "Shujie",
     "Hu"
    ],
    [
     "Jiajun",
     "Deng"
    ],
    [
     "Mingyu",
     "Cui"
    ],
    [
     "Mengzhe",
     "Geng"
    ],
    [
     "Xunying",
     "Liu"
    ]
   ],
   "title": "One-pass Multiple Conformer and Foundation Speech Systems Compression and Quantization Using An All-in-one Neural Model",
   "original": "703",
   "order": 925,
   "page_count": 5,
   "abstract": [
    "We propose a novel one-pass multiple ASR systems joint compression and quantization approach using an all-in-one neural model. A single compression cycle allows multiple nested systems with varying Encoder depths, widths, and quantization precision settings to be simultaneously constructed without the need to train and store individual target systems separately. Experiments consistently demonstrate the multiple ASR systems compressed in a single all-in-one model produced a word error rate (WER) comparable to, or lower by up to 1.01% absolute (6.98% relative) than individually trained systems of equal complexity. A 3.4x overall system compression and training time speed-up was achieved. Maximum model size compression ratios of 12.8x and 3.93x were obtained over the baseline Switchboard-300hr Conformer and LibriSpeech-100hr fine-tuned wav2vec2.0 models, respectively, incurring no statistically significant WER increase."
   ],
   "p1": 4503,
   "pn": 4507,
   "doi": "10.21437/Interspeech.2024-703",
   "url": "interspeech_2024/li24o_interspeech.html"
  },
  "tsunoo24_interspeech": {
   "authors": [
    [
     "Emiru",
     "Tsunoo"
    ],
    [
     "Hayato",
     "Futami"
    ],
    [
     "Yosuke",
     "Kashiwagi"
    ],
    [
     "Siddhant",
     "Arora"
    ],
    [
     "Shinji",
     "Watanabe"
    ]
   ],
   "title": "Decoder-only Architecture for Streaming End-to-end Speech Recognition",
   "original": "705",
   "order": 917,
   "page_count": 5,
   "abstract": [
    "Decoder-only language models (LMs) have been successfully adopted for speech-processing tasks including automatic speech recognition (ASR). The LMs have ample expressiveness and perform efficiently. This efficiency is a suitable characteristic for streaming applications of ASR. In this work, we propose to use a decoder-only architecture for blockwise streaming ASR. In our approach, speech features are compressed using CTC output and context embedding using blockwise speech subnetwork, and are sequentially provided as prompts to the decoder. The decoder estimates the output tokens promptly at each block. To this end, we also propose a novel training scheme using random-length prefix prompts to make the model robust to the truncated prompts caused by blockwise processing. An experimental comparison shows that our proposal decoder-only streaming ASR achieves 8% relative word error rate reduction in the LibriSpeech test-other set while being twice as fast as the baseline model."
   ],
   "p1": 4463,
   "pn": 4467,
   "doi": "10.21437/Interspeech.2024-705",
   "url": "interspeech_2024/tsunoo24_interspeech.html"
  },
  "jin24d_interspeech": {
   "authors": [
    [
     "Sichen",
     "Jin"
    ],
    [
     "Youngmoon",
     "Jung"
    ],
    [
     "Seungjin",
     "Lee"
    ],
    [
     "Jaeyoung",
     "Roh"
    ],
    [
     "Changwoo",
     "Han"
    ],
    [
     "Hoonyoung",
     "Cho"
    ]
   ],
   "title": "CTC-aligned Audio-Text Embedding for Streaming Open-vocabulary Keyword Spotting",
   "original": "706",
   "order": 68,
   "page_count": 5,
   "abstract": [
    "This paper introduces a novel approach for streaming open-vocabulary keyword spotting (KWS) with text-based keyword enrollment. For every input frame, the proposed method finds the optimal alignment ending at the frame using connectionist temporal classification (CTC) and aggregates the frame-level acoustic embedding (AE) to obtain higher-level (i.e., character, word, or phrase) AE that aligns with the text embedding (TE) of the target keyword text. After that, we calculate the similarity of the aggregated AE and the TE. To the best of our knowledge, this is the first attempt to dynamically align the audio and the keyword text on-the-fly to attain the joint audio-text embedding for KWS.  Despite operating in a streaming fashion, our approach achieves competitive performance on the LibriPhrase dataset compared to the non-streaming methods with a mere 155K model parameters and a decoding algorithm with time complexity O(U), where U is the length of the target keyword at inference time."
   ],
   "p1": 332,
   "pn": 336,
   "doi": "10.21437/Interspeech.2024-706",
   "url": "interspeech_2024/jin24d_interspeech.html"
  },
  "kurihara24_interspeech": {
   "authors": [
    [
     "Kiyoshi",
     "Kurihara"
    ],
    [
     "Masanori",
     "Sano"
    ]
   ],
   "title": "Enhancing Japanese Text-to-Speech Accuracy with a Novel Combination Transformer-BERT-based G2P: Integrating Pronunciation Dictionaries and Accent Sandhi",
   "original": "708",
   "order": 574,
   "page_count": 5,
   "abstract": [
    "This study proposes an approach of Japanese language grapheme-to-phoneme (G2P) conversion by combining the Transformer framework and Bidirectional Encoder Representations from Transformers (BERT) to utilize external datasets (e.g., dictionaries). Conventional transformer-based methods encounter limitations in referencing specific dictionary readings due to the inability of the model to directly modify its intermediate processes and weights. Therefore, this study employs a dual Transformer strategy to improve the accurate pronunciation of proper nouns, numerals, and counter words. The first Transformer method facilitates the application of external data, and the second Transformer employs BERT to predict accent sandhi. This combination of Transformer-based techniques with dictionary integration enables the accurate and arbitrary pronunciation of proper nouns, numerals, and counter words, which contribute to the ongoing development of text-to-speech technologies. "
   ],
   "p1": 2790,
   "pn": 2794,
   "doi": "10.21437/Interspeech.2024-708",
   "url": "interspeech_2024/kurihara24_interspeech.html"
  },
  "li24p_interspeech": {
   "authors": [
    [
     "Jiahao",
     "Li"
    ],
    [
     "Miao",
     "Liu"
    ],
    [
     "Shu",
     "Yang"
    ],
    [
     "Jing",
     "Wang"
    ],
    [
     "Xiang",
     "Xie"
    ]
   ],
   "title": "Motion Based Audio-Visual Segmentation",
   "original": "709",
   "order": 790,
   "page_count": 5,
   "abstract": [
    "Recently, a novel task called audio-visual segmentation (AVS) has emerged, focusing on pixel-wise segmentation of sounding objects in videos. This task is particularly challenging as it involves segmenting individual pixels based on objects in video frames accompanied by sound. We propose a Motion Based Audio-Visual Segmentation model, which incorporates optical flow maps with motion information into the AVS task for the first time. The Motion-Vision Attention Module (MVA) is proposed to facilitate the fusion of motion and visual features to exploit motion information. Additionally, the Cross-Modal Bilateral-Attention Module (CMBA) is introduced to integrate multimodal features through crossmodal attention. The proposed model is evaluated on two distinct datasets, S4 and MS3, the outperformance of which demonstrates its effectiveness and feasibility in addressing the AVS task."
   ],
   "p1": 3834,
   "pn": 3838,
   "doi": "10.21437/Interspeech.2024-709",
   "url": "interspeech_2024/li24p_interspeech.html"
  },
  "xu24b_interspeech": {
   "authors": [
    [
     "Le",
     "Xu"
    ],
    [
     "Jiangyan",
     "Yi"
    ],
    [
     "Tao",
     "Wang"
    ],
    [
     "Yong",
     "Ren"
    ],
    [
     "Rongxiu",
     "Zhong"
    ],
    [
     "Zhengqi",
     "Wen"
    ],
    [
     "Jianhua",
     "Tao"
    ]
   ],
   "title": "Residual Speaker Representation for One-Shot Voice Conversion",
   "original": "710",
   "order": 568,
   "page_count": 5,
   "abstract": [
    "Recently, there have been significant advancements in voice conversion, resulting in high-quality performance. However, there are still two critical challenges in this field. Firstly, current voice conversion methods have limited robustness when encountering unseen speakers. Secondly, they also have limited ability to control timbre representation. To address these challenges, this paper presents a novel approach that leverages tokens of multi-layer residual approximations to enhance robustness when dealing with unseen speakers, called the residual speaker module. Introducing multi-layer approximations facilitates the separation of information from the timbre, enabling effective control over timbre in voice conversion. The proposed method outperforms baselines in subjective and objective evaluations, demonstrating superior performance and increased robustness. Our demo page is publicly available."
   ],
   "p1": 2760,
   "pn": 2764,
   "doi": "10.21437/Interspeech.2024-710",
   "url": "interspeech_2024/xu24b_interspeech.html"
  },
  "futami24_interspeech": {
   "authors": [
    [
     "Hayato",
     "Futami"
    ],
    [
     "Siddhant",
     "Arora"
    ],
    [
     "Yosuke",
     "Kashiwagi"
    ],
    [
     "Emiru",
     "Tsunoo"
    ],
    [
     "Shinji",
     "Watanabe"
    ]
   ],
   "title": "Finding Task-specific Subnetworks in Multi-task Spoken Language Understanding Model",
   "original": "712",
   "order": 162,
   "page_count": 5,
   "abstract": [
    "Recently, multi-task spoken language understanding (SLU) models have emerged, designed to address various speech processing tasks. However, these models often rely on a large number of parameters. Also, they often encounter difficulties in adapting to new data for a specific task without experiencing catastrophic forgetting of previously trained tasks. In this study, we propose finding task-specific subnetworks within a multi-task SLU model via neural network pruning. In addition to model compression, we expect that the forgetting of previously trained tasks can be mitigated by updating only a task-specific subnetwork. We conduct experiments on top of the state-of-the-art multi-task SLU model ``UniverSLU'', trained for several tasks such as emotion recognition (ER), intent classification (IC), and automatic speech recognition (ASR). We show that pruned models were successful in adapting to additional ASR or IC data with minimal performance degradation on previously trained tasks."
   ],
   "p1": 802,
   "pn": 806,
   "doi": "10.21437/Interspeech.2024-712",
   "url": "interspeech_2024/futami24_interspeech.html"
  },
  "cai24_interspeech": {
   "authors": [
    [
     "Pengfei",
     "Cai"
    ],
    [
     "Yan",
     "Song"
    ],
    [
     "Kang",
     "Li"
    ],
    [
     "Haoyu",
     "Song"
    ],
    [
     "Ian",
     "McLoughlin"
    ]
   ],
   "title": "MAT-SED: A Masked Audio Transformer with Masked-Reconstruction Based Pre-training for Sound Event Detection",
   "original": "714",
   "order": 113,
   "page_count": 5,
   "abstract": [
    "Sound event detection (SED) methods that leverage a large pre-trained Transformer encoder network have shown promising performance in recent DCASE challenges. However, they still rely on an RNN-based context network to model temporal dependencies, largely due to the scarcity of labeled data. In this work, we propose a pure Transformer-based SED model with masked-reconstruction based pre-training, termed MAT-SED. Specifically, a Transformer with relative positional encoding is first designed as the context network, pre-trained by the masked-reconstruction task on all available target data in a self-supervised way. Both the encoder and the context network are jointly fine-tuned in a semi-supervised manner. Furthermore, a global-local feature fusion strategy is proposed to enhance the localization capability. Evaluation of MAT-SED on DCASE2023 task4 surpasses state-of-the-art performance, achieving 0.587/0.896 PSDS1/PSDS2 respectively."
   ],
   "p1": 557,
   "pn": 561,
   "doi": "10.21437/Interspeech.2024-714",
   "url": "interspeech_2024/cai24_interspeech.html"
  },
  "korotkova24_interspeech": {
   "authors": [
    [
     "Yuliya",
     "Korotkova"
    ],
    [
     "Ilya",
     "Kalinovskiy"
    ],
    [
     "Tatiana",
     "Vakhrusheva"
    ]
   ],
   "title": "Word-level Text Markup for Prosody Control in Speech Synthesis",
   "original": "715",
   "order": 471,
   "page_count": 5,
   "abstract": [
    "Modern Text-to-Speech (TTS) technologies generate speech very close to the natural one, but synthesized voices still lack variation in intonation which, in addition, is hard to control. In this work, we address the problem of prosody control, aiming to capture information about intonation in a markup without hand-labeling and linguistic expertise. We propose a method of encoding prosodic knowledge from textual and acoustic modalities, which are obtained with the help of models pretrained on self-supervised tasks, into latent quantized space with interpretable features. Based on these features, the prosodic markup is constructed, and it is used as an additional input to the TTS model to solve the one-to-many problem and is predicted by text. Moreover, this method allows for prosody control during inference time and scalability to new data and other languages."
   ],
   "p1": 2280,
   "pn": 2284,
   "doi": "10.21437/Interspeech.2024-715",
   "url": "interspeech_2024/korotkova24_interspeech.html"
  },
  "zezario24_interspeech": {
   "authors": [
    [
     "Ryandhimas E.",
     "Zezario"
    ],
    [
     "Fei",
     "Chen"
    ],
    [
     "Chiou-Shann",
     "Fuh"
    ],
    [
     "Hsin-Min",
     "Wang"
    ],
    [
     "Yu",
     "Tsao"
    ]
   ],
   "title": "Non-Intrusive Speech Intelligibility Prediction for Hearing Aids using Whisper and Metadata",
   "original": "716",
   "order": 792,
   "page_count": 5,
   "abstract": [
    "Automated speech intelligibility assessment is pivotal for hearing aid (HA) development. In this paper, we present three novel methods to improve intelligibility prediction accuracy and introduce MBI-Net+, an enhanced version of MBI-Net, the top-performing system in the 1st Clarity Prediction Challenge. MBI-Net+ leverages Whisper's embeddings to create cross-domain acoustic features and includes metadata from speech signals by using a classifier that distinguishes different enhancement methods. Furthermore, MBI-Net+ integrates the hearing-aid speech perception index (HASPI) as a supplementary metric into the objective function to further boost prediction performance. Experimental results demonstrate that MBI-Net+ surpasses several intrusive baseline systems and MBI-Net on the Clarity Prediction Challenge 2023 dataset, validating the effectiveness of incorporating Whisper embeddings, speech metadata, and related complementary metrics to improve prediction performance for HA."
   ],
   "p1": 3844,
   "pn": 3848,
   "doi": "10.21437/Interspeech.2024-716",
   "url": "interspeech_2024/zezario24_interspeech.html"
  },
  "xu24c_interspeech": {
   "authors": [
    [
     "Anfeng",
     "Xu"
    ],
    [
     "Kevin",
     "Huang"
    ],
    [
     "Tiantian",
     "Feng"
    ],
    [
     "Lue",
     "Shen"
    ],
    [
     "Helen",
     "Tager-Flusberg"
    ],
    [
     "Shrikanth",
     "Narayanan"
    ]
   ],
   "title": "Exploring Speech Foundation Models for Speaker Diarization in Child-Adult Dyadic Interactions",
   "original": "717",
   "order": 1063,
   "page_count": 5,
   "abstract": [
    "Speech foundation models, trained on vast datasets, have opened unique opportunities in addressing challenging low-resource speech understanding, such as child speech. In this work, we explore the capabilities of speech foundation models on child-adult speaker diarization. We show that exemplary foundation models can achieve 39.5% and 62.3% relative reductions in Diarization Error Rate and Speaker Confusion Rate, respectively, compared to previous speaker diarization methods. In addition, we benchmark and evaluate the speaker diarization results of the speech foundation models with varying the input audio window size, speaker demographics, and training data ratio. Our results highlight promising pathways for understanding and adopting speech foundation models to facilitate child speech understanding."
   ],
   "p1": 5193,
   "pn": 5197,
   "doi": "10.21437/Interspeech.2024-717",
   "url": "interspeech_2024/xu24c_interspeech.html"
  },
  "zusag24_interspeech": {
   "authors": [
    [
     "Mario",
     "Zusag"
    ],
    [
     "Laurin",
     "Wagner"
    ],
    [
     "Bernhad",
     "Thallinger"
    ]
   ],
   "title": "CrisperWhisper: Accurate Timestamps on Verbatim Speech Transcriptions",
   "original": "731",
   "order": 262,
   "page_count": 5,
   "abstract": [
    "We demonstrate that carefully adjusting the tokenizer of the Whisper speech recognition model significantly improves the precision of word-level timestamps when applying dynamic time warping to the decoder’s cross-attention scores. We fine- tune the model to produce more verbatim speech transcriptions and employ several techniques to increase robustness against multiple speakers and background noise. These adjustments achieve state-of-the-art performance on benchmarks for verba- tim speech transcription, word segmentation, and the timed de- tection of filler events, and can further mitigate transcription hallucinations. The code is available open source."
   ],
   "p1": 1265,
   "pn": 1269,
   "doi": "10.21437/Interspeech.2024-731",
   "url": "interspeech_2024/zusag24_interspeech.html"
  },
  "pineiromartin24_interspeech": {
   "authors": [
    [
     "Andrés",
     "Piñeiro-Martín"
    ],
    [
     "Carmen",
     "García-Mateo"
    ],
    [
     "Laura",
     "Docio-Fernandez"
    ],
    [
     "María del Carmen",
     "López-Pérez"
    ],
    [
     "Georg",
     "Rehm"
    ]
   ],
   "title": "Weighted Cross-entropy for Low-Resource Languages in Multilingual Speech Recognition",
   "original": "734",
   "order": 256,
   "page_count": 5,
   "abstract": [
    "This paper addresses the challenge of integrating low-resource languages into multilingual automatic speech recognition (ASR) systems. We introduce a novel application of weighted cross-entropy, typically used for unbalanced datasets, to facilitate the integration of low-resource languages into pre-trained multilingual ASR models within the context of continual multilingual learning. We fine-tune the Whisper multilingual ASR model on five high-resource languages and one low-resource language, employing language-weighted dynamic cross-entropy and data augmentation. The results show a remarkable 6.69% word error rate (WER) reduction for the low-resource language compared to the fine-tuned model without applying our approach, and a 48.86% WER reduction compared to the original Whisper model. In addition, our approach yields an average WER reduction of 3.29% across the six languages, showing no degradation for the high-resource languages."
   ],
   "p1": 1235,
   "pn": 1239,
   "doi": "10.21437/Interspeech.2024-734",
   "url": "interspeech_2024/pineiromartin24_interspeech.html"
  },
  "yan24b_interspeech": {
   "authors": [
    [
     "Yujie",
     "Yan"
    ],
    [
     "Xiran",
     "Xu"
    ],
    [
     "Haolin",
     "Zhu"
    ],
    [
     "Pei",
     "Tian"
    ],
    [
     "Zhongshu",
     "Ge"
    ],
    [
     "Xihong",
     "Wu"
    ],
    [
     "Jing",
     "Chen"
    ]
   ],
   "title": "Auditory Attention Decoding in Four-Talker Environment with EEG",
   "original": "739",
   "order": 88,
   "page_count": 5,
   "abstract": [
    "Auditory Attention Decoding (AAD) is a technique that determines the focus of a listener's attention in complex auditory scenes according to cortical neural responses. Existing research largely examines two-talker scenarios, insufficient for real-world complexity. This study introduced a new AAD database for a four-talker scenario with speeches from four distinct talkers simultaneously presented and spatially separated, and listeners' EEG was recorded. Temporal response functions (TRFs) analysis showed that attended speech TRFs are stronger than each unattended speech. AAD methods based on stimulus-reconstruction (SR) and cortical spatial lateralization were employed and compared. Results indicated decoding accuracy of 77.5% in 60s (chance level of 25%) using SR. Using auditory spatial attention detection (ASAD) methods also indicated high accuracy (94.7% with DenseNet-3D in 1s), demonstrating ASAD methods' generalization performance. "
   ],
   "p1": 432,
   "pn": 436,
   "doi": "10.21437/Interspeech.2024-739",
   "url": "interspeech_2024/yan24b_interspeech.html"
  },
  "fan24_interspeech": {
   "authors": [
    [
     "Cunhang",
     "Fan"
    ],
    [
     "Shunbo",
     "Dong"
    ],
    [
     "Jun",
     "Xue"
    ],
    [
     "Yujie",
     "Chen"
    ],
    [
     "Jiangyan",
     "Yi"
    ],
    [
     "Zhao",
     "Lv"
    ]
   ],
   "title": "Frequency-mix Knowledge Distillation for Fake Speech Detection",
   "original": "740",
   "order": 463,
   "page_count": 5,
   "abstract": [
    "In telephony scenarios, the fake speech detection (FSD) task to combat speech spoofing attacks is challenging. Data augmentation (DA) methods are considered effective means to address the FSD task in telephony scenarios, typically divided into time domain and frequency domain stages. While each has its advantages, both can result in information loss. To tackle this issue, we propose a novel DA method, Frequency-mix (Freqmix), and introduce the Freqmix knowledge distillation (FKD) to enhance model information extraction and generalization abilities. Specifically, we use Freqmix-enhanced data as input for the teacher model, while the student model's input undergoes time-domain DA method. We use a multi-level feature distillation approach to restore information and improve the model's generalization capabilities. Our approach achieves state-of-the-art results on ASVspoof 2021 LA dataset, showing a 31\\% improvement over baseline and performs competitively on ASVspoof 2021 DF dataset."
   ],
   "p1": 2240,
   "pn": 2244,
   "doi": "10.21437/Interspeech.2024-740",
   "url": "interspeech_2024/fan24_interspeech.html"
  },
  "chen24l_interspeech": {
   "authors": [
    [
     "Yafeng",
     "Chen"
    ],
    [
     "Siqi",
     "Zheng"
    ],
    [
     "Hui",
     "Wang"
    ],
    [
     "Luyao",
     "Cheng"
    ],
    [
     "Qian",
     "Chen"
    ],
    [
     "Shiliang",
     "Zhang"
    ],
    [
     "Junjie",
     "Li"
    ]
   ],
   "title": "ERes2NetV2: Boosting Short-Duration Speaker Verification Performance with Computational Efficiency",
   "original": "742",
   "resource": "https://doi.org/10.5281/zenodo.12789409",
   "order": 665,
   "page_count": 5,
   "abstract": [
    "Speaker verification systems experience significant performance degradation when tasked with short-duration trial recordings. To address this challenge, a multi-scale feature fusion approach has been proposed to effectively capture speaker characteristics from short utterances. Constrained by the model’s size, a robust backbone Enhanced Res2Net (ERes2Net) combining global and local feature fusion demonstrates sub-optimal performance in short-duration speaker verification. To further improve the short-duration feature extraction capability of ERes2Net, we expand the channel dimension within each stage. However, this modification also increases the number of model parameters and computational complexity. To alleviate this problem, we propose an improved ERes2NetV21 by pruning redundant structures, ultimately reducing both the model parameters and its computational cost. A range of experiments conducted on the VoxCeleb datasets exhibits the superiority of ERes2NetV2, which achieves EER of 0.61% for the full duration trial, 0.98% for the 3s-duration trial, and 1.48% for the 2s-duration trial on VoxCeleb1-O, respectively."
   ],
   "p1": 3245,
   "pn": 3249,
   "doi": "10.21437/Interspeech.2024-742",
   "url": "interspeech_2024/chen24l_interspeech.html"
  },
  "spiesberger24_interspeech": {
   "authors": [
    [
     "Anika A.",
     "Spiesberger"
    ],
    [
     "Andreas",
     "Triantafyllopoulos"
    ],
    [
     "Alexander",
     "Kathan"
    ],
    [
     "Anastasia",
     "Semertzidou"
    ],
    [
     "Caterina",
     "Gawrilow"
    ],
    [
     "Tilman",
     "Reinelt"
    ],
    [
     "Wolfgang A.",
     "Rauch"
    ],
    [
     "Björn",
     "Schuller"
    ]
   ],
   "title": "“So . . . my child . . . ” – How Child ADHD Influences the Way Parents Talk",
   "original": "744",
   "order": 411,
   "page_count": 5,
   "abstract": [
    "Attention-deficit/hyperactivity disorder (ADHD) exerts a psychological burden not only on affected individuals but also on their social support systems. Of particular interest are the parents, who often face challenges related to their child’s condition, including its impact on their own mental well-being. The interaction among the child’s symptomatology, parental mental health, and the parent-child relationship is a crucial area of investigation. Expressed Emotion (EE), as assessed through the Preschool Five Minute Speech Sample (PFMSS), serves as a valuable measure. However, manual annotation of EE can be cumbersome and impractical for continuous monitoring. To address this, we propose leveraging machine learning methods. This study presents an initial exploration into predicting children’s ADHD diagnosis using linguistic and paralinguistic features derived from the PFMSS. Despite achieving a UAR score of 67.1%, our results have not surpassed the performance of manually annotated EE."
   ],
   "p1": 2010,
   "pn": 2014,
   "doi": "10.21437/Interspeech.2024-744",
   "url": "interspeech_2024/spiesberger24_interspeech.html"
  },
  "zhou24c_interspeech": {
   "authors": [
    [
     "Lifeng",
     "Zhou"
    ],
    [
     "Yuke",
     "Li"
    ],
    [
     "Rui",
     "Deng"
    ],
    [
     "Yuting",
     "Yang"
    ],
    [
     "Haoqi",
     "Zhu"
    ]
   ],
   "title": "Cross-Modal Denoising: A Novel Training Paradigm for Enhancing Speech-Image Retrieval",
   "original": "745",
   "order": 836,
   "page_count": 5,
   "abstract": [
    "The success of speech-image retrieval relies on establishing an effective alignment between speech and image. Existing methods often model cross-modal interaction through simple cosine similarity of the global feature of each modality, which fall short in capturing fine-grained details within modalities. To address this issue, we introduce an effective framework and a novel learning task named cross-modal denoising(CMD) to enhance cross-modal interaction to achieve finerlevel cross-modal alignment. Specifically, CMD is a denoising task designed to reconstruct semantic features from noisy features within one modality by interacting features from another modality. Notably, CMD operates exclusively during model training and can be removed during inference without adding extra inference time. The experimental results demonstrate that our framework outperforms the state-of-the-art method by 2.0% in mean R@1 on the Flickr8k dataset and by 1.7% in mean R@1 on the SpokenCOCO dataset for the speech-image retrieval tasks, respectively. These experimental results validate the efficiency and effectiveness of our framework."
   ],
   "p1": 4064,
   "pn": 4068,
   "doi": "10.21437/Interspeech.2024-745",
   "url": "interspeech_2024/zhou24c_interspeech.html"
  },
  "svenssonlundmark24_interspeech": {
   "authors": [
    [
     "Malin",
     "Svensson Lundmark"
    ]
   ],
   "title": "Magnitude and timing of acceleration peaks in stressed and unstressed syllables",
   "original": "746",
   "order": 542,
   "page_count": 5,
   "abstract": [
    "Segment transitions can be accounted for by acceleration peaks, which appear at the edges of articulatory speech postures. The present study builds on previous research on timing of acceleration, and expands it by investigating magnitude of acceleration peaks at segment offset, comparing stressed and unstressed syllables. Acceleration peaks of lower lip and lower jaw are measured on ten Swedish speakers using EMA methodology. Results show strong correlation between acceleration and velocity peak magnitude, with overall more magnitude in stressed syllables, and highest acceleration peaks on lower lip. However, timing of the acceleration peaks, as measured from minimal velocity, is not affected by how fast the articulators are moving. The results reveal possible different functions of acceleration peak magnitude vs timing, also between the two articulators. The study stresses the significance of mapping acceleration peaks in speech for use in prosodic research."
   ],
   "p1": 2630,
   "pn": 2634,
   "doi": "10.21437/Interspeech.2024-746",
   "url": "interspeech_2024/svenssonlundmark24_interspeech.html"
  },
  "huang24f_interspeech": {
   "authors": [
    [
     "Ruizhe",
     "Huang"
    ],
    [
     "Mahsa",
     "Yarmohammadi"
    ],
    [
     "Sanjeev",
     "Khudanpur"
    ],
    [
     "Daniel",
     "Povey"
    ]
   ],
   "title": "Improving Neural Biasing for Contextual Speech Recognition by Early Context Injection and Text Perturbation",
   "original": "749",
   "order": 152,
   "page_count": 5,
   "abstract": [
    "Existing research suggests that automatic speech recognition(ASR) models can benefit from additional contexts (e.g., contact lists, user specified vocabulary). Rare words and named entities can be better recognized with contexts. In this work, we propose two simple yet effective techniques to improve context-aware ASR models. First, we inject contexts into the encoders at an early stage instead of merely at their last layers. Second, to enforce the model to leverage the contexts during training, we perturb the reference transcription with alternative spellings so that the model learns to rely on the contexts to make correct predictions. On LibriSpeech, our techniques together reduce the rare word error rate by 60% and 25% relatively compared to no biasing and shallow fusion, making the new state-of-the-art performance. On SPGISpeech and a real-world dataset ConEC, our techniques also yield good improvements over the baselines."
   ],
   "p1": 752,
   "pn": 756,
   "doi": "10.21437/Interspeech.2024-749",
   "url": "interspeech_2024/huang24f_interspeech.html"
  },
  "li24q_interspeech": {
   "authors": [
    [
     "Yue",
     "Li"
    ],
    [
     "Xinsheng",
     "Wang"
    ],
    [
     "Li",
     "Zhang"
    ],
    [
     "Lei",
     "Xie"
    ]
   ],
   "title": "SCDNet: Self-supervised Learning Feature based Speaker Change Detection",
   "original": "752",
   "order": 968,
   "page_count": 5,
   "abstract": [
    "Speaker Change Detection (SCD) is to identify boundaries among speakers in a conversation. Motivated by the success of fine-tuning wav2vec 2.0 models for the SCD task, a further investigation of self-supervised learning (SSL) features for SCD is conducted in this work. Specifically, an SCD model, named SCDNet, is proposed. With this model, various state-of-the-art SSL models, including Hubert, wav2vec 2.0, and WavLm are investigated. To discern the most potent layer of SSL models for SCD, a learnable weighting method is employed to analyze the effectiveness of intermediate representations. Additionally, a fine-tuning-based approach is also implemented to further compare the characteristics of SSL models in the SCD task. Furthermore, a contrastive learning method is proposed to mitigate the overfitting tendencies in the training of both the fine-tuning-based method and SCDNet. Experiments showcase the superiority of WavLm in the SCD task and also demonstrate the good design of SCDNet."
   ],
   "p1": 4718,
   "pn": 4722,
   "doi": "10.21437/Interspeech.2024-752",
   "url": "interspeech_2024/li24q_interspeech.html"
  },
  "lin24f_interspeech": {
   "authors": [
    [
     "Zijie",
     "Lin"
    ],
    [
     "Tianyu",
     "He"
    ],
    [
     "Siqi",
     "Cai"
    ],
    [
     "Haizhou",
     "Li"
    ]
   ],
   "title": "ASA: An Auditory Spatial Attention Dataset with Multiple Speaking Locations",
   "original": "753",
   "resource": "https://doi.org/10.5281/zenodo.11541114",
   "order": 89,
   "page_count": 5,
   "abstract": [
    "Recent studies have demonstrated the feasibility of localizing an attended sound source from electroencephalography (EEG) signals in a cocktail party scenario. This is referred to as EEG-enabled Auditory Spatial Attention Detection (ASAD). Despite the promise, there is a lack of ASAD datasets. Most existing ASAD datasets are recorded from two speaking locations. To bridge this gap, we introduce a new Auditory Spatial Attention (ASA) dataset, featuring multiple speaking locations of sound sources. The new dataset is designed to challenge and refine deep neural network solutions in real-world applications. Furthermore, we build a channel attention convolutional neural network (CA-CNN) as a reference model for ASA, that serves as a competitive benchmark for future studies."
   ],
   "p1": 437,
   "pn": 441,
   "doi": "10.21437/Interspeech.2024-753",
   "url": "interspeech_2024/lin24f_interspeech.html"
  },
  "chen24m_interspeech": {
   "authors": [
    [
     "Nan",
     "Chen"
    ],
    [
     "Yonghe",
     "Wang"
    ],
    [
     "Feilong",
     "Bao"
    ]
   ],
   "title": "Parameter-Efficient Adapter Based on Pre-trained Models for Speech Translation",
   "original": "759",
   "order": 73,
   "page_count": 5,
   "abstract": [
    "Multi-task learning (MTL) approach leverages pre-trained models in speech and machine translation and has significantly advanced speech-to-text translation tasks. However, it introduces a considerable number of parameters, leading to increasing training costs. Most parameter-efficient fine-tuning (PEFT) methods only train additional modules to effectively reduce the number of trainable parameters. Nevertheless, the increase in trainable parameters caused by the PEFT method remains non-negligible in multilingual speech translation settings. In this paper, we first propose the parameter-sharing adapter, which reduces parameters by 7/8 compared to regular adapters, with only approximately 0.7% performance decrease. For the balance between model parameter quantity and performance, we present a neural network search (NAS) based model. Experimental results revealed that the performance of adapter is closest to fine-tuning, while LoRA exhibits the poorest performance."
   ],
   "p1": 357,
   "pn": 361,
   "doi": "10.21437/Interspeech.2024-759",
   "url": "interspeech_2024/chen24m_interspeech.html"
  },
  "lun24_interspeech": {
   "authors": [
    [
     "Tin Mei",
     "Lun"
    ],
    [
     "Ekaterina",
     "Voskoboinik"
    ],
    [
     "Ragheb",
     "Al-Ghezi"
    ],
    [
     "Tamas",
     "Grosz"
    ],
    [
     "Mikko",
     "Kurimo"
    ]
   ],
   "title": "Oversampling, Augmentation and Curriculum Learning for Speaking Assessment with Limited Training Data",
   "original": "760",
   "resource": "https://doi.org/10.5281/zenodo.12750336",
   "order": 827,
   "page_count": 5,
   "abstract": [
    "Automated assessment systems for spontaneous speech are an increasingly important component in language proficiency tests and learning platforms. These systems have seen remarkable development in recent years, driven by advances in self-supervised learning. Nevertheless, in languages such as Finnish and Finland Swedish, their performance is still limited by the low-resource and imbalance nature of their data. To alleviate these issues, this work evaluates two data-level methods: oversampling and curriculum learning. Our results reveal that combining these methods results in the greatest boost to model performance, achieved without additional data or modification to the model structure."
   ],
   "p1": 4019,
   "pn": 4023,
   "doi": "10.21437/Interspeech.2024-760",
   "url": "interspeech_2024/lun24_interspeech.html"
  },
  "zhao24b_interspeech": {
   "authors": [
    [
     "Fei",
     "Zhao"
    ],
    [
     "Jinjiang",
     "Liu"
    ],
    [
     "Xueliang",
     "Zhang"
    ]
   ],
   "title": "SDAEC: Signal Decoupling for Advancing Acoustic Echo Cancellation",
   "original": "763",
   "resource": "https://doi.org/10.5281/zenodo.12734124",
   "order": 36,
   "page_count": 5,
   "abstract": [
    "In deep learning-based acoustic echo cancellation methods, neural networks implicitly learn echo paths to cancel echoes. However, under low signal-to-echo ratio conditions, the substantial energy discrepancy between the microphone signal and the reference signal impedes the network's ability, resulting in poor performance. In this study, we propose a Signal Decoupling-based monaural Acoustic Echo Cancellation method called SDAEC. Specifically, we model the energy of the reference signal and the microphone signal to obtain an energy scaling factor. The reference signal is then multiplied by this energy scaling factor before being fed into the subsequent echo cancellation network. This approach reduces the difficulty of the subsequent echo cancellation step, thereby improving the overall cancellation performance. Experimental results demonstrate that the proposed method enhances the performance of multiple baseline models."
   ],
   "p1": 172,
   "pn": 176,
   "doi": "10.21437/Interspeech.2024-763",
   "url": "interspeech_2024/zhao24b_interspeech.html"
  },
  "yin24_interspeech": {
   "authors": [
    [
     "YongKang",
     "Yin"
    ],
    [
     "Xu",
     "Li"
    ],
    [
     "Ying",
     "Shan"
    ],
    [
     "YueXian",
     "Zou"
    ]
   ],
   "title": "AFL-Net: Integrating Audio, Facial, and Lip Modalities with a Two-step Cross-attention for Robust Speaker Diarization in the Wild",
   "original": "764",
   "order": 10,
   "page_count": 5,
   "abstract": [
    "Speaker diarization in real-world videos presents significant challenges due to varying acoustic conditions, diverse scenes, the presence of off-screen speakers, etc. This paper builds upon a previous study (AVR-Net) and introduces a novel multi-modal speaker diarization system, AFL-Net. The proposed AFL-Net incorporates dynamic lip movement as an additional modality to enhance the identity distinction. Besides, unlike AVR-Net which extracts high-level representations from each modality independently, AFL-Net employs a two-step cross-attention mechanism to sufficiently fuse different modalities, resulting in more comprehensive information to enhance the performance. Moreover, we also incorporated a masking strategy during training, where the face and lip modalities are randomly obscured. This strategy enhances the impact of the audio modality on the system outputs. Experimental results demonstrate that AFL-Net outperforms state-of-the-art baselines, such as the AVR-Net and DyViSE."
   ],
   "p1": 42,
   "pn": 46,
   "doi": "10.21437/Interspeech.2024-764",
   "url": "interspeech_2024/yin24_interspeech.html"
  },
  "wan24_interspeech": {
   "authors": [
    [
     "Genshun",
     "Wan"
    ],
    [
     "Mengzhi",
     "Wang"
    ],
    [
     "Tingzhi",
     "Mao"
    ],
    [
     "Hang",
     "Chen"
    ],
    [
     "Zhongfu",
     "Ye"
    ]
   ],
   "title": "Lightweight Transducer Based on Frame-Level Criterion",
   "original": "768",
   "order": 51,
   "page_count": 5,
   "abstract": [
    "The transducer model trained based on sequence-level criterion requires a lot of memory due to the generation of the large probability matrix. We proposed a lightweight transducer model based on frame-level criterion, which uses the results of the CTC forced alignment algorithm to determine the label for each frame. Then the encoder output can be combined with the decoder output at the corresponding time, rather than adding each element output by the encoder to each element output by the decoder as in the transducer. This significantly reduces memory and computation requirements. To address the problem of imbalanced classification caused by excessive blanks in the label, we decouple the blank and non-blank probabilities and truncate the gradient of the blank classifier to the main network. This enables the lightweight transducer achieving similar results to transducer. Additionally, we use richer information to predict the probability of blank, achieving superior results to transducer."
   ],
   "p1": 247,
   "pn": 251,
   "doi": "10.21437/Interspeech.2024-768",
   "url": "interspeech_2024/wan24_interspeech.html"
  },
  "kanagawa24_interspeech": {
   "authors": [
    [
     "Hiroki",
     "Kanagawa"
    ],
    [
     "Takafumi",
     "Moriya"
    ],
    [
     "Yusuke",
     "Ijima"
    ]
   ],
   "title": "Pre-training Neural Transducer-based Streaming Voice Conversion for Faster Convergence and Alignment-free Training",
   "original": "771",
   "order": 567,
   "page_count": 5,
   "abstract": [
    "Seq2seq-based voice conversion (seq2seq VC) can model without pre-aligning speech lengths, but non-monotonic attention matrices may cause unnatural VC. The VC-T, a streaming VC based on neural transducer, ensures monotonic alignments, outperforming seq2seq VC. However, VC-T demands guiding alignments and time-consuming training due to tensor computations. The guiding alignments are generated from manually annotated phoneme labels with labor-intensive efforts, and they are essential for eliminating improbable paths, thus stabilizing VC-T training. This work proposes a two-stage VC-T training pipeline for fast convergence: 1) VC-T pre-training to learn probable paths, which form a matrix, optimized by L1 loss, 2) Fine-tuning refines the pre-trained VC-T, outputting a probable tensor from the start. This enables VC-T training without the guiding alignment. Experiments show our pipeline achieves superior streaming VC while significantly reducing training time compared to conventional VC-T."
   ],
   "p1": 2755,
   "pn": 2759,
   "doi": "10.21437/Interspeech.2024-771",
   "url": "interspeech_2024/kanagawa24_interspeech.html"
  },
  "wu24f_interspeech": {
   "authors": [
    [
     "Jing",
     "Wu"
    ],
    [
     "Ting",
     "Chen"
    ],
    [
     "Minchuan",
     "Chen"
    ],
    [
     "Wei",
     "Hu"
    ],
    [
     "Shaojun",
     "Wang"
    ],
    [
     "Jing",
     "Xiao"
    ]
   ],
   "title": "Improving Multilingual Text-to-Speech with Mixture-of-Language-Experts and Accent Disentanglement",
   "original": "775",
   "order": 1018,
   "page_count": 5,
   "abstract": [
    "Code-switching and accent control is particularly valuable in multilingual text-to-speech (TTS) systems as both of them contribute to improving the authenticity and comprehensibility. However, the issues of seamless integration of languages within a single utterance and the thorough disentanglement of different attributes without bilingual data remains to be solved. To conquer these problems, a computation-efficient model is proposed in this paper. Firstly, the Mixture of Language Experts (MoLE) module is introduced as the encoder to extract language-specific features and fuse intra-utterance semantic information. Secondly, embedding methods together with several regularization strategies and speaker consistency constraints are utilized to ensure that the generated speech aligns with the desired accent. Experiments show that the proposed model can improve the performance of code-switching accent-controllable multilingual TTS over the baseline model in terms of fluency and naturalness."
   ],
   "p1": 4968,
   "pn": 4972,
   "doi": "10.21437/Interspeech.2024-775",
   "url": "interspeech_2024/wu24f_interspeech.html"
  },
  "wu24g_interspeech": {
   "authors": [
    [
     "Boyong",
     "Wu"
    ],
    [
     "Chao",
     "Yan"
    ],
    [
     "Haoran",
     "Pu"
    ]
   ],
   "title": "Transferable speech-to-text large language model alignment module",
   "original": "777",
   "order": 617,
   "page_count": 5,
   "abstract": [
    "By leveraging the power of Large Language Models(LLMs) and speech foundation models, state of the art speech-text bimodal works can achieve challenging tasks like spoken translation(ST) and question answering(SQA) altogether with much simpler architectures. In this paper, we utilize the capability of Whisper encoder and pre-trained Yi-6B. Empirical results reveal that modal alignment can be achieved with one layer module and hundred hours of speech-text multitask corpus. We further swap the Yi-6B with human preferences aligned version of Yi-6B-Chat during inference, and discover that the alignment capability is applicable as well. In addition, the alignment subspace revealed by singular value decomposition(SVD) also implies linear alignment subspace is sparse, which leaves the possibility to concatenate other features like voice-print or video to expand modality."
   ],
   "p1": 3005,
   "pn": 3009,
   "doi": "10.21437/Interspeech.2024-777",
   "url": "interspeech_2024/wu24g_interspeech.html"
  },
  "edlund24_interspeech": {
   "authors": [
    [
     "Jens",
     "Edlund"
    ],
    [
     "Christina",
     "Tånnander"
    ],
    [
     "Sébastien",
     "Le Maguer"
    ],
    [
     "Petra",
     "Wagner"
    ]
   ],
   "title": "Assessing the impact of contextual framing on subjective TTS quality",
   "original": "781",
   "order": 250,
   "page_count": 5,
   "abstract": [
    "Text-To-Speech (TTS) evaluations are habitually carried out without contextual and situational framing. Since humans adapt their speaking style to situation specific communicative needs, such evaluations may not generalize across situations. Without clearly defined framing, it is even unclear in which situations evaluation results hold at all. We test the hypothesized impact of framing on TTS evaluation in a crowdsourced MOS evaluation of four TTS voices, systematically varying (a) the intended TTS task (domestic humanoid robot, child’s voice replacement, fiction audio books and long and information-rich texts) and (b) the framing of that task. The results show that framing differentiated MOS responses, with individual TTS performance varying significantly across tasks and framings. This corroborates the assumption that decontextualized MOS evaluations do not generalize, and suggests that TTS evaluations should not be reported without the type of framing that was employed, if any."
   ],
   "p1": 1205,
   "pn": 1209,
   "doi": "10.21437/Interspeech.2024-781",
   "url": "interspeech_2024/edlund24_interspeech.html"
  },
  "wang24r_interspeech": {
   "authors": [
    [
     "Kexin",
     "Wang"
    ],
    [
     "Carlos",
     "Ishi"
    ],
    [
     "Ryoko",
     "Hayashi"
    ]
   ],
   "title": "A multimodal analysis of different types of laughter expression in conversational dialogues",
   "original": "782",
   "order": 959,
   "page_count": 5,
   "abstract": [
    "Laughter is an important form of nonverbal communication and is manifested in various forms. In the present work, we analyzed an audio-visual conversation dataset and categorized laughter into several types according to its function. Four predominant categories were focused for multimodal analysis: mirthful, boosting, smoothing, and softening. We investigated the relationship between laughter types and expression modalities, including voice quality, facial expression, gaze, and body gestures. The results revealed that mirthful and boosting laughter produced with positive emotions or attitudes tend to be higher, longer and tenser voice quality, and accompanied by larger changes in facial expressions and body movements. We also conducted a careful analysis of gaze behaviors during laughter events, suggesting that boosting laughter tended to keep eye contact with both dialogue partners more than mirthful laughter."
   ],
   "p1": 4673,
   "pn": 4677,
   "doi": "10.21437/Interspeech.2024-782",
   "url": "interspeech_2024/wang24r_interspeech.html"
  },
  "kando24_interspeech": {
   "authors": [
    [
     "Shunsuke",
     "Kando"
    ],
    [
     "Yusuke",
     "Miyao"
    ],
    [
     "Jason",
     "Naradowsky"
    ],
    [
     "Shinnosuke",
     "Takamichi"
    ]
   ],
   "title": "Textless Dependency Parsing by Labeled Sequence Prediction",
   "original": "783",
   "order": 277,
   "page_count": 5,
   "abstract": [
    "Traditional spoken language processing involves cascading an automatic speech recognition (ASR) system into text processing models. In contrast, ``textless'' methods process speech representations without ASR systems, enabling the direct use of acoustic speech features. Although their effectiveness is shown in capturing acoustic features, it is unclear in capturing lexical knowledge. This paper proposes a textless method for dependency parsing, examining its effectiveness and limitations. Our proposed method predicts a dependency tree from a speech signal  without transcribing, representing the tree as a labeled sequence. scading method outperforms the textless method in overall parsing accuracy, the latter excels in instances with important acoustic features. Our findings highlight the importance of fusing word-level representations and sentence-level prosody for enhanced parsing performance. The code and models are made publicly available."
   ],
   "p1": 1340,
   "pn": 1344,
   "doi": "10.21437/Interspeech.2024-783",
   "url": "interspeech_2024/kando24_interspeech.html"
  },
  "zhao24c_interspeech": {
   "authors": [
    [
     "Zugang",
     "Zhao"
    ],
    [
     "Jinghong",
     "Zhang"
    ],
    [
     "Yonghui",
     "Liu"
    ],
    [
     "Jianbing",
     "Liu"
    ],
    [
     "Kai",
     "Niu"
    ],
    [
     "Zhiqiang",
     "He"
    ]
   ],
   "title": "Streamlining Speech Enhancement DNNs: an Automated Pruning Method Based on Dependency Graph with Advanced Regularized Loss Strategies",
   "original": "785",
   "order": 134,
   "page_count": 5,
   "abstract": [
    "In the burgeoning field of speech enhancement, the quest for high-performing deep neural networks(DNNs) often grapples with the challenge of increased computational demand and model size. This study unveils a novel structured pruning method that optimizes model via Dependency Graph, achieving automatic dimension reduction of network layers without manual settings of pruning ratios—a milestone not previously accomplished. Additionally, we propose a regularized loss strategy that adapts to variable scale sparsity, enhancing compression efficiency. Through extensive experiments, we demonstrate our method's ability to achieve substantial reductions in model size and computational costs while maintaining performance. Notably, Our findings also question the utility of grouping trick in linear layers, suggesting it may impede effective pruning. This research not only propels forward the capabilities of speech enhancement DNN compression, but also enriches the discourse on pruning methodology."
   ],
   "p1": 662,
   "pn": 666,
   "doi": "10.21437/Interspeech.2024-785",
   "url": "interspeech_2024/zhao24c_interspeech.html"
  },
  "lin24g_interspeech": {
   "authors": [
    [
     "Jingru",
     "Lin"
    ],
    [
     "Meng",
     "Ge"
    ],
    [
     "Junyi",
     "Ao"
    ],
    [
     "Liqun",
     "Deng"
    ],
    [
     "Haizhou",
     "Li"
    ]
   ],
   "title": "SA-WavLM: Speaker-Aware Self-Supervised Pre-training for Mixture Speech",
   "original": "787",
   "order": 121,
   "page_count": 5,
   "abstract": [
    "It was shown that pre-trained models with self-supervised learning (SSL) techniques are effective in various downstream speech tasks. However, most such models are trained on single-speaker speech data, limiting their effectiveness in mixture speech. This motivates us to explore pre-training on mixture speech. This work presents SA-WavLM, a novel pre-trained model for mixture speech. Specifically, SA-WavLM follows an \"extract-merge-predict\" pipeline in which the representations of each speaker in the input mixture are first extracted individually and then merged before the final prediction. In this pipeline, SA-WavLM performs speaker-informed extractions with the consideration of the interactions between different speakers. Furthermore, a speaker shuffling strategy is proposed to enhance the robustness towards the speaker absence. Experiments show that SA-WavLM either matches or improves upon the state-of-the-art pre-trained models."
   ],
   "p1": 597,
   "pn": 601,
   "doi": "10.21437/Interspeech.2024-787",
   "url": "interspeech_2024/lin24g_interspeech.html"
  },
  "ma24b_interspeech": {
   "authors": [
    [
     "Ziyang",
     "Ma"
    ],
    [
     "Mingjie",
     "Chen"
    ],
    [
     "Hezhao",
     "Zhang"
    ],
    [
     "Zhisheng",
     "Zheng"
    ],
    [
     "Wenxi",
     "Chen"
    ],
    [
     "Xiquan",
     "Li"
    ],
    [
     "Jiaxin",
     "Ye"
    ],
    [
     "Xie",
     "Chen"
    ],
    [
     "Thomas",
     "Hain"
    ]
   ],
   "title": "EmoBox: Multilingual Multi-corpus Speech Emotion Recognition Toolkit and Benchmark",
   "original": "788",
   "resource": "https://doi.org/10.48550/arXiv.2406.07162",
   "order": 325,
   "page_count": 5,
   "abstract": [
    "Speech emotion recognition (SER) is an important part of human-computer interaction, receiving extensive attention from both industry and academia. However, the current research field of SER has long suffered from the following problems: 1) There are few reasonable and universal splits of the datasets, making comparing different models and methods difficult. 2) No commonly used benchmark covers numerous corpus and languages for researchers to refer to, making reproduction a burden. In this paper, we propose EmoBox, an out-of-the box multilingual multi-corpus speech emotion recognition toolkit, along with a benchmark for both intra-corpus and cross-corpus settings. For intra-corpus settings, we carefully designed the data partitioning for different datasets. For cross-corpus settings, we employ a foundation SER model, emotion2vec, to mitigate annotation errors and obtain a test set that is fully balanced in speakers and emotions distributions. Based on EmoBox, we present the intra-corpus SER results of 10 pre-trained speech models on 32 emotion datasets with 14 languages, and the cross-corpus SER results on 4 datasets with the fully balanced test sets. To the best of our knowledge, this is the largest SER benchmark, across language scopes and quantity scales. We hope that our toolkit and benchmark can facilitate the research of SER in the community."
   ],
   "p1": 1580,
   "pn": 1584,
   "doi": "10.21437/Interspeech.2024-788",
   "url": "interspeech_2024/ma24b_interspeech.html"
  },
  "li24r_interspeech": {
   "authors": [
    [
     "Haoyu",
     "Li"
    ],
    [
     "Baochen",
     "Yang"
    ],
    [
     "Yu",
     "Xi"
    ],
    [
     "Linfeng",
     "Yu"
    ],
    [
     "Tian",
     "Tan"
    ],
    [
     "Hao",
     "Li"
    ],
    [
     "Kai",
     "Yu"
    ]
   ],
   "title": "Text-aware Speech Separation for Multi-talker Keyword Spotting",
   "original": "789",
   "order": 69,
   "page_count": 5,
   "abstract": [
    "For noisy environments, ensuring the robustness of keyword spotting (KWS) systems is essential. While much research has focused on noisy KWS, less attention has been paid to multi-talker mixed speech scenarios. Unlike the usual cocktail party problem where multi-talker speech is separated using speaker clues, the key challenge here is to extract the target speech for KWS based on text clues. To address it, this paper proposes a novel Text-aware Permutation Determinization Training method for multi-talker KWS with a clue-based Speech Separation front-end (TPDT-SS). Our research highlights the critical role of SS front-ends and shows that incorporating keyword-specific clues into these models can greatly enhance the effectiveness. TPDT-SS shows remarkable success in addressing permutation problems in mixed keyword speech, thereby greatly boosting the performance of the backend. Additionally, fine-tuning our system on unseen mixed speech results in further performance improvement."
   ],
   "p1": 337,
   "pn": 341,
   "doi": "10.21437/Interspeech.2024-789",
   "url": "interspeech_2024/li24r_interspeech.html"
  },
  "lee24h_interspeech": {
   "authors": [
    [
     "Jaesong",
     "Lee"
    ],
    [
     "Soyoon",
     "Kim"
    ],
    [
     "Hanbyul",
     "Kim"
    ],
    [
     "Joon Son",
     "Chung"
    ]
   ],
   "title": "Lightweight Audio Segmentation for Long-form Speech Translation",
   "original": "790",
   "order": 171,
   "page_count": 5,
   "abstract": [
    "Speech segmentation is an essential part of speech translation (ST) systems in real-world scenarios. Since most ST models are designed to process speech segments, long-form audio must be partitioned into shorter segments before translation. Recently, data-driven approaches for the speech segmentation task have been developed. Although these approaches improve overall translation quality, a performance gap exists due to a mismatch between the models and ST systems.In addition, the prior works require large self-supervised speech models, which consume significant computational resources.In this work, we propose a segmentation model that achieves better speech translation quality with a small model size. We propose an ASR-with-punctuation task as an effective pre-training strategy for the segmentation model. We also show that proper integration of the speech segmentation model into the underlying ST system is critical to improve overall translation quality at inference time."
   ],
   "p1": 847,
   "pn": 851,
   "doi": "10.21437/Interspeech.2024-790",
   "url": "interspeech_2024/lee24h_interspeech.html"
  },
  "lebourdais24_interspeech": {
   "authors": [
    [
     "Martin",
     "Lebourdais"
    ],
    [
     "Théo",
     "Mariotte"
    ],
    [
     "Antonio",
     "Almudévar"
    ],
    [
     "Marie",
     "Tahon"
    ],
    [
     "Alfonso",
     "Ortega"
    ]
   ],
   "title": "Explainable by-design Audio Segmentation through Non-Negative Matrix Factorization and Probing",
   "original": "791",
   "order": 975,
   "page_count": 5,
   "abstract": [
    "Audio segmentation is a key task for many speech technologies, most of which are based on neural networks, usually considered as black boxes, with high level performances. However, in many domains, among which health or forensics, there is not only a need for good performances but also for explanations about the output decision. Explanations derived directly from latent representations need to satisfy ``good'' properties such as informativeness, compactness or modularity, to be interpretable.In this article, we propose an explainable-by-design audio segmentation model based on non-negative matrix factorization (NMF) which is a good candidate for the design of interpretable representations. This paper shows that our model reaches good segmentation performances, and presents deep analyses of the latent representation extracted from the non-negative matrix. The proposed approach opens new perspectives towards the evaluation of interpretable representations according to 'good' properties."
   ],
   "p1": 4753,
   "pn": 4757,
   "doi": "10.21437/Interspeech.2024-791",
   "url": "interspeech_2024/lebourdais24_interspeech.html"
  },
  "zhou24d_interspeech": {
   "authors": [
    [
     "Nan",
     "Zhou"
    ],
    [
     "Youhai",
     "Jiang"
    ],
    [
     "Jialin",
     "Tan"
    ],
    [
     "Chongmin",
     "Qi"
    ]
   ],
   "title": "PLDNet: PLD-Guided Lightweight Deep Network Boosted by Efﬁcient Attention for Handheld Dual-Microphone Speech Enhancement",
   "original": "801",
   "order": 690,
   "page_count": 5,
   "abstract": [
    "Low-complexity speech enhancement on mobile phones is crucial in the era of 5G. Thus, focusing on handheld mobile phone communication scenario, based on power level difference (PLD) algorithm and lightweight U-Net, we propose PLD-guided lightweight deep network (PLDNet), an extremely lightweight dual-microphone speech enhancement method that integrates the guidance of signal processing algorithm and lightweight attention-augmented U-Net. For the guidance information, we employ PLD algorithm to pre-process dual-microphone spectrum, and feed the output into subsequent deep neural network, which utilizes a lightweight U-Net with our proposed gated convolution augmented frequency attention (GCAFA) module to extract desired clean speech. Experimental results demonstrate that our proposed method achieves competitive performance with recent top-performing models while reducing computational cost by over 90%, highlighting the potential for low-complexity speech enhancement on mobile phones."
   ],
   "p1": 3370,
   "pn": 3374,
   "doi": "10.21437/Interspeech.2024-801",
   "url": "interspeech_2024/zhou24d_interspeech.html"
  },
  "valente24_interspeech": {
   "authors": [
    [
     "Martina",
     "Valente"
    ],
    [
     "Fabio",
     "Brugnara"
    ],
    [
     "Giovanni",
     "Morrone"
    ],
    [
     "Enrico",
     "Zovato"
    ],
    [
     "Leonardo",
     "Badino"
    ]
   ],
   "title": "Exploring Spoken Language Identification Strategies for Automatic Transcription of Multilingual Broadcast and Institutional Speech",
   "original": "802",
   "resource": "https://doi.org/10.5281/zenodo.12784314",
   "order": 338,
   "page_count": 5,
   "abstract": [
    "This paper addresses spoken language identification (SLI) and speech recognition of multilingual broadcast and institutional speech, real application scenarios that have been rarely addressed in the SLI literature. Observing that in these domains language changes are mostly associated with speaker changes, we propose a cascaded system consisting of speaker diarization and language identification and compare it with more traditional language identification and language diarization systems. Results show that the proposed system often achieves lower language classification and language diarization error rates (up to 10% relative language diarization error reduction and 60% relative language confusion reduction) and leads to lower WERs on multilingual test sets (more than 8% relative WER reduction), while at the same time does not negatively affect speech recognition on monolingual audio (with an absolute WER increase between 0.1% and 0.7% w.r.t. monolingual ASR)."
   ],
   "p1": 1645,
   "pn": 1649,
   "doi": "10.21437/Interspeech.2024-802",
   "url": "interspeech_2024/valente24_interspeech.html"
  },
  "fujita24b_interspeech": {
   "authors": [
    [
     "Kenichi",
     "Fujita"
    ],
    [
     "Takanori",
     "Ashihara"
    ],
    [
     "Marc",
     "Delcroix"
    ],
    [
     "Yusuke",
     "Ijima"
    ]
   ],
   "title": "Lightweight Zero-shot Text-to-Speech with Mixture of Adapters",
   "original": "803",
   "order": 140,
   "page_count": 5,
   "abstract": [
    "The advancements in zero-shot text-to-speech (TTS) methods, based on large-scale models, have demonstrated high fidelity in reproducing speaker characteristics. However, these models are too large for practical daily use. We propose a lightweight zero-shot TTS method using a mixture of adapters (MoA). Our proposed method incorporates MoA modules into the decoder and the variance adapter of a non-autoregressive TTS model. These modules enhance the ability to adapt a wide variety of speakers in a zero-shot manner by selecting appropriate adapters associated with speaker characteristics on the basis of speaker embeddings. Our method achieves high-quality speech synthesis with minimal additional parameters. Through objective and subjective evaluations, we confirmed that our method achieves better performance than the baseline with less than 40% of parameters at 1.9 times faster inference speed. "
   ],
   "p1": 692,
   "pn": 696,
   "doi": "10.21437/Interspeech.2024-803",
   "url": "interspeech_2024/fujita24b_interspeech.html"
  },
  "liu24e_interspeech": {
   "authors": [
    [
     "Junzhe",
     "Liu"
    ],
    [
     "Jianwei",
     "Yu"
    ],
    [
     "Xie",
     "Chen"
    ]
   ],
   "title": "Improved Factorized Neural Transducer Model For Text-only Domain Adaptation",
   "original": "812",
   "order": 155,
   "page_count": 5,
   "abstract": [
    "Adapting End-to-End ASR models to out-of-domain datasets with text data is challenging. Factorized neural Transducer (FNT) aims to address this issue by introducing a separate vocabulary decoder to predict the vocabulary. Nonetheless, this approach has limitations in fusing acoustic and language information seamlessly. Moreover, a degradation in word error rate (WER) on the general test sets was also observed, leading to doubts about its overall performance. In response to this challenge, we present the improved factorized neural Transducer (IFNT) model structure designed to comprehensively integrate acoustic and language information while enabling effective text adaptation. We assess the performance of our proposed method on English and Mandarin datasets. The results indicate that IFNT not only surpasses the neural Transducer and FNT in baseline performance in both scenarios but also exhibits superior adaptation ability compared to FNT. On source domains, IFNT demonstrated statistically significant accuracy im- provements, achieving a relative enhancement of 1.2% to 2.8% in baseline accuracy compared to the neural Transducer. On out-of-domain datasets, IFNT shows relative WER(CER) improvements of up to 30.2% over the standard neural Transducer with shallow fusion, and relative WER(CER) reductions ranging from 1.1% to 2.8% on test sets compared to the FNT model."
   ],
   "p1": 767,
   "pn": 771,
   "doi": "10.21437/Interspeech.2024-812",
   "url": "interspeech_2024/liu24e_interspeech.html"
  },
  "hoscilowicz24_interspeech": {
   "authors": [
    [
     "Jakub",
     "Hoscilowicz"
    ],
    [
     "Adam",
     "Wiacek"
    ],
    [
     "Jan",
     "Chojnacki"
    ],
    [
     "Adam",
     "Cieslak"
    ],
    [
     "Leszek",
     "Michon"
    ],
    [
     "Artur",
     "Janicki"
    ]
   ],
   "title": "Non-Linear Inference Time Intervention: Improving LLM Truthfulness",
   "original": "819",
   "order": 842,
   "page_count": 5,
   "abstract": [
    "In this work, we explore LLM's internal representation space to identify attention heads that contain the most truthful and accurate information. We further developed the Inference Time Intervention (ITI) framework, which lets bias LLM without the need for fine-tuning. The improvement manifests in introducing a non-linear multi-token probing and multi-token intervention: Non-Linear ITI (NL-ITI), which significantly enhances performance on evaluation benchmarks. NL-ITI is tested on diverse multiple-choice datasets, including TruthfulQA, on which we report over 14% relative MC1 (accuracy of model pointing to the correct answer) improvement with respect to the baseline ITI results. Moreover, we achieved a 10% relative improvement over the recently released Truth Forest (TrFf) method that also focused on ITI improvement."
   ],
   "p1": 4094,
   "pn": 4098,
   "doi": "10.21437/Interspeech.2024-819",
   "url": "interspeech_2024/hoscilowicz24_interspeech.html"
  },
  "elie24_interspeech": {
   "authors": [
    [
     "Benjamin",
     "Elie"
    ],
    [
     "Juraj",
     "Simko"
    ],
    [
     "Alice",
     "Turk"
    ]
   ],
   "title": "A data-driven model of acoustic speech intelligibility for optimization-based models of speech production",
   "original": "822",
   "order": 738,
   "page_count": 5,
   "abstract": [
    "This paper presents a data-driven model of intelligibility which is intended to be used in an optimization-based model of speech production. The BiLSTM-based model is trained as a phoneme classifier and takes a sequence of real articulatory trajectories as input and returns the probability of phonemes over time. The optimization minimizes a cost function which is the weighted sum of the conflicting demands of being intelligible and least articulatory effort. The data-driven intelligibility model presented in this paper is used to compute the intelligibility score. Simulations support Lindblom's hypo- and hyper-articulation theory of speech, as the degree of hyper-articulation of speech can be modified and tuned along a continuum by balancing the importance given to both requirements of intelligibility and least articulatory effort. "
   ],
   "p1": 3610,
   "pn": 3614,
   "doi": "10.21437/Interspeech.2024-822",
   "url": "interspeech_2024/elie24_interspeech.html"
  },
  "zhang24g_interspeech": {
   "authors": [
    [
     "Jinghong",
     "Zhang"
    ],
    [
     "Zugang",
     "Zhao"
    ],
    [
     "Yonghui",
     "Liu"
    ],
    [
     "Jianbing",
     "Liu"
    ],
    [
     "Zhiqiang",
     "He"
    ],
    [
     "Kai",
     "Niu"
    ]
   ],
   "title": "TD-PLC: A Semantic-Aware Speech Encoding for Improved Packet Loss Concealment",
   "original": "823",
   "order": 358,
   "page_count": 5,
   "abstract": [
    "In the domain of real-time communication, Packet Loss Concealment (PLC) strategies are crucial for mitigating the adverse effects of packet loss and delays, thereby preserving the integrity of speech transmission. Existing methods, while effective to all degrees, often fall short in scenarios characterized by prolonged packet loss, where the quality of speech recovery deteriorates with increased loss duration. Our study introducesSemantic-Aware Speech Encoding, a novel encoding approach that integrates semantic information with audio to improve PLC. By leveraging advances in neural audio coding, our method efficiently extracts and integrates semantic cues alongside audio signals, facilitating their joint transmission over the network. This semantically enriched methodology concurrently facilitating enhanced accuracy in the receiver’s reconstruction of lost speech elements. Experiment results demonstrate its effectiveness, especially in long burst packet loss scenarios, highlighting its reliability and significant impact on PLC advancements."
   ],
   "p1": 1745,
   "pn": 1749,
   "doi": "10.21437/Interspeech.2024-823",
   "url": "interspeech_2024/zhang24g_interspeech.html"
  },
  "almudevar24_interspeech": {
   "authors": [
    [
     "Antonio",
     "Almudévar"
    ],
    [
     "Théo",
     "Mariotte"
    ],
    [
     "Alfonso",
     "Ortega"
    ],
    [
     "Marie",
     "Tahon"
    ],
    [
     "Luis",
     "Vicente"
    ],
    [
     "Antonio",
     "Miguel"
    ],
    [
     "Eduardo",
     "Lleida"
    ]
   ],
   "title": "Predefined Prototypes for Intra-Class Separation and Disentanglement",
   "original": "825",
   "order": 785,
   "page_count": 5,
   "abstract": [
    "Prototypical Learning is based on the idea that there is a point (which we call prototype) around which the embeddings of a class are clustered. It has shown promising results in scenarios with little labeled data or to design explainable models. Typically, prototypes are either defined as the average of the embeddings of a class or are designed to be trainable. In this work, we propose to predefine prototypes following human-specified criteria, which simplify the training pipeline and brings different advantages. Specifically, in this work we explore two of these advantages: increasing the inter-class separability of embeddings and disentangling embeddings with respect to different variance factors, which can translate into the possibility of having explainable predictions. Finally, we propose different experiments that help to understand our proposal and demonstrate empirically the mentioned advantages."
   ],
   "p1": 3809,
   "pn": 3813,
   "doi": "10.21437/Interspeech.2024-825",
   "url": "interspeech_2024/almudevar24_interspeech.html"
  },
  "raissi24_interspeech": {
   "authors": [
    [
     "Tina",
     "Raissi"
    ],
    [
     "Christoph",
     "Lüscher"
    ],
    [
     "Simon",
     "Berger"
    ],
    [
     "Ralf",
     "Schlüter"
    ],
    [
     "Hermann",
     "Ney"
    ]
   ],
   "title": "Investigating the Effect of Label Topology and Training Criterion on ASR Performance and Alignment Quality",
   "original": "830",
   "order": 803,
   "page_count": 5,
   "abstract": [
    "The ongoing research scenario for automatic speech recognition (ASR) envisions a clear division between end-to-end approaches and classic modular systems. Even though a high-level comparison between the two approaches in terms of their requirements and (dis)advantages is commonly addressed, a closer comparison under similar conditions is not readily available in the literature. In this work, we present a comparison focused on the label topology and training criterion. We compare two discriminative alignment models with hidden Markov model (HMM) and connectionist temporal classification topology, and two first-order label context ASR models utilizing factored HMM and strictly monotonic recurrent neural network transducer, respectively. We use different measurements for the evaluation of the alignment quality, and compare word error rate and real time factor of our best systems. Experiments are conducted on the LibriSpeech 960h and Switchboard 300h tasks. "
   ],
   "p1": 3899,
   "pn": 3903,
   "doi": "10.21437/Interspeech.2024-830",
   "url": "interspeech_2024/raissi24_interspeech.html"
  },
  "tulchynska24_interspeech": {
   "authors": [
    [
     "Kira",
     "Tulchynska"
    ],
    [
     "Sylvanus",
     "Job"
    ],
    [
     "Alena",
     "Witzlack-Makarevich"
    ],
    [
     "Margaret",
     "Zellers"
    ]
   ],
   "title": "Prosodic marking of syntactic boundaries in Khoekhoe",
   "original": "833",
   "order": 760,
   "page_count": 5,
   "abstract": [
    "Khoekhoe (ISO 639-3: naq) is an understudied low-resource Khoe-Kwadi language spoken in Namibia. Khoekhoe is an SOV language with a strict syntactic clause-medial position: the first constituent of the clause is separated from the rest of the clause by a group of auxiliaries. We investigate the locations of candidate intonation phrase boundaries in relation to syntactic boundaries, observing the phonetic-prosodic characteristics of our candidate intonation phrases in order to confirm the initial phrasing analysis and compare boundary strength associated with different syntactic boundary types. We find evidence of differential prosodic marking strategies employed in clause-final versus the clause-medial position following auxiliaries, with, for example, stronger effects of final segmental lengthening following auxiliaries compared to rather attenuated final lengthening in clause-final position."
   ],
   "p1": 3684,
   "pn": 3688,
   "doi": "10.21437/Interspeech.2024-833",
   "url": "interspeech_2024/tulchynska24_interspeech.html"
  },
  "nayak24_interspeech": {
   "authors": [
    [
     "Premanand",
     "Nayak"
    ],
    [
     "Kamini",
     "Sabu"
    ],
    [
     "M. Ali Basha",
     "Shaik"
    ]
   ],
   "title": "Multi-mic Echo Cancellation Coalesced with Beamforming for Real World Adverse Acoustic Conditions",
   "original": "834",
   "order": 31,
   "page_count": 5,
   "abstract": [
    "Robust acoustic echo cancellation (AEC) is essential for voice enabled smart devices. Multi-channel signals are used in AEC along with beamformer (BF) for better residual echo suppression (RES). In this work, we introduce a deep neural network (DNN) based novel unified framework for multi-microphone AEC (MMAEC) and RES under adverse signal-to-echo (SER) conditions. We propose the use of deep-MVDR which uses deep steering vector and deep power spectral density (Deep PSD) estimator to conceptually implement minimum variance distortionless beamformer. We also introduce additional novelty in our framework by jointly training the MMAEC and deep-MVDR modules. Both of these methods give consistent significant improvement in ERLE which is further enriched by the incorporation of playback reconstruction loss. Our system outperforms competitive baselines while being robust in adverse real-world conditions such as very low input SER, dominant far-end sources, and moving near-end speech sources."
   ],
   "p1": 147,
   "pn": 151,
   "doi": "10.21437/Interspeech.2024-834",
   "url": "interspeech_2024/nayak24_interspeech.html"
  },
  "dibratto24_interspeech": {
   "authors": [
    [
     "Martina",
     "Di Bratto"
    ],
    [
     "Maria",
     "Di Maro"
    ],
    [
     "Antonio",
     "Origlia"
    ]
   ],
   "title": "On the Use of Plausible Arguments in Explainable Conversational AI",
   "original": "839",
   "order": 834,
   "page_count": 5,
   "abstract": [
    "Conversational Artificial Intelligence has evolved to facilitate more efficient communication of user preferences through dialogues. This paper delves into Argumentative Conversational AI systems, and, more specifically in the definition of a methodology of selecting and using plausible arguments to support recommendations. We propose a cross-disciplinary model grounded in cognitive pragmatics to enhance recommendation quality. We evaluate this linguistically motivated strategy in isolation using simulated dialogues and collect human judgements to verify that the expected interaction is believable. Next, we test the full interaction model with human users to evaluate its usability. Results indicate high scores for naturalness and argument selection, validating the system's plausibility and effectiveness. Concerning usability, the system is perceived as attractive and reliable although technical issues concerning the system's reactiveness are present."
   ],
   "p1": 4054,
   "pn": 4058,
   "doi": "10.21437/Interspeech.2024-839",
   "url": "interspeech_2024/dibratto24_interspeech.html"
  },
  "yue24_interspeech": {
   "authors": [
    [
     "Yaoyao",
     "Yue"
    ],
    [
     "Michael",
     "Proctor"
    ],
    [
     "Luping",
     "Zhou"
    ],
    [
     "Rijul",
     "Gupta"
    ],
    [
     "Tharinda",
     "Piyadasa"
    ],
    [
     "Amelia",
     "Gully"
    ],
    [
     "Kirrie",
     "Ballard"
    ],
    [
     "Craig",
     "Jin"
    ]
   ],
   "title": "Towards Speech Classification from Acoustic and Vocal Tract data in Real-time MRI",
   "original": "840",
   "order": 278,
   "page_count": 5,
   "abstract": [
    "Real-time magnetic resonance image (rtMRI) data of the upper airway provides a rich source of information about vocal tract shaping that can inform phonemic analysis and classification. We describe a multimodal phonemic classifier that combines articulatory data with speech audio features to improve performance. A deep network model processes rtMRI video data using ResNet18 and speech audio using a custom CNN and then combines the two data streams using a Transformer layer to fully explore the correlation of the two streams towards better vowel-consonant-vowel classification via the Transformer's multi-head self-attention mechanism. The classification accuracy of both the unimodal and multimodal models show substantial improvement on previous work (> 38%). The addition of audio features improves classification accuracy in the multimodal model by 7% compared with the unimodal model using articulatory data.  We analyze the model and discuss the phonetic implications."
   ],
   "p1": 1345,
   "pn": 1349,
   "doi": "10.21437/Interspeech.2024-840",
   "url": "interspeech_2024/yue24_interspeech.html"
  },
  "trachu24_interspeech": {
   "authors": [
    [
     "Thanapat",
     "Trachu"
    ],
    [
     "Chawan",
     "Piansaddhayanon"
    ],
    [
     "Ekapol",
     "Chuangsuwanich"
    ]
   ],
   "title": "Thunder : Unified Regression-Diffusion Speech Enhancement with a Single Reverse Step using Brownian Bridge",
   "original": "841",
   "order": 245,
   "page_count": 5,
   "abstract": [
    "Diffusion-based speech enhancement has shown promising results, but can suffer from a slower inference time. Initializing the diffusion process with the enhanced audio generated by a regression-based model can be used to reduce the computational steps required. However, these approaches often necessitate a regression model, further increasing the system's complexity. We propose Thunder, a unified regression-diffusion model that utilizes the Brownian bridge process which can allow the model to act in both modes. The regression mode can be accessed by setting the diffusion time step closed to 1. However, the standard score-based diffusion modeling does not perform well in this setup due to gradient instability.  To mitigate this problem, we modify the diffusion model to predict the clean speech instead of the score function, achieving competitive performance with a more compact model size and fewer reverse steps."
   ],
   "p1": 1180,
   "pn": 1184,
   "doi": "10.21437/Interspeech.2024-841",
   "url": "interspeech_2024/trachu24_interspeech.html"
  },
  "paturi24_interspeech": {
   "authors": [
    [
     "Rohit",
     "Paturi"
    ],
    [
     "Xiang",
     "Li"
    ],
    [
     "Sundararajan",
     "Srinivasan"
    ]
   ],
   "title": "AG-LSEC: Audio Grounded Lexical Speaker Error Correction",
   "original": "845",
   "order": 339,
   "page_count": 5,
   "abstract": [
    "Speaker Diarization (SD) systems are typically audio-based and operate independently of the ASR system in traditional speech transcription pipelines and can have speaker errors due to SD and/or ASR reconciliation, especially around speaker turns and regions of speech overlap. To reduce these errors, a Lexical Speaker Error Correction (LSEC), in which an external language model provides lexical information to correct the speaker errors, was recently proposed. Though the approach achieves good Word Diarization error rate (WDER) improvements, it does not use any additional acoustic information and is prone to miscorrections. In this paper, we propose to enhance and acoustically ground the LSEC system with speaker scores directly derived from the existing SD pipeline. This approach achieves significant relative WDER reductions in the range of 25-40% over the audio-based SD, ASR system and beats the LSEC system by 15-25% relative on RT03-CTS, Callhome American English and Fisher datasets."
   ],
   "p1": 1650,
   "pn": 1654,
   "doi": "10.21437/Interspeech.2024-845",
   "url": "interspeech_2024/paturi24_interspeech.html"
  },
  "chiu24_interspeech": {
   "authors": [
    [
     "Sheng-Chieh",
     "Chiu"
    ],
    [
     "Chia-Hua",
     "Wu"
    ],
    [
     "Jih-Kang",
     "Hsieh"
    ],
    [
     "Yu",
     "Tsao"
    ],
    [
     "Hsin-Min",
     "Wang"
    ]
   ],
   "title": "Learnable Layer Selection and Model Fusion for Speech Self-Supervised Learning Models",
   "original": "849",
   "order": 806,
   "page_count": 5,
   "abstract": [
    "In this paper, we investigate methods for fusing feature representations derived from multiple speech self-supervised learning (SSL) models, along with techniques to determine the optimal layer within each model. We evaluate five fusing strategies, finding that temporal interleaved concatenation is the most robust and effective for the SUPERB ASR task. Additionally, we demonstrate that Gumbel layer selection can automatically select the most appropriate SSL layer with better performance than the commonly used weighted sum method. Furthermore, dimension-wise Gumbel layer selection shows promise in adaptive combination of layers of a single SSL model. Finally, we show that joint SSL model fusion and dimension-wise Gumbel layer selection further enhances effectiveness."
   ],
   "p1": 3914,
   "pn": 3918,
   "doi": "10.21437/Interspeech.2024-849",
   "url": "interspeech_2024/chiu24_interspeech.html"
  },
  "jiang24b_interspeech": {
   "authors": [
    [
     "Yicong",
     "Jiang"
    ],
    [
     "Tianzi",
     "Wang"
    ],
    [
     "Xurong",
     "Xie"
    ],
    [
     "Juan",
     "Liu"
    ],
    [
     "Wei",
     "Sun"
    ],
    [
     "Nan",
     "Yan"
    ],
    [
     "Hui",
     "Chen"
    ],
    [
     "Lan",
     "Wang"
    ],
    [
     "Xunying",
     "Liu"
    ],
    [
     "Feng",
     "Tian"
    ]
   ],
   "title": "Perceiver-Prompt: Flexible Speaker Adaptation in Whisper for Chinese Disordered Speech Recognition",
   "original": "852",
   "order": 414,
   "page_count": 5,
   "abstract": [
    "Disordered speech recognition profound implications for improving the quality of life for individuals afflicted with, for example, dysarthria. Dysarthric speech recognition encounters challenges including limited data, substantial dissimilarities between dysarthric and non-dysarthric speakers, and significant speaker variations stemming from the disorder. This paper introduces Perceiver-Prompt, a method for speaker adaptation that utilizes P-Tuning on the Whisper large-scale model. We first fine-tune Whisper using LoRA and then integrate a trainable Perceiver to generate fixed-length speaker prompts from variable-length inputs, to improve model recognition of Chinese dysarthric speech. Experimental results from our Chinese dysarthric speech dataset demonstrate consistent improvements in recognition performance with Perceiver-Prompt. Relative reduction up to 13.04% in CER is obtained over the fine-tuned Whisper."
   ],
   "p1": 2025,
   "pn": 2029,
   "doi": "10.21437/Interspeech.2024-852",
   "url": "interspeech_2024/jiang24b_interspeech.html"
  },
  "nayak24b_interspeech": {
   "authors": [
    [
     "Premanand",
     "Nayak"
    ],
    [
     "M. Ali Basha",
     "Shaik"
    ]
   ],
   "title": "Elucidating Clock-drift Using Real-world Audios In Wireless Mode For Time-offset Insensitive End-to-End Asynchronous Acoustic Echo Cancellation",
   "original": "854",
   "order": 129,
   "page_count": 5,
   "abstract": [
    "External playback and microphone array devices connected over wireless channels lack a common clock reference. This leads to non-linear asynchronous clock-drift effects with respect to effective echo path. Thereby, causing time-accelerated exacerbation when conventional filter based echo cancellers are used. We delineate clock-drift associated problems quantitatively by utilizing real-world audio streams recorded using various devices in wireless mode. We revisit and compare in situ conventional signal processing and deep neural network methods. We also introduce a novel end-to-end approach for asynchronous acoustic echo cancellation using Convolutional Recurrent Neural Networks and demonstrate state-of-the-art echo suppression performance without the need of time-resynchronization in the real-world observed non-linear clock-drift conditions."
   ],
   "p1": 637,
   "pn": 641,
   "doi": "10.21437/Interspeech.2024-854",
   "url": "interspeech_2024/nayak24b_interspeech.html"
  },
  "tan24_interspeech": {
   "authors": [
    [
     "Hao",
     "Tan"
    ],
    [
     "Xiaochen",
     "Liu"
    ],
    [
     "Huan",
     "Zhang"
    ],
    [
     "Junjian",
     "Zhang"
    ],
    [
     "Yaguan",
     "Qian"
    ],
    [
     "Zhaoquan",
     "Gu"
    ]
   ],
   "title": "DualPure: An Efficient Adversarial Purification Method for Speech Command Recognition",
   "original": "855",
   "order": 265,
   "page_count": 5,
   "abstract": [
    "Adversarial examples pose a security threat to Autopilot's speech command recognition module, which attracted widespread attention from researchers. Previous works purify the malicious adversarial perturbations through pre-processing data from the time and frequency domain information. However, these methods either have a weak purification capacity or require a significant purification cost. To tackle this problem, we propose a real-time and efficient purification-based defense method DualPure, which combines the two defense aspects in the time and frequency domain for copurification. Specifically, we first disrupt the potential malicious perturbation in the sample at the waveform level and then apply an unconditional diffusion model to purify the feature at the frequency level. Numerous experiments show that the proposed method can effectively purify and achieve good adversarial robustness in white-box attacks (+ 6.3%) and black-box attacks (+  1.08%)."
   ],
   "p1": 1280,
   "pn": 1284,
   "doi": "10.21437/Interspeech.2024-855",
   "url": "interspeech_2024/tan24_interspeech.html"
  },
  "flynn24_interspeech": {
   "authors": [
    [
     "Robert",
     "Flynn"
    ],
    [
     "Anton",
     "Ragni"
    ]
   ],
   "title": "Self-Train Before You Transcribe",
   "original": "858",
   "order": 584,
   "page_count": 5,
   "abstract": [
    "When there is a mismatch between the training and test domains, current speech recognition systems show significant performance degradation. Self-training methods, such as noisy student teacher training, can help address this and enable the adaptation of models under such domain shifts. However, self-training typically requires a collection of unlabelled target domain data. For settings where this is not practical, we investigate the benefit of performing noisy student teacher training on recordings in the test set as a test-time adaptation approach. Similarly to the dynamic evaluation approach in language modelling, this enables the transfer of information across utterance boundaries and functions as a method of domain adaptation. A range of in-domain and out-of-domain datasets are used for experiments demonstrating large relative gains of up to 32.2%. Interestingly, our method showed larger gains than the typical self-training setup that utilises separate adaptation data."
   ],
   "p1": 2840,
   "pn": 2844,
   "doi": "10.21437/Interspeech.2024-858",
   "url": "interspeech_2024/flynn24_interspeech.html"
  },
  "khaertdinov24_interspeech": {
   "authors": [
    [
     "Bulat",
     "Khaertdinov"
    ],
    [
     "Pedro",
     "Jeruis"
    ],
    [
     "Annanda",
     "Sousa"
    ],
    [
     "Enrique",
     "Hortal"
    ]
   ],
   "title": "Exploring Self-Supervised Multi-view Contrastive Learning for Speech Emotion Recognition with Limited Annotations",
   "original": "860",
   "order": 966,
   "page_count": 5,
   "abstract": [
    "Recent advancements in Deep and Self-Supervised Learning (SSL) have led to substantial improvements in Speech Emotion Recognition (SER) performance, reaching unprecedented levels. However, obtaining sufficient amounts of accurately labeled data for training or fine-tuning the models remains a costly and challenging task. In this paper, we propose a multi-view SSL pre-training technique that can be applied to various representations of speech, including the ones generated by large speech models, to improve SER performance in scenarios where annotations are limited. Our experiments, based on wav2vec 2.0, spectral and paralinguistic features, demonstrate that the proposed framework boosts the SER performance by up to 10% in Unweighted Average Recall, in settings with extremely sparse data annotations."
   ],
   "p1": 4708,
   "pn": 4712,
   "doi": "10.21437/Interspeech.2024-860",
   "url": "interspeech_2024/khaertdinov24_interspeech.html"
  },
  "yang24j_interspeech": {
   "authors": [
    [
     "Chengxu",
     "Yang"
    ],
    [
     "Lin",
     "Zheng"
    ],
    [
     "Sanli",
     "Tian"
    ],
    [
     "Gaofeng",
     "Cheng"
    ],
    [
     "Sujie",
     "Xiao"
    ],
    [
     "Ta",
     "Li"
    ]
   ],
   "title": "Contextual Biasing with Confidence-based Homophone Detector for Mandarin End-to-End Speech Recognition",
   "original": "869",
   "order": 151,
   "page_count": 5,
   "abstract": [
    "Deep biasing methods and shallow fusion methods have been demonstrated to improve the performance of end-to-end ASR effectively. However, accurate recognition often becomes challenging when specific words within the contextual phrases occur too infrequently in the training corpus or are out-of-vocabulary. To address this issue, we introduce a confidence-based homophone detector and syllable bias model to correct context phrases that may have been recognized incorrectly. The detector utilizes confidence distribution peaks resulting from homophone substitutions in ASR decoding outputs and employs their coefficient of variation for discrimination to avoid loss of general performance. Experiments on the biased word subset of Aishell-1 show that our proposed method obtains a 31.2% relative CER improvement over the baseline and a relative decrease of 52.0% for context phrases. When cascaded with the deep fusion and shallow fusion methods, the improvements become 13.7% and 33.5% respectively."
   ],
   "p1": 747,
   "pn": 751,
   "doi": "10.21437/Interspeech.2024-869",
   "url": "interspeech_2024/yang24j_interspeech.html"
  },
  "flynn24b_interspeech": {
   "authors": [
    [
     "Robert",
     "Flynn"
    ],
    [
     "Anton",
     "Ragni"
    ]
   ],
   "title": "How Much Context Does My Attention-Based ASR System Need?",
   "original": "870",
   "order": 45,
   "page_count": 5,
   "abstract": [
    "For the task of speech recognition, the use of more than 30 seconds of acoustic context during training is uncommon and under-investigated in literature. In this work, we conduct an empirical study on the effect of scaling the sequence length used to train/evaluate (dense-attention-based) acoustic models on speech recognition performance. For these experiments, a dataset of roughly 100,000 pseudo-labelled Spotify podcasts is used, with context lengths of 5 seconds to 1 hour being explored. Zero-shot evaluations are presented on the long-format datasets: Earnings-22, Tedlium and Rev16. Results demonstrate a benefit from training with up to 21.8 minutes of acoustic context, showing up to a 14.5% relative improvement from a baseline trained with 10 seconds of context. We find that the model's width/depth, positional encoding scheme and number of attention heads impact its ability to use longer contexts."
   ],
   "p1": 217,
   "pn": 221,
   "doi": "10.21437/Interspeech.2024-870",
   "url": "interspeech_2024/flynn24b_interspeech.html"
  },
  "fagniart24_interspeech": {
   "authors": [
    [
     "Sophie",
     "Fagniart"
    ],
    [
     "Brigitte",
     "Charlier"
    ],
    [
     "Véronique",
     "Delvaux"
    ],
    [
     "Bernard",
     "Harmegnies"
    ],
    [
     "Anne",
     "Huberlant"
    ],
    [
     "Myriam",
     "Piccaluga"
    ],
    [
     "Kathy",
     "Huet"
    ]
   ],
   "title": "Production of fricative consonants in French-speaking children with cochlear implants and typical hearing: acoustic and phonological analyses.",
   "original": "871",
   "order": 177,
   "page_count": 5,
   "abstract": [
    "The following study investigates fricative consonant production skills in 23 children with cochlear implants (CI group) and 47 children with typical hearing (TH group), matched by chronological and auditory age. The voiceless (/f/,/s/,/ʃ/) and voiced (/v/,/z/,/ʒ/) fricative consonants of French were studied from children's productions to a picture naming task. The results showed lower percentages of correct fricatives as well as fricativization and stopping errors in the CI group. Acoustic analyses showed productions differing between our two groups, with lower mid-frequency amplitude peak values for the /f,s,z/ phonemes, higher amplitude in the low-frequency bands and lower high-frequency energy in the CI group. Furthermore, links between phonological performance and acoustic productions is demonstrated: higher spectral values distinction are associated with a higher percentage of correct phonological production and fewer stopping/fricativization errors."
   ],
   "p1": 877,
   "pn": 881,
   "doi": "10.21437/Interspeech.2024-871",
   "url": "interspeech_2024/fagniart24_interspeech.html"
  },
  "gao24c_interspeech": {
   "authors": [
    [
     "Ming",
     "Gao"
    ],
    [
     "Hang",
     "Chen"
    ],
    [
     "Jun",
     "Du"
    ],
    [
     "Xin",
     "Xu"
    ],
    [
     "Hongxiao",
     "Guo"
    ],
    [
     "Hui",
     "Bu"
    ],
    [
     "Jianxing",
     "Yang"
    ],
    [
     "Ming",
     "Li"
    ],
    [
     "Chin-Hui",
     "Lee"
    ]
   ],
   "title": "Enhancing Voice Wake-Up for Dysarthria: Mandarin Dysarthria Speech Corpus Release and Customized System Design",
   "original": "879",
   "order": 508,
   "page_count": 5,
   "abstract": [
    "Smart home technology has gained widespread adoption, facilitating effortless control of devices through voice commands. However, individuals with dysarthria, a motor speech disorder, face challenges due to the variability of their speech. This paper addresses the wake-up word spotting (WWS) task for dysarthric individuals, aiming to integrate them into real-world applications. To support this, we release the open-source Mandarin Dysarthria Speech Corpus (MDSC), a dataset designed for dysarthric individuals in home environments. MDSC encompasses information on age, gender, disease types, and intelligibility evaluations. Furthermore, we perform comprehensive experimental analysis on MDSC, highlighting the challenges encountered. We also develop a customized dysarthria WWS system that showcases robustness in handling intelligibility and achieving exceptional performance. MDSC will be released on https://www.aishelltech.com/AISHELL_6B."
   ],
   "p1": 2465,
   "pn": 2469,
   "doi": "10.21437/Interspeech.2024-879",
   "url": "interspeech_2024/gao24c_interspeech.html"
  },
  "chang24_interspeech": {
   "authors": [
    [
     "Jeremy",
     "Chang"
    ],
    [
     "Kuan-Yu",
     "Chen"
    ],
    [
     "Chung-Hsien",
     "Wu"
    ]
   ],
   "title": "Applying Reinforcement Learning and Multi-Generators for Stage Transition in an Emotional Support Dialogue System",
   "original": "882",
   "order": 725,
   "page_count": 5,
   "abstract": [
    "The use of empathetic dialogue systems has grown recently. However, establishing them for users experiencing mental depression requires more advanced consoling skills. In this paper, a dialogue system based on Emotional Support was developed. The system offers coping strategies through stages designed to address users' distress in long-term conversations. It employs a recurrent-based approach integrated with reinforcement learning for a decision model, which selects a generator from three specialized conditional generation models to generate empathetic responses. Experimental results showed improvements in BLEU, Rouge-L, and Distinct-n metrics compared to the baseline. On average, the system's BLEU score increased by 0.87, Rouge-L by 1.85, Distinct-1 by 0.69, and Distinct-2 by 2.26. As a result, the system generates responses aligned with Emotional Support skills, ultimately comforting the user’s distress."
   ],
   "p1": 3545,
   "pn": 3549,
   "doi": "10.21437/Interspeech.2024-882",
   "url": "interspeech_2024/chang24_interspeech.html"
  },
  "bai24_interspeech": {
   "authors": [
    [
     "Bingsong",
     "Bai"
    ],
    [
     "Fengping",
     "Wang"
    ],
    [
     "Yingming",
     "Gao"
    ],
    [
     "Ya",
     "Li"
    ]
   ],
   "title": "SPA-SVC: Self-supervised Pitch Augmentation for Singing Voice Conversion",
   "original": "888",
   "order": 895,
   "page_count": 5,
   "abstract": [
    "Diffusion-based singing voice conversion (SVC) models have shown better synthesis quality compared to traditional methods. However, in cross-domain SVC scenarios, where there is a significant disparity in pitch between the source and target voice domains, the models tend to generate audios with hoarseness, posing challenges in achieving high-quality vocal outputs. Therefore, in this paper, we propose a Self-supervised Pitch Augmentation method for Singing Voice Conversion (SPA-SVC), which can enhance the voice quality in SVC tasks without requiring additional data or increasing model parameters. We innovatively introduce a cycle pitch shifting training strategy and Structural Similarity Index (SSIM) loss into our SVC model, effectively enhancing its performance. Experimental results on the public singing datasets M4Singer indicate that our proposed method significantly improves model performance in both general SVC scenarios and particularly in cross-domain SVC scenarios."
   ],
   "p1": 4353,
   "pn": 4357,
   "doi": "10.21437/Interspeech.2024-888",
   "url": "interspeech_2024/bai24_interspeech.html"
  },
  "li24s_interspeech": {
   "authors": [
    [
     "Song",
     "Li"
    ],
    [
     "Yongbin",
     "You"
    ],
    [
     "Xuezhi",
     "Wang"
    ],
    [
     "Zhengkun",
     "Tian"
    ],
    [
     "Ke",
     "Ding"
    ],
    [
     "Guanglu",
     "Wan"
    ]
   ],
   "title": "MSR-86K: An Evolving, Multilingual Corpus with 86,300 Hours of Transcribed Audio for Speech Recognition Research",
   "original": "890",
   "order": 258,
   "page_count": 5,
   "abstract": [
    "Recently, multilingual artificial intelligence assistants, exemplified by ChatGPT, have gained immense popularity. As a crucial gateway to human-computer interaction, multilingual automatic speech recognition (ASR) has also garnered significant attention, as evidenced by systems like Whisper. However, the proprietary nature of the training data has impeded researchers’ efforts to study multilingual ASR. This paper introduces MSR-86K, an evolving, large-scale multilingual corpus for speech recognition research. The corpus is derived from publicly accessible videos on YouTube, comprising 15 languages and a total of 86,300 hours of transcribed ASR data. We also introduce how to use the MSR-86K corpus and other open-source corpora to train a robust multilingual ASR model that is competitive with Whisper. MSR-86K will be publicly released on HuggingFace, and we believe that such a large corpus will pave new avenues for research in multilingual ASR. "
   ],
   "p1": 1245,
   "pn": 1249,
   "doi": "10.21437/Interspeech.2024-890",
   "url": "interspeech_2024/li24s_interspeech.html"
  },
  "song24_interspeech": {
   "authors": [
    [
     "Zheshu",
     "Song"
    ],
    [
     "Jianheng",
     "Zhuo"
    ],
    [
     "Yifan",
     "Yang"
    ],
    [
     "Ziyang",
     "Ma"
    ],
    [
     "Shixiong",
     "Zhang"
    ],
    [
     "Xie",
     "Chen"
    ]
   ],
   "title": "LoRA-Whisper: Parameter-Efficient and Extensible Multilingual ASR",
   "original": "892",
   "order": 810,
   "page_count": 5,
   "abstract": [
    "Recent years have witnessed significant progress in multilingual automatic speech recognition (ASR), driven by the emergence of end-to-end (E2E) models and the scaling of multilingual datasets. Despite that, two main challenges persist in multilingual ASR: language interference and the incorporation of new languages without degrading the performance of the existing ones. This paper proposes LoRA-Whisper, which incorporates LoRA matrix into Whisper for multilingual ASR, effectively mitigating language interference. Furthermore, by leveraging LoRA and the similarities between languages, we can achieve better performance on new languages while upholding consistent performance on original ones. Experiments on a real-world task across eight languages demonstrate that our proposed LoRA-Whisper yields a relative gain of 18.5% and 23.0% over the baseline system for multilingual ASR and language expansion respectively."
   ],
   "p1": 3934,
   "pn": 3938,
   "doi": "10.21437/Interspeech.2024-892",
   "url": "interspeech_2024/song24_interspeech.html"
  },
  "hu24b_interspeech": {
   "authors": [
    [
     "Na",
     "Hu"
    ],
    [
     "Hugo",
     "Schnack"
    ],
    [
     "Amalia",
     "Arvaniti"
    ]
   ],
   "title": "Automatic pitch accent classification through image classification",
   "original": "895",
   "order": 425,
   "page_count": 5,
   "abstract": [
    "The classification of pitch accents has posed significant challenges in automatic intonation labelling. Previous research primarily adopted feature-based approaches, predicting pitch accents using a finite set of features including acoustic features (F0, duration, intensity) and lexical features. In this study, we explored a novel approach, classifying pitch accents as images represented in pixels. To evaluate this method’s effectiveness, we used a relatively simple classification task involving only two types of pitch accents (H* and L+H*). The training of a basic neural network model for classifying images of these two types of accents (N= 2,025) yielded an average accuracy of 93.5% across 10 runs on the test set, showcasing the potential effectiveness of this new approach. "
   ],
   "p1": 2050,
   "pn": 2054,
   "doi": "10.21437/Interspeech.2024-895",
   "url": "interspeech_2024/hu24b_interspeech.html"
  },
  "xie24b_interspeech": {
   "authors": [
    [
     "Wei-lin",
     "Xie"
    ],
    [
     "Yu-Xuan",
     "Xi"
    ],
    [
     "Yan",
     "Song"
    ],
    [
     "Jian-tao",
     "Zhang"
    ],
    [
     "Hao-yu",
     "Song"
    ],
    [
     "Ian",
     "McLoughlin"
    ]
   ],
   "title": "DB-PMAE: Dual-Branch Prototypical Masked AutoEncoder with locality for domain robust speaker verification",
   "original": "897",
   "order": 445,
   "page_count": 5,
   "abstract": [
    "Existing speaker verification (SV) systems mainly consist of a frontend deep embedding network pretrained for speaker identification(SID) followed by a backend network finetuned to provide a similarity measure. Despite their success, the performance may degrade remarkably due to domain mismatch. In this paper, we present a novel dual-branch prototypical masked autoencoder(DB-PMAE) based SRE framework. Specifically, the teacher and student branches with siamese encoders are pretrained to jointly learn patch-level features and prototypes. A multi-task learning framework is exploited for finetuning with SID and SV tasks, where the similarity is measured by finding local correspondence to improve domain robustness. Experiments on CNCeleb corpus demonstrate the superiority of DB-PMAE."
   ],
   "p1": 2150,
   "pn": 2154,
   "doi": "10.21437/Interspeech.2024-897",
   "url": "interspeech_2024/xie24b_interspeech.html"
  },
  "oneata24_interspeech": {
   "authors": [
    [
     "Dan",
     "Oneata"
    ],
    [
     "Herman",
     "Kamper"
    ]
   ],
   "title": "Translating speech with just images",
   "original": "903",
   "order": 79,
   "page_count": 5,
   "abstract": [
    "Visually grounded speech models link speech to images. We extend this connection by linking images to text via an existing image captioning system, and as a result gain the ability to map speech audio directly to text. This approach can be used for speech translation with just images by having the audio in a different language from the generated captions. We investigate such a system on a real low-resource language, Yoruba, and propose a Yoruba-to-English speech translation model that leverages pretrained components in order to be able to learn in the low-resource regime. To limit overfitting, we find that it is essential to use a decoding scheme that produces diverse image captions for training. Results show that the predicted translations capture the main semantics of the spoken audio, albeit in a simpler and shorter form."
   ],
   "p1": 387,
   "pn": 391,
   "doi": "10.21437/Interspeech.2024-903",
   "url": "interspeech_2024/oneata24_interspeech.html"
  },
  "kummervold24_interspeech": {
   "authors": [
    [
     "Per E",
     "Kummervold"
    ],
    [
     "Javier",
     "de la Rosa"
    ],
    [
     "Freddy",
     "Wetjen"
    ],
    [
     "Rolv-Arild",
     "Braaten"
    ],
    [
     "Per Erik",
     "Solberg"
    ]
   ],
   "title": "Whispering in Norwegian: Navigating Orthographic and Dialectic Challenges",
   "original": "907",
   "resource": "https://doi.org/10.57967/hf/2716",
   "order": 820,
   "page_count": 5,
   "abstract": [
    "This paper presents NB-Whisper, a tailored adaptation of OpenAI’s Whisper model, specifically fine-tuned to address the unique challenges of Norwegian language Automatic Speech Recognition (ASR). We highlight its key contributions and summarise the results achieved in converting spoken Norwegian into written forms and translating other languages into Norwegian. By training on a 22,000 hour weakly aligned dataset, we show that we are able to improve the Norwegian Bokmål transcription by OpenAI Whisper Large-v3 from a WER of 10.4 to 6.6 on the Fleurs Dataset and from 6.8 to 2.2 on the NST dataset."
   ],
   "p1": 3984,
   "pn": 3988,
   "doi": "10.21437/Interspeech.2024-907",
   "url": "interspeech_2024/kummervold24_interspeech.html"
  },
  "ortizperez24_interspeech": {
   "authors": [
    [
     "David",
     "Ortiz-Perez"
    ],
    [
     "Jose",
     "Garcia-Rodriguez"
    ],
    [
     "David",
     "Tomás"
    ]
   ],
   "title": "Cognitive Insights Across Languages: Enhancing Multimodal Interview Analysis",
   "original": "914",
   "order": 192,
   "page_count": 5,
   "abstract": [
    "Cognitive decline is a natural process that occurs as individuals age. Early diagnosis of anomalous decline is crucial for initiating professional treatment that can enhance the quality of life of those affected. To address this issue, we propose a multimodal model capable of predicting Mild Cognitive Impairment and cognitive scores. The TAUKADIAL dataset is used to conduct the evaluation, which comprises audio recordings of clinical interviews. The proposed model demonstrates the ability to transcribe and differentiate between languages used in the interviews. Subsequently, the model extracts audio and text features, combining them into a multimodal architecture to achieve robust and generalized results. Our approach involves in-depth research to implement various features obtained from the proposed modalities."
   ],
   "p1": 952,
   "pn": 956,
   "doi": "10.21437/Interspeech.2024-914",
   "url": "interspeech_2024/ortizperez24_interspeech.html"
  },
  "mariotte24_interspeech": {
   "authors": [
    [
     "Théo",
     "Mariotte"
    ],
    [
     "Anthony",
     "Larcher"
    ],
    [
     "Silvio",
     "Montrésor"
    ],
    [
     "Jean-Hugh",
     "Thomas"
    ]
   ],
   "title": "ASoBO: Attentive Beamformer Selection for Distant Speaker Diarization in Meetings",
   "original": "917",
   "order": 333,
   "page_count": 5,
   "abstract": [
    "Speaker Diarization (SD) aims at grouping speech segments that belong to the same speaker. This task is required in many speech-processing applications, such as rich meeting transcription. In this context, distant microphone arrays usually capture the audio signal. Beamforming, i.e., spatial filtering, is a common practice to process multi-microphone audio data. However, it often requires an explicit localization of the active source to steer the filter. This paper proposes a self-attention-based algorithm to select the output of a bank of fixed spatial filters. This method serves as a feature extractor for joint Voice Activity (VAD) and Overlapped Speech Detection (OSD). The speaker diarization is then inferred from the detected segments. The approach shows convincing distant VAD, OSD, and SD performance, e.g. 14.5% DER on the AISHELL-4 dataset. The analysis of the self-attention weights demonstrates their explainability, as they correlate with the speaker's angular locations."
   ],
   "p1": 1620,
   "pn": 1624,
   "doi": "10.21437/Interspeech.2024-917",
   "url": "interspeech_2024/mariotte24_interspeech.html"
  },
  "gong24_interspeech": {
   "authors": [
    [
     "Rong",
     "Gong"
    ],
    [
     "Hongfei",
     "Xue"
    ],
    [
     "Lezhi",
     "Wang"
    ],
    [
     "Xin",
     "Xu"
    ],
    [
     "Qisheng",
     "Li"
    ],
    [
     "Lei",
     "Xie"
    ],
    [
     "Hui",
     "Bu"
    ],
    [
     "Shaomei",
     "Wu"
    ],
    [
     "Jiaming",
     "Zhou"
    ],
    [
     "Yong",
     "Qin"
    ],
    [
     "Binbin",
     "Zhang"
    ],
    [
     "Jun",
     "Du"
    ],
    [
     "Jia",
     "Bin"
    ],
    [
     "Ming",
     "Li"
    ]
   ],
   "title": "AS-70: A Mandarin stuttered speech dataset for automatic speech recognition and stuttering event detection",
   "original": "918",
   "resource": "https://doi.org/10.5281/zenodo.12738201",
   "order": 1044,
   "page_count": 5,
   "abstract": [
    "The rapid advancements in speech technologies over the past two decades have led to human-level performance in tasks like automatic speech recognition (ASR) for fluent speech. However, the efficacy of these models diminishes when applied to atypical speech, such as stuttering. This paper introduces AS-70, the first publicly available Mandarin stuttered speech dataset, which stands out as the largest dataset in its category. Encompassing conversational and voice command reading speech, AS-70 includes verbatim manual transcription, rendering it suitable for various speech-related tasks. Furthermore, baseline systems are established, and experimental results are presented for ASR and stuttering event detection (SED) tasks. By incorporating this dataset into the model fine-tuning, significant improvement in the state-of-the-art ASR models, e.g., Whisper and Hubert, are observed, enhancing their inclusivity in addressing stuttered speech."
   ],
   "p1": 5098,
   "pn": 5102,
   "doi": "10.21437/Interspeech.2024-918",
   "url": "interspeech_2024/gong24_interspeech.html"
  },
  "ranjan24_interspeech": {
   "authors": [
    [
     "Sumit",
     "Ranjan"
    ],
    [
     "Rupayan",
     "Chakraborty"
    ],
    [
     "Sunil Kumar",
     "Kopparapu"
    ]
   ],
   "title": "Reinforcement Learning based Data Augmentation for Noise Robust Speech Emotion Recognition",
   "original": "921",
   "order": 217,
   "page_count": 5,
   "abstract": [
    "Speech emotion recognition (SER) is an indispensable component of any human machine interactions, and enables building empathetic voice user interfaces. Ability to accurately recognize emotion in noisy environments is important in practical scenarios when a person is interacting with a machine or an agent as in the case of a voice based call center. In this paper, we propose reinforcement learning (RL) based data augmentation technique to enable building a robust SER system. The reward function used in RL enables picking selective noises spread over different frequency bands for data augmentation. We show that the proposed RL based augmentation technique is superior to a recently proposed random selection based technique for the noise robust SER task. We use IEMOCAP dataset with four emotion classes for validating the proposed technique. Moreover, we test the noise robustness of SER system in both cross-corpus and cross-language scenarios.                   "
   ],
   "p1": 1040,
   "pn": 1044,
   "doi": "10.21437/Interspeech.2024-921",
   "url": "interspeech_2024/ranjan24_interspeech.html"
  },
  "rahou24_interspeech": {
   "authors": [
    [
     "Bilal",
     "Rahou"
    ],
    [
     "Hervé",
     "Bredin"
    ]
   ],
   "title": "Multi-latency look-ahead for streaming speaker segmentation",
   "original": "923",
   "order": 331,
   "page_count": 5,
   "abstract": [
    "We address the task of streaming speaker diarization and propose several contributions to achieve a better trade-off between latency and accuracy. First, computational latency is reduced to its bare minimum by switching to a causal frame-wise speaker segmentation architecture. Then, a multi-latency look-ahead mechanism is used during training to support adaptive latency during inference at no additional computational cost. Finally, we detail the method used during inference to achieve the final frame-wise segmentation. We evaluate the impact of these contributions on the AMI meeting dataset with a focus on the speaker segmentation step, seen through the prism of voice activity detection, overlapped speech detection and speaker change detection."
   ],
   "p1": 1610,
   "pn": 1614,
   "doi": "10.21437/Interspeech.2024-923",
   "url": "interspeech_2024/rahou24_interspeech.html"
  },
  "kanagawa24b_interspeech": {
   "authors": [
    [
     "Hiroki",
     "Kanagawa"
    ],
    [
     "Yusuke",
     "Ijima"
    ]
   ],
   "title": "Knowledge Distillation from Self-Supervised Representation Learning Model with Discrete Speech Units for Any-to-Any Streaming Voice Conversion",
   "original": "924",
   "order": 903,
   "page_count": 5,
   "abstract": [
    "SSL models like HuBERT and WavLM serve as effective content encoders for non-parallel voice conversion (VC), but their large size and design for offline operation make streaming use a challenge. Thus, we derive novel lightweight streaming VC using knowledge distillation (KD) from the SSL model. A promising SSL model and its vector quantizer are used as the teacher content encoder. The student content encoder predicts discrete content from the teacher, ensuring consistency within the KD framework. To stabilize the converted speech's prosody, a prosody predictor using content and speaker information is employed. A HiFi-GAN-like decoder generates waveforms from speaker, content, and prosody inputs. Our student VC leverages the SSL model's robust content encoding without relying on it for inferencing, enabling streaming operation. Evaluations on any-to-any VC tasks show our approach achieved naturalness comparable to modern offline VCs and the teacher with SSL model while being streamable."
   ],
   "p1": 4393,
   "pn": 4397,
   "doi": "10.21437/Interspeech.2024-924",
   "url": "interspeech_2024/kanagawa24b_interspeech.html"
  },
  "hu24c_interspeech": {
   "authors": [
    [
     "Yuchen",
     "Hu"
    ],
    [
     "Chen",
     "Chen"
    ],
    [
     "Ruizhe",
     "Li"
    ],
    [
     "Qiushi",
     "Zhu"
    ],
    [
     "Eng Siong",
     "Chng"
    ]
   ],
   "title": "Noise-aware Speech Enhancement using Diffusion Probabilistic Model",
   "original": "929",
   "order": 460,
   "page_count": 5,
   "abstract": [
    "With recent advances of diffusion model, generative speech enhancement (SE) has attracted a surge of research interest due to its great potential for unseen testing noises. However, existing efforts mainly focus on inherent properties of clean speech, underexploiting the varying noise information in real world. In this paper, we propose a noise-aware speech enhancement (NASE) approach that extracts noise-specific information to guide the reverse process in diffusion model. Specifically, we design a noise classification (NC) model to produce acoustic embedding as a noise conditioner to guide the reverse denoising process. Meanwhile, a multi-task learning scheme is devised to jointly optimize SE and NC tasks to enhance the noise specificity of conditioner. NASE is shown to be a plug-and-play module that can be generalized to any diffusion SE models. Experiments on VB-DEMAND dataset show that NASE effectively improves multiple mainstream diffusion SE models, especially on unseen noises."
   ],
   "p1": 2225,
   "pn": 2229,
   "doi": "10.21437/Interspeech.2024-929",
   "url": "interspeech_2024/hu24c_interspeech.html"
  },
  "chen24n_interspeech": {
   "authors": [
    [
     "Jinming",
     "Chen"
    ],
    [
     "Jingyi",
     "Fang"
    ],
    [
     "Yuanzhong",
     "Zheng"
    ],
    [
     "Yaoxuan",
     "Wang"
    ],
    [
     "Haojun",
     "Fei"
    ]
   ],
   "title": "Qifusion-Net: Layer-adapted Stream/Non-stream Model for End-to-End Multi-Accent Speech Recognition",
   "original": "930",
   "order": 494,
   "page_count": 5,
   "abstract": [
    " Currently, end-to-end (E2E) speech recognition methods have achieved promising performance. However, auto speech recognition (ASR) models still face challenges in recognizing multi-accent speech accurately. We propose a layer-adapted fusion (LAF) model, called Qifusion-Net, which does not require any prior knowledge about the target accent. Based on dynamic chunk strategy, our approach enables streaming decoding and can extract frame-level acoustic feature, facilitating fine-grained information fusion. Experiment results demonstrate that our proposed methods outperform the baseline with relative reductions of 22.1% and  17.2% in character error rate (CER) across multi accent test datasets on KeSpeech and MagicData-RMAC."
   ],
   "p1": 2395,
   "pn": 2399,
   "doi": "10.21437/Interspeech.2024-930",
   "url": "interspeech_2024/chen24n_interspeech.html"
  },
  "wu24h_interspeech": {
   "authors": [
    [
     "Tianci",
     "Wu"
    ],
    [
     "Shulin",
     "He"
    ],
    [
     "Jiahui",
     "Pan"
    ],
    [
     "Haifeng",
     "Huang"
    ],
    [
     "Zhijian",
     "Mo"
    ],
    [
     "Xueliang",
     "Zhang"
    ]
   ],
   "title": "Unified Audio Visual Cues for Target Speaker Extraction",
   "original": "934",
   "order": 893,
   "page_count": 5,
   "abstract": [
    "The target speaker extraction aims to isolate the target speaker's speech from other interfering speakers. Typically, an auxiliary reference, such as a pre-recorded speech or lip movements, is vital to direct attention to the target speaker. Existing methods use one of these cues or fuse both via attention mechanisms, yielding a shared feature of the target speaker. While both cues represent the same speaker, they have distinct attributes. The audio cue registers the speaker's timbre, but lip movements illustrate the synchrony. To blend the strengths of different cues, we propose a unified TSE network termed Uni-Net that employs a divide-and-conquer strategy to fuse audio and lip cues into distinct networks, capitalizing on each cue's unique information. Speech extracted from various cues acts as prior information, further refined by the post-processing network. We conducted the experiments on the public VoxCeleb2 corpus and Uni-Net achieves SOTA performance compared with baselines."
   ],
   "p1": 4343,
   "pn": 4347,
   "doi": "10.21437/Interspeech.2024-934",
   "url": "interspeech_2024/wu24h_interspeech.html"
  },
  "wang24s_interspeech": {
   "authors": [
    [
     "Hui",
     "Wang"
    ],
    [
     "Shiwan",
     "Zhao"
    ],
    [
     "Jiaming",
     "Zhou"
    ],
    [
     "Xiguang",
     "Zheng"
    ],
    [
     "Haoqin",
     "Sun"
    ],
    [
     "Xuechen",
     "Wang"
    ],
    [
     "Yong",
     "Qin"
    ]
   ],
   "title": "Uncertainty-Aware Mean Opinion Score Prediction",
   "original": "937",
   "order": 252,
   "page_count": 5,
   "abstract": [
    "Mean Opinion Score (MOS) prediction has made significant progress in specific domains. However, the unstable performance of MOS prediction models across diverse samples presents ongoing challenges in the practical application of these systems. In this paper, we point out that the absence of uncertainty modeling is a significant limitation hindering MOS prediction systems from applying to the real and open world. We analyze the sources of uncertainty in the MOS prediction task and propose to establish an uncertainty-aware MOS prediction system that models aleatory uncertainty and epistemic uncertainty by heteroscedastic regression and Monte Carlo dropout separately. The experimental results show that the system captures uncertainty well and is capable of performing selective prediction and out-of-domain detection. Such capabilities significantly enhance the practical utility of MOS systems in diverse real and open-world environments."
   ],
   "p1": 1215,
   "pn": 1219,
   "doi": "10.21437/Interspeech.2024-937",
   "url": "interspeech_2024/wang24s_interspeech.html"
  },
  "zanonboito24_interspeech": {
   "authors": [
    [
     "Marcely",
     "Zanon Boito"
    ],
    [
     "Vivek",
     "Iyer"
    ],
    [
     "Nikolaos",
     "Lagos"
    ],
    [
     "Laurent",
     "Besacier"
    ],
    [
     "Ioan",
     "Calapodescu"
    ]
   ],
   "title": "mHuBERT-147: A Compact Multilingual HuBERT Model",
   "original": "938",
   "order": 811,
   "page_count": 5,
   "abstract": [
    "We present mHuBERT-147, the first general-purpose massively multilingual HuBERT speech representation model trained on 90K hours of clean, open-license data. To scale up the multi-iteration HuBERT approach, we use faiss-based clustering, achieving 5.2x faster label assignment than the original method. We also apply a new multilingual batching up-sampling strategy, leveraging both language and dataset diversity. After 3 training iterations, our compact 95M parameter mHuBERT-147 outperforms larger models trained on substantially more data. We rank second and first on the ML-SUPERB 10min and 1h leaderboards, with SOTA scores for 3 tasks. Across ASR/LID tasks, our model consistently surpasses XLS-R (300M params; 436K hours) and demonstrates strong competitiveness against the much larger MMS (1B params; 491K hours). Our findings indicate that mHuBERT-147 is a promising model for multilingual speech tasks, offering an unprecedented balance between high performance and parameter efficiency."
   ],
   "p1": 3939,
   "pn": 3943,
   "doi": "10.21437/Interspeech.2024-938",
   "url": "interspeech_2024/zanonboito24_interspeech.html"
  },
  "kim24j_interspeech": {
   "authors": [
    [
     "Seung-bin",
     "Kim"
    ],
    [
     "Chan-yeong",
     "Lim"
    ],
    [
     "Jungwoo",
     "Heo"
    ],
    [
     "Ju-ho",
     "Kim"
    ],
    [
     "Hyun-seo",
     "Shin"
    ],
    [
     "Kyo-Won",
     "Koo"
    ],
    [
     "Ha-Jin",
     "Yu"
    ]
   ],
   "title": "MR-RawNet: Speaker verification system with multiple temporal resolutions for variable duration utterances using raw waveforms",
   "original": "939",
   "resource": "https://doi.org/10.48550/arXiv.2406.07103",
   "order": 440,
   "page_count": 5,
   "abstract": [
    "In speaker verification systems, the utilization of short utterances presents a persistent challenge, leading to performance degradation primarily due to insufficient phonetic information to characterize the speakers. To overcome this obstacle, we propose a novel structure, MR-RawNet, designed to enhance the robustness of speaker verification systems against variable duration utterances using raw waveforms. The MR-RawNet extracts time-frequency representations from raw waveforms via a multi-resolution feature extractor that optimally adjusts both temporal and spectral resolutions simultaneously. Furthermore, we apply a multi-resolution attention block that focuses on diverse and extensive temporal contexts, ensuring robustness against changes in utterance length. The experimental results, conducted on VoxCeleb1 dataset, demonstrate that the MR-RawNet exhibits superior performance in handling utterances of variable duration compared to other raw waveform-based systems."
   ],
   "p1": 2125,
   "pn": 2129,
   "doi": "10.21437/Interspeech.2024-939",
   "url": "interspeech_2024/kim24j_interspeech.html"
  },
  "porjazovski24_interspeech": {
   "authors": [
    [
     "Dejan",
     "Porjazovski"
    ],
    [
     "Anssi",
     "Moisio"
    ],
    [
     "Mikko",
     "Kurimo"
    ]
   ],
   "title": "Out-of-distribution generalisation in spoken language understanding",
   "original": "940",
   "resource": "https://doi.org/10.5281/zenodo.12742503",
   "order": 163,
   "page_count": 5,
   "abstract": [
    "Test data is said to be out-of-distribution (OOD) when it unexpectedly differs from the training data, a common challenge in real-world use cases of machine learning. Although OOD generalisation has gained interest in recent years, few works have focused on OOD generalisation in spoken language understanding (SLU) tasks. To facilitate research on this topic, we introduce a modified version of the popular SLU dataset SLURP, featuring data splits for testing OOD generalisation in the SLU task. We call our modified dataset SLURP For OOD generalisation, or SLURPFOOD. Utilising our OOD data splits, we find end-to-end SLU models to have limited capacity for generalisation. Furthermore, by employing model interpretability techniques, we shed light on the factors contributing to the generalisation difficulties of the models. To improve the generalisation, we experiment with two techniques, which improve the results on some, but not all the splits, emphasising the need for new techniques."
   ],
   "p1": 807,
   "pn": 811,
   "doi": "10.21437/Interspeech.2024-940",
   "url": "interspeech_2024/porjazovski24_interspeech.html"
  },
  "martindonas24_interspeech": {
   "authors": [
    [
     "Juan M.",
     "Martín-Doñas"
    ],
    [
     "Aitor",
     "Álvarez"
    ],
    [
     "Eros",
     "Rosello"
    ],
    [
     "Angel M.",
     "Gomez"
    ],
    [
     "Antonio M.",
     "Peinado"
    ]
   ],
   "title": "Exploring Self-supervised Embeddings and Synthetic Data Augmentation for Robust Audio Deepfake Detection",
   "original": "942",
   "order": 432,
   "page_count": 5,
   "abstract": [
    "This work explores the performance of large speech self-supervised models as robust audio deepfake detectors. Despite the current trend of fine-tuning the upstream network, in this paper, we revisit the use of pre-trained models as feature extractors to adapt specialized downstream audio deepfake classifiers. The goal is to keep the general knowledge of the audio foundation model to extract discriminative features to feed up a simplified deepfake classifier. In addition, the generalization capabilities of the system are improved by augmenting the training corpora using additional synthetic data from different vocoder algorithms. This strategy is also complemented by various data augmentations covering challenging acoustic conditions. Our proposal is evaluated under different benchmark datasets for audio deepfake and anti-spoofing tasks, showing state-of-the-art performance. Furthermore, we analyze the relevant parts of the downstream classifier to achieve a robust system."
   ],
   "p1": 2085,
   "pn": 2089,
   "doi": "10.21437/Interspeech.2024-942",
   "url": "interspeech_2024/martindonas24_interspeech.html"
  },
  "tanaka24_interspeech": {
   "authors": [
    [
     "Kou",
     "Tanaka"
    ],
    [
     "Hirokazu",
     "Kameoka"
    ],
    [
     "Takuhiro",
     "Kaneko"
    ],
    [
     "Yuto",
     "Kondo"
    ]
   ],
   "title": "PRVAE-VC2: Non-Parallel Voice Conversion by Distillation of Speech Representations",
   "original": "947",
   "order": 897,
   "page_count": 5,
   "abstract": [
    "This paper describes a knowledge distillation approach to non-parallel many-to-many voice conversion (VC) using self-supervised speech representation techniques: perturbation-resistant variational autoencoder (PRVAE) and a hidden-unit BERT (HuBERT). PRVAE has achieved a breakthrough by significantly improving non-streaming and low-latency streaming VC performances. However, a notable gap persists between the real target and converted speech, posing a continuing challenge. To narrow this gap, we present PRVAE-VC2, an improved version of PRVAE-VC that leverages rich and contextually informed representations derived from pre-trained HuBERT. Furthermore, we apply knowledge distillation techniques to make PRVAE-VC2 a streamable method, ensuring that the advantage of PRVAE-VC as a streamable VC is not compromised. Evaluation results demonstrate that our approaches effectively reduce discrepancies between the converted speech and the target. Audio samples can be accessed at our webpage."
   ],
   "p1": 4363,
   "pn": 4367,
   "doi": "10.21437/Interspeech.2024-947",
   "url": "interspeech_2024/tanaka24_interspeech.html"
  },
  "rezackova24_interspeech": {
   "authors": [
    [
     "Markéta",
     "Řezáčková"
    ],
    [
     "Daniel",
     "Tihelka"
    ],
    [
     "Jindřich",
     "Matoušek"
    ]
   ],
   "title": "Homograph Disambiguation with Text-to-Text Transfer Transformer",
   "original": "949",
   "order": 573,
   "page_count": 5,
   "abstract": [
    "In recent years, the Text-to-Text Transfer Transformer (T5) neural model has proved very powerful in many text-to-text tasks, including text normalization and grapheme-to-phoneme conversion. In the presented paper, we fine-tuned the T5 model for the task of homograph disambiguation, which is one of the essential components of text-to-speech (TTS) systems. To compare our results to those of other studies, we used an online dataset of US English homographs called Wikipedia Homograph Data. We present our results, which outperformed the previously published single-model approaches. We also focus on more detailed error analysis, model performance on different types of homographs, and the impact of training set size on homograph disambiguation."
   ],
   "p1": 2785,
   "pn": 2789,
   "doi": "10.21437/Interspeech.2024-949",
   "url": "interspeech_2024/rezackova24_interspeech.html"
  },
  "guillaume24_interspeech": {
   "authors": [
    [
     "Séverine",
     "Guillaume"
    ],
    [
     "Maxime",
     "Fily"
    ],
    [
     "Alexis",
     "Michaud"
    ],
    [
     "Guillaume",
     "Wisniewski"
    ]
   ],
   "title": "Gender and Language Identification in Multilingual Models of Speech: Exploring the Genericity and Robustness of Speech Representations",
   "original": "953",
   "order": 682,
   "page_count": 5,
   "abstract": [
    "Models such as XLS-R and UniSpeech have proven effective in speech processing across diverse languages, even with limited annotated data, enabling, for instance, the development of transcription systems for some under-documented languages. This work aims to test the hypothesis that these models can build “generic” representations of an audio snippet that do not depend on characteristics that are irrelevant to understanding the message conveyed. Through two sets of experiments, we assess their ability to abstract away from speaker-specific details and distill core informational contents — in an informational-communicational sense to be refined further: all the information contained in the audio signal that contributes evidence on the speaker's communicative intent. The results of our experiments show that pre-trained  models of speech such as XLS-R do not necessarily encode information in the same way, depending on the speaker's gender."
   ],
   "p1": 3330,
   "pn": 3334,
   "doi": "10.21437/Interspeech.2024-953",
   "url": "interspeech_2024/guillaume24_interspeech.html"
  },
  "lee24i_interspeech": {
   "authors": [
    [
     "Beomseok",
     "Lee"
    ],
    [
     "Ioan",
     "Calapodescu"
    ],
    [
     "Marco",
     "Gaido"
    ],
    [
     "Matteo",
     "Negri"
    ],
    [
     "Laurent",
     "Besacier"
    ]
   ],
   "title": "Speech-MASSIVE: A Multilingual Speech Dataset for SLU and Beyond",
   "original": "957",
   "order": 165,
   "page_count": 5,
   "abstract": [
    "We present Speech-MASSIVE, a multilingual Spoken Language Understanding (SLU) dataset comprising the speech counterpart for a portion of the MASSIVE textual corpus. Speech-MASSIVE covers 12 languages from different families and inherits from MASSIVE the annotations for the intent prediction and slot-filling tasks. Our extension is prompted by the scarcity of massively multilingual SLU datasets and the growing need for versatile speech datasets to assess foundation models (LLMs, speech encoders) across languages and tasks. We provide a multimodal, multitask, multilingual dataset and report SLU baselines using both cascaded and end-to-end architectures in various training scenarios (zero-shot, few-shot, and full fine-tune). Furthermore, we demonstrate the suitability of Speech-MASSIVE for benchmarking other tasks such as speech transcription, language identification, and speech translation. The dataset, models, and code are publicly available at: https://github.com/hlt-mt/Speech-MASSIVE"
   ],
   "p1": 817,
   "pn": 821,
   "doi": "10.21437/Interspeech.2024-957",
   "url": "interspeech_2024/lee24i_interspeech.html"
  },
  "cheng24_interspeech": {
   "authors": [
    [
     "Longbiao",
     "Cheng"
    ],
    [
     "Ashutosh",
     "Pandey"
    ],
    [
     "Buye",
     "Xu"
    ],
    [
     "Tobi",
     "Delbruck"
    ],
    [
     "Shih-Chii",
     "Liu"
    ]
   ],
   "title": "Dynamic Gated Recurrent Neural Network for Compute-efficient Speech Enhancement",
   "original": "958",
   "order": 137,
   "page_count": 5,
   "abstract": [
    "This paper introduces a new Dynamic Gated Recurrent Neural Network (DG-RNN) for compute-efficient speech enhancement models running on resource-constrained hardware platforms. It leverages the slow evolution characteristic of RNN hidden states over steps, and updates only a selected set of neurons at each step by adding a newly proposed select gate to the RNN model. This select gate allows the computation cost of the conventional RNN to be reduced during network inference. As a realization of the DG-RNN, we further propose the Dynamic Gated Recurrent Unit (D-GRU) which does not require additional parameters. Test results obtained from several state-of-the-art compute-efficient RNN-based speech enhancement architectures using the DNS challenge dataset, show that the D-GRU based model variants maintain similar speech intelligibility and quality metrics comparable to the baseline GRU based models even with an average 50% reduction in GRU computes."
   ],
   "p1": 677,
   "pn": 681,
   "doi": "10.21437/Interspeech.2024-958",
   "url": "interspeech_2024/cheng24_interspeech.html"
  },
  "saget24_interspeech": {
   "authors": [
    [
     "Félix",
     "Saget"
    ],
    [
     "Meysam",
     "Shamsi"
    ],
    [
     "Marie",
     "Tahon"
    ]
   ],
   "title": "Lifelong Learning MOS Prediction for Synthetic Speech Quality Evaluation",
   "original": "959",
   "order": 253,
   "page_count": 5,
   "abstract": [
    "Mean Opinion Score (MOS) has been a long-standing standard for perceptive evaluation of quality of speech synthesis models; however, this criterion is hardly reproducible, and costly. Automatic, neural MOS predictors have emerged as a solution to the objective assessment of synthetic speech. These predictors are trained once on data collected from past listening tests, and thus may suffer from adaptation to new technology breakthrough in speech synthesis. In this study, we investigate the applicability of lifelong learning for MOS predictors, where the training samples would be fed to the model in the chronological order. A sequential lifelong mode and a cumulative lifelong mode have been compared with traditional batch training using the BVCC and Blizzard Challenge datasets. The experiments show the advantages of lifelong learning in cross-corpus evaluation as well as in a constrained data availability scenario."
   ],
   "p1": 1220,
   "pn": 1224,
   "doi": "10.21437/Interspeech.2024-959",
   "url": "interspeech_2024/saget24_interspeech.html"
  },
  "huckvale24_interspeech": {
   "authors": [
    [
     "Mark",
     "Huckvale"
    ],
    [
     "Gaston",
     "Hilkhuysen"
    ]
   ],
   "title": "Evaluating a 3-factor listener model for prediction of speech intelligibility to hearing-impaired listeners",
   "original": "961",
   "order": 176,
   "page_count": 5,
   "abstract": [
    "A speech intelligibility prediction model for hearing impaired listeners would be useful in the development of better signal enhancement methods and for the fitting of hearing aids. Most current prediction models use only information from a pure-tone audiogram to characterise impaired listeners, although evidence suggests that listeners vary in ways not captured by pure-tone thresholds. In this paper we evaluate a model in which each listener is described by three factors: average pure-tone thresholds, sensitivity to phonetic distortion and sensitivity to word likelihood. We build and evaluate the model using the corpus collected by the second Clarity Prediction Challenge, which contains over 13,000 intelligibility judgments by 31 hearing impaired listeners. We describe how the factors were estimated and test their independence. We show that incorporating the listener-dependent factors into an existing intelligibility metric can improve the accuracy of prediction on held-out test data with a 9.8% relative improvement in prediction error."
   ],
   "p1": 872,
   "pn": 876,
   "doi": "10.21437/Interspeech.2024-961",
   "url": "interspeech_2024/huckvale24_interspeech.html"
  },
  "baroudi24_interspeech": {
   "authors": [
    [
     "Séverin",
     "Baroudi"
    ],
    [
     "Thomas",
     "Pellegrini"
    ],
    [
     "Hervé",
     "Bredin"
    ]
   ],
   "title": "Specializing Self-Supervised Speech Representations for Speaker Segmentation",
   "original": "962",
   "order": 777,
   "page_count": 5,
   "abstract": [
    "Self-supervised speech representation learning has been shown to be very effective for a wide range of speech processing downstream tasks. However, most of these models have been pretrained using clean pre-segmented single-speaker utterances, which is not representative of tasks involving realistic multi-speaker conversational speech like speaker diarization. WavLM pretraining mitigates this domain mismatch using artificial mixtures of single-speaker utterances, and outperforms other pretrained models such as wav2vec2 or HuBERT for speaker diarization. We propose to further specialize WavLM for speaker diarization in two ways: pretraining on real-world multi-speaker conversational speech, and crafting targets of pretraining pretext task to benefit the most to speaker diarization. When finetuned with recently proposed powerset multi-class cross entropy loss, we outperform, often by a large margin, the state-of-the-art on most speaker diarization benchmarks."
   ],
   "p1": 3769,
   "pn": 3773,
   "doi": "10.21437/Interspeech.2024-962",
   "url": "interspeech_2024/baroudi24_interspeech.html"
  },
  "gong24b_interspeech": {
   "authors": [
    [
     "Xun",
     "Gong"
    ],
    [
     "Anqi",
     "Lv"
    ],
    [
     "Zhiming",
     "Wang"
    ],
    [
     "Yanmin",
     "Qian"
    ]
   ],
   "title": "Contextual Biasing Speech Recognition in Speech-enhanced Large Language Model",
   "original": "965",
   "order": 53,
   "page_count": 5,
   "abstract": [
    "Recently, the rapid advancements in audio- and speech-enhanced large language models (SpeechLLMs), such as Qwen-Audio and SALMONN, have significantly propelled automatic speech recognition (ASR) forward. However, despite the improvements in universal recognition capabilities, bias word recognition persists as a prominent challenge for SpeechLLM, and is not extensively studied. In this study, we introduce two contextual biasing strategies aimed at improving the bias word recognition of SpeechLLM. Firstly, we explored two types of biasing prompts for SpeechLLMs, achieving 10% relative reduction in bias word error rate (WER). However, as the size of the bias list increased, performance significantly declined due to hallucination. Subsequently, we built the biasing fusion network for SpeechLLM that integrates high-level bias embeddings with the SpeechLLM framework. Our experiments conducted on the LibriSpeech test-clean/-other datasets demonstrate that our method achieves up to 10%/35% relative reduction in overall/bias WER compared to our baseline."
   ],
   "p1": 257,
   "pn": 261,
   "doi": "10.21437/Interspeech.2024-965",
   "url": "interspeech_2024/gong24b_interspeech.html"
  },
  "li24t_interspeech": {
   "authors": [
    [
     "Yangze",
     "Li"
    ],
    [
     "Xiong",
     "Wang"
    ],
    [
     "Songjun",
     "Cao"
    ],
    [
     "Yike",
     "Zhang"
    ],
    [
     "Long",
     "Ma"
    ],
    [
     "Lei",
     "Xie"
    ]
   ],
   "title": "A Transcription Prompt-based Efficient Audio Large Language Model for Robust Speech Recognition",
   "original": "968",
   "order": 390,
   "page_count": 5,
   "abstract": [
    " Audio-LLM introduces audio modality into a large language model (LLM) to enable a powerful LLM to recognize, understand, and generate audio. However, during speech recognition in noisy environments, we observed the presence of illusions and repetition issues in audio-LLM, leading to substitution and insertion errors. This paper proposes a transcription prompt-based audio-LLM by introducing an ASR expert as a transcription tokenizer and a hybrid Autoregressive (AR) Non-autoregressive (NAR) decoding approach to solve the above problems. Experiments on 10k-hour WenetSpeech Mandarin corpus show that our approach decreases 12.2% and 9.6% CER relatively on Test_Net and Test_Meeting evaluation sets compared with baseline. Notably, we reduce the decoding repetition rate on the evaluation set to zero, showing that the decoding repetition problem has been solved fundamentally."
   ],
   "p1": 1905,
   "pn": 1909,
   "doi": "10.21437/Interspeech.2024-968",
   "url": "interspeech_2024/li24t_interspeech.html"
  },
  "gong24c_interspeech": {
   "authors": [
    [
     "Cheng",
     "Gong"
    ],
    [
     "Erica",
     "Cooper"
    ],
    [
     "Xin",
     "Wang"
    ],
    [
     "Chunyu",
     "Qiang"
    ],
    [
     "Mengzhe",
     "Geng"
    ],
    [
     "Dan",
     "Wells"
    ],
    [
     "Longbiao",
     "Wang"
    ],
    [
     "Jianwu",
     "Dang"
    ],
    [
     "Marc",
     "Tessier"
    ],
    [
     "Aidan",
     "Pine"
    ],
    [
     "Korin",
     "Richmond"
    ],
    [
     "Junichi",
     "Yamagishi"
    ]
   ],
   "title": "An Initial Investigation of Language Adaptation for TTS Systems under Low-resource Scenarios",
   "original": "969",
   "order": 1017,
   "page_count": 5,
   "abstract": [
    "Self-supervised learning (SSL) representations from massively multilingual models offer a promising solution for low-resource language speech tasks. Despite advancements, language adaptation in TTS systems remains an open problem. This paper explores the language adaptation capability of ZMM-TTS, a recent SSL-based multilingual TTS system proposed in our previous work. We conducted experiments on 12 languages using limited data with various fine-tuning configurations. We demonstrate that the similarity in phonetics between the pre-training and target languages, as well as the language category, affects the target language’s adaptation performance. Additionally, we find that the fine-tuning dataset size and number of speakers influence adaptability. Surprisingly, we also observed that using paired data for fine-tuning is not always optimal compared to audio-only data. Beyond speech intelligibility, our analysis covers speaker similarity, language identification, and predicted MOS."
   ],
   "p1": 4963,
   "pn": 4967,
   "doi": "10.21437/Interspeech.2024-969",
   "url": "interspeech_2024/gong24c_interspeech.html"
  },
  "meng24c_interspeech": {
   "authors": [
    [
     "Lingwei",
     "Meng"
    ],
    [
     "Jiawen",
     "Kang"
    ],
    [
     "Yuejiao",
     "Wang"
    ],
    [
     "Zengrui",
     "Jin"
    ],
    [
     "Xixin",
     "Wu"
    ],
    [
     "Xunying",
     "Liu"
    ],
    [
     "Helen",
     "Meng"
    ]
   ],
   "title": "Empowering Whisper as a Joint Multi-Talker and Target-Talker Speech Recognition System",
   "original": "971",
   "order": 955,
   "page_count": 5,
   "abstract": [
    "Multi-talker speech recognition and target-talker speech recognition, both involve transcription in multi-talker contexts, remain significant challenges. However, existing methods rarely attempt to simultaneously address both tasks. In this study, we propose a pioneering approach to empower Whisper, which is a speech foundation model, to tackle joint multi-talker and target-talker speech recognition tasks. Specifically, (i) we freeze Whisper and plug a Sidecar separator into its encoder to separate mixed embedding for multiple talkers; (ii) a Target Talker Identifier is introduced to identify the embedding flow of the target talker on the fly, requiring only three-second enrollment speech as a cue; (iii) soft prompt tuning for decoder is explored for better task adaptation. Our method outperforms previous methods on two- and three-talker LibriMix and LibriSpeechMix datasets for both tasks, and delivers acceptable zero-shot performance on multi-talker ASR on AishellMix Mandarin dataset."
   ],
   "p1": 4653,
   "pn": 4657,
   "doi": "10.21437/Interspeech.2024-971",
   "url": "interspeech_2024/meng24c_interspeech.html"
  },
  "igarashi24_interspeech": {
   "authors": [
    [
     "Takuto",
     "Igarashi"
    ],
    [
     "Yuki",
     "Saito"
    ],
    [
     "Kentaro",
     "Seki"
    ],
    [
     "Shinnosuke",
     "Takamichi"
    ],
    [
     "Ryuichi",
     "Yamamoto"
    ],
    [
     "Kentaro",
     "Tachibana"
    ],
    [
     "Hiroshi",
     "Saruwatari"
    ]
   ],
   "title": "Noise-Robust Voice Conversion by Conditional Denoising Training Using Latent Variables of Recording Quality and Environment",
   "original": "972",
   "order": 566,
   "page_count": 5,
   "abstract": [
    "We propose noise-robust voice conversion (VC) which takes into account the recording quality and environment of noisy source speech. Conventional denoising training improves the noise robustness of a VC model by learning noisy-to-clean VC process. However, the naturalness of the converted speech is limited when the noise of the source speech is unseen during the training. To this end, our proposed training conditions a VC model on two latent variables representing the recording quality and environment of the source speech. These latent variables are derived from deep neural networks pre-trained on recording quality assessment and acoustic scene classification and calculated in an utterance-wise or frame-wise manner. As a result, the trained VC model can explicitly learn information about speech degradation during the training. Objective and subjective evaluations show that our training improves the quality of the converted speech compared to the conventional training."
   ],
   "p1": 2750,
   "pn": 2754,
   "doi": "10.21437/Interspeech.2024-972",
   "url": "interspeech_2024/igarashi24_interspeech.html"
  },
  "li24u_interspeech": {
   "authors": [
    [
     "Zuoliang",
     "Li"
    ],
    [
     "Wu",
     "Guo"
    ],
    [
     "Bin",
     "Gu"
    ],
    [
     "Shengyu",
     "Peng"
    ],
    [
     "Jie",
     "Zhang"
    ]
   ],
   "title": "Contrastive Learning and Inter-Speaker Distribution Alignment Based Unsupervised Domain Adaptation for Robust Speaker Verification",
   "original": "973",
   "order": 782,
   "page_count": 5,
   "abstract": [
    "Unsupervised domain adaptation (UDA) can tackle the mismatch between the source and target domains for real-world speaker verification applications. In this paper, we propose an UDA method by leveraging the target-domain data through a self-supervised method. Firstly, we use momentum contrastive learning to effectively utilize the latent speaker labels in the target domain, enhancing intra-speaker compactness and inter-speaker separability simultaneously. Secondly, we improve the inter-speaker feature distribution alignment loss, ensuring the stability of the source-domain statistics and mitigating the impact of false negative pairs. These two methods are further combined with conventional supervised learning in the source domain. Using Voxceleb2 as the source domain and CN-Celeb1 as the target domain, experimental results demonstrate the effectiveness of our proposed method."
   ],
   "p1": 3794,
   "pn": 3798,
   "doi": "10.21437/Interspeech.2024-973",
   "url": "interspeech_2024/li24u_interspeech.html"
  },
  "demir24_interspeech": {
   "authors": [
    [
     "Kubilay Can",
     "Demir"
    ],
    [
     "Belén",
     "Lojo Rodríguez"
    ],
    [
     "Tobias",
     "Weise"
    ],
    [
     "Andreas",
     "Maier"
    ],
    [
     "Seung Hee",
     "Yang"
    ]
   ],
   "title": "Towards Intelligent Speech Assistants in Operating Rooms: A Multimodal Model for Surgical Workflow Analysis",
   "original": "975",
   "order": 302,
   "page_count": 5,
   "abstract": [
    "To develop intelligent speech assistants and integrate them seamlessly with intra-operative decision-support frameworks, accurate and efficient surgical phase recognition is a prerequisite. In this study, we propose a multimodal framework based on Gated Multimodal Units (GMU) and Multi-Stage Temporal Convolutional Networks (MS-TCN) to recognize surgical phases of port-catheter placement operations. Our method merges speech and image models and uses them separately in different surgical phases. Based on the evaluation of 28 operations, we report a frame-wise accuracy of 92.65 ± 3.52% and an F1-score of 92.30 ± 3.82%. Our results show approximately 10% improvement in both metrics over previous work and validate the effectiveness of integrating multimodal data for the surgical phase recognition task. We further investigate the contribution of individual data channels by comparing mono-modal models with multimodal models."
   ],
   "p1": 1465,
   "pn": 1469,
   "doi": "10.21437/Interspeech.2024-975",
   "url": "interspeech_2024/demir24_interspeech.html"
  },
  "kim24k_interspeech": {
   "authors": [
    [
     "Dong-Hyun",
     "Kim"
    ],
    [
     "Joon-Hyuk",
     "Chang"
    ]
   ],
   "title": "Mitigating Overfitting in Structured Pruning of ASR Models with Gradient-Guided Parameter Regularization",
   "original": "976",
   "order": 923,
   "page_count": 5,
   "abstract": [
    "Recent advancements in automatic speech recognition such as Wav2vec 2.0 and Whisper, confront deployment challenges due to their substantial model parameters. Model compression through joint distillation and structured pruning emerges as an effective solution but still faces overfitting and catastrophic forgetting, exacerbated by domain shifts or limited data availability. To address this issue, we propose the gradient-guided parameter regularization method aimed at maintaining the model's generality. Our approach employs gradient values to detect overfit-prone parameters in the student model and subsequently regularize these parameters to align closely with their counterparts in the teacher model. Through extensive experiments, we demonstrate the efficacy of our approach in reducing overfitting and enhancing performance, especially in scenarios characterized by domain shifts and limited data availability."
   ],
   "p1": 4493,
   "pn": 4497,
   "doi": "10.21437/Interspeech.2024-976",
   "url": "interspeech_2024/kim24k_interspeech.html"
  },
  "schrufer24_interspeech": {
   "authors": [
    [
     "Oliver",
     "Schrüfer"
    ],
    [
     "Manuel",
     "Milling"
    ],
    [
     "Felix",
     "Burkhardt"
    ],
    [
     "Florian",
     "Eyben"
    ],
    [
     "Björn",
     "Schuller"
    ]
   ],
   "title": "Are you sure? Analysing Uncertainty Quantification Approaches for Real-world Speech Emotion Recognition",
   "original": "977",
   "order": 658,
   "page_count": 5,
   "abstract": [
    "Uncertainty Quantification (UQ) is an important building block for the reliable use of neural networks in real-world scenarios, as it can be a useful tool in identifying faulty predictions. Speech emotion recognition (SER) models can suffer from particularly many sources of uncertainty, such as the ambiguity of emotions, Out-of-Distribution (OOD) data or, in general, poor recording conditions. Reliable UQ methods are thus of particular interest as in many SER applications no prediction is better than a faulty prediction. While the effects of label ambiguity on uncertainty are well documented in the literature, we focus our work on an evaluation of UQ methods for SER under common challenges in real-world application, such as corrupted signals, and the absence of speech. We show that simple UQ methods can already give an indication of the uncertainty of a prediction and that training with additional OOD data can greatly improve the identification of such signals."
   ],
   "p1": 3210,
   "pn": 3214,
   "doi": "10.21437/Interspeech.2024-977",
   "url": "interspeech_2024/schrufer24_interspeech.html"
  },
  "anderer24_interspeech": {
   "authors": [
    [
     "Katharina",
     "Anderer"
    ],
    [
     "Andreas",
     "Reich"
    ],
    [
     "Matthias",
     "Wölfel"
    ]
   ],
   "title": "MaViLS, a Benchmark Dataset for Video-to-Slide Alignment, Assessing Baseline Accuracy with a Multimodal Alignment Algorithm Leveraging Speech, OCR, and Visual Features",
   "original": "978",
   "resource": "https://doi.org/10.5281/zenodo.12747839",
   "order": 284,
   "page_count": 5,
   "abstract": [
    "This paper presents a benchmark dataset for aligning lecture videos with corresponding slides and introduces a novel multimodal algorithm leveraging features from speech, text, and images. It achieves an average accuracy of 0.82 in comparison to SIFT (0.56) while being approximately 11 times faster. Using dynamic programming the algorithm tries to determine the optimal slide sequence. The results show that penalizing slide transitions increases accuracy. Features obtained via optical character recognition (OCR) contribute the most to a high matching accuracy, followed by image features. The findings highlight that audio transcripts alone provide valuable information for alignment and are beneficial if OCR data is lacking. Variations in matching accuracy across different lectures highlight the challenges associated with video quality and lecture style. The novel multimodal algorithm demonstrates robustness to some of these challenges, underscoring the potential of the approach."
   ],
   "p1": 1375,
   "pn": 1379,
   "doi": "10.21437/Interspeech.2024-978",
   "url": "interspeech_2024/anderer24_interspeech.html"
  },
  "xue24b_interspeech": {
   "authors": [
    [
     "Jinlong",
     "Xue"
    ],
    [
     "Yayue",
     "Deng"
    ],
    [
     "Yingming",
     "Gao"
    ],
    [
     "Ya",
     "Li"
    ]
   ],
   "title": "Retrieval Augmented Generation in Prompt-based Text-to-Speech Synthesis with Context-Aware Contrastive Language-Audio Pretraining",
   "original": "980",
   "order": 369,
   "page_count": 5,
   "abstract": [
    "Recent prompt-based text-to-speech (TTS) models can clone an unseen speaker using only a short speech prompt. They leverage a strong in-context ability to mimic the speech prompts, including speaker style, prosody, and emotion. Therefore, the selection of a speech prompt greatly influences the generated speech, akin to the importance of a prompt in large language models (LLMs). However, current prompt-based TTS models choose the speech prompt manually or simply at random. Hence, in this paper, we adapt retrieval augmented generation (RAG) from LLMs to prompt-based TTS. Unlike traditional RAG methods, we additionally consider contextual information during the retrieval process and present a Context-Aware Contrastive Language-Audio Pre-training (CA-CLAP) model to extract context-aware, style-related features. The objective and subjective evaluations demonstrate that our proposed RAG method outperforms baselines, and our CA-CLAP achieves better results than text-only retrieval methods."
   ],
   "p1": 1800,
   "pn": 1804,
   "doi": "10.21437/Interspeech.2024-980",
   "url": "interspeech_2024/xue24b_interspeech.html"
  },
  "gosztolya24_interspeech": {
   "authors": [
    [
     "Gábor",
     "Gosztolya"
    ],
    [
     "László",
     "Tóth"
    ]
   ],
   "title": "Combining Acoustic Feature Sets for Detecting Mild Cognitive Impairment in the Interspeech'24 TAUKADIAL Challenge",
   "original": "984",
   "order": 193,
   "page_count": 5,
   "abstract": [
    "Shared tasks or challenges provide valuable opportunities for the machine learning community, as they offer a chance to compare the performance of machine learning approaches without peeking (due to the hidden test set). We present the approach of our team for the Interspeech'24 TAUKADIAL Challenge, where the task is to distinguish patients of Mild Cognitive Impairment (MCI) from healthy controls based on their speech. Our workflow focuses entirely on the acoustics, mixing standard feature sets (ComParE functionals and wav2vec2 embeddings) and custom attributes focusing on the amount of silent and filled pause segments. By training dedicated SVM classifiers on the three speech tasks and combining the predictions over the different speech tasks and feature sets, we obtained F1 values of up to 0.76 for the MCI identification task using cross-validation, while our RMSE scores for the MMSE estimation task were as low as 2.769 (cross-validation) and 2.608 (test)."
   ],
   "p1": 957,
   "pn": 961,
   "doi": "10.21437/Interspeech.2024-984",
   "url": "interspeech_2024/gosztolya24_interspeech.html"
  },
  "tang24_interspeech": {
   "authors": [
    [
     "Zhiyuan",
     "Tang"
    ],
    [
     "Dong",
     "Wang"
    ],
    [
     "Shen",
     "Huang"
    ],
    [
     "Shidong",
     "Shang"
    ]
   ],
   "title": "Pinyin Regularization in Error Correction for Chinese Speech Recognition with Large Language Models",
   "original": "987",
   "order": 391,
   "page_count": 5,
   "abstract": [
    "Recent studies have demonstrated the efficacy of large language models (LLMs) in error correction for automatic speech recognition (ASR). However, much of the research focuses on the English language. This paper redirects the attention to Chinese. Firstly, we construct a specialized benchmark dataset aimed at error correction for Chinese ASR with 724K hypotheses-transcription pairs, named the Chinese Hypotheses Paradise dataset (ChineseHP), which contains a wide range of scenarios and presents significant challenges. Subsequently, we conduct a preliminary evaluation using the dataset for both direct-prompting and fine-tuning pre-trained LLMs. Furthermore, we propose a straightforward method of Pinyin regularization for prompts, which involves the transcription of Pinyin directly from text hypotheses. The experimental results reveal that Pinyin regularization consistently enhances the error-correcting ability of LLMs when compared with those without regularization. he dataset is available on the website."
   ],
   "p1": 1910,
   "pn": 1914,
   "doi": "10.21437/Interspeech.2024-987",
   "url": "interspeech_2024/tang24_interspeech.html"
  },
  "naderi24_interspeech": {
   "authors": [
    [
     "Maryam",
     "Naderi"
    ],
    [
     "Enno",
     "Hermann"
    ],
    [
     "Alexandre",
     "Nanchen"
    ],
    [
     "Sevada",
     "Hovsepyan"
    ],
    [
     "Mathew",
     "Magimai.-Doss"
    ]
   ],
   "title": "Towards interfacing large language models with ASR systems using confidence measures and prompting",
   "original": "989",
   "order": 612,
   "page_count": 5,
   "abstract": [
    "As large language models (LLMs) grow in parameter size and capabilities, such as interaction through prompting, they open up new ways of interfacing with automatic speech recognition (ASR) systems beyond rescoring n-best lists. This work investigates post-hoc correction of ASR transcripts with LLMs. To avoid introducing errors into likely accurate transcripts, we propose a range of confidence-based filtering methods. Our results indicate that this can improve the performance of less competitive ASR systems."
   ],
   "p1": 2980,
   "pn": 2984,
   "doi": "10.21437/Interspeech.2024-989",
   "url": "interspeech_2024/naderi24_interspeech.html"
  },
  "charuau24_interspeech": {
   "authors": [
    [
     "Delphine",
     "Charuau"
    ],
    [
     "Andrea",
     "Briglia"
    ],
    [
     "Erika",
     "Godde"
    ],
    [
     "Gérard",
     "Bailly"
    ]
   ],
   "title": "Training speech-breathing coordination in computer-assisted reading",
   "original": "992",
   "order": 1050,
   "page_count": 5,
   "abstract": [
    "Aims of this study are: 1) identify respiratory features that could serve as objective markers of fluency of aloud reading; 2) investigate the effects of computer-assisted training on the improvement of speech-breathing coordination. Our training method combines the principles of repeated and assisted close-shadowed reading. Reading assistance takes over the principles of karaoke: highlighting text units in sync with pre-recorded adult performance. The aloud reading of 66 French young pupils is studied. They were divided into 3 groups: control (repeated reading only with no assistance), trained with word highlighting vs. trained with word and breath group highlighting. All children were recorded before and after 3 weeks of training while reading both a trained and an untrained text. The results indicated that respiratory planning and pauses management improved by computer-assisted reading training. However, there was no significant transfer of these improvements to the untrained text."
   ],
   "p1": 5128,
   "pn": 5132,
   "doi": "10.21437/Interspeech.2024-992",
   "url": "interspeech_2024/charuau24_interspeech.html"
  },
  "miyazaki24_interspeech": {
   "authors": [
    [
     "Koichi",
     "Miyazaki"
    ],
    [
     "Yoshiki",
     "Masuyama"
    ],
    [
     "Masato",
     "Murata"
    ]
   ],
   "title": "Exploring the Capability of Mamba in Speech Applications",
   "original": "994",
   "order": 49,
   "page_count": 5,
   "abstract": [
    "This paper explores the capability of Mamba, a recently proposed architecture based on state space models (SSMs), as a competitive alternative to Transformer-based models. In the speech domain, well-designed Transformer-based models, such as the Conformer and E-Branchformer, have become the de facto standards. Extensive evaluations have demonstrated the effectiveness of these Transformer-based models across a wide range of speech tasks. In contrast, the evaluation of SSMs has been limited to a few tasks, such as automatic speech recognition (ASR) and speech synthesis. In this paper, we compared Mamba with state-of-the-art Transformer variants in various speech applications, including ASR, text-to-speech, spoken language understanding, and speech summarization. Experimental evaluations revealed that Mamba achieves comparable or better performance than Transformer-based models, and demonstrated its efficiency in long-form speech processing."
   ],
   "p1": 237,
   "pn": 241,
   "doi": "10.21437/Interspeech.2024-994",
   "url": "interspeech_2024/miyazaki24_interspeech.html"
  },
  "gosztolya24b_interspeech": {
   "authors": [
    [
     "Gábor",
     "Gosztolya"
    ],
    [
     "Mercedes",
     "Vetráb"
    ],
    [
     "Veronika",
     "Svindt"
    ],
    [
     "Judit",
     "Bóna"
    ],
    [
     "Ildikó",
     "Hoffmann"
    ]
   ],
   "title": "Wav2vec 2.0 Embeddings Are No Swiss Army Knife -- A Case Study for Multiple Sclerosis",
   "original": "995",
   "order": 515,
   "page_count": 5,
   "abstract": [
    "In the past few years, self-supervised learning has revolutionalized automatic speech recognition. Self-supervised models such as wav2vec2, due to their generalization ability on huge unannotated audio corpora, were claimed to be state-of-the-art feature extractors in paralinguistic and pathological applications as well. In this study we test embeddings extracted from a wav2vec 2.0 model fine-tuned on the target language as features on a multiple sclerosis audio corpus, using three speech tasks. After comparing the resulting classification performances with traditional features such as ComParE functionals, ECAPA-TDNN and activations of a HMM/DNN hybrid acoustic model, we found that wav2vec2-based models, surprisingly, only produced a mediocre classification performance. In contrast, the decade-old ComParE functionals feature set consistently led to high scores. Our results also indicate that the number of features correlates surprisingly well with classification performance."
   ],
   "p1": 2499,
   "pn": 2503,
   "doi": "10.21437/Interspeech.2024-995",
   "url": "interspeech_2024/gosztolya24b_interspeech.html"
  },
  "kalabakov24_interspeech": {
   "authors": [
    [
     "Stefan",
     "Kalabakov"
    ],
    [
     "Monica",
     "Gonzalez-Machorro"
    ],
    [
     "Florian",
     "Eyben"
    ],
    [
     "Björn W.",
     "Schuller"
    ],
    [
     "Bert",
     "Arnrich"
    ]
   ],
   "title": "A Comparative Analysis of Federated Learning for Speech-Based Cognitive Decline Detection",
   "original": "996",
   "order": 506,
   "page_count": 5,
   "abstract": [
    "Speech-based machine learning models that can distinguish between a healthy cognitive state and different stages of cognitive decline would enable a more appropriate and timely treatment of patients. However, their development is often hampered by data scarcity. Federated Learning (FL) is a potential solution that could enable entities with limited voice recordings to collectively build effective models. Motivated by this, we compare centralised, local, and federated learning for building speech-based models to discern Alzheimer’s Disease, Mild Cognitive Impairment, and a healthy state. For a more realistic evaluation, we use three independently collected datasets to simulate healthcare institutions employing these strategies. Our initial analysis shows that FL may not be the best solution in every scenario, as performance improvements are not guaranteed even with small amounts of available data, and further research is needed to determine the conditions under which it is beneficial."
   ],
   "p1": 2455,
   "pn": 2459,
   "doi": "10.21437/Interspeech.2024-996",
   "url": "interspeech_2024/kalabakov24_interspeech.html"
  },
  "polzehl24_interspeech": {
   "authors": [
    [
     "Tim",
     "Polzehl"
    ],
    [
     "Tim",
     "Herzig"
    ],
    [
     "Friedrich",
     "Wicke"
    ],
    [
     "Kathleen",
     "Wermke"
    ],
    [
     "Razieh",
     "Khamsehashari"
    ],
    [
     "Michiko",
     "Dahlem"
    ],
    [
     "Sebastian",
     "Möller"
    ]
   ],
   "title": "Towards Classifying Mother Tongue from Infant Cries - Findings Substantiating Prenatal Learning Theory",
   "original": "1000",
   "order": 864,
   "page_count": 5,
   "abstract": [
    "In this work we introduce automatic mother tongue classification based on infant cries.  We use data of 63 German and Japanese healthy, term-born neonates, and model their cries with the help of data augmentation and Pre-trained Audio Neural Networks (PANNs), leveraging transfer learning methods suited to the very limited data at hand.  Applying small models on top of PANNs we obtain F1 scores of 85% and above on a held-out test set. We conduct several experiments analyzing model reliability, all of which indicate the network focuses on the actual infant cries rather than on confounding factors. We visualize the network focus to adhere to pitch contour and harmonics thereof, rendering prosody central for our model prediction.  Eventually, our models add a novel objectively obtained perspective to neonate crying analysis, while our results substantiate an extremely early vocal learning indication."
   ],
   "p1": 4199,
   "pn": 4203,
   "doi": "10.21437/Interspeech.2024-1000",
   "url": "interspeech_2024/polzehl24_interspeech.html"
  },
  "giroud24_interspeech": {
   "authors": [
    [
     "Jérémy",
     "Giroud"
    ],
    [
     "Jessica",
     "Lei"
    ],
    [
     "Kirsty",
     "Phillips"
    ],
    [
     "Matthew H.",
     "Davis"
    ]
   ],
   "title": "Behavioral evidence for higher speech rate convergence following natural than artificial time altered speech",
   "original": "1001",
   "order": 538,
   "page_count": 5,
   "abstract": [
    "As AI progresses, our exposure to artificially generated spoken content varying in naturalness and speed increases. This trend is amplified by AI-powered personal assistants’ proliferation, multiplying our interactions with intelligent systems. Research is crucial to understand if phenomena observed in human-human interactions can inform these new interactions. For instance, in everyday conversation, people adjust their speaking rate to match their partner's, a phenomenon known as speech rate convergence. It is crucial for effective communication, occurs automatically and is present in more artificial interaction scenarios. We investigated how the nature (natural vs. artificial) and the presentation rate (normal vs. fast) of the speech signal impact speech rate convergence. Data from 116 participants across two experiments reveal higher convergence towards naturally produced speech compared to artificially time altered speech. Implications for human-machine interactions are discussed."
   ],
   "p1": 2610,
   "pn": 2614,
   "doi": "10.21437/Interspeech.2024-1001",
   "url": "interspeech_2024/giroud24_interspeech.html"
  },
  "andrusenko24_interspeech": {
   "authors": [
    [
     "Andrei",
     "Andrusenko"
    ],
    [
     "Aleksandr",
     "Laptev"
    ],
    [
     "Vladimir",
     "Bataev"
    ],
    [
     "Vitaly",
     "Lavrukhin"
    ],
    [
     "Boris",
     "Ginsburg"
    ]
   ],
   "title": "Fast Context-Biasing for CTC and Transducer ASR models with CTC-based Word Spotter",
   "original": "1002",
   "order": 153,
   "page_count": 5,
   "abstract": [
    "Accurate recognition of rare and new words remains a pressing problem for contextualized Automatic Speech Recognition (ASR) systems. Most context-biasing methods involve modification of the ASR model or the beam-search decoding algorithm, complicating model reuse and slowing down inference. This work presents a new approach to fast context-biasing with CTC-based Word Spotter (CTC-WS) for CTC and Transducer (RNN-T) ASR models. The proposed method matches CTC log-probabilities against a compact context  graph to detect potential context-biasing candidates. The valid candidates then replace their greedy recognition counterparts in corresponding frame intervals. A Hybrid Transducer-CTC model enables the CTC-WS application for the Transducer model. The results demonstrate a significant acceleration of the context-biasing recognition with a simultaneous improvement in F-score and WER compared to baseline methods. The proposed method is publicly available in the NVIDIA NeMo toolkit."
   ],
   "p1": 757,
   "pn": 761,
   "doi": "10.21437/Interspeech.2024-1002",
   "url": "interspeech_2024/andrusenko24_interspeech.html"
  },
  "zhong24b_interspeech": {
   "authors": [
    [
     "Huihang",
     "Zhong"
    ],
    [
     "Yanlu",
     "Xie"
    ],
    [
     "ZiJin",
     "Yao"
    ]
   ],
   "title": "Leveraging Large Language Models to Refine Automatic Feedback Generation at Articulatory Level in Computer Aided Pronunciation Training",
   "original": "1005",
   "order": 536,
   "page_count": 5,
   "abstract": [
    "This study explores the potential of leveraging Large Language Models (LLMs) to refine automatic feedback generation in Computer-Aided Pronunciation Training (CAPT). Specifically, it evaluates the impact of two factors on the effectiveness of automatically generated pronunciation feedbacks: (1) the use of mispronunciation detection at different fine-grained levels as prompts for GPT-4 models to generate automatic feedback, and (2) the fine-tuning of GPT-4 models using specific prompt-feedback pairs aimed at optimizing feedback generation. Feedback generated through each approach is rated by second language (L2) learners in terms of comprehensibility and helpfulness. The results highlight both the potential of using LLMs for automatic feedback generation and the effectiveness of articulatory level representations. Our accessible demonstrations invite further exploration."
   ],
   "p1": 2600,
   "pn": 2604,
   "doi": "10.21437/Interspeech.2024-1005",
   "url": "interspeech_2024/zhong24b_interspeech.html"
  },
  "zhao24d_interspeech": {
   "authors": [
    [
     "Qiuming",
     "Zhao"
    ],
    [
     "Guangzhi",
     "Sun"
    ],
    [
     "Chao",
     "Zhang"
    ],
    [
     "Mingxing",
     "Xu"
    ],
    [
     "Thomas Fang",
     "Zheng"
    ]
   ],
   "title": "SAML: Speaker Adaptive Mixture of LoRA Experts for End-to-End ASR",
   "original": "1006",
   "order": 157,
   "page_count": 5,
   "abstract": [
    "Mixture-of-experts (MoE) models have achieved excellent results in many tasks. However, conventional MoE models are often very large, making them challenging to deploy on resource-constrained edge devices. In this paper, we propose a novel speaker adaptive mixture of LoRA experts (SAML) approach, which uses low-rank adaptation (LoRA) modules as experts to reduce the number of trainable parameters in MoE. Specifically, SAML is applied to the quantised and personalised end-to-end automatic speech recognition models, which combines test-time speaker adaptation to improve the performance of heavily compressed models in speaker-specific scenarios. Experiments have been performed on the LibriSpeech and the TED-LIUM 3 corpora. Remarkably, with a 7x reduction in model size, 29.1% and 31.1% relative word error rate reductions were achieved on the quantised Whisper model and Conformer-based attention-based encoder-decoder ASR model respectively, comparing to the original full precision models."
   ],
   "p1": 777,
   "pn": 781,
   "doi": "10.21437/Interspeech.2024-1006",
   "url": "interspeech_2024/zhao24d_interspeech.html"
  },
  "wang24t_interspeech": {
   "authors": [
    [
     "Siyang",
     "Wang"
    ],
    [
     "Éva",
     "Székely"
    ],
    [
     "Joakim",
     "Gustafson"
    ]
   ],
   "title": "Contextual Interactive Evaluation of TTS Models in Dialogue Systems",
   "original": "1008",
   "order": 609,
   "page_count": 5,
   "abstract": [
    "Evaluation of text-to-speech (TTS) models is currently dominated by Mean-Opinion-Score (MOS) listening test, but MOS has been increasingly questioned for its validity. MOS tests place listeners in a passive setup, in which they do not actively interact with the TTS and usually evaluate isolated utterances without context. Thus it gives no indication for how well a TTS model suits an interactive application like spoken dialogue system, in which the capability of generating appropriate speech in the dialogue context is paramount. We aim to take a first step towards addressing this shortcoming by evaluating several state-of-the-art neural TTS models, including one that adapts to dialogue context, in a custom-built spoken dialogue system. We present system design, experiment setup, and results. Our work is the first to evaluate TTS in contextual dialogue system interactions. We also discuss the shortcomings and future opportunities of the proposed evaluation paradigm.  "
   ],
   "p1": 2965,
   "pn": 2969,
   "doi": "10.21437/Interspeech.2024-1008",
   "url": "interspeech_2024/wang24t_interspeech.html"
  },
  "gosztolya24c_interspeech": {
   "authors": [
    [
     "Gábor",
     "Gosztolya"
    ],
    [
     "Veronika",
     "Svindt"
    ],
    [
     "Judit",
     "Bóna"
    ],
    [
     "Ildikó",
     "Hoffmann"
    ]
   ],
   "title": "Automatic Longitudinal Investigation of Multiple Sclerosis Subjects",
   "original": "1009",
   "order": 190,
   "page_count": 5,
   "abstract": [
    "Multiple Sclerosis is a chronic inflammatory disease of the central nervous system. Over time, people with MS may experience significant changes in cognition, language and speech processes. In this study we investigate speech utterances recorded over the course of three years for 16 MS subjects and 12 healthy controls. Our examination is based on speaker category classification (healthy or MS) using wav2vec2 embeddings as features. We found that subject classification performance improved over time: the 0.745-0.844 AUC values from year one increased to 0.891-0.979 in the third year. By analyzing the posterior estimates, we measured a statistically significant improvement in the scores corresponding to the third year for the MS category, while for the control subjects there was no such tendency. This, in our view, indicates that the change is due to a subtle deterioration in the condition of MS patients, which was detected by our machine learning workflow."
   ],
   "p1": 942,
   "pn": 946,
   "doi": "10.21437/Interspeech.2024-1009",
   "url": "interspeech_2024/gosztolya24c_interspeech.html"
  },
  "benamor24_interspeech": {
   "authors": [
    [
     "Imen",
     "Ben-Amor"
    ],
    [
     "Jean-Francois",
     "Bonastre"
    ],
    [
     "Salima",
     "Mdhaffar"
    ]
   ],
   "title": "Extraction of interpretable and shared speaker-specific speech attributes through binary auto-encoder",
   "original": "1011",
   "order": 662,
   "page_count": 5,
   "abstract": [
    "In speaker recognition systems, embeddings lack explicit speaker-related information, posing challenges for interpretability. Recently, a binary representation of speech extracts, where a coefficient indicates the presence or absence of a given voice attribute, has been proposed to overcome this lack. It consists of an adaptation of x-vector extractor followed by a binarisation step. This approach has proved its worth in terms of explainability, but has two shortcomings. Firstly, the objective of shared attribute modeling is indirectly taken into account. Secondly, binarization is not integrated into the modeling, but added as an afterthought. In this paper, we follow the same principle but propose a new approach that addresses the two limitations outlined above. Our proposal is based on a binary auto-encoder for restructuring conventional embeddings. The expected attribute-based behavior of the binary representation is then explicitly introduced in a new cost function. Experiments on VoxCeleb databases show the effectiveness of our proposal, with a relative reduction in EER of 47% compared to the original approach (from 3.7% to 1.96% of EER), while offering the same level of explainability."
   ],
   "p1": 3230,
   "pn": 3234,
   "doi": "10.21437/Interspeech.2024-1011",
   "url": "interspeech_2024/benamor24_interspeech.html"
  },
  "popescu24_interspeech": {
   "authors": [
    [
     "Anisia",
     "Popescu"
    ],
    [
     "Lori",
     "Lamel"
    ],
    [
     "Ioana",
     "Vasilescu"
    ],
    [
     "Laurence",
     "Devillers"
    ]
   ],
   "title": "Automatic Speech Recognition with parallel L1 and L2 acoustic phone models to evaluate /l/ allophony in L2 English speech production",
   "original": "1014",
   "order": 212,
   "page_count": 5,
   "abstract": [
    "The acoustic and articulatory characteristics of the syllable position lateral allophony in English (clear /l/ in onsets vs. dark /l/ in codas) have been well documented. The present study tests whether speech technology derived methods can be used to evaluate lateral allophony in L2 English production, by combining classic acoustic analyses and automatic speech recognition (ASR). In this study, an ASR system is forced to choose between English and French /l/ acoustic phone models when force-aligning a corpus consisting of read English texts by 43 L2 French learners. The output is correlated with a staple measure for /l/ darkness: the difference between the second and first formants (F2-F1).  Results show that segments aligned with the French /l/ acoustic model correspond to “clearer” /l/s (i.e. higher values of F2-F1) suggesting automatic, less time consuming methods of speech processing could be used to identify L1 transfer in L2 production."
   ],
   "p1": 1015,
   "pn": 1019,
   "doi": "10.21437/Interspeech.2024-1014",
   "url": "interspeech_2024/popescu24_interspeech.html"
  },
  "lin24h_interspeech": {
   "authors": [
    [
     "Zizhen",
     "Lin"
    ],
    [
     "Xiaoting",
     "Chen"
    ],
    [
     "Junyu",
     "Wang"
    ]
   ],
   "title": "MUSE: Flexible Voiceprint Receptive Fields and Multi-Path Fusion Enhanced Taylor Transformer for U-Net-based Speech Enhancement",
   "original": "1017",
   "resource": "https://doi.org/10.5281/zenodo.12790442",
   "order": 136,
   "page_count": 5,
   "abstract": [
    "Achieving a balance between lightweight design and high per- formance remains a challenging task for speech enhancement. In this paper, we introduce Multi-path Enhanced Taylor (MET) Transformer based U-Net for Speech Enhancement (MUSE), a lightweight speech enhancement network built upon the U-Net architecture. Our approach incorporates a novel Multi-path Enhanced Taylor (MET) Transformer block, which integrates Deformable Embedding (DE) to enable flexible receptive fields for voiceprints. The MET Transformer is uniquely designed to fuse Channel and Spatial Attention (CSA) branches, facilitating channel information exchange and addressing spatial attention deficits within the Taylor-Transformer framework. Through extensive experiments conducted on the VoiceBank+DEMAND dataset, we demonstrate that MUSE achieves competitive performance while significantly reducing both training and deployment costs, boasting a mere 0.51M parameters."
   ],
   "p1": 672,
   "pn": 676,
   "doi": "10.21437/Interspeech.2024-1017",
   "url": "interspeech_2024/lin24h_interspeech.html"
  },
  "liu24f_interspeech": {
   "authors": [
    [
     "Yin-Long",
     "Liu"
    ],
    [
     "Rui",
     "Feng"
    ],
    [
     "Jia-Hong",
     "Yuan"
    ],
    [
     "Zhen-Hua",
     "Ling"
    ]
   ],
   "title": "Clever Hans Effect Found in Automatic Detection of Alzheimer's Disease through Speech",
   "original": "1018",
   "order": 502,
   "page_count": 5,
   "abstract": [
    "We uncover an underlying bias present in the audio recordings produced from the picture description task of the Pitt corpus, the largest publicly accessible database for Alzheimer’s Disease (AD) detection research. Even by solely utilizing the silent segments of these audio recordings, we achieve nearly 100% accuracy in AD detection. However, employing the same methods to other datasets and preprocessed Pitt recordings results in typical levels (approximately 80%) of AD detection accuracy. These results demonstrate a Clever Hans effect in AD detection on the Pitt corpus. Our findings emphasize the crucial importance of maintaining vigilance regarding inherent biases in datasets utilized for training deep learning models, and highlight the necessity for a better understanding of the models’ performance."
   ],
   "p1": 2435,
   "pn": 2439,
   "doi": "10.21437/Interspeech.2024-1018",
   "url": "interspeech_2024/liu24f_interspeech.html"
  },
  "karan24_interspeech": {
   "authors": [
    [
     "Biswajit",
     "Karan"
    ],
    [
     "Joshua",
     "Jansen van Vüren"
    ],
    [
     "Febe",
     "de Wet"
    ],
    [
     "Thomas",
     "Niesler"
    ]
   ],
   "title": "A Transformer-Based Voice Activity Detector",
   "original": "1019",
   "order": 787,
   "page_count": 5,
   "abstract": [
    "Voice activity detection (VAD) is the task of distinguishing speech from other types of audio signals, such as music or background noise. We introduce a novel end-to-end VAD architecture which incorporates a pre-trained transformer model (Wav2Vec2-XLS-R). We evaluate the proposed architecture on an established VAD dataset, AVA-Speech, and a manually-segmented corpus of under-resourced multilingual speech. As benchmarks, we include a hybrid CNN-BiLSTM system and an off-the-shelf enterprise VAD. On the AVA-Speech test set, our proposed VAD achieves an area under the curve (AUC) of 96.2% while the benchmarks achieve 94.8% and 81.9% respectively. On the multilingual dataset, the gap widens to 92.2% for the transformer-based VAD and 80.8% and 74.6% for the two baselines. Therefore, the proposed VAD offers improved performance in all cases, with an absolute increase of more than 11% for our target domain. We conclude that the proposed end-to-end architecture improves VAD performance."
   ],
   "p1": 3819,
   "pn": 3823,
   "doi": "10.21437/Interspeech.2024-1019",
   "url": "interspeech_2024/karan24_interspeech.html"
  },
  "sadekova24_interspeech": {
   "authors": [
    [
     "Tasnima",
     "Sadekova"
    ],
    [
     "Mikhail",
     "Kudinov"
    ],
    [
     "Vadim",
     "Popov"
    ],
    [
     "Assel",
     "Yermekova"
    ],
    [
     "Artem",
     "Khrapov"
    ]
   ],
   "title": "PitchFlow: adding pitch control to a Flow-matching based TTS model",
   "original": "1023",
   "order": 908,
   "page_count": 5,
   "abstract": [
    "In recent years, there have been various attempts to improve denoising diffusion probabilistic models and make them more suitable for real-world applications. One of the recent advances in this research direction is a flow-matching models framework which has already shown good results in image and speech generation tasks. Despite high quality and generation speed, flow-matching text-to-speech models still have problems with stability and control. To mitigate this issue, we propose two techniques: speaker scoring and pitch guidance allowing to control timbre and pitch contour of the generated speech. We show that the optimal choice of the prior leads to considerable improvement of similarity and a specific design of classifier guidance allows for fine-grained pitch control with high naturalness. Moreover, these techniques may be used to implement a voice conversion system of a competitive quality."
   ],
   "p1": 4418,
   "pn": 4422,
   "doi": "10.21437/Interspeech.2024-1023",
   "url": "interspeech_2024/sadekova24_interspeech.html"
  },
  "rajkhowa24_interspeech": {
   "authors": [
    [
     "Tonmoy",
     "Rajkhowa"
    ],
    [
     "Amartya Roy",
     "Chowdhury"
    ],
    [
     "Sankalp",
     "Nagaonkar"
    ],
    [
     "Achyut Mani",
     "Tripathi"
    ],
    [
     "Mahadeva",
     "Prasanna"
    ]
   ],
   "title": "TM-PATHVQA: 90000+ Textless Multilingual Questions for Medical Visual Question Answering",
   "original": "1036",
   "order": 830,
   "page_count": 5,
   "abstract": [
    "In healthcare and medical diagnostics, Visual Question Answering (VQA) may emerge as a pivotal tool in scenarios where analysis of intricate medical images becomes critical for accurate diagnoses. Current text-based VQA systems limit their utility in scenarios where hands-free interaction and accessibility are crucial while performing tasks. A speech-based VQA system may provide a better means of interaction where information can be accessed while performing tasks simultaneously. To this end, this work implements a speech-based VQA system by introducing a Textless Multilingual Pathological VQA (TM-PathVQA) dataset, an expansion of the PathVQA dataset, containing spoken questions in English, German and French. This dataset comprises 98,397 multilingual spoken questions and answers based on 5,004 pathological images along with 70 hours of audio. Finally, this work benchmarks and compares TM-PathVQA systems implemented using various combinations of acoustic and visual features."
   ],
   "p1": 4034,
   "pn": 4038,
   "doi": "10.21437/Interspeech.2024-1036",
   "url": "interspeech_2024/rajkhowa24_interspeech.html"
  },
  "shams24_interspeech": {
   "authors": [
    [
     "Erfan A.",
     "Shams"
    ],
    [
     "Iona",
     "Gessinger"
    ],
    [
     "Patrick Cormac",
     "English"
    ],
    [
     "Julie",
     "Carson-Berndsen"
    ]
   ],
   "title": "Are Articulatory Feature Overlaps Shrouded in Speech Embeddings?",
   "original": "1039",
   "order": 946,
   "page_count": 5,
   "abstract": [
    "Domain-informed probing can offer important insights into the types of phonetic information encoded in transformer-based speech recognition models. This paper focuses on phonetic feature probes and investigates whether feature spreading and assimilation are evident in the speech embeddings of the transformer model. Probes are trained for place and manner of articulation and voicing features according to the IPA feature classification, and exemplar fricative consonant clusters where local assimilation would be expected are selected. By then following the articulation trajectory of all of the features during inference, we explore how the transformer model encodes coarticulation and transitions between sounds in the latent representations, by tracking not only the features with the highest activation value but also alternative activations. The patterns identified appear to be in line with expectations from the literature and demonstrate the explanatory power of such an approach."
   ],
   "p1": 4608,
   "pn": 4612,
   "doi": "10.21437/Interspeech.2024-1039",
   "url": "interspeech_2024/shams24_interspeech.html"
  },
  "hovsepyan24_interspeech": {
   "authors": [
    [
     "Sevada",
     "Hovsepyan"
    ],
    [
     "Mathew",
     "Magimai.-Doss"
    ]
   ],
   "title": "Neurocomputational model of speech recognition for pathological speech detection: a case study on Parkinson's disease speech detection",
   "original": "1041",
   "order": 734,
   "page_count": 5,
   "abstract": [
    "This paper presents a computational model for distinguishing between healthy speech and pathological speech, specifically speech from patients with Parkinson's disease. The model is based on neurophysiologically plausible computational models of speech and syllable recognition. These models were designed to uncover the functional roles of brain activity during speech perception. The proposed model is a two-level generative model that uses predictive coding to identify whether the input syllable corresponds to the healthy or Parkinson's disease condition. During inference, the model accumulates the evidence associated with each condition. Although early results are modest (around 60% AUC), they suggest that this approach has merit and should be further investigated."
   ],
   "p1": 3590,
   "pn": 3594,
   "doi": "10.21437/Interspeech.2024-1041",
   "url": "interspeech_2024/hovsepyan24_interspeech.html"
  },
  "chowdhury24_interspeech": {
   "authors": [
    [
     "Anurag",
     "Chowdhury"
    ],
    [
     "Abhinav",
     "Misra"
    ],
    [
     "Mark C.",
     "Fuhs"
    ],
    [
     "Monika",
     "Woszczyna"
    ]
   ],
   "title": "Investigating Confidence Estimation Measures for Speaker Diarization",
   "original": "1044",
   "order": 6,
   "page_count": 5,
   "abstract": [
    "Speaker diarization systems segment a conversation recording based on the speakers' identity. Such systems can misclassify the speaker of a portion of audio due to a variety of factors, such as speech pattern variation, background noise, and overlapping speech. These errors propagate to, and can adversely affect, downstream systems that rely on the speaker's identity, such as speaker-adapted speech recognition. One of the ways to mitigate these errors is to provide segment-level diarization confidence scores to downstream systems. In this work, we investigate multiple methods for generating diarization confidence scores, including those derived from the original diarization system and those derived from an external model. Our experiments across multiple datasets and diarization systems demonstrate that the most competitive confidence score methods can isolate 30% of the diarization errors within segments with the lowest 10% of confidence scores."
   ],
   "p1": 22,
   "pn": 26,
   "doi": "10.21437/Interspeech.2024-1044",
   "url": "interspeech_2024/chowdhury24_interspeech.html"
  },
  "qian24_interspeech": {
   "authors": [
    [
     "Mengjie",
     "Qian"
    ],
    [
     "Siyuan",
     "Tang"
    ],
    [
     "Rao",
     "Ma"
    ],
    [
     "Kate M.",
     "Knill"
    ],
    [
     "Mark J.F.",
     "Gales"
    ]
   ],
   "title": "Learn and Don't Forget: Adding a New Language to ASR Foundation Models",
   "original": "1045",
   "order": 524,
   "page_count": 5,
   "abstract": [
    "Foundation ASR models often support many languages, e.g. 100 languages in Whisper. However, there has been limited work on integrating an additional, typically low-resource, language, while maintaining performance on the original language set. Fine-tuning, while simple, may degrade the accuracy on the original set. We compare three approaches that exploit adaptation parameters: soft language code tuning, train only the language code; soft prompt tuning, train prepended tokens; and LoRA where a small set of additional parameters are optimised. Elastic Weight Consolidation (EWC) offers an alternative compromise with the potential to maintain performance in specific target languages. Results show that direct fine-tuning yields the best performance for the new language but degrades existing language capabilities. EWC can address this issue for specific languages. If only adaptation parameters are used, the language capabilities are maintained but at the cost of performance in the new language."
   ],
   "p1": 2544,
   "pn": 2548,
   "doi": "10.21437/Interspeech.2024-1045",
   "url": "interspeech_2024/qian24_interspeech.html"
  },
  "vanniekerk24_interspeech": {
   "authors": [
    [
     "Benjamin",
     "van Niekerk"
    ],
    [
     "Julian",
     "Zaïdi"
    ],
    [
     "Marc-André",
     "Carbonneau"
    ],
    [
     "Herman",
     "Kamper"
    ]
   ],
   "title": "Spoken-Term Discovery using Discrete Speech Units",
   "original": "1051",
   "order": 740,
   "page_count": 5,
   "abstract": [
    "Discovering a lexicon from unlabeled audio is a longstanding challenge for zero-resource speech processing. One approach is to search for frequently occurring patterns in speech. We revisit this idea by proposing DUSTED: Discrete Unit Spoken-TErm Discovery. Leveraging self-supervised models, we encode input audio into sequences of discrete units. Inspired by alignment algorithms from bioinformatics, we find repeated speech patterns by searching for similar sub-sequences of units. Since discretization discards speaker information, DUSTED finds better matches across speakers, improving the coverage and consistency of the discovered patterns. We demonstrate these improvements on the ZeroSpeech Challenge, achieving state-of-the-art results on the spoken-term discovery track. Finally, we analyze the duration distribution of the patterns, showing that our method finds longer word- or phrase-like terms."
   ],
   "p1": 3620,
   "pn": 3624,
   "doi": "10.21437/Interspeech.2024-1051",
   "url": "interspeech_2024/vanniekerk24_interspeech.html"
  },
  "mohamed24_interspeech": {
   "authors": [
    [
     "Mukhtar",
     "Mohamed"
    ],
    [
     "Oli Danyi",
     "Liu"
    ],
    [
     "Hao",
     "Tang"
    ],
    [
     "Sharon",
     "Goldwater"
    ]
   ],
   "title": "Orthogonality and isotropy of speaker and phonetic information in self-supervised speech representations",
   "original": "1054",
   "order": 741,
   "page_count": 5,
   "abstract": [
    "Self-supervised speech representations can hugely benefit downstream speech technologies, yet the properties that make them useful are still poorly understood. Two candidate properties related to the geometry of the representation space have been hypothesized to correlate well with downstream tasks: (1) the degree of orthogonality between the subspaces spanned by the speaker centroids and phone centroids, and (2) the isotropy of the space, i.e., the degree to which all dimensions are effectively utilized. To study them, we introduce a new measure, Cumulative Residual Variance (CRV), which can be used to assess both properties. Using linear classifiers for speaker and phone ID to probe the representations of six different self-supervised models and two untrained baselines, we ask whether either orthogonality or isotropy correlate with linear probing accuracy. We find that both measures correlate with phonetic probing accuracy, though our results on isotropy are more nuanced."
   ],
   "p1": 3625,
   "pn": 3629,
   "doi": "10.21437/Interspeech.2024-1054",
   "url": "interspeech_2024/mohamed24_interspeech.html"
  },
  "xue24c_interspeech": {
   "authors": [
    [
     "Jinlong",
     "Xue"
    ],
    [
     "Yayue",
     "Deng"
    ],
    [
     "Yicheng",
     "Han"
    ],
    [
     "Yingming",
     "Gao"
    ],
    [
     "Ya",
     "Li"
    ]
   ],
   "title": "Improving Audio Codec-based Zero-Shot Text-to-Speech Synthesis with Multi-Modal Context and Large Language Model",
   "original": "1056",
   "order": 138,
   "page_count": 5,
   "abstract": [
    "Recent advances in large language models (LLMs) and development of audio codecs greatly propel the zero-shot TTS. They can synthesize personalized speech with only a 3-second speech of an unseen speaker as acoustic prompt. However, they only support short speech prompts and cannot leverage longer context information, as required in audiobook and conversational TTS scenarios. In this paper, we introduce a novel audio codec-based TTS model to adapt context features with multiple enhancements. Inspired by the success of Qformer, we propose a multi-modal context-enhanced Qformer (MMCE-Qformer) to utilize additional multi-modal context information. Besides, we adapt a pretrained LLM to leverage its understanding ability to predict semantic tokens, and use a SoundStorm to generate acoustic tokens thereby enhancing audio quality and speaker similarity. The extensive objective and subjective evaluations show that our proposed method outperforms baselines across various context TTS scenarios."
   ],
   "p1": 682,
   "pn": 686,
   "doi": "10.21437/Interspeech.2024-1056",
   "url": "interspeech_2024/xue24c_interspeech.html"
  },
  "plaquet24_interspeech": {
   "authors": [
    [
     "Alexis",
     "Plaquet"
    ],
    [
     "Hervé",
     "Bredin"
    ]
   ],
   "title": "On the calibration of powerset speaker diarization models",
   "original": "1060",
   "resource": "https://doi.org/10.5281/zenodo.12747242",
   "order": 776,
   "page_count": 5,
   "abstract": [
    "End-to-end neural diarization models have usually relied on a multilabel-classification formulation of the speaker diarization problem. Recently, we proposed a powerset multiclass formulation that has beaten the state-of-the-art on multiple datasets. In this paper, we propose to study the calibration of a powerset speaker diarization model, and explore some of its uses. We study the calibration in-domain, as well as out-of-domain, and explore the data in low-confidence regions. The reliability of model confidence is then tested in practice: we use the confidence of the pretrained model to selectively create training and validation subsets out of unannotated data, and compare this to random selection. We find that top-label confidence can be used to reliably predict high-error regions. Moreover, training on low-confidence regions provides a better calibrated model, and validating on low-confidence regions can be more annotation-efficient than random regions."
   ],
   "p1": 3764,
   "pn": 3768,
   "doi": "10.21437/Interspeech.2024-1060",
   "url": "interspeech_2024/plaquet24_interspeech.html"
  },
  "gupta24c_interspeech": {
   "authors": [
    [
     "Kishan",
     "Gupta"
    ],
    [
     "Nicola",
     "Pia"
    ],
    [
     "Srikanth",
     "Korse"
    ],
    [
     "Andreas",
     "Brendel"
    ],
    [
     "Guillaume",
     "Fuchs"
    ],
    [
     "Markus",
     "Multrus"
    ]
   ],
   "title": "On Improving Error Resilience of Neural End-to-End Speech Coders",
   "original": "1061",
   "order": 360,
   "page_count": 5,
   "abstract": [
    "Error resilient tools like Packet Loss Concealment (PLC) and Forward Error Correction (FEC) are essential to maintain a reliable speech communication for applications like Voice over Internet Protocol (VoIP), where packets are frequently delayed and lost. In recent times, end-to-end neural speech codecs have seen a significant rise, due to their ability to transmit speech signal at low bitrates but few considerations were made about their error resilience in a real system. Recently introduced Neural End-to-End Speech Codec (NESC) can reproduce high quality natural speech at low bitrates. We extend its robustness to packet losses by adding a low complexity network to predict the codebook indices in latent space. Furthermore, we propose a method to add an in-band FEC at an additional bitrate of 0.8 kbps. Both subjective and objective assessment indicate the effectiveness of proposed methods, and demonstrate that coupling PLC and FEC provide significant robustness against packet losses."
   ],
   "p1": 1755,
   "pn": 1759,
   "doi": "10.21437/Interspeech.2024-1061",
   "url": "interspeech_2024/gupta24c_interspeech.html"
  },
  "masumura24_interspeech": {
   "authors": [
    [
     "Ryo",
     "Masumura"
    ],
    [
     "Naoki",
     "Makishima"
    ],
    [
     "Tomohiro",
     "Tanaka"
    ],
    [
     "Mana",
     "Ihori"
    ],
    [
     "Naotaka",
     "Kawata"
    ],
    [
     "Shota",
     "Orihashi"
    ],
    [
     "Kazutoshi",
     "Shinoda"
    ],
    [
     "Taiga",
     "Yamane"
    ],
    [
     "Saki",
     "Mizuno"
    ],
    [
     "Keita",
     "Suzuki"
    ],
    [
     "Satoshi",
     "Suzuki"
    ],
    [
     "Nobukatsu",
     "Hojo"
    ],
    [
     "Takafumi",
     "Moriya"
    ],
    [
     "Atsushi",
     "Ando"
    ]
   ],
   "title": "Unified Multi-Talker ASR with and without Target-speaker Enrollment",
   "original": "1062",
   "order": 147,
   "page_count": 5,
   "abstract": [
    "This paper proposes a novel multi-talker automatic speech recognition (MT-ASR) system that can perform both a target-speaker enrollment-driven process and a target-speaker-free process in a unified modeling framework. In previous studies, these two MT-ASR forms were independently modeled with unshareable parameters. However, the independent modeling cannot mutually utilize knowledge trained with different tasks. Our key idea for bridging the gap between the two forms is to introduce modeling that can regard the target-speaker-free process as the target-speaker enrollment-driven process enrolled with no target-speaker information. Therefore, our method constructs a unified autoregressive model with a removable target-speaker encoder, and its shareable model parameters are trained jointly using training datasets with and without target-speaker enrollment. Experiments demonstrated that our unified modeling significantly outperforms the independent modeling in both MT-ASR forms."
   ],
   "p1": 727,
   "pn": 731,
   "doi": "10.21437/Interspeech.2024-1062",
   "url": "interspeech_2024/masumura24_interspeech.html"
  },
  "li24v_interspeech": {
   "authors": [
    [
     "Guinan",
     "Li"
    ],
    [
     "Jiajun",
     "Deng"
    ],
    [
     "Youjun",
     "Chen"
    ],
    [
     "Mengzhe",
     "Geng"
    ],
    [
     "Shujie",
     "Hu"
    ],
    [
     "Zhe",
     "Li"
    ],
    [
     "Zengrui",
     "Jin"
    ],
    [
     "Tianzi",
     "Wang"
    ],
    [
     "Xurong",
     "Xie"
    ],
    [
     "Helen",
     "Meng"
    ],
    [
     "Xunying",
     "Liu"
    ]
   ],
   "title": "Joint Speaker Features Learning for Audio-visual Multichannel Speech Separation and Recognition",
   "original": "1063",
   "order": 394,
   "page_count": 5,
   "abstract": [
    "This paper proposes joint speaker feature learning methods for zero-shot adaptation of audio-visual multichannel speech separation and recognition systems. xVector and ECAPA-TDNN speaker encoders are connected using purpose-built fusion blocks and tightly integrated with the complete system training. Experiments conducted on LRS3-TED data simulated multichannel overlapped speech suggest that joint speaker feature learning consistently improves speech separation and recognition performance over the baselines without joint speaker feature estimation. Further analyses reveal performance improvements are strongly correlated with increased inter-speaker discrimination measured using cosine similarity. The best-performing joint speaker feature learning adapted system outperformed the baseline fine-tuned WavLM model by statistically significant WER reductions of 21.6% and 25.3% absolute (67.5% and 83.5% relative) on Dev and Test sets after incorporating WavLM features and video modality."
   ],
   "p1": 1925,
   "pn": 1929,
   "doi": "10.21437/Interspeech.2024-1063",
   "url": "interspeech_2024/li24v_interspeech.html"
  },
  "pan24_interspeech": {
   "authors": [
    [
     "Zexu",
     "Pan"
    ],
    [
     "Gordon",
     "Wichern"
    ],
    [
     "François G.",
     "Germain"
    ],
    [
     "Kohei",
     "Saijo"
    ],
    [
     "Jonathan",
     "Le Roux"
    ]
   ],
   "title": "PARIS: Pseudo-AutoRegressIve Siamese Training for Online Speech Separation",
   "original": "1066",
   "order": 118,
   "page_count": 5,
   "abstract": [
    "While offline speech separation models have made significant advances, the streaming regime remains less explored and is typically limited to causal modifications of existing offline networks. This study focuses on empowering a streaming speech separation model with autoregressive capability, in which the current step separation is conditioned on separated samples from past steps. To do so, we introduce pseudo-autoregressive Siamese (PARIS) training: with only two forward passes through a Siamese-style network for each batch, PARIS avoids the training-inference mismatch in teacher forcing and the need for numerous autoregressive steps during training. The proposed PARIS training improves the recent online SkiM model by 1.5 dB in SI-SNR on the WSJ0-2mix dataset, with minimal change to the network architecture and inference time."
   ],
   "p1": 582,
   "pn": 586,
   "doi": "10.21437/Interspeech.2024-1066",
   "url": "interspeech_2024/pan24_interspeech.html"
  },
  "li24w_interspeech": {
   "authors": [
    [
     "Jizhen",
     "Li"
    ],
    [
     "Xinmeng",
     "Xu"
    ],
    [
     "Weiping",
     "Tu"
    ],
    [
     "Yuhong",
     "Yang"
    ],
    [
     "Rong",
     "Zhu"
    ]
   ],
   "title": "Improving Speech Enhancement by Integrating Inter-Channel and Band Features with Dual-branch Conformer",
   "original": "1069",
   "order": 353,
   "page_count": 5,
   "abstract": [
    "Recent speech enhancement methods based on convolutional neural networks (CNNs) and transformer have been demonstrated to efficaciously capture time-frequency (T-F) information on spectrogram. However, the correlation of each channels of speech features is failed to explore. Theoretically, each channel map of speech features obtained by different convolution kernels contains information with different scales demonstrating strong correlations. To fill this gap, we propose a novel dual-branch architecture named channel-aware dualbranch conformer (CADB-Conformer), which effectively explores the long range time and frequency correlations among different channels, respectively, to extract channel relation aware time-frequency information. Ablation studies conducted on DNS-Challenge 2020 dataset demonstrate the importance of channel feature leveraging while showing the significance of channel relation aware T-F information for speech enhancement. Extensive experiments also show that the proposed model achieves superior performance than recent methods with an attractive computational costs."
   ],
   "p1": 1720,
   "pn": 1724,
   "doi": "10.21437/Interspeech.2024-1069",
   "url": "interspeech_2024/li24w_interspeech.html"
  },
  "moschopoulos24_interspeech": {
   "authors": [
    [
     "Vasileios",
     "Moschopoulos"
    ],
    [
     "Thanasis",
     "Kotsiopoulos"
    ],
    [
     "Pablo",
     "Peso Parada"
    ],
    [
     "Konstantinos",
     "Nikiforidis"
    ],
    [
     "Alexandros",
     "Stergiadis"
    ],
    [
     "Gerasimos",
     "Papakostas"
    ],
    [
     "Md Asif",
     "Jalal"
    ],
    [
     "Jisi",
     "Zhang"
    ],
    [
     "Anastasios",
     "Drosou"
    ],
    [
     "Karthikeyan",
     "Saravanan"
    ]
   ],
   "title": "Exploring compressibility of transformer based text-to-music (TTM) models",
   "original": "1071",
   "order": 676,
   "page_count": 5,
   "abstract": [
    "State-of-the art Text-To-Music (TTM) generative AI models are large and require desktop or server class compute, making them infeasible for deployment on mobile phones. This paper presents an analysis of trade-offs between model compression and generation performance of TTM models. We study compression through knowledge distillation and specific modifications that enable applicability over the various components of the TTM model (encoder, generative model and the decoder). Leveraging these methods we create TinyTTM (89.2M params) that achieves a FAD of 3.66 and KL of 1.32 on MusicBench dataset, better than MusicGen-Small (557.6M params) but not lower than MusicGen-small fine-tuned on MusicBench."
   ],
   "p1": 3300,
   "pn": 3304,
   "doi": "10.21437/Interspeech.2024-1071",
   "url": "interspeech_2024/moschopoulos24_interspeech.html"
  },
  "muller24c_interspeech": {
   "authors": [
    [
     "Thomas",
     "Muller"
    ],
    [
     "Stephane",
     "Ragot"
    ],
    [
     "Laetitia",
     "Gros"
    ],
    [
     "Pierrick",
     "Philippe"
    ],
    [
     "Pascal",
     "Scalart"
    ]
   ],
   "title": "Speech quality evaluation of neural audio codecs",
   "original": "1072",
   "order": 361,
   "page_count": 5,
   "abstract": [
    "This paper presents speech quality results to characterize the state of the art and technological advance of recent neural audio codecs targeting low bitrates. Audio quality was evaluated in one clean speech experiment (in French). Degradation Mean Opinion Score (DMOS) results are reported and discussed for neural audio codecs (LPCNet, Lyra V2, EnCodec, AudioCraft, AudioDec, Descript Audio Codec) – traditional codecs (Opus, EVS) are also included as performance yardsticks. We also discuss observed codec complexity to complement subjective test results."
   ],
   "p1": 1760,
   "pn": 1764,
   "doi": "10.21437/Interspeech.2024-1072",
   "url": "interspeech_2024/muller24c_interspeech.html"
  },
  "lin24i_interspeech": {
   "authors": [
    [
     "Yi-Cheng",
     "Lin"
    ],
    [
     "Haibin",
     "Wu"
    ],
    [
     "Huang-Cheng",
     "Chou"
    ],
    [
     "Chi-Chun",
     "Lee"
    ],
    [
     "Hung-yi",
     "Lee"
    ]
   ],
   "title": "Emo-bias: A Large Scale Evaluation of Social Bias on Speech Emotion Recognition",
   "original": "1073",
   "order": 951,
   "page_count": 5,
   "abstract": [
    "The rapid growth of Speech Emotion Recognition (SER) has diverse global applications, from improving human-computer interactions to aiding mental health diagnostics. However, SER models might contain social bias toward gender, leading to unfair outcomes. This study analyzes gender bias in SER models trained with Self-Supervised Learning (SSL) at scale, exploring factors influencing it. SSL-based SER models are chosen for their cutting-edge performance. Our research pioneering research gender bias in SER from both upstream model and data perspectives. Our findings reveal that females exhibit slightly higher overall SER performance than males.  Modified CPC and XLS-R, two well-known SSL models, notably exhibit significant bias. Moreover, models trained with Mandarin datasets display a pronounced bias toward valence. Lastly, we find that gender-wise emotion distribution differences in training data significantly affect gender bias, while upstream model representation has a limited impact. "
   ],
   "p1": 4633,
   "pn": 4637,
   "doi": "10.21437/Interspeech.2024-1073",
   "url": "interspeech_2024/lin24i_interspeech.html"
  },
  "dineley24_interspeech": {
   "authors": [
    [
     "Judith",
     "Dineley"
    ],
    [
     "Ewan",
     "Carr"
    ],
    [
     "Lauren L.",
     "White"
    ],
    [
     "Catriona",
     "Lucas"
    ],
    [
     "Zahia",
     "Rahman"
    ],
    [
     "Tian",
     "Pan"
    ],
    [
     "Faith",
     "Matcham"
    ],
    [
     "Johnny",
     "Downs"
    ],
    [
     "Richard J.",
     "Dobson"
    ],
    [
     "Thomas F.",
     "Quatieri"
    ],
    [
     "Nicholas",
     "Cummins"
    ]
   ],
   "title": "Variability of speech timing features across repeated recordings: a comparison of open-source extraction techniques",
   "original": "1074",
   "order": 412,
   "page_count": 5,
   "abstract": [
    "Variations in speech timing features have been reliably linked to symptoms of various health conditions, demonstrating clinical potential. However, replication challenges hinder their translation; extracted speech features are susceptible to methodological variations in the recording and processing pipeline. Investigating this, we compared exemplar timing features extracted via three different techniques from recordings of healthy speech. Our results show that features extracted via an intensity-based method differ from those produced by forced alignment. Different extraction methods also led to differing estimates of within-speaker feature variability over time in an analysis of recordings repeated systematically over three sessions in one day (n=26) and in one week (n=28). Our findings highlight the importance of feature extraction in study design and interpretation, and the need for consistent, accurate extraction techniques for clinical research."
   ],
   "p1": 2015,
   "pn": 2019,
   "doi": "10.21437/Interspeech.2024-1074",
   "url": "interspeech_2024/dineley24_interspeech.html"
  },
  "amiri24_interspeech": {
   "authors": [
    [
     "Mahdi",
     "Amiri"
    ],
    [
     "Ina",
     "Kodrasi"
    ]
   ],
   "title": "Adversarial Robustness Analysis in Automatic Pathological Speech Detection Approaches",
   "original": "1075",
   "order": 292,
   "page_count": 5,
   "abstract": [
    "Automatic pathological speech detection relies on deep learning (DL), showing promising performance for various pathologies.  Despite the critical importance of robustness in healthcare applications like pathological speech detection, the sensitivity of DL-based pathological speech detection approaches to adversarial attacks remains unexplored. This paper explores the impact of acoustically imperceptible adversarial perturbations on DL-based pathological speech detection. Imperceptibility of perturbations, generated using the projected gradient descent algorithm, is evaluated using speech enhancement metrics. Results reveal a high vulnerability of DL-based pathological speech detection to adversarial perturbations, with adversarial training ineffective in enhancing robustness. Analysis of the perturbations provide insights into the speech components that the approaches attend to. These findings highlight the need for research in robust pathological speech detection."
   ],
   "p1": 1415,
   "pn": 1419,
   "doi": "10.21437/Interspeech.2024-1075",
   "url": "interspeech_2024/amiri24_interspeech.html"
  },
  "kuan24_interspeech": {
   "authors": [
    [
     "Chun-Yi",
     "Kuan"
    ],
    [
     "Wei-Ping",
     "Huang"
    ],
    [
     "Hung-yi",
     "Lee"
    ]
   ],
   "title": "Understanding Sounds, Missing the Questions: The Challenge of Object Hallucination in Large Audio-Language Models",
   "original": "1076",
   "order": 852,
   "page_count": 5,
   "abstract": [
    "Large audio-language models (LALMs) enhance traditional large language models by integrating audio perception capabilities, allowing them to tackle audio-related tasks. Previous research has primarily focused on assessing the performance of LALMs across various tasks, yet overlooking their reliability, particularly concerning issues like object hallucination. In our study, we introduce methods to assess the extent of object hallucination of publicly available LALMs. Our findings reveal that LALMs are comparable to specialized audio captioning models in their understanding of audio content, but struggle to answer discriminative questions, specifically those requiring the identification of the presence of particular object sounds within an audio clip. This limitation highlights a critical weakness in current LALMs: their inadequate understanding of discriminative queries. Moreover, we explore the potential of prompt engineering to enhance LALMs' performance on discriminative questions."
   ],
   "p1": 4144,
   "pn": 4148,
   "doi": "10.21437/Interspeech.2024-1076",
   "url": "interspeech_2024/kuan24_interspeech.html"
  },
  "yang24k_interspeech": {
   "authors": [
    [
     "Yiyuan",
     "Yang"
    ],
    [
     "Niki",
     "Trigoni"
    ],
    [
     "Andrew",
     "Markham"
    ]
   ],
   "title": "Pre-training Feature Guided Diffusion Model for Speech Enhancement",
   "original": "1077",
   "order": 246,
   "page_count": 5,
   "abstract": [
    "Speech enhancement significantly improves the clarity and intelligibility of speech in noisy environments, improving communication and listening experiences. In this paper, we introduce a novel pretraining feature-guided diffusion model tailored for efficient speech enhancement, addressing the limitations of existing discriminative and generative models. By integrating spectral features into a variational autoencoder (VAE) and leveraging pre-trained features for guidance during the reverse process, coupled with the utilization of the deterministic discrete integration method (DDIM) to streamline sampling steps, our model improves efficiency and speech enhancement quality. Demonstrating state-of-the-art results on two public datasets with different SNRs, our model outshines other baselines in efficiency and robustness. The proposed method not only optimizes performance but also enhances practical deployment capabilities, without increasing computational demands."
   ],
   "p1": 1185,
   "pn": 1189,
   "doi": "10.21437/Interspeech.2024-1077",
   "url": "interspeech_2024/yang24k_interspeech.html"
  },
  "wu24i_interspeech": {
   "authors": [
    [
     "Chung-Wen",
     "Wu"
    ],
    [
     "Berlin",
     "Chen"
    ]
   ],
   "title": "Optimizing Automatic Speech Assessment: W-RankSim Regularization and Hybrid Feature Fusion Strategies",
   "original": "1078",
   "order": 824,
   "page_count": 5,
   "abstract": [
    "Automatic Speech Assessment (ASA) has seen notable advancements with the utilization of self-supervised features (SSL) in recent research. However, a key challenge in ASA lies in the imbalanced distribution of data, particularly evident in English test datasets. To address this challenge, we approach ASA as an ordinal classification task, introducing Weighted Vectors Ranking Similarity (W-RankSim) as a novel regularization technique. W-RankSim encourages closer proximity of weighted vectors in the output layer for similar classes, implying that feature vectors with similar labels would be gradually nudged closer to each other as they converge towards corresponding weighted vectors. Extensive experimental evaluations confirm the effectiveness of our approach in improving ordinal classification performance for ASA. Furthermore, we propose a hybrid model that combines SSL and handcrafted features, showcasing how the inclusion of handcrafted features enhances performance in an ASA system."
   ],
   "p1": 4004,
   "pn": 4008,
   "doi": "10.21437/Interspeech.2024-1078",
   "url": "interspeech_2024/wu24i_interspeech.html"
  },
  "qian24b_interspeech": {
   "authors": [
    [
     "Livia",
     "Qian"
    ],
    [
     "Gabriel",
     "Skantze"
    ]
   ],
   "title": "Joint Learning of Context and Feedback Embeddings in Spoken Dialogue",
   "original": "1082",
   "order": 607,
   "page_count": 5,
   "abstract": [
    "Short feedback responses, such as backchannels, play an important role in spoken dialogue. So far, most of the modeling of feedback responses has focused on their timing, often neglecting how their lexical and prosodic form influence their contextual appropriateness and conversational function. In this paper, we investigate the possibility of embedding short dialogue contexts and feedback responses in the same representation space using a contrastive learning objective. In our evaluation, we primarily focus on how such embeddings can be used as a context-feedback appropriateness metric and thus for feedback response ranking in U.S. English dialogues. Our results show that the model outperforms humans given the same ranking task and that the learned embeddings carry information about the conversational function of feedback responses."
   ],
   "p1": 2955,
   "pn": 2959,
   "doi": "10.21437/Interspeech.2024-1082",
   "url": "interspeech_2024/qian24b_interspeech.html"
  },
  "loddo24_interspeech": {
   "authors": [
    [
     "Nicolò",
     "Loddo"
    ],
    [
     "Francisca",
     "Pessanha"
    ],
    [
     "Almila",
     "Akdag"
    ]
   ],
   "title": "What if HAL breathed? Enhancing Empathy in Human-AI Interactions with Breathing Speech Synthesis",
   "original": "1083",
   "order": 541,
   "page_count": 5,
   "abstract": [
    "AI Agents increasingly leverages speech synthesis models to communicate with their users. This study explores the integration of breathing patterns into synthesized speech to deepen empathy towards AI agents. The research methodologically diverges from traditional empathy studies and speech evaluation standards by proposing to the subjects the resolution of an emotional dilemma within a cooperative game scenario, where they face a choice reflecting their empathetic engagement with an AI partner. The introduction of a novel speech assessment method that takes into account the interactive and contextual aspect of conversational speech is the first novelty of the paper. The second novelty is in the findings which indicate that breathing in synthesized speech significantly enhances agents' perceived naturalness and users' empathy towards them. "
   ],
   "p1": 2625,
   "pn": 2629,
   "doi": "10.21437/Interspeech.2024-1083",
   "url": "interspeech_2024/loddo24_interspeech.html"
  },
  "khurana24_interspeech": {
   "authors": [
    [
     "Sameer",
     "Khurana"
    ],
    [
     "Chiori",
     "Hori"
    ],
    [
     "Antoine",
     "Laurent"
    ],
    [
     "Gordon",
     "Wichern"
    ],
    [
     "Jonathan",
     "Le Roux"
    ]
   ],
   "title": "ZeroST: Zero-Shot Speech Translation",
   "original": "1088",
   "order": 80,
   "page_count": 5,
   "abstract": [
    "Our work introduces the Zero-Shot Speech Translation (ZeroST) framework, leveraging the synergistic potential of pre trained multilingual speech and text foundation models. Inspired by recent advances in multimodal foundation models, ZeroST utilizes a Query Transformer (Q-Former) to seamlessly connect a speech foundation model, such as Whisper or Massively Multilingual Speech (MMS), with a text translation model like No-Language-Left-Behind (NLLB). Our proposed learning framework enables the model to perform the speech-to-text translation in a zero-shot manner, bypassing the need for explicit supervision from expensive-to-collect speech-text translation pairs during training. Our extensive experiments, notably on the Europarl-ST benchmark, demonstrate that ZeroST achieves results comparable to those of a strong cascaded translation system and significantly outperforms baseline models. This promising approach paves the way for future research in zero-shot speech translation."
   ],
   "p1": 392,
   "pn": 396,
   "doi": "10.21437/Interspeech.2024-1088",
   "url": "interspeech_2024/khurana24_interspeech.html"
  },
  "goria24_interspeech": {
   "authors": [
    [
     "Stefano",
     "Goria"
    ],
    [
     "Roseline",
     "Polle"
    ],
    [
     "Salvatore",
     "Fara"
    ],
    [
     "Nicholas",
     "Cummins"
    ]
   ],
   "title": "Revealing Confounding Biases: A Novel Benchmarking Approach for Aggregate-Level Performance Metrics in Health Assessments",
   "original": "1092",
   "order": 297,
   "page_count": 5,
   "abstract": [
    "Numerous speech-based health assessment studies report high accuracy rates for machine learning models which detect conditions such as depression and Alzheimer’s disease. There are growing concerns that these reported performances are often overestimated, especially in small-scale cross-sectional studies. Possible causes for this overestimation include overfitting, publication biases and a lack of standard procedures to report findings and testing methodology. Another key source of misrepresentation is the reliance on aggregate-level performance metrics. Speech is a highly variable signal that can be affected by factors including age, sex, and accent, which can easily bias models. We highlight this impact by presenting a simple benchmark model for assessing the extent to which aggregate metrics exaggerate the efficacy of a machine learning model in the presence of confounders. We then demonstrate the usefulness of this model on exemplar speech-health assessment datasets."
   ],
   "p1": 1440,
   "pn": 1444,
   "doi": "10.21437/Interspeech.2024-1092",
   "url": "interspeech_2024/goria24_interspeech.html"
  },
  "blockmedin24_interspeech": {
   "authors": [
    [
     "Lucas",
     "Block Medin"
    ],
    [
     "Thomas",
     "Pellegrini"
    ],
    [
     "Lucile",
     "Gelin"
    ]
   ],
   "title": "Self-Supervised Models for Phoneme Recognition: Applications in Children's Speech for Reading Learning",
   "original": "1095",
   "order": 1058,
   "page_count": 5,
   "abstract": [
    "Child speech recognition is still an underdeveloped area of research due to the lack of data (especially on non-English languages) and the specific difficulties of this task. Having explored various architectures for child speech recognition in previous work, in this article we tackle recent self-supervised models. We first compare wav2vec 2.0, HuBERT and WavLM models adapted to phoneme recognition in French child speech, and continue our experiments with the best of them, WavLM base+. We then further adapt it by unfreezing its transformer blocks during fine-tuning on child speech, which greatly improves its performance and makes it significantly outperform our base model, a Transformer+CTC. Finally, we study in detail the behaviour of these two models under the real conditions of our application, and show that WavLM base+ is more robust to various reading tasks and noise levels."
   ],
   "p1": 5168,
   "pn": 5172,
   "doi": "10.21437/Interspeech.2024-1095",
   "url": "interspeech_2024/blockmedin24_interspeech.html"
  },
  "cauzinille24_interspeech": {
   "authors": [
    [
     "Jules",
     "Cauzinille"
    ],
    [
     "Benoît",
     "Favre"
    ],
    [
     "Ricard",
     "Marxer"
    ],
    [
     "Dena",
     "Clink"
    ],
    [
     "Abdul Hamid",
     "Ahmad"
    ],
    [
     "Arnaud",
     "Rey"
    ]
   ],
   "title": "Investigating self-supervised speech models' ability to classify animal vocalizations: The case of gibbon's vocal signatures",
   "original": "1096",
   "order": 28,
   "page_count": 5,
   "abstract": [
    "With the advent of pre-trained self-supervised learning (SSL) models, speech processing research is showing increasing interest towards disentanglement and explainability. Amongst other methods, probing speech classifiers has emerged as a promising approach to gain new insights into SSL models out-of-domain performances. We explore knowledge transfer capabilities of pre-trained speech models with vocalizations from the closest living relatives of humans: non-human primates. We focus on classifying the identity of northern grey gibbons (Hylobates funereus) from their calls with probing and layer-wise analysis of state-of-the-art SSL speech models compared to pre-trained bird species classifiers and audio taggers. By testing the reliance of said models on background noise and timewise information, as well as performance variations across layers, we propose a new understanding of the mechanisms underlying speech models efficacy as bioacoustic tools. "
   ],
   "p1": 132,
   "pn": 136,
   "doi": "10.21437/Interspeech.2024-1096",
   "url": "interspeech_2024/cauzinille24_interspeech.html"
  },
  "gerczuk24_interspeech": {
   "authors": [
    [
     "Maurice",
     "Gerczuk"
    ],
    [
     "Shahin",
     "Amiriparian"
    ],
    [
     "Justina",
     "Lutz"
    ],
    [
     "Wolfgang",
     "Strube"
    ],
    [
     "Irina",
     "Papazova"
    ],
    [
     "Alkomiet",
     "Hasan"
    ],
    [
     "Björn W.",
     "Schuller"
    ]
   ],
   "title": "Exploring Gender-Specific Speech Patterns in Automatic Suicide Risk Assessment",
   "original": "1097",
   "order": 228,
   "page_count": 5,
   "abstract": [
    "In emergency medicine, timely intervention for patients at risk of suicide is often hindered by delayed access to specialised psychiatric care. To bridge this gap, we introduce a speech-based approach for automatic suicide risk assessment. Our study involves a novel dataset comprising speech recordings of 20 patients who read neutral texts. We extract four speech representations encompassing interpretable and deep features. Further, we explore the impact of gender-based modelling and phrase-level normalisation. By applying gender-exclusive modelling, features extracted from an emotion fine-tuned wav2vec2.0 model can be utilised to discriminate high- from low suicide risk with a balanced accuracy of 81%. Finally, our analysis reveals a discrepancy in the relationship of speech characteristics and suicide risk between female and male subjects. For men in our dataset, suicide risk increases together with agitation while voice characteristics of female subjects point the other way."
   ],
   "p1": 1095,
   "pn": 1099,
   "doi": "10.21437/Interspeech.2024-1097",
   "url": "interspeech_2024/gerczuk24_interspeech.html"
  },
  "rolland24_interspeech": {
   "authors": [
    [
     "Thomas",
     "Rolland"
    ],
    [
     "Alberto",
     "Abad"
    ]
   ],
   "title": "Introduction To Partial Fine-tuning: A Comprehensive Evaluation Of End-to-end Children’s Automatic Speech Recognition Adaptation",
   "original": "1102",
   "order": 1060,
   "page_count": 5,
   "abstract": [
    "Automatic Speech Recognition (ASR) encounters unique challenges when dealing with children's speech, mainly due to the scarcity of available data. Training large ASR models with constrained data presents a significant challenge. To address this, fine-tuning strategy is frequently employed. However, fine-tuning an entire large pre-trained model with limited children's speech data may overfit leading to decreased performance. This study offers a granular evaluation of children's ASR fine-tuning, departing from conventional whole-network tunning. We present a partial fine-tuning approach spotlighting the importance of the Encoder and Feedforward Neural Network modules in Transformer-based models. Remarkably, this method surpasses the efficacy of whole-model fine-tuning, with a relative word error rate improvement of 9\\% when dealing with limited data. Our findings highlight the critical role of partial fine-tuning in advancing children's ASR model development."
   ],
   "p1": 5178,
   "pn": 5182,
   "doi": "10.21437/Interspeech.2024-1102",
   "url": "interspeech_2024/rolland24_interspeech.html"
  },
  "rolland24b_interspeech": {
   "authors": [
    [
     "Thomas",
     "Rolland"
    ],
    [
     "Alberto",
     "Abad"
    ]
   ],
   "title": "Shared-Adapters: A Novel Transformer-based Parameter Efficient Transfer Learning Approach For Children’s Automatic Speech Recognition",
   "original": "1105",
   "order": 489,
   "page_count": 5,
   "abstract": [
    "Automatic Speech Recognition (ASR) often faces challenges in processing children's speech due to data scarcity. Training large ASR models becomes particularly challenging in such scenarios. To mitigate this issue, fine-tuning is commonly employed, leveraging pre-trained adult models. However, fine-tuning large pre-trained models with limited data poses its own challenges. In response, this study investigates Parameter-Efficient Finetuning (PEFT) for children’s ASR. Various PEFT approaches are explored, with a specific emphasis on good ASR performance while minimising the number of parameters during training. Our investigation identifies residual Adapters as the most efficient technique.  Moreover, motivated by Transformer-based model redundancies, we propose the Shared-Adapter and its highly parameter-efficient variant, the Light Shared-Adapter. Our findings demonstrate that Shared-Adapters strike an exceptional balance between recognition performance and parameter efficiency."
   ],
   "p1": 2370,
   "pn": 2374,
   "doi": "10.21437/Interspeech.2024-1105",
   "url": "interspeech_2024/rolland24b_interspeech.html"
  },
  "seki24_interspeech": {
   "authors": [
    [
     "Kentaro",
     "Seki"
    ],
    [
     "Shinnosuke",
     "Takamichi"
    ],
    [
     "Norihiro",
     "Takamune"
    ],
    [
     "Yuki",
     "Saito"
    ],
    [
     "Kanami",
     "Imamura"
    ],
    [
     "Hiroshi",
     "Saruwatari"
    ]
   ],
   "title": "Spatial Voice Conversion: Voice Conversion Preserving Spatial Information and Non-target Signals",
   "original": "1107",
   "order": 37,
   "page_count": 5,
   "abstract": [
    "This paper proposes a new task called spatial voice conversion, which aims to convert a target voice while preserving spatial information and non-target signals. Traditional voice conversion methods focus on single-channel waveforms, ignoring the stereo listening experience inherent in human hearing. Our baseline approach addresses this gap by integrating blind source separation (BSS), voice conversion (VC), and spatial mixing to handle multi-channel waveforms. Through experimental evaluations, we organize and identify the key challenges inherent in this task, such as maintaining audio quality and accurately preserving spatial information. Our results highlight the fundamental difficulties in balancing these aspects, providing a benchmark for future research in spatial voice conversion. The proposed method's code is publicly available to encourage further exploration in this domain."
   ],
   "p1": 177,
   "pn": 181,
   "doi": "10.21437/Interspeech.2024-1107",
   "url": "interspeech_2024/seki24_interspeech.html"
  },
  "benway24_interspeech": {
   "authors": [
    [
     "Nina R.",
     "Benway"
    ],
    [
     "Jonathan L.",
     "Preston"
    ],
    [
     "Carol",
     "Espy-Wilson"
    ]
   ],
   "title": "Examining Vocal Tract Coordination in Childhood Apraxia of Speech with Acoustic-to-Articulatory Speech Inversion Feature Sets",
   "original": "1114",
   "order": 1052,
   "page_count": 5,
   "abstract": [
    "Childhood apraxia of speech is a genetically driven, neurodevelopmental speech sound disorder with speech deficits theorized to reflect difficulty in the spatiotemporal programming of speech movements. Therefore, this work examined how well articulatory coordination features generated from audio-estimated kinematic data distinguished speakers with childhood apraxia of speech versus non-apraxic speech sound disorder. Two correlation-based feature sets motivated by recent literature demonstrated high performance in replicated 6-fold nested cross validated studies, with no statistically significant difference between feature sets (mean AUROC = .90, σ = .04). An ablation study emphasized the importance of source-filter coordination in this population of apraxic speakers, with the source-ablated feature set performing significantly worse than the lip-ablated, the tongue-ablated, and full feature set (ΔAUROC = -.19, SE = 0.01, p < .001)."
   ],
   "p1": 5138,
   "pn": 5142,
   "doi": "10.21437/Interspeech.2024-1114",
   "url": "interspeech_2024/benway24_interspeech.html"
  },
  "stein24_interspeech": {
   "authors": [
    [
     "Anna",
     "Stein"
    ],
    [
     "Kevin",
     "Tang"
    ]
   ],
   "title": "Modeling probabilistic reduction across domains with Naive Discriminative Learning",
   "original": "1118",
   "resource": "https://doi.org/10.5281/zenodo.12755169",
   "order": 875,
   "page_count": 5,
   "abstract": [
    "The predictability of a word modulates its acoustic duration. Such probabilistic effects can compete across linguistic domains (segments, syllables and adjacent-word contexts e.g., frequent words with infrequent syllables) and across local and aggregate contexts (e.g., a generally unpredictable word in a predictable context). This study aims to tease apart competing effects using Naive Discriminative Learning, which incorporates cue competition. The model was trained on English conversational speech from the Buckeye Corpus, using words as outcomes and segments, syllables, and adjacent words as cues. The connections between cues and outcomes were used to predict acoustic word duration. Results show that a word's duration is more strongly predicted by its syllables than its segments, and  a word's predictability aggregated over all contexts is a stronger predictor than its specific local contexts. Our study presents a unified approach to modeling competition in probabilistic reduction."
   ],
   "p1": 4253,
   "pn": 4257,
   "doi": "10.21437/Interspeech.2024-1118",
   "url": "interspeech_2024/stein24_interspeech.html"
  },
  "koudounas24_interspeech": {
   "authors": [
    [
     "Alkis",
     "Koudounas"
    ],
    [
     "Gabriele",
     "Ciravegna"
    ],
    [
     "Marco",
     "Fantini"
    ],
    [
     "Erika",
     "Crosetti"
    ],
    [
     "Giovanni",
     "Succo"
    ],
    [
     "Tania",
     "Cerquitelli"
    ],
    [
     "Elena",
     "Baralis"
    ]
   ],
   "title": "Voice Disorder Analysis: a Transformer-based Approach",
   "original": "1122",
   "order": 624,
   "page_count": 5,
   "abstract": [
    "Voice disorders are pathologies significantly affecting patient quality of life. However, non-invasive automated diagnosis of these pathologies is still under-explored, due to both a shortage of pathological voice data, and diversity of the recording types used for the diagnosis. This paper proposes a novel solution that adopts transformers directly working on raw voice signals and addresses data shortage through synthetic data generation and data augmentation. Further, we consider many recording types at the same time, such as sentence reading and sustained vowel emission, by employing a Mixture of Expert ensemble to align the predictions on different data types. The experimental results, obtained on both public and private datasets, show the effectiveness of our solution in the disorder detection and classification tasks and largely improve over existing approaches. "
   ],
   "p1": 3040,
   "pn": 3044,
   "doi": "10.21437/Interspeech.2024-1122",
   "url": "interspeech_2024/koudounas24_interspeech.html"
  },
  "nam24b_interspeech": {
   "authors": [
    [
     "KiHyun",
     "Nam"
    ],
    [
     "Hee-Soo",
     "Heo"
    ],
    [
     "Jee-weon",
     "Jung"
    ],
    [
     "Joonson",
     "Chung"
    ]
   ],
   "title": "Disentangled Representation Learning for Environment-agnostic Speaker Recognition",
   "original": "1124",
   "order": 441,
   "page_count": 5,
   "abstract": [
    "This work presents a framework based on feature disentanglement to learn speaker embeddings that are robust to environmental variations. Our framework utilises an auto-encoder as a disentangler, dividing the input speaker embedding into components related to the speaker and other residual information. We employ a group of objective functions to ensure that the auto-encoder's code representation - used as the refined embedding - condenses only the speaker characteristics. We show the versatility of our framework through its compatibility with any existing speaker embedding extractor, requiring no structural modifications or adaptations for integration. We validate the effectiveness of our framework by incorporating it into two popularly used embedding extractors and conducting experiments across various benchmarks. The results show a performance improvement of up to 16%. We release our code for this work to be available here."
   ],
   "p1": 2130,
   "pn": 2134,
   "doi": "10.21437/Interspeech.2024-1124",
   "url": "interspeech_2024/nam24b_interspeech.html"
  },
  "koriyama24_interspeech": {
   "authors": [
    [
     "Tomoki",
     "Koriyama"
    ]
   ],
   "title": "VAE-based Phoneme Alignment Using Gradient Annealing and SSL Acoustic Features",
   "original": "1127",
   "order": 786,
   "page_count": 5,
   "abstract": [
    "This paper presents an accurate phoneme alignment model that aims for speech analysis and video content creation. We propose a variational autoencoder (VAE)-based alignment model in which a probable path is searched using encoded acoustic and linguistic embeddings in an unsupervised manner. Our proposed model is based on one TTS alignment (OTA) and extended to obtain phoneme boundaries. Specifically, we incorporate a VAE architecture to maintain consistency between the embedding and input, apply gradient annealing to avoid local optimum during training, and introduce a self-supervised learning (SSL)-based acoustic-feature input and state-level linguistic unit to utilize rich and detailed information. Experimental results show that the proposed model generated phoneme boundaries closer to annotated ones compared with the conventional OTA model, the CTC-based segmentation model, and the widely-used tool MFA."
   ],
   "p1": 3814,
   "pn": 3818,
   "doi": "10.21437/Interspeech.2024-1127",
   "url": "interspeech_2024/koriyama24_interspeech.html"
  },
  "ewert24_interspeech": {
   "authors": [
    [
     "Iva",
     "Ewert"
    ],
    [
     "Marvin",
     "Borsdorf"
    ],
    [
     "Haizhou",
     "Li"
    ],
    [
     "Tanja",
     "Schultz"
    ]
   ],
   "title": "Does the Lombard Effect Matter in Speech Separation? Introducing the Lombard-GRID-2mix Dataset",
   "original": "1131",
   "order": 117,
   "page_count": 5,
   "abstract": [
    "Inspired by the human ability of selective listening, speech separation aims to equip machines with the capability to disentangle cocktail party soundscapes into the individual sound sources. Recently, neural network based algorithms have been studied to work reliably under various conditions. However, to the best of our knowledge, a change in the speaking style has not yet been studied. The Lombard effect, a reflexive change in speaking style triggered by noisy environments, is a typical behavior in everyday conversational situations. In this work, we introduce a new first of its kind dataset, called Lombard-GRID-2mix, to study speech separation for two-speaker mixtures on normal speech and Lombard speech. In a comprehensive study, we show that speech separation systems can be equipped to work for both normal speech and Lombard speech. We apply a carefully designed finetuning method to enable the system to work even if noise is present in the Lombard speech for different SNR ratios."
   ],
   "p1": 577,
   "pn": 581,
   "doi": "10.21437/Interspeech.2024-1131",
   "url": "interspeech_2024/ewert24_interspeech.html"
  },
  "hughes24_interspeech": {
   "authors": [
    [
     "Cliodhna",
     "Hughes"
    ],
    [
     "Guy",
     "Brown"
    ],
    [
     "Ning",
     "Ma"
    ],
    [
     "Nicola",
     "Dibben"
    ]
   ],
   "title": "Acoustic Effects of Facial Feminisation Surgery on Speech and Singing: A Case Study",
   "original": "1132",
   "order": 629,
   "page_count": 5,
   "abstract": [
    "Transfeminine people may undergo facial feminisation surgery, a term covering a range of procedures that aim to alter the appearance of facial features, thereby potentially changing characteristics of the vocal tract. Effects of facial feminisation surgery on the voice are relatively understudied, however, so, little information on the vocal effects of these surgeries is available to people considering undergoing these procedures. In this single-case study, we present an acoustic analysis of speech and singing data collected from a transgender singer before and after facial feminisation surgery, alongside an examination of longitudinal interview data from the participant. Our quantitative results suggest facial feminisation surgery can have an impact on the voice, and our qualitative analysis suggests this may not only be as a result of the altered characteristics of the vocal tract, but also as a result of the altered social context. Several issues for future research are identified."
   ],
   "p1": 3065,
   "pn": 3069,
   "doi": "10.21437/Interspeech.2024-1132",
   "url": "interspeech_2024/hughes24_interspeech.html"
  },
  "laperriere24_interspeech": {
   "authors": [
    [
     "Gaëlle",
     "Laperrière"
    ],
    [
     "Sahar",
     "Ghannay"
    ],
    [
     "Bassam",
     "Jabaian"
    ],
    [
     "Yannick",
     "Estève"
    ]
   ],
   "title": "A dual task learning approach to fine-tune a multilingual semantic speech encoder for Spoken Language Understanding",
   "original": "1133",
   "order": 164,
   "page_count": 5,
   "abstract": [
    "Self-Supervised Learning is vastly used to efficiently represent speech for Spoken Language Understanding, gradually replacing conventional approaches. Meanwhile, textual SSL models are proposed to encode language-agnostic semantics. SAMU-XLSR framework employed this semantic information to enrich multilingual speech representations. A recent study investigated SAMU-XLSR in-domain semantic enrichment by specializing it on downstream transcriptions, leading to state-of-the-art results on a challenging SLU task. This study's interest lies in the loss of multilingual performances and lack of specific-semantics training induced by such specialization in close languages without any SLU implication. We also consider SAMU-XLSR's loss of initial cross-lingual abilities due to a separate SLU fine-tuning. Therefore, this paper proposes a dual task learning approach to improve SAMU-XLSR semantic enrichment while considering distant languages for multilingual and language portability experiments."
   ],
   "p1": 812,
   "pn": 816,
   "doi": "10.21437/Interspeech.2024-1133",
   "url": "interspeech_2024/laperriere24_interspeech.html"
  },
  "kealey24_interspeech": {
   "authors": [
    [
     "Jacob",
     "Kealey"
    ],
    [
     "John R.",
     "Hershey"
    ],
    [
     "François",
     "Grondin"
    ]
   ],
   "title": "Unsupervised Improved MVDR Beamforming for Sound Enhancement",
   "original": "1136",
   "order": 450,
   "page_count": 5,
   "abstract": [
    "Neural networks have recently become the dominant approach to sound separation. Their good performance relies on large datasets of isolated recordings. For speech and music, isolated single channel data are readily available; however, the same does not hold in the multi-channel case, and with most other sound classes. Multi-channel methods have the potential to outperform single channel approaches as they can exploit both spatial and spectral features, but the lack of training data remains a challenge. We propose unsupervised improved minimum variation distortionless response (UIMVDR), which enables multi-channel separation to leverage in-the-wild single-channel data through unsupervised training and beamforming. Results show that UIMVDR generalizes well and improves separation performance compared to supervised models, particularly in cases with limited supervised data. By using data available online, it also reduces the effort required to gather data for multi-channel approaches."
   ],
   "p1": 2175,
   "pn": 2179,
   "doi": "10.21437/Interspeech.2024-1136",
   "url": "interspeech_2024/kealey24_interspeech.html"
  },
  "bando24_interspeech": {
   "authors": [
    [
     "Yoshiaki",
     "Bando"
    ],
    [
     "Tomohiko",
     "Nakamura"
    ],
    [
     "Shinji",
     "Watanabe"
    ]
   ],
   "title": "Neural Blind Source Separation and Diarization for Distant Speech Recognition",
   "original": "1137",
   "order": 146,
   "page_count": 5,
   "abstract": [
    "This paper presents a neural method for distant speech recognition (DSR) that jointly separates and diarizes speech mixtures without supervision by isolated signals. A standard separation method for multi-talker DSR is a statistical multichannel method called guided source separation (GSS). While GSS does not require signal-level supervision, it relies on speaker diarization results to handle unknown numbers of active speakers. To overcome this limitation, we introduce and train a neural inference model in a weakly-supervised manner, employing the objective function of a statistical separation method. This training requires only multichannel mixtures and their temporal annotations of speaker activities. In contrast to GSS, the trained model can jointly separate and diarize speech mixtures without any auxiliary information. The experiments with the AMI corpus show that our method outperforms GSS with oracle diarization results regarding word error rates. The code is available online."
   ],
   "p1": 722,
   "pn": 726,
   "doi": "10.21437/Interspeech.2024-1137",
   "url": "interspeech_2024/bando24_interspeech.html"
  },
  "cheng24b_interspeech": {
   "authors": [
    [
     "Jian",
     "Cheng"
    ]
   ],
   "title": "Context-Aware Speech Recognition Using Prompts for Language Learners",
   "original": "1142",
   "order": 825,
   "page_count": 5,
   "abstract": [
    "We aim to enhance automatic speech recognition (ASR) systems with context-aware prompts, improving accuracy without needing complex domain-specific language models or fine-tuning. This is particularly valuable for spoken language learning, where instruction/assessment apps often present short spoken texts to elicit spoken responses. These elicitors reduce the range of expected, sensible spoken responses. Prompting ASR engines (Whisper and Gemini Audio) with an utterance's elicitor yields context-awareness and significantly improves performance. In two L2 English datasets, using elicitor texts as prompts improved Whisper and Gemini accuracy by up to 24.0% (relative WER). For one activity type, the elicitor text reduces errors in target words by half. Out-of-domain, prompt-enhanced Gemini bettered a conventional ASR system trained on in-domain data by 35.3% (relative WER); enhanced Whisper bettered it by 21.3%."
   ],
   "p1": 4009,
   "pn": 4013,
   "doi": "10.21437/Interspeech.2024-1142",
   "url": "interspeech_2024/cheng24b_interspeech.html"
  },
  "daoudi24_interspeech": {
   "authors": [
    [
     "Khalid",
     "Daoudi"
    ],
    [
     "Solange",
     "Milhé de Saint Victor"
    ],
    [
     "Alexandra",
     "Foubert-Samier"
    ],
    [
     "Margherita",
     "Fabbri"
    ],
    [
     "Anne",
     "Pavy-Le Traon"
    ],
    [
     "Olivier",
     "Rascol"
    ],
    [
     "Virginie",
     "Woisard"
    ],
    [
     "Wassilios G.",
     "Meissner"
    ]
   ],
   "title": "Electroglottography for the assessment of dysphonia in Parkinson's disease and multiple system atrophy",
   "original": "1144",
   "order": 848,
   "page_count": 5,
   "abstract": [
    "Electroglottography (EGG) is a noninvasive technique which allows accurate measurements of vocal folds dynamics and perturbations during speech. It has been widely used in medical diagnosis and monitoring of several laryngeal pathologies, but its use in neurological disorders has been very limited. This paper presents the first study on EGG in early stages of Parkinson's disease (PD) and an atypical parkinsonian disorder, multiple system atrophy (MSA). Our first objective was to investigate whether EGG can reveal distinctive dysphonia features that could help in the challenging early differential diagnosis between PD and MSA-P, the parkinsonian variant of MSA. The second objective was to verify some hypothesis on early markers of PD drawn from speech-alone acoustic analysis.  For MSA-P patients, our analysis revealed a reduced open quotient and confirmed excessive pitch variation. The analysis also mitigated some hypothesis on dysphonia in early stages of PD."
   ],
   "p1": 4124,
   "pn": 4128,
   "doi": "10.21437/Interspeech.2024-1144",
   "url": "interspeech_2024/daoudi24_interspeech.html"
  },
  "arya24_interspeech": {
   "authors": [
    [
     "Arunav",
     "Arya"
    ],
    [
     "Murtiza",
     "Ali"
    ],
    [
     "Karan",
     "Nathwani"
    ]
   ],
   "title": "Exploiting Wavelet Scattering Transform for an Unsupervised Speaker Diarization in Deep Neural Network Framework",
   "original": "1146",
   "order": 11,
   "page_count": 5,
   "abstract": [
    "Advancements in diarization have prompted the development of supervised learning models. These models extract fixed-length embeddings from audio files of varying lengths. Despite challenges, commercial API models like Speechbrain, Resemblyzer, Whisper AI, and Pyannote have addressed this issue. However, these models typically utilize Mel-Frequency Cepstral Coefficients (MFCC) features, convolution layers, and dimension reduction techniques to create embeddings. Our proposal method introduces a Wavelet Scattering Transform (WST) that prioritizes information content, allowing users to customize the shape of embeddings according to their model requirements. Coupling WST with AutoEncoders (WST-AE) in a residual manner enhances semantic latent space representations, which can be clustered segment-wise in an unsupervised manner. Testing on AMI and VoxConverse datasets has shown a reduction in Diarization Error Rate (DER) with fewer training parameters and without the need for separate embedding models."
   ],
   "p1": 47,
   "pn": 51,
   "doi": "10.21437/Interspeech.2024-1146",
   "url": "interspeech_2024/arya24_interspeech.html"
  },
  "barnhill24_interspeech": {
   "authors": [
    [
     "Alexander",
     "Barnhill"
    ],
    [
     "Elmar",
     "Noeth"
    ],
    [
     "Andreas",
     "Maier"
    ],
    [
     "Christian",
     "Bergler"
    ]
   ],
   "title": "ANIMAL-CLEAN – A Deep Denoising Toolkit for Animal-Independent Signal Enhancement",
   "original": "1151",
   "resource": "https://doi.org/10.5281/zenodo.12733949",
   "order": 128,
   "page_count": 5,
   "abstract": [
    "Signal enhancement in bioacoustics can be of vital importance due to the fact that recordings are largely done in noise-heavy environments, in which anthrophonic, geophonic, or biophonic disturbances are myriad and can impede downstream analysis. Existing audio denoising techniques largely focus on human speech, whereas non-human animal vocalizations may not be compatible with frequency or temporal assumptions present in those methods. This work introduces ANIMAL–CLEAN, an animal-independent, Noise2Noise-based deep denoising toolkit utilizing a combination of signals from the target of interest as well as existing datasets used for human speech enhancement. This toolkit enables users to develop deep learning models capable of denoising bioacoustic signals with a wide range of frequencies and signal lengths, which, when used as a preprocessing step, improve results for downstream signal segmentation and classification, without the need for noise-free bioacoustic recordings."
   ],
   "p1": 632,
   "pn": 636,
   "doi": "10.21437/Interspeech.2024-1151",
   "url": "interspeech_2024/barnhill24_interspeech.html"
  },
  "fernandezlopez24_interspeech": {
   "authors": [
    [
     "Adriana",
     "Fernandez-Lopez"
    ],
    [
     "Honglie",
     "Chen"
    ],
    [
     "Pingchuan",
     "Ma"
    ],
    [
     "Lu",
     "Yin"
    ],
    [
     "Qiao",
     "Xiao"
    ],
    [
     "Stavros",
     "Petridis"
    ],
    [
     "Shiwei",
     "Liu"
    ],
    [
     "Maja",
     "Pantic"
    ]
   ],
   "title": "MSRS: Training Multimodal Speech Recognition Models from Scratch with Sparse Mask Optimization",
   "original": "1153",
   "order": 580,
   "page_count": 5,
   "abstract": [
    "Pre-trained models have been a foundational approach in speech recognition, albeit with associated additional costs. In this study, we propose a regularization technique that facilitates the training of visual and audio-visual speech recognition models (VSR and AVSR) from scratch. This approach, abbreviated as MSRS (Multimodal Speech Recognition from Scratch), introduces a sparse regularization that rapidly learns sparse structures within the dense model at the very beginning of training, which receives healthier gradient flow than the dense equivalent. Once the sparse mask stabilizes, our method allows transitioning to a dense model or keeping a sparse model by updating non-zero values. MSRS achieves competitive results in VSR and AVSR with 21.1% and 0.9% WER on the LRS3 benchmark, while reducing training time by at least 2x. We explore other sparse approaches and show that only MSRS enables training from scratch by implicitly masking the weights affected by vanishing gradients."
   ],
   "p1": 2820,
   "pn": 2824,
   "doi": "10.21437/Interspeech.2024-1153",
   "url": "interspeech_2024/fernandezlopez24_interspeech.html"
  },
  "tanner24_interspeech": {
   "authors": [
    [
     "James",
     "Tanner"
    ],
    [
     "Morgan",
     "Sonderegger"
    ],
    [
     "Jane",
     "Stuart-Smith"
    ],
    [
     "Tyler",
     "Kendall"
    ],
    [
     "Jeff",
     "Mielke"
    ],
    [
     "Robin",
     "Dodsworth"
    ],
    [
     "Erik",
     "Thomas"
    ]
   ],
   "title": "Exploring the anatomy of articulation rate in spontaneous English speech: relationships between utterance length effects and social factors",
   "original": "1154",
   "order": 95,
   "page_count": 5,
   "abstract": [
    "Speech rate has been shown to vary across social categories such as gender, age, and dialect, while also being conditioned by properties of speech planning. The effect of utterance length, where speech rate is faster and less variable for longer utterances, has also been shown to reduce the role of social factors once it has been accounted for, leaving unclear the relationship between social factors and speech production in conditioning speech rate. Through modelling of speech rate across 13 English speech corpora, it is found that utterance length has the largest effect on speech rate, though this effect itself varies little across corpora and speakers. While age and gender also modulate speech rate, their effects are much smaller in magnitude. These findings suggest utterance length effects may be conditioned by articulatory and perceptual constraints, and that social influences on speech rate should be interpreted in the broader context of how speech rate variation is structured."
   ],
   "p1": 467,
   "pn": 471,
   "doi": "10.21437/Interspeech.2024-1154",
   "url": "interspeech_2024/tanner24_interspeech.html"
  },
  "kim24l_interspeech": {
   "authors": [
    [
     "Lila",
     "Kim"
    ],
    [
     "Cédric",
     "Gendrot"
    ]
   ],
   "title": "Using wav2vec 2.0 for phonetic classification tasks: methodological aspects",
   "original": "1155",
   "order": 315,
   "page_count": 5,
   "abstract": [
    "Self-supervised learning, particularly in the context of speech, has been shown to be effective in a variety of tasks such as speaker recognition and speaker verification. Our research question focuses on the effectiveness of vector representations extracted from shorter versus longer phoneme sequences in detecting nasality. Two distinct approaches were studied: extracting vectors over the duration of the phoneme and taking longer sequences with one second added on each side of the phoneme, then recovering the central part a posteriori. The results show that the models react differently depending on the phone and the speaker, with variability observed at both levels. The long sequence model outperformed the short sequence model by correlating more robustly with nasal airflow. "
   ],
   "p1": 1530,
   "pn": 1534,
   "doi": "10.21437/Interspeech.2024-1155",
   "url": "interspeech_2024/kim24l_interspeech.html"
  },
  "wang24u_interspeech": {
   "authors": [
    [
     "Zhaoyu",
     "Wang"
    ],
    [
     "Haohe",
     "Liu"
    ],
    [
     "Harry",
     "Coppock"
    ],
    [
     "Björn",
     "Schuller"
    ],
    [
     "Mark D.",
     "Plumbley"
    ]
   ],
   "title": "Neural Compression Augmentation for Contrastive Audio Representation Learning",
   "original": "1156",
   "order": 683,
   "page_count": 5,
   "abstract": [
    "The choice of data augmentation is pivotal in contrastive self-supervised learning. Current augmentation techniques for audio data, such as the widely used Random Resize Crop (RRC), underperform in pitch-sensitive music tasks and lack generalisation across various types of audio. This study aims to address these limitations by introducing Neural Compression Augmentation (NCA), an approach based on lossy neural compression. We use the Audio Barlow Twins (ABT), a contrastive self-supervised framework for audio, as our backbone. We experiment with both NCA and several baseline augmentation methods in the augmentation block of ABT and train the models on AudioSet. Experimental results show that models integrated with NCA considerably surpass the original performance of ABT, especially in the music tasks of the HEAR benchmark, demonstrating the effectiveness of compression-based augmentation for audio contrastive self-supervised learning."
   ],
   "p1": 3335,
   "pn": 3339,
   "doi": "10.21437/Interspeech.2024-1156",
   "url": "interspeech_2024/wang24u_interspeech.html"
  },
  "choi24b_interspeech": {
   "authors": [
    [
     "Kwanghee",
     "Choi"
    ],
    [
     "Ankita",
     "Pasad"
    ],
    [
     "Tomohiko",
     "Nakamura"
    ],
    [
     "Satoru",
     "Fukayama"
    ],
    [
     "Karen",
     "Livescu"
    ],
    [
     "Shinji",
     "Watanabe"
    ]
   ],
   "title": "Self-Supervised Speech Representations are More Phonetic than Semantic",
   "original": "1157",
   "resource": "https://doi.org/10.5281/zenodo.12741309",
   "order": 940,
   "page_count": 5,
   "abstract": [
    "Self-supervised speech models (S3Ms) have become an effective backbone for speech applications. Various analyses suggest that S3Ms encode linguistic properties. In this work, we seek a more fine-grained analysis of the word-level linguistic properties encoded in S3Ms. Specifically, we curate a novel dataset of near homophone (phonetically similar) and synonym (semantically similar) word pairs and measure the similarities between S3M word representation pairs. Our study reveals that S3M representations consistently and significantly exhibit more phonetic than semantic similarity. Further, we question whether widely used intent classification datasets such as Fluent Speech Commands and Snips Smartlights are adequate for measuring semantic abilities. Our simple baseline, using only the word identity, surpasses S3M-based models. This corroborates our findings and suggests that high scores on these datasets do not necessarily guarantee the presence of semantic content."
   ],
   "p1": 4578,
   "pn": 4582,
   "doi": "10.21437/Interspeech.2024-1157",
   "url": "interspeech_2024/choi24b_interspeech.html"
  },
  "hutiri24_interspeech": {
   "authors": [
    [
     "Wiebke",
     "Hutiri"
    ],
    [
     "Tanvina",
     "Patel"
    ],
    [
     "Aaron Yi",
     "Ding"
    ],
    [
     "Odette",
     "Scharenborg"
    ]
   ],
   "title": "As Biased as You Measure: Methodological Pitfalls of Bias Evaluations in Speaker Verification Research",
   "original": "1158",
   "order": 878,
   "page_count": 5,
   "abstract": [
    "Detecting and mitigating bias in speaker verification systems is important, as datasets, processing choices and algorithms can lead to performance differences that systematically favour some groups of people while disadvantaging others. Prior studies have thus measured performance differences across groups to evaluate bias. However, when comparing results across studies, it becomes apparent that they draw contradictory conclusions, hindering progress in this area. In this paper we investigate how measurement impacts the outcomes of bias evaluations. We show empirically that bias evaluations are strongly influenced by base metrics that measure performance, by the choice of ratio or difference-based bias measure, and by the aggregation of bias measures into meta-measures. Based on our findings, we recommend the use of ratio-based bias measures, in particular when the values of base metrics are small, or when base metrics with different orders of magnitude need to be compared."
   ],
   "p1": 4268,
   "pn": 4272,
   "doi": "10.21437/Interspeech.2024-1158",
   "url": "interspeech_2024/hutiri24_interspeech.html"
  },
  "mcghee24_interspeech": {
   "authors": [
    [
     "Charles",
     "McGhee"
    ],
    [
     "Kate",
     "Knill"
    ],
    [
     "Mark",
     "Gales"
    ]
   ],
   "title": "Highly Intelligible Speaker-Independent Articulatory Synthesis",
   "original": "1160",
   "order": 691,
   "page_count": 5,
   "abstract": [
    "An articulatory synthesiser which could accurately map vocal tract features to speech would enable novel evaluation of acoustic-to-articulatory inversion models beyond the small, typically monolingual, articulatory datasets available. However, current deep articulatory synthesisers and physical simulation-based synthesisers struggle to produce consistently intelligible speech, with Word Error Rates (WER) of around 20% for real or hand-crafted articulatory input. Additionally, deep learning methods have often only achieved this level of intelligibility when training and evaluating on the same speaker (speaker-dependent training). In this paper, we create a highly intelligible (WER 7% for real data and 10% for synthetic), speaker-independent articulatory synthesiser by training a deep synthesiser on a combination of high-quality real data and synthetic data generated by inversion. We then perform a multilingual evaluation of the joint inversion-synthesis system."
   ],
   "p1": 3375,
   "pn": 3379,
   "doi": "10.21437/Interspeech.2024-1160",
   "url": "interspeech_2024/mcghee24_interspeech.html"
  },
  "wei24_interspeech": {
   "authors": [
    [
     "Xizi",
     "Wei"
    ],
    [
     "Stephen",
     "McGregor"
    ]
   ],
   "title": "Prompt Tuning for Speech Recognition on Unknown Spoken Name Entities",
   "original": "1162",
   "order": 154,
   "page_count": 5,
   "abstract": [
    "This paper explores the challenge of recognising relevant but previously unheard named entities in spoken input. This scenario pertains to real-world applications where establishing an automatic speech recognition (ASR) model trained on new entity phrases may not be efficient. We propose a technique that involves fine-tuning a Whisper model with a list of entity phrases as prompts. We establish a task-specific dataset where stratification of different entity phrases supports evaluation of three different scenarios in which entities might be encountered. We focus our analysis on a seen-but-unheard scenario, reflecting a situation where only textual representations of novel entity phrases are available for a commercial banking assistant bot. We show that a model tuned to anticipate prompts reflecting novel named entities makes substantial improvements in entity recall over non-tuned baseline models, and meaningful improvements in performance over models fine-tuned without a prompt. "
   ],
   "p1": 762,
   "pn": 766,
   "doi": "10.21437/Interspeech.2024-1162",
   "url": "interspeech_2024/wei24_interspeech.html"
  },
  "uro24_interspeech": {
   "authors": [
    [
     "Rémi",
     "Uro"
    ],
    [
     "Marie",
     "Tahon"
    ],
    [
     "David",
     "Doukhan"
    ],
    [
     "Antoine",
     "Laurent"
    ],
    [
     "Albert",
     "Rilliard"
    ]
   ],
   "title": "Detecting the terminality of speech-turn boundary for spoken interactions in French TV and Radio content",
   "original": "1163",
   "order": 728,
   "page_count": 5,
   "abstract": [
    "Transition Relevance Places are defined as the end of an utterance where the interlocutor may take the floor without interrupting the current speaker -i.e., a place where the turn is terminal. Analyzing turn terminality is useful to study the dynamic of turn-taking in spontaneous conversations. This paper presents an automatic classification of spoken utterances as Terminal or Non-Terminal in multi-speaker settings. We compared audio, text, and fusions of both approaches on a French corpus of TV and Radio extracts annotated with turn-terminality information at each speaker change. Our models are based on pre-trained self-supervised representations. We report results for different fusion strategies and varying context sizes. This study also questions the problem of performance variability by analyzing the differences in results for multiple training runs with random initialization. The measured accuracy would allow the use of these models for large-scale analysis of turn-taking."
   ],
   "p1": 3560,
   "pn": 3564,
   "doi": "10.21437/Interspeech.2024-1163",
   "url": "interspeech_2024/uro24_interspeech.html"
  },
  "liu24g_interspeech": {
   "authors": [
    [
     "Liwei",
     "Liu"
    ],
    [
     "Huihui",
     "Wei"
    ],
    [
     "Dongya",
     "Liu"
    ],
    [
     "Zhonghua",
     "Fu"
    ]
   ],
   "title": "HarmoNet: Partial DeepFake Detection Network based on Multi-scale HarmoF0 Feature Fusion",
   "original": "1164",
   "order": 466,
   "page_count": 5,
   "abstract": [
    "Audio DeepFake detection (ADD) has become an increasingly challenging task recently, with the rise of various spoofing attacks utilizing artificially generated audio. The track 2 of ADD 2023 requires not only detecting DeepFake audio but also locating the manipulated regions. To tackle this unique challenge, we have proposed an innovative framework HarmoNet that leverages the Multi-scale harmonic F0 and Wav2Vec features with attention mechanism. This allows the model to effectively capture changes in each region of the utterance. Furthermore, we have introduced a new loss function named Partial Loss, which focuses more on the boundary between real and fake region. Additionally, we have designed a post-processor to refine the output of the model. Our framework achieved 70.61% in track 2 of ADD 2023, an improvement of 67.12% over baseline, and achieved the best performance. Moreover, HarmoNet also shows competitive performance on other DeepFake datasets."
   ],
   "p1": 2255,
   "pn": 2259,
   "doi": "10.21437/Interspeech.2024-1164",
   "url": "interspeech_2024/liu24g_interspeech.html"
  },
  "phan24_interspeech": {
   "authors": [
    [
     "Nhan",
     "Phan"
    ],
    [
     "Anna",
     "von Zansen"
    ],
    [
     "Maria",
     "Kautonen"
    ],
    [
     "Ekaterina",
     "Voskoboinik"
    ],
    [
     "Tamas",
     "Grosz"
    ],
    [
     "Raili",
     "Hilden"
    ],
    [
     "Mikko",
     "Kurimo"
    ]
   ],
   "title": "Automated content assessment and feedback for Finnish L2 learners in a picture description speaking task",
   "original": "1166",
   "resource": "https://doi.org/10.5281/zenodo.11385109",
   "order": 65,
   "page_count": 5,
   "abstract": [
    "We propose a framework to address several unsolved challenges in second language (L2) automatic speaking assessment (ASA) and feedback. The challenges include: 1. ASA of visual task completion, 2. automated content grading and explanation of spontaneous L2 speech, 3. corrective feedback generation for L2 learners, and 4. all the above for a language that has minimal speech data of L2 learners. The proposed solution combines visual natural language generation (NLG), automatic speech recognition (ASR) and prompting a large language model (LLM) for low-resource L2 learners. We describe the solution and the outcomes of our case study for a picture description task in Finnish. Our results indicate substantial agreement with human experts in grading, explanation and feedback. This framework has the potential for a significant impact in constructing next-generation computer-assisted language learning systems to provide automatic scoring with feedback for learners of low-resource languages."
   ],
   "p1": 317,
   "pn": 321,
   "doi": "10.21437/Interspeech.2024-1166",
   "url": "interspeech_2024/phan24_interspeech.html"
  },
  "hutin24_interspeech": {
   "authors": [
    [
     "Mathilde",
     "Hutin"
    ],
    [
     "Junfei",
     "Hu"
    ],
    [
     "Liesbeth",
     "Degand"
    ]
   ],
   "title": "Uh, um and mh: Are filled pauses prone to conversational converge?",
   "original": "1168",
   "order": 731,
   "page_count": 5,
   "abstract": [
    "Filled pauses are very frequent words, such as uh, um or mh in English. They have been shown to serve many purposes in interaction management, yet whether these conversational functions are intended by the speaker to ease the interaction (hearer-oriented) or just interpretations by the interlocutor of otherwise unintentional vocalizations (speaker-oriented) remains an open question. To participate in this debate, we investigate whether filled pauses converge in terms of form (uh, um or mh) in the course of 5 conversations made up of 12 asymmetrical sub-conversations. Results show that each conversation displays a different pattern in the choice of the filled pause, yet none of them shows any clear evidence that participants align with their interlocutors. This indicates that, unlike other frequent words from the lexicon, filled pauses do not converge, at least in their form, and that their use is more likely speaker-oriented than hearer-oriented."
   ],
   "p1": 3575,
   "pn": 3579,
   "doi": "10.21437/Interspeech.2024-1168",
   "url": "interspeech_2024/hutin24_interspeech.html"
  },
  "gessinger24_interspeech": {
   "authors": [
    [
     "Iona",
     "Gessinger"
    ],
    [
     "Bistra",
     "Andreeva"
    ],
    [
     "Benjamin R.",
     "Cowan"
    ]
   ],
   "title": "The Use of Modifiers and f0 in Remote Referential Communication with Human and Computer Partners",
   "original": "1169",
   "resource": "https://doi.org/10.17605/OSF.IO/VUAJM",
   "order": 324,
   "page_count": 5,
   "abstract": [
    "The present study investigates referring expressions in a remote interaction context with a human or computer partner (both simulated). Across these conditions, we compare the effect of competitor information being available to both partners (common ground) or only the speaker (privileged ground) on target item descriptions. We analyse the number of adjectival modifiers uttered and show that participants responded to the manipulation of information status in both partner conditions. In addition, we examine whether the information status also affects the prosodic realisation of the descriptions. No sufficient evidence was found for this. As expected, adjectives showed a slightly higher peak f0 when a competitor was present in the common ground than when there was no competitor. However, when analysing the overall f0 contour, there was no systematic difference between conditions."
   ],
   "p1": 1575,
   "pn": 1579,
   "doi": "10.21437/Interspeech.2024-1169",
   "url": "interspeech_2024/gessinger24_interspeech.html"
  },
  "borsdorf24_interspeech": {
   "authors": [
    [
     "Marvin",
     "Borsdorf"
    ],
    [
     "Zexu",
     "Pan"
    ],
    [
     "Haizhou",
     "Li"
    ],
    [
     "Tanja",
     "Schultz"
    ]
   ],
   "title": "wTIMIT2mix: A Cocktail Party Mixtures Database to Study Target Speaker Extraction for Normal and Whispered Speech",
   "original": "1172",
   "order": 1032,
   "page_count": 5,
   "abstract": [
    "Target speaker extraction (TSE) seeks to single out a target speaker's voice from a given speech mixture signal with the help of a target reference signal. This algorithm enables novel speech applications such as smart hearing aids. A TSE system has to work reliably in any everyday conversational situation. This may also include speakers who switch naturally between normal and whispered speech modes. This work represents the first attempt to perform TSE for whispered speech. For this, we construct a new first of its kind database, called wTIMIT2mix, which comprises two-speaker speech mixtures and target speaker reference signals given in both normal and whispered speech modes. Our results on TSE show that if these conditions are included in the training, a model can be equipped to work under all closed-set conditions."
   ],
   "p1": 5038,
   "pn": 5042,
   "doi": "10.21437/Interspeech.2024-1172",
   "url": "interspeech_2024/borsdorf24_interspeech.html"
  },
  "bahrman24_interspeech": {
   "authors": [
    [
     "Louis",
     "Bahrman"
    ],
    [
     "Mathieu",
     "Fontaine"
    ],
    [
     "Jonathan",
     "Le Roux"
    ],
    [
     "Gaël",
     "Richard"
    ]
   ],
   "title": "Speech dereverberation constrained on room impulse response characteristics",
   "original": "1173",
   "order": 126,
   "page_count": 5,
   "abstract": [
    "Single-channel speech dereverberation aims at extracting a dry speech signal from a recording affected by the acoustic reflections in a room. However, most current deep learning-based approaches for speech dereverberation are not interpretable for room acoustics, and can be considered as black-box systems in that regard. In this work, we address this problem by regularizing the training loss using a novel physical coherence loss which encourages the room impulse response (RIR) induced by the dereverberated output of the model to match the acoustic properties of the room in which the signal was recorded. Our investigation demonstrates the preservation of the original dereverberated signal alongside the provision of a more physically coherent RIR."
   ],
   "p1": 622,
   "pn": 626,
   "doi": "10.21437/Interspeech.2024-1173",
   "url": "interspeech_2024/bahrman24_interspeech.html"
  },
  "li24x_interspeech": {
   "authors": [
    [
     "Xiang",
     "Li"
    ],
    [
     "Vivek",
     "Govindan"
    ],
    [
     "Rohit",
     "Paturi"
    ],
    [
     "Sundararajan",
     "Srinivasan"
    ]
   ],
   "title": "Speakers Unembedded: Embedding-free Approach to Long-form Neural Diarization",
   "original": "1174",
   "order": 7,
   "page_count": 5,
   "abstract": [
    "End-to-end neural diarization (EEND) models offer significant improvements over traditional embedding-based Speaker Diarization (SD) approaches but falls short on generalizing to long-form audio with large number of speakers. EEND-vector-clustering method mitigates this by combining local EEND with global clustering of speaker embeddings from local windows, but this requires an additional speaker embedding framework alongside the EEND module. In this paper, we propose a novel framework applying EEND both locally and globally for long-form audio without separate speaker embeddings. This approach achieves significant relative DER reduction of 13% and 10% over the conventional 1-pass EEND on Callhome American English and RT03-CTS datasets respectively and marginal improvements over EEND-vector-clustering without the need for additional speaker embeddings. Furthermore, we discuss the computational complexity of our proposed framework and explore strategies for reducing processing times."
   ],
   "p1": 27,
   "pn": 31,
   "doi": "10.21437/Interspeech.2024-1174",
   "url": "interspeech_2024/li24x_interspeech.html"
  },
  "samptur24_interspeech": {
   "authors": [
    [
     "Neelesh",
     "Samptur"
    ],
    [
     "Tanuka",
     "Bhattacharjee"
    ],
    [
     "Anirudh",
     "Chakravarty K"
    ],
    [
     "Seena",
     "Vengalil"
    ],
    [
     "Yamini",
     "Belur"
    ],
    [
     "Atchayaram",
     "Nalini"
    ],
    [
     "Prasanta Kumar",
     "Ghosh"
    ]
   ],
   "title": "Exploring Syllable Discriminability during Diadochokinetic Task with Increasing Dysarthria Severity for Patients with Amyotrophic Lateral Sclerosis",
   "original": "1175",
   "order": 846,
   "page_count": 5,
   "abstract": [
    "We explore the discriminability among /pa/, /ta/, and /ka/ syllables, spoken during diadochokinetic (DDK) task, at varied severity levels of amyotrophic lateral sclerosis (ALS) induced dysarthria. Though DDK rate is known to decline with increasing severity, the extent to which the discriminability among the syllables gets impacted at each severity level is not well understood. We perform manual and automatic classification of these three syllables on 100 ALS and 35 healthy subjects. Manual classification is done through listening tests. Spectral and self-supervised speech cues with deep neural classifiers are used for automatic classification. Manual classification accuracies decline from 84.07% on healthy utterances to 27.41% on utterances of the most severe patients. Automatic methods are found to outperform humans achieving 15.93% and 50.37% higher accuracies (absolute), respectively. Thus, discriminative acoustic cues seem to persist among the syllables, which automatic methods capture."
   ],
   "p1": 4114,
   "pn": 4118,
   "doi": "10.21437/Interspeech.2024-1175",
   "url": "interspeech_2024/samptur24_interspeech.html"
  },
  "elie24b_interspeech": {
   "authors": [
    [
     "Benjamin",
     "Elie"
    ],
    [
     "David",
     "Doukhan"
    ],
    [
     "Rémi",
     "Uro"
    ],
    [
     "Lucas",
     "Ondel-Yang"
    ],
    [
     "Albert",
     "Rilliard"
    ],
    [
     "Simon",
     "Devauchelle"
    ]
   ],
   "title": "Articulatory Configurations across Genders and Periods in French Radio and TV archives",
   "original": "1177",
   "order": 633,
   "page_count": 5,
   "abstract": [
    "This paper studies changes in articulatory configurations across genders and periods using an inversion from acoustic to articulatory parameters. From a diachronic corpus based on French media archives spanning 60 years from 1955 to 2015, automatic transcription and forced alignment allowed extracting the central frame of each vowel. More than one million frames were obtained from over a thousand speakers across gender and age categories. Their formants were used from these vocalic frames to fit the parameters of Maeda's articulatory model. Evaluations of the quality of these processes are provided. We focus here on two parameters of Maeda’s model linked to total vocal tract length: the relative position of the larynx (higher for females) and the lips protrusion (more protruded for males). Implications for voice quality across genders are discussed. The effect across periods seems gender independent; thus, the assertion that females lowered their pitch with time is not supported."
   ],
   "p1": 3085,
   "pn": 3089,
   "doi": "10.21437/Interspeech.2024-1177",
   "url": "interspeech_2024/elie24b_interspeech.html"
  },
  "gao24d_interspeech": {
   "authors": [
    [
     "Lingyun",
     "Gao"
    ],
    [
     "Cristian",
     "Tejedor-Garcia"
    ],
    [
     "Helmer",
     "Strik"
    ],
    [
     "Catia",
     "Cucchiarini"
    ]
   ],
   "title": "Reading Miscue Detection in Primary School through Automatic Speech Recognition",
   "original": "1180",
   "order": 1055,
   "page_count": 5,
   "abstract": [
    "Automatic reading diagnosis systems can benefit both teachers for more efficient scoring of reading exercises and students for accessing reading exercises with feedback more easily. However, there are limited studies on Automatic Speech Recognition (ASR) for child speech in languages other than English, and limited research on ASR-based reading diagnosis systems. This study investigates how efficiently state-of-the-art (SOTA) pretrained ASR models recognize Dutch native children speech and manage to detect reading miscues. We found that Hubert Large finetuned on Dutch speech achieves SOTA phoneme-level child speech recognition (PER at 23.1%), while Whisper (Faster Whisper Large-v2) achieves SOTA word-level performance (WER at 9.8%). Our findings suggest that Wav2Vec2 Large and Whisper are the two best ASR models for reading miscue detection. Specifically, Wav2Vec2 Large shows the highest recall at 0.83, whereas Whisper exhibits the highest precision at 0.52 and an F1 score of 0.52."
   ],
   "p1": 5153,
   "pn": 5157,
   "doi": "10.21437/Interspeech.2024-1180",
   "url": "interspeech_2024/gao24d_interspeech.html"
  },
  "watkins24_interspeech": {
   "authors": [
    [
     "Michaela",
     "Watkins"
    ],
    [
     "Paul",
     "Boersma"
    ],
    [
     "Silke",
     "Hamann"
    ]
   ],
   "title": "Revisiting Pitch Jumps: F0 Ratio in Seoul Korean",
   "original": "1184",
   "order": 643,
   "page_count": 5,
   "abstract": [
    "Pitch tracking algorithms can show upward or downward jumps in F0 by one octave. These “octave jumps” are sometimes thought of as pitch-tracking “errors”, in the sense that they constitute a “mistake” in the algorithm. Using Praat software, we discuss the point (which has been made before) that measured octave jumps often actually reflect genuine changes in periodicity and glottal-fold vibration. We illustrate this with the example of creaky voice in fortis stops in Seoul Korean. We argue (1) that when the goal is to capture periodicity or vocal-fold vibration, pitch-tracking algorithms capture F0 well, with pitch jumps possibly reflecting an important language-specific feature, and (2) that ignoring such jumps (due to assuming an error) could lead to misrepresentation of the properties of the language. To quantify these real F0 jumps, we introduce the notion of the “F0 ratio”, which identifies potential F0 jumps and helps to chart the frequency of pitch jumps in a language."
   ],
   "p1": 3135,
   "pn": 3139,
   "doi": "10.21437/Interspeech.2024-1184",
   "url": "interspeech_2024/watkins24_interspeech.html"
  },
  "chen24o_interspeech": {
   "authors": [
    [
     "Xuanjun",
     "Chen"
    ],
    [
     "Haibin",
     "Wu"
    ],
    [
     "Roger",
     "Jang"
    ],
    [
     "Hung-yi",
     "Lee"
    ]
   ],
   "title": "Singing Voice Graph Modeling for SingFake Detection",
   "original": "1185",
   "order": 993,
   "page_count": 5,
   "abstract": [
    "Detecting singing voice deepfakes, or SingFake, involves determining the authenticity and copyright of a singing voice. Existing models for speech deepfake detection have struggled to adapt to unseen attacks in this unique singing voice domain of human vocalization. To bridge the gap, we present a groundbreaking SingGraph model. The model synergizes the capabilities of the MERT acoustic music understanding model for pitch and rhythm analysis with the wav2vec2.0 model for linguistic analysis of lyrics. Additionally, we advocate for using RawBoost and beat matching techniques grounded in music domain knowledge for singing voice augmentation, thereby enhancing SingFake detection performance. Our proposed method achieves new state-of-the-art (SOTA) results within the SingFake dataset, surpassing the previous SOTA model across three distinct scenarios: it improves EER relatively for seen singers by 13.2%, for unseen singers by 24.3%, and unseen singers using different codecs by 37.1%."
   ],
   "p1": 4843,
   "pn": 4847,
   "doi": "10.21437/Interspeech.2024-1185",
   "url": "interspeech_2024/chen24o_interspeech.html"
  },
  "yu24b_interspeech": {
   "authors": [
    [
     "Chin-Yun",
     "Yu"
    ],
    [
     "György",
     "Fazekas"
    ]
   ],
   "title": "Differentiable Time-Varying Linear Prediction in the Context of End-to-End Analysis-by-Synthesis",
   "original": "1187",
   "resource": "https://doi.org/10.5281/zenodo.12786789",
   "order": 373,
   "page_count": 5,
   "abstract": [
    "Training the linear prediction (LP) operator end-to-end for audio synthesis in modern deep learning frameworks is slow due to its recursive formulation. In addition, frame-wise approximation as an acceleration method cannot generalise well to test time conditions where the LP is computed sample-wise. Efficient differentiable sample-wise LP for end-to-end training is the key to removing this barrier. We generalise the efficient time-invariant LP implementation from the GOLF vocoder to time-varying cases. Combining this with the classic source-filter model, we show that the improved GOLF learns LP coefficients and reconstructs the voice better than its frame-wise counterparts. Moreover, in our listening test, synthesised outputs from GOLF scored higher in quality ratings than the state-of-the-art differentiable WORLD vocoder."
   ],
   "p1": 1820,
   "pn": 1824,
   "doi": "10.21437/Interspeech.2024-1187",
   "url": "interspeech_2024/yu24b_interspeech.html"
  },
  "chen24p_interspeech": {
   "authors": [
    [
     "Xuanjun",
     "Chen"
    ],
    [
     "Jiawei",
     "Du"
    ],
    [
     "Haibin",
     "Wu"
    ],
    [
     "Jyh-Shing Roger",
     "Jang"
    ],
    [
     "Hung-yi",
     "Lee"
    ]
   ],
   "title": "Neural Codec-based Adversarial Sample Detection for Speaker Verification",
   "original": "1191",
   "order": 106,
   "page_count": 5,
   "abstract": [
    "Automatic Speaker Verification (ASV), increasingly used in security-critical applications, faces vulnerabilities from rising adversarial attacks, with few effective defenses available. In this paper, we propose a neural codec-based adversarial sample detection method for ASV. The approach leverages the codec's ability to discard redundant perturbations and retain essential information. Specifically, we distinguish between genuine and adversarial samples by comparing ASV score differences between original and re-synthesized audio (by codec models). This comprehensive study explores all open-source neural codecs and their variant models for experiments. The Descript-audio-codec model stands out by delivering the highest detection rate among 15 neural codecs and surpassing seven prior state-of-the-art (SOTA) detection methods. Note that, our single-model method even outperforms a SOTA ensemble method by a large margin."
   ],
   "p1": 522,
   "pn": 526,
   "doi": "10.21437/Interspeech.2024-1191",
   "url": "interspeech_2024/chen24p_interspeech.html"
  },
  "dang24_interspeech": {
   "authors": [
    [
     "Trung",
     "Dang"
    ],
    [
     "David",
     "Aponte"
    ],
    [
     "Dung",
     "Tran"
    ],
    [
     "Kazuhito",
     "Koishida"
    ]
   ],
   "title": "LiveSpeech: Low-Latency Zero-shot Text-to-Speech via Autoregressive Modeling of Audio Discrete Codes",
   "original": "1192",
   "order": 695,
   "page_count": 5,
   "abstract": [
    "Prior works have demonstrated zero-shot text-to-speech by using a generative language model on audio tokens obtained via a neural audio codec. It is still challenging, however, to adapt them to low-latency scenarios. In this paper, we present LiveSpeech - a fully autoregressive language model-based approach for zero-shot text-to-speech, enabling low-latency streaming of the output audio. To allow multiple token prediction within a single decoding step, we propose (1) using adaptive codebook loss weights that consider codebook contribution in each frame and focus on hard instances, and (2) grouping codebooks and processing groups in parallel. Experiments show our proposed models achieve competitive results to state-of-the-art baselines in terms of content accuracy, speaker similarity, audio quality, and inference speed while being suitable for low-latency streaming applications."
   ],
   "p1": 3395,
   "pn": 3399,
   "doi": "10.21437/Interspeech.2024-1192",
   "url": "interspeech_2024/dang24_interspeech.html"
  },
  "adigwe24_interspeech": {
   "authors": [
    [
     "Adaeze",
     "Adigwe"
    ],
    [
     "Sarenne",
     "Wallbridge"
    ],
    [
     "Simon",
     "King"
    ]
   ],
   "title": "What do people hear? Listeners’ Perception of Conversational Speech",
   "original": "1193",
   "order": 251,
   "page_count": 5,
   "abstract": [
    "Conversational agents are becoming increasingly popular, prompting the need for text-to-speech (TTS) systems that sound conversational. Previous research has focused on training TTS models on elicited or found conversational speech then measuring an improved listener preference. Preference ratings cannot pinpoint why TTS voices fall short of conversational expectations, underscoring our limited understanding of conversational speaking styles. In this pilot study, we conduct interviews with naive listeners who evaluate if speech was taken from a conversation or not, then give their explanation. Our results indicate that listeners are capable of distinguishing conversational utterances from read speech from acoustic features alone. While listeners’ explanations vary, they generally allude to pronunciation, rhythmic organisation, and inappropriate prosody. Using targeted prosodic modifications to synthesise speech, we shed light on the complexity of evaluating conversational style."
   ],
   "p1": 1210,
   "pn": 1214,
   "doi": "10.21437/Interspeech.2024-1193",
   "url": "interspeech_2024/adigwe24_interspeech.html"
  },
  "peng24b_interspeech": {
   "authors": [
    [
     "Yifan",
     "Peng"
    ],
    [
     "Jinchuan",
     "Tian"
    ],
    [
     "William",
     "Chen"
    ],
    [
     "Siddhant",
     "Arora"
    ],
    [
     "Brian",
     "Yan"
    ],
    [
     "Yui",
     "Sudo"
    ],
    [
     "Muhammad",
     "Shakeel"
    ],
    [
     "Kwanghee",
     "Choi"
    ],
    [
     "Jiatong",
     "Shi"
    ],
    [
     "Xuankai",
     "Chang"
    ],
    [
     "Jee-weon",
     "Jung"
    ],
    [
     "Shinji",
     "Watanabe"
    ]
   ],
   "title": "OWSM v3.1: Better and Faster Open Whisper-Style Speech Models based on E-Branchformer",
   "original": "1194",
   "order": 72,
   "page_count": 5,
   "abstract": [
    "Recent studies have highlighted the importance of fully open foundation models. The Open Whisper-style Speech Model (OWSM) is an initial step towards reproducing OpenAI Whisper using public data and open-source toolkits. However, previous versions of OWSM (v1 to v3) are still based on standard Transformer, which might lead to inferior performance compared to state-of-the-art speech encoder architectures. This work aims to improve the performance and efficiency of OWSM without additional data. We present a series of E-Branchformer-based models named OWSM v3.1, ranging from 100M to 1B parameters. OWSM v3.1 outperforms its predecessor, OWSM v3, in most evaluation benchmarks, while showing an improved inference speed of up to 25%. We further reveal the emergent ability of OWSM v3.1 in zero-shot contextual biasing speech recognition. We also provide a model trained on a subset of data with low license restrictions. We will publicly release the code, pre-trained models, and training logs.1"
   ],
   "p1": 352,
   "pn": 356,
   "doi": "10.21437/Interspeech.2024-1194",
   "url": "interspeech_2024/peng24b_interspeech.html"
  },
  "shinoda24_interspeech": {
   "authors": [
    [
     "Kazutoshi",
     "Shinoda"
    ],
    [
     "Nobukatsu",
     "Hojo"
    ],
    [
     "Saki",
     "Mizuno"
    ],
    [
     "Keita",
     "Suzuki"
    ],
    [
     "Satoshi",
     "Kobashikawa"
    ],
    [
     "Ryo",
     "Masumura"
    ]
   ],
   "title": "Learning from Multiple Annotator Biased Labels in Multimodal Conversation",
   "original": "1197",
   "order": 841,
   "page_count": 5,
   "abstract": [
    "In multimodal conversation analysis, annotating social signals such as speakers' communication skills is inherently subjective and prone to individual annotator bias, which is annotator's tendency to assign labels based on their values. These biases can contribute to label distributions biased towards specific speakers and classes that match annotators' values, leading to degraded classification performance for minority classes and speakers. Existing methods for addressing class imbalance and dataset bias often overlook the variable biases introduced by multiple annotators, which can lead to overfitting to the majority. Thus, we propose a novel two-stage debiasing method, MAD-LM, that first learns the typical label distribution for each annotator and then promotes the learning of untypical labels. MAD-LM effectively mitigates performance degradation for the minority in a multimodal conversation dataset with multiple annotator labels, while maintaining the performance for the majority."
   ],
   "p1": 4089,
   "pn": 4093,
   "doi": "10.21437/Interspeech.2024-1197",
   "url": "interspeech_2024/shinoda24_interspeech.html"
  },
  "ghaffarzadegan24_interspeech": {
   "authors": [
    [
     "Shabnam",
     "Ghaffarzadegan"
    ],
    [
     "Luca",
     "Bondi"
    ],
    [
     "Wei-Chang",
     "Lin"
    ],
    [
     "Abinaya",
     "Kumar"
    ],
    [
     "Ho-Hsiang",
     "Wu"
    ],
    [
     "Hans-Georg",
     "Horst"
    ],
    [
     "Samarjit",
     "Das"
    ]
   ],
   "title": "Sound of Traffic: A Dataset for Acoustic Traffic Identification and Counting",
   "original": "1205",
   "resource": "https://doi.org/10.5281/zenodo.10700792",
   "order": 25,
   "page_count": 5,
   "abstract": [
    "We introduce Sound of Traffic, the largest publicly available dataset for traffic identification and counting to date. With over 415 hours of multichannel acoustic traffic data recorded in six different locations, it encompasses varying levels of traffic density and environmental conditions. In this work, we discuss strategies for automatic collection and alignment of large amount of labeled data, leveraging existing asynchronous urban sensors such as radar, cameras, and inductive coils. In addition to the dataset, we propose a simple baseline system for vehicle counting divided by type of the vehicle (passenger vs. commercial vehicle) and direction of travel (right-to-left and left-to-right), a fundamental task for traffic analysis. The dataset and baseline system serve as a starting point for researchers to develop more advanced algorithms and models in this field. The dataset can be accessed at https://zenodo.org/records/10700792 and https://zenodo.org/records/11209838."
   ],
   "p1": 117,
   "pn": 121,
   "doi": "10.21437/Interspeech.2024-1205",
   "url": "interspeech_2024/ghaffarzadegan24_interspeech.html",
   "erratum": "<p>The name of the third author is <b>Wei-Cheng Lin</b> instead of Wei-Chang Lin.\n</p>"
  },
  "weise24_interspeech": {
   "authors": [
    [
     "Tobias",
     "Weise"
    ],
    [
     "Philipp",
     "Klumpp"
    ],
    [
     "Kubilay Can",
     "Demir"
    ],
    [
     "Paula Andrea",
     "Pérez-Toro"
    ],
    [
     "Maria",
     "Schuster"
    ],
    [
     "Elmar",
     "Noeth"
    ],
    [
     "Bjoern",
     "Heismann"
    ],
    [
     "Andreas",
     "Maier"
    ],
    [
     "Seung Hee",
     "Yang"
    ]
   ],
   "title": "Speaker- and Text-Independent Estimation of Articulatory Movements and Phoneme Alignments from Speech",
   "original": "1208",
   "order": 318,
   "page_count": 5,
   "abstract": [
    "This paper introduces a novel combination of two tasks, previously treated separately: acoustic-to-articulatory speech inversion (AAI) and phoneme-to-articulatory (PTA) motion estimation. We refer to this joint task as acoustic phoneme-to-articulatory speech inversion (APTAI) and explore two different approaches, both working speaker- and text-independently during inference. We use a multi-task learning setup, with the end-to-end goal of taking raw speech as input and estimating the corresponding articulatory movements, phoneme sequence, and phoneme alignment. While both proposed approaches share these same requirements, they differ in their way of achieving phoneme-related predictions: one is based on frame classification, the other on a two-staged training procedure and forced alignment. We reach competitive performance of 0.73 mean correlation for the AAI task and achieve up to approximately 87% frame overlap compared to a state-of-the-art text-dependent phoneme force aligner."
   ],
   "p1": 1545,
   "pn": 1549,
   "doi": "10.21437/Interspeech.2024-1208",
   "url": "interspeech_2024/weise24_interspeech.html"
  },
  "aldeneh24_interspeech": {
   "authors": [
    [
     "Zakaria",
     "Aldeneh"
    ],
    [
     "Takuya",
     "Higuchi"
    ],
    [
     "Jee-weon",
     "Jung"
    ],
    [
     "Skyler",
     "Seto"
    ],
    [
     "Tatiana",
     "Likhomanenko"
    ],
    [
     "Stephen",
     "Shum"
    ],
    [
     "Ahmed",
     "Hussen Abdelaziz"
    ],
    [
     "Shinji",
     "Watanabe"
    ],
    [
     "Barry-John",
     "Theobald"
    ]
   ],
   "title": "Can you Remove the Downstream Model for Speaker Recognition with Self-Supervised Speech Features?",
   "original": "1212",
   "order": 954,
   "page_count": 5,
   "abstract": [
    "Self-supervised features are typically used in place of filter-bank features in speaker verification models. However, these models were originally designed to ingest filter-banks as inputs, and thus, training them on self-supervised features assumes that both feature types require the same amount of learning for the task. In this work, we observe that pre-trained self-supervised speech features inherently include information required for a downstream speaker verification task, and therefore, we can simplify the downstream model without sacrificing performance. To this end, we revisit the design of the downstream model for speaker verification using self-supervised features. We show that we can simplify the model to use 97.51% fewer parameters while achieving a 29.93% average improvement in performance on SUPERB. Consequently, we show that the simplified downstream model is more data efficient compared to the baseline---it achieves better performance with only 60% of the training data."
   ],
   "p1": 4648,
   "pn": 4652,
   "doi": "10.21437/Interspeech.2024-1212",
   "url": "interspeech_2024/aldeneh24_interspeech.html"
  },
  "erickson24_interspeech": {
   "authors": [
    [
     "Donna",
     "Erickson"
    ],
    [
     "Albert",
     "Rilliard"
    ],
    [
     "Malin",
     "Svensson Lundmark"
    ],
    [
     "Adelaide",
     "Silva"
    ],
    [
     "Leticia",
     "Rebollo Couto"
    ],
    [
     "Oliver",
     "Niebuhr"
    ],
    [
     "João Antonio de",
     "Moraes"
    ]
   ],
   "title": "Collecting Mandible Movement in Brazilian Portuguese",
   "original": "1216",
   "order": 645,
   "page_count": 5,
   "abstract": [
    "This paper reports on a corpus of Brazilian Portuguese (BP) mandible movements. The data was collected using a recently available technique, the MARRYS helmet, which allows for quick and reliable collection of mandible data of a large number of speakers. Audio and mandible were recorded from more than 90 L1 and L2 BP speakers. The recording process and signal synchronization are presented. A partial set of the corpus, based on 37 L1 speakers producing three sentences mostly composed of /a/ vowels, was segmented at the phone level. The jaw movements are compared to the sentence's prosodic structure. Results indicate that, similar to other languages, Brazilian Portuguese speakers show increased mandible lowering for stressed syllables but also quick mandible closing. Some interesting mandible opening and closing patterns are also reported on the prestress or post-stress positions in relation to the prosodic structure of Brazilian Portuguese."
   ],
   "p1": 3145,
   "pn": 3149,
   "doi": "10.21437/Interspeech.2024-1216",
   "url": "interspeech_2024/erickson24_interspeech.html"
  },
  "pahuja24_interspeech": {
   "authors": [
    [
     "Saurav",
     "Pahuja"
    ],
    [
     "Gabriel",
     "Ivucic"
    ],
    [
     "Pascal",
     "Himmelmann"
    ],
    [
     "Siqi",
     "Cai"
    ],
    [
     "Tanja",
     "Schultz"
    ],
    [
     "Haizhou",
     "Li"
    ]
   ],
   "title": "Leveraging Graphic and Convolutional Neural Networks for Auditory Attention Detection with EEG",
   "original": "1217",
   "order": 90,
   "page_count": 5,
   "abstract": [
    "Recent work has shown that the locus of selective auditory attention in multi-speaker settings can be decoded from single-trial electroencephalography (EEG). This study represents the first effort to investigate the decoding of selective auditory attention through the utilization of an ensemble model. Specifically, we combine predictions solely based on brain data using two stacked deep learning-based models, namely the SpatioTemporal Attention Network (STAnet) and SpatioTemporal Graph Convolutional Network (ST-GCN), through an average-soft voting layer. This ensemble approach demonstrates improved generalizability within short 1-second decision windows, incorporating subtle distinctions in spatial features extracted by the networks from the EEG. This results in an effective trial-independent prediction of spatial auditory attention, outperforming baseline models by a substantial margin of 10% across two publicly available auditory attention datasets1."
   ],
   "p1": 442,
   "pn": 446,
   "doi": "10.21437/Interspeech.2024-1217",
   "url": "interspeech_2024/pahuja24_interspeech.html"
  },
  "leem24_interspeech": {
   "authors": [
    [
     "Seong-Gyun",
     "Leem"
    ],
    [
     "Daniel",
     "Fulford"
    ],
    [
     "Jukka-Pekka",
     "Onnela"
    ],
    [
     "David",
     "Gard"
    ],
    [
     "Carlos",
     "Busso"
    ]
   ],
   "title": "Keep, Delete, or Substitute: Frame Selection Strategy for Noise-Robust Speech Emotion Recognition",
   "original": "1218",
   "order": 770,
   "page_count": 5,
   "abstract": [
    "Speech emotion recognition (SER) system can exploit an Speech enhancement (SE) model to increase its noise robustness by suppressing the background noise. However, SE could also suppress emotionally discriminative features, affecting the emotion prediction. We propose an alternative framework, Keep or Delete (KoD), to keep the information of the original speech while minimizing the influence of background noise. We train a frame reliability predictor that determines clean frames to keep, discarding the noisy frames. We expand this framework by replacing the dropped frames with those extracted from the enhanced speech to keep the lexical information. We refer to this implementation as Keep or Substitute (KoS). Our experiment shows that the KoD model improves the SER results under noisy conditions without fine-tuning the whole model. Also, the KoS framework performs better than enhancing all the frames, indicating the importance of avoiding speech distortion."
   ],
   "p1": 3734,
   "pn": 3738,
   "doi": "10.21437/Interspeech.2024-1218",
   "url": "interspeech_2024/leem24_interspeech.html"
  },
  "koudounas24b_interspeech": {
   "authors": [
    [
     "Alkis",
     "Koudounas"
    ],
    [
     "Flavio",
     "Giobergia"
    ],
    [
     "Eliana",
     "Pastor"
    ],
    [
     "Elena",
     "Baralis"
    ]
   ],
   "title": "A Contrastive Learning Approach to Mitigate Bias in Speech Models",
   "original": "1219",
   "order": 167,
   "page_count": 5,
   "abstract": [
    "Speech models may be affected by performance imbalance in different population subgroups, raising concerns about fair treatment across these groups. Prior attempts to mitigate unfairness either focus on user-defined subgroups, potentially overlooking other affected subgroups, or do not explicitly improve the internal representation at the subgroup level. This paper proposes the first adoption of contrastive learning to mitigate speech model bias in underperforming subgroups. We employ a three-level learning technique that guides the model in focusing on different scopes for the contrastive loss, i.e., task, subgroup, and the errors within subgroups. The experiments on two spoken language understanding datasets and two languages demonstrate that our approach improves internal subgroup representations, thus reducing model bias and enhancing performance."
   ],
   "p1": 827,
   "pn": 831,
   "doi": "10.21437/Interspeech.2024-1219",
   "url": "interspeech_2024/koudounas24b_interspeech.html"
  },
  "bhattacharya24_interspeech": {
   "authors": [
    [
     "Debasmita",
     "Bhattacharya"
    ],
    [
     "Eleanor",
     "Lin"
    ],
    [
     "Run",
     "Chen"
    ],
    [
     "Julia",
     "Hirschberg"
    ]
   ],
   "title": "Switching Tongues, Sharing Hearts: Identifying the Relationship between Empathy and Code-switching in Speech",
   "original": "1224",
   "order": 100,
   "page_count": 5,
   "abstract": [
    "Among the many multilingual speakers of the world, code-switching (CSW) is a common linguistic phenomenon. Prior sociolinguistic work has shown that factors such as expressing group identity and solidarity, performing affective function, and reflecting shared experiences are related to CSW prevalence in multilingual speech. We build on prior studies by asking: is the expression of empathy a motivation for CSW in speech? To begin to answer this question, we examine several multilingual speech corpora representing diverse language families and apply recent modeling advances in the study of empathetic monolingual speech. We find a generally stronger positive relationship of spoken CSW with the lexical correlates of empathy than with acoustic-prosodic ones, which holds across three language pairs. Our work is a first step toward establishing a motivation for CSW that has thus far mainly been studied qualitatively."
   ],
   "p1": 492,
   "pn": 496,
   "doi": "10.21437/Interspeech.2024-1224",
   "url": "interspeech_2024/bhattacharya24_interspeech.html"
  },
  "goncalves24_interspeech": {
   "authors": [
    [
     "Lucas",
     "Goncalves"
    ],
    [
     "Donita",
     "Robinson"
    ],
    [
     "Elizabeth",
     "Richerson"
    ],
    [
     "Carlos",
     "Busso"
    ]
   ],
   "title": "Bridging Emotions Across Languages: Low Rank Adaptation for Multilingual Speech Emotion Recognition",
   "original": "1226",
   "order": 962,
   "page_count": 5,
   "abstract": [
    "The field of speech emotion recognition (SER) is constantly evolving with the surge in voice data and linguistic diversity. This growth highlights the need for SER systems capable of overcoming language barriers in both linguistic structure and cultural expression of emotions. We envision a SER framework that captures general trends in the expression of emotions, while also modeling language-specific information. Our study investigates low rank adaptation (LoRA) for creating multilingual SER models, applying LoRA in a multilingual context to efficiently adapt pre-trained models to new languages with minimal changes. This enhances cross-lingual adaptability and efficiency of SER systems, refining models to recognize emotions across languages without extensive retraining. In this study, we focus on exploring this method to bridge the gap between English and Taiwanese Mandarin in naturalistic settings, demonstrating strong performance in both languages."
   ],
   "p1": 4688,
   "pn": 4692,
   "doi": "10.21437/Interspeech.2024-1226",
   "url": "interspeech_2024/goncalves24_interspeech.html"
  },
  "naini24_interspeech": {
   "authors": [
    [
     "Abinay Reddy",
     "Naini"
    ],
    [
     "Lucas",
     "Goncalves"
    ],
    [
     "Mary A.",
     "Kohler"
    ],
    [
     "Donita",
     "Robinson"
    ],
    [
     "Elizabeth",
     "Richerson"
    ],
    [
     "Carlos",
     "Busso"
    ]
   ],
   "title": "WHiSER: White House Tapes Speech Emotion Recognition Corpus",
   "original": "1227",
   "order": 328,
   "page_count": 5,
   "abstract": [
    "There are several applications for speech-emotion recognition (SER) systems in areas such as security and defense and healthcare. SER systems have achieved high performance when they are trained and tested in similar conditions. However, the performance often drops in more realistic and diverse conditions. Most existing SER datasets are too controlled and do not capture complex scenarios relevant to practical applications. This paper presents the White House tapes speech emotion recognition (WHiSER) corpus, which includes distant speech with real emotions from conversations in the Oval Office in 1972. This dataset is unique because it combines natural emotional expressions with various background noises, making it a perfect tool to test and improve SER models. Its real-world complexity and authenticity make the WHiSER corpus an excellent corpus for advancing emotion recognition technology, offering insights into how human emotions can be accurately recognized in complex environments."
   ],
   "p1": 1595,
   "pn": 1599,
   "doi": "10.21437/Interspeech.2024-1227",
   "url": "interspeech_2024/naini24_interspeech.html"
  },
  "liu24h_interspeech": {
   "authors": [
    [
     "Joseph",
     "Liu"
    ],
    [
     "Mahesh Kumar",
     "Nandwana"
    ],
    [
     "Janne",
     "Pylkkönen"
    ],
    [
     "Hannes",
     "Heikinheimo"
    ],
    [
     "Morgan",
     "McGuire"
    ]
   ],
   "title": "Enhancing Multilingual Voice Toxicity Detection with Speech-Text Alignment",
   "original": "1228",
   "order": 884,
   "page_count": 5,
   "abstract": [
    "Toxicity classification for voice heavily relies on the semantic content of speech. We propose a novel framework that utilizes cross-modal learning to integrate the semantic embedding of text into a multilabel speech toxicity classifier during training. This enables us to incorporate textual information during training while still requiring only audio during inference. We evaluate this classifier on large-scale datasets with real-world characteristics to validate the effectiveness of this framework. Through ablation studies, we demonstrate that general-purpose semantic text embeddings are rich and aligned with speech for toxicity classification purposes. Conducting experiments across multiple languages at scale, we show improvements in voice toxicity classification across five languages and different toxicity categories."
   ],
   "p1": 4298,
   "pn": 4302,
   "doi": "10.21437/Interspeech.2024-1228",
   "url": "interspeech_2024/liu24h_interspeech.html"
  },
  "dumpala24_interspeech": {
   "authors": [
    [
     "Sri Harsha",
     "Dumpala"
    ],
    [
     "Dushyant",
     "Sharma"
    ],
    [
     "Chandramouli",
     "Shama Sastry"
    ],
    [
     "Stanislav",
     "Kruchinin"
    ],
    [
     "James",
     "Fosburgh"
    ],
    [
     "Patrick A.",
     "Naylor"
    ]
   ],
   "title": "XANE: eXplainable Acoustic Neural Embeddings",
   "original": "1229",
   "order": 788,
   "page_count": 5,
   "abstract": [
    "We present a novel method for extracting neural embeddings that model the background acoustics of a speech signal. The extracted embeddings are used to estimate specific parameters related to the background acoustic properties of the signal in a non-intrusive manner, which allows the embeddings to be explainable in terms of those parameters. We illustrate the value of  these embeddings by performing clustering experiments on unseen test data and show that the proposed embeddings achieve a mean F1 score of 95.2% for three different tasks, outperforming significantly the WavLM based signal embeddings. We also show that the proposed method can explain the embeddings by estimating 14 acoustic parameters characterizing the background acoustics, including reverberation and noise levels, overlapped speech detection, CODEC type detection and noise type detection with high accuracy and a real-time factor 17 times lower than an external baseline method."
   ],
   "p1": 3824,
   "pn": 3828,
   "doi": "10.21437/Interspeech.2024-1229",
   "url": "interspeech_2024/dumpala24_interspeech.html"
  },
  "leivaditi24_interspeech": {
   "authors": [
    [
     "Spyretta",
     "Leivaditi"
    ],
    [
     "Tatsunari",
     "Matsushima"
    ],
    [
     "Matt",
     "Coler"
    ],
    [
     "Shekhar",
     "Nayak"
    ],
    [
     "Vass",
     "Verkhodanova"
    ]
   ],
   "title": "Fine-Tuning Strategies for Dutch Dysarthric Speech Recognition: Evaluating the Impact of Healthy, Disease-Specific, and Speaker-Specific Data",
   "original": "1231",
   "order": 268,
   "page_count": 5,
   "abstract": [
    "Despite significant advancements in automatic speech recognition technology  (ASR) the performance of such systems on dysarthric speech is still inadequate for widespread use. One key reason is the lack of sufficiently rich and diverse dysarthric speech datasets to train machine learning models that could handle all types and varieties of such speech. Motivated by the data scarcity problem, as well as by successful applications of self-supervised learning (SSL) in ASR for low-resource languages, this paper investigates and evaluates the effectiveness of three different data-centric SSL training strategies in improving Dutch dysarthric speech recognition. The first strategy involves fine-tuning with both dysarthric and healthy speech data, the second with disease-specific data and the third with speaker-specific data. The first and third strategies are proven effective, while the second one, though ineffective, provides valuable insights for further research."
   ],
   "p1": 1295,
   "pn": 1299,
   "doi": "10.21437/Interspeech.2024-1231",
   "url": "interspeech_2024/leivaditi24_interspeech.html"
  },
  "parnamaa24_interspeech": {
   "authors": [
    [
     "Tanel",
     "Pärnamaa"
    ],
    [
     "Ando",
     "Saabas"
    ]
   ],
   "title": "Personalized Speech Enhancement Without a Separate Speaker Embedding Model",
   "original": "1234",
   "order": 997,
   "page_count": 5,
   "abstract": [
    "Personalized speech enhancement (PSE) models can improve the audio quality of teleconferencing systems by adapting to the characteristics of a speaker's voice. However, most existing methods require a separate speaker embedding model to extract a vector representation of the speaker from enrollment audio, which adds complexity to the training and deployment process. We propose to use the internal representation of the PSE model itself as the speaker embedding, thereby avoiding the need for a separate model. We show that our approach performs equally well or better than the standard method of using a pre-trained speaker embedding model on noise suppression and echo cancellation tasks. Moreover, our approach surpasses the ICASSP 2023 Deep Noise Suppression Challenge winner by 0.15 in Mean Opinion Score."
   ],
   "p1": 4863,
   "pn": 4867,
   "doi": "10.21437/Interspeech.2024-1234",
   "url": "interspeech_2024/parnamaa24_interspeech.html"
  },
  "gupta24d_interspeech": {
   "authors": [
    [
     "Deepanshu",
     "Gupta"
    ],
    [
     "Javier",
     "Latorre"
    ]
   ],
   "title": "Positional Description for Numerical Normalization ",
   "original": "1237",
   "order": 578,
   "page_count": 5,
   "abstract": [
    "We present a Positional Description Scheme (PDS) tailored for digit sequences, integrating placeholder value information for each digit. Given the structural limitations of subword tokenization algorithms, language models encounter critical Text Normalization (TN) challenges [1] when handling numerical tasks. Our schema addresses this challenge through straightforward pre-processing, preserving the model architecture while significantly simplifying number normalization, rendering the problem tractable. This simplifies the task and facilitates more compact production-ready models capable of learning from smaller datasets. Furthermore, our investigations reveal that PDS enhances the arithmetic processing capabilities of language models, resulting in a relative accuracy improvement of 23% to 51% on complex arithmetic tasks. We demonstrate that PDS effectively mitigates fatal numerical normalization errors in neural models, requiring only a modest amount of training data without rule-based Finite State Transducers (FST). We demonstrate that PDS is essential for both the Text-To-Speech and Speech Recognition text processing, enabling effective TN under production constraints."
   ],
   "p1": 2810,
   "pn": 2814,
   "doi": "10.21437/Interspeech.2024-1237",
   "url": "interspeech_2024/gupta24d_interspeech.html"
  },
  "zhang24h_interspeech": {
   "authors": [
    [
     "Wangyou",
     "Zhang"
    ],
    [
     "Robin",
     "Scheibler"
    ],
    [
     "Kohei",
     "Saijo"
    ],
    [
     "Samuele",
     "Cornell"
    ],
    [
     "Chenda",
     "Li"
    ],
    [
     "Zhaoheng",
     "Ni"
    ],
    [
     "Jan",
     "Pirklbauer"
    ],
    [
     "Marvin",
     "Sach"
    ],
    [
     "Shinji",
     "Watanabe"
    ],
    [
     "Tim",
     "Fingscheidt"
    ],
    [
     "Yanmin",
     "Qian"
    ]
   ],
   "title": "URGENT Challenge: Universality, Robustness, and Generalizability For Speech Enhancement",
   "original": "1239",
   "order": 998,
   "page_count": 5,
   "abstract": [
    "The last decade has witnessed significant advancements in deep learning-based speech enhancement (SE). However, most existing SE research has limitations on the coverage of SE sub-tasks, data diversity and amount, and evaluation metrics. To fill this gap and promote research toward universal SE, we establish a new SE challenge, named URGENT, to focus on the universality, robustness, and generalizability of SE. We aim to extend the SE definition to cover different sub-tasks to explore the limits of SE models, starting from denoising, dereverberation, bandwidth extension, and declipping. A novel framework is proposed to unify all these sub-tasks in a single model, allowing the use of all existing SE approaches. We collected public speech and noise data from different domains to construct diverse evaluation data. Finally, we discuss the insights gained from our preliminary baseline experiments based on both generative and discriminative SE methods with 12 curated metrics."
   ],
   "p1": 4868,
   "pn": 4872,
   "doi": "10.21437/Interspeech.2024-1239",
   "url": "interspeech_2024/zhang24h_interspeech.html"
  },
  "saijo24_interspeech": {
   "authors": [
    [
     "Kohei",
     "Saijo"
    ],
    [
     "Gordon",
     "Wichern"
    ],
    [
     "François G.",
     "Germain"
    ],
    [
     "Zexu",
     "Pan"
    ],
    [
     "Jonathan",
     "Le Roux"
    ]
   ],
   "title": "Enhanced Reverberation as Supervision for Unsupervised Speech Separation",
   "original": "1241",
   "order": 123,
   "page_count": 5,
   "abstract": [
    "Reverberation as supervision (RAS) is a framework that allows for training monaural speech separation models from multi-channel mixtures in an unsupervised manner.  In RAS, models are trained so that sources predicted from a mixture at an input channel can be mapped to reconstruct a mixture at a target channel.  However, stable unsupervised training has so far only been achieved in over-determined source-channel conditions, leaving the key determined case unsolved. This work proposes enhanced RAS (ERAS) for solving this problem. Through qualitative analysis, we found that stable training can be achieved by leveraging the loss term to alleviate the frequency-permutation problem. Separation performance is also boosted by adding a novel loss term where separated signals mapped back to their own input mixture are used as pseudo-targets for the signals separated from other channels and mapped to the same channel. Experimental results demonstrate high stability and performance of ERAS."
   ],
   "p1": 607,
   "pn": 611,
   "doi": "10.21437/Interspeech.2024-1241",
   "url": "interspeech_2024/saijo24_interspeech.html"
  },
  "hu24d_interspeech": {
   "authors": [
    [
     "ChengHung",
     "Hu"
    ],
    [
     "Yusuke",
     "Yasuda"
    ],
    [
     "Tomoki",
     "Toda"
    ]
   ],
   "title": "Embedding Learning for Preference-based Speech Quality Assessment",
   "original": "1243",
   "order": 553,
   "page_count": 5,
   "abstract": [
    "One goal of Speech Quality Assessment is to compare the quality of different utterances. Recently, several models based on preferences have been developed. These models typically use comparisons of MOS as preference scores during training. However, they often treat pairs of utterances with large differences in MOS and those with similar MOS equally, which increase the cost of accurate MOS prediction. To tackle this issue, this study suggests using embedding loss to bring pairs of utterance embeddings with similar MOS closer while separating those with dissimilar MOS. Our experiments showed that models trained with embedding loss perform better in both in-domain and out-domain scenarios. Furthermore, we use t-SNE visualization to analyze the distribution of embeddings extracted by models trained with and without embedding loss. Results indicate that embeddings of utterances with similar MOS scores are brought closer, whereas those with differing MOS scores are effectively separated."
   ],
   "p1": 2685,
   "pn": 2689,
   "doi": "10.21437/Interspeech.2024-1243",
   "url": "interspeech_2024/hu24d_interspeech.html"
  },
  "mote24_interspeech": {
   "authors": [
    [
     "Pravin",
     "Mote"
    ],
    [
     "Berrak",
     "Sisman"
    ],
    [
     "Carlos",
     "Busso"
    ]
   ],
   "title": "Unsupervised Domain Adaptation for Speech Emotion Recognition using K-Nearest Neighbors Voice Conversion",
   "original": "1248",
   "order": 218,
   "page_count": 5,
   "abstract": [
    "Abundant speech data for speech emotion recognition (SER) is often unlabeled, rendering it ineffective for model training. Models trained on existing labeled datasets struggle with unlabeled data due to mismatches in data distributions. To avoid the cost of annotating speech data, it is imperative to explore unsupervised adaptation techniques to leverage the potential of unlabeled data. Motivated by this observation, we propose a novel use of voice conversion (VC) for SER, which effectively enhances emotion recognition performance on an unlabeled dataset. Our approach involves leveraging the simplicity and efficacy of the k-nearest neighbor (kNN)-based VC technique to transform speech samples from the unlabeled domain to the labeled domain. In contrast to conventional domain adaptation methods, our approach avoids re-training of a model on transformed unlabeled data. We achieve good results by testing transformed unlabeled samples on a model trained with a different labeled dataset."
   ],
   "p1": 1045,
   "pn": 1049,
   "doi": "10.21437/Interspeech.2024-1248",
   "url": "interspeech_2024/mote24_interspeech.html"
  },
  "botelho24_interspeech": {
   "authors": [
    [
     "Catarina",
     "Botelho"
    ],
    [
     "John",
     "Mendonça"
    ],
    [
     "Anna",
     "Pompili"
    ],
    [
     "Tanja",
     "Schultz"
    ],
    [
     "Alberto",
     "Abad"
    ],
    [
     "Isabel",
     "Trancoso"
    ]
   ],
   "title": "Macro-descriptors for Alzheimer's disease detection using large language models",
   "original": "1255",
   "order": 404,
   "page_count": 5,
   "abstract": [
    "This work explores the potential of Large Language Models (LLMs) as annotators of high-level characteristics of speech transcriptions, which may be relevant for detecting Alzheimer's disease (AD). These low-dimension interpretable features, here designated as macro-descriptors (e.g. text coherence, lexical diversity), are then used to train a binary classifier. Our experiments compared the extraction of these features from both manual and automatic transcriptions obtained with different types of speech recognition systems, and involved both open and closed source LLMs, with several prompting strategies. The experiments also compared the use of macro-descriptors with the direct prediction of AD by the LLM, given the transcription. Even though LLMs are not trained for this task, our experiments show that they achieve up to 81% accuracy, surpassing the baseline of previous AD detection challenges, particularly when used as extractors of macro-descriptors."
   ],
   "p1": 1975,
   "pn": 1979,
   "doi": "10.21437/Interspeech.2024-1255",
   "url": "interspeech_2024/botelho24_interspeech.html"
  },
  "salman24_interspeech": {
   "authors": [
    [
     "Ali N.",
     "Salman"
    ],
    [
     "Zongyang",
     "Du"
    ],
    [
     "Shreeram Suresh",
     "Chandra"
    ],
    [
     "İsmail Rasim",
     "Ülgen"
    ],
    [
     "Carlos",
     "Busso"
    ],
    [
     "Berrak",
     "Sisman"
    ]
   ],
   "title": "Towards Naturalistic Voice Conversion: NaturalVoices Dataset with an Automatic Processing Pipeline",
   "original": "1256",
   "order": 896,
   "page_count": 5,
   "abstract": [
    "Voice conversion (VC) research traditionally depends on scripted or acted speech, which lacks the natural spontaneity of real-life conversations. While natural speech data is limited for VC, our study focuses on filling in this gap. We introduce a novel data-sourcing pipeline that makes the release of a natural speech dataset for VC, named NaturalVoices. The pipeline extracts rich information in speech such as emotion and signal-to-noise ratio (SNR) from raw podcast data, utilizing recent deep learning methods and providing flexibility and ease of use. NaturalVoices marks a large-scale, spontaneous, expressive, and emotional speech dataset, comprising over 3,800 hours speech sourced from the original podcasts in the MSP-Podcast dataset. Objective and subjective evaluations demonstrate the effectiveness of using our pipeline for providing natural and expressive data for VC, suggesting the potential of NaturalVoices for broader speech generation tasks."
   ],
   "p1": 4358,
   "pn": 4362,
   "doi": "10.21437/Interspeech.2024-1256",
   "url": "interspeech_2024/salman24_interspeech.html"
  },
  "shakeel24_interspeech": {
   "authors": [
    [
     "Muhammad",
     "Shakeel"
    ],
    [
     "Yui",
     "Sudo"
    ],
    [
     "Yifan",
     "Peng"
    ],
    [
     "Shinji",
     "Watanabe"
    ]
   ],
   "title": "Contextualized End-to-end Automatic Speech Recognition with Intermediate Biasing Loss",
   "original": "1257",
   "order": 805,
   "page_count": 5,
   "abstract": [
    "Contextualized end-to-end automatic speech recognition has been an active research area, with recent efforts focusing on the implicit learning of contextual phrases based on the final loss objective. However, these approaches ignore the useful contextual knowledge encoded in the intermediate layers. We hypothesize that employing explicit biasing loss as an auxiliary task in the encoder intermediate layers may better align text tokens or audio frames with the desired objectives. Our proposed intermediate biasing loss brings more regularization and contextualization to the network. Our method outperforms a conventional contextual biasing baseline on the LibriSpeech corpus, achieving a relative improvement of 22.5% in biased word error rate (B-WER) and up to 44% compared to the non-contextual baseline with a biasing list size of 100. Moreover, employing RNN-transducer-driven joint decoding further reduces the unbiased word error rate (U-WER), resulting in a more robust network."
   ],
   "p1": 3909,
   "pn": 3913,
   "doi": "10.21437/Interspeech.2024-1257",
   "url": "interspeech_2024/shakeel24_interspeech.html"
  },
  "liu24i_interspeech": {
   "authors": [
    [
     "Ailin",
     "Liu"
    ],
    [
     "Pepijn",
     "Vunderink"
    ],
    [
     "Jose",
     "Vargas Quiros"
    ],
    [
     "Chirag",
     "Raman"
    ],
    [
     "Hayley",
     "Hung"
    ]
   ],
   "title": "How Private is Low-Frequency Speech Audio in the Wild? An Analysis of Verbal Intelligibility by Humans and Machines",
   "original": "1258",
   "order": 561,
   "page_count": 5,
   "abstract": [
    "Low-frequency audio has been proposed as a promising privacy-preserving modality to study social dynamics in real-world settings. To this end, researchers have developed wearable devices that can record audio at frequencies as low as 1250 Hz to mitigate the automatic extraction of the verbal content of speech that may contain private details. This paper investigates the validity of this hypothesis, examining the degree to which low-frequency speech ensures verbal privacy. It includes simulating a potential privacy attack in various noise environments. Further, it explores the trade-off between the performance of voice activity detection, which is fundamental for understanding social behavior, and privacy-preservation. The evaluation incorporates subjective human intelligibility and automatic speech recognition performance, comprehensively analyzing the delicate balance between effective social behavior analysis and preserving verbal privacy."
   ],
   "p1": 2725,
   "pn": 2729,
   "doi": "10.21437/Interspeech.2024-1258",
   "url": "interspeech_2024/liu24i_interspeech.html"
  },
  "mosner24_interspeech": {
   "authors": [
    [
     "Ladislav",
     "Mošner"
    ],
    [
     "Romain",
     "Serizel"
    ],
    [
     "Lukáš",
     "Burget"
    ],
    [
     "Oldřich",
     "Plchot"
    ],
    [
     "Emmanuel",
     "Vincent"
    ],
    [
     "Junyi",
     "Peng"
    ],
    [
     "Jan",
     "Černocký"
    ]
   ],
   "title": "Multi-Channel Extension of Pre-trained Models for Speaker Verification",
   "original": "1260",
   "order": 442,
   "page_count": 5,
   "abstract": [
    "In this work, we focus on designing a multi-channel speech processing system based on large pre-trained models. These models are typically trained for single-channel scenarios via self-supervised learning (SSL). A common approach to using the SSL models with microphone array data is to prepend it with a multi-channel speech enhancement. The downside is that spatial information can be leveraged only by the pre-processing stage, and enhancement errors get propagated to the SSL model. We aim to alleviate the issue by designing METRO, a Multi-channel ExTension of pRe-trained mOdels. It interleaves per-channel processing with cross-channel information exchange, eventually fusing channels into one. While our approach is general, here we focus on multi-channel speaker verification. Our experiments on the MultiSV corpus show noteworthy improvements over the best-published results on the dataset."
   ],
   "p1": 2135,
   "pn": 2139,
   "doi": "10.21437/Interspeech.2024-1260",
   "url": "interspeech_2024/mosner24_interspeech.html"
  },
  "ravenscroft24_interspeech": {
   "authors": [
    [
     "William",
     "Ravenscroft"
    ],
    [
     "George",
     "Close"
    ],
    [
     "Stefan",
     "Goetze"
    ],
    [
     "Thomas",
     "Hain"
    ],
    [
     "Mohammad",
     "Soleymanpour"
    ],
    [
     "Anurag",
     "Chowdhury"
    ],
    [
     "Mark C.",
     "Fuhs"
    ]
   ],
   "title": "Transcription-Free Fine-Tuning of Speech Separation Models for Noisy and Reverberant Multi-Speaker Automatic Speech Recognition",
   "original": "1264",
   "order": 1024,
   "page_count": 5,
   "abstract": [
    "One solution to automatic speech recognition (ASR) of overlapping speakers is to separate speech and then perform ASR on the separated signals. Commonly, the separator produces artefacts which often degrade ASR performance. Addressing this issue typically requires reference transcriptions to jointly train the separation and ASR networks. This is often not viable for training on real-world in-domain audio where reference transcript information is not always available. This paper proposes a transcription-free method for joint training using only audio signals. The proposed method uses embedding differences of pre-trained ASR encoders as a loss with a proposed modification to permutation invariant training (PIT) called guided PIT (GPIT). The method achieves a 6.4% improvement in word error rate (WER) measures over a signal-level loss and also shows enhancement improvements in perceptual measures such as short-time objective intelligibility (STOI)."
   ],
   "p1": 4998,
   "pn": 5002,
   "doi": "10.21437/Interspeech.2024-1264",
   "url": "interspeech_2024/ravenscroft24_interspeech.html"
  },
  "hsieh24b_interspeech": {
   "authors": [
    [
     "Tsun-An",
     "Hsieh"
    ],
    [
     "Heeyoul",
     "Choi"
    ],
    [
     "Minje",
     "Kim"
    ]
   ],
   "title": "Multimodal Representation Loss Between Timed Text and Audio for Regularized Speech Separation",
   "original": "1265",
   "order": 120,
   "page_count": 5,
   "abstract": [
    "Recent studies highlight the potential of textual modalities in conditioning the speech separation model's inference process. However, regularization-based methods remain underexplored despite their advantages of not requiring auxiliary text data during the test time. To address this gap, we introduce a timed text-based regularization (TTR) method that uses language model-derived semantics to improve speech separation models. Our approach involves two steps. We begin with two pretrained audio and language models, WavLM and BERT, respectively. Then, a Transformer-based audio summarizer is learned to align the audio and word embeddings and to minimize their gap. The summarizer Transformer, incorporated as a regularizer, promotes the separated sources' alignment with the semantics from the timed text. Experimental results show that the proposed TTR method consistently improves the various objective metrics of the separation results over the unregularized baselines."
   ],
   "p1": 592,
   "pn": 596,
   "doi": "10.21437/Interspeech.2024-1265",
   "url": "interspeech_2024/hsieh24b_interspeech.html"
  },
  "zhang24i_interspeech": {
   "authors": [
    [
     "Wangyou",
     "Zhang"
    ],
    [
     "Kohei",
     "Saijo"
    ],
    [
     "Jee-weon",
     "Jung"
    ],
    [
     "Chenda",
     "Li"
    ],
    [
     "Shinji",
     "Watanabe"
    ],
    [
     "Yanmin",
     "Qian"
    ]
   ],
   "title": "Beyond Performance Plateaus: A Comprehensive Study on Scalability in Speech Enhancement",
   "original": "1266",
   "order": 357,
   "page_count": 5,
   "abstract": [
    "Deep learning-based speech enhancement (SE) models have achieved impressive performance in the past decade. Numerous advanced architectures have been designed to deliver state-of-the-art performance; however, their scalability potential remains unrevealed. Meanwhile, the majority of research focuses on small-sized datasets with restricted diversity, leading to a plateau in performance improvement. In this paper, we aim to provide new insights for addressing the above issues by exploring the scalability of SE models in terms of architectures, model sizes, compute budgets, and dataset sizes. Our investigation involves several popular SE architectures and speech data from different domains. Experiments reveal both similarities and distinctions between the scaling effects in SE and other tasks such as speech recognition. These findings further provide insights into the under-explored SE directions, e.g., larger-scale multi-domain corpora and efficiently scalable architectures."
   ],
   "p1": 1740,
   "pn": 1744,
   "doi": "10.21437/Interspeech.2024-1266",
   "url": "interspeech_2024/zhang24i_interspeech.html"
  },
  "sun24c_interspeech": {
   "authors": [
    [
     "Jianyuan",
     "Sun"
    ],
    [
     "Wenwu",
     "Wang"
    ],
    [
     "Mark D.",
     "Plumbley"
    ]
   ],
   "title": "PFCA-Net: Pyramid Feature Fusion and Cross Content Attention Network for Automated Audio Captioning",
   "original": "1268",
   "order": 235,
   "page_count": 5,
   "abstract": [
    "Automated audio captioning (AAC) aims to generate textual descriptions for a given audio clip. Despite the existing AAC models obtaining promising performance, they struggle to capture intricate audio patterns due to only using a high-dimensional representation. In this paper, we propose a new encoder-decoder model for AAC, called the Pyramid Feature Fusion and Cross Context Attention Network (PFCA-Net). In PFCA-Net, the encoder is constructed using a pyramid network, facilitating the extraction of audio features across multiple scales. It achieves this by combining top-down and bottom-up connections to fuse features across scales, resulting in feature maps at various scales. In the decoder, cross-content attention is designed to fuse the different scale features which allows the propagation of information from a low-scale to a high-scale. Experimental results show that PFCA-Net achieves considerable improvement over existing models."
   ],
   "p1": 1130,
   "pn": 1134,
   "doi": "10.21437/Interspeech.2024-1268",
   "url": "interspeech_2024/sun24c_interspeech.html"
  },
  "chung24_interspeech": {
   "authors": [
    [
     "Woo-Jin",
     "Chung"
    ],
    [
     "Hong-Goo",
     "Kang"
    ]
   ],
   "title": "Speaker-Independent Acoustic-to-Articulatory Inversion through Multi-Channel Attention Discriminator",
   "original": "1269",
   "order": 317,
   "page_count": 5,
   "abstract": [
    "We present a novel speaker-independent acoustic-to-articulatory inversion (AAI) model, overcoming the limitations observed in conventional AAI models that rely on acoustic features derived from restricted datasets. To address these challenges, we leverage representations from a pre-trained self-supervised learning (SSL) model to more effectively estimate the global, local, and kinematic pattern information in Electromagnetic Articulography (EMA) signals during the AAI process. We train our model using an adversarial approach and introduce an attention-based Multi-duration phoneme discriminator (MDPD) designed to fully capture the intricate relationship among multi-channel articulatory signals. Our method achieves a Pearson correlation coefficient of 0.847, marking state-of-the-art performance in speaker-independent AAI models. The implementation details and code can be found online."
   ],
   "p1": 1540,
   "pn": 1544,
   "doi": "10.21437/Interspeech.2024-1269",
   "url": "interspeech_2024/chung24_interspeech.html"
  },
  "teleki24_interspeech": {
   "authors": [
    [
     "Maria",
     "Teleki"
    ],
    [
     "Xiangjue",
     "Dong"
    ],
    [
     "Soohwan",
     "Kim"
    ],
    [
     "James",
     "Caverlee"
    ]
   ],
   "title": "Comparing ASR Systems in the Context of Speech Disfluencies",
   "original": "1270",
   "order": 934,
   "page_count": 5,
   "abstract": [
    "In this work, we evaluate the disfluency capabilities of two automatic speech recognition systems - Google ASR and WhisperX - through a study of 10 human-annotated podcast episodes and a larger set of 82,601 podcast episodes. We employ a state-of-the-art disfluency annotation model to perform a fine-grained analysis of the disfluencies in both the scripted and non-scripted podcasts. We find, on the set of 10 podcasts, that while WhisperX overall tends to perform better, Google ASR outperforms in WIL and BLEU scores for non-scripted podcasts. We also find that Google ASR’s transcripts tend to contain closer to the ground truth number of edited-type disfluent nodes, while WhisperX’s transcripts are closer for interjection-type disfluent nodes. This same pattern is present in the larger set. Our findings have implications for the choice of an ASR model when building a larger system, as the choice should be made depending on the distribution of disfluent nodes present in the data."
   ],
   "p1": 4548,
   "pn": 4552,
   "doi": "10.21437/Interspeech.2024-1270",
   "url": "interspeech_2024/teleki24_interspeech.html"
  },
  "garcia24_interspeech": {
   "authors": [
    [
     "Ricardo",
     "García"
    ],
    [
     "Rodrigo",
     "Mahu"
    ],
    [
     "Nicolás",
     "Grágeda"
    ],
    [
     "Alejandro",
     "Luzanto"
    ],
    [
     "Nicolas",
     "Bohmer"
    ],
    [
     "Carlos",
     "Busso"
    ],
    [
     "Néstor",
     "Becerra Yoma"
    ]
   ],
   "title": "Speech emotion recognition with deep learning beamforming  on a distant human-robot interaction scenario",
   "original": "1273",
   "order": 659,
   "page_count": 5,
   "abstract": [
    "Human-robot interaction (HRI) is becoming a truly relevant topic imposing many challenges for state-of-the-art speech technology. This paper describes the first evaluation of speech emotion recognition (SER) technology with non-acted speech data recorded in a real indoor HRI scenario using deep learning-based beamforming technologies. The results presented show that deep learning beamforming gives in average an average concordance correlation coefficient (CCC) that is 15.03% higher than the ordinary minimum variance distortionless response (MVDR) beamformer when the SER system was trained with simulated conditions, which included an acoustic model of the testing HRI environment. Training by simulating the test scenarios and testing with real HRI static data provides on average an average CCC that is just 22.5% smaller than the ideal condition where training and testing were performed with the original MSP-Podcast database. This suggests the possibility to train SER engines with methods that emulates complex testing scenarios without recording further data."
   ],
   "p1": 3215,
   "pn": 3219,
   "doi": "10.21437/Interspeech.2024-1273",
   "url": "interspeech_2024/garcia24_interspeech.html"
  },
  "yadav24_interspeech": {
   "authors": [
    [
     "Sarthak",
     "Yadav"
    ],
    [
     "Zheng-Hua",
     "Tan"
    ]
   ],
   "title": "Audio Mamba: Selective State Spaces for Self-Supervised Audio Representations",
   "original": "1274",
   "order": 112,
   "page_count": 5,
   "abstract": [
    " Despite its widespread adoption as the prominent neural architecture,  the Transformer has spurred several independent lines of work to address its limitations. One such approach is selective state space models, which have demonstrated promising results for language modelling. However, their feasibility for learning self-supervised, general-purpose audio representations is yet to be investigated. This work proposes Audio Mamba, a selective state space model for learning general-purpose audio representations from randomly masked spectrogram patches through self-supervision. Empirical results on ten diverse audio recognition downstream tasks show that the proposed models, pretrained on the AudioSet dataset, consistently outperform comparable self-supervised audio spectrogram transformer (SSAST) baselines by a considerable margin and demonstrate better performance in dataset size, sequence length and model size comparisons."
   ],
   "p1": 552,
   "pn": 556,
   "doi": "10.21437/Interspeech.2024-1274",
   "url": "interspeech_2024/yadav24_interspeech.html"
  },
  "houston24_interspeech": {
   "authors": [
    [
     "Brady",
     "Houston"
    ],
    [
     "Omid",
     "Sadjadi"
    ],
    [
     "Zejiang",
     "Hou"
    ],
    [
     "Srikanth",
     "Vishnubhotla"
    ],
    [
     "Kyu J.",
     "Han"
    ]
   ],
   "title": "Improving Multilingual ASR Robustness to Errors in Language Input",
   "original": "1278",
   "order": 259,
   "page_count": 5,
   "abstract": [
    "Explicitly adding language information to multilingual ASR models during training has been shown to improve their performance. However, this also requires using language information during inference. In cascaded systems, this language label may come from external language identification models, which are susceptible to errors. In this work, we characterize the sensitivity to errors in language inputs of several common language-incorporation strategies used in multilingual ASR. We show that some of these strategies are highly sensitive to the correctness of language information being used during inference, and also demonstrate that introducing a small amount of language label noise during training can greatly improve the model’s robustness to incorrect language information. As multilingual ASR continues to become more common, this work demonstrates the importance of understanding the sensitivity of these models to language inputs and ensuring models are robust to errors."
   ],
   "p1": 1250,
   "pn": 1254,
   "doi": "10.21437/Interspeech.2024-1278",
   "url": "interspeech_2024/houston24_interspeech.html"
  },
  "nguyen24_interspeech": {
   "authors": [
    [
     "Minh",
     "Nguyen"
    ],
    [
     "Franck",
     "Dernoncourt"
    ],
    [
     "Seunghyun",
     "Yoon"
    ],
    [
     "Hanieh",
     "Deilamsalehy"
    ],
    [
     "Hao",
     "Tan"
    ],
    [
     "Ryan",
     "Rossi"
    ],
    [
     "Quan Hung",
     "Tran"
    ],
    [
     "Trung",
     "Bui"
    ],
    [
     "Thien Huu",
     "Nguyen"
    ]
   ],
   "title": "Identifying Speakers in Dialogue Transcripts: A Text-based Approach Using Pretrained Language Models",
   "original": "1280",
   "order": 783,
   "page_count": 5,
   "abstract": [
    "We introduce an approach to identifying speaker names in dialogue transcripts, a crucial task for enhancing content accessibility and searchability in digital media archives. Despite the advancements in speech recognition, the task of text-based speaker identification (SpeakerID) has received limited attention, lacking large-scale, diverse datasets for effective model training. Addressing these gaps, we present a novel, large-scale dataset derived from the MediaSum corpus, encompassing transcripts from a wide range of media sources. We propose novel transformer-based models tailored for SpeakerID, leveraging contextual cues within dialogues to accurately attribute speaker names. Through extensive experiments, our best model achieves a great precision of 80.3%, setting a new benchmark for SpeakerID. The data and code are publicly available here: https: //github.com/adobe-research/speaker-identification"
   ],
   "p1": 3799,
   "pn": 3803,
   "doi": "10.21437/Interspeech.2024-1280",
   "url": "interspeech_2024/nguyen24_interspeech.html"
  },
  "perez24_interspeech": {
   "authors": [
    [
     "Matthew",
     "Perez"
    ],
    [
     "Aneesha",
     "Sampath"
    ],
    [
     "Minxue",
     "Niu"
    ],
    [
     "Emily",
     "Mower Provost"
    ]
   ],
   "title": "Beyond Binary: Multiclass Paraphasia Detection with Generative Pretrained Transformers and End-to-End Models",
   "original": "1281",
   "order": 847,
   "page_count": 5,
   "abstract": [
    "Aphasia is a language disorder that can lead to speech errors known as paraphasias, which involve the misuse, substitution, or invention of words. Automatic paraphasia detection can help those with Aphasia by facilitating clinical assessment and treatment planning options. However, most automatic paraphasia detection works have focused solely on binary detection, which involves recognizing only the presence or absence of a paraphasia. Multiclass paraphasia detection represents an unexplored area of research that focuses on identifying multiple types of paraphasias and where they occur in a given speech segment. We present novel approaches that use a generative pretrained transformer (GPT) to identify paraphasias from transcripts as well as two end-to-end approaches that focus on modeling both automatic speech recognition (ASR) and paraphasia classification as multiple sequences vs. a single sequence. We demonstrate that a single sequence model outperforms GPT baselines for multiclass paraphasia detection."
   ],
   "p1": 4119,
   "pn": 4123,
   "doi": "10.21437/Interspeech.2024-1281",
   "url": "interspeech_2024/perez24_interspeech.html"
  },
  "klein24_interspeech": {
   "authors": [
    [
     "Nicholas",
     "Klein"
    ],
    [
     "Tianxiang",
     "Chen"
    ],
    [
     "Hemlata",
     "Tak"
    ],
    [
     "Ricardo",
     "Casal"
    ],
    [
     "Elie",
     "Khoury"
    ]
   ],
   "title": "Source Tracing of Audio Deepfake Systems",
   "original": "1283",
   "resource": "https://doi.org/10.5281/zenodo.11593133",
   "order": 229,
   "page_count": 5,
   "abstract": [
    "Recent progress in generative AI technology has made audio deepfakes remarkably more realistic. While current research on anti-spoofing systems primarily focuses on assessing whether a given audio sample is fake or genuine, there has been limited attention on discerning the specific techniques to create the audio deepfakes. Algorithms commonly used in audio deepfake generation, like text-to-speech (TTS) and voice conversion (VC), undergo distinct stages including input processing, acoustic modeling, and waveform generation. In this work, we introduce a system designed to classify various spoofing attributes, capturing the distinctive features of individual modules throughout the entire generation pipeline. We evaluate our system on two datasets: the ASVspoof 2019 Logical Access and the Multi-Language Audio Anti-Spoofing Dataset (MLAAD). Results from both experiments demonstrate the robustness of the system to identify the different spoofing attributes of deepfake generation systems."
   ],
   "p1": 1100,
   "pn": 1104,
   "doi": "10.21437/Interspeech.2024-1283",
   "url": "interspeech_2024/klein24_interspeech.html"
  },
  "boeddeker24_interspeech": {
   "authors": [
    [
     "Christoph",
     "Boeddeker"
    ],
    [
     "Tobias",
     "Cord-Landwehr"
    ],
    [
     "Reinhold",
     "Haeb-Umbach"
    ]
   ],
   "title": "Once more Diarization: Improving meeting transcription systems through segment-level speaker reassignment",
   "original": "1286",
   "order": 332,
   "page_count": 5,
   "abstract": [
    "Diarization is a crucial component in meeting transcription systems to ease the challenges of speech enhancement and attribute the transcriptions to the correct speaker. Particularly in the presence of overlapping or noisy speech, these systems have problems reliably assigning the correct speaker labels, leading to a significant amount of speaker confusion errors. We propose to add segment-level speaker reassignment to address this issue. By revisiting, after speech enhancement, the speaker attribution for each segment, speaker confusion errors from the initial diarization stage are significantly reduced. Through experiments across different system configurations and datasets, we further demonstrate the effectiveness and applicability in various domains. Our results show that segment-level speaker reassignment successfully rectifies at least 40% of speaker confusion word errors, highlighting its potential for enhancing diarization accuracy in meeting transcription systems."
   ],
   "p1": 1615,
   "pn": 1619,
   "doi": "10.21437/Interspeech.2024-1286",
   "url": "interspeech_2024/boeddeker24_interspeech.html"
  },
  "pirlogeanu24_interspeech": {
   "authors": [
    [
     "Gabriel",
     "Pîrlogeanu"
    ],
    [
     "Octavian",
     "Pascu"
    ],
    [
     "Alexandru-Lucian",
     "Georgescu"
    ],
    [
     "Horia",
     "Cucu"
    ]
   ],
   "title": "Hybrid-Diarization System with Overlap Post-Processing for the DISPLACE 2024 Challenge",
   "original": "1287",
   "order": 334,
   "page_count": 5,
   "abstract": [
    "This paper describes our team's collaborative efforts in participating in the Track 1 for Speaker Diarization of the Diarization of Speaker and Language in Conversational Environments (DISPLACE) Challenge 2024. Our submission focuses on creating a diarization system that is robust to noisy conditions, as well as high amounts of overlapped speech. We conduct an exhaustive study on each component of a hybrid system using techniques such as semi-supervised learning, ensemble of several systems and experiment with both a neural overlap detection module, as well as a post-processing technique using an external overlap detection system. Our final system achieves a diarization error rate (DER) of 28.04% on Phase 1 Eval set, representing a relative improvement of 19.33% compared to the baseline DER of 34.76%."
   ],
   "p1": 1625,
   "pn": 1629,
   "doi": "10.21437/Interspeech.2024-1287",
   "url": "interspeech_2024/pirlogeanu24_interspeech.html"
  },
  "bras24_interspeech": {
   "authors": [
    [
     "Chris",
     "Bras"
    ],
    [
     "Tanvina",
     "Patel"
    ],
    [
     "Odette",
     "Scharenborg"
    ]
   ],
   "title": "Using articulated speech EEG signals for imagined speech decoding",
   "original": "1289",
   "order": 83,
   "page_count": 5,
   "abstract": [
    "Brain-Computer Interfaces (BCIs) open avenues for communication among individuals unable to use voice or gestures. Silent speech interfaces are one such approach for BCIs that could offer a transformative means of connecting with the external world. Performance on imagined speech decoding however is rather low due to, amongst others, data scarcity and the lack of a clear starting and end point of the imagined speech in the brain signal. We investigate whether using electroencephalography (EEG) signals from articulated speech can be used to improve imagined speech decoding in two ways: we investigate whether articulated speech EEG signals can be used to predict the end point of the imagined speech and use the articulated speech EEG as extra training data for speaker-independent imagined vowel classification. Our results show that using EEG data from articulated speech did not improve classification of vowels in imagined speech, probably due to high variability in EEG signals amongst speakers."
   ],
   "p1": 407,
   "pn": 411,
   "doi": "10.21437/Interspeech.2024-1289",
   "url": "interspeech_2024/bras24_interspeech.html"
  },
  "phung24_interspeech": {
   "authors": [
    [
     "Emmy",
     "Phung"
    ],
    [
     "Harsh",
     "Deshpande"
    ],
    [
     "Ahmad",
     "Emami"
    ],
    [
     "Kanishk",
     "Singh"
    ]
   ],
   "title": "AR-NLU: A Framework for Enhancing Natural Language Understanding Model Robustness against ASR Errors",
   "original": "1292",
   "order": 274,
   "page_count": 5,
   "abstract": [
    "A major challenge with pipeline spoken language understanding systems is that errors in the upstream automatic speech recognition (ASR) engine adversely impact downstream natural language understanding (NLU) models. To address this challenge, we propose an ASR-Robust NLU (AR-NLU) framework that extends a pre-existing NLU model by training it simultaneously on two input streams: human generated or gold transcripts and noisy ASR transcripts. We apply contrastive learning to make the model learn the same representations and predictions for both gold and ASR inputs, thereby enhancing its robustness against ASR noises. To demonstrate the effectiveness of this framework, we present two AR-NLU models: a Robust Intent DEtection (RIDE) and ASR-Robust BI-encoder for NameD Entity Recognition (AR-BINDER). Experimental results show that our proposed AR-NLU framework is applicable to various NLU models and significantly outperforms the original models in both sequence and token classification tasks."
   ],
   "p1": 1325,
   "pn": 1329,
   "doi": "10.21437/Interspeech.2024-1292",
   "url": "interspeech_2024/phung24_interspeech.html"
  },
  "bayestehtashk24_interspeech": {
   "authors": [
    [
     "Alireza",
     "Bayestehtashk"
    ],
    [
     "Amit",
     "Kumar"
    ],
    [
     "Mike",
     "Wurtz"
    ]
   ],
   "title": "Design of Feedback Active Noise Cancellation Filter Using Nested Recurrent Neural Networks",
   "original": "1295",
   "order": 670,
   "page_count": 5,
   "abstract": [
    "We examine the problem of Feedback (FB) Active Noise Cancellation (ANC) and propose a novel DSP-inspired neural network for designing FB ANC filters. It reinterprets the ANC problem as a neural network optimization issue, thus allowing the use of modern machine learning tools. At the same time it also benefits from classical DSP techniques as it retains and extends the state of the art for efficient generation of anti-noise. The proposed approach leverages machine learning algorithms to learn stochastically optimal FB ANC filter coefficients under a variety of conditions and constraints that make the problem intractable for classical methods. One challenge with FB ANC filter design is to guarantee the stability of the IIR filter structure as well as the feedback loop around it. The proposed method meets this stability requirement while providing new avenues for improved system efficiency and adaptability. We demonstrate its effectiveness through simulations using real-world models. "
   ],
   "p1": 3270,
   "pn": 3274,
   "doi": "10.21437/Interspeech.2024-1295",
   "url": "interspeech_2024/bayestehtashk24_interspeech.html"
  },
  "tuttosi24_interspeech": {
   "authors": [
    [
     "Paige",
     "Tuttösí"
    ],
    [
     "H. Henny",
     "Yeung"
    ],
    [
     "Yue",
     "Wang"
    ],
    [
     "Fenqi",
     "Wang"
    ],
    [
     "Guillaume",
     "Denis"
    ],
    [
     "Jean-Julien",
     "Aucouturier"
    ],
    [
     "Angelica",
     "Lim"
    ]
   ],
   "title": "Mmm whatcha say? Uncovering distal and proximal context effects in first and second-language word perception using psychophysical reverse correlation",
   "original": "1296",
   "resource": "https://doi.org/10.5281/zenodo.12761242",
   "order": 211,
   "page_count": 5,
   "abstract": [
    "Acoustic context effects, where surrounding changes in pitch, rate or timbre influence the perception of a sound, are well documented in speech perception, but how they interact with lan- guage background remains unclear. Using a reverse-correlation approach, we systematically varied the pitch and speech rate in phrases around different pairs of vowels for second language (L2) speakers of English (/i/-/I/) and French (/u/-/y/), thus reconstructing, in a data-driven manner, the prosodic profiles that bias their perception. Testing English and French speakers (n=25), we showed that vowel perception is in fact influenced by conflicting effects from the surrounding pitch and speech rate: a congruent proximal effect 0.2s pre-target and a distal contrastive effect up to 1s before; and found that L1 and L2 speakers exhibited strikingly similar prosodic profiles in perception. We provide a novel method to investigate acoustic context effects across stimuli, timescales, and acoustic domain."
   ],
   "p1": 1010,
   "pn": 1014,
   "doi": "10.21437/Interspeech.2024-1296",
   "url": "interspeech_2024/tuttosi24_interspeech.html"
  },
  "baade24_interspeech": {
   "authors": [
    [
     "Alan",
     "Baade"
    ],
    [
     "Puyuan",
     "Peng"
    ],
    [
     "David",
     "Harwath"
    ]
   ],
   "title": "Neural Codec Language Models for Disentangled and Textless Voice Conversion",
   "original": "1298",
   "order": 38,
   "page_count": 5,
   "abstract": [
    "We introduce a method for textless any-to-any voice conversion based on the recent progress in speech synthesis driven by neural codec language models. To disentangle the speaker and linguistic information, we adapt a speaker normalizing procedure for discrete semantic units, and then generate with an autoregressive language model for greatly improved diversity. We further improve the similarity of the output audio to the target speaker's voice by leveraging classifier free guidance. We evaluate our techniques against current text to speech synthesis and voice conversion systems and compare the effectiveness of different neural codec language model pipelines. We demonstrate state-of-the-art results in accent disentanglement and speaker similarity for voice conversion with significantly less compute than existing codec language models such as VALL-E."
   ],
   "p1": 182,
   "pn": 186,
   "doi": "10.21437/Interspeech.2024-1298",
   "url": "interspeech_2024/baade24_interspeech.html"
  },
  "friedrichs24_interspeech": {
   "authors": [
    [
     "Daniel",
     "Friedrichs"
    ],
    [
     "Monica",
     "Lancheros"
    ],
    [
     "Sam",
     "Kirkham"
    ],
    [
     "Lei",
     "He"
    ],
    [
     "Andrew",
     "Clark"
    ],
    [
     "Clemens",
     "Lutz"
    ],
    [
     "Volker",
     "Dellwo"
    ],
    [
     "Steven",
     "Moran"
    ]
   ],
   "title": "Temporal Co-Registration of Simultaneous Electromagnetic Articulography and Electroencephalography for Precise Articulatory and Neural Data Alignment",
   "original": "1299",
   "order": 640,
   "page_count": 5,
   "abstract": [
    "This study presents a temporal co-registration method combining electromagnetic articulography (EMA) and electroencephalography (EEG) to capture the neural planning and execution phases of speech with high precision. Traditional EEG alignment based on acoustic vocal onset is often inaccurate due to the variable lag between articulatory and acoustic onsets. Our approach synchronizes EMA-derived speech kinematics with EEG data, addressing these challenges. We also examined the interaction between EMA and EEG systems, focusing on the integrity of EMA signals in the presence of EEG equipment and the electromagnetic influence of EMA on EEG signal quality. The method achieved a mean alignment delay of 2.7 ms (SD = 0.4 ms), enabling detailed analysis of pre-articulatory brain activities. Additionally, our evaluations confirmed the robustness of EMA signals and EEG event-related potentials, supporting the method's precision, feasibility, and reliability for speech planning research."
   ],
   "p1": 3120,
   "pn": 3124,
   "doi": "10.21437/Interspeech.2024-1299",
   "url": "interspeech_2024/friedrichs24_interspeech.html"
  },
  "pascu24_interspeech": {
   "authors": [
    [
     "Octavian",
     "Pascu"
    ],
    [
     "Adriana",
     "Stan"
    ],
    [
     "Dan",
     "Oneata"
    ],
    [
     "Elisabeta",
     "Oneata"
    ],
    [
     "Horia",
     "Cucu"
    ]
   ],
   "title": "Towards generalisable and calibrated audio deepfake detection with self-supervised representations",
   "original": "1302",
   "order": 990,
   "page_count": 5,
   "abstract": [
    "Generalisation—the ability of a model to perform well on unseen data—is crucial for building reliable deepfake detectors. However, recent studies have shown that the current audio deepfake models fall short of this desideratum. In this work we investigate the potential of pretrained self-supervised representations in building general and calibrated audio deepfake detection models. We show that large frozen representations coupled with a simple logistic regression classifier are extremely effective in achieving strong generalisation capabilities: compared to the RawNet2 model, this approach reduces the equal error rate from 30.9% to 8.8% on a benchmark of eight deepfake datasets, while learning less than 2k parameters. Moreover, the proposed method produces considerably more reliable predictions compared to previous approaches making it more suitable for realistic use."
   ],
   "p1": 4828,
   "pn": 4832,
   "doi": "10.21437/Interspeech.2024-1302",
   "url": "interspeech_2024/pascu24_interspeech.html"
  },
  "shon24_interspeech": {
   "authors": [
    [
     "Suwon",
     "Shon"
    ],
    [
     "Kwangyoun",
     "Kim"
    ],
    [
     "Yi-Te",
     "Hsu"
    ],
    [
     "Prashant",
     "Sridhar"
    ],
    [
     "Shinji",
     "Watanabe"
    ],
    [
     "Karen",
     "Livescu"
    ]
   ],
   "title": "DiscreteSLU: A Large Language Model with Self-Supervised Discrete Speech Units for Spoken Language Understanding",
   "original": "1306",
   "order": 854,
   "page_count": 5,
   "abstract": [
    "The integration of pre-trained text-based large language models (LLM) with speech input has enabled instruction-following capabilities for diverse speech tasks. This integration requires the use of a speech encoder, a speech adapter, and an LLM, trained on diverse tasks. We propose the use of discrete speech units (DSU), rather than continuous-valued speech encoder outputs, that are converted to the LLM token embedding space using the speech adapter. We generate DSU using a self-supervised speech encoder followed by k-means clustering. The proposed model shows robust performance on speech inputs from seen/unseen domains and instruction-following capability in spoken question answering. We also explore various types of DSU extracted from different layers of the self-supervised speech encoder, as well as Mel frequency Cepstral Coefficients (MFCC). Our findings suggest that the ASR task and datasets are not crucial in instruction-tuning for spoken question answering tasks."
   ],
   "p1": 4154,
   "pn": 4158,
   "doi": "10.21437/Interspeech.2024-1306",
   "url": "interspeech_2024/shon24_interspeech.html"
  },
  "zulfikar24_interspeech": {
   "authors": [
    [
     "Wazeer",
     "Zulfikar"
    ],
    [
     "Nishat",
     "Protyasha"
    ],
    [
     "Camila",
     "Canales"
    ],
    [
     "Heli",
     "Patel"
    ],
    [
     "James",
     "Williamson"
    ],
    [
     "Laura",
     "Sarnie"
    ],
    [
     "Lisa",
     "Nowinski"
    ],
    [
     "Nataliya",
     "Kosmyna"
    ],
    [
     "Paige",
     "Townsend"
    ],
    [
     "Sophia",
     "Yuditskaya"
    ],
    [
     "Tanya",
     "Talkar"
    ],
    [
     "Utkarsh Oggy",
     "Sarawgi"
    ],
    [
     "Christopher",
     "McDougle"
    ],
    [
     "Thomas",
     "Quatieri"
    ],
    [
     "Pattie",
     "Maes"
    ],
    [
     "Maria",
     "Mody"
    ]
   ],
   "title": "Analyzing Speech Motor Movement using Surface Electromyography in Minimally Verbal Adults with Autism Spectrum Disorder",
   "original": "1309",
   "order": 1045,
   "page_count": 5,
   "abstract": [
    "Adults who are minimally verbal with autism spectrum disorder (mvASD) have pronounced speech difficulties linked to impaired motor skills. Existing research and clinical assessments primarily use indirect methods such as standardized tests, video-based facial features, and handwriting tasks, which may not directly target speech-related motor skills. In this study, we measure activity from eight facial muscles associated with speech using surface electromyography (sEMG), during carefully designed tasks. The findings reveal a higher power in the sEMG signals and a significantly greater correlation between the sEMG channels in mvASD adults (N=12) compared to age and gender-matched neurotypical controls (N=14). This suggests stronger muscle activation and greater synchrony in the discharge patterns of motor units. Further, eigenvalues derived from correlation matrices indicate lower complexity in muscle coordination in mvASD, implying fewer degrees of freedom in motor control."
   ],
   "p1": 5103,
   "pn": 5107,
   "doi": "10.21437/Interspeech.2024-1309",
   "url": "interspeech_2024/zulfikar24_interspeech.html"
  },
  "janiczek24_interspeech": {
   "authors": [
    [
     "John",
     "Janiczek"
    ],
    [
     "Dading",
     "Chong"
    ],
    [
     "Dongyang",
     "Dai"
    ],
    [
     "Arlo",
     "Faria"
    ],
    [
     "Chao",
     "Wang"
    ],
    [
     "Tao",
     "Wang"
    ],
    [
     "Yuzong",
     "Liu"
    ]
   ],
   "title": "Multi-modal Adversarial Training for Zero-Shot Voice Cloning",
   "original": "1313",
   "order": 697,
   "page_count": 5,
   "abstract": [
    "A text-to-speech (TTS) model trained to reconstruct speech given text tends towards predictions that are close to the average characteristics of a dataset, failing to model the variations that make human speech sound natural. This problem is magnified for zero-shot voice cloning, a task that requires training data with high variance in speaking styles. We build off of recent works which have used Generative Advsarial Networks (GAN) by proposing a Transformer encoder-decoder architecture to conditionally discriminates between real and generated speech features. The discriminator is used in a training pipeline that improves both the acoustic and prosodic features of a TTS model. We introduce our novel adversarial training technique by applying it to a FastSpeech2 acoustic model and training on Libriheavy, a large multi-speaker dataset, for the task of zero-shot voice cloning. Our model achieves improvements over the baseline in terms of speech quality and speaker similarity."
   ],
   "p1": 3405,
   "pn": 3409,
   "doi": "10.21437/Interspeech.2024-1313",
   "url": "interspeech_2024/janiczek24_interspeech.html"
  },
  "jing24b_interspeech": {
   "authors": [
    [
     "Xin",
     "Jing"
    ],
    [
     "Andreas",
     "Triantafyllopoulos"
    ],
    [
     "Björn",
     "Schuller"
    ]
   ],
   "title": "ParaCLAP – Towards a general language-audio model for computational paralinguistic tasks",
   "original": "1315",
   "order": 240,
   "page_count": 5,
   "abstract": [
    "Contrastive language-audio pretraining (CLAP) has recently emerged as a method for making audio analysis more generalisable. Specifically, CLAP-style models are able to 'answer' a diverse set of language queries, extending the capabilities of audio models beyond a closed set of labels. However, CLAP relies on a large set of (audio, query) pairs for pretraining. While such sets are available for general audio tasks, like captioning or sound event detection, there are no datasets with matched audio and text queries for computational paralinguistic (CP) tasks. As a result, the community relies on generic CLAP models trained for general audio with limited success. In the present study, we explore training considerations for ParaCLAP, a CLAP-style model suited to CP, including a novel process for creating audio-language queries. We demonstrate its effectiveness on a set of computational paralinguistic tasks, where it is shown to surpass the performance of open-source state-of-the-art models. Our code and resources are publicly available at: https://github.com/KeiKinn/ParaCLAP"
   ],
   "p1": 1155,
   "pn": 1159,
   "doi": "10.21437/Interspeech.2024-1315",
   "url": "interspeech_2024/jing24b_interspeech.html"
  },
  "neumann24_interspeech": {
   "authors": [
    [
     "Michael",
     "Neumann"
    ],
    [
     "Hardik",
     "Kothare"
    ],
    [
     "Jackson",
     "Liscombe"
    ],
    [
     "Emma C.L.",
     "Leschly"
    ],
    [
     "Oliver",
     "Roesler"
    ],
    [
     "Vikram",
     "Ramanarayanan"
    ]
   ],
   "title": "Multimodal Digital Biomarkers for Longitudinal Tracking of Speech Impairment Severity in ALS: An Investigation of Clinically Important Differences",
   "original": "1318",
   "order": 507,
   "page_count": 5,
   "abstract": [
    "Speech biomarkers have shown promise for the remote assessment of ALS. However, to demonstrate clinical utility at tracking longitudinal progress of the disease, one needs to understand how well these biomarkers capture changes that are ‘clinically meaningful’, a concept that is not always clearly defined. Therefore, this paper defines and explores multiple methods of computing minimal clinically important difference (MCID) using ratings of speech impairment severity and listener effort as clinical anchors. We analyze how these methods impact the estimated responsiveness of various metrics collected from 125 ALS patients via a multimodal dialog based remote assessment platform. We find that select biomarkers are more responsive than the clinical standard ALSFRS-R across the board at tracking clinically meaningful changes related to speech severity. We further discuss advantages and disadvantages of different MCID computation methods for assessing ALS disease progression."
   ],
   "p1": 2460,
   "pn": 2464,
   "doi": "10.21437/Interspeech.2024-1318",
   "url": "interspeech_2024/neumann24_interspeech.html"
  },
  "eskimez24_interspeech": {
   "authors": [
    [
     "Sefik Emre",
     "Eskimez"
    ],
    [
     "Xiaofei",
     "Wang"
    ],
    [
     "Manthan",
     "Thakker"
    ],
    [
     "Chung-Hsien",
     "Tsai"
    ],
    [
     "Canrun",
     "Li"
    ],
    [
     "Zhen",
     "Xiao"
    ],
    [
     "Hemin",
     "Yang"
    ],
    [
     "Zirun",
     "Zhu"
    ],
    [
     "Min",
     "Tang"
    ],
    [
     "Jinyu",
     "Li"
    ],
    [
     "Sheng",
     "Zhao"
    ],
    [
     "Naoyuki",
     "Kanda"
    ]
   ],
   "title": "Total-Duration-Aware Duration Modeling for Text-to-Speech Systems",
   "original": "1327",
   "order": 473,
   "page_count": 5,
   "abstract": [
    "Accurate control of the total duration of generated speech by adjusting the speech rate is crucial for various text-to-speech (TTS) applications. However, the impact of adjusting the speech rate on speech quality, such as intelligibility and speaker characteristics, has been underexplored. In this work, we propose a novel total-duration-aware (TDA) duration model for TTS, where phoneme durations are predicted not only from the text input but also from an additional input of the total target duration. We also propose a MaskGIT-based duration model that enhances the diversity and quality of the predicted phoneme durations. Our results demonstrate that the proposed TDA duration models achieve better intelligibility and speaker similarity for various speech rate configurations compared to the baseline models. We also show that the proposed MaskGIT-based model can generate phoneme durations with higher quality and diversity compared to its regression or flow-matching counterparts."
   ],
   "p1": 2290,
   "pn": 2294,
   "doi": "10.21437/Interspeech.2024-1327",
   "url": "interspeech_2024/eskimez24_interspeech.html"
  },
  "suzuki24_interspeech": {
   "authors": [
    [
     "Keita",
     "Suzuki"
    ],
    [
     "Nobukatsu",
     "Hojo"
    ],
    [
     "Kazutoshi",
     "Shinoda"
    ],
    [
     "Saki",
     "Mizuno"
    ],
    [
     "Ryo",
     "Masumura"
    ]
   ],
   "title": "Participant-Pair-Wise Bottleneck Transformer for Engagement Estimation from Video Conversation",
   "original": "1329",
   "order": 839,
   "page_count": 5,
   "abstract": [
    "This study investigates the task of estimating the engagement of a target participant from video and audio during a multi-person conversation. For this task, interaction should be modeled effectively, considering the redundancy of video and audio across frames among multiple participants. Conventional Transformer-based methods in multimodal sentiment analysis succeeded in such efficient modeling by constraining the at- tention across multimodal data streams to go through only a small set of latent fusion units (“global tokens”) that form an attention bottleneck. However, performance can be limited in the multi-person model because it needs to model interaction among a larger number of data streams based on only a single global token sequence. To address this problem, we propose a participant-pair-wise bottleneck transformer (PPBT) that involves multiple global token sequences, each of which is dedicated to a particular pair of participants and demonstrates its effect."
   ],
   "p1": 4079,
   "pn": 4083,
   "doi": "10.21437/Interspeech.2024-1329",
   "url": "interspeech_2024/suzuki24_interspeech.html"
  },
  "xiao24b_interspeech": {
   "authors": [
    [
     "Qiao",
     "Xiao"
    ],
    [
     "Pingchuan",
     "Ma"
    ],
    [
     "Adriana",
     "Fernandez-Lopez"
    ],
    [
     "Boqian",
     "Wu"
    ],
    [
     "Lu",
     "Yin"
    ],
    [
     "Stavros",
     "Petridis"
    ],
    [
     "Mykola",
     "Pechenizkiy"
    ],
    [
     "Maja",
     "Pantic"
    ],
    [
     "Decebal Constantin",
     "Mocanu"
    ],
    [
     "Shiwei",
     "Liu"
    ]
   ],
   "title": "Dynamic Data Pruning for Automatic Speech Recognition",
   "original": "1330",
   "order": 922,
   "page_count": 5,
   "abstract": [
    "The recent success of Automatic Speech Recognition (ASR) is largely attributed to the ever-growing amount of training data. However, this trend has made model training prohibitively costly and imposed computational demands. While data pruning has been proposed to mitigate this issue by identifying a small subset of relevant data, its application in ASR has been barely explored, and existing works often entail significant overhead to achieve meaningful results. To fill this gap, this paper presents the first investigation of dynamic data pruning for ASR, finding that we can reach the full-data performance by dynamically selecting 70% of data. Furthermore, we introduce Dynamic Data Pruning for ASR (DDP-ASR), which offers several fine-grained pruning granularities specifically tailored for speech-related datasets, going beyond the conventional pruning of entire time sequences. Our intensive experiments show that DDP-ASR can save up to 1.6× training time with negligible performance loss."
   ],
   "p1": 4488,
   "pn": 4492,
   "doi": "10.21437/Interspeech.2024-1330",
   "url": "interspeech_2024/xiao24b_interspeech.html"
  },
  "chaudhary24_interspeech": {
   "authors": [
    [
     "Aryan",
     "Chaudhary"
    ],
    [
     "Arshdeep",
     "Singh"
    ],
    [
     "Vinayak",
     "Abrol"
    ],
    [
     "Mark D.",
     "Plumbley"
    ]
   ],
   "title": "Efficient CNNs with Quaternion Transformations and Pruning for Audio Tagging",
   "original": "1331",
   "order": 239,
   "page_count": 5,
   "abstract": [
    "This paper presents a novel approach to make convolutional neural networks (CNNs) efficient by reducing their computa- tional cost and memory footprint. Even though large-scale CNNs show state-of-the-art performance in many tasks, high computational costs and the requirement of a large memory footprint make them resource-hungry. Therefore, deploying large-scale CNNs on resource-constrained devices poses significant challenges. To address this challenge, we propose to use quaternion CNNs, where quaternion algebra enables the memory footprint to be reduced. Furthermore, we investigate methods to reduce the memory footprint and computational cost further through pruning the quaternion CNNs. Experimental evaluation of the audio tagging task involving the classification of 527 audio events from AudioSet shows that the quaternion algebra and pruning reduce memory footprint by 90% and computational cost by 70% compared to the original CNN model while maintaining similar performance."
   ],
   "p1": 1150,
   "pn": 1154,
   "doi": "10.21437/Interspeech.2024-1331",
   "url": "interspeech_2024/chaudhary24_interspeech.html"
  },
  "bai24b_interspeech": {
   "authors": [
    [
     "Yatong",
     "Bai"
    ],
    [
     "Trung",
     "Dang"
    ],
    [
     "Dung",
     "Tran"
    ],
    [
     "Kazuhito",
     "Koishida"
    ],
    [
     "Somayeh",
     "Sojoudi"
    ]
   ],
   "title": "ConsistencyTTA: Accelerating Diffusion-Based Text-to-Audio Generation with Consistency Distillation",
   "original": "1333",
   "resource": "https://doi.org/10.5281/zenodo.12798294",
   "order": 673,
   "page_count": 5,
   "abstract": [
    "Diffusion models are instrumental in text-to-audio (TTA) generation. Unfortunately, they suffer from slow inference due to an excessive number of queries to the underlying denoising network per generation. To address this bottleneck, we introduce ConsistencyTTA, a framework requiring only a single non-autoregressive network query, thereby accelerating TTA by hundreds of times. We achieve so by proposing “CFG-aware latent consistency model,” which adapts consistency generation into a latent space and incorporates classifier-free guidance (CFG) into model training. Moreover, unlike diffusion models, ConsistencyTTA can be finetuned closed-loop with audio-space text-aware metrics, such as CLAP score, to further enhance the generations. Our objective and subjective evaluation on the AudioCaps dataset shows that compared to diffusion-based counterparts, ConsistencyTTA reduces inference computation by 400x while retaining generation quality and diversity."
   ],
   "p1": 3285,
   "pn": 3289,
   "doi": "10.21437/Interspeech.2024-1333",
   "url": "interspeech_2024/bai24b_interspeech.html"
  },
  "lux24_interspeech": {
   "authors": [
    [
     "Florian",
     "Lux"
    ],
    [
     "Sarina",
     "Meyer"
    ],
    [
     "Lyonel",
     "Behringer"
    ],
    [
     "Frank",
     "Zalkow"
    ],
    [
     "Phat",
     "Do"
    ],
    [
     "Matt",
     "Coler"
    ],
    [
     "Emanuël A. P.",
     "Habets"
    ],
    [
     "Ngoc Thang",
     "Vu"
    ]
   ],
   "title": "Meta Learning Text-to-Speech Synthesis in over 7000 Languages",
   "original": "1335",
   "order": 1016,
   "page_count": 5,
   "abstract": [
    "In this work, we take on the challenging task of building a single text-to-speech synthesis system that is capable of generating speech in over 7000 languages, many of which lack sufficient data for traditional TTS development. By leveraging a novel integration of massively multilingual pretraining and meta learning to approximate language representations, our approach enables zero-shot speech synthesis in languages without any available data. We validate our system's performance through objective measures and human evaluation across a diverse linguistic landscape. By releasing our code and models publicly, we aim to empower communities with limited linguistic resources and foster further innovation in the field of speech technology."
   ],
   "p1": 4958,
   "pn": 4962,
   "doi": "10.21437/Interspeech.2024-1335",
   "url": "interspeech_2024/lux24_interspeech.html"
  },
  "wang24v_interspeech": {
   "authors": [
    [
     "Xiaofei",
     "Wang"
    ],
    [
     "Sefik Emre",
     "Eskimez"
    ],
    [
     "Manthan",
     "Thakker"
    ],
    [
     "Hemin",
     "Yang"
    ],
    [
     "Zirun",
     "Zhu"
    ],
    [
     "Min",
     "Tang"
    ],
    [
     "Yufei",
     "Xia"
    ],
    [
     "Jinzhu",
     "Li"
    ],
    [
     "Sheng",
     "Zhao"
    ],
    [
     "Jinyu",
     "Li"
    ],
    [
     "Naoyuki",
     "Kanda"
    ]
   ],
   "title": "An Investigation of Noise Robustness for Flow-Matching-Based Zero-Shot TTS",
   "original": "1336",
   "order": 139,
   "page_count": 5,
   "abstract": [
    "Recently, zero-shot text-to-speech (TTS) systems, capable of synthesizing any speaker’s voice from a short audio prompt, have made rapid advancements. However, the quality of the generated speech significantly deteriorates when the audio prompt contains noise, and limited research has been conducted to address this issue. In this paper, we explored various strategies to enhance the quality of audio generated from noisy audio prompts within the context of flow-matching-based zero-shot TTS. Our investigation includes comprehensive training strategies: unsupervised pre-training with masked speech denoising, multi-speaker detection and DNSMOS-based data filtering on the pre-training data, and fine-tuning with random noise mixing. The results of our experiments demonstrate significant improvements in intelligibility, speaker similarity, and overall audio quality compared to the approach of applying speech enhancement to the audio prompt."
   ],
   "p1": 687,
   "pn": 691,
   "doi": "10.21437/Interspeech.2024-1336",
   "url": "interspeech_2024/wang24v_interspeech.html"
  },
  "bott24_interspeech": {
   "authors": [
    [
     "Thomas",
     "Bott"
    ],
    [
     "Florian",
     "Lux"
    ],
    [
     "Ngoc Thang",
     "Vu"
    ]
   ],
   "title": "Controlling Emotion in Text-to-Speech with Natural Language Prompts",
   "original": "1337",
   "order": 368,
   "page_count": 5,
   "abstract": [
    "In recent years, prompting has quickly become one of the standard ways of steering the outputs of generative machine learning models, due to its intuitive use of natural language. In this work, we propose a system conditioned on embeddings derived from an emotionally rich text that serves as prompt. Thereby, a joint representation of speaker and prompt embeddings is integrated at several points within a transformer-based architecture. Our approach is trained on merged emotional speech and text datasets and varies prompts in each training iteration to increase the generalization capabilities of the model. Objective and subjective evaluation results demonstrate the ability of the conditioned synthesis system to accurately transfer the emotions present in a prompt to speech. At the same time, precise tractability of speaker identities as well as overall high speech quality and intelligibility are maintained."
   ],
   "p1": 1795,
   "pn": 1799,
   "doi": "10.21437/Interspeech.2024-1337",
   "url": "interspeech_2024/bott24_interspeech.html"
  },
  "yen24_interspeech": {
   "authors": [
    [
     "Hao",
     "Yen"
    ],
    [
     "Pin-Jui",
     "Ku"
    ],
    [
     "Sabato Marco",
     "Siniscalchi"
    ],
    [
     "Chin-Hui",
     "Lee"
    ]
   ],
   "title": "Language-Universal Speech Attributes Modeling for Zero-Shot Multilingual Spoken Keyword Recognition",
   "original": "1342",
   "order": 70,
   "page_count": 5,
   "abstract": [
    "We propose a novel language-universal approach to end-to-end automatic spoken keyword recognition (SKR) leveraging upon (i) a self-supervised pre-trained model, and (ii) a set of universal speech attributes (manner and place of articulation). Specifically, Wav2Vec2.0 is used to generate robust speech representations, followed by a linear output layer to produce attribute sequences. A non-trainable pronunciation model then maps sequences of attributes into spoken keywords in a multilingual setting. Experiments on the Multilingual Spoken Words Corpus show comparable performances to character- and phoneme-based SKR in seen languages. The inclusion of domain adversarial training (DAT) improves the proposed framework, outperforming both character- and phoneme-based SKR approaches with 13.73% and 17.22% relative word error rate (WER) reduction in seen languages, and achieves 32.14% and 19.92% WER reduction for unseen languages in zero-shot settings."
   ],
   "p1": 342,
   "pn": 346,
   "doi": "10.21437/Interspeech.2024-1342",
   "url": "interspeech_2024/yen24_interspeech.html"
  },
  "labrak24_interspeech": {
   "authors": [
    [
     "Yanis",
     "Labrak"
    ],
    [
     "Adel",
     "Moumen"
    ],
    [
     "Richard",
     "Dufour"
    ],
    [
     "Mickael",
     "Rouvier"
    ]
   ],
   "title": "Zero-Shot End-To-End Spoken Question Answering In Medical Domain",
   "original": "1344",
   "order": 413,
   "page_count": 5,
   "abstract": [
    "In the rapidly evolving landscape of spoken question-answering (SQA), the integration of large language models (LLMs) has emerged as a transformative development. Conventional approaches often entail the use of separate models for question audio transcription and answer selection, resulting in significant resource utilization and error accumulation. To tackle these challenges, we explore the effectiveness of end-to-end (E2E) methodologies for SQA in the medical domain. Our study introduces a novel zero-shot SQA approach, compared to traditional cascade systems. Through a comprehensive evaluation conducted on a new open benchmark of 8 medical tasks and 48 hours of synthetic audio, we demonstrate that our approach requires up to 14.7 times fewer resources than a combined 1.3B parameters LLM with a 1.55B parameters ASR model while improving average accuracy by 0.5%. These findings underscore the potential of E2E methodologies for SQA in resource-constrained contexts."
   ],
   "p1": 2020,
   "pn": 2024,
   "doi": "10.21437/Interspeech.2024-1344",
   "url": "interspeech_2024/labrak24_interspeech.html"
  },
  "jung24c_interspeech": {
   "authors": [
    [
     "Jee-weon",
     "Jung"
    ],
    [
     "Wangyou",
     "Zhang"
    ],
    [
     "Jiatong",
     "Shi"
    ],
    [
     "Zakaria",
     "Aldeneh"
    ],
    [
     "Takuya",
     "Higuchi"
    ],
    [
     "Alex",
     "Gichamba"
    ],
    [
     "Barry-John",
     "Theobald"
    ],
    [
     "Ahmed",
     "Hussen Abdelaziz"
    ],
    [
     "Shinji",
     "Watanabe"
    ]
   ],
   "title": "ESPnet-SPK: full pipeline speaker embedding toolkit with reproducible recipes, self-supervised front-ends, and off-the-shelf models",
   "original": "1345",
   "order": 880,
   "page_count": 5,
   "abstract": [
    "This paper introduces ESPnet-SPK, a toolkit designed for training and utilizing speaker embedding extractors. It offers an open-source platform, facilitating effortless construction of models ranging from the x-vector to the SKA-TDNN, thanks to its modular architecture that simplifies the development of variants. The toolkit advances the use of speaker embeddings across various tasks where outdated embeddings are often employed, enabling the broader research community to use advanced speaker embeddings effortlessly. Pre-trained extractors are readily available for off-the-shelf use. The toolkit also supports integration with various self-supervised learning features. ESPnet-SPK features over 30 recipes: seven speaker verification recipes, including reproducible WavLM-ECAPA with an EER of 0.39% on the Vox1-O benchmark and diverse downstream tasks, including text-to-speech and target speaker extraction. It even supports speaker similarity evaluation for singing voice synthesis and more."
   ],
   "p1": 4278,
   "pn": 4282,
   "doi": "10.21437/Interspeech.2024-1345",
   "url": "interspeech_2024/jung24c_interspeech.html"
  },
  "pan24b_interspeech": {
   "authors": [
    [
     "Jing",
     "Pan"
    ],
    [
     "Jian",
     "Wu"
    ],
    [
     "Yashesh",
     "Gaur"
    ],
    [
     "Sunit",
     "Sivasankaran"
    ],
    [
     "Zhuo",
     "Chen"
    ],
    [
     "Shujie",
     "Liu"
    ],
    [
     "Jinyu",
     "Li"
    ]
   ],
   "title": "COSMIC: Data Efficient Instruction-tuning For Speech In-Context Learning",
   "original": "1346",
   "order": 856,
   "page_count": 5,
   "abstract": [
    "We present a cost-effective method to integrate speech into a large language model (LLM), resulting in a Contextual Speech Model with Instruction-following/in-context-learning Capabilities (COSMIC) multi-modal LLM. Using GPT-3.5, we generate Speech Comprehension Test Question-Answer (SQA) pairs from speech transcriptions for supervised instruction tuning. With under 30 million trainable parameters and only 450 hours of English speech data, COSMIC demonstrates emerging capabilities in instruction-following and in-context learning. Equipped with such capabilities, COSMIC achieves a maximum 33.18 BLEU score in 0-shot EN-to-X speech to text translation (S2TT) and a significant boost in the 1-shot setting. Additionally, there is an average 25.8% relative Word Error Rate (WER) reduction for 1-shot cross-domain adaptation. COSMIC exhibits a significant automatic speech recognition (ASR) accuracy gain in contextual biasing tasks due to its instruction-following capability."
   ],
   "p1": 4164,
   "pn": 4168,
   "doi": "10.21437/Interspeech.2024-1346",
   "url": "interspeech_2024/pan24b_interspeech.html"
  },
  "wang24w_interspeech": {
   "authors": [
    [
     "Weiran",
     "Wang"
    ],
    [
     "Zelin",
     "Wu"
    ],
    [
     "Diamantino",
     "Caseiro"
    ],
    [
     "Tsendsuren",
     "Munkhdalai"
    ],
    [
     "Khe Chai",
     "Sim"
    ],
    [
     "Pat",
     "Rondon"
    ],
    [
     "Golan",
     "Pundak"
    ],
    [
     "Gan",
     "Song"
    ],
    [
     "Rohit",
     "Prabhavalkar"
    ],
    [
     "Zhong",
     "Meng"
    ],
    [
     "Ding",
     "Zhao"
    ],
    [
     "Tara",
     "Sainath"
    ],
    [
     "Yanzhang",
     "He"
    ],
    [
     "Pedro",
     "Moreno Mengibar"
    ]
   ],
   "title": "Contextual Biasing with the Knuth-Morris-Pratt Matching Algorithm",
   "original": "1349",
   "order": 58,
   "page_count": 5,
   "abstract": [
    "We propose a GPU/TPU-friendly implementation for contextual biasing based on the Knuth-Morris-Pratt (KMP) pattern matching algorithm. Our algorithms simulate classical search-based biasing approaches which are often implemented in the weighted finite state transducer (WFST) framework, with careful considerations on memory footprint and efficiency by vectorization. We design scoring mechanisms such that, during beam search, a token extension receives a bonus if it extends matching into a biasing phrase, and receives a penalty to cancel previously received bonus otherwise. Our methods could be incorporated in either the shallow fusion or on-the-fly rescoring manner, to trade off accuracy with efficiency. On a large-scale voice search dataset, our method achieves significant word error rate (WER) reductions on biasing test sets without introducing additional model parameters, and yields further performance gain when combined with a model-based biasing method."
   ],
   "p1": 282,
   "pn": 286,
   "doi": "10.21437/Interspeech.2024-1349",
   "url": "interspeech_2024/wang24w_interspeech.html"
  },
  "feng24b_interspeech": {
   "authors": [
    [
     "Tiantian",
     "Feng"
    ],
    [
     "Dimitrios",
     "Dimitriadis"
    ],
    [
     "Shrikanth S.",
     "Narayanan"
    ]
   ],
   "title": "Can Synthetic Audio From Generative Foundation Models Assist Audio Recognition and Speech Modeling?",
   "original": "1350",
   "order": 110,
   "page_count": 5,
   "abstract": [
    "Recent advances in foundation models have enabled audio-generative models that produce high-fidelity sounds associated with music, events, and human actions. Despite the success achieved in modern audio-generative models, the conventional approach to assessing the quality of the audio generation relies heavily on distance metrics like Frechet Audio Distance. In contrast, we aim to evaluate the quality of audio generation by examining the effectiveness of using them as training data. Specifically, we conduct studies to explore the use of synthetic audio for audio recognition. Moreover, we investigate whether synthetic audio can serve as a resource for data augmentation in speech-related modeling. Our comprehensive experiments demonstrate the potential of using synthetic audio for audio recognition and speech-related modeling. Our code is available at https://github.com/usc-sail/SynthAudio."
   ],
   "p1": 542,
   "pn": 546,
   "doi": "10.21437/Interspeech.2024-1350",
   "url": "interspeech_2024/feng24b_interspeech.html"
  },
  "anderson24_interspeech": {
   "authors": [
    [
     "Grant",
     "Anderson"
    ],
    [
     "Emma",
     "Hart"
    ],
    [
     "Dimitra",
     "Gkatzia"
    ],
    [
     "Ian",
     "Beaver"
    ]
   ],
   "title": "Automated Human-Readable Label Generation in Open Intent Discovery",
   "original": "1351",
   "order": 724,
   "page_count": 5,
   "abstract": [
    "The correct determination of user intent is key in dialog systems. However, an intent classifier often requires a large, labelled training dataset to identify a set of known intents. The creation of such a dataset is a complex and time-consuming task which usually involves humans applying clustering tools to unlabelled data, analysing the results, and creating human-readable labels for each cluster. While many Open Intent Discovery works tackle the problem of discovering clusters of common intent, few generate a human-readable label that can be used to make decisions in downstream systems. To address this, we introduce a novel candidate label extraction method then evaluate six combinations of candidate extraction and label selection methods on three datasets. We find that our extraction method produces more detailed labels than the alternatives and that high quality intent labels can be generated from unlabelled data without resorting to applying costly pre-trained language models."
   ],
   "p1": 3540,
   "pn": 3544,
   "doi": "10.21437/Interspeech.2024-1351",
   "url": "interspeech_2024/anderson24_interspeech.html"
  },
  "barreraaltuna24_interspeech": {
   "authors": [
    [
     "Benjamin",
     "Barrera-Altuna"
    ],
    [
     "Daeun",
     "Lee"
    ],
    [
     "Zaima",
     "Zarnaz"
    ],
    [
     "Jinyoung",
     "Han"
    ],
    [
     "Seungbae",
     "Kim"
    ]
   ],
   "title": "The Interspeech 2024 TAUKADIAL Challenge: Multilingual Mild Cognitive Impairment Detection with Multimodal Approach",
   "original": "1352",
   "order": 195,
   "page_count": 5,
   "abstract": [
    "Mild cognitive impairment (MCI) and dementia significantly impact millions worldwide and rank as a major cause of mortality. Since traditional diagnostic methods are often costly and result in delayed diagnoses, many efforts have been made to propose automatic detection approaches. However, most methods focus on monolingual cases, limiting the scalability of their models to individuals speaking different languages. To understand the common characteristics of people with MCI speaking different languages, we propose a multilingual MCI detection model using multimodal approaches that analyze both acoustic and linguistic features. It outperforms existing machine learning models by identifying universal MCI indicators across languages. Particularly, we find that speech duration and pauses are crucial in detecting MCI in multilingual settings. Our findings can potentially facilitate early intervention in cognitive decline across diverse linguistic backgrounds."
   ],
   "p1": 967,
   "pn": 971,
   "doi": "10.21437/Interspeech.2024-1352",
   "url": "interspeech_2024/barreraaltuna24_interspeech.html"
  },
  "fan24b_interspeech": {
   "authors": [
    [
     "Ruchao",
     "Fan"
    ],
    [
     "Natarajan",
     "Balaji Shankar"
    ],
    [
     "Abeer",
     "Alwan"
    ]
   ],
   "title": "Benchmarking Children's ASR with Supervised and Self-supervised Speech Foundation Models",
   "original": "1353",
   "order": 1059,
   "page_count": 5,
   "abstract": [
    "Speech foundation models (SFMs) have achieved state-of- the-art results for various speech tasks in supervised (e.g. Whisper) or self-supervised systems (e.g. WavLM). However, the performance of SFMs for child ASR has not been systematically studied. In addition, there is no benchmark for child ASR with standard evaluations, making the comparisons of novel ideas difficult. In this paper, we initiate and present a comprehensive benchmark on several child speech databases based on various SFMs (Whisper, Wav2vec2.0, HuBERT, and WavLM). Moreover, we investigate finetuning strategies by comparing various data augmentation and parameter-efficient finetuning (PEFT) methods. We observe that the behaviors of these methods are different when the model size increases. For example, PEFT matches the performance of full finetuning for large models but worse for small models. To stabilize finetuning using augmented data, we propose a perturbation invariant finetuning (PIF) loss as a regularization."
   ],
   "p1": 5173,
   "pn": 5177,
   "doi": "10.21437/Interspeech.2024-1353",
   "url": "interspeech_2024/fan24b_interspeech.html"
  },
  "jung24d_interspeech": {
   "authors": [
    [
     "Jee-weon",
     "Jung"
    ],
    [
     "Xin",
     "Wang"
    ],
    [
     "Nicholas",
     "Evans"
    ],
    [
     "Shinji",
     "Watanabe"
    ],
    [
     "Hye-jin",
     "Shim"
    ],
    [
     "Hemlata",
     "Tak"
    ],
    [
     "Siddhant",
     "Arora"
    ],
    [
     "Junichi",
     "Yamagishi"
    ],
    [
     "Joon Son",
     "Chung"
    ]
   ],
   "title": "To what extent can ASV systems naturally defend against spoofing attacks?",
   "original": "1354",
   "order": 664,
   "page_count": 5,
   "abstract": [
    "The current automatic speaker verification (ASV) task involves making binary decisions on two types of trials: target and non-target. However, emerging advancements in speech generation technology pose significant threats to the reliability of ASV systems. This study investigates whether ASV effortlessly acquires robustness against spoofing attacks (i.e., zero-shot capability) by systematically exploring diverse ASV systems and spoofing attacks, ranging from traditional to cutting-edge techniques. Through extensive analyses conducted on eight distinct ASV systems and 29 spoofing attack systems, we demonstrate that the evolution of ASV inherently incorporates defense mechanisms against spoofing attacks. Nevertheless, our findings also underscore that the advancement of spoofing attacks far outpaces that of ASV systems, hence necessitating further research on spoofing-robust ASV methodologies."
   ],
   "p1": 3240,
   "pn": 3244,
   "doi": "10.21437/Interspeech.2024-1354",
   "url": "interspeech_2024/jung24d_interspeech.html"
  },
  "ni24_interspeech": {
   "authors": [
    [
     "Ye",
     "Ni"
    ],
    [
     "Cong",
     "Pang"
    ],
    [
     "Chengwei",
     "Huang"
    ],
    [
     "Cairong",
     "Zou"
    ]
   ],
   "title": "MSA-DPCRN: A Multi-Scale Asymmetric Dual-Path Convolution Recurrent Network with Attentional Feature Fusion for Acoustic Echo Cancellation",
   "original": "1355",
   "order": 34,
   "page_count": 5,
   "abstract": [
    "Echo cancellation plays a crucial role in modern speech applications. Numerous deep-learning models have been developed for the echo cancellation task and achieved great progress by incorporating additional features; however, the majority of these models overlook the characteristics of different features and simply merge them along the channel dimension. In this paper, we proposed a multi-scale asymmetric dual-path convolution recurrent network (MSA-DPCRN) consisting of two asymmetric encoding paths to extract spectrum and relevant features from the input reference and microphone signals. Moreover, we propose a frequency-wise attentional feature fusion (AFF) method to fuse the two features while maintaining the original dynamic range. The experiments validate the effectiveness of each component in MSA-DPCRN and indicate that our model outperforms the AEC challenge baseline in terms of the Echo-MOS metrics."
   ],
   "p1": 162,
   "pn": 166,
   "doi": "10.21437/Interspeech.2024-1355",
   "url": "interspeech_2024/ni24_interspeech.html"
  },
  "ma24c_interspeech": {
   "authors": [
    [
     "Min",
     "Ma"
    ],
    [
     "Yuma",
     "Koizumi"
    ],
    [
     "Shigeki",
     "Karita"
    ],
    [
     "Heiga",
     "Zen"
    ],
    [
     "Jason",
     "Riesa"
    ],
    [
     "Haruko",
     "Ishikawa"
    ],
    [
     "Michiel",
     "Bacchiani"
    ]
   ],
   "title": "FLEURS-R: A Restored Multilingual Speech Corpus for Generation Tasks",
   "original": "1356",
   "order": 376,
   "page_count": 5,
   "abstract": [
    "This paper introduces FLEURS-R, a speech restoration applied version of the Few-shot Learning Evaluation of Universal Representations of Speech (FLEURS) corpus. FLEURS-R maintains an N-way parallel speech corpus in 102 languages as FLEURS, with improved audio quality and fidelity by applying the speech restoration model Miipher. The aim of FLEURS-R is to advance speech technology in more languages and catalyze research in- cluding text-to-speech (TTS) and other speech generation tasks in low-resource languages. Comprehensive evaluations with the restored speech and TTS baseline models trained from the new corpus show that the new corpus obtained significantly improved speech quality while maintaining the semantic contents of the speech. The corpus is publicly released via Hugging Face."
   ],
   "p1": 1835,
   "pn": 1839,
   "doi": "10.21437/Interspeech.2024-1356",
   "url": "interspeech_2024/ma24c_interspeech.html"
  },
  "demopoulos24_interspeech": {
   "authors": [
    [
     "Carly",
     "Demopoulos"
    ],
    [
     "Linnea",
     "Lampinen"
    ],
    [
     "Cristian",
     "Preciado"
    ],
    [
     "Hardik",
     "Kothare"
    ],
    [
     "Vikram",
     "Ramanarayanan"
    ]
   ],
   "title": "Preliminary Investigation of Psychometric Properties of a Novel Multimodal Dialog Based Affect Production Task in Children and Adolescents with Autism",
   "original": "1359",
   "order": 1049,
   "page_count": 5,
   "abstract": [
    "Impairments in nonverbal communication are a defining feature of autism spectrum disorder (ASD) and can manifest as difficulty with, or even complete lack of, communication of emotional states via production of facial affect or vocal affect. The purpose of this study was to evaluate psychometric properties of a novel multimodal dialog based Affect Production Task (APT) in children and adolescents (ages 8-17) with a diagnosis of autism (N=72) or neurotypical controls (N=37). Participants completed activities designed to quantify objective facial and vocal affect production ability using audiovisual capture. Criterion, ecological, and discriminant validity were assessed. Psychometric performance across task conditions, age, sex, and race-ethnicity also was examined. Results of this initial psychometric evaluation suggest that the APT is a valid measure of affect production abilities in children and adolescents, and that psychometric performance is invariant to age, sex, or race/ethnicity."
   ],
   "p1": 5123,
   "pn": 5127,
   "doi": "10.21437/Interspeech.2024-1359",
   "url": "interspeech_2024/demopoulos24_interspeech.html"
  },
  "wang24x_interspeech": {
   "authors": [
    [
     "Shiyao",
     "Wang"
    ],
    [
     "Shiwan",
     "Zhao"
    ],
    [
     "Jiaming",
     "Zhou"
    ],
    [
     "Aobo",
     "Kong"
    ],
    [
     "Yong",
     "Qin"
    ]
   ],
   "title": "Enhancing Dysarthric Speech Recognition for Unseen Speakers via Prototype-Based Adaptation",
   "original": "1360",
   "order": 270,
   "page_count": 5,
   "abstract": [
    "Dysarthric speech recognition (DSR) presents a formidable challenge due to inherent inter-speaker variability, leading to severe performance degradation when applying DSR models to new dysarthric speakers. Traditional speaker adaptation methodologies typically involve fine-tuning models for each speaker, but this strategy is cost-prohibitive and inconvenient for disabled users, requiring substantial data collection. To address this issue, we introduce a prototype-based approach that markedly improves DSR performance for unseen dysarthric speakers without additional fine-tuning. Our method employs a feature extractor trained with HuBERT to produce per-word prototypes that encapsulate the characteristics of previously unseen speakers. These prototypes serve as the basis for classification. Additionally, we incorporate supervised contrastive learning to refine feature extraction. By enhancing representation quality, we further improve DSR performance, enabling effective personalized DSR."
   ],
   "p1": 1305,
   "pn": 1309,
   "doi": "10.21437/Interspeech.2024-1360",
   "url": "interspeech_2024/wang24x_interspeech.html"
  },
  "palaskar24_interspeech": {
   "authors": [
    [
     "Shruti",
     "Palaskar"
    ],
    [
     "Ognjen",
     "Rudovic"
    ],
    [
     "Sameer",
     "Dharur"
    ],
    [
     "Florian",
     "Pesce"
    ],
    [
     "Gautam",
     "Krishna"
    ],
    [
     "Aswin",
     "Sivaraman"
    ],
    [
     "Jack",
     "Berkowitz"
    ],
    [
     "Ahmed",
     "Hussen Abdelaziz"
    ],
    [
     "Saurabh",
     "Adya"
    ],
    [
     "Ahmed",
     "Tewfik"
    ]
   ],
   "title": "Multimodal Large Language Models with Fusion Low Rank Adaptation for Device Directed Speech Detection",
   "original": "1361",
   "order": 980,
   "page_count": 5,
   "abstract": [
    "Although Large Language Models (LLMs) have shown promise for human-like conversations, they are primarily pre-trained on text data. Incorporating audio or video improves performance, but collecting large-scale multimodal data and pre-training multimodal LLMs is challenging. To this end, we propose a Fusion Low Rank Adaptation (FLoRA) technique that efficiently adapts a pre-trained unimodal LLM to consume new, previously unseen modalities via low rank adaptation. For device-directed speech detection, using FLoRA, the multimodal LLM achieves 22% relative reduction in equal error rate (EER) over the text-only approach and attains performance parity with its full fine-tuning (FFT) counterpart while needing to tune only a fraction of its parameters. Furthermore, with the newly introduced adapter dropout, FLoRA is robust to missing data, improving over FFT by 20% lower EER and 56% lower false accept rate. The proposed approach scales well for model sizes from 16M to 3B parameters."
   ],
   "p1": 4778,
   "pn": 4782,
   "doi": "10.21437/Interspeech.2024-1361",
   "url": "interspeech_2024/palaskar24_interspeech.html"
  },
  "zhang24j_interspeech": {
   "authors": [
    [
     "Lin",
     "Zhang"
    ],
    [
     "Xin",
     "Wang"
    ],
    [
     "Erica",
     "Cooper"
    ],
    [
     "Mireia",
     "Diez"
    ],
    [
     "Federico",
     "Landini"
    ],
    [
     "Nicholas",
     "Evans"
    ],
    [
     "Junichi",
     "Yamagishi"
    ]
   ],
   "title": "Spoof Diarization: &quot;What Spoofed When&quot; in Partially Spoofed Audio",
   "original": "1365",
   "order": 102,
   "page_count": 5,
   "abstract": [
    "This paper defines Spoof Diarization as a novel task in the Partial Spoof (PS) scenario. It aims to determine what spoofed when, which includes not only locating spoof regions but also clustering them according to different spoofing methods. As a pioneering study in spoof diarization, we focus on defining the task, establishing evaluation metrics, and proposing a bench- mark model, namely the Countermeasure-Condition Clustering (3C) model. Utilizing this model, we first explore how to effectively train countermeasures to support spoof diarization using three labeling schemes. We then utilize spoof localization predictions to enhance the diarization performance. This first study reveals the high complexity of the task, even in restricted scenarios where only a single speaker per audio file and an oracle number of spoofing methods are considered. Our code is available at https://github.com/ nii-yamagishilab/PartialSpoof."
   ],
   "p1": 502,
   "pn": 506,
   "doi": "10.21437/Interspeech.2024-1365",
   "url": "interspeech_2024/zhang24j_interspeech.html"
  },
  "zhang24k_interspeech": {
   "authors": [
    [
     "Zehua",
     "Zhang"
    ],
    [
     "Xuyi",
     "Zhuang"
    ],
    [
     "Yukun",
     "Qian"
    ],
    [
     "Mingjiang",
     "Wang"
    ]
   ],
   "title": "Lightweight Dynamic Sparse Transformer for Monaural Speech Enhancement",
   "original": "1368",
   "order": 135,
   "page_count": 5,
   "abstract": [
    "Speech enhancement can effectively suppress environmental noise and improve the intelligibility of speech signals, which is a key task in the front-end processing of speech signals. We propose a monaural speech enhancement model called the lightweight dynamic sparse Transformer (LDSTransformer). From the complementarity perspective, we propose a dual branch structure combining coarse and fine branches. The coarse branch and the fine branch estimate the magnitude spectrum and the complex spectrum, respectively. Both branches share an innovative lightweight dynamic sparse Transformer block (LDSTB), which can efficiently extract deep time-frequency features. Furthermore, we propose a novel deep feature aggregation block to aggregate the deep features extracted by the LDSTBs. On the 1st Deep Noise Suppression Challenge blind test set, in environments with reverberation, our proposed model achieves an average improvement of 2.05 WB-PESQ, 10.05% STOI, and 10.65 SI-SDR."
   ],
   "p1": 667,
   "pn": 671,
   "doi": "10.21437/Interspeech.2024-1368",
   "url": "interspeech_2024/zhang24k_interspeech.html"
  },
  "tomita24_interspeech": {
   "authors": [
    [
     "Yu",
     "Tomita"
    ],
    [
     "Yingxiang",
     "Gao"
    ],
    [
     "Nobuaki",
     "Minematsu"
    ],
    [
     "Noriko",
     "Nakanishi"
    ],
    [
     "Daisuke",
     "Saito"
    ]
   ],
   "title": "Analysis and Visualization of Directional Diversity in Listening Fluency of World Englishes Speakers in the Framework of Mutual Shadowing",
   "original": "1373",
   "order": 828,
   "page_count": 5,
   "abstract": [
    "English is spoken as a lingua franca with a diversity of pronunciations, called accents, and they have been well studied so far. In this study, a diversity of listening behaviors are focused on, and listening disfluencies are measured objectively while listening to World Englishes (WE). When speaker X listens to Y fluently, it does not always mean that Y listens to X fluently. After collecting different passages read aloud by different WE speakers, the collected oral passages are shadowed by the speakers themselves to quantify their listening disfluencies. Results show that, when X listens to Y, X's listening disfluency becomes larger when Y's pronunciation deviates from X's to a larger degree. Further, a method is proposed to visualize simultaneously a) how fluently a speaker listens to WE speakers and b) how fluently the WE speakers listen to that specific speaker. With this visualization, WE speakers are grouped based on their communicability in global contexts."
   ],
   "p1": 4024,
   "pn": 4028,
   "doi": "10.21437/Interspeech.2024-1373",
   "url": "interspeech_2024/tomita24_interspeech.html"
  },
  "xi24_interspeech": {
   "authors": [
    [
     "Yuxuan",
     "Xi"
    ],
    [
     "Yan",
     "Song"
    ],
    [
     "Lirong",
     "Dai"
    ],
    [
     "Haoyu",
     "Song"
    ],
    [
     "Ian",
     "McLoughlin"
    ]
   ],
   "title": "An Effective Local Prototypical Mapping Network for Speech Emotion Recognition",
   "original": "1374",
   "order": 220,
   "page_count": 5,
   "abstract": [
    "Speech emotion recognition (SER) systems are generally optimized through utterance-level supervision, but emotion is complex and often varies within an utterance. This paper propose a local prototypical mapping network (LPMN) to model frame-level emotional variance and better exploit within-frame dynamics to improve performance. Specifically, a codebook of prototypes is first constructed to characterize complex frame-level features output from a pre-trained backbone network. An utterance-level embedding is obtained by selecting the most emotion-related mappings via a similarity measure between features and prototypes, motivated by multiple instance learning algorithms. Prototypes can be jointly optimized with quantization loss and CE loss. A prototype selection scheme is further proposed to select emotion-aware prototypes to reduce bias caused by irrelevant factors. Evaluations on IEMOCAP and MER2023 benchmarks demonstrate the effectiveness of LPMN."
   ],
   "p1": 1055,
   "pn": 1059,
   "doi": "10.21437/Interspeech.2024-1374",
   "url": "interspeech_2024/xi24_interspeech.html"
  },
  "liu24j_interspeech": {
   "authors": [
    [
     "Yun",
     "Liu"
    ],
    [
     "Xuechen",
     "Liu"
    ],
    [
     "Xiaoxiao",
     "Miao"
    ],
    [
     "Junichi",
     "Yamagishi"
    ]
   ],
   "title": "Target Speaker Extraction with Curriculum Learning",
   "original": "1375",
   "order": 894,
   "page_count": 5,
   "abstract": [
    "This paper presents a novel approach to target speaker extraction\n(TSE) using Curriculum Learning (CL) techniques, addressing\nthe challenge of distinguishing a target speaker’s voice\nfrom a mixture containing interfering speakers. For efficient\ntraining, we propose designing a curriculum that selects subsets\nof increasing complexity, such as increasing similarity between\ntarget and interfering speakers, and that selects training data\nstrategically. Our CL strategies include both variants using predefined\ndifficulty measures (e.g. gender, speaker similarity, and\nsignal-to-distortion ratio) and ones using the TSE’s standard objective\nfunction, each designed to expose the model gradually\nto more challenging scenarios. Comprehensive testing on the\nLibri2talker dataset demonstrated that our CL strategies for TSE\nimproved the performance, and the results markedly exceeded\nbaseline models without CL about 1 dB."
   ],
   "p1": 4348,
   "pn": 4352,
   "doi": "10.21437/Interspeech.2024-1375",
   "url": "interspeech_2024/liu24j_interspeech.html"
  },
  "kim24m_interspeech": {
   "authors": [
    [
     "Jihyun",
     "Kim"
    ],
    [
     "Stijn",
     "Kindt"
    ],
    [
     "Nilesh",
     "Madhu"
    ],
    [
     "Hong-Goo",
     "Kang"
    ]
   ],
   "title": "Enhanced Deep Speech Separation in Clustered Ad Hoc Distributed Microphone Environments",
   "original": "1378",
   "order": 452,
   "page_count": 5,
   "abstract": [
    "Ad-hoc distributed microphone environments, where microphone locations and numbers are unpredictable, present a challenge to traditional deep learning models, which typically require fixed architectures. To tailor deep learning models to accommodate arbitrary array configurations, the Transform-Average-Concatenate (TAC) layer was previously introduced. In this work, we integrate TAC layers with dual-path transformers for speech separation from two simultaneous talkers in realistic settings. However, the distributed nature makes it hard to fuse information across microphones efficiently. Therefore, we explore the efficacy of blindly clustering microphones around sources of interest prior to enhancement. Experimental results show that this deep cluster-informed approach significantly improves the system's capacity to cope with the inherent variability observed in ad-hoc distributed microphone environments."
   ],
   "p1": 2185,
   "pn": 2189,
   "doi": "10.21437/Interspeech.2024-1378",
   "url": "interspeech_2024/kim24m_interspeech.html"
  },
  "gholami24_interspeech": {
   "authors": [
    [
     "Behnam",
     "Gholami"
    ],
    [
     "Mostafa",
     "El-Khamy"
    ],
    [
     "KeeBong",
     "Song"
    ]
   ],
   "title": "Knowledge Distillation for Tiny Speech Enhancement with Latent Feature Augmentation",
   "original": "1383",
   "order": 132,
   "page_count": 5,
   "abstract": [
    "Recent deep neural network (DNN) models have achieved high performance in speech enhancement. However, deploying such complex models in resource-constrained environments can be challenging without significant performance degradation. Knowledge distillation (KD), a technique where a smaller (student) model is trained to mimic the behavior of a larger, more complex (teacher) model, has emerged as a popular approach to address this challenge.  In this paper, we propose a feature-augmentation based knowledge distillation method for speech enhancement, leveraging the information stored in the intermediate latent features of the DNN teacher model to train a smaller, more efficient student model. Experimental results on VoiceBank+DEMAND dataset demonstrate the effectiveness of the proposed knowledge distillation method for speech enhancement."
   ],
   "p1": 652,
   "pn": 656,
   "doi": "10.21437/Interspeech.2024-1383",
   "url": "interspeech_2024/gholami24_interspeech.html"
  },
  "kothare24_interspeech": {
   "authors": [
    [
     "Hardik",
     "Kothare"
    ],
    [
     "Michael",
     "Neumann"
    ],
    [
     "Cathy",
     "Zhang"
    ],
    [
     "Jackson",
     "Liscombe"
    ],
    [
     "Jordi W J",
     "van Unnik"
    ],
    [
     "Lianne C M",
     "Botman"
    ],
    [
     "Leonard H",
     "van den Berg"
    ],
    [
     "Ruben P A",
     "van Eijk"
    ],
    [
     "Vikram",
     "Ramanarayanan"
    ]
   ],
   "title": "How Consistent are Speech-Based Biomarkers in Remote Tracking of ALS Disease Progression Across Languages? A Case Study of English and Dutch",
   "original": "1390",
   "order": 410,
   "page_count": 5,
   "abstract": [
    "Previous work has demonstrated the utility of speech-based digital biomarkers for remotely tracking longitudinal progression in people with Amyotrophic Lateral Sclerosis (pALS). Here, we investigate the responsiveness of these biomarkers across languages for consistency. We collected audiovisual data using a cloud-based multimodal dialogue platform, where pALS interacted with a virtual guide to perform several speaking exercises. We automatically extracted speech, linguistic and orofacial metrics from 143 English-speaking pALS (36 bulbar onset, 107 non-bulbar onset) and 26 Dutch-speaking pALS (10 bulbar, 16 non-bulbar onset). We used growth curve models to estimate the trajectory of these metrics over time. We observe that for most of these metrics, English-speaking pALS and Dutch-speaking pALS follow similar trajectories, i.e. the slopes are not statistically different from each other, demonstrating the potential of such speech-based biomarkers for remote monitoring across languages."
   ],
   "p1": 2005,
   "pn": 2009,
   "doi": "10.21437/Interspeech.2024-1390",
   "url": "interspeech_2024/kothare24_interspeech.html"
  },
  "yang24l_interspeech": {
   "authors": [
    [
     "Dongchao",
     "Yang"
    ],
    [
     "Dingdong",
     "Wang"
    ],
    [
     "Haohan",
     "Guo"
    ],
    [
     "Xueyuan",
     "Chen"
    ],
    [
     "Xixin",
     "Wu"
    ],
    [
     "Helen",
     "Meng"
    ]
   ],
   "title": "SimpleSpeech: Towards Simple and Efficient Text-to-Speech with Scalar Latent Transformer Diffusion Models",
   "original": "1392",
   "order": 904,
   "page_count": 5,
   "abstract": [
    "In this study, we propose a simple and efficient Non-\nAutoregressive (NAR) text-to-speech (TTS) system based on\ndiffusion, named SimpleSpeech. Its simpleness shows in three\naspects: (1) It can be trained on the speech-only dataset, without\nany alignment information; (2) It directly takes plain text as\ninput and generates speech through an NAR way; (3) It tries to\nmodel speech in a finite and compact latent space, which alleviates \nthe modeling difficulty of diffusion. More specifically, we\npropose a novel speech codec model (SQ-Codec) with scalar\nquantization, SQ-Codec effectively maps the complex speech\n signal into a finite and compact latent space, named scalar latent\nspace. Benefits from SQ-Codec, we apply a novel transformer \ndiffusion model in the scalar latent space of SQ-Codec.\nWe train SimpleSpeech on 4k hours of a speech-only dataset,\nit shows natural prosody and voice cloning ability. Compared\n with previous large-scale TTS models, it presents significant\n speech quality and generation speed improvement. Demos are\n released."
   ],
   "p1": 4398,
   "pn": 4402,
   "doi": "10.21437/Interspeech.2024-1392",
   "url": "interspeech_2024/yang24l_interspeech.html"
  },
  "xiaowang24_interspeech": {
   "authors": [
    [
     "Liu",
     "Xiaowang"
    ],
    [
     "Jinsong",
     "Zhang"
    ]
   ],
   "title": "A Study on the Information Mechanism of the 3rd Tone Sandhi Rule in Mandarin Disyllabic Words",
   "original": "1393",
   "order": 321,
   "page_count": 5,
   "abstract": [
    "Tone plays a crucial role in distinguishing lexical meanings in\nMandarin. In the 3rd tone sandhi, whether and to what extent\ndoes the tonal shift from the T3 to the T2 causes semantic\nconfusion? Previous studies from acoustic and perceptual \nperspectives have not yielded a definitive conclusion. This\npaper attempts to investigate the information mechanism\nbehind the phenomenon of the 3rd tone sandhi from the \nperspective of the linguistic communicative function. We\n examine the number of lexical minimal pairs, lexical similarity,\nand sentence information loss after sandhi at both lexical and\nsentence levels. The results reveal that at the lexical level, the\nnumber of minimal pairs formed between T3T3 and T2T3 in\ndictionaries of different sizes is the smallest; at the sentence\nlevel, sentences with no information loss are most numerous\n when T3 is confused with T2, and both the average and ranking\nof information loss are the lowest. These indicate that the \ncommunicative risk caused by confusing T3 and T2 was small.\nThese findings provide a new perspective on the mechanism of\nthe 3rd tone sandhi."
   ],
   "p1": 1560,
   "pn": 1564,
   "doi": "10.21437/Interspeech.2024-1393",
   "url": "interspeech_2024/xiaowang24_interspeech.html"
  },
  "ahn24c_interspeech": {
   "authors": [
    [
     "Byeongjoo",
     "Ahn"
    ],
    [
     "Karren",
     "Yang"
    ],
    [
     "Brian",
     "Hamilton"
    ],
    [
     "Jonathan",
     "Sheaffer"
    ],
    [
     "Anurag",
     "Ranjan"
    ],
    [
     "Miguel",
     "Sarabia"
    ],
    [
     "Oncel",
     "Tuzel"
    ],
    [
     "Jen-Hao Rick",
     "Chang"
    ]
   ],
   "title": "Novel-view Acoustic Synthesis From 3D Reconstructed Rooms",
   "original": "1396",
   "order": 668,
   "page_count": 5,
   "abstract": [
    "We investigate the benefit of combining blind audio recordings with 3D scene information for novel-view acoustic synthesis. Given audio recordings from 2–4 microphones and the 3D geometry and material of a scene containing multiple unknown sound sources, we estimate the sound anywhere in the scene. We identify the main challenges of novel-view acoustic synthesis as sound source localization, separation, and dereverberation. While naively training an end-to-end network fails to produce high-quality results, we show that incorporating room impulse responses (RIRs) derived from 3D reconstructed rooms enables the same network to jointly tackle these tasks. Our method outperforms existing methods designed for the individual tasks, demonstrating its effectiveness at utilizing 3D visual information. In a simulated study on the Matterport3D-NVAS dataset, our model achieves near-perfect accuracy on source localization, a PSNR of 26.44 dB and a SDR of 14.23 dB for source separation and dereverberation, resulting in a PSNR of 25.55 dB and a SDR of 14.20 dB on novel-view acoustic synthesis. We release our code and model on our project website at https://github.com/apple/ml-nvas3d. Please wear headphones when listening to the results."
   ],
   "p1": 3260,
   "pn": 3264,
   "doi": "10.21437/Interspeech.2024-1396",
   "url": "interspeech_2024/ahn24c_interspeech.html"
  },
  "halpern24_interspeech": {
   "authors": [
    [
     "Bence Mark",
     "Halpern"
    ],
    [
     "Thomas",
     "Tienkamp"
    ],
    [
     "Wen-Chin",
     "Huang"
    ],
    [
     "Lester Phillip",
     "Violeta"
    ],
    [
     "Teja",
     "Rebernik"
    ],
    [
     "Sebastiaan",
     "de Visscher"
    ],
    [
     "Max",
     "Witjes"
    ],
    [
     "Martijn",
     "Wieling"
    ],
    [
     "Defne",
     "Abur"
    ],
    [
     "Tomoki",
     "Toda"
    ]
   ],
   "title": "Quantifying the effect of speech pathology on automatic and human speaker verification",
   "original": "1400",
   "order": 619,
   "page_count": 5,
   "abstract": [
    "This study investigates how surgical intervention for speech pathology (specifically, as a result of oral cancer surgery) impacts the performance of an automatic speaker verification (ASV) system. Using two recently collected Dutch datasets with parallel pre and post-surgery audio from the same speaker, NKI-OC-VC and SPOKE, we assess the extent to which speech pathology influences ASV performance, and whether objective/subjective measures of speech severity are correlated with the performance. Finally, we carry out a perceptual study to compare judgements of ASV and human listeners. Our findings reveal that pathological speech negatively affects ASV performance, and the severity of the speech is negatively correlated with the performance. There is a moderate agreement in perceptual and objective scores of speaker similarity and severity, however, we could not clearly establish in the perceptual study, whether the same phenomenon also exists in human perception."
   ],
   "p1": 3015,
   "pn": 3019,
   "doi": "10.21437/Interspeech.2024-1400",
   "url": "interspeech_2024/halpern24_interspeech.html"
  },
  "huang24g_interspeech": {
   "authors": [
    [
     "Xinghao",
     "Huang"
    ],
    [
     "Weiwei",
     "Jiang"
    ],
    [
     "Long",
     "Rao"
    ],
    [
     "Wei",
     "Xu"
    ],
    [
     "Wenqing",
     "Cheng"
    ]
   ],
   "title": "Active Speaker Detection in Fisheye Meeting Scenes with Scene Spatial Spectrums",
   "original": "1402",
   "order": 881,
   "page_count": 5,
   "abstract": [
    "Active Speaker Detection (ASD) plays a crucial role in scene understanding tasks by determining whether an on-screen person in a given scene is speaking. In this work, to address the ASD in the context of multi-party roundtable meetings, we propose a novel approach that incorporates the fusion of spatial information of the scenes. To leverage the multiple data sources of the scenes, our method involves generating audio spatial spectrum heatmaps from the multi-channel audio and integrating them with the panoramic images. Additionally, we propose the novel FisheyeMeeting dataset, which combines fisheye panoramic video recordings with muti-channel audio captured from a six-channel circular microphone array. By enabling the multi-modal model to capture audio-visual cues in multi-party meeting scenes, our approach achieves an impressive 89.11% mAP on the FisheyeMeeting dataset. Notably, this outperforms the current SOTA methods by a significant 2.3% mAP improvement."
   ],
   "p1": 4283,
   "pn": 4287,
   "doi": "10.21437/Interspeech.2024-1402",
   "url": "interspeech_2024/huang24g_interspeech.html"
  },
  "singh24c_interspeech": {
   "authors": [
    [
     "Vishwanath Pratap",
     "Singh"
    ],
    [
     "Federico",
     "Malato"
    ],
    [
     "Ville",
     "Hautamäki"
    ],
    [
     "Md.",
     "Sahidullah"
    ],
    [
     "Tomi",
     "Kinnunen"
    ]
   ],
   "title": "ROAR: Reinforcing Original to Augmented Data Ratio Dynamics for Wav2vec2.0 Based ASR",
   "original": "1403",
   "order": 593,
   "page_count": 5,
   "abstract": [
    "While automatic speech recognition (ASR) greatly benefits \nfrom data augmentation, the augmentation recipes themselves\n tend to be heuristic. In this paper, we address one of\nthe heuristic approach associated with balancing the right\n amount of augmented data in ASR training by introducing\na reinforcement learning (RL) based dynamic adjustment\nof original-to-augmented data ratio (OAR). Unlike\nthe fixed OAR approach in conventional data augmentation,\n our proposed method employs a deep Q-network (DQN) as\nthe RL mechanism to learn the optimal dynamics of OAR \nthroughout the wav2vec2.0 based ASR training. We conduct\n experiments using the LibriSpeech dataset with varying \namounts of training data, specifically, the 10Min, 1H,\n10H, and 100H splits to evaluate the efficacy of the proposed\n method under different data conditions. Our proposed\n method, on average, achieves a relative improvement\nof 4.96% over the open-source wav2vec2.0 base model on\nstandard LibriSpeech test sets."
   ],
   "p1": 2885,
   "pn": 2889,
   "doi": "10.21437/Interspeech.2024-1403",
   "url": "interspeech_2024/singh24c_interspeech.html"
  },
  "zhao24e_interspeech": {
   "authors": [
    [
     "Bin",
     "Zhao"
    ],
    [
     "Mingxuan",
     "Huang"
    ],
    [
     "Chenlu",
     "Ma"
    ],
    [
     "Jinyi",
     "Xue"
    ],
    [
     "Aijun",
     "Li"
    ],
    [
     "Kunyu",
     "Xu"
    ]
   ],
   "title": "Decoding Human Language Acquisition: EEG Evidence for Predictive Probabilistic Statistics in Word Segmentation",
   "original": "1404",
   "order": 537,
   "page_count": 5,
   "abstract": [
    "The huge success of large language models has highlighted the relationship between human language acquisition and statistical learning ability: it appears that humans, too, glean lexical rules through predictive probabilistic statistics. To validate this hypothesis, we conducted a EEG study comparing the auditory responses of language-specific cortical regions to speech streams containing novel tri-syllable words and random syllable sequences. Distinctive time-frequency patterns were detected with a lexical learning effect, especially in the primary auditory cortex of the left hemisphere. Besides, the middle temporal gyrus was found to be instrumental in the initial segmenting of speech streams, while the anterior temporal lobe was more prominent in orchestrating the top-down prediction based on established word structures. These findings reinforce the assertion that humans perceive and segment words by internalizing rule-based combinations in a predictive and statistical manner."
   ],
   "p1": 2605,
   "pn": 2609,
   "doi": "10.21437/Interspeech.2024-1404",
   "url": "interspeech_2024/zhao24e_interspeech.html"
  },
  "baihaqi24_interspeech": {
   "authors": [
    [
     "Muhammad Yeza",
     "Baihaqi"
    ],
    [
     "Angel",
     "Garcia Contreras"
    ],
    [
     "Seiya",
     "Kawano"
    ],
    [
     "Koichiro",
     "Yoshino"
    ]
   ],
   "title": "Rapport-Driven Virtual Agent: Rapport Building Dialogue Strategy for Improving User Experience at First Meeting",
   "original": "1406",
   "order": 835,
   "page_count": 5,
   "abstract": [
    "Rapport is known as a conversational aspect focusing on relationship building, which influences outcomes in collaborative tasks. This study aims to establish human-agent rapport through small talk by using a rapport-building strategy. We implemented this strategy for the virtual agents based on dialogue strategies by prompting a large language model (LLM). In particular, we utilized two dialogue strategies—predefined sequence and free-form—to guide the dialogue generation framework. We conducted analyses based on human evaluations, examining correlations between total turn, utterance characters, rapport score, and user experience variables: naturalness, satisfaction, interest, engagement, and usability. We investigated correlations between rapport score and naturalness, satisfaction, engagement, and conversation flow. Our experimental results also indicated that using free-form to prompt the rapport-building strategy performed the best in subjective scores."
   ],
   "p1": 4059,
   "pn": 4063,
   "doi": "10.21437/Interspeech.2024-1406",
   "url": "interspeech_2024/baihaqi24_interspeech.html"
  },
  "kumar24_interspeech": {
   "authors": [
    [
     "Sahil",
     "Kumar"
    ],
    [
     "Jialu",
     "Li"
    ],
    [
     "Youshan",
     "Zhang"
    ]
   ],
   "title": "Vision Transformer Segmentation for Visual Bird Sound Denoising",
   "original": "1412",
   "order": 26,
   "page_count": 5,
   "abstract": [
    "Audio denoising, especially in the context of bird sounds, remains a challenging task due to persistent residual noise. Traditional and deep learning methods often struggle with artificial or low-frequency noise. In this work, we propose ViTVS, a novel approach that leverages the power of the vision transformer (ViT) architecture. ViTVS adeptly combines segmentation techniques to disentangle clean audio from complex signal mixtures. Our key contributions encompass the development of ViTVS, introducing comprehensive, long-range, and multi-scale representations. These contributions directly tackle the limitations inherent in conventional approaches. Extensive experiments demonstrate that ViTVS outperforms state-of-the-art methods, positioning it as a benchmark solution for real-world bird sound denoising applications. Source code is available at: https://github.com/aiai-4/ViVTS."
   ],
   "p1": 122,
   "pn": 126,
   "doi": "10.21437/Interspeech.2024-1412",
   "url": "interspeech_2024/kumar24_interspeech.html"
  },
  "khanagha24_interspeech": {
   "authors": [
    [
     "Vahid",
     "Khanagha"
    ],
    [
     "Dimitris",
     "Koutsaidis"
    ],
    [
     "Kaustubh",
     "Kalgaonkar"
    ],
    [
     "Sriram",
     "Srinivasan"
    ]
   ],
   "title": "Interference Aware Training Target for DNN based joint Acoustic Echo Cancellation and Noise Suppression",
   "original": "1414",
   "order": 32,
   "page_count": 5,
   "abstract": [
    "Despite remarkable performance of Deep Learning based Acoustic Echo Cancellation (AEC) systems, effective handling of double-talk scenarios remains a challenge. During double-talk the speech signal from the far-end talker overlaps with the target near-end speech and results in degraded performance in form of near-end speech deletions or audible echo residuals shadowing the voice of target speaker.  This paper introduces an approach to reduce the shadowing effect through altering the ground truth used for model training such that the model can effectively clean up spectral components where the interference is stronger than the target speech. The alteration is accomplished leveraging the availability of interference signal during training data generation by masking spectral components of the ground truth where target speech is significantly weaker than the interference. Large scale subjective evaluation trials show that human listeners prefer the outputs generated by the new approach."
   ],
   "p1": 152,
   "pn": 156,
   "doi": "10.21437/Interspeech.2024-1414",
   "url": "interspeech_2024/khanagha24_interspeech.html"
  },
  "siriwardena24_interspeech": {
   "authors": [
    [
     "Yashish M.",
     "Siriwardena"
    ],
    [
     "Nathan",
     "Swedlow"
    ],
    [
     "Audrey",
     "Howard"
    ],
    [
     "Evan",
     "Gitterman"
    ],
    [
     "Dan",
     "Darcy"
    ],
    [
     "Carol",
     "Espy-Wilson"
    ],
    [
     "Andrea",
     "Fanelli"
    ]
   ],
   "title": "Accent Conversion with Articulatory Representations",
   "original": "1416",
   "resource": "https://doi.org/10.5281/zenodo.12790933",
   "order": 901,
   "page_count": 5,
   "abstract": [
    "Conversion of non-native accented speech to native (American) English has a wide range of applications such as improving intelligibility of non-native speech. Previous work on this domain has used phonetic posteriograms as the target speech representation to train an acoustic model which is then used to extract a compact representation of input speech for accent conversion. In this work, we introduce the idea of using an effective articulatory speech representation, extracted from an acoustic-to-articulatory speech inversion system, to improve the acoustic model used in accent conversion. The idea to incorporate articulatory representations originates from their ability to well characterize accents in speech. To incorporate articulatory representations with conventional phonetic posteriograms, a multi-task learning based acoustic model is proposed. Objective and subjective evaluations show that the use of articulatory representations can improve the effectiveness of accent conversion."
   ],
   "p1": 4383,
   "pn": 4387,
   "doi": "10.21437/Interspeech.2024-1416",
   "url": "interspeech_2024/siriwardena24_interspeech.html"
  },
  "hussein24_interspeech": {
   "authors": [
    [
     "Amir",
     "Hussein"
    ],
    [
     "Desh",
     "Raj"
    ],
    [
     "Matthew",
     "Wiesner"
    ],
    [
     "Daniel",
     "Povey"
    ],
    [
     "Paola",
     "Garcia"
    ],
    [
     "Sanjeev",
     "Khudanpur"
    ]
   ],
   "title": "Enhancing Neural Transducer for Multilingual ASR with Synchronized Language Diarization",
   "original": "1418",
   "order": 822,
   "page_count": 5,
   "abstract": [
    "In multilingual environments, seamless language switching, including code-switching (CS) within utterances, is essential for real-time applications. Conventional Automatic Speech Recognition (ASR) combined with language diarization requires post-processing to synchronize language labels with recognized words accurately, presenting a considerable challenge. In this study, we introduce a multitask learning framework that synchronizes Language Identification (LID) with ASR, utilizing a neural transducer architecture. This auxiliary task integrates both acoustic and lexical features to perform LID. Furthermore, we use resulting language representation as an auxiliary input to improve ASR. We demonstrate the efficacy of our proposed approach on conversational multilingual (Arabic, Spanish, Mandarin) and CS (Spanish-English, Mandarin-English) test sets."
   ],
   "p1": 3994,
   "pn": 3998,
   "doi": "10.21437/Interspeech.2024-1418",
   "url": "interspeech_2024/hussein24_interspeech.html"
  },
  "wang24y_interspeech": {
   "authors": [
    [
     "Haolan",
     "Wang"
    ],
    [
     "Amin",
     "Edraki"
    ],
    [
     "Wai-Yip",
     "Chan"
    ],
    [
     "Iván",
     "López-Espejo"
    ],
    [
     "Jesper",
     "Jensen"
    ]
   ],
   "title": "No-Reference Speech Intelligibility Prediction Leveraging a Noisy-Speech ASR Pre-Trained Model",
   "original": "1421",
   "order": 793,
   "page_count": 5,
   "abstract": [
    "Recent advances in deep learning have improved the capabilities of data-driven speech intelligibility prediction (SIP) algorithms. Nevertheless, the scarcity of speech intelligibility datasets limits the development of data-driven algorithms. This study introduces a set of no-reference SIP algorithms leveraging a pre-trained wav2vec 2.0 backbone. We adapt wav2vec 2.0 for automatic speech recognition under additive noise conditions with a parameter-efficient methodology, low-rank adaptation. We demonstrate no-reference SIP algorithms designed with this approach using a moderate amount of training data. The best designs perform on par or even better than a state-of-the-art reference-based SIP algorithm across a variety of datasets comprising different degradation types."
   ],
   "p1": 3849,
   "pn": 3853,
   "doi": "10.21437/Interspeech.2024-1421",
   "url": "interspeech_2024/wang24y_interspeech.html"
  },
  "hoang24_interspeech": {
   "authors": [
    [
     "Bao",
     "Hoang"
    ],
    [
     "Yijiang",
     "Pang"
    ],
    [
     "Hiroko",
     "Dodge"
    ],
    [
     "Jiayu",
     "Zhou"
    ]
   ],
   "title": "Translingual Language Markers for Cognitive Assessment from Spontaneous Speech",
   "original": "1422",
   "order": 197,
   "page_count": 5,
   "abstract": [
    "Mild Cognitive Impairment (MCI) is considered a prodromal stage of dementia, including Alzheimer’s disease. It is characterized by behavioral changes and decreased cognitive function, while individuals can still maintain their independence. Early detection of MCI is critical, as it allows for timely intervention, enrichment of clinical trial cohorts, and the development of therapeutic approaches. Recently, language markers have been shown to be a promising approach to identifying MCI in a non-intrusive, affordable, and accessible fashion. In the InterSpeech 2024 TAUKADIAL Challenge, we study language markers from spontaneous speech in English and Chinese and use the bilingual language markers to identify MCI cases and predict the Mini-Mental Status Examination (MMSE) scores. Our proposed framework combines the power from 1) feature extraction of a comprehensive set of bilingual acoustic features, and semantic and syntactic features from language models; 2) careful treatment of model complexity for small sample size; 3) consideration of imbalanced demographic structure, potential outlier removal, and a multi-task treatment that uses the prediction of clinical classification as prior for MMSE prediction. The proposed approach delivers an average of 78.2% Balanced Accuracy in MCI detection and an averaged RMSE of 2.705 in predicting MMSE. Our empirical evaluation shows that translingual language markers can improve the detection of MCI from spontaneous speech. Our codes are provided in https://github.com/illidanlab/translingual-language-markers."
   ],
   "p1": 977,
   "pn": 981,
   "doi": "10.21437/Interspeech.2024-1422",
   "url": "interspeech_2024/hoang24_interspeech.html"
  },
  "shang24_interspeech": {
   "authors": [
    [
     "Hengchao",
     "Shang"
    ],
    [
     "Zongyao",
     "Li"
    ],
    [
     "Jiaxin",
     "Guo"
    ],
    [
     "Shaojun",
     "Li"
    ],
    [
     "Zhiqiang",
     "Rao"
    ],
    [
     "Yuanchang",
     "Luo"
    ],
    [
     "Daimeng",
     "Wei"
    ],
    [
     "Hao",
     "Yang"
    ]
   ],
   "title": "An End-to-End Speech Summarization Using Large Language Model",
   "original": "1428",
   "order": 399,
   "page_count": 5,
   "abstract": [
    "Abstractive Speech Summarization (SSum) aims to generate human-like text summaries from spoken content. It encounters difficulties in handling long speech input and capturing the intricate cross-modal mapping between long speech inputs and short text summaries. Research on large language models (LLMs) and multimodal information fusion has provided new insights for addressing these challenges. In this paper, we propose an end-to-end SSum model that utilizes Q-Former as a connector for the audio-text modality and employs LLMs to generate text summaries directly from speech features. We adopt a multi-stage training approach that includes LLM based ASR and Text Summarization (TSum) tasks as auxiliary tasks. ASR tasks are used to align feature spaces and enhance the LLM's ability to handle longer speech. Then, we utilize a curriculum learning strategy to facilitate the model's transition from TSum to SSum. Finally, our model achieves competitive performance on the How-2 dataset."
   ],
   "p1": 1950,
   "pn": 1954,
   "doi": "10.21437/Interspeech.2024-1428",
   "url": "interspeech_2024/shang24_interspeech.html"
  },
  "li24y_interspeech": {
   "authors": [
    [
     "Shuhua",
     "Li"
    ],
    [
     "Qirong",
     "Mao"
    ],
    [
     "Jiatong",
     "Shi"
    ]
   ],
   "title": "PL-TTS: A Generalizable Prompt-based Diffusion TTS Augmented by Large Language Model",
   "original": "1429",
   "order": 1002,
   "page_count": 5,
   "abstract": [
    "With the increasing demand for style-controlled speech synthesis, traditional TTS methods for controlling acoustic features clearly have significant limitations. Therefore, using text style descriptions to achieve style-controlled TTS has become a current hot topic. However, existing methods often have unsatisfactory results when dealing with unseen style descriptions and ignore the issue of adding various style conditions to the model, which can lead to poor training performance of the original model. In this context, we propose PL-TTS, an enhanced diffusion-based TTS combined with prompts embedded by a large language model. In order to improve synthesis quality and style control ability, an enhanced diffusion-based framework and a method for fine-tuning large language models have been proposed. Experimental results in LibriTTS-R validate the effectiveness of PL-TTS in fine grained style control and generalization."
   ],
   "p1": 4888,
   "pn": 4892,
   "doi": "10.21437/Interspeech.2024-1429",
   "url": "interspeech_2024/li24y_interspeech.html"
  },
  "rittergutierrez24_interspeech": {
   "authors": [
    [
     "Fabian",
     "Ritter-Gutierrez"
    ],
    [
     "Kuan-Po",
     "Huang"
    ],
    [
     "Jeremy H. M.",
     "Wong"
    ],
    [
     "Dianwen",
     "Ng"
    ],
    [
     "Hung-yi",
     "Lee"
    ],
    [
     "Nancy F.",
     "Chen"
    ],
    [
     "Eng-Siong",
     "Chng"
    ]
   ],
   "title": "Dataset-Distillation Generative Model for Speech Emotion Recognition",
   "original": "1430",
   "order": 544,
   "page_count": 5,
   "abstract": [
    "Deep learning models for speech rely on large datasets, presenting computational challenges. Yet, performance hinges on training data size. Dataset Distillation (DD) aims to learn a smaller dataset without much performance degradation when training with it.  DD has been investigated in computer vision but not yet in speech. This paper presents the first approach for DD to speech targeting Speech Emotion Recognition on IEMOCAP. We employ Generative Adversarial Networks (GANs) not to mimic real data but to distil key discriminative information of IEMOCAP that is useful for downstream training. The GAN then replaces the original dataset and can sample custom synthetic dataset sizes. It performs comparably when following the original class imbalance but improves performance by 0.3% absolute UAR with balanced classes. It also reduces dataset storage and accelerates downstream training by 95% in both cases and reduces speaker information which could help for a privacy application."
   ],
   "p1": 2640,
   "pn": 2644,
   "doi": "10.21437/Interspeech.2024-1430",
   "url": "interspeech_2024/rittergutierrez24_interspeech.html"
  },
  "hai24_interspeech": {
   "authors": [
    [
     "Jiarui",
     "Hai"
    ],
    [
     "Karan",
     "Thakkar"
    ],
    [
     "Helin",
     "Wang"
    ],
    [
     "Zengyi",
     "Qin"
    ],
    [
     "Mounya",
     "Elhilali"
    ]
   ],
   "title": "DreamVoice: Text-Guided Voice Conversion",
   "original": "1432",
   "order": 899,
   "page_count": 5,
   "abstract": [
    "Generative voice technologies are rapidly evolving, offering opportunities for more personalized and inclusive experiences. Traditional one-shot voice conversion (VC) requires a target recording during inference, limiting ease of usage in generating desired voice timbres. Text-guided generation offers an intuitive solution to convert voices to desired \"DreamVoices\" according to the users' needs. Our paper presents two major contributions to VC technology: (1) DreamVoiceDB, a robust dataset of voice timbre annotations for 900 speakers from VCTK and LibriTTS. (2) Two text-guided VC methods: DreamVC, an end-to-end diffusion-based text-guided VC model; and DreamVG, a versatile text-to-voice generation plugin that can be combined with any one-shot VC models. The experimental results demonstrate that our proposed methods trained on the DreamVoiceDB dataset generate voice timbres accurately aligned with the text prompt and achieve high-quality VC."
   ],
   "p1": 4373,
   "pn": 4377,
   "doi": "10.21437/Interspeech.2024-1432",
   "url": "interspeech_2024/hai24_interspeech.html"
  },
  "tang24b_interspeech": {
   "authors": [
    [
     "Yuwu",
     "Tang"
    ],
    [
     "Ziang",
     "Ma"
    ],
    [
     "Haitao",
     "Zhang"
    ]
   ],
   "title": "Enhanced Feature Learning with Normalized Knowledge Distillation for Audio Tagging",
   "original": "1433",
   "order": 348,
   "page_count": 5,
   "abstract": [
    "Pre-trained transformer-based models have been the mainstream of audio tagging. Transformer-based models bring high performance at the cost of huge model size and slow inference speed, while pre-training methods heavily rely on large-scale data and vast computing resources. We argue that a more lightweight CNN-based backbone with customized feature learning can achieve the comparable performance as transformers. Thus an efficient audio tagging framework is proposed to capture more abundant feature information with several enhanced feature learning blocks. We further employ the method of knowledge distillation (KD) and propose a normalized KD loss calculation with adaptive temperature coefficients according to the samples distribution. Extensive experiments demonstrate that our method obtains the state-of-the-art results with a lightweight CNN-based model."
   ],
   "p1": 1695,
   "pn": 1699,
   "doi": "10.21437/Interspeech.2024-1433",
   "url": "interspeech_2024/tang24b_interspeech.html"
  },
  "joseph24_interspeech": {
   "authors": [
    [
     "George",
     "Joseph"
    ],
    [
     "Arun",
     "Baby"
    ]
   ],
   "title": "Speaker Personalization for Automatic Speech Recognition using Weight-Decomposed Low-Rank Adaptation",
   "original": "1434",
   "order": 591,
   "page_count": 5,
   "abstract": [
    "Personalizing automated speech recognition (ASR) for voice assistant systems is often considered the holy grail, requiring meticulous attention to detail in model optimization. When dealing with limited speaker data, the selection of hyper-parameters becomes paramount in fine-tuning large ASR models. One effective method for this optimization is low-rank adaptation (LoRA), which proves instrumental in enhancing the performance of large language models (LLMs). A variation of LoRA, Weight-Decomposed Low-Rank Adaptation (DoRA) also promises enhanced performance. In our study, we employed LoRA and DoRA, to refine the state-of-the-art cascaded conformer transducer model for speaker personalization. This involved adding a small number of speaker-specific weights to the existing model and fine-tuning them accordingly. Experimental assessments show an average relative improvement of 20% in word error rate across speakers with limited data, showcasing its efficacy in addressing the challenge of personalizing ASR systems in real-world applications."
   ],
   "p1": 2875,
   "pn": 2879,
   "doi": "10.21437/Interspeech.2024-1434",
   "url": "interspeech_2024/joseph24_interspeech.html"
  },
  "guo24c_interspeech": {
   "authors": [
    [
     "Yinlin",
     "Guo"
    ],
    [
     "Yening",
     "Lv"
    ],
    [
     "Jinqiao",
     "Dou"
    ],
    [
     "Yan",
     "Zhang"
    ],
    [
     "Yuehai",
     "Wang"
    ]
   ],
   "title": "FLY-TTS: Fast, Lightweight and High-Quality End-to-End Text-to-Speech Synthesis",
   "original": "1435",
   "resource": "https://doi.org/10.5281/zenodo.12743768",
   "order": 1006,
   "page_count": 5,
   "abstract": [
    "While recent advances in Text-To-Speech synthesis have yielded remarkable improvements in generating high-quality speech, research on lightweight and fast models is limited. This paper introduces FLY-TTS, a new fast, lightweight and high-quality speech synthesis system based on VITS. Specifically, 1) We replace the decoder with ConvNeXt blocks that generate Fourier spectral coefficients followed by the inverse short-time Fourier transform to synthesize waveforms; 2) To compress the model size, we introduce the grouped parameter-sharing mechanism to the text encoder and flow-based model; 3) We further employ the large pre-trained WavLM model for adversarial training to improve synthesis quality. Experimental results show that our model achieves a real-time factor of 0.0139 on an Intel Core i9 CPU, 8.8x faster than the baseline (0.1221), with a 1.6x parameter compression. Objective and subjective evaluations indicate that FLY-TTS exhibits comparable speech quality to the strong baseline."
   ],
   "p1": 4908,
   "pn": 4912,
   "doi": "10.21437/Interspeech.2024-1435",
   "url": "interspeech_2024/guo24c_interspeech.html"
  },
  "bae24_interspeech": {
   "authors": [
    [
     "Hanbin",
     "Bae"
    ],
    [
     "Pavel",
     "Andreev"
    ],
    [
     "Azat",
     "Saginbaev"
    ],
    [
     "Nicholas",
     "Babaev"
    ],
    [
     "WonJun",
     "Lee"
    ],
    [
     "Hosang",
     "Sung"
    ],
    [
     "Hoon-Young",
     "Cho"
    ]
   ],
   "title": "Speech Boosting: Low-Latency Live Speech Enhancement for TWS Earbuds",
   "original": "1444",
   "order": 131,
   "page_count": 5,
   "abstract": [
    "This paper introduces a speech enhancement solution tailored for true wireless stereo (TWS) earbuds on-device usage.  The solution was specifically designed to support conversations in noisy environments, with active noise cancellation (ANC) activated.  The primary challenges for speech enhancement models in this context arise from computational complexity that limits on-device usage and latency that must be less than 3 ms to preserve a live conversation. To address these issues, we evaluated several crucial design elements, including the network architecture and domain, design of loss functions, pruning method, and hardware-specific optimization. Consequently, we demonstrated substantial improvements in speech enhancement quality compared with that in baseline models, while simultaneously reducing the computational complexity and algorithmic latency."
   ],
   "p1": 647,
   "pn": 651,
   "doi": "10.21437/Interspeech.2024-1444",
   "url": "interspeech_2024/bae24_interspeech.html"
  },
  "talkar24_interspeech": {
   "authors": [
    [
     "Tanya",
     "Talkar"
    ],
    [
     "Sherman",
     "Charles"
    ],
    [
     "Chelsea",
     "Krantsevich"
    ],
    [
     "Kan",
     "Kawabata"
    ]
   ],
   "title": "Detection of Cognitive Impairment And Alzheimer's Disease Using a Speech- and Language-Based Protocol",
   "original": "1446",
   "order": 621,
   "page_count": 5,
   "abstract": [
    "Alzheimer’s Disease (AD) is a neurodegenerative disease that primarily affects cognitive functions and memory loss. The maximum effectiveness of medications rely on early detection of the presence of AD, which requires invasive extensive blood tests and/or brain imaging. In this study, we present a speech-based tool to detect the presence of mild cognitive impairment (MCI) and probable AD (pAD). Through the Speech Vitals application, in conjunction with the Bio-Hermes study, we administer a set of speech and language-based tasks targeting motor and cognitive impairments. We extract metrics from speech and language tasks to generate models that discriminate between healthy controls and individuals with MCI or pAD with an AUC of 0.83, and between controls and individuals with pAD with an AUC of 0.90. These models highlight the promise of a speech-based tool for screening cognitive impairment and AD, which could help inform individuals when they should seek additional diagnostic tools. "
   ],
   "p1": 3025,
   "pn": 3029,
   "doi": "10.21437/Interspeech.2024-1446",
   "url": "interspeech_2024/talkar24_interspeech.html"
  },
  "cho24b_interspeech": {
   "authors": [
    [
     "Hyunjae",
     "Cho"
    ],
    [
     "Junhyeok",
     "Lee"
    ],
    [
     "Wonbin",
     "Jung"
    ]
   ],
   "title": "JenGAN: Stacked Shifted Filters in GAN-Based Speech Synthesis",
   "original": "1447",
   "order": 799,
   "page_count": 5,
   "abstract": [
    "Non-autoregressive GAN-based neural vocoders are widely used due to their fast inference speed and high perceptual quality. However, they often suffer from audible artifacts such as tonal artifacts in their generated results. Therefore, we propose JenGAN, a new training strategy that involves stacking shifted low-pass filters to ensure the shift-equivariant property. This method helps prevent aliasing and reduce artifacts while preserving the model structure used during inference. In our experimental evaluation, JenGAN consistently enhances the performance of vocoder models, yielding significantly superior scores across the majority of evaluation metrics."
   ],
   "p1": 3879,
   "pn": 3883,
   "doi": "10.21437/Interspeech.2024-1447",
   "url": "interspeech_2024/cho24b_interspeech.html"
  },
  "kang24c_interspeech": {
   "authors": [
    [
     "Iwen E",
     "Kang"
    ],
    [
     "Christophe",
     "Van Gysel"
    ],
    [
     "Man-Hung",
     "Siu"
    ]
   ],
   "title": "Transformer-based Model for ASR N-Best Rescoring and Rewriting",
   "original": "1449",
   "order": 717,
   "page_count": 5,
   "abstract": [
    "Voice assistants increasingly use on-device Automatic Speech Recognition (ASR) to ensure speed and privacy. However, due to resource constraints on the device, queries pertaining to complex information domains often require further processing by a search engine. For such applications, we propose a novel Transformer based model capable of rescoring and rewriting, by exploring full context of the N-best hypotheses in parallel. We also propose a new discriminative sequence training objective that can work well for both rescore and rewrite tasks. We show that our Rescore+Rewrite model outperforms the Rescore-only baseline, and achieves up to an average 8.6% relative Word Error Rate (WER) reduction over the ASR system by itself."
   ],
   "p1": 3505,
   "pn": 3509,
   "doi": "10.21437/Interspeech.2024-1449",
   "url": "interspeech_2024/kang24c_interspeech.html"
  },
  "kim24n_interspeech": {
   "authors": [
    [
     "Jaewon",
     "Kim"
    ],
    [
     "Won-Gook",
     "Choi"
    ],
    [
     "Seyun",
     "Ahn"
    ],
    [
     "Joon-Hyuk",
     "Chang"
    ]
   ],
   "title": "Sound of Vision: Audio Generation from Visual Text Embedding through Training Domain Discriminator",
   "original": "1451",
   "order": 677,
   "page_count": 5,
   "abstract": [
    "Recent advancements in text-to-audio (TTA) models have demonstrated their ability to generate sound that aligns with user intentions. Despite this advancement, a notable limitation arises from the models' inability to effectively synthesize audio from visual-domain texts. In this study, we address this challenge by utilizing a novel dataset that pairs visual and acoustic-domain texts, derived using ChatGPT-3.5, and encoding switch through a domain discriminator. This approach ensures not only computational efficiency but also enhances the model's generalization, adaptability, and flexibility. It addresses concerns that training exclusively with visual texts might compromise audio generation quality from audio texts. This study presents a novel methodology for enhancing text-to-audio synthesis, demonstrating significant improvements in audio output fidelity from visual-text inputs."
   ],
   "p1": 3305,
   "pn": 3309,
   "doi": "10.21437/Interspeech.2024-1451",
   "url": "interspeech_2024/kim24n_interspeech.html"
  },
  "zhang24l_interspeech": {
   "authors": [
    [
     "Haojie",
     "Zhang"
    ],
    [
     "Tao",
     "Zhang"
    ],
    [
     "Ganjun",
     "Liu"
    ],
    [
     "Dehui",
     "Fu"
    ],
    [
     "Xiaohui",
     "Hou"
    ],
    [
     "Ying",
     "Lv"
    ]
   ],
   "title": "DysArinVox: DYSphonia &amp; DYSarthria mandARIN speech corpus",
   "original": "1452",
   "order": 188,
   "page_count": 5,
   "abstract": [
    "This paper introduces DysArinVox, a new pathological speech corpus in Chinese. It included 173 participants from 27 healthy individuals and 146 voice disorders, whose various types and severities of vocal impairments as diagnosed by speech pathology experts via auditory perceptual evaluations and laryngoscopic imagery. DysArinVox is designed to provide a high-quality Chinese resource for AI-driven diagnostics and prognostics. To ensure the efficiency of corpus collection, we meticulously crafted recording scripts represent Mandarin phonetically, ensuring comprehensive syllable representation with minimal lexical complexity. Additionally, incorporating laryngoscopic images of patients into the dataset offers extra visual information, facilitating the development of advanced diagnostic frameworks. To our knowledge, this database represents the most comprehensive corpus of Chinese pathological speech to date."
   ],
   "p1": 932,
   "pn": 936,
   "doi": "10.21437/Interspeech.2024-1452",
   "url": "interspeech_2024/zhang24l_interspeech.html"
  },
  "a24_interspeech": {
   "authors": [
    [
     "Noumida",
     "A"
    ],
    [
     "Rajeev",
     "Rajan"
    ]
   ],
   "title": "Multi-label Bird Species Classification from Field Recordings using Mel_Graph-GCN Framework",
   "original": "1453",
   "order": 983,
   "page_count": 5,
   "abstract": [
    "This paper proposes a novel approach called the Mel_Graph-GCN framework, which utilizes graph convolutional neural networks to identify multiple bird species from field recordings. The process involves creating a graph from the Mel-spectrogram of the audio file using a trained deep convolutional neural network (deep CNN), and employing SpecAugment to generate additional Mel-spectrograms for enhanced training of the deep CNN. Subsequently, the graph is fed to a GCN for classification. The algorithm's performance is evaluated using the Xeno-canto bird sound database and compared with state-of-the-art models, demonstrating superior performance with a macro F1 score of 0.85."
   ],
   "p1": 4793,
   "pn": 4797,
   "doi": "10.21437/Interspeech.2024-1453",
   "url": "interspeech_2024/a24_interspeech.html"
  },
  "buddi24_interspeech": {
   "authors": [
    [
     "Sai Srujana",
     "Buddi"
    ],
    [
     "Satyam",
     "Kumar"
    ],
    [
     "Utkarsh",
     "Sarawgi"
    ],
    [
     "Vineet",
     "Garg"
    ],
    [
     "Shivesh",
     "Ranjan"
    ],
    [
     "Ognjen",
     "Rudovic"
    ],
    [
     "Ahmed",
     "Hussen Abdelaziz"
    ],
    [
     "Saurabh",
     "Adya"
    ]
   ],
   "title": "Comparative Analysis of Personalized Voice Activity Detection Systems: Assessing Real-World Effectiveness",
   "original": "1454",
   "order": 978,
   "page_count": 5,
   "abstract": [
    "Voice activity detection (VAD) is a critical component in various applications such as speech recognition, speech enhancement, and hands-free communication systems. With the increasing demand for personalized and context-aware technologies, the need for effective personalized VAD systems has become paramount. In this paper, we present a comparative analysis of Personalized Voice Activity Detection (PVAD) systems to assess their real-world effectiveness. We introduce a comprehensive approach to assess PVAD systems, incorporating various performance metrics such as frame-level and utterance-level error rates, detection latency and accuracy, alongside user-level analysis. Through extensive experimentation and evaluation, we provide a thorough understanding of the strengths and limitations of various PVAD variants. This paper advances the understanding of PVAD technology by offering insights into its efficacy and viability in practical applications using a comprehensive set of metrics."
   ],
   "p1": 4768,
   "pn": 4772,
   "doi": "10.21437/Interspeech.2024-1454",
   "url": "interspeech_2024/buddi24_interspeech.html"
  },
  "choi24c_interspeech": {
   "authors": [
    [
     "Ho-Young",
     "Choi"
    ],
    [
     "Won-Gook",
     "Choi"
    ],
    [
     "Joon-Hyuk",
     "Chang"
    ]
   ],
   "title": "Retrieval-Augmented Classifier Guidance for Audio Generation",
   "original": "1456",
   "order": 678,
   "page_count": 5,
   "abstract": [
    "Most audio datasets utilized for training in the audio generation fields are low-quality, leading to difficulties in the generation of high-quality, single-event audio. However, to acquire single-event audio with noise-free, high costs are incurred. In this paper, we propose a simple retrieval-augmented classifier-guided sampling strategy for foley sound synthesis. Specifically, to guide the diffusion model during sampling with classifier guidance, given an input class, we first retrieve relevant audio features by utilizing a Contrastive Language-Audio Pretraining model. The gradients from a classifier for the retrieved audio features are then calculated to serve as additional guidance. Our evaluation, conducted on the DCASE 2023 challenge task 7 dataset, demonstrates that our proposed method overall improves a Frechet audio distance score."
   ],
   "p1": 3310,
   "pn": 3314,
   "doi": "10.21437/Interspeech.2024-1456",
   "url": "interspeech_2024/choi24c_interspeech.html"
  },
  "mohapatra24_interspeech": {
   "authors": [
    [
     "Payal",
     "Mohapatra"
    ],
    [
     "Shamika",
     "Likhite"
    ],
    [
     "Subrata",
     "Biswas"
    ],
    [
     "Bashima",
     "Islam"
    ],
    [
     "Qi",
     "Zhu"
    ]
   ],
   "title": "Missingness-resilient Video-enhanced Multimodal Disfluency Detection",
   "original": "1458",
   "resource": "https://doi.org/10.6084/m9.figshare.25526953.v1",
   "order": 1043,
   "page_count": 5,
   "abstract": [
    "Most existing speech disfluency detection techniques only rely upon acoustic data. In this work, we present a practical multimodal disfluency detection approach that leverages available video data together with audio. We curate an audio-visual dataset and propose a novel fusion technique with unified weight-sharing modality-agnostic encoders to learn the temporal and semantic context. Our resilient design accommodates real-world scenarios where the video modality may sometimes be missing during inference. We also present alternative fusion strategies when both modalities are assured to be complete. In experiments across five disfluency-detection tasks, our unified multimodal approach significantly outperforms Audio-only unimodal methods, yielding an average absolute improvement of 10% (i.e., 10 percentage point increase) when both video and audio modalities are always available, and 7% even when video modality is missing in half of the samples."
   ],
   "p1": 5093,
   "pn": 5097,
   "doi": "10.21437/Interspeech.2024-1458",
   "url": "interspeech_2024/mohapatra24_interspeech.html"
  },
  "li24z_interspeech": {
   "authors": [
    [
     "Qifei",
     "Li"
    ],
    [
     "Yingming",
     "Gao"
    ],
    [
     "Yuhua",
     "Wen"
    ],
    [
     "Cong",
     "Wang"
    ],
    [
     "Ya",
     "Li"
    ]
   ],
   "title": "Enhancing Modal Fusion by Alignment and Label Matching for Multimodal Emotion Recognition",
   "original": "1462",
   "resource": "https://doi.org/10.5281/zenodo.12754917",
   "order": 957,
   "page_count": 5,
   "abstract": [
    "To address the limitation in multimodal emotion recognition (MER) performance arising from inter-modal information fusion, we propose a novel MER framework based on multitask learning where fusion occurs after alignment, called Foal-Net. The framework is designed to enhance the effectiveness of modality fusion and includes two auxiliary tasks: audio-video emotion alignment (AVEL) and cross-modal emotion label matching (MEM). First, AVEL achieves alignment of emotional information in audio-video representations through contrastive learning. Then, a modal fusion network integrates the aligned features. Meanwhile, MEM assesses whether the emotions of the current sample pair are the same, providing assistance for modal information fusion and guiding the model to focus more on emotional information. The experimental results conducted on IEMOCAP corpus show that Foal-Net outperforms the state-of-the-art methods and emotion alignment is necessary before modal fusion. The code is open-source."
   ],
   "p1": 4663,
   "pn": 4667,
   "doi": "10.21437/Interspeech.2024-1462",
   "url": "interspeech_2024/li24z_interspeech.html"
  },
  "chen24q_interspeech": {
   "authors": [
    [
     "Zhengyang",
     "Chen"
    ],
    [
     "Xuechen",
     "Liu"
    ],
    [
     "Erica",
     "Cooper"
    ],
    [
     "Junichi",
     "Yamagishi"
    ],
    [
     "Yanmin",
     "Qian"
    ]
   ],
   "title": "Generating Speakers by Prompting Listener Impressions for Pre-trained Multi-Speaker Text-to-Speech Systems",
   "original": "1465",
   "order": 910,
   "page_count": 5,
   "abstract": [
    "This paper proposes a speech synthesis system that allows users to specify and control the acoustic characteristics of a speaker by means of prompts describing the speaker's traits of synthesized speech. Unlike previous approaches, our method utilizes listener impressions to construct prompts, which are easier to collect and align more naturally with everyday descriptions of speaker traits. We adopt the Lowrank Adaptation (LoRA) technique to swiftly tailor a pretrained language model to our needs, facilitating the extraction of speakerrelated traits from the prompt text. Besides, different from other promptdriven texttospeech (TTS) systems, we separate the prompttospeaker module from the multispeaker TTS system, enhancing system flexibility and compatibility with various pretrained multispeaker TTS systems. Moreover, for the prompt-to-speaker characteristic module, we also compared the discriminative method and flow-matching based generative method and we found that combining both methods can help the system simultaneously capture speaker-related information from prompts better and generate speech with higher fidelity."
   ],
   "p1": 4428,
   "pn": 4432,
   "doi": "10.21437/Interspeech.2024-1465",
   "url": "interspeech_2024/chen24q_interspeech.html"
  },
  "wang24z_interspeech": {
   "authors": [
    [
     "Junxu",
     "Wang"
    ],
    [
     "Zhihua",
     "Fang"
    ],
    [
     "Liang",
     "He"
    ]
   ],
   "title": "Self-Supervised Speaker Verification with Mini-Batch Prediction Correction",
   "original": "1466",
   "order": 967,
   "page_count": 5,
   "abstract": [
    "Applying self-supervised learning to speaker verification tasks has been a challenge. In the two-stage solution, the clustering-iteration step in stage 2 determines the upper bound of the system. Since the pseudo-labels obtained through clustering contain a lot of noise, in order to deal with them, in this paper, we propose a new method for learning with noisy pseudo-labels focusing on small batches, using a unified alignment method based on the model predicted mean and exponential moving average to determine the samples that can be rectified in noisy pseudo-labels. In addition, we explore different iterative training methods, and propose a training method that takes into account the effects of re-clustering and noisy pseudo-labels. By combining these techniques, our system achieves similar or better results compared with previous studies."
   ],
   "p1": 4713,
   "pn": 4717,
   "doi": "10.21437/Interspeech.2024-1466",
   "url": "interspeech_2024/wang24z_interspeech.html"
  },
  "meng24d_interspeech": {
   "authors": [
    [
     "Zhong",
     "Meng"
    ],
    [
     "Zelin",
     "Wu"
    ],
    [
     "Rohit",
     "Prabhavalkar"
    ],
    [
     "Cal",
     "Peyser"
    ],
    [
     "Weiran",
     "Wang"
    ],
    [
     "Nanxin",
     "Chen"
    ],
    [
     "Tara N.",
     "Sainath"
    ],
    [
     "Bhuvana",
     "Ramabhadran"
    ]
   ],
   "title": "Text Injection for Neural Contextual Biasing",
   "original": "1471",
   "order": 613,
   "page_count": 5,
   "abstract": [
    "Neural contextual biasing effectively improves automatic\nspeech recognition (ASR) for crucial phrases within a speaker’s\ncontext, particularly those that are infrequent in the training\ndata. This work proposes contextual text injection (CTI) to \nenhance contextual ASR. CTI leverages not only the paired \nspeech-text data, but also a much larger corpus of unpaired \ntext to optimize the ASR model and its neural biasing component.\nUnpaired text is converted into speech-like representations \nand used to guide the model’s attention towards relevant \nbias phrases. Moreover, we introduce a contextual text-injected\n(CTI) minimum word error rate (MWER) training, which minimizes\nthe expected WER caused by contextual biasing when\n unpaired text is injected into the model. Experiments show that\nCTI with 100 billion text sentences can achieve up to 43.3% relative\n WER reduction from a strong neural biasing model. CTIMWER\nprovides a further relative improvement of 23.5%."
   ],
   "p1": 2985,
   "pn": 2989,
   "doi": "10.21437/Interspeech.2024-1471",
   "url": "interspeech_2024/meng24d_interspeech.html"
  },
  "pan24c_interspeech": {
   "authors": [
    [
     "Zihan",
     "Pan"
    ],
    [
     "Tianchi",
     "Liu"
    ],
    [
     "Hardik B.",
     "Sailor"
    ],
    [
     "Qiongqiong",
     "Wang"
    ]
   ],
   "title": "Attentive Merging of Hidden Embeddings from Pre-trained Speech Model for Anti-spoofing Detection",
   "original": "1472",
   "order": 433,
   "page_count": 5,
   "abstract": [
    "Self-supervised learning (SSL) speech representation models, trained on large speech corpora, have demonstrated effectiveness in extracting hierarchical speech embeddings through multiple transformer layers. However, the behavior of these embeddings in specific tasks remains uncertain. This paper investigates the multi-layer behavior of the WavLM model in anti-spoofing and proposes an attentive merging method to leverage the hierarchical hidden embeddings. Results demonstrate the feasibility of fine-tuning WavLM to achieve the best equal error rate (EER) of 0.65%, 3.50%, and 3.19% on the ASVspoof 2019LA, 2021LA, and 2021DF evaluation sets, respectively. Notably, We find that the early hidden transformer layers of the WavLM large model contribute significantly to anti-spoofing task, enabling computational efficiency by utilizing a partial pre-trained model."
   ],
   "p1": 2090,
   "pn": 2094,
   "doi": "10.21437/Interspeech.2024-1472",
   "url": "interspeech_2024/pan24c_interspeech.html"
  },
  "onda24_interspeech": {
   "authors": [
    [
     "Kentaro",
     "Onda"
    ],
    [
     "Joonyong",
     "Park"
    ],
    [
     "Nobuaki",
     "Minematsu"
    ],
    [
     "Daisuke",
     "Saito"
    ]
   ],
   "title": "A Pilot Study of GSLM-based Simulation of Foreign Accentuation Only Using Native Speech Corpora",
   "original": "1473",
   "order": 736,
   "page_count": 5,
   "abstract": [
    "We propose a method of simulating the human process of foreign accentuation using Generative Spoken Language Model (GSLM) only with native speech corpora. When one listens to spoken words of a foreign language and repeats them, the repeated speech is often with the accent of that listener’s L1. This is said to be because the spoken words are mentally represented as a sequence of phonological units of the L1, and those units are used for oral reproduction. We simulate this process by inputting speech of language A into GSLM of language B to add B’s accent onto the input speech. The process of running ASR of the L1 for foreign input speech and giving the ASR result to TTS of the L1 can be viewed as a naive implementation of this approach. The results of our experiments show that the synthesized accent of the output speech is highly natural, compared to real samples of A generated by speakers whose L1 is B, and that the degree of accentuation is controllable."
   ],
   "p1": 3600,
   "pn": 3604,
   "doi": "10.21437/Interspeech.2024-1473",
   "url": "interspeech_2024/onda24_interspeech.html"
  },
  "seong24_interspeech": {
   "authors": [
    [
     "Donghyun",
     "Seong"
    ],
    [
     "Joon-Hyuk",
     "Chang"
    ]
   ],
   "title": "H4C-TTS: Leveraging Multi-Modal Historical Context for Conversational Text-to-Speech",
   "original": "1480",
   "order": 1011,
   "page_count": 5,
   "abstract": [
    "Conversational text-to-speech (TTS) aims to synthesize natural voices appropriate to a situation by considering the context of past conversations as well as the current text. However, analyzing and modeling the context of a conversation remains challenging. Most conversational TTS use the content of historical and recent conversations without distinguishing between them and often generate speech that does not fit the situation. Hence, we introduce a novel conversational TTS, H4C-TTS, that leverages multi-modal historical context to realize contextually appropriate natural speech synthesis. To facilitate conversational context modeling, we design a context encoder that incorporates historical and recent contexts and a multi-modal encoder that processes textual and acoustic inputs. Experimental results demonstrate that the proposed model significantly improves the naturalness and quality of speech in conversational contexts compared with existing conversational TTS."
   ],
   "p1": 4933,
   "pn": 4937,
   "doi": "10.21437/Interspeech.2024-1480",
   "url": "interspeech_2024/seong24_interspeech.html"
  },
  "mehta24_interspeech": {
   "authors": [
    [
     "Daryush D.",
     "Mehta"
    ],
    [
     "Jarrad H. Van",
     "Stan"
    ],
    [
     "Hamzeh",
     "Ghasemzadeh"
    ],
    [
     "Robert E.",
     "Hillman"
    ]
   ],
   "title": "Comparing ambulatory voice measures during daily life with brief laboratory assessments in speakers with and without vocal hyperfunction",
   "original": "1484",
   "order": 300,
   "page_count": 5,
   "abstract": [
    "The most common types of voice disorders are associated with hyperfunctional voice use in daily life. Although current clinical practice uses measures from brief laboratory recordings to assess vocal function, it is unclear how these relate to an individual’s habitual voice use. The purpose of this study was to quantify the correlation and offset between voice features computed from laboratory and ambulatory recordings in speakers with and without vocal hyperfunction. Features derived from a neck-surface accelerometer included estimates of sound pressure level, fundamental frequency, cepstral peak prominence, and spectral tilt. Whereas some measures from laboratory recordings correlated significantly with those captured during daily life, only approximately 6–52% of the actual variance was accounted for. Thus, brief voice assessments are quite limited in the extent to which they can accurately characterize the daily voice use of speakers with and without vocal hyperfunction."
   ],
   "p1": 1455,
   "pn": 1459,
   "doi": "10.21437/Interspeech.2024-1484",
   "url": "interspeech_2024/mehta24_interspeech.html"
  },
  "li24aa_interspeech": {
   "authors": [
    [
     "Li",
     "Li"
    ],
    [
     "Shogo",
     "Seki"
    ]
   ],
   "title": "Improved Remixing Process for Domain Adaptation-Based Speech Enhancement by Mitigating Data Imbalance in Signal-to-Noise Ratio",
   "original": "1488",
   "order": 351,
   "page_count": 5,
   "abstract": [
    "RemixIT and Remixed2Remixed are domain adaptation-based speech enhancement (DASE) methods that use a teacher model trained under full supervision to generate pseudo-paired data by remixing the outputs of the teacher model. The student model for enhancing real-world recorded signals is trained using pseudo-paired data without the ground truth. Because noisy signals are recorded in natural environments, the dataset inevitably suffers from data imbalance in some acoustic properties, leading to subpar performance for the underrepresented data. The signal-to-noise ratio (SNR), inherently balanced in supervised learning, is a prime example. In this paper, we provide empirical evidence that the SNR of pseudo-data has a significant impact on model performance using the dataset of the CHiME-7 UDASE task, highlighting the importance of a balanced SNR in DASE. Furthermore, we propose adopting curriculum learning to encompass a broad range of SNRs to boost the performance of underrepresented data."
   ],
   "p1": 1710,
   "pn": 1714,
   "doi": "10.21437/Interspeech.2024-1488",
   "url": "interspeech_2024/li24aa_interspeech.html"
  },
  "lin24j_interspeech": {
   "authors": [
    [
     "Yuke",
     "Lin"
    ],
    [
     "Ming",
     "Cheng"
    ],
    [
     "Fulin",
     "Zhang"
    ],
    [
     "Yingying",
     "Gao"
    ],
    [
     "Shilei",
     "Zhang"
    ],
    [
     "Ming",
     "Li"
    ]
   ],
   "title": "VoxBlink2: A 100K+ Speaker Recognition Corpus and the Open-Set Speaker-Identification Benchmark",
   "original": "1490",
   "order": 877,
   "page_count": 5,
   "abstract": [
    "In this paper, we provide a large audio-visual speaker recognition dataset, VoxBlink2, which includes approximately 10M utterances with videos from 110K+ speakers in the wild. This dataset represents a significant expansion over the VoxBlink dataset, encompassing a broader diversity of speakers and scenarios by the grace of an optimized data collection pipeline. Afterward, we explore the impact of training strategies, data scale, and model complexity on speaker verification and finally establish a new single-model state-of-the-art EER at 0.170% and minDCF at 0.006% on the VoxCeleb1-O test set. Such remarkable results motivate us to explore speaker recognition from a new challenging perspective. We raise the Open-Set Speaker-Identification task, which is designed to either match a probe utterance with a known gallery speaker or categorize it as an unknown query. Associated with this task, we design concrete benchmark and evaluation protocols. The data and model resources can be found in http://voxblink2.github.io."
   ],
   "p1": 4263,
   "pn": 4267,
   "doi": "10.21437/Interspeech.2024-1490",
   "url": "interspeech_2024/lin24j_interspeech.html"
  },
  "saif24_interspeech": {
   "authors": [
    [
     "A F M",
     "Saif"
    ],
    [
     "Lisha",
     "Chen"
    ],
    [
     "Xiaodong",
     "Cui"
    ],
    [
     "Songtao",
     "Lu"
    ],
    [
     "Brian",
     "Kingsbury"
    ],
    [
     "Tianyi",
     "Chen"
    ]
   ],
   "title": "M2ASR: Multilingual Multi-task Automatic Speech Recognition via Multi-objective Optimization",
   "original": "1492",
   "order": 257,
   "page_count": 5,
   "abstract": [
    "To enable the capability of speech models across multiple languages, training multilingual, multi-task automatic speech recognition (ASR) models has gained growing interest. However, different languages and tasks result in distinct training objectives, potentially leading to conflicts during training and degrading the model's performance. To overcome this issue, we introduce M2ASR, a multilingual, multi-task ASR framework, which formulates the problem as a constrained multi-objective optimization (MOO), where multilingual multi-task supervised training augmented by speech-to-text translation (S2TT) serve as supervised objectives and are subject to the desired performance of multilingual unsupervised training. We employ MOO techniques to avoid conflicts among multiple linguistic representations and tasks during training. Extensive experiments demonstrate that M2ASR outperforms conventional multilingual ASR models by 28.3% to 38.6% across diverse ASR tasks."
   ],
   "p1": 1240,
   "pn": 1244,
   "doi": "10.21437/Interspeech.2024-1492",
   "url": "interspeech_2024/saif24_interspeech.html"
  },
  "nie24_interspeech": {
   "authors": [
    [
     "Liangyu",
     "Nie"
    ],
    [
     "Sudarsana Reddy",
     "Kadiri"
    ],
    [
     "Ruchit",
     "Agrawal"
    ]
   ],
   "title": "MMSD-Net: Towards Multi-modal Stuttering Detection",
   "original": "1497",
   "order": 1047,
   "page_count": 5,
   "abstract": [
    "Stuttering is a common speech impediment that is caused by irregular disruptions in speech production, affecting over 70 million people across the world. Standard automatic speech processing tools do not take speech ailments into account and are thereby not able to generate meaningful results when presented with stuttered speech as input. The automatic detection of stuttering is an integral step towards building efficient, context-aware speech processing systems. While previous approaches explore both statistical and neural approaches for stuttering detection, all of these methods are uni-modal in nature. This paper presents MMSD-Net, the first multi-modal neural framework for stuttering detection. Experiments and results demonstrate that incorporating the visual signal significantly aids stuttering detection, and our model yields an improvement of 2-17% in the F1-score over existing state-of-the-art uni-modal approaches."
   ],
   "p1": 5113,
   "pn": 5117,
   "doi": "10.21437/Interspeech.2024-1497",
   "url": "interspeech_2024/nie24_interspeech.html"
  },
  "shi24c_interspeech": {
   "authors": [
    [
     "Mingyue",
     "Shi"
    ],
    [
     "Huali",
     "Zhou"
    ],
    [
     "Qinglin",
     "Meng"
    ],
    [
     "Nengheng",
     "Zheng"
    ]
   ],
   "title": "DBD-CI: Doubling the Band Density for Bilateral Cochlear Implants",
   "original": "1505",
   "order": 535,
   "page_count": 5,
   "abstract": [
    "Cochlear implants (CIs) have limited spectral resolution due to a limited number of electrodes and inter-electrode current interaction, causing difficulties in speech-in-noise perception even for bilateral CI (BiCI) users. Previous studies have suggested alternately stimulating odd and even electrodes between the two sides to reduce current interaction in several ways, showing promising results if dichotic stimuli could be effectively integrated. To utilize the total electrode number of a pair of BiCIs, we propose Doubling the signal analysis Band Density (DBD) and encoding odd and even bands alternately for each side.  Two preliminary vocoder-simulation experiments in spectral-temporally modulated ripple discrimination and speech-in-noise perception were carried out to compare DBD with the default setting of the advanced combination encoder (ACE) strategy. The proposed method showed promising benefits as well as limitations to be further resolved in theory and evaluated in BiCI users."
   ],
   "p1": 2595,
   "pn": 2599,
   "doi": "10.21437/Interspeech.2024-1505",
   "url": "interspeech_2024/shi24c_interspeech.html"
  },
  "wang24aa_interspeech": {
   "authors": [
    [
     "Peidong",
     "Wang"
    ],
    [
     "Jian",
     "Xue"
    ],
    [
     "Jinyu",
     "Li"
    ],
    [
     "Junkun",
     "Chen"
    ],
    [
     "Aswin Shanmugam",
     "Subramanian"
    ]
   ],
   "title": "Soft Language Identification for Language-Agnostic Many-to-One End-to-End Speech Translation",
   "original": "1507",
   "order": 77,
   "page_count": 5,
   "abstract": [
    "Language-agnostic many-to-one end-to-end speech translation models can convert audio signals from different source languages into text in a target language. These models do not need source language identification, which improves user experience. In some cases, the input language can be given or estimated. Our goal is to use this additional language information while preserving the quality of the other languages. We accomplish this by introducing a simple and effective linear input network. The linear input network is initialized as an identity matrix, which ensures that the model can perform as well as, or better than, the original model. Experimental results show that the proposed method can successfully enhance the specified language, while keeping the language-agnostic ability of the many-to-one ST models."
   ],
   "p1": 377,
   "pn": 381,
   "doi": "10.21437/Interspeech.2024-1507",
   "url": "interspeech_2024/wang24aa_interspeech.html"
  },
  "saeki24_interspeech": {
   "authors": [
    [
     "Takaaki",
     "Saeki"
    ],
    [
     "Soumi",
     "Maiti"
    ],
    [
     "Shinnosuke",
     "Takamichi"
    ],
    [
     "Shinji",
     "Watanabe"
    ],
    [
     "Hiroshi",
     "Saruwatari"
    ]
   ],
   "title": "SpeechBERTScore: Reference-Aware Automatic Evaluation of Speech Generation Leveraging NLP Evaluation Metrics",
   "original": "1508",
   "order": 1013,
   "page_count": 5,
   "abstract": [
    "While subjective assessments have been the gold standard for evaluating speech generation, there is a growing need for objective metrics that are highly correlated with human subjective judgments due to their cost efficiency. This paper proposes reference-aware automatic evaluation methods for speech generation inspired by evaluation metrics in natural language processing. The proposed SpeechBERTScore computes the BERTScore for self-supervised dense speech features of the generated and reference speech, which can have different sequential lengths. We also propose SpeechBLEU and SpeechTokenDistance, which are computed on speech discrete tokens. The evaluations on synthesized speech show that our method correlates better with human subjective ratings than mel cepstral distortion and a recent mean opinion score prediction model. Also, they are effective in noisy speech evaluation and have cross-lingual applicability."
   ],
   "p1": 4943,
   "pn": 4947,
   "doi": "10.21437/Interspeech.2024-1508",
   "url": "interspeech_2024/saeki24_interspeech.html"
  },
  "zhu24_interspeech": {
   "authors": [
    [
     "Kang",
     "Zhu"
    ],
    [
     "Cunhang",
     "Fan"
    ],
    [
     "Jianhua",
     "Tao"
    ],
    [
     "Zhao",
     "Lv"
    ]
   ],
   "title": "Prompt Link Multimodal Fusion in Multimodal Sentiment Analysis",
   "original": "1512",
   "order": 958,
   "page_count": 5,
   "abstract": [
    "Multimodal sentiment analysis aims to analyze sentiment by integrating information from various modalities. Combining different modalities can be challenging due to their inherent differences in distance. While researchers employ complex methods to reduce distances, connecting multiple modalities remains limited. In this paper, we introduce the technique of prompt learning and propose the Prompt Link Multimodal Fusion (PLMF), which consists of three components: Channel Prompt Link (CPL), Spatial Prompt Link (SPL), and Fusion Result Constraints (FRC). CPL facilitates fine-grained sentiment feature linkage in the channel dimension, while SPL connects overall sentiment semantic information in the temporal dimension. Due to the randomness of connecting vectors, FRC is proposed to constrain the linkage toward the direction of optimal fusion results. Through the collaborative efforts of these three modules, PLMF achieves state-of-the-art results on three publicly available datasets."
   ],
   "p1": 4668,
   "pn": 4672,
   "doi": "10.21437/Interspeech.2024-1512",
   "url": "interspeech_2024/zhu24_interspeech.html"
  },
  "shih24b_interspeech": {
   "authors": [
    [
     "Min-Han",
     "Shih"
    ],
    [
     "Ho-Lam",
     "Chung"
    ],
    [
     "Yu-Chi",
     "Pai"
    ],
    [
     "Ming-Hao",
     "Hsu"
    ],
    [
     "Guan-Ting",
     "Lin"
    ],
    [
     "Shang-Wen",
     "Li"
    ],
    [
     "Hung-yi",
     "Lee"
    ]
   ],
   "title": "GSQA: An End-to-End Model for Generative Spoken Question Answering",
   "original": "1514",
   "order": 610,
   "page_count": 5,
   "abstract": [
    "In recent advancements in spoken question answering (SQA), endto-end models have made significant strides. However, previous research has primarily focused on extractive span selection. While this extractive-based approach is effective when answers are present directly within the input, it falls short in addressing abstractive questions, where answers are not directly extracted but inferred from the given information. To bridge this gap, we introduce the first end-to-end Generative Spoken Question Answering (GSQA) model that empowers the system to engage in abstractive reasoning. The challenge in training our GSQA model lies in the absence of a spoken abstractive QA dataset. We propose using text models for initialization and leveraging the extractive QA dataset to transfer knowledge from the text generative model to the spoken generative model. Experimental results indicate that our model surpasses the previous extractive model by 3% on extractive QA datasets. Furthermore, the GSQA model has only been fine-tuned on the spoken extractive QA dataset. Despite not having seen any spoken abstractive QA data, it can still closely match the performance of the cascade model. In conclusion, our GSQA model shows the potential to generalize to a broad spectrum of questions, thus further expanding the SQA capabilities of abstractive QA."
   ],
   "p1": 2970,
   "pn": 2974,
   "doi": "10.21437/Interspeech.2024-1514",
   "url": "interspeech_2024/shih24b_interspeech.html"
  },
  "zhao24f_interspeech": {
   "authors": [
    [
     "Yiyang",
     "Zhao"
    ],
    [
     "Shuai",
     "Wang"
    ],
    [
     "Guangzhi",
     "Sun"
    ],
    [
     "Zehua",
     "Chen"
    ],
    [
     "Chao",
     "Zhang"
    ],
    [
     "Mingxing",
     "Xu"
    ],
    [
     "Thomas Fang",
     "Zheng"
    ]
   ],
   "title": "Whisper-PMFA: Partial Multi-Scale Feature Aggregation for Speaker Verification using Whisper Models",
   "original": "1515",
   "order": 552,
   "page_count": 5,
   "abstract": [
    "In this paper, Whisper, a large-scale pre-trained model for automatic speech recognition, is proposed to apply to speaker verification. A partial multi-scale feature aggregation (PMFA) approach is proposed based on a subset of Whisper encoder blocks to derive highly discriminative speaker embeddings. Experimental results demonstrate that using the middle to later blocks of the Whisper encoder keeps more speaker information. On the VoxCeleb1 and CN-Celeb1 datasets, our system achieves 1.42% and 8.23% equal error rates (EERs) respectively, receiving 0.58% and 1.81% absolute EER reductions over the ECAPA-TDNN baseline, and 0.46% and 0.97% over the ResNet34 baseline. Furthermore, our results indicate that using Whisper models trained on multilingual data can effectively enhance the model's robustness across languages. Finally, the low-rank adaptation approach is evaluated, which reduces the trainable model parameters by approximately 45 times while only slightly increasing EER by 0.2%."
   ],
   "p1": 2680,
   "pn": 2684,
   "doi": "10.21437/Interspeech.2024-1515",
   "url": "interspeech_2024/zhao24f_interspeech.html"
  },
  "oura24_interspeech": {
   "authors": [
    [
     "Anna",
     "Oura"
    ],
    [
     "Hideaki",
     "Kikuchi"
    ],
    [
     "Tetsunori",
     "Kobayashi"
    ]
   ],
   "title": "Preprocessing for acoustic-to-articulatory inversion using real-time MRI movies of Japanese speech",
   "original": "1517",
   "order": 319,
   "page_count": 5,
   "abstract": [
    "Acoustic-to-articulatory inversion (AAI) estimates the articulatory movements by using acoustic speech signals. The traditional AAI relies on indirect estimation using articulatory models. However, recent advancements have proposed the use of machine learning models to directly output real-time MRI (rtMRI) movies. This study applied the existing model to rtMRI movies of Japanese speech to test its potential for achieving highly accurate estimations using the devised preprocessing methods. Preprocessing involves normalization of face alignment and filtering to remove extraneous regions. For objective evaluation, we measured the complex wavelet structural similarity (CW--SSIM). The results indicate that combining the normalization and filtering processes can produce smooth rtMRI movies that closely resemble the original (average CW--SSIM: LSTM, 0.795; BLSTM, 0.793). Therefore, the effectiveness of the preprocessing was demonstrated."
   ],
   "p1": 1550,
   "pn": 1554,
   "doi": "10.21437/Interspeech.2024-1517",
   "url": "interspeech_2024/oura24_interspeech.html"
  },
  "yang24m_interspeech": {
   "authors": [
    [
     "Yuting",
     "Yang"
    ],
    [
     "Guodong",
     "Ma"
    ],
    [
     "Yuke",
     "Li"
    ],
    [
     "Binbin",
     "Du"
    ],
    [
     "Haoqi",
     "Zhu"
    ],
    [
     "Liang",
     "Ruan"
    ]
   ],
   "title": "Learning from Back Chunks: Acquiring More Future Knowledge for Streaming ASR Models via Self Distillation",
   "original": "1527",
   "order": 916,
   "page_count": 5,
   "abstract": [
    "The performance of streaming automatic speech recognition (ASR) is often inferior to that of non-streaming speech recognition due to the absence of complete contextual information. However, we cannot optimize the model by merely accessing more future frames, as this would lead to considerable latency. In this paper, we propose Future-aware Transformer (FaT) that models long-distance future contextual dependencies by transferring information from later chunks to former chunks through look-ahead windows. Specifically, the chunk-based context is used to encode audio sequence features. On this basis, the look-ahead window provides more context information for each chunk and acts as a bridge to progressively transfer long-distance future information from later chunks to earlier ones via a future-aware distillation mechanism. Experiments on AISHELL-1 and AISHELL-2 demonstrate that the proposed method achieves superior accuracy and better streaming latency than several strong baselines."
   ],
   "p1": 4458,
   "pn": 4462,
   "doi": "10.21437/Interspeech.2024-1527",
   "url": "interspeech_2024/yang24m_interspeech.html"
  },
  "song24b_interspeech": {
   "authors": [
    [
     "Yakun",
     "Song"
    ],
    [
     "Zhuo",
     "Chen"
    ],
    [
     "Xiaofei",
     "Wang"
    ],
    [
     "Ziyang",
     "Ma"
    ],
    [
     "Guanrou",
     "Yang"
    ],
    [
     "Xie",
     "Chen"
    ]
   ],
   "title": "TacoLM: GaTed Attention Equipped Codec Language Model are Efficient Zero-Shot Text to Speech Synthesizers",
   "original": "1531",
   "order": 911,
   "page_count": 5,
   "abstract": [
    "Neural codec language model (LM) has demonstrated strong capability in zero-shot text-to-speech (TTS) synthesis. However, the codec LM often suffers from limitations in inference speed and stability, due to its auto-regressive nature and implicit alignment between text and audio. In this work, to handle these challenges, we introduce a new variant of neural codec LM, namely TacoLM. Specifically, TacoLM introduces a gated attention mechanism to improve the training and inference efficiency and reduce the model size. Meanwhile, an additional gated cross-attention layer is included for each decoder layer, which improves the efficiency and content accuracy of the synthesized speech. In the evaluation of the Librispeech corpus, the proposed TacoLM achieves a better word error rate, speaker similarity, and mean opinion score, with 90% fewer parameters and 5.2 times speed up, compared with VALL-E.  Demo and code is available at https://ereboas.github.io/TacoLM/."
   ],
   "p1": 4433,
   "pn": 4437,
   "doi": "10.21437/Interspeech.2024-1531",
   "url": "interspeech_2024/song24b_interspeech.html"
  },
  "xu24d_interspeech": {
   "authors": [
    [
     "Yaoxun",
     "Xu"
    ],
    [
     "Shi-Xiong",
     "Zhang"
    ],
    [
     "Jianwei",
     "Yu"
    ],
    [
     "Zhiyong",
     "Wu"
    ],
    [
     "Dong",
     "Yu"
    ]
   ],
   "title": "Comparing Discrete and Continuous Space LLMs for Speech Recognition",
   "original": "1533",
   "order": 517,
   "page_count": 5,
   "abstract": [
    "This paper investigates discrete and continuous speech representations in Large Language Model (LLM)-based Automatic Speech Recognition (ASR), organizing them by feature continuity and training approach into four categories: supervised and unsupervised for both discrete and continuous types. We further classify LLMs based on their input and autoregressive feedback into continuous and discrete-space models. Using specialized encoders and comparative analysis with a Joint-Training-From-Scratch Language Model (JTFS LM) and pre-trained LLaMA2-7b, we provide a detailed examination of their effectiveness. Our work marks the first extensive comparison of speech representations in LLM-based ASR and explores various modeling techniques. We present an open-sourced achievement of a state-of-the-art Word Error Rate (WER) of 1.69% on LibriSpeech using a HuBERT encoder, offering valuable insights for advancing ASR and natural language processing (NLP) research."
   ],
   "p1": 2509,
   "pn": 2513,
   "doi": "10.21437/Interspeech.2024-1533",
   "url": "interspeech_2024/xu24d_interspeech.html"
  },
  "yin24b_interspeech": {
   "authors": [
    [
     "Chun",
     "Yin"
    ],
    [
     "Tai-Shih",
     "Chi"
    ],
    [
     "Yu",
     "Tsao"
    ],
    [
     "Hsin-Min",
     "Wang"
    ]
   ],
   "title": "SVSNet+: Enhancing Speaker Voice Similarity Assessment Models with Representations from Speech Foundation Models",
   "original": "1540",
   "order": 248,
   "page_count": 5,
   "abstract": [
    "Representations from pre-trained speech foundation models (SFMs) have shown impressive performance in many downstream tasks. However, the potential benefits of incorporating pre-trained SFM representations into speaker voice similarity assessment have not been thoroughly investigated. In this paper, we propose SVSNet+, a model that integrates pre-trained SFM representations to improve performance in assessing speaker voice similarity. Experimental results on the Voice Conversion Challenge 2018 and 2020 datasets show that SVSNet+ incorporating WavLM representations shows significant improvements compared to baseline models. In addition, while fine-tuning WavLM with a small dataset of the downstream task does not improve performance, using the same dataset to learn a weighted-sum representation of WavLM can substantially improve performance. Furthermore, when WavLM is replaced by other SFMs, SVSNet+ still outperforms the baseline models and exhibits strong generalization ability."
   ],
   "p1": 1195,
   "pn": 1199,
   "doi": "10.21437/Interspeech.2024-1540",
   "url": "interspeech_2024/yin24b_interspeech.html"
  },
  "roesler24_interspeech": {
   "authors": [
    [
     "Oliver",
     "Roesler"
    ],
    [
     "Jackson",
     "Liscombe"
    ],
    [
     "Michael",
     "Neumann"
    ],
    [
     "Hardik",
     "Kothare"
    ],
    [
     "Abhishek",
     "Hosamath"
    ],
    [
     "Lakshmi",
     "Arbatti"
    ],
    [
     "Doug",
     "Habberstad"
    ],
    [
     "Christiane",
     "Suendermann-Oeft"
    ],
    [
     "Meredith",
     "Bartlett"
    ],
    [
     "Cathy",
     "Zhang"
    ],
    [
     "Nikhil",
     "Sukhdev"
    ],
    [
     "Kolja",
     "Wilms"
    ],
    [
     "Anusha",
     "Badathala"
    ],
    [
     "Sandrine",
     "Istas"
    ],
    [
     "Steve",
     "Ruhmel"
    ],
    [
     "Bryan",
     "Hansen"
    ],
    [
     "Madeline",
     "Hannan"
    ],
    [
     "David",
     "Henley"
    ],
    [
     "Arthur",
     "Wallace"
    ],
    [
     "Ira",
     "Shoulson"
    ],
    [
     "David",
     "Suendermann-Oeft"
    ],
    [
     "Vikram",
     "Ramanarayanan"
    ]
   ],
   "title": "Towards Scalable Remote Assessment of Mild Cognitive Impairment Via Multimodal Dialog",
   "original": "1541",
   "order": 406,
   "page_count": 5,
   "abstract": [
    "Early assessment of mild cognitive impairment (MCI) has the potential to expedite interventions and slow disease progress for people at risk of developing dementia. We investigate the feasibility of administering remote assessments of speech, orofacial and cognitive function to an elderly population with MCI via a cloud-based conversational remote monitoring platform, and the utility of automatically extracted multimodal biomarkers and self-reported problems in identifying MCI patients. We analyzed data from 90 MCI patients and 91 controls who each completed two assessments. 90% of participants reported excellent engagement and liked their overall user experience. Furthermore, combining multiple facial, speech and cognitive markers performed best at distinguishing MCI patients from controls with an AUC of 0.75 using a support vector machine classifier. Finally, we found that MCI patients reported significantly more problems related to memory, falls, anxiety and speech than controls."
   ],
   "p1": 1985,
   "pn": 1989,
   "doi": "10.21437/Interspeech.2024-1541",
   "url": "interspeech_2024/roesler24_interspeech.html"
  },
  "kusunoki24_interspeech": {
   "authors": [
    [
     "Nahomi",
     "Kusunoki"
    ],
    [
     "Yosuke",
     "Higuchi"
    ],
    [
     "Tetsuji",
     "Ogawa"
    ],
    [
     "Tetsunori",
     "Kobayashi"
    ]
   ],
   "title": "Hierarchical Multi-Task Learning with CTC and Recursive Operation",
   "original": "1542",
   "order": 587,
   "page_count": 5,
   "abstract": [
    "We present hierarchical recursive CTC (HR-CTC), an effective hierarchical multi-task learning (HMTL) model for end-to-end automatic speech recognition (ASR). HMTL enables a model to learn suitable intermediate representations for predicting high-level and sparse targets (e.g., words). This is achieved by applying auxiliary CTC losses to intermediate layers of the model, which are calculated using lower-level targets (e.g., phonemes or smaller subwords). In this work, we propose to enhance the hierarchical generation capability in HMTL by designing a recursive structure that iteratively uses the same model layers to refine intermediate predictions. These improved predictions are used to explicitly condition the deeper model layers, thereby facilitating more accurate predictions at the higher level. Experimental results show that HR-CTC outperforms conventional HMTL models across various ASR tasks, providing an additional benefit of balancing accuracy and inference speed."
   ],
   "p1": 2855,
   "pn": 2859,
   "doi": "10.21437/Interspeech.2024-1542",
   "url": "interspeech_2024/kusunoki24_interspeech.html"
  },
  "szekely24_interspeech": {
   "authors": [
    [
     "Eva",
     "Szekely"
    ],
    [
     "Maxwell",
     "Hope"
    ]
   ],
   "title": "An inclusive approach to creating a palette of synthetic voices for gender diversity",
   "original": "1543",
   "order": 630,
   "page_count": 5,
   "abstract": [
    "Mainstream text-to-speech (TTS) technologies predominantly rely on binary, cisgender speech, failing to adequately represent the diversity of gender expansive (e.g., transgender and/or nonbinary) people. This poses challenges, particularly for users of Speech Generating Devices (SGDs) seeking TTS voices that authentically reflect their identity and desired expressive nuances. This paper introduces a novel approach for constructing a palette of controllable gender-expansive TTS voices using recordings from 14 gender-expansive speakers. We employ Constrained PCA to extract gender-independent speaker identity vectors from x-vectors, using acoustic Vocal Tract Length (aVTL) as a known component. The result is applied as a speaker embedding in neural TTS, allowing control over the aVTL and several emergent properties captured as a representation of the vocal space across speakers. In addition to quantitative metrics, we present a community evaluation conducted by nonbinary SGD users."
   ],
   "p1": 3070,
   "pn": 3074,
   "doi": "10.21437/Interspeech.2024-1543",
   "url": "interspeech_2024/szekely24_interspeech.html"
  },
  "kim24o_interspeech": {
   "authors": [
    [
     "Dail",
     "Kim"
    ],
    [
     "Da-Hee",
     "Yang"
    ],
    [
     "Donghyun",
     "Kim"
    ],
    [
     "Joon-Hyuk",
     "Chang"
    ],
    [
     "Jeonghwan",
     "Choi"
    ],
    [
     "Moa",
     "Lee"
    ],
    [
     "Jaemo",
     "Yang"
    ],
    [
     "Han-gil",
     "Moon"
    ]
   ],
   "title": "Guided conditioning with predictive network on score-based diffusion model for speech enhancement",
   "original": "1545",
   "order": 247,
   "page_count": 5,
   "abstract": [
    "Although diffusion-based speech enhancement (SE) models have emerged, they exhibit lower ability in noise removal than other predictive-based SE models. This reflects a trade-off between generative models, which are capable of producing more natural speech based on estimated target distribution, and predictive models, which are more effective in noise removal. To mitigate this trade-off, we propose a novel conditioning method for score-based diffusion models. The proposed approach involves guiding the diffusion model with a pretrained predictive model without joint training, thereby enabling enhanced speech to offer the proper direction to the diffusion model. The effectiveness of the proposed method is highlighted by outperforming the baseline method, with only half the number of sampling steps."
   ],
   "p1": 1190,
   "pn": 1194,
   "doi": "10.21437/Interspeech.2024-1545",
   "url": "interspeech_2024/kim24o_interspeech.html"
  },
  "take24_interspeech": {
   "authors": [
    [
     "Osamu",
     "Take"
    ],
    [
     "Shinnosuke",
     "Takamichi"
    ],
    [
     "Kentaro",
     "Seki"
    ],
    [
     "Yoshiaki",
     "Bando"
    ],
    [
     "Hiroshi",
     "Saruwatari"
    ]
   ],
   "title": "SaSLaW: Dialogue Speech Corpus with Audio-visual Egocentric Information Toward Environment-adaptive Dialogue Speech Synthesis",
   "original": "1554",
   "order": 381,
   "page_count": 5,
   "abstract": [
    "This paper presents SaSLaW, a spontaneous dialogue speech corpus containing synchronous recordings of what speakers speak, listen to, and watch. Humans consider the diverse environmental factors and then control the features of their utterances in face-to-face voice communications. Spoken dialogue systems capable of this adaptation to these audio environments enable natural and seamless communications. SaSLaW was developed to model human-speech adjustment for audio environments via first-person audio-visual perceptions in spontaneous dialogues. We propose the construction methodology of SaSLaW and display the analysis result of the corpus. We additionally conducted an experiment to develop text-to-speech models using SaSLaW and evaluate their performance of adaptations to audio environments. The results indicate that models incorporating hearing-audio data output more plausible speech tailored to diverse audio environments than the vanilla text-to-speech model."
   ],
   "p1": 1860,
   "pn": 1864,
   "doi": "10.21437/Interspeech.2024-1554",
   "url": "interspeech_2024/take24_interspeech.html"
  },
  "li24ba_interspeech": {
   "authors": [
    [
     "Hanzhao",
     "Li"
    ],
    [
     "Liumeng",
     "Xue"
    ],
    [
     "Haohan",
     "Guo"
    ],
    [
     "Xinfa",
     "Zhu"
    ],
    [
     "Yuanjun",
     "Lv"
    ],
    [
     "Lei",
     "Xie"
    ],
    [
     "Yunlin",
     "Chen"
    ],
    [
     "Hao",
     "Yin"
    ],
    [
     "Zhifei",
     "Li"
    ]
   ],
   "title": "Single-Codec: Single-Codebook Speech Codec towards High-Performance Speech Generation",
   "original": "1559",
   "order": 694,
   "page_count": 5,
   "abstract": [
    "The multi-codebook speech codec enables the application of large language models (LLM) in TTS but bottlenecks efficiency and robustness due to multi-sequence prediction. To avoid this obstacle, we propose Single-Codec, a singlecodebook single-sequence codec, which employs a disentangled VQ-VAE to decouple speech into a time-invariant embedding and a phonetically-rich discrete sequence. Furthermore, the encoder is enhanced with 1) contextual modeling with a BLSTM module to exploit the temporal information, 2) a hybrid sampling module to alleviate distortion from upsampling and downsampling, and 3) a resampling module to encourage discrete units to carry more phonetic information. Compared with multi-codebook codecs, e.g., EnCodec and TiCodec, Single- Codec demonstrates higher reconstruction quality with a lower bandwidth of only 304bps. The effectiveness of Single-Code is further validated by LLM-TTS experiments, showing improved naturalness and intelligibility."
   ],
   "p1": 3390,
   "pn": 3394,
   "doi": "10.21437/Interspeech.2024-1559",
   "url": "interspeech_2024/li24ba_interspeech.html"
  },
  "heo24_interspeech": {
   "authors": [
    [
     "Woon-Haeng",
     "Heo"
    ],
    [
     "Joongyu",
     "Maeng"
    ],
    [
     "Yoseb",
     "Kang"
    ],
    [
     "Namhyun",
     "Cho"
    ]
   ],
   "title": "Centroid Estimation with Transformer-Based Speaker Embedder for Robust Target Speaker Extraction",
   "original": "1560",
   "order": 891,
   "page_count": 5,
   "abstract": [
    "Target speaker extraction (TSE) is a technique for separating the target speaker from mixed speech using speaker embedding. However, speaker embeddings may contain, in addition to speaker information, text dependent information and environmental information, such as noise, microphone characteristics, and reverberation, which can decrease TSE performance, especially when the enrollment and target utterances are in different environments. To address this issue, we propose a Transformer-based embedder for centroid estimation, and a role division training method to enhance the training stability of the TSE separator. This embedder estimates the speaker centroid from the enrollment utterance, aiding the separator in extracting the target speaker. The proposed methods considerably improve speech quality and speech recognition performance compared to the baseline."
   ],
   "p1": 4333,
   "pn": 4337,
   "doi": "10.21437/Interspeech.2024-1560",
   "url": "interspeech_2024/heo24_interspeech.html"
  },
  "wu24j_interspeech": {
   "authors": [
    [
     "Xinyi",
     "Wu"
    ],
    [
     "Changqing",
     "Xu"
    ],
    [
     "Nan",
     "Li"
    ],
    [
     "Rongfeng",
     "Su"
    ],
    [
     "Lan",
     "Wang"
    ],
    [
     "Nan",
     "Yan"
    ]
   ],
   "title": "Depression Enhances Internal Inconsistency between Spoken and Semantic Emotion: Evidence from the Analysis of Emotion Expression in Conversation",
   "original": "1562",
   "order": 868,
   "page_count": 5,
   "abstract": [
    "Spoken emotion and semantic emotion are two components of emotion expression. In human conversation, emotions expressed by these two modalities are similar in healthy individuals. However, rich evidence documents that depression might affect emotional expression. Nevertheless, the consistency between spoken and semantic emotion in depressed patients has rarely been studied previously. In the present study, we investigated the consistency between emotions expressed by acoustical features and text content in depressed and healthy individuals during natural conversations. It was found that depressed patients tended to talk about negative topics in a neutral emotional tone and talk about neutral or positive topics in a depressed tone. These findings suggest that depression not only affects the emotion expression of a single modality but also results in an inconsistency between emotions expressed by these two modalities."
   ],
   "p1": 4219,
   "pn": 4223,
   "doi": "10.21437/Interspeech.2024-1562",
   "url": "interspeech_2024/wu24j_interspeech.html"
  },
  "xiong24_interspeech": {
   "authors": [
    [
     "Yan",
     "Xiong"
    ],
    [
     "Visar",
     "Berisha"
    ],
    [
     "Julie",
     "Liss"
    ],
    [
     "Chaitali",
     "Chakrabarti"
    ]
   ],
   "title": "Improving Speech-Based Dysarthria Detection using Multi-task Learning with Gradient Projection",
   "original": "1563",
   "order": 182,
   "page_count": 5,
   "abstract": [
    "Speech analytic models based on deep learning are popular in clinical diagnostics. However, constraints on clinical data collection and sharing place limits on available dataset sizes, which adversely impacts trained model performance. Multi-task learning (MTL) has been utilized to mitigate the effect of limited sample size by jointly training on multiple tasks that are considered to be related. However, discrepancies between clinical and non-clinical tasks can reduce MTL efficiency and can even cause it to fail, especially when there are gradient conflicts. In this paper, we enhance the performance of dysarthria detection by using MTL with an auxiliary task of learning speaker embeddings. We propose a task-specific gradient projection method to overcome gradient conflicts. Our evaluation shows that the proposed MTL paradigm outperforms both single-task learning and conventional MTL under different data availability settings."
   ],
   "p1": 902,
   "pn": 906,
   "doi": "10.21437/Interspeech.2024-1563",
   "url": "interspeech_2024/xiong24_interspeech.html"
  },
  "tannander24_interspeech": {
   "authors": [
    [
     "Christina",
     "Tånnander"
    ],
    [
     "Shivam",
     "Mehta"
    ],
    [
     "Jonas",
     "Beskow"
    ],
    [
     "Jens",
     "Edlund"
    ]
   ],
   "title": "Beyond graphemes and phonemes: continuous phonological features in neural text-to-speech synthesis",
   "original": "1565",
   "order": 579,
   "page_count": 5,
   "abstract": [
    "We introduce continuous phonological features as input to TTS with the dual objective of more precise control over phonological aspects and better potential for exploration of latent features in TTS models for speech science purposes. In our framework, the TTS is conditioned on continuous values between 0.0 and 1.0, where each phoneme has a specified position on each feature axis. We chose 11 features to represent US English and trained a voice with Matcha-TTS. Effectiveness was assessed by investigating two selected features in two ways: through a categorical perception experiment confirming the expected alignment of feature positions and phoneme perception, and through analysis of acoustic correlates confirming a gradual, monotonic change of acoustic features consistent with changes in the phonemic input features.  "
   ],
   "p1": 2815,
   "pn": 2819,
   "doi": "10.21437/Interspeech.2024-1565",
   "url": "interspeech_2024/tannander24_interspeech.html",
   "erratum": "<p>The illustration in figure 2 shows the F1 values in reversed order. The corrected figure is shown here:\n\n<figure>\n    <img src=\"./errata/tannander24_interspeech/figure2.svg\" style=\"width:50%;\" />\n    <figcaption><u>Figure 2:</u> Counts of perception of / ɪ , ɛ, æ/ (left y-axis) for each of the 9 settings (x-axis) of V-HEIGHT. Training target phonemes labels are aligned with their rank order. Average F1 value is shown as black dots (right y-axis).</figcaption>\n</figure>\n</p>"
  },
  "wu24k_interspeech": {
   "authors": [
    [
     "Peter",
     "Wu"
    ],
    [
     "Ryan",
     "Kaveh"
    ],
    [
     "Raghav",
     "Nautiyal"
    ],
    [
     "Christine",
     "Zhang"
    ],
    [
     "Albert",
     "Guo"
    ],
    [
     "Anvitha",
     "Kachinthaya"
    ],
    [
     "Tavish",
     "Mishra"
    ],
    [
     "Bohan",
     "Yu"
    ],
    [
     "Alan W",
     "Black"
    ],
    [
     "Rikky",
     "Muller"
    ],
    [
     "Gopala Krishna",
     "Anumanchipalli"
    ]
   ],
   "title": "Towards EMG-to-Speech with Necklace Form Factor",
   "original": "1568",
   "order": 82,
   "page_count": 5,
   "abstract": [
    "Electrodes for decoding speech from electromyography (EMG) are typically placed on the face, requiring adhesives that are inconvenient and skin-irritating if used regularly. We explore a different device form factor, where dry electrodes are placed around the neck instead. 11-word, multi-speaker voiced EMG classifiers trained on data recorded with this device achieve 92.7% accuracy. Ablation studies reveal the importance of having more than two electrodes on the neck, and phonological analyses reveal similar classification confusions between neck-only and neck-and-face form factors. Finally, speech-EMG correlation experiments demonstrate a linear relationship between many EMG spectrogram frequencies and self-supervised speech representation dimensions."
   ],
   "p1": 402,
   "pn": 406,
   "doi": "10.21437/Interspeech.2024-1568",
   "url": "interspeech_2024/wu24k_interspeech.html"
  },
  "nguyen24b_interspeech": {
   "authors": [
    [
     "Tuan",
     "Nguyen"
    ],
    [
     "Huy Dat",
     "Tran"
    ]
   ],
   "title": "LingWav2Vec2: Linguistic-augmented wav2vec 2.0 for Vietnamese Mispronunciation Detection",
   "original": "1569",
   "order": 486,
   "page_count": 5,
   "abstract": [
    "Pronunciation error detection algorithms rely on both acoustic and linguistic information to identify errors. However, these algorithms face challenges due to limited training data, often just a few hours, insufficient for building robust phoneme recognition models. This has led to the adoption of self-supervised learning models like wav2vec 2.0. We propose an innovative approach that combine canonical text and audio inputs to balance the trade-off between accurate phoneme recognition performance and pronunciation scoring. This is done by feeding audio-encoded and normalized canonical phoneme embedding into a linguistic encoder including multi-head attention (MHA) layer and specifically designed feed forward module (FFN). Our system, with only 4.3 million parameters on top of pretrained wav2vec 2.0, achieved top-1 performance at the VLSP Vietnamese Mispronunciation Detection 2023 challenge with 9.72% relative improvement of F1 score over the previous state-of-the-art."
   ],
   "p1": 2355,
   "pn": 2359,
   "doi": "10.21437/Interspeech.2024-1569",
   "url": "interspeech_2024/nguyen24b_interspeech.html"
  },
  "li24ca_interspeech": {
   "authors": [
    [
     "Jinyu",
     "Li"
    ],
    [
     "Leonardo",
     "Lancia"
    ]
   ],
   "title": "A multimodal approach to study the nature of coordinative patterns underlying speech rhythm",
   "original": "1571",
   "order": 81,
   "page_count": 5,
   "abstract": [
    "Research on speech rhythm suggests that coordination between syllable and supra-syllabic prominence defines rhythmic differences between languages. This study investigates the role of language-specific phonological processes in the emergence of language-specific coordinative patterns underlying speech rhythm, which result from sensorimotor processes during speech production. French and German speakers repeated disyllabic utterances simultaneously with pre-recorded productions of these utterances. We manipulated the location of prominence in the stimuli and asked speakers to reproduce the heard prominence pattern. Despite the surface cross-linguistic similarity of the recorded productions between the speakers of the two languages, the analysis of laryngeal activity revealed language-dependent coordination between syllables and prominence production, suggesting an influence of language-specific phonology on speech rhythm control even when producing unfamiliar prosodic patterns."
   ],
   "p1": 397,
   "pn": 401,
   "doi": "10.21437/Interspeech.2024-1571",
   "url": "interspeech_2024/li24ca_interspeech.html"
  },
  "noroozi24_interspeech": {
   "authors": [
    [
     "Vahid",
     "Noroozi"
    ],
    [
     "Zhehuai",
     "Chen"
    ],
    [
     "Somshubra",
     "Majumdar"
    ],
    [
     "Steve",
     "Huang"
    ],
    [
     "Jagadeesh",
     "Balam"
    ],
    [
     "Boris",
     "Ginsburg"
    ]
   ],
   "title": "Instruction Data Generation and Unsupervised Adaptation for Speech Language Models",
   "original": "1575",
   "order": 833,
   "page_count": 5,
   "abstract": [
    "In this paper, we propose three methods for generating synthetic samples to train and evaluate multimodal large language models capable of processing both text and speech inputs. Addressing the scarcity of samples containing both modalities, synthetic data generation emerges as a crucial strategy to enhance the performance of such systems and facilitate the modeling of cross-modal relationships between the speech and text domains. Our process employs large language models to generate textual components and text-to-speech systems to generate speech components. The proposed methods offer a practical and effective means to expand the training dataset for these models. Experimental results show progress in achieving an integrated understanding of text and speech. We also highlight the potential of using unlabeled speech data to generate synthetic samples comparable in quality to those with available transcriptions, enabling the expansion of these models to more languages. "
   ],
   "p1": 4049,
   "pn": 4053,
   "doi": "10.21437/Interspeech.2024-1575",
   "url": "interspeech_2024/noroozi24_interspeech.html"
  },
  "dang24b_interspeech": {
   "authors": [
    [
     "Shaoxiang",
     "Dang"
    ],
    [
     "Tetsuya",
     "Matsumoto"
    ],
    [
     "Yoshinori",
     "Takeuchi"
    ],
    [
     "Takashi",
     "Tsuboi"
    ],
    [
     "Yasuhiro",
     "Tanaka"
    ],
    [
     "Daisuke",
     "Nakatsubo"
    ],
    [
     "Satoshi",
     "Maesawa"
    ],
    [
     "Ryuta",
     "Saito"
    ],
    [
     "Masahisa",
     "Katsuno"
    ],
    [
     "Hiroaki",
     "Kudo"
    ]
   ],
   "title": "Developing vocal system impaired patient-aimed voice quality assessment approach using ASR representation-included multiple features",
   "original": "1577",
   "order": 504,
   "page_count": 5,
   "abstract": [
    "The potential of deep learning in clinical speech processing is immense, yet the hurdles of limited and imbalanced clinical data samples loom large. This article addresses these challenges by showcasing the utilization of automatic speech recognition and self-supervised learning representations, pre-trained on extensive datasets of normal speech. This innovative approach aims to estimate voice quality of patients with impaired vocal systems. Experiments involve checks on PVQD dataset, covering various causes of vocal system damage in English, and a Japanese dataset focusing on patients with Parkinson's disease before and after undergoing subthalamic nucleus deep brain stimulation (STN-DBS) surgery. The results on PVQD reveal a notable correlation (>0.8 on PCC) and an extraordinary accuracy (<0.5 on MSE) in predicting Grade, Breathy, and Asthenic indicators. Meanwhile, progress has been achieved in predicting the voice quality of patients in the context of STN-DBS."
   ],
   "p1": 2445,
   "pn": 2449,
   "doi": "10.21437/Interspeech.2024-1577",
   "url": "interspeech_2024/dang24b_interspeech.html"
  },
  "li24da_interspeech": {
   "authors": [
    [
     "Xuyuan",
     "Li"
    ],
    [
     "Zengqiang",
     "Shang"
    ],
    [
     "Peiyang",
     "Shi"
    ],
    [
     "Hua",
     "Hua"
    ],
    [
     "Ta",
     "Li"
    ],
    [
     "Pengyuan",
     "Zhang"
    ]
   ],
   "title": "Expressive paragraph text-to-speech synthesis with multi-step variational autoencoder",
   "original": "1581",
   "order": 372,
   "page_count": 5,
   "abstract": [
    "Neural networks have been able to generate high-quality single-sentence speech. However, it remains a challenge concerning audio-book speech synthesis due to the intra-paragraph correlation of semantic and acoustic features as well as variable styles. In this paper, we propose a highly expressive paragraph speech synthesis system with a multi-step variational autoencoder, called EP-MSTTS. EP-MSTTS is the first VITS-based paragraph speech synthesis model and models the variable style of paragraph speech at five levels: frame, phoneme, word, sentence, and paragraph. We also propose a series of improvements to enhance the performance of this hierarchical model. In addition, we directly train EP-MSTTS on speech sliced by paragraph rather than sentence. Experiment results on the single-speaker French audiobook corpus released at Blizzard Challenge 2023 show EP-MSTTS obtains better performance than baseline models."
   ],
   "p1": 1815,
   "pn": 1819,
   "doi": "10.21437/Interspeech.2024-1581",
   "url": "interspeech_2024/li24da_interspeech.html"
  },
  "mehta24b_interspeech": {
   "authors": [
    [
     "Shivam",
     "Mehta"
    ],
    [
     "Harm",
     "Lameris"
    ],
    [
     "Rajiv",
     "Punmiya"
    ],
    [
     "Jonas",
     "Beskow"
    ],
    [
     "Eva",
     "Szekely"
    ],
    [
     "Gustav Eje",
     "Henter"
    ]
   ],
   "title": "Should you use a probabilistic duration model in TTS? Probably! Especially for spontaneous speech",
   "original": "1582",
   "resource": "https://doi.org/10.5281/zenodo.12760900",
   "order": 472,
   "page_count": 5,
   "abstract": [
    "Converting input symbols to output audio in TTS requires modelling the durations of speech sounds. Leading non-autoregressive (NAR) TTS models treat duration modelling as a regression problem. The same utterance is then spoken with identical timings every time, unlike when a human speaks. Probabilistic models of duration have been proposed, but there is mixed evidence of their benefits. However, prior studies generally only consider speech read aloud, and ignore spontaneous speech, despite the latter being both a more common and a more variable mode of speaking. We compare the effect of conventional deterministic duration modelling to durations sampled from a powerful probabilistic model based on conditional flow matching (OT-CFM), in three different NAR TTS approaches: regression-based, deep generative, and end-to-end. Across four different corpora, stochastic duration modelling improves probabilistic NAR TTS approaches, especially for spontaneous speech."
   ],
   "p1": 2285,
   "pn": 2289,
   "doi": "10.21437/Interspeech.2024-1582",
   "url": "interspeech_2024/mehta24b_interspeech.html"
  },
  "sun24d_interspeech": {
   "authors": [
    [
     "Haitong",
     "Sun"
    ],
    [
     "Jaehyun",
     "Choi"
    ],
    [
     "Nobuaki",
     "Minematsu"
    ],
    [
     "Daisuke",
     "Saito"
    ]
   ],
   "title": "Acceleration of Posteriorgram-based DTW by Distilling the Class-to-class Distances Encoded in the Classifier Used to Calculate Posteriors",
   "original": "1583",
   "order": 615,
   "page_count": 5,
   "abstract": [
    "In media technology, comparison of a sequential data with another is a fundamental technique for many applications, and dynamic time warping is often conducted. Conventionally, two sequences of raw features were compared, and recently, more abstract representations are used, which are obtained with deep neural networks. One of these representations is posteriorgram, where each frame is composed of n-dimensional class posteriors, and frame-to-frame distance is often calculated using a divergence metric such as Bhattacharyya distance. In this study, a novel method is proposed to distill the class-to-class distances encoded in any classifier used to calculate posteriors, and the distances are effectively used to accelerate posteriorgram DTW by approximating it as DTW between two sequences of most likely classes. Our proposal shows a much faster and even stabler performance and guarantees no requirement of calculating frame-to-frame distances on the fly during DTW processing."
   ],
   "p1": 2995,
   "pn": 2999,
   "doi": "10.21437/Interspeech.2024-1583",
   "url": "interspeech_2024/sun24d_interspeech.html"
  },
  "li24ea_interspeech": {
   "authors": [
    [
     "Xu",
     "Li"
    ],
    [
     "Qirui",
     "Wang"
    ],
    [
     "Xiaoyu",
     "Liu"
    ]
   ],
   "title": "MaskSR: Masked Language Model for Full-band Speech Restoration",
   "original": "1584",
   "order": 470,
   "page_count": 5,
   "abstract": [
    "Speech restoration aims at restoring high quality speech in the presence of a diverse set of distortions. Although several deep learning paradigms have been studied for this task, the power of the recently emerging language models has not been fully explored. In this paper, we propose MaskSR, a masked language model capable of restoring full-band 44.1 kHz speech jointly considering noise, reverb, clipping, and low bandwidth. MaskSR works with discrete acoustic tokens extracted using a pre-trained neural codec. During training, MaskSR is optimized to predict randomly masked tokens extracted from the high quality target speech, conditioned on the corrupted speech with various distortions. During inference, MaskSR reconstructs the target speech tokens with efficient iterative sampling. Extensive experiments show that MaskSR obtains competitive results on both the full-band speech restoration task and also on sub-tasks compared with a wide range of models."
   ],
   "p1": 2275,
   "pn": 2279,
   "doi": "10.21437/Interspeech.2024-1584",
   "url": "interspeech_2024/li24ea_interspeech.html"
  },
  "udupa24_interspeech": {
   "authors": [
    [
     "Sathvik",
     "Udupa"
    ],
    [
     "Jesuraj",
     "Bandekar"
    ],
    [
     "Saurabh",
     "Kumar"
    ],
    [
     "Deekshitha",
     "G"
    ],
    [
     "Sandhya",
     "B"
    ],
    [
     "Abhayjeet",
     "S"
    ],
    [
     "Savitha",
     "Murthy"
    ],
    [
     "Priyanka",
     "Pai"
    ],
    [
     "Srinivasa",
     "Raghavan"
    ],
    [
     "Raoul",
     "Nanavati"
    ],
    [
     "Prasanta Kumar",
     "Ghosh"
    ]
   ],
   "title": "Adapter pre-training for improved speech recognition in unseen domains using low resource adapter tuning of self-supervised models",
   "original": "1587",
   "order": 521,
   "page_count": 5,
   "abstract": [
    "Adapter tuning is an approach to fine-tune large neural network models on new tasks. These methods can be used to efficiently fine-tune large self-supervised learning (SSL) models for speech recognition tasks. In this work, we aim to perform improved low-resource adaptation of SSL features from source to target domain. Toward this, we experiment with adapter pre-training for Wav2Vec2-based models over different source and target configurations. We experiment over 3 datasets consisting 14 languages, including very low-resource languages. Further, we show the consistency of this method across different adapter dimensions and analyse the feature transformation due to the adapter pre-training process. With the proposed methods, we obtain over 10%-30% relative improvement in WER and CER with Viterbi decoding in 13 languages. Further, we obtain consistent performance gains using LM decoding on many of these languages."
   ],
   "p1": 2529,
   "pn": 2533,
   "doi": "10.21437/Interspeech.2024-1587",
   "url": "interspeech_2024/udupa24_interspeech.html"
  },
  "galvez24_interspeech": {
   "authors": [
    [
     "Daniel",
     "Galvez"
    ],
    [
     "Vladimir",
     "Bataev"
    ],
    [
     "Hainan",
     "Xu"
    ],
    [
     "Tim",
     "Kaldewey"
    ]
   ],
   "title": "Speed of Light Exact Greedy Decoding for RNN-T Speech Recognition Models on GPU",
   "original": "1591",
   "order": 57,
   "page_count": 5,
   "abstract": [
    "The vast majority of inference time for RNN Transducer (RNN-T) models today is spent on decoding. Current state-of-the-art RNN-T decoding implementations leave the GPU idle 80% of the time. Leveraging a new CUDA 12.4 feature, CUDA graph conditional nodes, we present an exact GPU-based implementation of greedy decoding for RNN-T models that eliminates this idle time. Our optimizations speed up a 1.1 billion parameter RNN-T model end-to-end by a factor of 2.5x. This technique can applied to the \"label looping\" alternative greedy decoding algorithm as well, achieving 1.7x and 1.4x end-to-end speedups when applied to 1.1 billion parameter RNN-T and Token and Duration Transducer models respectively. This work enables a 1.1 billion parameter RNN-T model to run only 16% slower than a similarly sized CTC model, contradicting the common belief that RNN-T models are not suitable for high throughput inference. The implementation is available in NVIDIA NeMo."
   ],
   "p1": 277,
   "pn": 281,
   "doi": "10.21437/Interspeech.2024-1591",
   "url": "interspeech_2024/galvez24_interspeech.html"
  },
  "weirich24_interspeech": {
   "authors": [
    [
     "Melanie",
     "Weirich"
    ],
    [
     "Daniel",
     "Duran"
    ],
    [
     "Stefanie",
     "Jannedy"
    ]
   ],
   "title": "Gender and age based f0-variation in the German Plapper Corpus",
   "original": "1592",
   "order": 322,
   "page_count": 5,
   "abstract": [
    "This study is the first exploration of data collected with the smartphone-app Plapper with which participants from Germany recorded themselves reading several sentences containing target sounds for future analyses of differences in fine phonetic detail and then donated their speech for inclusion to a large speech corpus. To this date, just short of 2.000 participants have contributed to this corpus. First analyses of differences in f0 on read speech from these German participants reveals an effect of age on mean f0 in females only and additional effects of self-rated femininity/masculinity (higher mean f0 in male and female speakers with higher self-rated femininity scores and vice versa). Also, there is an effect of region with speakers in the north of Germany inexplicably having lower mean f0 values than speakers from the other regions. Results leave room for speculation on the social meaning (what do speakers code and what do listeners interpret) of differences in f0."
   ],
   "p1": 1565,
   "pn": 1569,
   "doi": "10.21437/Interspeech.2024-1592",
   "url": "interspeech_2024/weirich24_interspeech.html"
  },
  "eungi24_interspeech": {
   "authors": [
    [
     "Han",
     "EunGi"
    ],
    [
     "Oh",
     "Hyun-Bin"
    ],
    [
     "Kim",
     "Sung-Bin"
    ],
    [
     "Corentin",
     "Nivelet Etcheberry"
    ],
    [
     "Suekyeong",
     "Nam"
    ],
    [
     "Janghoon",
     "Ju"
    ],
    [
     "Tae-Hyun",
     "Oh"
    ]
   ],
   "title": "Enhancing Speech-Driven 3D Facial Animation with Audio-Visual Guidance from Lip Reading Expert",
   "original": "1595",
   "order": 604,
   "page_count": 5,
   "abstract": [
    "Speech-driven 3D facial animation has recently garnered attention due to its cost-effective usability in multimedia production. However, most current advances overlook the intelligibility of lip movements, limiting the realism of facial expressions. In this paper, we introduce a method for speech-driven 3D facial animation to generate accurate lip movements, proposing an audio-visual multimodal perceptual loss. This loss provides guidance to train the speech-driven 3D facial animators to generate plausible lip motions aligned with the spoken transcripts. Furthermore, to incorporate the proposed audio-visual perceptual loss, we devise an audio-visual lip reading expert leveraging its prior knowledge about correlations between speech and lip motions. We validate the effectiveness of our approach through broad experiments, showing noticeable improvements in lip synchronization and lip readability performance. Codes are available at https://3d-talking-head-avguide.github.io/."
   ],
   "p1": 2940,
   "pn": 2944,
   "doi": "10.21437/Interspeech.2024-1595",
   "url": "interspeech_2024/eungi24_interspeech.html"
  },
  "chen24r_interspeech": {
   "authors": [
    [
     "Liangwei",
     "Chen"
    ],
    [
     "Xiren",
     "Zhou"
    ],
    [
     "Qiang",
     "Tu"
    ],
    [
     "Huanhuan",
     "Chen"
    ]
   ],
   "title": "Enhancing Speech and Music Discrimination Through the Integration of Static and Dynamic Features",
   "original": "1596",
   "order": 888,
   "page_count": 5,
   "abstract": [
    "Audio is inherently temporal data, where features extracted from each segment evolve over time, yielding dynamic traits. These dynamics, relative to the acoustic characteristics inherent in raw audio features, primarily serve as complementary aids for audio classification. This paper employs the reservoir computing model to fit the audio feature sequences efficiently, capturing feature-sequence dynamics into the readout models, and without the need for offline iterative training. Additionally, stacked autoencoders further integrate the extracted static features (i.e., raw audio features) with the captured dynamics, resulting in more stable and effective classification performance. The entire framework is called Static-Dynamic Integration Network (SDIN). The conducted experiments demonstrate the effectiveness of SDIN in speech-music classification tasks."
   ],
   "p1": 4318,
   "pn": 4322,
   "doi": "10.21437/Interspeech.2024-1596",
   "url": "interspeech_2024/chen24r_interspeech.html"
  },
  "wan24b_interspeech": {
   "authors": [
    [
     "Yan",
     "Wan"
    ],
    [
     "Mengyi",
     "Sun"
    ],
    [
     "Xinchen",
     "Kang"
    ],
    [
     "Jingting",
     "Li"
    ],
    [
     "Pengfei",
     "Guo"
    ],
    [
     "Ming",
     "Gao"
    ],
    [
     "Su-Jing",
     "Wang"
    ]
   ],
   "title": "CDSD: Chinese Dysarthria Speech Database",
   "original": "1597",
   "order": 845,
   "page_count": 5,
   "abstract": [
    "Dysarthric speech poses significant challenges for individuals with dysarthria, impacting their ability to communicate socially. Despite the widespread use of Automatic Speech Recognition (ASR), accurately recognizing dysarthric speech remains a formidable task, largely due to the limited availability of dysarthric speech data. To address this gap, we developed the Chinese Dysarthria Speech Database (CDSD), the most extensive collection of Chinese dysarthria data to date, featuring 133 hours of recordings from 44 speakers. Our benchmarks reveal a best Character Error Rate (CER) of 16.4%. Compared to the CER of 20.45% from our additional human experiments, Dysarthric Speech Recognition (DSR) demonstrates its potential in significant improvement of communication for individuals with dysarthria. The CDSD database will be made  publicly available at http://melab.psych.ac.cn/CDSD.html."
   ],
   "p1": 4109,
   "pn": 4113,
   "doi": "10.21437/Interspeech.2024-1597",
   "url": "interspeech_2024/wan24b_interspeech.html"
  },
  "shen24c_interspeech": {
   "authors": [
    [
     "Qingye",
     "Shen"
    ],
    [
     "Leonardo",
     "Lancia"
    ],
    [
     "Noel",
     "Nguyen"
    ]
   ],
   "title": "A novel experimental design for the study of listener-to-listener convergence in phoneme categorization",
   "original": "1598",
   "resource": "https://doi.org/10.17605/OSF.IO/8MUKX",
   "order": 539,
   "page_count": 5,
   "abstract": [
    "We present a novel experimental design that combines highly accurate psychometric methods with an interactive task to characterize how two or more listeners can converge towards each other in the categorization of speech sounds. The design is implemented as a cooperative game, in which listeners are presented with a sequence of sounds that range on a continuum between two endpoints unambiguously associated with two phoneme categories in a joint phoneme identification task. To play the game successfully, listeners must comply with both a distinctivity constraint (identify the endpoints as being different from each other) and an agreement constraint (identify the stimuli in the same way as their partner). Our first results show that our experimental design opens new avenues for research on convergence between listeners in speech perception."
   ],
   "p1": 2615,
   "pn": 2619,
   "doi": "10.21437/Interspeech.2024-1598",
   "url": "interspeech_2024/shen24c_interspeech.html"
  },
  "shu24_interspeech": {
   "authors": [
    [
     "Yuchun",
     "Shu"
    ],
    [
     "Bo",
     "Hu"
    ],
    [
     "Yifeng",
     "He"
    ],
    [
     "Hao",
     "Shi"
    ],
    [
     "Longbiao",
     "Wang"
    ],
    [
     "Jianwu",
     "Dang"
    ]
   ],
   "title": "Error Correction by Paying Attention to Both Acoustic and Confidence References for Automatic Speech Recognition",
   "original": "1605",
   "order": 716,
   "page_count": 5,
   "abstract": [
    "Accurately finding the wrong words in the automatic speech recognition (ASR) hypothesis and recovering them well-founded is the goal of speech error correction. In this paper, we propose a non-autoregressive speech error correction method. A Confidence Module measures the uncertainty of each word of the N-best ASR hypotheses as the reference to find the wrong word position. Besides, the acoustic feature from the ASR encoder is also used to provide the correct pronunciation references. N-best candidates from ASR are aligned using the edit path, to confirm each other and recover some missing character errors. Furthermore, the cross-attention mechanism fuses the information between error correction references and the ASR hypothesis. The experimental results show that both the acoustic and confidence references help with error correction. The proposed system reduces the error rate by 21% compared with the ASR model."
   ],
   "p1": 3500,
   "pn": 3504,
   "doi": "10.21437/Interspeech.2024-1605",
   "url": "interspeech_2024/shu24_interspeech.html"
  },
  "chen24s_interspeech": {
   "authors": [
    [
     "Meiling",
     "Chen"
    ],
    [
     "Pengjie",
     "Liu"
    ],
    [
     "Heng",
     "Yang"
    ],
    [
     "Haofeng",
     "Wang"
    ]
   ],
   "title": "Towards End-to-End Unified Recognition for Mandarin and Cantonese",
   "original": "1606",
   "order": 488,
   "page_count": 5,
   "abstract": [
    "Constructing an automatic speech recognition (ASR) system that supports Mandarin and Cantonese is demanding and challenging. The method of pre-training two speech recognition models and then selecting a specific model for recognition through extra means is resource-consuming and complex. This paper presents an end-to-end system for unified Mandarin-Cantonese recognition and a complete model training method in scenarios where high-resource and low-resource languages coexist, while reducing complexity. The impact of different modeling units on character error rate (CER) and training efficiency was also studied. Besides, this system incorporates a language identification (LID) module to reduce context confusion during recognition. Experiments show that compared to Mandarin-only and Cantonese-only models, our system achieves 12.71% and 21.23% relative CER reduction for Mandarin and Cantonese respectively and training efficiency can be doubled."
   ],
   "p1": 2365,
   "pn": 2369,
   "doi": "10.21437/Interspeech.2024-1606",
   "url": "interspeech_2024/chen24s_interspeech.html"
  },
  "kye24_interspeech": {
   "authors": [
    [
     "Ted",
     "Kye"
    ]
   ],
   "title": "Affricates in Lushootseed",
   "original": "1607",
   "order": 763,
   "page_count": 5,
   "abstract": [
    "In this study, I conduct the first acoustic analysis of affricates in Lushootseed (Coast Salish branch). The findings reveal that several acoustic measurements, such as VOT, release frication duration, intensity, spectral moments (Center of Gravity), and voice onset quality, characterize affricate contrasts with respect to their place of articulation and laryngeal type (i.e., voiced, voiceless, ejective). This study raises questions concerning (a) the acoustic-articulatory correlates of affricates, (b) typological classification of ejective affricates, and (c) the diachronic status of voiced affricates in Coast Salish languages."
   ],
   "p1": 3699,
   "pn": 3703,
   "doi": "10.21437/Interspeech.2024-1607",
   "url": "interspeech_2024/kye24_interspeech.html"
  },
  "song24c_interspeech": {
   "authors": [
    [
     "Zeyang",
     "Song"
    ],
    [
     "Qianhui",
     "Liu"
    ],
    [
     "Qu",
     "Yang"
    ],
    [
     "Yizhou",
     "Peng"
    ],
    [
     "Haizhou",
     "Li"
    ]
   ],
   "title": "ED-sKWS: Early-Decision Spiking Neural Networks for Rapid, and Energy-Efficient Keyword Spotting",
   "original": "1609",
   "order": 930,
   "page_count": 5,
   "abstract": [
    "Keyword Spotting (KWS) is essential in edge computing requiring rapid and energy-efficient responses. Spiking Neural Networks (SNNs) are well-suited for KWS for their efficiency and temporal capacity for speech. To further reduce the latency and energy consumption, this study introduces ED-sKWS, an SNN-based KWS model with an early-decision mechanism that can stop speech processing and output the result before the end of speech utterance. Furthermore, we introduce a Cumulative Temporal (CT) loss that can enhance prediction accuracy at both the intermediate and final timesteps. To evaluate early-decision performance, we present the SC-100 dataset including 100 speech commands with beginning and end timestamp annotation. Experiments on the Google Speech Commands v2 and our SC-100 datasets show that ED-sKWS maintains competitive accuracy with 61% timesteps and 52% energy consumption compared to SNN models without early-decision mechanism, ensuring rapid response and energy efficiency."
   ],
   "p1": 4528,
   "pn": 4532,
   "doi": "10.21437/Interspeech.2024-1609",
   "url": "interspeech_2024/song24c_interspeech.html"
  },
  "li24fa_interspeech": {
   "authors": [
    [
     "Zimeng",
     "Li"
    ],
    [
     "Zhongxuan",
     "Mao"
    ],
    [
     "Shengting",
     "Shen"
    ],
    [
     "Ivan",
     "Yuen"
    ],
    [
     "Ping",
     "Tang"
    ]
   ],
   "title": "The Production of Contrastive Focus by  7 to 13-year-olds Learning Mandarin Chinese",
   "original": "1611",
   "order": 861,
   "page_count": 5,
   "abstract": [
    "Contrastive focus highlights the important information in discourse, acoustically manifested in fundamental frequency (F0) and duration. In tonal languages, F0 conveys both lexical (lexical tones) and discourse meanings (contrastive focus). Given the importance of tones in word recognition, children acquire tonal productions early before age 3, but it was unclear when and how children acquire contrastive focus. This study tested 190 7-13-year-olds and 20 adult controls. Their productions of noun phrases with and without contrastive focus were elicited and perceptually evaluated for the presence/absence of focus. The perceptual judgement revealed that the perceived accuracy improved from 76% at age 7 to 93% at age 13. The acoustic analysis on the “perceived focus” productions revealed that 7-year-olds were able to use both F0 and durational cues, although fine-tuning continues until 13 years. These findings revealed that focus acquisition in Mandarin might be a late and gradual process."
   ],
   "p1": 4184,
   "pn": 4188,
   "doi": "10.21437/Interspeech.2024-1611",
   "url": "interspeech_2024/li24fa_interspeech.html"
  },
  "meyer24_interspeech": {
   "authors": [
    [
     "Sarina",
     "Meyer"
    ],
    [
     "Florian",
     "Lux"
    ],
    [
     "Ngoc Thang",
     "Vu"
    ]
   ],
   "title": "Probing the Feasibility of Multilingual Speaker Anonymization",
   "original": "1615",
   "resource": "https://doi.org/10.5281/zenodo.12801027",
   "order": 914,
   "page_count": 5,
   "abstract": [
    "In speaker anonymization, speech recordings are modified in a way that the identity of the speaker remains hidden. While this technology could help to protect the privacy of individuals around the globe, current research restricts this by focusing almost exclusively on English data. In this study, we extend a state-of-the-art anonymization system to nine languages by transforming language-dependent components to their multilingual counterparts. Experiments testing the robustness of the anonymized speech against privacy attacks and speech deterioration show an overall success of this system for all languages. The results suggest that speaker embeddings trained on English data can be applied across languages, and that the anonymization performance for a language is mainly affected by the quality of the speech synthesis component used for it. "
   ],
   "p1": 4448,
   "pn": 4452,
   "doi": "10.21437/Interspeech.2024-1615",
   "url": "interspeech_2024/meyer24_interspeech.html"
  },
  "wang24ba_interspeech": {
   "authors": [
    [
     "Yuejiao",
     "Wang"
    ],
    [
     "Xianmin",
     "Gong"
    ],
    [
     "Lingwei",
     "Meng"
    ],
    [
     "Xixin",
     "Wu"
    ],
    [
     "Helen",
     "Meng"
    ]
   ],
   "title": "Large Language Model-based FMRI Encoding of Language Functions for Subjects with Neurocognitive Disorder",
   "original": "1616",
   "order": 306,
   "page_count": 5,
   "abstract": [
    "Functional magnetic resonance imaging (fMRI) is essential for developing encoding models that identify functional changes in language-related brain areas of individuals with Neurocognitive Disorders (NCD). While large language model (LLM)-based fMRI encoding has shown promise, existing studies predominantly focus on healthy, young adults, overlooking older NCD populations and cognitive level correlations. This paper explores language-related functional changes in older NCD adults using LLM-based fMRI encoding and brain scores, addressing current limitations. We analyze the correlation between brain scores and cognitive scores at both whole-brain and language-related ROI levels. Our findings reveal that higher cognitive abilities correspond to better brain scores, with correlations peaking in the middle temporal gyrus. This study highlights the potential of fMRI encoding models and brain scores for detecting early functional changes in NCD patients."
   ],
   "p1": 1485,
   "pn": 1489,
   "doi": "10.21437/Interspeech.2024-1616",
   "url": "interspeech_2024/wang24ba_interspeech.html"
  },
  "gu24b_interspeech": {
   "authors": [
    [
     "Yue",
     "Gu"
    ],
    [
     "Zhihao",
     "Du"
    ],
    [
     "Shiliang",
     "Zhang"
    ],
    [
     "jiqing",
     "Han"
    ],
    [
     "Yongjun",
     "He"
    ]
   ],
   "title": "Personality-memory Gated Adaptation: An Efficient Speaker Adaptation for Personalized End-to-end Automatic Speech Recognition",
   "original": "1621",
   "order": 590,
   "page_count": 5,
   "abstract": [
    "In real-world applications, a preferred personalized automatic speech recognition (ASR) model should exhibit robust recognition capabilities that include both personalization and generalization. Speaker adaptation is a commonly used approach towards personalized ASR. However, most speaker adaptation methods only focus on improving the performance of the target speaker but neglect or even sacrifice generalization. In this paper, we propose the Personality-memory Gated Adaptation (PGA) approach to adapt models on target speakers while preserving generalized recognition performance. Specifically, we incorporate parallel adapters into the encoder to capture the target speaker’s vocal characteristics. The hidden outputs of the adapters and original encoder layers are fused via a scalar gate, which is derived from the similarity between input samples and personality memory units, i.e., personality embeddings from the target speaker. In this manner, the vocal characteristics of the target speaker are memorized by adapters while the ASR backbone preserves generalization. Experimental results show that, compared to the unadapted ASR backbone, the proposed PGA achieves 21.62% relative character error rate (CER) reduction on the target domain but a negligible CER increase on the source domain with limited adaptation data and training steps."
   ],
   "p1": 2870,
   "pn": 2874,
   "doi": "10.21437/Interspeech.2024-1621",
   "url": "interspeech_2024/gu24b_interspeech.html"
  },
  "haider24_interspeech": {
   "authors": [
    [
     "Daniel",
     "Haider"
    ],
    [
     "Felix",
     "Perfler"
    ],
    [
     "Vincent",
     "Lostanlen"
    ],
    [
     "Martin",
     "Ehler"
    ],
    [
     "Peter",
     "Balazs"
    ]
   ],
   "title": "Hold Me Tight: Stable Encoder-Decoder Design for Speech Enhancement",
   "original": "1622",
   "order": 1027,
   "page_count": 5,
   "abstract": [
    "Convolutional layers with 1-D filters are often used as frontend to encode audio signals. Unlike fixed time-frequency representations, they can adapt to the local characteristics of input data. However, 1-D filters on raw audio are hard to train and often suffer from instabilities. In this paper, we address these problems with hybrid solutions, i.e., combining theory-driven and data-driven approaches. First, we preprocess the audio signals via a auditory filterbank, guaranteeing good frequency localization for the learned encoder. Second, we use results from frame theory to define an unsupervised learning objective that encourages energy conservation and perfect reconstruction. Third, we adapt mixed compressed spectral norms as learning objectives to the encoder coefficients. Using these solutions in a low-complexity encoder-mask-decoder model significantly improves the perceptual evaluation of speech quality (PESQ) in speech enhancement."
   ],
   "p1": 5013,
   "pn": 5017,
   "doi": "10.21437/Interspeech.2024-1622",
   "url": "interspeech_2024/haider24_interspeech.html"
  },
  "sheikh24_interspeech": {
   "authors": [
    [
     "Muhammad Umer",
     "Sheikh"
    ],
    [
     "Hassan",
     "Abid"
    ],
    [
     "Bhuiyan Sanjid",
     "Shafique"
    ],
    [
     "Asif",
     "Hanif"
    ],
    [
     "Muhammad Haris",
     "Khan"
    ]
   ],
   "title": "Bird Whisperer: Leveraging Large Pre-trained Acoustic Model for Bird Call Classification",
   "original": "1623",
   "order": 1030,
   "page_count": 5,
   "abstract": [
    "Adapting large pre-trained acoustic models across diverse domains poses a significant challenge in speech processing, particularly when shifting from human to non-human contexts. This study aims to bridge this gap by utilizing the pre-trained Whisper model, initially intended for human speech recognition, for classifying bird calls. Our study reveals that when employed solely as a feature extractor, the Whisper encoder fails to yield meaningful features from bird calls, possibly due to categorizing them as background noise. We propose a simple but effective technique to enhance Whisper's ability to extract distinctive features from avian vocalizations, resulting in a remarkable 15% increase in F1-score over the baseline. Furthermore, we mitigate the issue of class imbalance within the dataset by introducing a series of data augmentations. Our findings underscore the potential of adapting large pre-trained acoustic models to tackle broader bioacoustic classification tasks. The code is available at https://github.\ncom/umer-sheikh/bird-whisperer."
   ],
   "p1": 5028,
   "pn": 5032,
   "doi": "10.21437/Interspeech.2024-1623",
   "url": "interspeech_2024/sheikh24_interspeech.html"
  },
  "coulange24_interspeech": {
   "authors": [
    [
     "Sylvain",
     "Coulange"
    ],
    [
     "Tsuneo",
     "Kato"
    ],
    [
     "Solange",
     "Rossato"
    ],
    [
     "Monica",
     "Masperi"
    ]
   ],
   "title": "Exploring Impact of Pausing and Lexical Stress Patterns on L2 English Comprehensibility in Real Time",
   "original": "1627",
   "order": 215,
   "page_count": 5,
   "abstract": [
    "A significant obstacle to effective L2 English speech lies in the inappropriate use of pauses and lexical stress. We observed the impact of pauses within phrases (WP) and incorrectly stressed words on real-time perceived comprehensibility. Sixty native English listeners were asked, while listening to short recordings of L2 speakers, to click on a button whenever they were struggling to understand the speaker. Analysis showed that click frequency tends to increase after WP pauses and incorrectly stressed words, especially 2-3 s after onset, while it remains under the average click frequency after pauses between clauses and correctly stressed words. These results demonstrate that WP pauses and incorrect stress patterns directly impact the listener's perception, and that this impact can measured with a dynamic rating protocol. Moreover, such a protocol appears to be a promising tool to expand our knowledge about real-time comprehension of L2 speech."
   ],
   "p1": 1030,
   "pn": 1034,
   "doi": "10.21437/Interspeech.2024-1627",
   "url": "interspeech_2024/coulange24_interspeech.html"
  },
  "mogridge24_interspeech": {
   "authors": [
    [
     "Rhiannon",
     "Mogridge"
    ],
    [
     "Anton",
     "Ragni"
    ]
   ],
   "title": "Learning from memory-based models",
   "original": "1628",
   "order": 487,
   "page_count": 5,
   "abstract": [
    "Recent work on the CPC2 speech intelligibility task shows promising results using an architecture inspired by memory models from the field of human psychology. This is surprising, given that previous work has shown memory models of this type to be inferior to parametric models, such as transformers, in most modern applications. This paper shows that the difference in performance is reduced or eliminated by using high quality features. Furthermore, we show for the first time that, despite being widely used in the field of human psychology and also for speech and language tasks, this model is a special case of a neural network. Experimental results from different tasks and datasets (CPC2/TIMIT/GoEmotions) confirm that this type of memory model is competitive with equivalently complex parametric models given sufficiently good feature representation, suggesting that high quality features may allow the use of simple, interpretable models without sacrificing performance."
   ],
   "p1": 2360,
   "pn": 2364,
   "doi": "10.21437/Interspeech.2024-1628",
   "url": "interspeech_2024/mogridge24_interspeech.html"
  },
  "lim24_interspeech": {
   "authors": [
    [
     "Chan-yeong",
     "Lim"
    ],
    [
     "Hyun-seo",
     "Shin"
    ],
    [
     "Ju-ho",
     "Kim"
    ],
    [
     "Jungwoo",
     "Heo"
    ],
    [
     "Kyo-Won",
     "Koo"
    ],
    [
     "Seung-bin",
     "Kim"
    ],
    [
     "Ha-Jin",
     "Yu"
    ]
   ],
   "title": "Improving Noise Robustness in Self-supervised Pre-trained Model for Speaker Verification",
   "original": "1630",
   "order": 549,
   "page_count": 5,
   "abstract": [
    "Adopting self-supervised pre-trained models (PMs) in speaker verification (SV) has shown remarkable performance, but their noise robustness is largely unexplored. In the field of automatic speech recognition, additional training strategies enhance the robustness of the models before fine-tuning to improve performance in noisy environments. However, directly applying these strategies to SV risks distorting speaker information. We propose a noise adaptive warm-up training for speaker verification (NAW-SV). The NAW-SV guides the PM to extract consistent representations in noisy conditions using teacher-student learning. In this approach, to prevent the speaker information distortion problem, we introduce a novel loss function called extended angular prototypical network loss, which assists in considering speaker information and exploring robust speaker embedding space. We validated our proposed framework on the noise-synthesized VoxCeleb1 test set, demonstrating promising robustness."
   ],
   "p1": 2665,
   "pn": 2669,
   "doi": "10.21437/Interspeech.2024-1630",
   "url": "interspeech_2024/lim24_interspeech.html"
  },
  "he24_interspeech": {
   "authors": [
    [
     "Jiajun",
     "He"
    ],
    [
     "Tomoki",
     "Toda"
    ]
   ],
   "title": "2DP-2MRC: 2-Dimensional Pointer-based Machine Reading Comprehension Method for Multimodal Moment Retrieval",
   "original": "1633",
   "order": 1039,
   "page_count": 5,
   "abstract": [
    "Moment retrieval aims to locate the most relevant moment in an untrimmed video based on a given natural language query. Existing solutions can be roughly categorized into moment-based and clip-based methods. The former often involves heavy computations, while the latter, due to overlooking coarse-grained information, typically underperforms compared to moment-based models. Hence, this paper proposes a novel 2-Dimensional Pointer-based Machine Reading Comprehension for Moment Retrieval Choice (2DP-2MRC) model to address the issue of imprecise localization in clip-based methods while maintaining lower computational complexity than moment-based methods. Specifically, we introduce an AV-Encoder to capture coarse-grained information at moment and video levels. Additionally, a 2D pointer encoder module is introduced to further enhance boundary detection for target moment. Extensive experiments on the HiREST dataset demonstrate that 2DP-2MRC significantly outperforms existing baseline models."
   ],
   "p1": 5073,
   "pn": 5077,
   "doi": "10.21437/Interspeech.2024-1633",
   "url": "interspeech_2024/he24_interspeech.html"
  },
  "fang24_interspeech": {
   "authors": [
    [
     "Qiang",
     "Fang"
    ]
   ],
   "title": "On The Performance of EMA-synchronized Speech and Stand-alone Speech in Acoustic-to-articulatory Inversion",
   "original": "1637",
   "order": 638,
   "page_count": 5,
   "abstract": [
    "Synchronized acoustic-articulatory data is the basis of various applications, such as acoustic to articulatory inversion (AAI), articulatory to acoustic mapping (MAP), etc. Most of the studies in these fields directly trained various models with EMA synchronized speech, while the target input or output are stand-alone speech in real applications. However, the recording conditions of EMA-synchronized speech and stand-alone speech are different, which may make the EMA-synchronized speech different to the stand-alone speech and degrade the performance of downstream tasks. In this study, we attempt to present a general view of whether the differences affect the performance of AAI by training 3 latest AAI model with EMAsynchronize speech and testing them with both EMAsynchronized speech and stand-alone speech. The results indicate that the performance of all the 3 AAI models degrade dramatically in the sense of both Root-Mean-Square errors and Pearson’s correlation coefficients."
   ],
   "p1": 3110,
   "pn": 3114,
   "doi": "10.21437/Interspeech.2024-1637",
   "url": "interspeech_2024/fang24_interspeech.html"
  },
  "gothi24_interspeech": {
   "authors": [
    [
     "Raj",
     "Gothi"
    ],
    [
     "Rahul",
     "Kumar"
    ],
    [
     "Mildred",
     "Pereira"
    ],
    [
     "Nagesh",
     "Nayak"
    ],
    [
     "Preeti",
     "Rao"
    ]
   ],
   "title": "A Dataset and Two-pass System for Reading Miscue Detection",
   "original": "1639",
   "order": 826,
   "page_count": 5,
   "abstract": [
    "Automatic speech recognition (ASR) has long been viewed as a promising solution to the resource-intensive task of oral reading fluency assessment. The demands on ASR accuracy, however, tend to be high, especially when applied to obtaining reliable reading diagnostics. The prior knowledge of reading prompts is typically used to limit the system WER. The accurate detection of mispronounced words, which can be relatively few in number, while limiting false positives, remains challenging. In this work, we present a new manually transcribed dataset of 1,110 elementary school children reading connected text in L2 English with wide-ranging proficiencies. Apart from local features derived from alternate decodings under different linguistic context constraints, we use an additional deep acoustic model. We discuss the performance gains achieved in a second pass over initial hybrid ASR hypotheses."
   ],
   "p1": 4014,
   "pn": 4018,
   "doi": "10.21437/Interspeech.2024-1639",
   "url": "interspeech_2024/gothi24_interspeech.html"
  },
  "xie24c_interspeech": {
   "authors": [
    [
     "Yuxin",
     "Xie"
    ],
    [
     "Zhihong",
     "Zhu"
    ],
    [
     "Xianwei",
     "Zhuang"
    ],
    [
     "Liming",
     "Liang"
    ],
    [
     "Zhichang",
     "Wang"
    ],
    [
     "Yuexian",
     "Zou"
    ]
   ],
   "title": "GPA: Global and Prototype Alignment for Audio-Text Retrieval",
   "original": "1642",
   "order": 1040,
   "page_count": 5,
   "abstract": [
    "Recent Audio-Text Retrieval (ATR) models have achieved progressive results, which pursue semantic interaction upon audio and text pairs. To clarify this coarse-grained global interaction and move a step further, we have to encounter challenging shell-breaking interactions for fine-grained cross-modal learning between audio and text. In this paper, we present GPA for ATR to achieve both Global (coarse-grained) and Prototype (fine-grained) Alignment. In detail, apart from performing vanilla global contrast between audio and text pairs, we model the frames in audio and words in text as prototypes, and align the prototypes to generate a prototype similarity matrix. Based on this, we introduce a Learnable Attention Similarity Scoring module, which can fully consider the information between different prototype pairs and obtain the retrieval score. Finally, we incorporate the Sinkhorn-Knopp algorithm to modify the retrieval score. Experimental results on two benchmark datasets with superior performance justify the efficacy of our proposed GPA."
   ],
   "p1": 5078,
   "pn": 5082,
   "doi": "10.21437/Interspeech.2024-1642",
   "url": "interspeech_2024/xie24c_interspeech.html"
  },
  "hao24_interspeech": {
   "authors": [
    [
     "Yaqian",
     "Hao"
    ],
    [
     "Chenguang",
     "Hu"
    ],
    [
     "Yingying",
     "Gao"
    ],
    [
     "Shilei",
     "Zhang"
    ],
    [
     "Junlan",
     "Feng"
    ]
   ],
   "title": "On Calibration of Speech Classification Models: Insights from Energy-Based Model Investigations",
   "original": "1643",
   "order": 651,
   "page_count": 5,
   "abstract": [
    "For speech classification tasks, deep learning models often achieve high accuracy but exhibit shortcomings in calibration, manifesting as classifiers exhibiting overconfidence. The significance of calibration lies in its critical role in guaranteeing the reliability of decision-making within deep learning systems. This study explores the effectiveness of Energy-Based Models (EBMs) in calibrating confidence for speech classification tasks by training a joint EBM integrating a discriminative and a generative model, thereby enhancing the classifier’s calibration and mitigating overconfidence. Experimental evaluations conducted on three speech classification tasks specifically: age, emotion, and language recognition. Our findings highlight the competitive performance of EBMs in calibrating the speech classification models. This research emphasizes the potential of EBMs in speech classification tasks, demonstrating their ability to enhance calibration without sacrificing accuracy."
   ],
   "p1": 3175,
   "pn": 3179,
   "doi": "10.21437/Interspeech.2024-1643",
   "url": "interspeech_2024/hao24_interspeech.html"
  },
  "omine24_interspeech": {
   "authors": [
    [
     "Taisei",
     "Omine"
    ],
    [
     "Kenta",
     "Akita"
    ],
    [
     "Reiji",
     "Tsuruno"
    ]
   ],
   "title": "Robust Laughter Segmentation with Automatic Diverse Data Synthesis",
   "original": "1644",
   "resource": "https://doi.org/10.5281/zenodo.12787930",
   "order": 974,
   "page_count": 5,
   "abstract": [
    "Laughter detection is important in the analysis of human communication. In recent years, machine learning has been commonly used to detect laughter. Laughter segmentation, the task of accurately identifying the location of laughter in audio, necessitates precise annotation of training data. However, manual annotation is very time-consuming and data preparation is not easy. We propose a method to facilitate the creation of training data for segmenting laughter in audio. Our method automatically annotates by synthesizing laughter and adding it to arbitrary audio. It allows a large amount of data to be created because the number and positions of laughs can be set freely, and data augmentation can be applied to the laughter separately. In addition, because our method can automatically annotate arbitrary audio, it can easily create datasets for training models on new data. Evaluation shows that our segmentation model outperforms existing models trained on manually annotated datasets."
   ],
   "p1": 4748,
   "pn": 4752,
   "doi": "10.21437/Interspeech.2024-1644",
   "url": "interspeech_2024/omine24_interspeech.html"
  },
  "leung24_interspeech": {
   "authors": [
    [
     "Wing-Zin",
     "Leung"
    ],
    [
     "Mattias",
     "Cross"
    ],
    [
     "Anton",
     "Ragni"
    ],
    [
     "Stefan",
     "Goetze"
    ]
   ],
   "title": "Training Data Augmentation for Dysarthric Automatic Speech Recognition by Text-to-Dysarthric-Speech Synthesis",
   "original": "1645",
   "order": 514,
   "page_count": 5,
   "abstract": [
    "Automatic speech recognition (ASR) research has achieved impressive performance in recent years and has significant potential for enabling access for people with dysarthria (PwD) in augmentative and alternative communication (AAC) and home environment systems. However, progress in dysarthric ASR (DASR) has been limited by high variability in dysarthric speech and limited public availability of dysarthric training data. This paper demonstrates that data augmentation using text-to-dysarthic-speech (TTDS) synthesis for finetuning large ASR models is effective for DASR. Specifically, diffusion-based text-to-speech (TTS) models can produce speech samples similar to dysarthric speech that can be used as additional training data for fine-tuning ASR foundation models, in this case Whisper. Results show improved synthesis metrics and ASR performance for the proposed multi-speaker diffusion-based TTDS data augmentation for ASR fine-tuning compared to current DASR baselines."
   ],
   "p1": 2494,
   "pn": 2498,
   "doi": "10.21437/Interspeech.2024-1645",
   "url": "interspeech_2024/leung24_interspeech.html"
  },
  "moussa24_interspeech": {
   "authors": [
    [
     "Denise",
     "Moussa"
    ],
    [
     "Sandra",
     "Bergmann"
    ],
    [
     "Christian",
     "Riess"
    ]
   ],
   "title": "Unmasking Neural Codecs: Forensic Identification of AI-compressed Speech",
   "original": "1652",
   "order": 467,
   "page_count": 5,
   "abstract": [
    "Compression traces are an important forensic cue to uncover the processing history and integrity of audio evidence. With continuous advances in the AI domain, efficient generative lossy neural codecs like Lyra-V2, EnCodec or Improved RVQGAN can compete with traditional speech and audio codecs. Their fundamentally different learning based approach compared to analytical lossy compression methods poses a new challenge for audio forensics. This calls for a closer examination of such techniques to prepare forensics for audio evidence processed by AI-based codecs. In this work, we thus want to take a first step towards robustly detecting traces of neural codecs in audio samples. We report that distinctive frequency artefacts enable for identifying neurally compressed audio and fingerprint specific AI-based codecs. We further analyse the robustness towards cross-dataset testing and noise, downsampling, and traditional compression post-processing."
   ],
   "p1": 2260,
   "pn": 2264,
   "doi": "10.21437/Interspeech.2024-1652",
   "url": "interspeech_2024/moussa24_interspeech.html"
  },
  "hao24b_interspeech": {
   "authors": [
    [
     "Yaqian",
     "Hao"
    ],
    [
     "Chenguang",
     "Hu"
    ],
    [
     "Yingying",
     "Gao"
    ],
    [
     "Shilei",
     "Zhang"
    ],
    [
     "Junlan",
     "Feng"
    ]
   ],
   "title": "Exploring Energy-Based Models for Out-of-Distribution Detection in Dialect Identification",
   "original": "1657",
   "order": 337,
   "page_count": 5,
   "abstract": [
    "The diverse nature of dialects presents challenges for models trained on specific linguistic patterns, rendering them susceptible to errors when confronted with unseen or out-of-distribution （OOD） data. This study introduces a novel margin-enhanced joint energy model (MEJEM) tailored specifically for OOD detection in dialects. By integrating a generative model and the energy margin loss, our approach aims to enhance the robustness of dialect identification systems. Furthermore, we explore two OOD scores for OOD dialect detection, and our findings conclusively demonstrate that the energy score outperforms the softmax score. Leveraging Sharpness-Aware Minimization to optimize the training process of the joint model, we enhance model generalization by minimizing both loss and sharpness. Experiments conducted on dialect identification tasks validate the efficacy of Energy-Based Models  and provide valuable insights into their performance."
   ],
   "p1": 1640,
   "pn": 1644,
   "doi": "10.21437/Interspeech.2024-1657",
   "url": "interspeech_2024/hao24b_interspeech.html"
  },
  "li24ga_interspeech": {
   "authors": [
    [
     "Hao",
     "Li"
    ],
    [
     "Yuan",
     "Fang"
    ],
    [
     "Xueliang",
     "Zhang"
    ],
    [
     "Fei",
     "Chen"
    ],
    [
     "Guanglai",
     "Gao"
    ]
   ],
   "title": "Cross-Attention-Guided WaveNet for EEG-to-MEL Spectrogram Reconstruction",
   "original": "1662",
   "order": 540,
   "page_count": 5,
   "abstract": [
    "This paper introduces an innovative approach that leverages a cross-attention-guided WaveNet combined with a coarse-to-fine granularity strategy to enhance the detailed reconstruction of Mel spectrograms from time-domain EEG signals. The proposed model utilizes WaveNet to sequentially reconstruct the envelope, 10-band Mel, 80-band Mel, and magnitude at progressively finer granularity levels. A cross-attention mechanism is introduced to explore correlations across modalities to address the modality gap. A combined loss function and Mixup augmentation technique are also employed to enhance the reconstruction performance. Notably, our approach achieves Pearson correlation values of 0.0651 ± 0.0153 for the validation set and 0.0413 ± 0.0169 for the heldout-subjects test set, securing the second position in the 2024 Auditory EEG Challenge. We also validated the contribution of each module through ablation experiments. The source code is available online."
   ],
   "p1": 2620,
   "pn": 2624,
   "doi": "10.21437/Interspeech.2024-1662",
   "url": "interspeech_2024/li24ga_interspeech.html"
  },
  "monteiro24_interspeech": {
   "authors": [
    [
     "Raul",
     "Monteiro"
    ]
   ],
   "title": "Adding User Feedback To Enhance CB-Whisper",
   "original": "1664",
   "resource": "https://doi.org/10.5281/zenodo.12794185",
   "order": 71,
   "page_count": 5,
   "abstract": [
    "Contextual biasing has been demonstrated to be effective in improving Whisper recall for named entities or domain-specific words. In a recent work, CB-Whisper takes an additional step and integrates a classifier for open-vocabulary keyword-spotting (OV-KWS) to retrieve keywords from an external database to form a restricted biasing list. Heavy dependence on text-to-speech (TTS) models for generating the speech for the keywords makes the system prone to the drawbacks of using TTS models to generate speech for graphemes with non-trivial phonetic transcriptions. This work proposes an extension to CB-Whisper that leverages user feedback to extend the database of keywords with audio extracted from natural speech. We experiment with different learning strategies for the OV-KWS classifier to assess its domain generalization capabilities for TTS-generated or natural-speech keyword audios and unseen languages."
   ],
   "p1": 347,
   "pn": 351,
   "doi": "10.21437/Interspeech.2024-1664",
   "url": "interspeech_2024/monteiro24_interspeech.html"
  },
  "wesolek24_interspeech": {
   "authors": [
    [
     "Sarah",
     "Wesolek"
    ],
    [
     "Piotr",
     "Gulgowski"
    ],
    [
     "Joanna",
     "Blaszczak"
    ],
    [
     "Marzena",
     "Zygis"
    ]
   ],
   "title": "The influence of L2 accent strength and different error types on personality trait ratings",
   "original": "1669",
   "order": 2,
   "page_count": 5,
   "abstract": [
    "Accents can have a detrimental impact on interpersonal evaluations. However, the influence of specific language errors remains less understood. The present study tests how accent strength (constructed as a graded factor obtained through ratings) impacts evaluations of speakers’ personality traits (warmth, competence) in the German-Polish context. Moreover, this study intertwines accentedness with two L2 typical error types: phonological (vowel substitutions) and grammatical (gender agreement errors). Results indicate that L2 accent strength had an unfavorable effect on speakers’ competence but positively influenced warmth. The perceived competence was reduced by both error types (phonological, grammatical), while warmth was decreased solely by grammatical errors. The error effects diminished with increasing L2 accent. Finally, Polish participants were less sensitive towards errors and particularly resistant towards phonological substitutions when rating speakers’ personality."
   ],
   "p1": 2,
   "pn": 6,
   "doi": "10.21437/Interspeech.2024-1669",
   "url": "interspeech_2024/wesolek24_interspeech.html"
  },
  "gudmalwar24_interspeech": {
   "authors": [
    [
     "Ashishkumar",
     "Gudmalwar"
    ],
    [
     "Nirmesh",
     "Shah"
    ],
    [
     "Sai",
     "Akarsh"
    ],
    [
     "Pankaj",
     "Wasnik"
    ],
    [
     "Rajiv Ratn",
     "Shah"
    ]
   ],
   "title": "VECL-TTS: Voice identity and Emotional style controllable Cross-Lingual Text-to-Speech",
   "original": "1672",
   "order": 616,
   "page_count": 5,
   "abstract": [
    "Despite the significant advancements in Text-to-Speech (TTS) systems, their full utilization in automatic dubbing remains limited. This task necessitates the extraction of voice identity and emotional style from a reference speech in a source language and subsequently transferring them to a target language using cross-lingual TTS techniques. While previous approaches have mainly concentrated on controlling voice identity within the cross-lingual TTS framework, there has been limited work on incorporating emotion and voice identity together. To this end, we introduce an end-to-end Voice Identity and Emotional Style Controllable Cross-Lingual (VECL) TTS system using multilingual speakers and an emotion embedding network. Moreover, we introduce content and style consistency losses to enhance the quality of synthesized speech further. The proposed system achieved an average relative improvement of 8.83% compared to the state-of-the-art (SOTA) methods on a database comprising English and three Indian languages (Hindi, Telugu, and Marathi)."
   ],
   "p1": 3000,
   "pn": 3004,
   "doi": "10.21437/Interspeech.2024-1672",
   "url": "interspeech_2024/gudmalwar24_interspeech.html"
  },
  "vegarodriguez24_interspeech": {
   "authors": [
    [
     "Jenifer",
     "Vega Rodriguez"
    ],
    [
     "Nathalie",
     "Vallée"
    ],
    [
     "Christophe",
     "Savariaux"
    ],
    [
     "Silvain",
     "Gerber"
    ]
   ],
   "title": "Nasal Air Flow During Speech Production In Korebaju",
   "original": "1674",
   "order": 762,
   "page_count": 5,
   "abstract": [
    "Korebaju, also known as Koreguaje (ISO 639-3: coe), is a tonal language spoken by approximately 2,000 people in the Amazonian foothills of Colombia. As part of an ongoing research project, a native female speaker participated in our key study at our laboratory in France. Using the EVA2 station, we recorded synchronized acoustic signals (WAV), electroglottographic signals (EGG), vertical larynx movements (LT), and oral (OAF) and nasal (NAF) airflows to highlight the specific features of nasal consonants in this language. Previous research has noted the presence of a burst noise at the end of nasal consonant production. Our results reject the implosive or ejective realization of nasal consonants and dismiss the possibility of an oral release of the labial or buccal articulation. Simultaneously, our study aims to observe a potential nasal carryover effect on adjacent vowels due to the reported nasal harmony."
   ],
   "p1": 3694,
   "pn": 3698,
   "doi": "10.21437/Interspeech.2024-1674",
   "url": "interspeech_2024/vegarodriguez24_interspeech.html"
  },
  "yoon24b_interspeech": {
   "authors": [
    [
     "Juhwan",
     "Yoon"
    ],
    [
     "WooSeok",
     "Ko"
    ],
    [
     "Seyun",
     "Um"
    ],
    [
     "Sungwoong",
     "Hwang"
    ],
    [
     "Soojoong",
     "Hwang"
    ],
    [
     "Changhwan",
     "Kim"
    ],
    [
     "Hong-Goo",
     "Kang"
    ]
   ],
   "title": "UNIQUE : Unsupervised Network for Integrated Speech Quality Evaluation",
   "original": "1675",
   "order": 1014,
   "page_count": 5,
   "abstract": [
    "The significance of an objective metric for evaluating synthetic speech lies in its ability to provide a quantitative measure for systematic assessment of speech quality. However, previous works have focused on predicting subjective quality scores in a supervised manner, requiring a large amount of paired data comprising speech and perceived quality scores. In this work, we introduce a novel metric, the UNIQUE score, that integrates the concept of anomaly detection to systematically evaluate input speech in an unsupervised manner. By leveraging speech features from a self-supervised model, the system can learn a sophisticated speech distribution that enables it to detect differences between real and synthesized speech. By comparing the UNIQUE score of synthetic speech across various text-to-speech models and datasets with other objective measures, we demonstrate that our metric provides an effective evaluation of speech quality that shows a higher correlation with human perceptions."
   ],
   "p1": 4948,
   "pn": 4952,
   "doi": "10.21437/Interspeech.2024-1675",
   "url": "interspeech_2024/yoon24b_interspeech.html"
  },
  "xu24e_interspeech": {
   "authors": [
    [
     "Xuenan",
     "Xu"
    ],
    [
     "Haohe",
     "Liu"
    ],
    [
     "Mengyue",
     "Wu"
    ],
    [
     "Wenwu",
     "Wang"
    ],
    [
     "Mark D.",
     "Plumbley"
    ]
   ],
   "title": "Efficient Audio Captioning with Encoder-Level Knowledge Distillation",
   "original": "1680",
   "order": 241,
   "page_count": 5,
   "abstract": [
    "Significant improvement has been achieved in automated audio captioning (AAC) with recent models. However, these models have become increasingly large as their performance is enhanced. In this work, we propose a knowledge distillation (KD) framework for AAC. Our analysis shows that in the encoder-decoder based AAC models, it is more effective to distill knowledge into the encoder as compared with the decoder. To this end, we incorporate encoder-level KD loss into training, in addition to the standard supervised loss and sequence-level KD loss. We investigate two encoder-level KD methods, based on mean squared error (MSE) loss and contrastive loss, respectively. Experimental results demonstrate that contrastive KD is more robust than MSE KD, exhibiting superior performance in data-scarce situations. By leveraging audio-only data into training in the KD framework, our student model achieves competitive performance, with an inference speed that is 19 times faster."
   ],
   "p1": 1160,
   "pn": 1164,
   "doi": "10.21437/Interspeech.2024-1680",
   "url": "interspeech_2024/xu24e_interspeech.html"
  },
  "wang24ca_interspeech": {
   "authors": [
    [
     "Zhiyong",
     "Wang"
    ],
    [
     "Ruibo",
     "Fu"
    ],
    [
     "Zhengqi",
     "Wen"
    ],
    [
     "Yuankun",
     "Xie"
    ],
    [
     "Yukun",
     "Liu"
    ],
    [
     "Xiaopeng",
     "Wang"
    ],
    [
     "Xuefei",
     "Liu"
    ],
    [
     "Yongwei",
     "Li"
    ],
    [
     "Jianhua",
     "Tao"
    ],
    [
     "Xin",
     "Qi"
    ],
    [
     "Yi",
     "Lu"
    ],
    [
     "Shuchen",
     "Shi"
    ]
   ],
   "title": "Generalized Fake Audio Detection via Deep Stable Learning",
   "original": "1686",
   "order": 979,
   "page_count": 5,
   "abstract": [
    "Although current fake audio detection approaches have achieved remarkable success on specific datasets, they often fail when evaluated with datasets from different distributions. Previous studies typically address distribution shift by focusing on using extra data or applying extra loss restrictions during training. However, these methods either require a substantial amount of data or complicate the training process. In this work, we propose a stable learning-based training scheme that involves a Sample Weight Learning (SWL) module, addressing distribution shift by decorrelating all selected features via learning weights from training samples. The proposed portable plug-in-like SWL is easy to apply to multiple base models and generalizes them without using extra data during training. Experiments conducted on the ASVspoof datasets clearly demonstrate the effectiveness of SWL in generalizing different models across three evaluation datasets from different distributions."
   ],
   "p1": 4773,
   "pn": 4777,
   "doi": "10.21437/Interspeech.2024-1686",
   "url": "interspeech_2024/wang24ca_interspeech.html"
  },
  "shi24d_interspeech": {
   "authors": [
    [
     "Haoxiang",
     "Shi"
    ],
    [
     "Ziqi",
     "Liang"
    ],
    [
     "Jun",
     "Yu"
    ]
   ],
   "title": "Emotional Cues Extraction and Fusion for Multi-modal Emotion Prediction and Recognition in Conversation",
   "original": "1688",
   "order": 838,
   "page_count": 5,
   "abstract": [
    "Emotion Prediction in Conversation (EPC) aims to forecast the emotions of forthcoming utterances by utilizing preceding dialogues. Previous EPC approaches relied on simple context modeling for emotion extraction, overlooking fine-grained emotion cues at the word level. Additionally, prior works failed to account for the intrinsic differences between modalities, resulting in redundant information. To overcome these limitations, we propose an emotional cues extraction and fusion network, which consists of two stages: a modality-specific learning stage that utilizes word-level labels and prosody learning to construct emotion embedding spaces for each modality, and a two-step fusion stage for integrating multi-modal features. Moreover, the emotion features extracted by our model are also applicable to the Emotion Recognition in Conversation (ERC) task. Experimental results validate the efficacy of the proposed method, demonstrating superior performance on both IEMOCAP and MELD datasets."
   ],
   "p1": 4074,
   "pn": 4078,
   "doi": "10.21437/Interspeech.2024-1688",
   "url": "interspeech_2024/shi24d_interspeech.html"
  },
  "xu24f_interspeech": {
   "authors": [
    [
     "Xuenan",
     "Xu"
    ],
    [
     "Pingyue",
     "Zhang"
    ],
    [
     "Ming",
     "Yan"
    ],
    [
     "Ji",
     "Zhang"
    ],
    [
     "Mengyue",
     "Wu"
    ]
   ],
   "title": "Enhancing Zero-shot Audio Classification using Sound Attribute Knowledge from Large Language Models",
   "original": "1692",
   "order": 986,
   "page_count": 5,
   "abstract": [
    "Zero-shot audio classification aims to recognize and classify a sound class that the model has never seen during training. This paper presents a novel approach for zero-shot audio classification using automatically generated sound attribute descriptions. We propose a list of sound attributes and leverage large language model’s domain knowledge to generate detailed attribute descriptions for each class. In contrast to previous works that primarily relied on class labels or simple descriptions, our method focuses on multi-dimensional innate auditory attributes, capturing different characteristics of sound classes. Additionally, we incorporate a contrastive learning approach to enhance zero-shot learning from textual labels. We validate the effectiveness of our method on VGGSound and AudioSet. Our results demonstrate a substantial improvement in zero-shot classification accuracy. Ablation results show robust performance enhancement, regardless of the model architecture."
   ],
   "p1": 4808,
   "pn": 4812,
   "doi": "10.21437/Interspeech.2024-1692",
   "url": "interspeech_2024/xu24f_interspeech.html"
  },
  "sahipjohn24_interspeech": {
   "authors": [
    [
     "Neha",
     "Sahipjohn"
    ],
    [
     "Ashishkumar",
     "Gudmalwar"
    ],
    [
     "Nirmesh",
     "Shah"
    ],
    [
     "Pankaj",
     "Wasnik"
    ],
    [
     "Rajiv Ratn",
     "Shah"
    ]
   ],
   "title": "DubWise: Video-Guided Speech Duration Control in Multimodal LLM-based Text-to-Speech for Dubbing",
   "original": "1700",
   "order": 608,
   "page_count": 5,
   "abstract": [
    "Audio-visual alignment after dubbing is a challenging research problem. To this end, we propose a novel method, DubWise: Multi-modal Large Language Model (LLM)-based Text-to-Speech (TTS), which can control the speech duration of synthesized speech in such a way that it aligns well with the speaker’s lip movements given in the reference video even when the spoken text is different or in a different language. To accomplish this, we propose to utilize cross-modal attention techniques in a pre-trained GPT-based TTS. We combine linguistic tokens from text, speaker identity tokens via a voice cloning network, and video tokens via a proposed duration controller network. We demonstrate the effectiveness of our system on the Lip2Wav-Chemistry and LRS2 datasets. Also, the proposed method achieves improved lip sync and naturalness compared to the SOTAs for the same language but different text (i.e., non-parallel) and the different language, different text (i.e., cross-lingual) scenarios."
   ],
   "p1": 2960,
   "pn": 2964,
   "doi": "10.21437/Interspeech.2024-1700",
   "url": "interspeech_2024/sahipjohn24_interspeech.html"
  },
  "xie24d_interspeech": {
   "authors": [
    [
     "Zeyu",
     "Xie"
    ],
    [
     "Baihan",
     "Li"
    ],
    [
     "Xuenan",
     "Xu"
    ],
    [
     "Zheng",
     "Liang"
    ],
    [
     "Kai",
     "Yu"
    ],
    [
     "Mengyue",
     "Wu"
    ]
   ],
   "title": "FakeSound: Deepfake General Audio Detection",
   "original": "1703",
   "order": 24,
   "page_count": 5,
   "abstract": [
    "With the advancement of audio generation, generative models can produce highly realistic audios. However, the proliferation of deepfake general audio can pose negative consequences. Therefore, we propose a new task, deepfake general audio detection, which aims to identify whether audio content is manipulated and to locate deepfake regions. Leveraging an automated manipulation pipeline, a dataset named FakeSound for deepfake general audio detection is proposed, and samples can be viewed on website https://FakeSoundData.github.io. The average binary accuracy of humans on all test sets is consistently below 0.6, which indicates the difficulty humans face in discerning deepfake audio and affirms the efficacy of the FakeSound dataset. A deepfake detection model utilizing a general audio pre-trained model is proposed as a benchmark system. Experimental results demonstrate that the performance of the proposed model surpasses the state-of-the-art in deepfake speech detection and human testers."
   ],
   "p1": 112,
   "pn": 116,
   "doi": "10.21437/Interspeech.2024-1703",
   "url": "interspeech_2024/xie24d_interspeech.html"
  },
  "kinnunen24_interspeech": {
   "authors": [
    [
     "Tomi H.",
     "Kinnunen"
    ],
    [
     "Rosa",
     "Gonzalez Hautamäki"
    ],
    [
     "Xin",
     "Wang"
    ],
    [
     "Junichi",
     "Yamagishi"
    ]
   ],
   "title": "Speaker Detection by the Individual Listener and the Crowd: Parametric Models Applicable to Bonafide and Deepfake Speech",
   "original": "1704",
   "order": 754,
   "page_count": 5,
   "abstract": [
    "Subjective speaker detection, whether for bonafide (real) or spoofed (fake) speech, is often implemented through crowdsourcing to facilitate comparison of systems, with less attention  paid to the source of the ratings--the listener. We characterize speaker detection both at the level of listener and the crowd. Each listener possesses certain sensitivity and bias for observing speaker differences. By combining detection model with random between-listener effects, we obtain a generalized linear mixed effects (GLME) model, demonstrated here for two different tasks. The first one involves bonafide data from VoxCeleb1 under a biased set-up containing varied role-play instructions; the second one, focused on spoofing, presents re-analysis of the ASVspoof 2019 subjective data. Our GLME enables sampling listeners and obtaining parametric detection error trade-off (DET) profiles and equal error rates (EERs)."
   ],
   "p1": 3654,
   "pn": 3658,
   "doi": "10.21437/Interspeech.2024-1704",
   "url": "interspeech_2024/kinnunen24_interspeech.html"
  },
  "shi24e_interspeech": {
   "authors": [
    [
     "Ying",
     "Shi"
    ],
    [
     "Lantian",
     "Li"
    ],
    [
     "Shi",
     "Yin"
    ],
    [
     "Dong",
     "Wang"
    ],
    [
     "Jiqing",
     "Han"
    ]
   ],
   "title": "Serialized Output Training by Learned Dominance",
   "original": "1710",
   "order": 144,
   "page_count": 5,
   "abstract": [
    "Serialized Output Training (SOT) has showcased state-of-the-art performance in multi-talker speech recognition by sequentially decoding the speech of individual speakers. To address the challenging label-permutation issue, prior methods have relied on either the Permutation Invariant Training (PIT) or the time-based First-In-First-Out (FIFO) rule. This study presents a model-based serialization strategy that incorporates an auxiliary module into the Attention Encoder-Decoder architecture, autonomously identifying the crucial factors to order the output sequence of the speech components in multi-talker speech. Experiments conducted on the LibriSpeech and LibriMix databases reveal that our approach significantly outperforms the PIT and FIFO baselines in both 2-mix and 3-mix scenarios. Further analysis shows that the serialization module identifies dominant speech components in a mixture by factors including loudness and gender, and orders speech components based on the dominance score. "
   ],
   "p1": 712,
   "pn": 716,
   "doi": "10.21437/Interspeech.2024-1710",
   "url": "interspeech_2024/shi24e_interspeech.html"
  },
  "yusuf24b_interspeech": {
   "authors": [
    [
     "Bolaji",
     "Yusuf"
    ],
    [
     "Jan Honza",
     "Cernocky"
    ],
    [
     "Murat",
     "Saraçlar"
    ]
   ],
   "title": "Pretraining End-to-End Keyword Search with Automatically Discovered Acoustic Units",
   "original": "1713",
   "order": 1038,
   "page_count": 5,
   "abstract": [
    "End-to-end (E2E) keyword search (KWS) has emerged as an alternative and complimentary approach to conventional keyword search which depends on the output of automatic speech recognition (ASR) systems. While E2E methods greatly simplify the KWS pipeline, they generally have worse performance than their ASR-based counterparts, which can benefit from pretraining with untranscribed data. In this work, we propose a method for pretraining E2E KWS systems with untranscribed data, which involves using acoustic unit discovery (AUD) to obtain discrete units for untranscribed data and then learning to locate sequences of such units in the speech. We conduct experiments across languages and AUD systems: we show that finetuning such a model significantly outperforms a model trained from scratch, and the performance improvements are generally correlated with the quality of the AUD system used for pretraining."
   ],
   "p1": 5068,
   "pn": 5072,
   "doi": "10.21437/Interspeech.2024-1713",
   "url": "interspeech_2024/yusuf24b_interspeech.html"
  },
  "liu24k_interspeech": {
   "authors": [
    [
     "Wei",
     "Liu"
    ],
    [
     "Jingyong",
     "Hou"
    ],
    [
     "Dong",
     "Yang"
    ],
    [
     "Muyong",
     "Cao"
    ],
    [
     "Tan",
     "Lee"
    ]
   ],
   "title": "LUPET: Incorporating Hierarchical Information Path into Multilingual ASR",
   "original": "1714",
   "order": 819,
   "page_count": 5,
   "abstract": [
    "Toward high-performance multilingual automatic speech recognition (ASR), various types of linguistic information and model design have demonstrated their effectiveness independently. They include language identity (LID), phoneme information, language-specific processing modules, and cross-lingual self-supervised speech representation. It is expected that leveraging their benefits synergistically in a unified solution would further improve the overall system performance. This paper presents a novel design of a hierarchical information path, named LUPET, which sequentially encodes, from the shallow layers to deep layers, multiple aspects of linguistic and acoustic information at diverse granularity scales. The path starts from LID prediction, followed by acoustic unit discovery, phoneme sharing, and finally token recognition routed by a mixture-of-expert.  ASR experiments are carried out on 10 languages in the Common Voice corpus. The results demonstrate the superior performance of LUPET as compared to the baseline systems. Most importantly, LUPET effectively mitigates the issue of performance compromise of high-resource languages with low-resource ones in the multilingual setting."
   ],
   "p1": 3979,
   "pn": 3983,
   "doi": "10.21437/Interspeech.2024-1714",
   "url": "interspeech_2024/liu24k_interspeech.html"
  },
  "xu24g_interspeech": {
   "authors": [
    [
     "Jing",
     "Xu"
    ],
    [
     "Minglin",
     "Wu"
    ],
    [
     "Xixin",
     "Wu"
    ],
    [
     "Helen",
     "Meng"
    ]
   ],
   "title": "Seamless Language Expansion: Enhancing Multilingual Mastery in Self-Supervised Models",
   "original": "1716",
   "order": 1019,
   "page_count": 5,
   "abstract": [
    "Self-supervised (SSL) models have shown great performance in various downstream tasks. However, they are typically developed for limited languages, and may encounter new languages in real-world. Developing a SSL model for each new language is costly. Thus, it is vital to figure out how to efficiently adapt existed SSL models to a new language without impairing its original abilities. We propose adaptation methods which integrate LoRA to existed SSL models to extend new language. We also develop preservation strategies which include data combination and re-clustering to retain abilities on existed languages. Applied to mHuBERT, we investigate their effectiveness on speech re-synthesis task. Experiments show that our adaptation methods enable mHuBERT to be applied to a new language (Mandarin) with MOS value increased about 1.6 and the relative value of WER reduced up to 61.72%. Also, our preservation strategies ensure that the performance on both existed and new languages remains intact."
   ],
   "p1": 4973,
   "pn": 4977,
   "doi": "10.21437/Interspeech.2024-1716",
   "url": "interspeech_2024/xu24g_interspeech.html"
  },
  "schubert24_interspeech": {
   "authors": [
    [
     "Martha",
     "Schubert"
    ],
    [
     "Daniel",
     "Duran"
    ],
    [
     "Ingo",
     "Siegert"
    ]
   ],
   "title": "Challenges of German Speech Recognition: A Study on Multi-ethnolectal Speech Among Adolescents",
   "original": "1717",
   "resource": "https://doi.org/10.5281/zenodo.12734413",
   "order": 625,
   "page_count": 5,
   "abstract": [
    "Despite significant advancements in speech recognition systems, challenges persist in accurately interpreting spontaneous speech from underrepresented groups like non-standard speakers or younger individuals. The difficulty increases when these conditions overlap. To further explore this topic, we employ a dataset featuring spontaneous as well as read speech from young speakers in Germany, including both, speakers from mono-ethnic and multi-ethnic backgrounds. Our study involves a comparative analysis of speech recognition performance, incorporating gender considerations, using three distinct Automatic Speech Recognition (ASR) engines: Whisper (OpenAI), NeMo (NVIDIA), and Wav2Vec2.0 (Meta AI). Furthermore, we conduct a comprehensive error analysis on the automatically generated transcripts, employing part-of-speech (POS) tagging. This allows us to discern the word types that pose the greatest challenge for comprehension by the ASR engines."
   ],
   "p1": 3045,
   "pn": 3049,
   "doi": "10.21437/Interspeech.2024-1717",
   "url": "interspeech_2024/schubert24_interspeech.html"
  },
  "kim24p_interspeech": {
   "authors": [
    [
     "Taewoo",
     "Kim"
    ],
    [
     "Choonsang",
     "Cho"
    ],
    [
     "Young Han",
     "Lee"
    ]
   ],
   "title": "Period Singer: Integrating Periodic and Aperiodic Variational Autoencoders for Natural-Sounding End-to-End Singing Voice Synthesis",
   "original": "1720",
   "order": 384,
   "page_count": 5,
   "abstract": [
    "In this paper, we present Period Singer, a novel end-to-end singing voice synthesis (SVS) model that utilizes variational inference for periodic and aperiodic components, aimed at producing natural-sounding waveforms. Recent end-to-end SVS models have demonstrated the capability of synthesizing high-fidelity singing voices. However, owing to deterministic pitch conditioning, they do not fully address the one-to-many problem. To address this problem, we present the Period Singer architecture, which integrates variational autoencoders for the periodic and aperiodic components. Additionally, our methodology eliminates the dependency on an external aligner by estimating the phoneme alignment through a monotonic alignment search within note boundaries. Our empirical evaluations show that Period Singer outperforms existing end-to-end SVS models on Mandarin and Korean datasets. The efficacy of the proposed method was further corroborated by ablation studies."
   ],
   "p1": 1875,
   "pn": 1879,
   "doi": "10.21437/Interspeech.2024-1720",
   "url": "interspeech_2024/kim24p_interspeech.html",
   "erratum": "<p>The name of the second author is <b>Choongsang Cho</b> instead of Choonsang Cho.\n</p>"
  },
  "hodoshima24_interspeech": {
   "authors": [
    [
     "Nao",
     "Hodoshima"
    ]
   ],
   "title": "Effects of talker and playback rate of reverberation-induced speech on speech intelligibility of older adults",
   "original": "1721",
   "order": 870,
   "page_count": 4,
   "abstract": [
    "Speech intelligibility in noise and reverberation is generally lower for older adults (OAs) than for young adults (YAs). One solution might be the Lombard effect or reverberation-induced speech. This study investigates whether the intelligibility of reverberation-induced speech depends on talkers and playback rates for OAs. Four YAs recorded sentences under quiet (Q) and reverberation (R). In R, the reverberant speech was fed back to the talkers via headphones. The playback rate was 80% (slow), 100% (original), and 120% (fast). Twenty-four OAs carried out word identification tests in reverberation. The results showed that R was significantly more intelligible than Q for two of the four talkers. Slow speech was significantly more intelligible than original and fast speech, and original speech was significantly more intelligible than fast speech. The results suggest that reverberation-induced speech might increase the intelligibility of public-address announcements for OAs."
   ],
   "p1": 4229,
   "pn": 4232,
   "doi": "10.21437/Interspeech.2024-1721",
   "url": "interspeech_2024/hodoshima24_interspeech.html"
  },
  "behera24_interspeech": {
   "authors": [
    [
     "Swarup Ranjan",
     "Behera"
    ],
    [
     "Abhishek",
     "Dhiman"
    ],
    [
     "Karthik",
     "Gowda"
    ],
    [
     "Aalekhya Satya",
     "Narayani"
    ]
   ],
   "title": "FastAST: Accelerating Audio Spectrogram Transformer via Token Merging and Cross-Model Knowledge Distillation",
   "original": "1723",
   "order": 971,
   "page_count": 5,
   "abstract": [
    "Audio classification models, particularly the Audio Spectrogram Transformer (AST), play a crucial role in efficient audio analysis. However, optimizing their efficiency without compromising accuracy remains a challenge. In this paper, we introduce FastAST, a framework that integrates Token Merging (ToMe) into the AST framework. FastAST enhances inference speed without requiring extensive retraining by merging similar tokens in audio spectrograms. Furthermore, during training, FastAST brings about significant speed improvements. The experiments indicate that FastAST can increase audio classification throughput with minimal impact on accuracy. To mitigate the accuracy impact, we integrate Cross-Model Knowledge Distillation (CMKD) into the FastAST framework. Integrating ToMe and CMKD into AST results in improved accuracy compared to AST while maintaining faster inference speeds. FastAST represents a step towards real-time, resource-efficient audio analysis."
   ],
   "p1": 4733,
   "pn": 4737,
   "doi": "10.21437/Interspeech.2024-1723",
   "url": "interspeech_2024/behera24_interspeech.html"
  },
  "ge24_interspeech": {
   "authors": [
    [
     "Zirui",
     "Ge"
    ],
    [
     "Xinzhou",
     "Xu"
    ],
    [
     "Haiyan",
     "Guo"
    ],
    [
     "Tingting",
     "Wang"
    ],
    [
     "Zhen",
     "Yang"
    ],
    [
     "Björn W.",
     "Schuller"
    ]
   ],
   "title": "DGPN: A Dual Graph Prototypical Network for Few-Shot Speech Spoofing Algorithm Recognition",
   "original": "1724",
   "order": 234,
   "page_count": 5,
   "abstract": [
    "As synthetic speech technologies rapidly advance, accurately classifying these synthesis algorithms has become increasingly critical in the speech anti-spoofing. Nevertheless, in the incipient stage of emerging spoofing algorithms, the acquisition of ample generated speech samples is often constrained, impeding the efficacy of conventional models. To this end, we introduce a novel methodology within the realm of few-shot learning, named Dual Graph Prototypical Network (DGPN), in view of  this limitation for the Speech Spoofing Algorithm Recognition (SSAR) task. The proposed method consists of intra-speech graph and inter-speech graph modules, where the former employs graph attention networks to model the low-level representations of an utterance, and the latter utilizes graph neural networks to depict high-level representations of different utterances. Experimental evaluations demonstrate that the proposed method outperforms existing models in classification accuracy, showcasing its effectiveness in addressing the challenge of the few-shot SSAR task."
   ],
   "p1": 1125,
   "pn": 1129,
   "doi": "10.21437/Interspeech.2024-1724",
   "url": "interspeech_2024/ge24_interspeech.html"
  },
  "li24ha_interspeech": {
   "authors": [
    [
     "Baihan",
     "Li"
    ],
    [
     "Zeyu",
     "Xie"
    ],
    [
     "Xuenan",
     "Xu"
    ],
    [
     "Yiwei",
     "Guo"
    ],
    [
     "Ming",
     "Yan"
    ],
    [
     "Ji",
     "Zhang"
    ],
    [
     "Kai",
     "Yu"
    ],
    [
     "Mengyue",
     "Wu"
    ]
   ],
   "title": "DiveSound: LLM-Assisted Automatic Taxonomy Construction for Diverse Audio Generation",
   "original": "1726",
   "order": 984,
   "page_count": 5,
   "abstract": [
    "Audio generation has attracted significant attention. Despite remarkable enhancement in audio quality, existing models overlook diversity evaluation. This is partially due to the lack of a systematic sound class diversity framework and a matching dataset. To address these issues, we propose DiveSound, a novel framework for constructing multimodal datasets with in-class diversified taxonomy, assisted by large language models. As both textual and visual information can be utilized to guide diverse generation, DiveSound leverages multimodal contrastive representations in data construction. Our framework is highly autonomous and can be easily scaled up. We provide a text-audio-image aligned diversity dataset whose sound event class tags have an average of 2.42 subcategories. Text-to-audio experiments on the constructed dataset show a substantial increase of diversity with the help of the guidance of visual information. Our samples are available at https://divesounddemo.github.io"
   ],
   "p1": 4798,
   "pn": 4802,
   "doi": "10.21437/Interspeech.2024-1726",
   "url": "interspeech_2024/li24ha_interspeech.html"
  },
  "miodonska24_interspeech": {
   "authors": [
    [
     "Zuzanna",
     "Miodonska"
    ],
    [
     "Michal",
     "Kręcichwost"
    ],
    [
     "Ewa",
     "Kwaśniok"
    ],
    [
     "Agata",
     "Sage"
    ],
    [
     "Pawel",
     "Badura"
    ]
   ],
   "title": "Frication noise features of Polish voiceless dental fricative and affricate produced by children with and without speech disorder",
   "original": "1731",
   "order": 641,
   "page_count": 5,
   "abstract": [
    "The study presents frication noise acoustic features of Polish dental voiceless sibilants (fricative /s/ and affricate /t͡s/) in the speech samples collected from 106 children (83 with normative dental articulation and 23 with disordered—interdental—articulation) aged 4;11–8;0. We aimed to 1) verify the differences between the characteristics of the frication noise accompanying these two sounds and 2) investigate the influence of interdentality on the frication noise features. The analysis employed features of the noise band (fricative formants, formant-related measures, and noise energies) and linear-mixed effect models. The results showed significant acoustic differences between the voiceless dental fricative and affricate. Experiments suggest that the place of articulation (dental/interdental) can also be distinguished based on the spectral features of the frication band; this finding may be employed in computer-aided speech diagnosis tools."
   ],
   "p1": 3125,
   "pn": 3129,
   "doi": "10.21437/Interspeech.2024-1731",
   "url": "interspeech_2024/miodonska24_interspeech.html"
  },
  "hu24e_interspeech": {
   "authors": [
    [
     "Ying",
     "Hu"
    ],
    [
     "Huamin",
     "Yang"
    ],
    [
     "Hao",
     "Huang"
    ],
    [
     "Liang",
     "He"
    ]
   ],
   "title": "Cross-modal Features Interaction-and-Aggregation Network with Self-consistency Training for Speech Emotion Recognition",
   "original": "1733",
   "order": 482,
   "page_count": 5,
   "abstract": [
    "In recent years, much research has been into speech emotion recognition (SER) using multimodal data. Selective fusion of the features from different modalities is critical for  multimodal SER. In this paper, we propose a cross-modal features interaction-and-aggregation network (CFIA-Net) with self-consistency training for SER. Specifically, we design a cross-modal features interaction-and-aggregation (CFIA) module to adaptively interact and integrate the features of audio and text modalities. Moreover, we introduce a self-consistency training strategy, which exploits the features from deeper layers to supervise those from shallower ones to obtain the SER task-related information. The experimental results show that compared with other bimodal SER methods, the CFIA-Net achieves the state-of-the-art performance on the weighted accuracy (WA) of 83.37%  and unweighted accuracy (UA) of 83.67% on the IEMOCAP dataset."
   ],
   "p1": 2335,
   "pn": 2339,
   "doi": "10.21437/Interspeech.2024-1733",
   "url": "interspeech_2024/hu24e_interspeech.html"
  },
  "seong24b_interspeech": {
   "authors": [
    [
     "Donghyun",
     "Seong"
    ],
    [
     "Hoyoung",
     "Lee"
    ],
    [
     "Joon-Hyuk",
     "Chang"
    ]
   ],
   "title": "TSP-TTS: Text-based Style Predictor with Residual Vector Quantization for Expressive Text-to-Speech",
   "original": "1734",
   "order": 365,
   "page_count": 5,
   "abstract": [
    "Expressive text-to-speech (TTS) aims to synthesize better human-like speech by incorporating diverse speech styles or emotions. While most expressive TTS models rely on reference speech to condition the style of the generated speech, they often fail to generate speech of regular quality. To ensure consistent speech quality, we propose an expressive TTS conditioned on style representation extracted from the text itself. To implement this text-based style predictor, we design a style module incorporating residual vector quantization. Furthermore, the style representation is enhanced through style-to-text alignment and a mel decoder with style hierarchical layer normalization (SHLN). Our experimental findings demonstrate that our proposed model accurately estimates style representation, enabling the generation of high-quality speech without the need for reference speech."
   ],
   "p1": 1780,
   "pn": 1784,
   "doi": "10.21437/Interspeech.2024-1734",
   "url": "interspeech_2024/seong24b_interspeech.html"
  },
  "zhao24g_interspeech": {
   "authors": [
    [
     "Ziping",
     "Zhao"
    ],
    [
     "Tian",
     "Gao"
    ],
    [
     "Haishuai",
     "Wang"
    ],
    [
     "Björn",
     "Schuller"
    ]
   ],
   "title": "MFDR: Multiple-stage Fusion and Dynamically Refined Network for Multimodal Emotion Recognition",
   "original": "1735",
   "order": 767,
   "page_count": 5,
   "abstract": [
    "Emotion recognition in conversation should not rely solely on discovering emotion keywords but also make comprehensive judgments after considering the context. To this end, we propose the MFDR to efficiently integrate acoustic and textual information. Specifically, acoustic-word combination and context perception are modeled sequentially in stages through the Sliding Adaptive Window Attention (SAWA) and Gated Context Perception Unit. More importantly, without additional memory overhead, SAWA allows the perception range to be adaptively adjusted according to the correlation strength to solve the misalignment and information loss caused by window truncation, modeling fusion under variable granularity. Furthermore, emotion refinement through Dynamic Frame Convolution strips out emotion-irrelevant frames, thereby generating a compact and emotionally discriminative fusion representation. The efficacy of MFDR is confirmed by IEMOCAP and CMU-MOSEI, where it demonstrates promising performance."
   ],
   "p1": 3719,
   "pn": 3723,
   "doi": "10.21437/Interspeech.2024-1735",
   "url": "interspeech_2024/zhao24g_interspeech.html"
  },
  "maji24_interspeech": {
   "authors": [
    [
     "Bubai",
     "Maji"
    ],
    [
     "Rajlakshmi",
     "Guha"
    ],
    [
     "Aurobinda",
     "Routray"
    ],
    [
     "Shazia",
     "Nasreen"
    ],
    [
     "Debabrata",
     "Majumdar"
    ]
   ],
   "title": "Investigation of Layer-Wise Speech Representations in Self-Supervised Learning Models: A Cross-Lingual Study in Detecting Depression",
   "original": "1737",
   "order": 620,
   "page_count": 5,
   "abstract": [
    "Automated depression detection (ADD) from speech signals allows early identification and intervention, reducing costs to medical healthcare. However, most of the existing ADD studies are trained and evaluated on a single language corpus with a lack of sufficient training data. These limits the generalizability of models in other demographic groups in distinct languages. In this study, Semi-Supervised Learning (SSL) was applied to depression detection on two different language datasets. We evaluate the HuBERT and WavLM models in single-language, mixed-language, and cross-language scenarios to investigate the generalization to diverse populations at different recording environments. Moreover, we thoroughly analyzed layer-wise performance in the upstream model and pooling methods (i.e. max and mean pooling) in the downstream task. The results show that the WavLM features generalize better than the HuBERT features. Our best model surpasses previous works in the frozen upstream conditions."
   ],
   "p1": 3020,
   "pn": 3024,
   "doi": "10.21437/Interspeech.2024-1737",
   "url": "interspeech_2024/maji24_interspeech.html"
  },
  "hao24c_interspeech": {
   "authors": [
    [
     "Yun",
     "Hao"
    ],
    [
     "Reihaneh",
     "Amooie"
    ],
    [
     "Wietse",
     "de Vries"
    ],
    [
     "Thomas",
     "Tienkamp"
    ],
    [
     "Rik",
     "van Noord"
    ],
    [
     "Martijn",
     "Wieling"
    ]
   ],
   "title": "Exploring Self-Supervised Speech Representations for Cross-lingual Acoustic-to-Articulatory Inversion",
   "original": "1740",
   "order": 945,
   "page_count": 5,
   "abstract": [
    "Acoustic-to-articulatory inversion (AAI) is the process of inferring vocal tract movements from acoustic speech signals. Despite its diverse potential applications, AAI research in languages other than English is scarce due to the challenges of collecting articulatory data. In recent years, self-supervised learning (SSL) based representations have shown great potential for addressing low-resource tasks. We utilize wav2vec 2.0 representations and English articulatory data for training AAI systems and investigates their effectiveness for a different language: Dutch. Results show that using mms-1b features can reduce the cross-lingual performance drop to less than 30%. We found that increasing model size, selecting intermediate rather than final layers, and including more pre-training data improved AAI performance. By contrast, fine-tuning on an ASR task did not. Our results therefore highlight promising prospects for implementing SSL in AAI for languages with limited articulatory data."
   ],
   "p1": 4603,
   "pn": 4607,
   "doi": "10.21437/Interspeech.2024-1740",
   "url": "interspeech_2024/hao24c_interspeech.html"
  },
  "liu24l_interspeech": {
   "authors": [
    [
     "Wei",
     "Liu"
    ],
    [
     "Jingyong",
     "Hou"
    ],
    [
     "Dong",
     "Yang"
    ],
    [
     "Muyong",
     "Cao"
    ],
    [
     "Tan",
     "Lee"
    ]
   ],
   "title": "A Parameter-efficient Language Extension Framework for Multilingual ASR",
   "original": "1745",
   "order": 809,
   "page_count": 5,
   "abstract": [
    "Covering all languages with a multilingual speech recognition model (MASR) is very difficult. Performing language extension on top of an existing MASR is a desirable choice. In this study, the MASR continual learning problem is probabilistically decomposed into language identity prediction (LP) and cross-lingual adaptation (XLA) sub-problems. Based on this, we propose an architecture-based framework for language extension that can fundamentally solve catastrophic forgetting, debudded as PELE. PELE is designed to be parameter-efficient, incrementally incorporating an add-on module to adapt to a new language. Specifically, different parameter-efficient fine-tuning (PEFT) modules and their variants are explored as potential candidates to perform XLA. Experiments are carried out on 5 new languages with a wide range of low-resourced data sizes. The best-performing PEFT candidate can achieve satisfactory performance across all languages and demonstrates superiority in three of five languages over the continual joint learning setting. Notably, PEFT methods focusing on weight parameters or input features are revealed to be limited in performance, showing significantly inferior extension capabilities compared to inserting a lightweight module in between layers such as an Adapter."
   ],
   "p1": 3929,
   "pn": 3933,
   "doi": "10.21437/Interspeech.2024-1745",
   "url": "interspeech_2024/liu24l_interspeech.html"
  },
  "mohapatra24b_interspeech": {
   "authors": [
    [
     "Debasish Ray",
     "Mohapatra"
    ],
    [
     "Victor",
     "Zappi"
    ],
    [
     "Sidney",
     "Fels"
    ]
   ],
   "title": "2.5D Vocal Tract Modeling: Bridging Low-Dimensional Efficiency with 3D Accuracy",
   "original": "1749",
   "order": 5,
   "page_count": 5,
   "abstract": [
    "We introduce an extended 2D (2.5D) wave solver that blends the computational efficiency of low-dimensional models with the accuracy of 3D approaches tailored for simulating tube geometries similar to vocal tracts. Unlike 1D and 2D models limited to radial symmetry, our lightweight 2.5D finite-difference time-domain solver handles irregular geometries bound only to mid-sagittal symmetry. We validated our model against state-of-the-art 2D and 3D solvers for three different vocal tract geometries, each having a unique cross-sectional shape. Results show that the frequency response of 2.5D simulations closely aligns with 3D up to 12 kHz with a Pearson correlation coefficient greater than 0.8 for all geometries. The proposed model also produces effects of higher-order modes associated with non-cylindrical vocal tracts, surpassing the limitations of the advanced 1D and 2D solvers. Moreover, it achieved a speed-up factor close to an order of magnitude compared to the 2D and 3D models."
   ],
   "p1": 17,
   "pn": 21,
   "doi": "10.21437/Interspeech.2024-1749",
   "url": "interspeech_2024/mohapatra24b_interspeech.html"
  },
  "bandekar24_interspeech": {
   "authors": [
    [
     "Jesuraj",
     "Bandekar"
    ],
    [
     "Sathvik",
     "Udupa"
    ],
    [
     "Prasanta Kumar",
     "Ghosh"
    ]
   ],
   "title": "Articulatory synthesis using representations learnt through phonetic label-aware contrastive loss",
   "original": "1756",
   "order": 87,
   "page_count": 5,
   "abstract": [
    "Articulatory speech synthesis is a challenging task which requires mapping of time-varying articulatory trajectories and speech. In recent years, deep learning methods have been proposed for speech synthesis which have achieved significant progress towards human-like speech generation. However, articulatory speech synthesis is far from human-level performance. Thus, in this work, we further improve the results of articulatory speech synthesis to enhance synthesis quality. We consider a deep learning-based sequence-to-sequence baseline. We improve upon this network using a novel approach of labelaware contrastive learning using framewise phoneme alignment to learn better representations of the articulatory trajectories. With this approach, we obtain a relative improvement in Word Error Rate (WER) of 5.8% over the baseline. We also conduct mean opinion score (MOS) tests and other objective metrics to further evaluate our proposed models."
   ],
   "p1": 427,
   "pn": 431,
   "doi": "10.21437/Interspeech.2024-1756",
   "url": "interspeech_2024/bandekar24_interspeech.html"
  },
  "si24_interspeech": {
   "authors": [
    [
     "Yongjie",
     "Si"
    ],
    [
     "Yanxiong",
     "Li"
    ],
    [
     "Jialong",
     "Li"
    ],
    [
     "Jiaxin",
     "Tan"
    ],
    [
     "Qianhua",
     "He"
    ]
   ],
   "title": "Fully Few-shot Class-incremental Audio Classification Using Expandable Dual-embedding Extractor",
   "original": "1758",
   "order": 982,
   "page_count": 5,
   "abstract": [
    "It’s assumed that training data is sufficient in base session of few-shot class-incremental audio classification. However, it’s difficult to collect abundant samples for model training in base session in some practical scenarios due to the data scarcity of some classes. This paper explores a new problem of fully few-shot class-incremental audio classification with few training samples in all sessions. Moreover, we propose a method using expandable dual-embedding extractor to solve it. The proposed model consists of an embedding extractor and an expandable classifier. The embedding extractor consists of a pretrained Audio Spectrogram Transformer (AST) and a finetuned AST. The expandable classifier consists of prototypes and each prototype represents a class. Experiments are conducted on three datasets (LS-100, NSynth-100 and FSC-89). Results show that our method exceeds seven baseline ones in average accuracy with statistical significance. Code is at: https://github.com/YongjieSi/EDE."
   ],
   "p1": 4788,
   "pn": 4792,
   "doi": "10.21437/Interspeech.2024-1758",
   "url": "interspeech_2024/si24_interspeech.html"
  },
  "rabatin24_interspeech": {
   "authors": [
    [
     "Rastislav",
     "Rabatin"
    ],
    [
     "Frank",
     "Seide"
    ],
    [
     "Ernie",
     "Chang"
    ]
   ],
   "title": "Navigating the Minefield of MT Beam Search in Cascaded Streaming Speech Translation",
   "original": "1759",
   "order": 76,
   "page_count": 5,
   "abstract": [
    "We adapt the well-known beam-search algorithm for machine translation to operate in a cascaded real-time speech translation system. This proved to be more complex than initially anticipated, due to four key challenges: (1) real-time processing of intermediate and final transcriptions with incomplete words from ASR, (2) emitting intermediate and final translations with minimal user perceived latency, (3) handling beam search hypotheses that have unequal length and different model state, and (4) handling sentence boundaries. Previous work in the field of simultaneous machine translation only implemented greedy decoding. We present a beam-search realization that handles all of the above, providing guidance through the minefield of challenges. Our approach increases the BLEU score by 1 point compared to greedy search, reduces the CPU time by up to 40% and character flicker rate by 20+% compared to a baseline heuristic that just retranslates input repeatedly."
   ],
   "p1": 372,
   "pn": 376,
   "doi": "10.21437/Interspeech.2024-1759",
   "url": "interspeech_2024/rabatin24_interspeech.html"
  },
  "dong24_interspeech": {
   "authors": [
    [
     "Ling",
     "Dong"
    ],
    [
     "Zhengtao",
     "Yu"
    ],
    [
     "Wenjun",
     "Wang"
    ],
    [
     "Yuxin",
     "Huang"
    ],
    [
     "Shengxiang",
     "Gao"
    ],
    [
     "Guojiang",
     "Zhou"
    ]
   ],
   "title": "Integrating Speech Self-Supervised Learning Models and Large Language Models for ASR",
   "original": "1760",
   "order": 814,
   "page_count": 5,
   "abstract": [
    "The integration of Large Language Models (LLMs) and speech Self-Supervised Learning (SSL) models has garnered increasing attention due to their potential to enhance tasks such as Automatic Speech Recognition (ASR) and Speech Translation (ST), thereby improving the model’s “listening and writing” capabilities without requiring a large amount of labeled data. However, effectively aligning speech representations with the LLMs remains a challenge. In this paper, we explore the potential of connecting a speech pretrained model with a decoder-only LLM for the ASR task under the encoder-decoder framework. We employ a word boundary-aware compression method along with the optimal transport algorithm to mitigate the modality gap between speech and text in both length and semantic. Experiments conducted on the LibriSpeech dataset demonstrate that our proposed method achieves satisfactory results compared to mainstream End-to-End ASR models."
   ],
   "p1": 3954,
   "pn": 3958,
   "doi": "10.21437/Interspeech.2024-1760",
   "url": "interspeech_2024/dong24_interspeech.html"
  },
  "jiang24c_interspeech": {
   "authors": [
    [
     "Anbai",
     "Jiang"
    ],
    [
     "Bing",
     "Han"
    ],
    [
     "Zhiqiang",
     "Lv"
    ],
    [
     "Yufeng",
     "Deng"
    ],
    [
     "Wei-Qiang",
     "Zhang"
    ],
    [
     "Xie",
     "Chen"
    ],
    [
     "Yanmin",
     "Qian"
    ],
    [
     "Jia",
     "Liu"
    ],
    [
     "Pingyi",
     "Fan"
    ]
   ],
   "title": "AnoPatch: Towards Better Consistency in Machine Anomalous Sound Detection",
   "original": "1761",
   "order": 23,
   "page_count": 5,
   "abstract": [
    "Large pre-trained models have demonstrated dominant performances in multiple areas, where the consistency between pre-training and fine-tuning is the key to success. However, few works reported satisfactory results of pre-trained models for the machine anomalous sound detection (ASD) task. This may be caused by the inconsistency of the pre-trained model and the inductive bias of machine audio, resulting in inconsistency in data and architecture. Thus, we propose AnoPatch which utilizes a ViT backbone pre-trained on AudioSet and fine-tunes it on machine audio. It is believed that machine audio is more related to audio datasets than speech datasets, and modeling it from patch level suits the sparsity of machine audio. As a result, AnoPatch showcases state-of-the-art (SOTA) performances on the DCASE 2020 ASD dataset and the DCASE 2023 ASD dataset. We also compare multiple pre-trained models and empirically demonstrate that better consistency yields considerable improvement."
   ],
   "p1": 107,
   "pn": 111,
   "doi": "10.21437/Interspeech.2024-1761",
   "url": "interspeech_2024/jiang24c_interspeech.html"
  },
  "manhtienanh24_interspeech": {
   "authors": [
    [
     "Nguyen",
     "Manh Tien Anh"
    ],
    [
     "Thach",
     "Ho Sy"
    ]
   ],
   "title": "Improving Speech Recognition with Prompt-based Contextualized ASR and LLM-based Re-predictor",
   "original": "1762",
   "order": 149,
   "page_count": 5,
   "abstract": [
    "In recent years, advancements in automatic speech recognition (ASR) systems have led to their widespread use in applications such as call center bots and virtual assistants. However, these systems encounter challenges in adverse speech conditions, lack of contextual information, and recognizing rare words. In this paper, we propose a novel architecture to tackle these limitations by integrating Large Language Models (LLMs) and prompt mechanisms, aiming to enhance ASR accuracy. By using a pre-trained text encoder with a text adapter for task-specific adaptation and an efficient LLM-based re-prediction mechanism, our method has shown remarkable results in various real-world scenarios. Our proposed system achieves an average relative word error rate improvement of 27% for conventional tasks, 30% for utterance-level contextual tasks, and 33% for word-level biasing tasks compared to a baseline ASR system on multiple public datasets."
   ],
   "p1": 737,
   "pn": 741,
   "doi": "10.21437/Interspeech.2024-1762",
   "url": "interspeech_2024/manhtienanh24_interspeech.html"
  },
  "taylor24_interspeech": {
   "authors": [
    [
     "Katelyn",
     "Taylor"
    ],
    [
     "Amelia",
     "Gully"
    ],
    [
     "Helena",
     "Daffern"
    ]
   ],
   "title": "Familiar and Unfamiliar Speaker Identification in Speech and Singing",
   "original": "1763",
   "order": 96,
   "page_count": 5,
   "abstract": [
    "Little research has been conducted to gauge a listener’s ability to recognise or identify speakers when presented with samples of singing within the field of Forensic Speech Science. Eight friends and two foil speakers were recorded speaking and singing to investigate the effects of speaker familiarity and singing in speaker identification tasks. The stimuli were used to create a listening test completed by close social network speakers, members of the wider social network, and general lay listeners. The study aimed to explore the impact of familiarity on an individual’s ability to recognise speakers when presented with spoken and sung stimuli. The results revealed that the listeners within the close social network were the most successful in the listening test. Overall, listeners performed best when both samples were spoken, however, those in the close social network were less affected by the use of sung samples, and scored higher, compared to those outside the close social network. "
   ],
   "p1": 472,
   "pn": 476,
   "doi": "10.21437/Interspeech.2024-1763",
   "url": "interspeech_2024/taylor24_interspeech.html"
  },
  "zhang24m_interspeech": {
   "authors": [
    [
     "Zihan",
     "Zhang"
    ],
    [
     "Xianjun",
     "Xia"
    ],
    [
     "Chuanzeng",
     "Huang"
    ],
    [
     "Yijian",
     "Xiao"
    ],
    [
     "Lei",
     "Xie"
    ]
   ],
   "title": "BS-PLCNet 2: Two-stage Band-split Packet Loss Concealment Network with Intra-model Knowledge Distillation",
   "original": "1764",
   "order": 359,
   "page_count": 5,
   "abstract": [
    "Audio packet loss is an inevitable problem in real-time speech communication. A band-split packet loss concealment network (BS-PLCNet) targeting full-band signals was recently proposed. Although it performs superiorly in the ICASSP 2024 PLC Challenge, BS-PLCNet is a large model with high computational complexity of 8.95G FLOPS. This paper presents its updated version, BS-PLCNet 2, to reduce computational complexity and improve performance further. Specifically, to compensate for the missing future information, in the wide-band module, we design a dual-path encoder structure (with noncausal and causal path) and leverage an intra-model knowledge distillation strategy to distill the future information from the non-causal teacher to the casual student. Moreover, we introduce a lightweight post-processing module after packet loss restoration to recover speech distortions and remove residual noise in the audio signal. With only 40% of original parameters in BS-PLCNet, BS-PLCNet 2 brings 0.18 PLCMOS improvement on the ICASSP 2024 PLC challenge blind set, achieving state-of-the-art performance on this dataset."
   ],
   "p1": 1750,
   "pn": 1754,
   "doi": "10.21437/Interspeech.2024-1764",
   "url": "interspeech_2024/zhang24m_interspeech.html"
  },
  "shi24f_interspeech": {
   "authors": [
    [
     "Shuchen",
     "Shi"
    ],
    [
     "Ruibo",
     "Fu"
    ],
    [
     "Zhengqi",
     "Wen"
    ],
    [
     "Jianhua",
     "Tao"
    ],
    [
     "Tao",
     "Wang"
    ],
    [
     "Chunyu",
     "Qiang"
    ],
    [
     "Yi",
     "Lu"
    ],
    [
     "Xin",
     "Qi"
    ],
    [
     "Xuefei",
     "Liu"
    ],
    [
     "Yukun",
     "Liu"
    ],
    [
     "Yongwei",
     "Li"
    ],
    [
     "Zhiyong",
     "Wang"
    ],
    [
     "Xiaopeng",
     "Wang"
    ]
   ],
   "title": "PPPR: Portable Plug-in Prompt Refiner for Text to Audio Generation",
   "original": "1771",
   "order": 1004,
   "page_count": 5,
   "abstract": [
    "Text-to-Audio (TTA) aims to generate audio that corresponds to the given text description, playing a crucial role in media production. The text descriptions in TTA datasets lack rich variations and diversity, resulting in a drop in TTA model performance when faced with complex text. To address this issue, we propose a method called Portable Plug-in Prompt Refiner, which utilizes rich knowledge about textual descriptions inherent in large language models to effectively enhance the robustness of TTA acoustic models without altering the acoustic training set. Furthermore, a Chain-of-Thought that mimics human verification is introduced to enhance the accuracy of audio descriptions, thereby improving the accuracy of generated content in practical applications. The experiments show that our method achieves a state-of-the-art Inception Score (IS) of 8.72, surpassing AudioGen, AudioLDM and Tango."
   ],
   "p1": 4898,
   "pn": 4902,
   "doi": "10.21437/Interspeech.2024-1771",
   "url": "interspeech_2024/shi24f_interspeech.html"
  },
  "wu24l_interspeech": {
   "authors": [
    [
     "Minglin",
     "Wu"
    ],
    [
     "Jing",
     "Xu"
    ],
    [
     "Xixin",
     "Wu"
    ],
    [
     "Helen",
     "Meng"
    ]
   ],
   "title": "Prompting Large Language Models with Mispronunciation Detection and Diagnosis Abilities",
   "original": "1772",
   "order": 614,
   "page_count": 5,
   "abstract": [
    "Large Language Models (LLMs) have demonstrated significant achievements across diverse modalities. In this paper, we propose ATP-LLM, a framework that utilizes Audio and Text to Prompt LLMs to perform mispronunciation detection and diagnosis (MDD) tasks in second language (L2) English. ATP-LLM consists of an audio encoder and an LLM decoder. The audio encoder converts L2 English speech into speech representations digestible for LLMs. These speech representations, along with the corresponding canonical pronunciation, serve as audio and text prompts that enable the LLM decoder to generate the phones articulated by L2 English learners. Experiments show that our proposed ATP-LLM achieves a new state-of-the-art (SOTA) performance on the CU-CHLOE corpus with a Phone Error Rate (PER) of 8.56% and an F1 of 82.02%, outperforming the existing wav2vec2-CTC method whose PER and F1 are 8.98% and 80.93%, respectively."
   ],
   "p1": 2990,
   "pn": 2994,
   "doi": "10.21437/Interspeech.2024-1772",
   "url": "interspeech_2024/wu24l_interspeech.html"
  },
  "hojo24_interspeech": {
   "authors": [
    [
     "Keigo",
     "Hojo"
    ],
    [
     "Yukoh",
     "Wakabayashi"
    ],
    [
     "Kengo",
     "Ohta"
    ],
    [
     "Atsunori",
     "Ogawa"
    ],
    [
     "Norihide",
     "Kitaoka"
    ]
   ],
   "title": "Boosting CTC-based ASR using inter-layer attention-based CTC loss",
   "original": "1776",
   "order": 588,
   "page_count": 5,
   "abstract": [
    "This paper addresses improving the performance of CTC-based models, which leverage the intermediate outputs of all encoder layers with an attention mechanism. Several previous studies have used the intermediate outputs of the encoder layer to modify CTC-based models. Here, we focus on the role of the Transformer encoder layer, and each encoder layer is computed for two CTC losses by weighting the intermediate outputs of its lower and upper layers using an attention mechanism. By dividing the layer into two groups, it is expected to be possible to calculate the loss, taking into account both acoustic and linguistic features. Experimental results showed that the proposed method improved the baseline recognition performance of TEDLIUM2 speech data, achieving a WER of 9.9% on the dev set and 11.8% on the test set. Our method outperformed the conventional methods for WER with only slightly increased inference speed measured by RTF."
   ],
   "p1": 2860,
   "pn": 2864,
   "doi": "10.21437/Interspeech.2024-1776",
   "url": "interspeech_2024/hojo24_interspeech.html"
  },
  "wells24_interspeech": {
   "authors": [
    [
     "Dan",
     "Wells"
    ],
    [
     "Andrea Lorena",
     "Aldana Blanco"
    ],
    [
     "Cassia",
     "Valentini"
    ],
    [
     "Erica",
     "Cooper"
    ],
    [
     "Aidan",
     "Pine"
    ],
    [
     "Junichi",
     "Yamagishi"
    ],
    [
     "Korin",
     "Richmond"
    ]
   ],
   "title": "Experimental evaluation of MOS, AB and BWS listening test designs",
   "original": "1778",
   "order": 555,
   "page_count": 5,
   "abstract": [
    "Mean Opinion Score (MOS) tests are the most widely used test type for subjective evaluation of speech samples. However, their use has been questioned, as results can vary significantly depending on the test material included. Forced-choice tests such as AB or Best Worst Scaling (BWS) can in principle mitigate some of these issues. Our aim here is to compare MOS, AB and BWS tests in 3 regards: 1) Which test type do listeners prefer in terms of ease, engagement and overall likeability? 2) How fast are listeners at each test type? 3) Does each test type provide the same pattern of results? To answer these questions we re-use a subset of stimuli from the Blizzard Challenge 2013 and conduct new MOS, AB and BWS tests. Overall, we conclude each test type is broadly equally valid, MOS may not in fact be the fastest or easiest test type for listeners, but the theoretical advantages of BWS are counterbalanced by it seeming less liked by our listeners here."
   ],
   "p1": 2695,
   "pn": 2699,
   "doi": "10.21437/Interspeech.2024-1778",
   "url": "interspeech_2024/wells24_interspeech.html"
  },
  "zheng24b_interspeech": {
   "authors": [
    [
     "Beida",
     "Zheng"
    ],
    [
     "Mijit",
     "Ablimit"
    ],
    [
     "Hankiz",
     "Yilahun"
    ],
    [
     "Askar",
     "Hamdulla"
    ]
   ],
   "title": "Convolutional gated MLP and attention improve end-to-end spoken language understanding",
   "original": "1780",
   "order": 720,
   "page_count": 5,
   "abstract": [
    "The task of spoken language understanding(SLU) consists mainly of intention detection and slot filling. Most studies model the two separately, with intention detection as a categorization task and slot filling as a sequence labeling task, which are interrelated. End-to-end(E2E) approaches have recently received much attention due to their structural simplicity and effectiveness. Still, the need for speech datasets is one of the main reasons for their limited development. Second, E2E SLU is plagued by long-tail words. To mitigate the above problems, the use of Hubert as a speech encoder combined with the Convolutional Gated MLP(CgMLP) module is proposed to improve the feature learning capability of the model. The intention detection and slot-filling tasks are jointly modeled E2E. At the same time, the long-tailed word problem is mitigated using the attention mechanism at the decoding, an improvement of 4.77\\% and 1.87\\% over the baseline model on the two tasks."
   ],
   "p1": 3520,
   "pn": 3524,
   "doi": "10.21437/Interspeech.2024-1780",
   "url": "interspeech_2024/zheng24b_interspeech.html"
  },
  "vinnikov24_interspeech": {
   "authors": [
    [
     "Alon",
     "Vinnikov"
    ],
    [
     "Amir",
     "Ivry"
    ],
    [
     "Aviv",
     "Hurvitz"
    ],
    [
     "Igor",
     "Abramovski"
    ],
    [
     "Sharon",
     "Koubi"
    ],
    [
     "Ilya",
     "Gurvich"
    ],
    [
     "Shai",
     "Peer"
    ],
    [
     "Xiong",
     "Xiao"
    ],
    [
     "Benjamin Martinez",
     "Elizalde"
    ],
    [
     "Naoyuki",
     "Kanda"
    ],
    [
     "Xiaofei",
     "Wang"
    ],
    [
     "Shalev",
     "Shaer"
    ],
    [
     "Stav",
     "Yagev"
    ],
    [
     "Yossi",
     "Asher"
    ],
    [
     "Sunit",
     "Sivasankaran"
    ],
    [
     "Yifan",
     "Gong"
    ],
    [
     "Min",
     "Tang"
    ],
    [
     "Huaming",
     "Wang"
    ],
    [
     "Eyal",
     "Krupka"
    ]
   ],
   "title": "NOTSOFAR-1 Challenge: New Datasets, Baseline, and Tasks for Distant Meeting Transcription",
   "original": "1788",
   "order": 1025,
   "page_count": 5,
   "abstract": [
    "We introduce the first Natural Office Talkers in Settings of Far-field Audio Recordings (NOTSOFAR) Challenge, datasets, and baseline system. The challenge focuses on distant speaker diarization and automatic speech recognition (DASR) in meeting scenarios, with single-channel and known-geometry multi-channel tracks, using a single device. We launch two new datasets: First, a benchmark dataset of 280 English meetings, averaging 6 minutes each, capturing a broad spectrum of acoustic and conversational patterns across 30 rooms with 4-8 attendees. Second, a 1000-hour simulated training dataset, synthesized for real-world generalization, incorporating 15,000 real acoustic transfer functions. The NOTSOFAR-1 Challenge aims to advance research in the field of DASR, providing key resources to unlock the potential of data-driven methods, which we believe are currently constrained by the absence of comprehensive high-quality training and benchmark datasets."
   ],
   "p1": 5003,
   "pn": 5007,
   "doi": "10.21437/Interspeech.2024-1788",
   "url": "interspeech_2024/vinnikov24_interspeech.html"
  },
  "li24ia_interspeech": {
   "authors": [
    [
     "Jinpeng",
     "Li"
    ],
    [
     "Yu",
     "Pu"
    ],
    [
     "Qi",
     "Sun"
    ],
    [
     "Wei-Qiang",
     "Zhang"
    ]
   ],
   "title": "Improving Whisper's Recognition Performance for Under-Represented Language Kazakh Leveraging Unpaired Speech and Text",
   "original": "1790",
   "order": 518,
   "page_count": 5,
   "abstract": [
    "Whisper and other large-scale automatic speech recognition models have made significant progress in performance. However, their performance on many low-resource languages, such as Kazakh, is not satisfactory. It is worth researching how to utilize low-cost data to improve the performance of Whisper on under-represented languages. In this study, we utilized easily accessible unpaired speech and text data and combined the language model GPT with Whisper on Kazakh. We implemented end of transcript (EOT) judgment modification and hallucination penalty to improve the performance of speech recognition. Further, we employed the decoding average token log probability as a criterion to select samples from unlabeled speech data and used pseudo-labeled data to fine-tune the model to further improve its performance. Ultimately, we achieved more than 10% absolute WER reduction in multiple experiments, and the whole process has the potential to be generalized to other under-represented languages."
   ],
   "p1": 2514,
   "pn": 2518,
   "doi": "10.21437/Interspeech.2024-1790",
   "url": "interspeech_2024/li24ia_interspeech.html"
  },
  "cai24b_interspeech": {
   "authors": [
    [
     "Yunrui",
     "Cai"
    ],
    [
     "Zhiyong",
     "Wu"
    ],
    [
     "Jia",
     "Jia"
    ],
    [
     "Helen",
     "Meng"
    ]
   ],
   "title": "LoRA-MER: Low-Rank Adaptation of Pre-Trained Speech Models for Multimodal Emotion Recognition Using Mutual Information",
   "original": "1793",
   "order": 956,
   "page_count": 5,
   "abstract": [
    "Multimodal emotion recognition (MER) is crucial for machines to understand human intentions. Although many deep learning models have been proposed, MER still faces practical challenges. The key challenge is how to extract high-dimensional features that are more relevant to emotions. Another challenge is how to effectively model multimodal features, achieving a balance between similarity and diversity. In this paper, we propose the method of LoRA-MER using mutual information. We fine-tune a pre-trained speech model with Low-Rank Adaptation (LoRA) strategy and utilize a frozen pre-trained text model to robustly extract emotional features. Additionally, we adopt a multimodal fusion approach based on Mutual Information Neural Estimation (MINE) to enhance their correlation. Experimental results demonstrate the effectiveness of each module proposed in our method, and the performance of our model surpasses that of state-of-the-art speaker-independent approaches on IEMOCAP dataset."
   ],
   "p1": 4658,
   "pn": 4662,
   "doi": "10.21437/Interspeech.2024-1793",
   "url": "interspeech_2024/cai24b_interspeech.html"
  },
  "sungbin24_interspeech": {
   "authors": [
    [
     "Kim",
     "Sung-Bin"
    ],
    [
     "Lee",
     "Chae-Yeon"
    ],
    [
     "Gihun",
     "Son"
    ],
    [
     "Oh",
     "Hyun-Bin"
    ],
    [
     "Janghoon",
     "Ju"
    ],
    [
     "Suekyeong",
     "Nam"
    ],
    [
     "Tae-Hyun",
     "Oh"
    ]
   ],
   "title": "MultiTalk: Enhancing 3D Talking Head Generation Across Languages with Multilingual Video Dataset",
   "original": "1794",
   "order": 285,
   "page_count": 5,
   "abstract": [
    "Recent studies in speech-driven 3D talking head generation have achieved convincing results in verbal articulations. However, generating accurate lip-syncs degrades when applied to input speech in other languages, possibly due to the lack of datasets covering a broad spectrum of facial movements across languages. In this work, we introduce a novel task to generate 3D talking heads from speeches of diverse languages. We collect a new multilingual 2D video dataset comprising over 420 hours of talking videos in 20 languages. With our proposed dataset, we present a multilingual enhanced model that incorporates language-specific style embeddings, enabling it to capture the unique mouth movements associated with each language. Additionally, we present a metric for assessing lip-sync accuracy in multilingual settings. We demonstrate that training a 3D talking head model with our proposed dataset significantly enhances its multilingual performance. Codes and datasets are available at https://multitalk.github.io/."
   ],
   "p1": 1380,
   "pn": 1384,
   "doi": "10.21437/Interspeech.2024-1794",
   "url": "interspeech_2024/sungbin24_interspeech.html"
  },
  "wang24da_interspeech": {
   "authors": [
    [
     "Wenjun",
     "Wang"
    ],
    [
     "Shangbin",
     "Mo"
    ],
    [
     "Ling",
     "Dong"
    ],
    [
     "Zhengtao",
     "Yu"
    ],
    [
     "Junjun",
     "Guo"
    ],
    [
     "Yuxin",
     "Huang"
    ]
   ],
   "title": "DGSRN: Noise-Robust Speech Recognition Method with Dual-Path Gated Spectral Refinement Network",
   "original": "1796",
   "order": 1028,
   "page_count": 5,
   "abstract": [
    "The advancements in speech recognition have led to significant progress in predicting clean speech. However, challenges persist in real-world noisy environments. Addressing issues such as speech distortion and noise residue in signals processed by speech enhancement models, we propose a noise-robust speech recognition method based on the Dual-Path Gated Spectral Refinement Network (DGSRN). We construct a single-channel speech enhancement model based on dense time-frequency convolutional networks for the first stage of noise suppression. And the Dual-Path Gated Spectral Refinement Network is designed to extract useful features from estimated noise to enhance speech quality. Multi-task joint training is conducted using a weighted speech distortion loss function. Experimental results demonstrate that compared to traditional joint training approaches, DGSRN achieves a 12.41% reduction in Character Error Rate, addressing the issue of mismatched performance on evaluation metrics."
   ],
   "p1": 5018,
   "pn": 5022,
   "doi": "10.21437/Interspeech.2024-1796",
   "url": "interspeech_2024/wang24da_interspeech.html"
  },
  "kim24q_interspeech": {
   "authors": [
    [
     "Gahye",
     "Kim"
    ],
    [
     "Yunjung",
     "Eom"
    ],
    [
     "Selina S.",
     "Sung"
    ],
    [
     "Seunghee",
     "Ha"
    ],
    [
     "Tae-Jin",
     "Yoon"
    ],
    [
     "Jungmin",
     "So"
    ]
   ],
   "title": "Automatic Children Speech Sound Disorder Detection with Age and Speaker Bias Mitigation",
   "original": "1799",
   "order": 293,
   "page_count": 5,
   "abstract": [
    "Addressing speech sound disorders (SSD) in early childhood is pivotal for mitigating cognitive and communicative impediments. Previous works on automatic SSD detection rely on audio features without considering the age and speaker bias which results in degraded performance. In this paper, we propose an SSD detection system in which debiasing techniques are applied to mitigate the biases. For the age bias, we use a multi-head model where the feature extractor is shared across different age groups but the final decision is made using the age-dependent classifier. For the speaker bias, we augment the dataset by mixing the audios of the multiple speakers in the same age group. When evaluated with our Korean SSD dataset, the proposed method showed significant improvements over previous approaches."
   ],
   "p1": 1420,
   "pn": 1424,
   "doi": "10.21437/Interspeech.2024-1799",
   "url": "interspeech_2024/kim24q_interspeech.html"
  },
  "chien24c_interspeech": {
   "authors": [
    [
     "Jen-Tzung",
     "Chien"
    ],
    [
     "I-Ping",
     "Yeh"
    ],
    [
     "Man-Wai",
     "Mak"
    ]
   ],
   "title": "Collaborative Contrastive Learning for Hypothesis Domain Adaptation",
   "original": "1800",
   "order": 661,
   "page_count": 5,
   "abstract": [
    "Achieving desirable performance for speaker recognition with severe domain mismatch is challenging. Such a challenge becomes even more harsh when the source data are missing. To enhance the low-resource speaker representation, this study deals with a practical scenario, called hypothesis domain adaptation, where a model trained on a source domain is adapted to a significantly different target domain as a hypothesis without access to source data. To pursue a domain-invariant representation, this paper proposes a novel collaborative hypothesis domain adaptation (CHDA) where the dual encoders are collaboratively trained to estimate the pseudo source data which are then utilized to maximize the domain confusion. Combined with the constrastive learning, this CHDA is further enhanced by increasing the domain matching as well as the speaker discrimination. The experiments on cross-language speaker recognition show the merit of the proposed method."
   ],
   "p1": 3225,
   "pn": 3229,
   "doi": "10.21437/Interspeech.2024-1800",
   "url": "interspeech_2024/chien24c_interspeech.html"
  },
  "li24ja_interspeech": {
   "authors": [
    [
     "Xuefei",
     "Li"
    ],
    [
     "Hao",
     "Huang"
    ],
    [
     "Ying",
     "Hu"
    ],
    [
     "Liang",
     "He"
    ],
    [
     "Jiabao",
     "Zhang"
    ],
    [
     "Yuyi",
     "Wang"
    ]
   ],
   "title": "YOLOPitch: A Time-Frequency Dual-Branch YOLO Model for Pitch Estimation",
   "original": "1805",
   "order": 16,
   "page_count": 5,
   "abstract": [
    "Pitch estimation is of fundamental importance in audio processing and music information retrieval. YOLO is a well developed model designed for image target detection. Here we introduce YOLOv7 into pitch estimation task and improve by proposing time-frequency (TF) dual-branch into the model according to pitch perception of human auditory. An additional advantage of the model over the state-of-the-art (SOTA) models is that it only needs to add an unvoiced class without additional unvoiced/voiced detection to achieve joint pitch estimation and voiced determination. Experiments show for both music and speech, the proposed TF dual-branch can boost pitch estimation accuracy over the back-bone. Our model exhibits superior pitch estimation performance over the SOTA and shows minimal performance degradation in noisy condition. The overall accuracy on the MDB-stem-synth dataset peaks at 99.4%, and voicing determination F-score reaches 99.9%."
   ],
   "p1": 72,
   "pn": 76,
   "doi": "10.21437/Interspeech.2024-1805",
   "url": "interspeech_2024/li24ja_interspeech.html"
  },
  "wu24m_interspeech": {
   "authors": [
    [
     "Qi",
     "Wu"
    ]
   ],
   "title": "Mandarin T3 Production by Chinese and Japanese Native Speakers",
   "original": "1806",
   "order": 216,
   "page_count": 5,
   "abstract": [
    "This study examines the production of Mandarin T3 by native Japanese and Chinese speakers, focusing on the F0 and the acoustic correlate of voice quality (H1*-H2*). Previous research showed the challenges that native Japanese speakers face in producing Mandarin tones, particularly the frequent mispronunciation of T3 as T2. This study aims to bridge the gap in research concerning the acquisition of Mandarin tones by native Japanese speakers, with a particular emphasis on the influence of adjacent tones on T3’s acoustic features. The results reveal significant differences in T3 production between the two groups, the phonation style, and the influence of adjacent tones mainly contributing to the challenges Japanese learners face. This study underscores the need for teaching strategies that address the phonetic features of Mandarin tones, including the introduction of creaky voice characteristics to improve T3 pronunciation for Japanese learners."
   ],
   "p1": 1035,
   "pn": 1039,
   "doi": "10.21437/Interspeech.2024-1806",
   "url": "interspeech_2024/wu24m_interspeech.html"
  },
  "luz24_interspeech": {
   "authors": [
    [
     "Saturnino",
     "Luz"
    ],
    [
     "Sofia",
     "De La Fuente Garcia"
    ],
    [
     "Fasih",
     "Haider"
    ],
    [
     "Davida",
     "Fromm"
    ],
    [
     "Brian",
     "MacWhinney"
    ],
    [
     "Alyssa",
     "Lanzi"
    ],
    [
     "Ya-Ning",
     "Chang"
    ],
    [
     "Chia-Ju",
     "Chou"
    ],
    [
     "Yi-Chien",
     "Liu"
    ]
   ],
   "title": "Connected Speech-Based Cognitive Assessment in Chinese and English",
   "original": "1807",
   "order": 191,
   "page_count": 5,
   "abstract": [
    "We present a novel benchmark dataset and prediction tasks for investigating approaches to assess cognitive function through analysis of connected speech. The dataset consists of speech samples and clinical information for speakers of Mandarin Chinese and English with different levels of cognitive impairment as well as individuals with normal cognition. These data have been carefully matched by age and sex by propensity score analysis to ensure balance and representativity in model training. The prediction tasks encompass mild cognitive impairment diagnosis and cognitive test score prediction. This framework was designed to encourage the development of approaches to speech-based cognitive assessment which generalise across languages. We illustrate it by presenting baseline prediction models that employ language-agnostic and comparable features for diagnosis and cognitive test score prediction. Unweighted average recall was 59.2% in diagnosis, and root mean squared error was 2.89 in score prediction."
   ],
   "p1": 947,
   "pn": 951,
   "doi": "10.21437/Interspeech.2024-1807",
   "url": "interspeech_2024/luz24_interspeech.html"
  },
  "lee24j_interspeech": {
   "authors": [
    [
     "Jae-Hong",
     "Lee"
    ],
    [
     "Sang-Eon",
     "Lee"
    ],
    [
     "Dong-Hyun",
     "Kim"
    ],
    [
     "DoHee",
     "Kim"
    ],
    [
     "Joon-Hyuk",
     "Chang"
    ]
   ],
   "title": "Online Subloop Search via Uncertainty Quantization for Efficient Test-Time Adaptation",
   "original": "1813",
   "order": 592,
   "page_count": 5,
   "abstract": [
    "Online test-time adaptation (OTTA) methods have demonstrated their effectiveness in real-time adapting to the target domain for speech recognition tasks. However, a common thread among these existing methods is their reliance on repetitive learning for each test utterance through a subloop, imposing prohibitive computational costs. This paper highlights the inefficiency inherent in applying a uniform number of subloop iterations to every test sample. To address this issue, we propose the online subloop search (OSS) method, which implicitly adjusts the number of iterations based on the test sample and domain characteristics. The proposed method operates within a framework comprising a chaser model updated via stochastic gradient descent and a leader model updated through the exponential moving average. The OSS method quantifies and quantizes the uncertainty in the chaser model relative to the leader model, using the quantized value to predict the number of iterations for the subloop."
   ],
   "p1": 2880,
   "pn": 2884,
   "doi": "10.21437/Interspeech.2024-1813",
   "url": "interspeech_2024/lee24j_interspeech.html"
  },
  "wang24ea_interspeech": {
   "authors": [
    [
     "Haoyu",
     "Wang"
    ],
    [
     "Guoqiang",
     "Hu"
    ],
    [
     "Guodong",
     "Lin"
    ],
    [
     "Wei-Qiang",
     "Zhang"
    ],
    [
     "Jian",
     "Li"
    ]
   ],
   "title": "Simul-Whisper: Attention-Guided Streaming Whisper with Truncation Detection",
   "original": "1814",
   "order": 921,
   "page_count": 5,
   "abstract": [
    "As a robust and large-scale multilingual speech recognition model, Whisper has demonstrated impressive results in many low-resource and out-of-distribution scenarios. However, its encoder-decoder structure hinders its application to streaming speech recognition. In this paper, we introduce Simul-Whisper, which uses the time alignment embedded in Whisper's cross-attention to guide auto-regressive decoding and achieve chunk-based streaming ASR without any fine-tuning of the pre-trained model. Furthermore, we observe the negative effect of the truncated words at the chunk boundaries on the decoding results and propose an integrate-and-fire-based truncation detection model to address this issue.  Experiments on multiple languages and Whisper architectures show that Simul-Whisper achieves an average absolute word error rate degradation of only 1.46% at a chunk size of 1 second, which significantly outperforms the current state-of-the-art baseline."
   ],
   "p1": 4483,
   "pn": 4487,
   "doi": "10.21437/Interspeech.2024-1814",
   "url": "interspeech_2024/wang24ea_interspeech.html"
  },
  "goel24_interspeech": {
   "authors": [
    [
     "Arnav",
     "Goel"
    ],
    [
     "Medha",
     "Hira"
    ],
    [
     "Anubha",
     "Gupta"
    ]
   ],
   "title": "Exploring Multilingual Unseen Speaker Emotion Recognition: Leveraging Co-Attention Cues in Multitask Learning",
   "original": "1820",
   "order": 483,
   "page_count": 5,
   "abstract": [
    "Advent of modern deep learning techniques has given rise to advancements in the field of Speech Emotion Recognition (SER). However, most systems prevalent in the field fail to generalize to speakers not seen during training. This study focuses on handling challenges of multilingual SER, specifically on unseen speakers. We introduce CAMuLeNet, a novel architecture leveraging co-attention based fusion and multitask learning to address this problem. Additionally, we benchmark pretrained encoders of Whisper, HuBERT, Wav2Vec2.0, and WavLM using 10-fold leave-speaker-out cross-validation on five existing multilingual benchmark datasets: IEMOCAP, RAVDESS, CREMA-D, EmoDB and CaFE and, release a novel dataset for SER on the Hindi language (BhavVani). CAMuLeNet shows an average improvement of approximately 8% over all benchmarks on unseen speakers determined by our cross-validation strategy."
   ],
   "p1": 2340,
   "pn": 2344,
   "doi": "10.21437/Interspeech.2024-1820",
   "url": "interspeech_2024/goel24_interspeech.html"
  },
  "kim24r_interspeech": {
   "authors": [
    [
     "Ilseok",
     "Kim"
    ],
    [
     "Ju-Seok",
     "Seong"
    ],
    [
     "Joon-Hyuk",
     "Chang"
    ]
   ],
   "title": "Few-Shot Keyword-Incremental Learning with Total Calibration",
   "original": "1823",
   "order": 1041,
   "page_count": 5,
   "abstract": [
    "Keyword spotting (KWS) models need to continuously recognize new keywords for user demand. However, two significant challenges exist in satisfying this requirement: catastrophic forgetting, where the model loses its ability to classify previously learned keywords, and insufficient data for new classes. To address these challenges, we propose a Few-shot keyword-Incremental Learning with total caLibration (FILL), a novel few-shot class-incremental learning (FSCIL) approach for KWS. FSCIL trains a model with sufficient data in an initial session, followed by incremental sessions where it learns new classes with limited data. FILL employs prototype calibration throughout total sessions to enhance class separation and mitigate misclassification. Notably, it utilizes manifold mixup in the initial session to generate new classes for prototype calibration. Experimental results on two KWS datasets demonstrate that FILL outperforms three baselines in terms of average accuracy."
   ],
   "p1": 5083,
   "pn": 5087,
   "doi": "10.21437/Interspeech.2024-1823",
   "url": "interspeech_2024/kim24r_interspeech.html"
  },
  "meghanani24_interspeech": {
   "authors": [
    [
     "Amit",
     "Meghanani"
    ],
    [
     "Thomas",
     "Hain"
    ]
   ],
   "title": "LASER: Learning by Aligning Self-supervised Representations of Speech for Improving Content-related Tasks",
   "original": "1824",
   "order": 583,
   "page_count": 5,
   "abstract": [
    "Self-supervised learning (SSL)-based speech models are extensively used for full-stack speech processing. However, it has been observed that improving SSL-based speech representations using unlabeled speech for content-related tasks is challenging and computationally expensive. Recent attempts have been made to address this issue with cost-effective self-supervised fine-tuning (SSFT) approaches. Continuing in this direction, a cost-effective SSFT method named “LASER: Learning by Aligning Self-supervised Representations” is presented. LASER is based on the soft-DTW alignment loss with temporal regularisation term. Experiments are conducted with HuBERT and WavLM models and evaluated on the SUPERB benchmark for two content-related tasks: automatic speech recognition (ASR) and phoneme recognition (PR). A relative improvement of 3.7% and 8.2% for HuBERT, and 4.1% and 11.7% for WavLM are observed, for the ASR and PR tasks respectively, with only < 3 hours of fine-tuning on a single GPU."
   ],
   "p1": 2835,
   "pn": 2839,
   "doi": "10.21437/Interspeech.2024-1824",
   "url": "interspeech_2024/meghanani24_interspeech.html"
  },
  "latif24_interspeech": {
   "authors": [
    [
     "Siddique",
     "Latif"
    ],
    [
     "Raja",
     "Jurdak"
    ],
    [
     "Björn W.",
     "Schuller"
    ]
   ],
   "title": "Evaluating Transformer-Enhanced Deep Reinforcement Learning for Speech Emotion Recognition",
   "original": "1827",
   "order": 329,
   "page_count": 5,
   "abstract": [
    "Emotion modelling in speech using deep reinforcement learning (RL) has gained attention within the speech-emotion recognition (SER) community. However, prior studies have primarily centred around recurrent neural networks (RNNs) to capture emotional contexts, with limited exploration of the potential offered by more recent transformer architectures. This paper explores a comprehensive evaluation of training a transformer-based model using deep RL and benchmark its efficacy in SER. Specifically, we explore the effectiveness of a pre trained Wav2vec2 (w2v2) model-based classifier within the deep RL setting. We evaluate the proposed deep RL framework using five publicly available datasets and benchmark the results with three recent SER studies using two deep RL methods. Based on the results, we show that the transformer-based RL agent not only demonstrates an improvement in SER accuracy but also shows a reduction in the time taken to begin emotion classification, outpacing the RNNs that have been commonly used to date. Moreover, by leveraging pre-trained transformers, we observe a reduced need for extensive pre-training which has been a norm in prior research."
   ],
   "p1": 1600,
   "pn": 1604,
   "doi": "10.21437/Interspeech.2024-1827",
   "url": "interspeech_2024/latif24_interspeech.html"
  },
  "yoon24c_interspeech": {
   "authors": [
    [
     "Eunseop",
     "Yoon"
    ],
    [
     "Hee Suk",
     "Yoon"
    ],
    [
     "John",
     "Harvill"
    ],
    [
     "Mark",
     "Hasegawa-Johnson"
    ],
    [
     "Chang D.",
     "Yoo"
    ]
   ],
   "title": "LI-TTA: Language Informed Test-Time Adaptation for Automatic Speech Recognition",
   "original": "1829",
   "order": 714,
   "page_count": 5,
   "abstract": [
    "Test-Time Adaptation (TTA) has emerged as a crucial solution to the domain shift challenge, wherein the target environment diverges from the original training environment. A prime exemplification is TTA for Automatic Speech Recognition (ASR), which enhances model performance by leveraging output prediction entropy minimization as a self-supervision signal. However, a key limitation of this self-supervision lies in its primary focus on acoustic features, with minimal attention to the linguistic properties of the input. To address this gap, we propose Language Informed Test-Time Adaptation (LI-TTA), which incorporates linguistic insights during TTA for ASR. LI-TTA integrates corrections from an external language model to merge linguistic with acoustic information by minimizing the CTC loss from the correction alongside the standard TTA loss. With extensive experiments, we show that LI-TTA effectively improves the performance of TTA for ASR in various distribution shift situations."
   ],
   "p1": 3490,
   "pn": 3494,
   "doi": "10.21437/Interspeech.2024-1829",
   "url": "interspeech_2024/yoon24c_interspeech.html"
  },
  "paraskevopoulos24_interspeech": {
   "authors": [
    [
     "Georgios",
     "Paraskevopoulos"
    ],
    [
     "Chara",
     "Tsoukala"
    ],
    [
     "Athanasios",
     "Katsamanis"
    ],
    [
     "Vassilis",
     "Katsouros"
    ]
   ],
   "title": "The Greek podcast corpus: Competitive speech models for low-resourced languages with weakly supervised data",
   "original": "1830",
   "order": 817,
   "page_count": 5,
   "abstract": [
    "The development of speech technologies for languages with limited digital representation poses significant challenges, pri- marily due to the scarcity of available data. This issue is exacerbated in the era of large, data-intensive models. Recent research has underscored the potential of leveraging weak supervision to augment the pool of available data. In this study, we compile an 800-hour corpus of Modern Greek from podcasts and employ Whisper large-v3 to generate silver transcriptions. This corpus is utilized to fine-tune our models, aiming to assess the efficacy of this approach in enhancing ASR performance. Our analysis spans 16 distinct podcast domains, alongside evaluations on established datasets for Modern Greek. The findings indicate consistent WER improvements, correlating with increases in both data volume and model size. Our study confirms that assembling large, weakly supervised corpora serves as a cost-effective strategy for advancing speech technologies in under-resourced languages."
   ],
   "p1": 3969,
   "pn": 3973,
   "doi": "10.21437/Interspeech.2024-1830",
   "url": "interspeech_2024/paraskevopoulos24_interspeech.html"
  },
  "zhang24n_interspeech": {
   "authors": [
    [
     "Qiquan",
     "Zhang"
    ],
    [
     "Hongxu",
     "Zhu"
    ],
    [
     "Xinyuan",
     "Qian"
    ],
    [
     "Eliathamby",
     "Ambikairajah"
    ],
    [
     "Haizhou",
     "Li"
    ]
   ],
   "title": "An Exploration of Length Generalization in Transformer-Based Speech Enhancement",
   "original": "1831",
   "order": 354,
   "page_count": 5,
   "abstract": [
    "The use of Transformer architectures has facilitated remarkable progress in speech enhancement. Training Transformers using substantially long speech utterances is often infeasible as self-attention suffers from quadratic complexity. It is a critical and unexplored challenge for a Transformer-based speech enhancement model to learn from short speech utterances and generalize to longer ones. In this paper, we conduct comprehensive experiments to explore the length generalization problem in speech enhancement with Transformer. Our findings first establish that position embedding provides an effective instrument to alleviate the impact of utterance length on Transformer-based speech enhancement. Specifically, we explore four different position embedding schemes to enable length generalization. The results confirm the superiority of relative position embeddings (RPEs) over absolute PE (APEs) in length generalization."
   ],
   "p1": 1725,
   "pn": 1729,
   "doi": "10.21437/Interspeech.2024-1831",
   "url": "interspeech_2024/zhang24n_interspeech.html"
  },
  "kalluri24_interspeech": {
   "authors": [
    [
     "Shareef Babu",
     "Kalluri"
    ],
    [
     "Prachi",
     "Singh"
    ],
    [
     "Pratik",
     "Roy Chowdhuri"
    ],
    [
     "Apoorva",
     "Kulkarni"
    ],
    [
     "Shikha",
     "Baghel"
    ],
    [
     "Pradyoth",
     "Hegde"
    ],
    [
     "Swapnil",
     "Sontakke"
    ],
    [
     "Deepak",
     "K T"
    ],
    [
     "S.R. Mahadeva",
     "Prasanna"
    ],
    [
     "Deepu",
     "Vijayasenan"
    ],
    [
     "Sriram",
     "Ganapathy"
    ]
   ],
   "title": "The Second DISPLACE Challenge: DIarization of SPeaker and LAnguage in Conversational Environments",
   "original": "1833",
   "resource": "https://doi.org/10.48550/arXiv.2406.09494",
   "order": 335,
   "page_count": 5,
   "abstract": [
    "The DIarization of SPeaker and LAnguage in Conversational Environments (DISPLACE) 2024 challenge is the second in the series of DISPLACE challenges, which involves tasks of speaker diarization (SD) and language diarization (LD) on a challenging multilingual conversational speech dataset.  In the DISPLACE 2024 challenge, we also introduced the task of automatic speech recognition (ASR) on this dataset. The dataset containing 158 hours of speech, consisting of both supervised and unsupervised mono-channel far-field recordings, was released for LD and SD tracks. Further, 12 hours of close-field mono-channel recordings were provided for the ASR track conducted on 5 Indian languages. The details of the dataset, baseline systems and the leader board results are highlighted in this paper.  We have also compared our baseline models and the team's performances on evaluation data of DISPLACE-2023 to emphasize the advancements made in this second version of the challenge. "
   ],
   "p1": 1630,
   "pn": 1634,
   "doi": "10.21437/Interspeech.2024-1833",
   "url": "interspeech_2024/kalluri24_interspeech.html"
  },
  "freixes24_interspeech": {
   "authors": [
    [
     "Marc",
     "Freixes"
    ],
    [
     "Marc",
     "Arnela"
    ],
    [
     "Joan Claudi",
     "Socoró"
    ],
    [
     "Luis",
     "Joglar-Ongay"
    ],
    [
     "Oriol",
     "Guasch"
    ],
    [
     "Francesc",
     "Alías-Pujol"
    ]
   ],
   "title": "Glottal inverse filtering and vocal tract tuning for the numerical simulation of vowel /a/ with different levels of vocal effort",
   "original": "1835",
   "order": 639,
   "page_count": 5,
   "abstract": [
    "Voice production models provide valuable information about the human voice generation. However, providing them with expressiveness remains a challenge. This work proposes a methodology to modify vocal effort (VE) in the numerical simulation of vowels using a glottal source Liljencrants-Fant (LF) model and a one-dimensional acoustic model based on the finite element method. Vowels recorded with high, mid, and low VE are inverse-filtered to obtain a glottal source signal, used to estimate the LF model Rd parameter. A tuning algorithm adjusts the vocal tract geometry to match the formants of the analysed vowel. Preliminary results for the vowel /a/ are presented. Objective analyses indicate the relevance of both glottal source and vocal tract changes in reproducing VE. They are also perceptually relevant for low VE, while the glottal source predominates in high VE. Perceptual assessment validates the methodology can convey different levels of VE, particularly low and medium."
   ],
   "p1": 3115,
   "pn": 3119,
   "doi": "10.21437/Interspeech.2024-1835",
   "url": "interspeech_2024/freixes24_interspeech.html"
  },
  "gao24e_interspeech": {
   "authors": [
    [
     "Shuochen",
     "Gao"
    ],
    [
     "Shun",
     "Lei"
    ],
    [
     "Fan",
     "Zhuo"
    ],
    [
     "Hangyu",
     "Liu"
    ],
    [
     "Feng",
     "Liu"
    ],
    [
     "Boshi",
     "Tang"
    ],
    [
     "Qiaochu",
     "Huang"
    ],
    [
     "Shiyin",
     "Kang"
    ],
    [
     "Zhiyong",
     "Wu"
    ]
   ],
   "title": "An End-to-End Approach for Chord-Conditioned Song Generation",
   "original": "1837",
   "order": 387,
   "page_count": 5,
   "abstract": [
    "The Song Generation task aims to synthesize music composed of vocals and accompaniment from given lyrics. While the existing method, Jukebox, has explored this task, its constrained control over the generations often leads to deficiency in music performance. To mitigate the issue, we introduce an important concept from music composition, namely chords, to song generation networks. Chords form the foundation of accompaniment and provide vocal melody with associated harmony. Given the inaccuracy of automatic chord extractors, we devise a robust cross-attention mechanism augmented with dynamic weight sequence to integrate extracted chord information into song generations and reduce frame-level flaws, and propose a novel model termed Chord-Conditioned Song Generator (CSG) based on it. Experimental evidence demonstrates our proposed method outperforms other approaches in terms of musical performance and control precision of generated songs."
   ],
   "p1": 1890,
   "pn": 1894,
   "doi": "10.21437/Interspeech.2024-1837",
   "url": "interspeech_2024/gao24e_interspeech.html"
  },
  "wang24fa_interspeech": {
   "authors": [
    [
     "Shuai",
     "Wang"
    ],
    [
     "Ke",
     "Zhang"
    ],
    [
     "Shaoxiong",
     "Lin"
    ],
    [
     "Junjie",
     "Li"
    ],
    [
     "Xuefei",
     "Wang"
    ],
    [
     "Meng",
     "Ge"
    ],
    [
     "Jianwei",
     "Yu"
    ],
    [
     "Yanmin",
     "Qian"
    ],
    [
     "Haizhou",
     "Li"
    ]
   ],
   "title": "WeSep: A Scalable and Flexible Toolkit Towards Generalizable Target Speaker Extraction",
   "original": "1840",
   "order": 879,
   "page_count": 5,
   "abstract": [
    "Target speaker extraction (TSE) focuses on isolating the speech of a specific target speaker from overlapped multi-talker speech, which is a typical setup in the cocktail party problem. In recent years, TSE draws increasing attention due to its potential for various applications such as user-customized interfaces and hearing aids, or as a crutial front-end processing technologies for subsequential tasks such as speech recognition and speaker recongtion. However, there are currently few open-source toolkits or available pre-trained models for off-the-shelf usage. In this work, we introduce WeSep, a toolkit designed for research and practical applications in TSE. WeSep is featured with flexible target speaker modeling, scalable data management, effective on-the-fly data simulation, structured recipes and deployment support. The toolkit will be publicly avaliable at  https://github.com/wenet-e2e/WeSep."
   ],
   "p1": 4273,
   "pn": 4277,
   "doi": "10.21437/Interspeech.2024-1840",
   "url": "interspeech_2024/wang24fa_interspeech.html"
  },
  "yang24n_interspeech": {
   "authors": [
    [
     "Minmin",
     "Yang"
    ],
    [
     "Rachid",
     "Ridouane"
    ]
   ],
   "title": "Intrusive schwa within French stop-liquid clusters: An acoustic analysis",
   "original": "1841",
   "order": 765,
   "page_count": 5,
   "abstract": [
    "This study investigated the prevalence and acoustic characteristics of intrusive schwas within French consonant clusters. Word-initial and word-final stop-liquid clusters in the productions of ten native French speakers were analyzed. We examined intrusive schwas' frequency, duration, and spectral properties, considering variables such as word position, liquid type, stop voicing, and place of articulation. Our results showed a significant occurrence of intrusive schwas, particularly within voiced stop-rhotic clusters. Temporal features of schwas varied depending on their position within the word, while their spectral attributes were shaped mainly by liquid type and stop place of articulation. Notable differences were observed between schwas occurring within consonant clusters and those following word-final clusters. These findings suggest that phonetic factors underlie schwa occurrences within consonant clusters, while phonological factors drive word-final schwas."
   ],
   "p1": 3709,
   "pn": 3713,
   "doi": "10.21437/Interspeech.2024-1841",
   "url": "interspeech_2024/yang24n_interspeech.html"
  },
  "guan24b_interspeech": {
   "authors": [
    [
     "Wenhao",
     "Guan"
    ],
    [
     "Kaidi",
     "Wang"
    ],
    [
     "Wangjin",
     "Zhou"
    ],
    [
     "Yang",
     "Wang"
    ],
    [
     "Feng",
     "Deng"
    ],
    [
     "Hui",
     "Wang"
    ],
    [
     "Lin",
     "Li"
    ],
    [
     "Qingyang",
     "Hong"
    ],
    [
     "Yong",
     "Qin"
    ]
   ],
   "title": "LAFMA: A Latent Flow Matching Model for Text-to-Audio Generation",
   "original": "1848",
   "order": 987,
   "page_count": 5,
   "abstract": [
    "Recently, the application of diffusion models has facilitated the significant development of speech and audio generation. Nevertheless, the quality of samples generated by diffusion models still needs improvement. And the effectiveness of the method is accompanied by the extensive number of sampling steps, leading to an extended synthesis time necessary for generating high-quality audio. Previous Text-to-Audio (TTA) methods mostly used diffusion models in the latent space for audio generation. In this paper, we explore the integration of the Flow Matching (FM) model into the audio latent space for audio generation. The FM is an alternative simulation-free method that trains continuous normalization flows (CNF) based on regressing vector fields. We demonstrate that our model significantly enhances the quality of generated audio samples, achieving better performance than prior models. Moreover, it reduces the number of inference steps to ten steps almost without sacrificing performance."
   ],
   "p1": 4813,
   "pn": 4817,
   "doi": "10.21437/Interspeech.2024-1848",
   "url": "interspeech_2024/guan24b_interspeech.html"
  },
  "wang24ga_interspeech": {
   "authors": [
    [
     "Xiaopeng",
     "Wang"
    ],
    [
     "Ruibo",
     "Fu"
    ],
    [
     "Zhengqi",
     "Wen"
    ],
    [
     "Zhiyong",
     "Wang"
    ],
    [
     "Yuankun",
     "Xie"
    ],
    [
     "Yukun",
     "Liu"
    ],
    [
     "Jianhua",
     "Tao"
    ],
    [
     "Xuefei",
     "Liu"
    ],
    [
     "Yongwei",
     "Li"
    ],
    [
     "Xin",
     "Qi"
    ],
    [
     "Yi",
     "Lu"
    ],
    [
     "Shuchen",
     "Shi"
    ]
   ],
   "title": "Genuine-Focused Learning using Mask AutoEncoder for Generalized Fake Audio Detection",
   "original": "1851",
   "order": 994,
   "page_count": 5,
   "abstract": [
    "The generalization of Fake Audio Detection (FAD) is critical due to the emergence of new spoofing techniques. Traditional FAD methods often focus solely on distinguishing between genuine and known spoofed audio. We propose a Genuine-Focused Learning (GFL) framework guided, aiming for highly generalized FAD, called GFL-FAD. This method incorporates a Counterfactual Reasoning Enhanced Representation (CRER) based on audio reconstruction using the Mask AutoEncoder (MAE) architecture to accurately model genuine audio features. To reduce the influence of spoofed audio during training, we introduce a genuine audio reconstruction loss, maintaining the focus on learning genuine data features. In addition, content-related bottleneck (BN) features are extracted from the MAE to supplement the knowledge of the original audio. These BN features are adaptively fused with CRER to further improve robustness. Our method achieves state-of-the-art performance with an EER of 0.25% on ASVspoof2019 LA."
   ],
   "p1": 4848,
   "pn": 4852,
   "doi": "10.21437/Interspeech.2024-1851",
   "url": "interspeech_2024/wang24ga_interspeech.html"
  },
  "chen24t_interspeech": {
   "authors": [
    [
     "Xueyuan",
     "Chen"
    ],
    [
     "Dongchao",
     "Yang"
    ],
    [
     "Dingdong",
     "Wang"
    ],
    [
     "Xixin",
     "Wu"
    ],
    [
     "Zhiyong",
     "Wu"
    ],
    [
     "Helen",
     "Meng"
    ]
   ],
   "title": "CoLM-DSR: Leveraging Neural Codec Language Modeling for Multi-Modal Dysarthric Speech Reconstruction",
   "original": "1852",
   "order": 849,
   "page_count": 5,
   "abstract": [
    "Dysarthric speech reconstruction (DSR) aims to transform dysarthric speech into normal speech. It still suffers from low speaker similarity and poor prosody naturalness. In this paper, we propose a multi-modal DSR model by leveraging neural codec language modeling to improve the reconstruction results, especially for the speaker similarity and prosody naturalness. Our proposed model consists of: (i) a multi-modal content encoder to extract robust phoneme embeddings from dysarthric speech with auxiliary visual inputs; (ii) a speaker codec encoder to extract and normalize the speaker-aware codecs from the dysarthric speech, in order to provide original timbre and normal prosody; (iii) a codec language model based speech decoder to reconstruct the speech based on the extracted phoneme embeddings and normalized codecs. Evaluations on the commonly used UASpeech corpus show that our proposed model can achieve significant improvements in terms of speaker similarity and prosody naturalness."
   ],
   "p1": 4129,
   "pn": 4133,
   "doi": "10.21437/Interspeech.2024-1852",
   "url": "interspeech_2024/chen24t_interspeech.html"
  },
  "chen24u_interspeech": {
   "authors": [
    [
     "Peikun",
     "Chen"
    ],
    [
     "Sining",
     "Sun"
    ],
    [
     "Changhao",
     "Shan"
    ],
    [
     "Qing",
     "Yang"
    ],
    [
     "Lei",
     "Xie"
    ]
   ],
   "title": "Streaming Decoder-Only Automatic Speech Recognition with Discrete Speech Units: A Pilot Study",
   "original": "1853",
   "order": 918,
   "page_count": 5,
   "abstract": [
    "Unified speech-text models like SpeechGPT, VioLA, and AudioPaLM have shown impressive performance across various speech-related tasks, especially in Automatic Speech Recognition (ASR). These models typically adopt a unified method to model discrete speech and text tokens, followed by training a decoder-only transformer. However, they are all designed for non-streaming ASR tasks, where the entire speech utterance is needed during decoding. Hence, we introduce a decoder-only model exclusively designed for streaming recognition, incorporating a dedicated boundary token to facilitate streaming recognition and employing causal attention masking during the training phase. Furthermore, we introduce right-chunk attention and various data augmentation techniques to improve the model’s contextual modeling abilities. While achieving streaming speech recognition, experiments on the AISHELL-1 and-2 datasets demonstrate the competitive performance of our streaming approach with non-streaming decoder-only counter-parts. The code we used for this work can be found here."
   ],
   "p1": 4468,
   "pn": 4472,
   "doi": "10.21437/Interspeech.2024-1853",
   "url": "interspeech_2024/chen24u_interspeech.html"
  },
  "pelloin24_interspeech": {
   "authors": [
    [
     "Valentin",
     "Pelloin"
    ],
    [
     "Léna",
     "Dodson"
    ],
    [
     "Émile",
     "Chapuis"
    ],
    [
     "Nicolas",
     "Hervé"
    ],
    [
     "David",
     "Doukhan"
    ]
   ],
   "title": "Automatic Classification of News Subjects in Broadcast News: Application to a Gender Bias Representation Analysis",
   "original": "1854",
   "resource": "https://doi.org/10.5281/zenodo.12793337",
   "order": 627,
   "page_count": 5,
   "abstract": [
    "This paper introduces a computational framework designed to delineate gender distribution biases in topics covered by French TV and radio news. We transcribe a dataset of 11.7k hours, broadcasted in 2023 on 21 French channels. A Large Language Model (LLM) is used in few-shot conversation mode to obtain a topic classification on those transcriptions. Using the generated LLM annotations, we explore the finetuning of a specialized smaller classification model, to reduce the computational cost. To evaluate the performances of these models, we construct and annotate a dataset of 804 dialogues. This dataset is made available free of charge for research purposes. We show that women are notably underrepresented in subjects such as sports, politics and conflicts. Conversely, on topics such as weather, commercials and health, women have more speaking time than their overall average across all subjects. We also observe representations differences between private and public service channels."
   ],
   "p1": 3055,
   "pn": 3059,
   "doi": "10.21437/Interspeech.2024-1854",
   "url": "interspeech_2024/pelloin24_interspeech.html"
  },
  "zhou24e_interspeech": {
   "authors": [
    [
     "Xuanru",
     "Zhou"
    ],
    [
     "Anshul",
     "Kashyap"
    ],
    [
     "Steve",
     "Li"
    ],
    [
     "Ayati",
     "Sharma"
    ],
    [
     "Brittany",
     "Morin"
    ],
    [
     "David",
     "Baquirin"
    ],
    [
     "Jet",
     "Vonk"
    ],
    [
     "Zoe",
     "Ezzes"
    ],
    [
     "Zachary",
     "Miller"
    ],
    [
     "Maria",
     "Tempini"
    ],
    [
     "Jiachen",
     "Lian"
    ],
    [
     "Gopala",
     "Anumanchipalli"
    ]
   ],
   "title": "YOLO-Stutter: End-to-end Region-Wise Speech Dysfluency Detection",
   "original": "1855",
   "resource": "https://doi.org/10.5281/zenodo.12736565",
   "order": 189,
   "page_count": 5,
   "abstract": [
    "Dysfluent speech detection is the bottleneck for disordered speech analysis and spoken language learning. Current state-of-the-art models are governed by rule-based systems which lack efficiency and robustness, and are sensitive to template design. In this paper, we propose YOLO-Stutter: a first end-to-end method that detects dysfluencies in a time-accurate manner. YOLO-Stutter takes  imperfect speech-text alignment as input, followed by a spatial feature aggregator, and a temporal dependency extractor to perform region-wise boundary and class predictions. We also introduce two dysfluency corpus, VCTK-Stutter and VCTK-TTS, that simulate natural spoken dysfluencies including repetition, block, missing, replacement, and prolongation. Our end-to-end method achieves state-of-the-art performance with a minimum number of trainable parameters for on both simulated data and real aphasia speech . Code and datasets are open-sourced at https://github.com/rorizzz/YOLO-Stutter"
   ],
   "p1": 937,
   "pn": 941,
   "doi": "10.21437/Interspeech.2024-1855",
   "url": "interspeech_2024/zhou24e_interspeech.html"
  },
  "ning24_interspeech": {
   "authors": [
    [
     "Ziqian",
     "Ning"
    ],
    [
     "Shuai",
     "Wang"
    ],
    [
     "Pengcheng",
     "Zhu"
    ],
    [
     "Zhichao",
     "Wang"
    ],
    [
     "Jixun",
     "Yao"
    ],
    [
     "Lei",
     "Xie"
    ],
    [
     "Mengxiao",
     "Bi"
    ]
   ],
   "title": "DualVC 3: Leveraging Language Model Generated Pseudo Context for End-to-end Low Latency Streaming Voice Conversion",
   "original": "1857",
   "order": 41,
   "page_count": 5,
   "abstract": [
    "Streaming voice conversion has gained popularity for its applicability in real-time applications. The recently proposed DualVC 2 has successfully achieved robust and high-quality streaming voice conversion in approximately 180ms.  However, DualVC 2 is based on the recognition-synthesis framework, with multi-level cascade models that cannot be jointly optimized, and faces severe performance drops with small chunks caused by the ASR encoder. To address these issues, we propose an end-to-end model DualVC 3. It incorporates K-means clustered SSL features to guide the training of the content encoder and adopts an optional language model for pseudo-content generation to improve the conversion quality. Experimental results demonstrate that DualVC 3 achieves comparable performance to DualVC 2 in both subjective and objective metrics, with a latency of only 50 ms. We have made our audio samples publicly available."
   ],
   "p1": 197,
   "pn": 201,
   "doi": "10.21437/Interspeech.2024-1857",
   "url": "interspeech_2024/ning24_interspeech.html"
  },
  "huang24h_interspeech": {
   "authors": [
    [
     "Chao-Wei",
     "Huang"
    ],
    [
     "Hui",
     "Lu"
    ],
    [
     "Hongyu",
     "Gong"
    ],
    [
     "Hirofumi",
     "Inaguma"
    ],
    [
     "Ilia",
     "Kulikov"
    ],
    [
     "Ruslan",
     "Mavlyutov"
    ],
    [
     "Sravya",
     "Popuri"
    ]
   ],
   "title": "Investigating Decoder-only Large Language Models for Speech-to-text Translation",
   "original": "1858",
   "order": 168,
   "page_count": 5,
   "abstract": [
    "Large language models (LLMs), known for their exceptional reasoning capabilities, generalizability, and fluency across diverse domains, present a promising avenue for enhancing speech-related tasks. In this paper, we focus on integrating decoder-only LLMs to the task of speech-to-text translation (S2TT). We propose a decoder-only architecture that enables the LLM to directly consume the encoded speech representation and generate the text translation. Additionally, we investigate the effects of different parameter-efficient fine-tuning techniques and task formulation. Our model achieves state-of-the-art performance on CoVoST 2 and FLEURS among models trained without proprietary data. We also conduct analyses to validate the design choices of our proposed model and bring insights to the integration of LLMs to S2TT."
   ],
   "p1": 832,
   "pn": 836,
   "doi": "10.21437/Interspeech.2024-1858",
   "url": "interspeech_2024/huang24h_interspeech.html"
  },
  "guo24d_interspeech": {
   "authors": [
    [
     "Dake",
     "Guo"
    ],
    [
     "Xinfa",
     "Zhu"
    ],
    [
     "Liumeng",
     "Xue"
    ],
    [
     "Yongmao",
     "Zhang"
    ],
    [
     "Wenjie",
     "Tian"
    ],
    [
     "Lei",
     "Xie"
    ]
   ],
   "title": "Text-aware and Context-aware Expressive Audiobook Speech Synthesis",
   "original": "1862",
   "order": 367,
   "page_count": 5,
   "abstract": [
    "Recent advances in text-to-speech have significantly improved the expressiveness of synthetic speech.However, a major challenge remains in generating speech that captures the diverse styles exhibited by professional narrators in audiobooks,without relying on manual labele or reference speech. To address this, we propose a text-aware and context-aware(TACA)style modeling approach for expressive audiobook speech synthesis. We first establish a text-aware style space to cover diverse styles via contrastive learning with the supervision of the speech-style space. Meanwhile, we adopt a context encoder to incorporate cross-sentence information and the style embedding obtained from text. Finally, we introduce the context encoder to two typical TTS models, including VITS-based TTS and language model-based TTS. Experimental results show that our proposed approach can effectively capture diverse styles and coherent prosody,and thus improve naturalness and expressiveness in audiobook speech synthesis"
   ],
   "p1": 1790,
   "pn": 1794,
   "doi": "10.21437/Interspeech.2024-1862",
   "url": "interspeech_2024/guo24d_interspeech.html"
  },
  "zhao24h_interspeech": {
   "authors": [
    [
     "Hang",
     "Zhao"
    ],
    [
     "Yifei",
     "Xin"
    ],
    [
     "Zhesong",
     "Yu"
    ],
    [
     "Bilei",
     "Zhu"
    ],
    [
     "Lu",
     "Lu"
    ],
    [
     "Zejun",
     "Ma"
    ]
   ],
   "title": "MINT: Boosting Audio-Language Model via Multi-Target Pre-Training and Instruction Tuning",
   "original": "1863",
   "order": 12,
   "page_count": 5,
   "abstract": [
    "In the realm of audio-language pre-training (ALP), the challenge of achieving cross-modal alignment is significant. Moreover, the integration of audio inputs with diverse distributions and task variations poses challenges in developing generic audio-language models. In this study, we present MINT, a novel ALP framework boosting audio-language models through multi-target pre-training and instruction tuning. MINT leverages the strength of frozen pre-trained audio encoders and large language models (LLM) to improve audio-language pretraining, enabling effective transferablility to both audio-text understanding and generation tasks. To address the modality gap, we introduce Bridge-Net, a trainable module that enhances cross-modality alignment and the model’s ability to follow in- structions for a variety of audio-text tasks. Bridge-Net is pivotal within MINT, initially enhancing audio-language representation learning through a multi-target pre-training approach. Subsequently, Bridge-Net further boosts audio-to-language generative learning by integrating a frozen language model with instruction tuning. This integration empowers MINT to extract features in a flexible and effective manner, specifically tailored to the provided instructions for diverse tasks. Experimental results demonstrate that MINT attains superior performance across various audio-language understanding and generation tasks, highlighting its robust generalization capabilities even in zero-shot scenarios."
   ],
   "p1": 52,
   "pn": 56,
   "doi": "10.21437/Interspeech.2024-1863",
   "url": "interspeech_2024/zhao24h_interspeech.html"
  },
  "yang24o_interspeech": {
   "authors": [
    [
     "Yudong",
     "Yang"
    ],
    [
     "Rongfeng",
     "Su"
    ],
    [
     "Rukiye",
     "Ruzi"
    ],
    [
     "Manwa",
     "Ng"
    ],
    [
     "Shaofeng",
     "Zhao"
    ],
    [
     "Nan",
     "Yan"
    ],
    [
     "Lan",
     "Wang"
    ]
   ],
   "title": "Optical Flow Guided Tongue Trajectory Generation for Diffusion-based Acoustic to Articulatory Inversion",
   "original": "1864",
   "order": 85,
   "page_count": 5,
   "abstract": [
    "The diffusion-based Acoustic-to-Articulatory Inversion (AAI) approach has been shown impressive results for converting audio into Ultrasound Tongue Imaging (UTI) data with clear tongue contours. However, Mean Square Error (MSE) based diffusion models focus on the pixel error between reference and generated UTI data, inherently omitting changes in tongue movements. This leads to the discrepancy in tongue trajectory between reference and generated UTI data. To address this issue, this paper presents an Optical Flow Guided tongue trajectory generation method for training the diffusionbased AAI model.  The optical flow method calculates the displacement information of the tongue contours in consecutive frames, enabling the tongue trajectory similarity between reference and generated UTI data to be used as an additional constraint for Diffusion Model network optimization. Experimental results show that our proposed diffusionbased AAI system with additional tongue trajectory constraint outperformed the baseline system across various evaluation metrics."
   ],
   "p1": 417,
   "pn": 421,
   "doi": "10.21437/Interspeech.2024-1864",
   "url": "interspeech_2024/yang24o_interspeech.html"
  },
  "lu24d_interspeech": {
   "authors": [
    [
     "Xiang-Li",
     "Lu"
    ],
    [
     "Yi-Fen",
     "Liu"
    ]
   ],
   "title": "Deep Prosodic Features in Tandem with Perceptual Judgments of Word Reduction for Tone Recognition in Conversed Speech",
   "original": "1869",
   "order": 935,
   "page_count": 5,
   "abstract": [
    "To tackle the tone classification problem in conversational speech, we propose a transformer-based encoding network to classify tones in an utterance on a syllable-by-syllable basis. Using just F0 and rhythmic information, the interaction encoder consolidates contour representations first. By jointly predicting word tones using perceived judgments on reduction degrees, the learning architecture improves automatic recognition of the underlying syllable tones. Leveraging these enhancements, the experiments show that the proposed model is very robust and achieved a 12% increase in tone classification accuracy."
   ],
   "p1": 4553,
   "pn": 4557,
   "doi": "10.21437/Interspeech.2024-1869",
   "url": "interspeech_2024/lu24d_interspeech.html"
  },
  "kunmei24_interspeech": {
   "authors": [
    [
     "Han",
     "Kunmei"
    ]
   ],
   "title": "Modelling Lexical Characteristics of the Healthy Aging Population: A Corpus-Based Study",
   "original": "1871",
   "order": 227,
   "page_count": 5,
   "abstract": [
    "Language ability at an old age is a balance between preservation and decline. Modelling baseline language variation in normal aging thus is important for our understanding of healthy aging, which can help detect cognitive impairments at the prodromal stage. Large-language databases and NLP tools enable us to conduct automated quantitative analysis of natural language data. In this study, we aim to demonstrate that (i) age and sex influence old adults’ lexical distribution and lexical concreteness; and (ii) using NLP tools and psycholinguistic metrics to process natural language datasets can help to set a normative benchmark of aging languages."
   ],
   "p1": 1090,
   "pn": 1094,
   "doi": "10.21437/Interspeech.2024-1871",
   "url": "interspeech_2024/kunmei24_interspeech.html"
  },
  "li24ka_interspeech": {
   "authors": [
    [
     "Shaojun",
     "Li"
    ],
    [
     "Daimeng",
     "Wei"
    ],
    [
     "Hengchao",
     "Shang"
    ],
    [
     "Jiaxin",
     "Guo"
    ],
    [
     "ZongYao",
     "Li"
    ],
    [
     "Zhanglin",
     "Wu"
    ],
    [
     "Zhiqiang",
     "Rao"
    ],
    [
     "Yuanchang",
     "Luo"
    ],
    [
     "Xianghui",
     "He"
    ],
    [
     "Hao",
     "Yang"
    ]
   ],
   "title": "Speaker-Smoothed kNN Speaker Adaptation for End-to-End ASR",
   "original": "1873",
   "order": 493,
   "page_count": 5,
   "abstract": [
    "Despite recent improvements in End-to-End Automatic Speech Recognition (E2E ASR) systems, the performance can degrade due to vocal characteristic mismatches between training and testing data, particularly with limited target speaker adaptation data. We propose a novel speaker adaptation approach Speaker-Smoothed kNN that leverages k-Nearest Neighbors (kNN) retrieval techniques to improve model output by finding correctly pronounced tokens from its pre-built datastore during the decoding phase. Moreover, we utilize x-vector to dynamically adjust kNN interpolation parameters for data sparsity issue. This approach was validated using KeSpeech and MagicData corpora under in-domain and all-domain settings. Our method consistently performs comparably to fine-tuning without the associated performance degradation during speaker changes. Furthermore, in the all-domain setting, our method achieves state-of-the-art results, reducing the CER in both single speaker and multi-speaker test scenarios."
   ],
   "p1": 2390,
   "pn": 2394,
   "doi": "10.21437/Interspeech.2024-1873",
   "url": "interspeech_2024/li24ka_interspeech.html"
  },
  "choi24d_interspeech": {
   "authors": [
    [
     "Jeong-Hwan",
     "Choi"
    ],
    [
     "Ye-Rin",
     "Jeoung"
    ],
    [
     "Ilseok",
     "Kim"
    ],
    [
     "Joon-Hyuk",
     "Chang"
    ]
   ],
   "title": "Efficient Speaker Embedding Extraction Using a Twofold Sliding Window Algorithm for Speaker Diarization",
   "original": "1874",
   "order": 773,
   "page_count": 5,
   "abstract": [
    "This paper proposes an efficient speaker embedding (SE) extraction method that employs a twofold sliding window algorithm (SWA) for speaker diarization (SD) systems. Non-overlapping short segments are obtained through the first SWA and fed into the frame-level neural networks of a pre-trained SE model to extract frame-level representations. The neighboring frame-level representations are concatenated along the time axis through the second SWA, which enables an overlap between representations. The concatenated representations are used to extract multiple SEs. Additionally, we propose a fine-tuning strategy that employs a residual adapter and knowledge distillation techniques on a pre-trained SE model to refine the frame-level representation. Experimental results using two SD benchmarks show the effectiveness of the proposed extraction method with a fine-tuned SE model in terms of floating-point operations while maintaining the diarization error rate."
   ],
   "p1": 3749,
   "pn": 3753,
   "doi": "10.21437/Interspeech.2024-1874",
   "url": "interspeech_2024/choi24d_interspeech.html"
  },
  "lee24k_interspeech": {
   "authors": [
    [
     "Mun-Hak",
     "Lee"
    ],
    [
     "Jae-Hong",
     "Lee"
    ],
    [
     "DoHee",
     "Kim"
    ],
    [
     "Ye-Eun",
     "Ko"
    ],
    [
     "Joon-Hyuk",
     "Chang"
    ]
   ],
   "title": "Balanced-Wav2Vec: Enhancing Stability and Robustness of Representation Learning Through Sample Reweighting Techniques",
   "original": "1875",
   "order": 1036,
   "page_count": 5,
   "abstract": [
    "Mode collapse refers to the phenomenon where a representation model fits only a subset of modes in the feature space. Today, numerous self-supervised learning algorithms, including Wav2Vec 2.0, encounter the problem of reduced expressiveness due to mode collapse or dimension collapse. In this study, we experimentally verify that the highly skewed codebook distribution of the Wav2Vec 2.0 exacerbates the mode collapse problem. Based on this empirical finding, we propose the balanced-infoNCE loss, which suppresses the emergence of over-represented modes. We show that the Wav2Vec 2.0 model trained with balanced-infoNCE loss maintains high codebook entropy and converges stably. Furthermore, through finetuning experiments on a multilingual dataset for the ASR task, we demonstrate that balanced-Wav2Vec 2.0 models exhibit superior generalization performance."
   ],
   "p1": 5058,
   "pn": 5062,
   "doi": "10.21437/Interspeech.2024-1875",
   "url": "interspeech_2024/lee24k_interspeech.html"
  },
  "chang24b_interspeech": {
   "authors": [
    [
     "Xuankai",
     "Chang"
    ],
    [
     "Jiatong",
     "Shi"
    ],
    [
     "Jinchuan",
     "Tian"
    ],
    [
     "Yuning",
     "Wu"
    ],
    [
     "Yuxun",
     "Tang"
    ],
    [
     "Yihan",
     "Wu"
    ],
    [
     "Shinji",
     "Watanabe"
    ],
    [
     "Yossi",
     "Adi"
    ],
    [
     "Xie",
     "Chen"
    ],
    [
     "Qin",
     "Jin"
    ]
   ],
   "title": "The Interspeech 2024 Challenge on Speech Processing Using Discrete Units",
   "original": "1878",
   "order": 527,
   "page_count": 5,
   "abstract": [
    "Representing speech and audio signals in discrete units has become a compelling alternative to traditional high-dimensional feature vectors. Numerous studies have highlighted the efficacy of discrete units in various applications such as speech compression and restoration, speech recognition, and speech generation. To foster exploration in this domain, we introduce the Interspeech 2024 Challenge, which focuses on new speech processing benchmarks using discrete units. It encompasses three pivotal tasks, namely multilingual automatic speech recognition, text-to-speech, and singing voice synthesis, and aims to assess the potential applicability of discrete units in these tasks. This paper outlines the challenge designs and baseline descriptions. We also collate baseline and selected submission systems, along with preliminary findings, offering valuable contributions to future research in this evolving field."
   ],
   "p1": 2559,
   "pn": 2563,
   "doi": "10.21437/Interspeech.2024-1878",
   "url": "interspeech_2024/chang24b_interspeech.html"
  },
  "su24_interspeech": {
   "authors": [
    [
     "Hang",
     "Su"
    ],
    [
     "Yuxiang",
     "Kong"
    ],
    [
     "Lichun",
     "Fan"
    ],
    [
     "Peng",
     "Gao"
    ],
    [
     "Yujun",
     "Wang"
    ],
    [
     "Zhiyong",
     "Wu"
    ]
   ],
   "title": "Speaker Change Detection with Weighted-sum Knowledge Distillation based on Self-supervised Pre-trained Models",
   "original": "1885",
   "order": 340,
   "page_count": 5,
   "abstract": [
    "Speaker Change Detection (SCD) is an essential problem in speech processing and has various applications in many fields. The self-supervised models have shown impressive performance on many downstream tasks in the pre-training and fine-tuning paradigm. However, it has limitations to apply a fine-tuned self-supervised pre-trained model to frame-level SCD task in real industry because it typically requires a smaller model that consumes fewer computational resources. To tackle this issue, we propose using Knowledge Distillation (KD) to leverage the capabilities of the self-supervised model. First, a basic KD method based on the pre-trained model is proposed. Then, a weighted-sum KD method is proposed to selectively extract information from the pre-trained model. Experimental results demonstrate the effectiveness of the basic KD method as well as a further improvement for the weighted-sum KD method. The proposed method is more suitable for industrial applications compared with fine-tuning."
   ],
   "p1": 1655,
   "pn": 1659,
   "doi": "10.21437/Interspeech.2024-1885",
   "url": "interspeech_2024/su24_interspeech.html"
  },
  "lin24k_interspeech": {
   "authors": [
    [
     "Yeh-Sheng",
     "Lin"
    ],
    [
     "Shu-Chuan",
     "Tseng"
    ],
    [
     "Jyh-Shing Roger",
     "Jang"
    ]
   ],
   "title": "Leveraging Phonemic Transcription and Whisper toward Clinically Significant Indices for Automatic Child Speech Assessment",
   "original": "1887",
   "order": 503,
   "page_count": 5,
   "abstract": [
    "Diagnosing speech sound disorders (SSD) in children requires professional assessment by speech-language pathologists. Detecting and diagnosing a medical condition takes time and is usually expensive in terms of labor. However, early identification and treatment are essential for subsequent care. ASR-based child speech assessment prioritizes semantic understanding over phonetic accuracy, making it unsuitable for pronunciation assessment. This study uses phonemic transcription available in a normative dataset and utilizes pre-trained speech models to develop an automatic phoneme recognition model with a Phoneme Error Rate (PER) as low as 3.76%. Clinically relevant indices calculated from the model prediction are highly correlated with those from the original normative data. We regard these experimental results as solid evidence that validates the feasibility of our evaluation workflow for practical application in early screening for phonological development delays."
   ],
   "p1": 2440,
   "pn": 2444,
   "doi": "10.21437/Interspeech.2024-1887",
   "url": "interspeech_2024/lin24k_interspeech.html"
  },
  "wang24ha_interspeech": {
   "authors": [
    [
     "Rui",
     "Wang"
    ],
    [
     "Liping",
     "Chen"
    ],
    [
     "Kong Aik",
     "Lee"
    ],
    [
     "Zhen-Hua",
     "Ling"
    ]
   ],
   "title": "Asynchronous Voice Anonymization Using Adversarial Perturbation On Speaker Embedding",
   "original": "1888",
   "order": 913,
   "page_count": 5,
   "abstract": [
    "Voice anonymization has been developed as a technique for preserving privacy by replacing the speaker's voice in a speech signal with that of a pseudo-speaker, thereby obscuring the original voice attributes from machine recognition and human perception. In this paper, we focus on altering the voice attributes against machine recognition while retaining human perception. We referred to this as the asynchronous voice anonymization. To this end, a speech generation framework incorporating a speaker disentanglement mechanism is employed to generate the anonymized speech. The speaker attributes are altered through adversarial perturbation applied on the speaker embedding, while human perception is preserved by controlling the intensity of perturbation. Experiments conducted on the LibriSpeech dataset showed that the speaker attributes were obscured with their human perception preserved for 60.71% of the processed utterances. Audio samples can be found in ."
   ],
   "p1": 4443,
   "pn": 4447,
   "doi": "10.21437/Interspeech.2024-1888",
   "url": "interspeech_2024/wang24ha_interspeech.html"
  },
  "li24la_interspeech": {
   "authors": [
    [
     "Yishuang",
     "Li"
    ],
    [
     "Wenhao",
     "Guan"
    ],
    [
     "Hukai",
     "Huang"
    ],
    [
     "Shiyu",
     "Miao"
    ],
    [
     "Qi",
     "Su"
    ],
    [
     "Lin",
     "Li"
    ],
    [
     "Qingyang",
     "Hong"
    ]
   ],
   "title": "Efficient Integrated Features Based on Pre-trained Models for Speaker Verification",
   "original": "1889",
   "order": 443,
   "page_count": 5,
   "abstract": [
    "Previous work has explored the application of pre-trained models (PTMs) in speaker verification(SV). Most researchers directly replaced handcrafted features with the universal representations of the PTMs, and jointly fine-tuned PTMs with the downstream SV networks, which undoubtedly discarded important spectral information contained in handcrafted features and also increased the training cost. In this paper, we proposed an efficient feature integration method that utilized a Fine-grained Fusion Module to fuse the multi-layer representations of the PTMs adaptively. Then we integrated the fused representations with handcrafted features to obtain the integrated features, which were subsequently fed into the SV network. The experimental results demonstrated that using the integrated features effectively enhanced the performance of the SV systems, and yielded decent results with no need to fine-tune the PTMs. Moreover, employing full-parameter fine-tuning led to the current optimal results."
   ],
   "p1": 2140,
   "pn": 2144,
   "doi": "10.21437/Interspeech.2024-1889",
   "url": "interspeech_2024/li24la_interspeech.html"
  },
  "feng24c_interspeech": {
   "authors": [
    [
     "Jiu",
     "Feng"
    ],
    [
     "Mehmet Hamza",
     "Erol"
    ],
    [
     "Joon Son",
     "Chung"
    ],
    [
     "Arda",
     "Senocak"
    ]
   ],
   "title": "ElasticAST: An Audio Spectrogram Transformer for All Length and Resolutions",
   "original": "1890",
   "order": 973,
   "page_count": 5,
   "abstract": [
    "Transformers have rapidly overtaken CNN-based architectures as the new standard in audio classification. Transformer-based models, such as the Audio Spectrogram Transformers (AST), also inherit the fixed-size input paradigm from CNNs. However, this leads to performance degradation for ASTs in the inference when input lengths vary from the training. This paper introduces an approach that enables the use of variable-length audio inputs with AST models during both training and inference. By employing sequence packing, our method ElasticAST, accommodates any audio length during training, thereby offering flexibility across all lengths and resolutions at the inference. This flexibility allows ElasticAST to maintain evaluation capabilities at various lengths or resolutions and achieve similar performance to standard ASTs trained at specific lengths or resolutions. Moreover, experiments demonstrate ElasticAST's better performance when trained and evaluated on native-length audio datasets. Code can be found at: https://github.com/JiuFengSC/ElasticAST."
   ],
   "p1": 4743,
   "pn": 4747,
   "doi": "10.21437/Interspeech.2024-1890",
   "url": "interspeech_2024/feng24c_interspeech.html"
  },
  "zou24_interspeech": {
   "authors": [
    [
     "Kun",
     "Zou"
    ],
    [
     "Fengyun",
     "Tan"
    ],
    [
     "Ziyang",
     "Zhuang"
    ],
    [
     "Chenfeng",
     "Miao"
    ],
    [
     "Tao",
     "Wei"
    ],
    [
     "Shaodan",
     "Zhai"
    ],
    [
     "Zijian",
     "Li"
    ],
    [
     "Wei",
     "Hu"
    ],
    [
     "Shaojun",
     "Wang"
    ],
    [
     "Jing",
     "Xiao"
    ]
   ],
   "title": "E-Paraformer: A Faster and Better  Parallel Transformer for Non-autoregressive End-to-End Mandarin Speech Recognition",
   "original": "1891",
   "order": 55,
   "page_count": 5,
   "abstract": [
    "Paraformer is a powerful non-autoregressive (NAR) model for Mandarin speech recognition. It relies on Continuous Integrate-and-Fire (CIF) to implement parallel decoding. However, the CIF mechanism needs to recursively obtain the acoustic boundary of the emitted token, which will lead to inefficiency. In this paper, we introduce a novel monotonic alignment mechanism as an alternative to CIF that can convert frame-level embeddings into token-level embeddings in parallel. Combining this method with other improvements to the model structure, we design a faster and better parallel transformer called the Efficient Paraformer (E-Paraformer). Experiments are performed on the AISHELL-1 benchmark. Compared to Paraformer baseline, the E-Paraformer achieves character error rates (CER) of 4.36%/4.79% on the AISHELL-1 dev/test dataset, representing 7.8% and 6.3% (relative) reductions, respectively. Moreover, it achieves about 2x inference speedup and 1.35x training speedup."
   ],
   "p1": 267,
   "pn": 271,
   "doi": "10.21437/Interspeech.2024-1891",
   "url": "interspeech_2024/zou24_interspeech.html"
  },
  "watanabe24_interspeech": {
   "authors": [
    [
     "Yu",
     "Watanabe"
    ],
    [
     "Koichiro",
     "Ito"
    ],
    [
     "Shigeki",
     "Matsubara"
    ]
   ],
   "title": "Utilization of Text Data for Response Timing Detection in Attentive Listening",
   "original": "1892",
   "order": 729,
   "page_count": 5,
   "abstract": [
    "Conversational agents are expected to play the role of listening to narratives instead of humans. To be recognized as narrative listeners, these agents are required to generate responses indicating attentive listening at appropriate times. However, narrative and response data are not always well accumulated to develop statistical models. One solution to this issue is to utilize other data besides response data. In this study, we utilize text data to train a response timing detection model inspired by the relationship between punctuation marks and attentive listening responses. Specifically, the model was trained on a punctuation insertion task using text data before being trained on the response data. A response timing detection experiment was conducted to evaluate the effect of utilizing text data in terms of the amount of response data. The results showed that the utilization of text data enhanced the performance of the response timing detection, especially with limited data."
   ],
   "p1": 3565,
   "pn": 3569,
   "doi": "10.21437/Interspeech.2024-1892",
   "url": "interspeech_2024/watanabe24_interspeech.html"
  },
  "cui24_interspeech": {
   "authors": [
    [
     "Ziyun",
     "Cui"
    ],
    [
     "Chang",
     "Lei"
    ],
    [
     "Wen",
     "Wu"
    ],
    [
     "Yinan",
     "Duan"
    ],
    [
     "Diyang",
     "Qu"
    ],
    [
     "Ji",
     "Wu"
    ],
    [
     "Runsen",
     "Chen"
    ],
    [
     "Chao",
     "Zhang"
    ]
   ],
   "title": "Spontaneous Speech-Based Suicide Risk Detection Using Whisper and Large Language Models",
   "original": "1895",
   "order": 599,
   "page_count": 5,
   "abstract": [
    "The early detection of suicide risk is important since it enables the intervention to prevent potential suicide attempts. This paper studies the automatic detection of suicide risk based on spontaneous speech from adolescents, and collects a Mandarin dataset with 15 hours of suicide speech from more than a thousand adolescents aged from ten to eighteen for our experiments. To leverage the diverse acoustic and linguistic features embedded in spontaneous speech, both the Whisper speech model and textual large language models (LLMs) are used for suicide risk detection. Both all-parameter finetuning and parameter-efficient finetuning approaches are used to adapt the pre-trained models for suicide risk detection, and multiple audio-text fusion approaches are evaluated to combine the representations of Whisper and the LLM. The proposed system achieves a detection accuracy of 0.807 and an F1-score of 0.846 on the test set with 119 subjects, indicating promising potential for real suicide risk detection applications. "
   ],
   "p1": 2915,
   "pn": 2919,
   "doi": "10.21437/Interspeech.2024-1895",
   "url": "interspeech_2024/cui24_interspeech.html"
  },
  "yeh24_interspeech": {
   "authors": [
    [
     "Chia-Kai",
     "Yeh"
    ],
    [
     "Chih-Chun",
     "Chen"
    ],
    [
     "Ching-Hsien",
     "Hsu"
    ],
    [
     "Jen-Tzung",
     "Chien"
    ]
   ],
   "title": "Cross-Modality Diffusion Modeling and Sampling for Speech Recognition",
   "original": "1898",
   "order": 808,
   "page_count": 5,
   "abstract": [
    "The diffusion model excels as a generative model for continuous data within a single modality. To extend its effectiveness to speech recognition, where the continuous speech frames are used as the condition to generate the discrete word tokens, building a conditional diffusion across discrete state space becomes crucial. This paper introduces a non-autoregressive discrete diffusion model, enabling parallel generation of a word string corresponding to a speech signal through iterative diffusion steps. An acoustic transformer encoder identifies the speech representation, serving as the condition for a denoising transformer decoder to predict the whole discrete sequence. To address the redundancy reduction in cross-modality diffusion, an additional feature decorrelation objective is integrated during optimization. This paper further reduces the inference time by using a fast sampling approach. The experiments on speech recognition illustrate the merit of the proposed method."
   ],
   "p1": 3924,
   "pn": 3928,
   "doi": "10.21437/Interspeech.2024-1898",
   "url": "interspeech_2024/yeh24_interspeech.html"
  },
  "aimaiti24_interspeech": {
   "authors": [
    [
     "Ainikaerjiang",
     "Aimaiti"
    ],
    [
     "Di",
     "Wu"
    ],
    [
     "Liting",
     "Jiang"
    ],
    [
     "Gulinigeer",
     "Abudouwaili"
    ],
    [
     "Hao",
     "Huang"
    ],
    [
     "Wushour",
     "Silamu"
    ]
   ],
   "title": "An Uyghur Extension to the MASSIVE Multi-lingual Spoken Language Understanding Corpus with Comprehensive Evaluations",
   "original": "1900",
   "resource": "https://doi.org/10.5281/zenodo.12790177",
   "order": 721,
   "page_count": 5,
   "abstract": [
    "Spoken Language Understanding (SLU) plays a crucial role in task-oriented dialogues, and the development of SLU in various languages has been rapid. However, progress in Uyghur SLU research has been slow due to the lack of publicly available datasets. To address this issue, we extend the MASSIVE dataset to include Uyghur language, thus creating the first Uyghur SLU dataset, MASSIVE-UG. After incorporating MASSIVE-UG, the average overall accuracy of the other 51 languages has improved, demonstrating the reliability of the dataset constructed in this paper. Considering the agglutinative nature of Uyghur, we segmented it into stem and affix and conducted experiments using different embedding methods and multiple baselines. The experimental results indicate that the performance of Uyghur SLU is influenced by several factors, including representation, embedding, and modeling approach. The dataset and code are available at https://github.com/xjuspeech/MASSIVE-UG. "
   ],
   "p1": 3525,
   "pn": 3529,
   "doi": "10.21437/Interspeech.2024-1900",
   "url": "interspeech_2024/aimaiti24_interspeech.html"
  },
  "baskar24_interspeech": {
   "authors": [
    [
     "Murali Karthick",
     "Baskar"
    ],
    [
     "Andrew",
     "Rosenberg"
    ],
    [
     "Bhuvana",
     "Ramabhadran"
    ],
    [
     "Neeraj",
     "Gaur"
    ],
    [
     "Zhong",
     "Meng"
    ]
   ],
   "title": "Speech Prefix-Tuning with RNNT Loss for Improving LLM Predictions",
   "original": "1903",
   "order": 388,
   "page_count": 5,
   "abstract": [
    "In this paper, we focus on addressing the constraints faced when applying LLMs to ASR. Recent works utilize prefixLM-type models, which directly apply speech as a prefix to LLMs for ASR. We have found that optimizing speech prefixes leads to better ASR performance and propose applying RNNT loss to perform speech prefix-tuning. This is a simple approach and does not increase the model complexity or alter the inference pipeline. We also propose language-based soft prompting to further improve with frozen LLMs. Empirical analysis on realtime testset from 10 Indic languages demonstrate that our proposed speech prefix-tuning yields improvements with both frozen and fine-tuned LLMs. Our recognition results on an average of 10 Indics show that the proposed prefix-tuning with RNNT loss results in a 12% relative improvement in WER over the baseline with a fine-tuned LLM. Our proposed approches with the frozen LLM leads to a 31% relative improvement over basic soft-prompting prefixLM."
   ],
   "p1": 1895,
   "pn": 1899,
   "doi": "10.21437/Interspeech.2024-1903",
   "url": "interspeech_2024/baskar24_interspeech.html"
  },
  "kadkhodaieelyaderani24_interspeech": {
   "authors": [
    [
     "Mojtaba",
     "Kadkhodaie Elyaderani"
    ],
    [
     "John",
     "Glover"
    ],
    [
     "Thomas",
     "Schaaf"
    ]
   ],
   "title": "Reference-Free Estimation of the Quality of Clinical Notes Generated from Doctor-Patient Conversations",
   "original": "1907",
   "order": 294,
   "page_count": 5,
   "abstract": [
    "This paper describes a simple yet robust approach to performing reference-free estimation of the quality of automatically-generated clinical notes derived from doctor-patient conversations. In the absence of human-written reference notes, this approach works by generating a diverse collection of \"pseudo-reference notes\" and comparing the generated note against those pseudo-references. This method has been applied to estimate the quality of clinical note sections generated by three different note generation models, using a collection of evaluation metrics that are based on natural language inference and clinical concept extraction. Our experiments show the proposed approach is robust to the choice of note generation models, and consistently produces higher correlations with reference-based counterparts when compared against a strong baseline method."
   ],
   "p1": 1425,
   "pn": 1429,
   "doi": "10.21437/Interspeech.2024-1907",
   "url": "interspeech_2024/kadkhodaieelyaderani24_interspeech.html"
  },
  "geng24_interspeech": {
   "authors": [
    [
     "Tianqi",
     "Geng"
    ],
    [
     "Hui",
     "Feng"
    ]
   ],
   "title": "Form and Function in Prosodic Representation:  In the Case of 'ma' in Tianjin Mandarin",
   "original": "1909",
   "order": 426,
   "page_count": 5,
   "abstract": [
    "This study investigates the acoustic features of “ma” in Tianjin Mandarin, analyzing its functions as deixis, pronoun, modal particle, and discourse marker. Eight native speakers participated in the research. Results revealed distinct prosodic patterns, with “ma” as a modal particle exhibiting the shortest duration, weakest intensity, and narrowest pitch range. As a discourse marker, “ma” displays the widest pitch range. As a deixis, “ma” shares similar acoustic characteristics with it as a pronoun but exhibits a more stable distribution of primary acoustic features. Clustering identifies two distinct pitch patterns – high-falling and low-falling tones, demonstrating various realizations speakers may choose under different functions. This study highlights the correlation between the form and function of the prosodic representation of “ma” in Tianjin Mandarin."
   ],
   "p1": 2055,
   "pn": 2059,
   "doi": "10.21437/Interspeech.2024-1909",
   "url": "interspeech_2024/geng24_interspeech.html"
  },
  "yang24p_interspeech": {
   "authors": [
    [
     "Muqiao",
     "Yang"
    ],
    [
     "Xiang",
     "Li"
    ],
    [
     "Umberto",
     "Cappellazzo"
    ],
    [
     "Shinji",
     "Watanabe"
    ],
    [
     "Bhiksha",
     "Raj"
    ]
   ],
   "title": "Towards Unified Evaluation of Continual Learning in Spoken Language Understanding",
   "original": "1911",
   "order": 719,
   "page_count": 5,
   "abstract": [
    "Continual learning has emerged as an important challenge across various tasks, including Spoken Language Understanding (SLU). The evaluation of continual learning algorithms typically involves assessing the model's stability, plasticity, and generalizability. However, existing continual learning metrics primarily focus on only one or two of the properties. They neglect the overall performance across all tasks, and do not adequately disentangle the plasticity versus stability/generalizability trade-offs in the model. In this work, we propose an evaluation metric that provides a unified evaluation on stability, plasticity, and generalizability. By employing the proposed metric, we demonstrate how introducing various knowledge distillations can improve these three properties, and we apply it to the SLU model evaluation. We further show that our proposed metric is more sensitive in capturing the impact of task ordering in continual learning, making it better suited for practical scenarios."
   ],
   "p1": 3515,
   "pn": 3519,
   "doi": "10.21437/Interspeech.2024-1911",
   "url": "interspeech_2024/yang24p_interspeech.html"
  },
  "lai24b_interspeech": {
   "authors": [
    [
     "Shijie",
     "Lai"
    ],
    [
     "Minglu",
     "He"
    ],
    [
     "Zijing",
     "Zhao"
    ],
    [
     "Kai",
     "Wang"
    ],
    [
     "Hao",
     "Huang"
    ],
    [
     "Jichen",
     "Yang"
    ]
   ],
   "title": "Synthesizing Long-Form Speech merely from Sentence-Level Corpus with Content Extrapolation and LLM Contextual Enrichment",
   "original": "1913",
   "order": 702,
   "page_count": 5,
   "abstract": [
    "Current text-to-speech (TTS) models can produce natural speech but often fail to synthesize long-form speech properly when only sentence-level corpus is available. The failure is mainly due to 1) poor length generalization of the acoustic model, 2) lack of appropriate pause marks in the inference text, and 3) absence of contextual information during training. We propose Content Extrapolation, which includes introducing Moving Average Equipped Gated Attention (MEGA) to improve the model's generalization for addressing 1) and presenting the Global-information-enhanced Classification Pause Insertion model (GCPI) to address 2). For 3), we propose LLM-based Contextual Enrichment (LLM-CE) to generate multiple sets of different contexts. Experiments show that the proposed methods solve the above issues and successfully generate long-form speech with clear pronunciation and natural prosody using only sentence-level corpus, reducing training costs."
   ],
   "p1": 3430,
   "pn": 3434,
   "doi": "10.21437/Interspeech.2024-1913",
   "url": "interspeech_2024/lai24b_interspeech.html"
  },
  "zhao24i_interspeech": {
   "authors": [
    [
     "Bingliang",
     "Zhao"
    ],
    [
     "Jiangping",
     "Kong"
    ],
    [
     "Xiyu",
     "Wu"
    ]
   ],
   "title": "Age-related Differences in Acoustic Cues for the Perception of Checked Syllables in Shengzhou Wu",
   "original": "1915",
   "order": 871,
   "page_count": 5,
   "abstract": [
    "The transition from checked to unchecked syllables is a crucial phenomenon of sound change in Wu Chinese. This study aims to discover the underlying mechanisms of this evolution, specifically by investigating perceptual age-related differences in Shengzhou Wu. After conducting acoustic and articulatory analyses of checked syllables, an identification task was performed to assess the perceptual importance of F0, vowel duration, and the glottal stop coda in two generations. The results show that older individuals tend to rely more on the glottal stop coda when identifying checked syllables, whereas youngsters place more perceptual weight on vowel duration. Additionally, younger people have a reduced ability to recognize checked syllables, potentially due to their exposure to standard Mandarin. These findings suggest that the glottal stop coda may weaken and disappear before the lengthening of vowel duration in the evolution of checked syllables in Wu Chinese."
   ],
   "p1": 4233,
   "pn": 4237,
   "doi": "10.21437/Interspeech.2024-1915",
   "url": "interspeech_2024/zhao24i_interspeech.html"
  },
  "ghosh24b_interspeech": {
   "authors": [
    [
     "Sreyan",
     "Ghosh"
    ],
    [
     "Sonal",
     "Kumar"
    ],
    [
     "Ashish",
     "Seth"
    ],
    [
     "Purva",
     "Chiniya"
    ],
    [
     "Utkarsh",
     "Tyagi"
    ],
    [
     "Ramani",
     "Duraiswami"
    ],
    [
     "Dinesh",
     "Manocha"
    ]
   ],
   "title": "LipGER: Visually-Conditioned Generative Error Correction for Robust Automatic Speech Recognition",
   "original": "1918",
   "resource": "https://doi.org/10.5281/zenodo.12733987",
   "order": 393,
   "page_count": 5,
   "abstract": [
    "Visual cues, like lip motion, have been shown to improve the performance of Automatic Speech Recognition (ASR) systems in noisy environments. We propose LipGER (Lip Motion aided Generative Error Correction), a novel framework for leveraging visual cues for noise-robust ASR. Instead of learning the cross-modal correlation between the audio and visual modalities, we make an LLM learn the task of visually-conditioned (generative) ASR error correction. Specifically, we instruct an LLM to predict the transcription from the N-best hypotheses generated using ASR beam-search. This is further conditioned on lip motions. This approach addresses key challenges in traditional AVSR learning, such as the lack of large-scale paired datasets and difficulties in adapting to new domains. We experiment on 4 datasets in various settings and show that LipGER improves the Word Error Rate in the range of 1.1%-49.2%. We also release LipHyp, a dataset with hypothesis-transcription pairs and lip motion cues."
   ],
   "p1": 1920,
   "pn": 1924,
   "doi": "10.21437/Interspeech.2024-1918",
   "url": "interspeech_2024/ghosh24b_interspeech.html"
  },
  "doukhan24_interspeech": {
   "authors": [
    [
     "David",
     "Doukhan"
    ],
    [
     "Lena",
     "Dodson"
    ],
    [
     "Manon",
     "Conan"
    ],
    [
     "Valentin",
     "Pelloin"
    ],
    [
     "Aurélien",
     "Clamouse"
    ],
    [
     "Mélina",
     "Lepape"
    ],
    [
     "Géraldine",
     "Van Hille"
    ],
    [
     "Cécile",
     "Méadel"
    ],
    [
     "Marlène",
     "Coulomb-Gully"
    ]
   ],
   "title": "Gender Representation in TV and Radio: Automatic Information Extraction methods versus Manual Analyses",
   "original": "1921",
   "order": 628,
   "page_count": 5,
   "abstract": [
    "This study investigates the relationship between automatic information extraction descriptors and manual analyses to describe gender representation disparities in TV and Radio. Automatic descriptors, including speech time, facial categorization and speech transcriptions are compared with channel reports on a vast 32,000-hour corpus of French broadcasts from 2023. Findings reveal systemic gender imbalances, with women underrepresented compared to men across all descriptors. Notably, manual channel reports show higher women’s presence than automatic estimates and references to women are lower than their speech time. Descriptors share common dynamics during high and low audiences, war coverage, or private versus public channels. While women are more visible than audible in French TV, this trend is inverted in news with unseen journalists depicting male protagonists. A statistical test shows 3 main effects influencing references to women: program category, channel and speaker gender."
   ],
   "p1": 3060,
   "pn": 3064,
   "doi": "10.21437/Interspeech.2024-1921",
   "url": "interspeech_2024/doukhan24_interspeech.html"
  },
  "gaur24_interspeech": {
   "authors": [
    [
     "Neeraj",
     "Gaur"
    ],
    [
     "Rohan",
     "Agrawal"
    ],
    [
     "Gary",
     "Wang"
    ],
    [
     "Parisa",
     "Haghani"
    ],
    [
     "Andrew",
     "Rosenberg"
    ],
    [
     "Bhuvana",
     "Ramabhadran"
    ]
   ],
   "title": "ASTRA: Aligning Speech and Text Representations for Asr without Sampling",
   "original": "1924",
   "order": 804,
   "page_count": 5,
   "abstract": [
    "This paper introduces ASTRA, a novel method for improving Automatic Speech Recognition (ASR) through text injection. Unlike prevailing techniques, ASTRA eliminates the need for sampling to match sequence lengths between speech and text modalities. Instead, it leverages the inherent alignments learned within CTC/RNNT models. This approach offers the following two advantages, namely, avoiding potential misalignment between speech and text features that could arise from upsampling and eliminating the need for models to accurately predict duration of sub-word tokens. This novel formulation of modality (length) matching as a weighted RNNT objective matches the performance of the state-of-the-art duration-based methods on the FLEURS benchmark, while opening up other avenues of research in speech processing."
   ],
   "p1": 3904,
   "pn": 3908,
   "doi": "10.21437/Interspeech.2024-1924",
   "url": "interspeech_2024/gaur24_interspeech.html"
  },
  "rameau24_interspeech": {
   "authors": [
    [
     "Anaïs",
     "Rameau"
    ],
    [
     "Satrajit",
     "Ghosh"
    ],
    [
     "Alexandros",
     "Sigaras"
    ],
    [
     "Olivier",
     "Elemento"
    ],
    [
     "Jean-Christophe",
     "Belisle-Pipon"
    ],
    [
     "Vardit",
     "Ravitsky"
    ],
    [
     "Maria",
     "Powell"
    ],
    [
     "Alistair",
     "Johnson"
    ],
    [
     "David",
     "Dorr"
    ],
    [
     "Philip",
     "Payne"
    ],
    [
     "Micah",
     "Boyer"
    ],
    [
     "Stephanie",
     "Watts"
    ],
    [
     "Ruth",
     "Bahr"
    ],
    [
     "Frank",
     "Rudzicz"
    ],
    [
     "Jordan",
     "Lerner-Ellis"
    ],
    [
     "Shaheen",
     "Awan"
    ],
    [
     "Don",
     "Bolser"
    ],
    [
     "Yael",
     "Bensoussan"
    ]
   ],
   "title": "Developing Multi-Disorder Voice Protocols: A team science approach involving clinical expertise, bioethics, standards, and DEI.",
   "original": "1926",
   "resource": "https://doi.org/10.5281/zenodo.12760724",
   "order": 298,
   "page_count": 5,
   "abstract": [
    "The world of voice biomarkers is rapidly evolving thanks to the use of artificial intelligence (AI) allowing large-scale analysis of voice, speech, and respiratory sound data. The Bridge2AI-Voice project aims to build a large-scale, ethically sourced, and diverse voice database of human voices linked to health information to help fuel Voice AI research, dubbed Audiomics. The current paper describes the development of protocols of data acquisition across 4 different adult cohorts of disease (voice, respiratory, neurodegenerative diseases, mood, and anxiety disorders) using a Team Science approach for broader adoption by the research community and feedback. Demographic Surveys, Confounders Assessments, Acoustic tasks, validated patient-reported outcome (PRO) questionnaires and clinician-validated diagnostic questions were grouped in a common PART A across all cohorts and individual PART B, with cohort-specific tasks."
   ],
   "p1": 1445,
   "pn": 1449,
   "doi": "10.21437/Interspeech.2024-1926",
   "url": "interspeech_2024/rameau24_interspeech.html"
  },
  "zhang24o_interspeech": {
   "authors": [
    [
     "Yuewei",
     "Zhang"
    ],
    [
     "Huanbin",
     "Zou"
    ],
    [
     "Jie",
     "Zhu"
    ]
   ],
   "title": "Sub-PNWR: Speech Enhancement Based on Signal Sub-Band Splitting and Pseudo Noisy Waveform Reconstruction Loss",
   "original": "1927",
   "order": 133,
   "page_count": 5,
   "abstract": [
    "Existing deep learning-based speech enhancement (SE) methods typically entail high computational complexity. In this paper, we propose to split the input audio into adjacent equally spaced sub-band signals by an analysis filter bank, and feed these sub-band signals into a SE model to recover the denoised sub-band signals. These denoised sub-band signals are then reconstructed back to the full-band signal by a synthesis filter bank. Meanwhile, we design a full-band information fusion module to complement the sub-band feature with full-band spectral information. We also devise a full-band spectrum prediction module to predict the target full-band spectrum, which assists model training. Additionally, a pseudo noisy waveform reconstruction (PNWR) loss is introduced for better SE performance. Experiments demonstrate that the proposed scheme reduces the computational volume by about half with nearly no performance loss. The final SE system (Sub-PNWR) outperforms the current advanced methods."
   ],
   "p1": 657,
   "pn": 661,
   "doi": "10.21437/Interspeech.2024-1927",
   "url": "interspeech_2024/zhang24o_interspeech.html"
  },
  "chang24c_interspeech": {
   "authors": [
    [
     "Kai-Wei",
     "Chang"
    ],
    [
     "Ming-Hao",
     "Hsu"
    ],
    [
     "Shan-Wen",
     "Li"
    ],
    [
     "Hung-yi",
     "Lee"
    ]
   ],
   "title": "Exploring In-Context Learning of Textless Speech Language Model for Speech Classification Tasks",
   "original": "1932",
   "order": 851,
   "page_count": 5,
   "abstract": [
    "Ever since the development of GPT-3 in the natural language processing (NLP) field, in-context learning (ICL) has played an essential role in utilizing large language models (LLMs). By presenting the LM utterance-label demonstrations at the input, the LM can accomplish few-shot learning without relying on gradient descent or requiring explicit modification of its parameters. This enables the LM to perform various downstream tasks in a black-box manner. Despite the success of ICL in NLP, little work is exploring the possibility of ICL in speech processing. This study is the first work exploring ICL for speech classification tasks with textless speech LM. We first show that the current speech LM lacks the ICL capability. We then perform warmup training on the speech LM, equipping the LM with demonstration learning capability. This paper explores and proposes the first speech LM capable of performing unseen classification tasks in an ICL manner."
   ],
   "p1": 4139,
   "pn": 4143,
   "doi": "10.21437/Interspeech.2024-1932",
   "url": "interspeech_2024/chang24c_interspeech.html"
  },
  "kato24_interspeech": {
   "authors": [
    [
     "Akihiro",
     "Kato"
    ],
    [
     "Hiroyuki",
     "Nagano"
    ],
    [
     "Kohei",
     "Chike"
    ],
    [
     "Masaki",
     "Nose"
    ]
   ],
   "title": "Self-Supervised Learning for ASR Pre-Training with Uniquely Determined Target Labels and Controlling Cepstrum Truncation for Speech Augmentation",
   "original": "1933",
   "order": 1034,
   "page_count": 5,
   "abstract": [
    "To utilize a pre-trained large-scale model is an effective choice to develop automatic speech recognition (ASR) at limited data conditions. However, if we try pre-training with supervised manner, it causes high costs, specifically for transcription. To tackle this problem, recent research has presented self-supervised learning and it has successfully performed at ASR tasks. For further improvement, we study a new approach to self-supervised learning for ASR including methods for generating self-supervised labels and data augmentation. Experimental results on Libri-Light and LibriSpeech corpora without any external language models demonstrate that our proposed method outperforms non pre-trained Conformer at limited data conditions in terms of character error rate (CER). Furthermore, the proposed method also exhibits comparable performance to HuBERT, which is one of the state-of-the-art model for self-supervised representation learning."
   ],
   "p1": 5048,
   "pn": 5052,
   "doi": "10.21437/Interspeech.2024-1933",
   "url": "interspeech_2024/kato24_interspeech.html"
  },
  "niu24b_interspeech": {
   "authors": [
    [
     "Fangjing",
     "Niu"
    ],
    [
     "Xiaozhe",
     "Qi"
    ],
    [
     "Xinya",
     "Chen"
    ],
    [
     "Liang",
     "He"
    ]
   ],
   "title": "Speech Topic Classification Based on Multi-Scale and Graph Attention Networks",
   "original": "1934",
   "order": 887,
   "page_count": 5,
   "abstract": [
    "Speech topic classification (STC) typically consists of two parts: first, the speech is automatically transcribed into text using automatic speech recognition (ASR), and then the transcribed text is subjected to text-based topic recognition. However, this method often suffers from issues such as error propagation and the lack of global structural information. In this paper, we employ a multi-scale convolutional network to capture local semantic features of different granularities in the temporal dimension by using convolutional kernels of various sizes. Then, we utilize an attention mechanism to learn the similarity relationships between nodes. By using the top-K mask, we select the K most relevant nodes to construct a graph network. Finally, we aggregate node features to capture the dependency relationships of global context. Our method achieved state-of-the-art performance on the Fisher and Switchboard datasets, even surpassing the classification accuracy on oracle transcripts."
   ],
   "p1": 4313,
   "pn": 4317,
   "doi": "10.21437/Interspeech.2024-1934",
   "url": "interspeech_2024/niu24b_interspeech.html"
  },
  "tian24_interspeech": {
   "authors": [
    [
     "Jinchuan",
     "Tian"
    ],
    [
     "Yifan",
     "Peng"
    ],
    [
     "William",
     "Chen"
    ],
    [
     "Kwanghee",
     "Choi"
    ],
    [
     "Karen",
     "Livescu"
    ],
    [
     "Shinji",
     "Watanabe"
    ]
   ],
   "title": "On the Effects of Heterogeneous Data Sources on Speech-to-Text Foundation Models",
   "original": "1938",
   "order": 815,
   "page_count": 5,
   "abstract": [
    "The Open Whisper-style Speech Model (OWSM) series was introduced to achieve full transparency in building advanced speech-to-text (S2T) foundation models. To this end, OWSM models are trained on 25 public speech datasets, which are heterogeneous in multiple ways. In this study, we advance the OWSM series by introducing OWSM v3.2, which improves on prior models by investigating and addressing the impacts of this data heterogeneity. Our study begins with a detailed analysis of each dataset, from which we derive two key strategies: data filtering with proxy task to enhance data quality, and the incorporation of punctuation and true-casing using an open large language model (LLM). With all other configurations staying the same, OWSM v3.2 improves performance over the OWSM v3.1 baseline while using 15% less training data."
   ],
   "p1": 3959,
   "pn": 3963,
   "doi": "10.21437/Interspeech.2024-1938",
   "url": "interspeech_2024/tian24_interspeech.html"
  },
  "zaheera24_interspeech": {
   "authors": [
    [
     "Fathima",
     "Zaheera"
    ],
    [
     "Supritha",
     "Shetty"
    ],
    [
     "Gayadhar",
     "Pradhan"
    ],
    [
     "Deepak",
     "K T"
    ]
   ],
   "title": "Automatic Assessment of Dysarthria using Speech and synthetically generated Electroglottograph signal",
   "original": "1939",
   "order": 844,
   "page_count": 5,
   "abstract": [
    "The formants are flat and dispersed in the short-term magnitude spectra (STMS) of dysarthric speech. This paper investigates the possibility of enhancing the performance of an automated dysarthric assessment by exploiting the complementary perceptual cues present in the STMS of speech and synthetically generated Electroglottograph (EGG) signal. To capture the complementary information through a single acoustic feature representation, the log Mel filterbank energy (LMFE) computed from both kinds of signal is averaged. The resulting LMFE is then used for the computation of Mel frequency cepstral coefficients (MFCCs). The analytical and experimental results presented on the UA-Speech corpus validate the efficacy of the proposed approach. For the x-vector-based automated dysarthric assessment system the accuracy and F1 score improved from 73% and 64% to 78% and 71%, respectively, in a speaker and text-independent mode when the MFCCs are computed from the averaged LMFE."
   ],
   "p1": 4104,
   "pn": 4108,
   "doi": "10.21437/Interspeech.2024-1939",
   "url": "interspeech_2024/zaheera24_interspeech.html"
  },
  "niu24c_interspeech": {
   "authors": [
    [
     "Yixiang",
     "Niu"
    ],
    [
     "Ning",
     "Chen"
    ],
    [
     "Hongqing",
     "Zhu"
    ],
    [
     "Zhiying",
     "Zhu"
    ],
    [
     "Guangqiang",
     "Li"
    ],
    [
     "Yibo",
     "Chen"
    ]
   ],
   "title": "Auditory Spatial Attention Detection Based on Feature Disentanglement and Brain Connectivity-Informed Graph Neural Networks",
   "original": "1940",
   "order": 179,
   "page_count": 5,
   "abstract": [
    "Auditory spatial attention detection (ASAD) aims to determine which speaker in a surround sound field a listener is focusing on from a single-trial electroencephalogram (EEG). Latest studies have represented non-Euclidean structured EEGs by graph-based modeling, but how to better incorporate brain connectivity into EEG graphs remains a great challenge. Moreover, due to inter-subject distribution shifts in EEGs, most existing models perform well only on specific subjects. To address these issues, we propose a new ASAD model. EEG graphs are constructed based on brain effective connectivity, and then mapped into embedding spaces by a graph neural network architecture. Meanwhile, feature disentanglement combined with correlation alignment is utilized to learn subject-invariant EEG patterns relevant to ASAD tasks. Experiments on open datasets demonstrate that in cross-subject scenarios, the proposed model outperforms state-of-the-art models, and the algorithmic complexity is relatively low."
   ],
   "p1": 887,
   "pn": 891,
   "doi": "10.21437/Interspeech.2024-1940",
   "url": "interspeech_2024/niu24c_interspeech.html"
  },
  "qi24_interspeech": {
   "authors": [
    [
     "Tianhua",
     "Qi"
    ],
    [
     "Shiyan",
     "Wang"
    ],
    [
     "Cheng",
     "Lu"
    ],
    [
     "Yan",
     "Zhao"
    ],
    [
     "Yuan",
     "Zong"
    ],
    [
     "Wenming",
     "Zheng"
    ]
   ],
   "title": "Towards Realistic Emotional Voice Conversion using Controllable Emotional Intensity",
   "original": "1941",
   "order": 42,
   "page_count": 5,
   "abstract": [
    "Realistic emotional voice conversion (EVC) aims to enhance emotional diversity of converted audios, making the synthesized voices more authentic and natural. To this end, we propose Emotional Intensity-aware Network (EINet), dynamically adjusting intonation and rhythm by incorporating controllable emotional intensity. To better capture nuances in emotional intensity, we go beyond mere distance measurements among acoustic features. Instead, an emotion evaluator is utilized to precisely quantify speaker’s emotional state. By employing an intensity mapper, intensity pseudo-labels are obtained to bridge the gap between emotional speech intensity modeling and run-time conversion. To ensure high speech quality while retaining controllability, an emotion renderer is used for combining linguistic features smoothly with manipulated emotional features at frame level. Furthermore, we employ a duration predictor to facilitate adaptive prediction of rhythm changes condition on specifying intensity value. Experimental results show EINet’s superior performance in naturalness and diversity of emotional expression compared to state-of-the-art EVC methods."
   ],
   "p1": 202,
   "pn": 206,
   "doi": "10.21437/Interspeech.2024-1941",
   "url": "interspeech_2024/qi24_interspeech.html"
  },
  "wang24ia_interspeech": {
   "authors": [
    [
     "Jincen",
     "Wang"
    ],
    [
     "Yan",
     "Zhao"
    ],
    [
     "Cheng",
     "Lu"
    ],
    [
     "Chuangao",
     "Tang"
    ],
    [
     "Sunan",
     "Li"
    ],
    [
     "Yuan",
     "Zong"
    ],
    [
     "Wenming",
     "Zheng"
    ]
   ],
   "title": "Boosting Cross-Corpus Speech Emotion Recognition using CycleGAN with Contrastive Learning",
   "original": "1947",
   "order": 330,
   "page_count": 5,
   "abstract": [
    "The premise for the success of most classic speech emotion recognition (SER) algorithms is that training and testing samples are independent and identically distributed. However, the premise is not always valid in real life. Thus, in this paper, we propose a novel transfer learning method called contrastive cycle generative adversarial network (C2GAN) to address cross-corpus SER, where training and testing data originates from different corpora. Specifically, we first adapt CycleGAN to generate synthetic data, transforming samples between source and target corpora, to enhance the variability of source data. Then, an emotion-guided contrastive learning module is introduced to jointly optimize original and synthetic data during training, leading to better class-level feature alignment. We conduct experiments on eNTERFACE, CASIA and EmoDB datasets with six different settings for evaluation. Extensive results confirm the excellent performance of C2GAN over other state-of-the-art methods."
   ],
   "p1": 1605,
   "pn": 1609,
   "doi": "10.21437/Interspeech.2024-1947",
   "url": "interspeech_2024/wang24ia_interspeech.html"
  },
  "lu24e_interspeech": {
   "authors": [
    [
     "Cheng",
     "Lu"
    ],
    [
     "Yuan",
     "Zong"
    ],
    [
     "Yan",
     "Zhao"
    ],
    [
     "Hailun",
     "Lian"
    ],
    [
     "Tianhua",
     "Qi"
    ],
    [
     "Björn",
     "Schuller"
    ],
    [
     "Wenming",
     "Zheng"
    ]
   ],
   "title": "Hierarchical Distribution Adaptation for Unsupervised Cross-corpus Speech Emotion Recognition",
   "original": "1948",
   "order": 771,
   "page_count": 5,
   "abstract": [
    "The primary issue of unsupervised cross-corpus speech emotion recognition (SER) is that domain shift between the training and testing data undermines the SER model’s ability to generalize on unknown testing datasets. In this paper, we propose a straightforward and effective strategy, called Hierarchical Distribution Adaptation (HDA), to address the domain bias issue. HDA leverages a hierarchical emotion representation module based on nested Transformers to extract speech emotion features at different levels (e.g., frame/segment/utterance-level), for capturing multiple-scale emotion correlations in speech. Furthermore, a hierarchical distribution adaptation module, including frame-level distribution adaptation (FDA), segment- level distribution adaptation (SDA), and utterance-level distribution adaptation (UDA), is developed to align the hierarchical-level emotion representations of the training and testing speech samples to effectively eliminate domain discrepancy. Extensive experimental results demonstrate the superiority of our proposed HDA over other state-of-the art (SOTA) methods."
   ],
   "p1": 3739,
   "pn": 3743,
   "doi": "10.21437/Interspeech.2024-1948",
   "url": "interspeech_2024/lu24e_interspeech.html"
  },
  "bitterman24_interspeech": {
   "authors": [
    [
     "Jacob",
     "Bitterman"
    ],
    [
     "Daniel",
     "Levi"
    ],
    [
     "Hilel Hagai",
     "Diamandi"
    ],
    [
     "Sharon",
     "Gannot"
    ],
    [
     "Tal",
     "Rosenwein"
    ]
   ],
   "title": "RevRIR: Joint Reverberant Speech and Room Impulse Response Embedding using Contrastive Learning with Application to Room Shape Classification",
   "original": "1951",
   "order": 672,
   "page_count": 5,
   "abstract": [
    "This paper focuses on room fingerprinting, a task involving the analysis of an audio recording to determine the specific volume and shape of the room in which it was captured. While it is relatively straightforward to determine the basic room parameters from the Room Impulse Responses (RIR), doing so from a speech signal is a cumbersome task. To address this challenge, we introduce a dual-encoder architecture that facilitates the estimation of room parameters directly from speech utterances. During pre-training, one encoder receives the RIR while the other processes the reverberant speech signal. A contrastive loss function is employed to embed the speech and the acoustic response jointly. In the fine-tuning stage, the specific classification task is trained. In the test phase, only the reverberant utterance is available, and its embedding is used for the task of room shape classification. The proposed scheme is extensively evaluated using simulated acoustic environments."
   ],
   "p1": 3280,
   "pn": 3284,
   "doi": "10.21437/Interspeech.2024-1951",
   "url": "interspeech_2024/bitterman24_interspeech.html"
  },
  "xu24h_interspeech": {
   "authors": [
    [
     "Tianyi",
     "Xu"
    ],
    [
     "Kaixun",
     "Huang"
    ],
    [
     "Pengcheng",
     "Guo"
    ],
    [
     "Yu",
     "Zhou"
    ],
    [
     "Longtao",
     "Huang"
    ],
    [
     "Hui",
     "Xue"
    ],
    [
     "Lei",
     "Xie"
    ]
   ],
   "title": "Towards Rehearsal-Free Multilingual ASR: A LoRA-based Case Study on Whisper ",
   "original": "1953",
   "order": 522,
   "page_count": 5,
   "abstract": [
    "Pre-trained multilingual speech foundation models, like Whisper, have shown impressive performance across different languages. However, adapting these models to new or specific languages is computationally extensive and faces catastrophic forgetting problems. Addressing these issues, our study investigates strategies to enhance the model on new languages in the absence of original training data, while also preserving the established performance on the original languages. Specifically, we first compare various LoRA-based methods to find out their vulnerability to forgetting. To mitigate this issue, we propose to leverage the LoRA parameters from the original model for approximate orthogonal gradient descent on the new samples. Additionally, we also introduce a learnable rank coefficient to allocate trainable parameters for more efficient training. Our experiments with a Chinese Whisper model (for Uyghur and Tibetan) yield better results with a more compact parameter set."
   ],
   "p1": 2534,
   "pn": 2538,
   "doi": "10.21437/Interspeech.2024-1953",
   "url": "interspeech_2024/xu24h_interspeech.html"
  },
  "wang24ja_interspeech": {
   "authors": [
    [
     "Jincen",
     "Wang"
    ],
    [
     "Yan",
     "Zhao"
    ],
    [
     "Cheng",
     "Lu"
    ],
    [
     "Hailun",
     "Lian"
    ],
    [
     "Hongli",
     "Chang"
    ],
    [
     "Yuan",
     "Zong"
    ],
    [
     "Wenming",
     "Zheng"
    ]
   ],
   "title": "Confidence-aware Hypothesis Transfer Networks for Source-Free Cross-Corpus Speech Emotion Recognition",
   "original": "1956",
   "order": 219,
   "page_count": 5,
   "abstract": [
    "The goal of Source-free cross-corpus speech emotion recognition (SER) is to transfer emotion knowledge from source corpus to target one without access to source data. To address this challenge, we develop a novel method named Confidence-aware Hypothesis Transfer Network (CaHTN) including two modules. To be specific, the first module called hypothesis implicit transfer leverages the frozen source classifier (hypothesis) to force target samples to implicitly align the source hypothesis by information maximization. Besides, a bidirectional confident self-training module is designed to exploit not only the positive pseudo label information but also the negative ones for target feature extraction enhancement. To verify its effectiveness, we design twelve source-free cross-corpus SER tasks and conduct extensive experiments on CASIA, EmoDB, EMOVO and eNTERFACE. Experimental results indicate CaHTN obtains state-of-the-art performance in addressing source-free cross-corpus SER."
   ],
   "p1": 1050,
   "pn": 1054,
   "doi": "10.21437/Interspeech.2024-1956",
   "url": "interspeech_2024/wang24ja_interspeech.html"
  },
  "schwartz24_interspeech": {
   "authors": [
    [
     "Ofer",
     "Schwartz"
    ],
    [
     "Sharon",
     "Gannot"
    ]
   ],
   "title": "Efficient Joint Bemforming and Acoustic Echo Cancellation Structure for Conference Call Scenarios",
   "original": "1957",
   "order": 35,
   "page_count": 5,
   "abstract": [
    "We propose an efficient scheme for combining beamformer (BF) and acoustic echo cancellation (AEC). We focus on conference call scenarios characterized by stationary background noise and multiple speakers who alternate frequently. Furthermore, aiming at low-resource devices, a common strategy is to apply a single AEC at the output of the BF rather than applying multiple AECs to each microphone signal. The main drawback of such a structure is the frequent change of the echo path due to the BF adaptation. To circumvent this problem, it is proposed to apply a single-channel pre-filter to the far-end signal, encompassing the BF weights and the relative acoustic responses between the reference microphone and all other microphones w.r.t. the echo loudspeaker. As a result, the AEC block becomes indifferent to the changes in the BF weights. The proposed scheme is evaluated using real recordings and was found advantageous over standard combined AEC-BF schemes in terms of speed of convergence."
   ],
   "p1": 167,
   "pn": 171,
   "doi": "10.21437/Interspeech.2024-1957",
   "url": "interspeech_2024/schwartz24_interspeech.html"
  },
  "li24ma_interspeech": {
   "authors": [
    [
     "Zhu",
     "Li"
    ],
    [
     "Xiyuan",
     "Gao"
    ],
    [
     "Yuqing",
     "Zhang"
    ],
    [
     "Shekhar",
     "Nayak"
    ],
    [
     "Matt",
     "Coler"
    ]
   ],
   "title": "A Functional Trade-off between Prosodic and Semantic Cues in Conveying Sarcasm",
   "original": "1962",
   "order": 223,
   "page_count": 5,
   "abstract": [
    "This study investigates the acoustic features of sarcasm and disentangles the interplay between the propensity of an utterance being used sarcastically and the presence of prosodic cues signaling sarcasm. Using a dataset of sarcastic utterances compiled from television shows, we analyze the prosodic features within utterances and key phrases belonging to three distinct sarcasm categories (embedded, propositional, and illocutionary), which vary in the degree of semantic cues present, and compare them to neutral expressions. Results show that in phrases where the sarcastic meaning is salient from the semantics, the prosodic cues are less relevant than when the sarcastic meaning is not evident from the semantics, suggesting a trade-off between prosodic and semantic cues of sarcasm at the phrase level. These findings highlight a lessened reliance on prosodic modulation in semantically dense sarcastic expressions and a nuanced interaction that shapes the communication of sarcastic intent."
   ],
   "p1": 1070,
   "pn": 1074,
   "doi": "10.21437/Interspeech.2024-1962",
   "url": "interspeech_2024/li24ma_interspeech.html"
  },
  "papadimitriou24_interspeech": {
   "authors": [
    [
     "Katerina",
     "Papadimitriou"
    ],
    [
     "Gerasimos",
     "Potamianos"
    ]
   ],
   "title": "Multimodal Continuous Fingerspelling Recognition via Visual Alignment Learning",
   "original": "1966",
   "order": 186,
   "page_count": 5,
   "abstract": [
    "Continuous fingerspelling recognition from videos is paramount for real-time sign language (SL) interpretation, enhancing accessibility. Despite deep learning progress, challenges persist, especially in signer-independent (SI) scenarios, due to signing variability. To address these, we propose a novel bimodal approach that integrates appearance and skeletal information, focusing solely on the signing hand. Our system relies on two basic modules: (a) a 3D-CNN model capturing spatial features, while adapting to motion variations and (b) a modulated spatio-temporal graph convolutional network (ST-GCN) based on 3D joint-rotation parameterization for skeletal feature modeling. Both modalities are combined with a BiGRU encoder and CTC decoding. To further enhance representation capacity, we introduce an alignment mechanism relying on two auxiliary losses. Through ensemble fusion and language model integration, our method achieves superior performance across three SI fingerspelling datasets."
   ],
   "p1": 922,
   "pn": 926,
   "doi": "10.21437/Interspeech.2024-1966",
   "url": "interspeech_2024/papadimitriou24_interspeech.html"
  },
  "udupa24b_interspeech": {
   "authors": [
    [
     "Sathvik",
     "Udupa"
    ],
    [
     "Soumi",
     "Maiti"
    ],
    [
     "Prasanta Kumar",
     "Ghosh"
    ]
   ],
   "title": "IndicMOS: Multilingual MOS Prediction for 7 Indian languages",
   "original": "1967",
   "order": 554,
   "page_count": 5,
   "abstract": [
    "Subjective evaluation is the gold standard for the evaluation of speech in different tasks such as text-to-speech (TTS), and voice-cloning (VC). However, these evaluations can be costly, time-consuming, and not easily scalable. Therefore, to tackle these challenges, we propose IndicMOS, a multilingual MOS predictor for Indian languages. We train our models on ratings data from Indic TTS and TTS + VC Challenges. We assess open-source MOS predictors, train unsupervised MOS predictors and fine-tune Wav2Vec2-based pre-trained models. We further incorporate additional features to enhance performance. Additionally, we analyze zero-shot evaluation results for Indian languages, presenting mean squared error and correlation metrics. Achieving a Kendall Tau of 0.8095 (system level) and 0.7143 (utterance level) for TTS, and 0.5131 (system level) and 0.4292 (utterance level) for TTS + VC, we also release our best models as open-source."
   ],
   "p1": 2690,
   "pn": 2694,
   "doi": "10.21437/Interspeech.2024-1967",
   "url": "interspeech_2024/udupa24b_interspeech.html"
  },
  "zheng24c_interspeech": {
   "authors": [
    [
     "Xiuwen",
     "Zheng"
    ],
    [
     "Bornali",
     "Phukon"
    ],
    [
     "Mark",
     "Hasegawa-Johnson"
    ]
   ],
   "title": "Fine-Tuning Automatic Speech Recognition for People with Parkinson's: An Effective Strategy for Enhancing Speech Technology Accessibility",
   "original": "1969",
   "order": 512,
   "page_count": 5,
   "abstract": [
    "This paper enhances dysarthric and dysphonic speech recognition by fine-tuning pretrained automatic speech recognition (ASR) models on the 2023-10-05 data package of the Speech Accessibility Project (SAP), which contains the speech of 253 people with Parkinson's disease. Experiments tested methods that have been effective for Cerebral Palsy, including the use of speaker clustering and severity-dependent models, weighted fine-tuning, and multi-task learning.  Best results were obtained using a multi-task learning model, in which the ASR is trained to produce an estimate of the speaker's impairment severity as an auxiliary output. The resulting word error rates are considerably improved relative to a baseline model fine-tuned using only Librispeech data, with word error rate improvements of 37.62% and 26.97% compared to fine-tuning on 100h and 960h of LibriSpeech data, respectively."
   ],
   "p1": 2485,
   "pn": 2489,
   "doi": "10.21437/Interspeech.2024-1969",
   "url": "interspeech_2024/zheng24c_interspeech.html"
  },
  "birkholz24_interspeech": {
   "authors": [
    [
     "Peter",
     "Birkholz"
    ],
    [
     "Patrick",
     "Häsner"
    ]
   ],
   "title": "Measurement and simulation of pressure losses due to airflow in vocal tract models",
   "original": "1970",
   "order": 637,
   "page_count": 5,
   "abstract": [
    "We propose a unified model for viscous and kinetic energy losses in a discrete tube model of the vocal system including the glottis. In this model, a lossless Bernoulli flow is assumed at each transition between two tube sections if the downstream section has a smaller diameter than the upstream section, and otherwise the recovery of a fixed fraction of the dynamic pressure. For viscous losses, we propose a general equation according to which the pressure drop within a tube section is inversely proportional to a certain power of its cross-sectional area. The parameters of the model were adjusted to reproduce the results of measurements with physical replicas of the glottis and the vocal tract. The best agreement with the experimental data was achieved when 29% of the dynamic pressure were recovered at tube expansions, and when the viscous losses were proportional to the tube area to the power of -2.9. These results may improve articulatory speech synthesis.\n"
   ],
   "p1": 3105,
   "pn": 3109,
   "doi": "10.21437/Interspeech.2024-1970",
   "url": "interspeech_2024/birkholz24_interspeech.html"
  },
  "le24_interspeech": {
   "authors": [
    [
     "Khanh",
     "Le"
    ],
    [
     "Duc",
     "Chau"
    ]
   ],
   "title": "Improving Streaming Speech Recognition With Time-Shifted Contextual Attention And Dynamic Right Context Masking",
   "original": "1971",
   "order": 920,
   "page_count": 5,
   "abstract": [
    "Chunk-based inference stands out as a popular approach in developing real-time streaming speech recognition, valued for its simplicity and efficiency. However, because it restricts the model's focus to only the history and current chunk context, it may result in performance degradation in scenarios that demand consideration of future context. Addressing this, we propose a novel approach featuring Time-Shifted Contextual Attention (TSCA) and Dynamic Right Context (DRC) masking. Our method shows a relative word error rate reduction of 10 to 13.9\\% on the Librispeech dataset with the inclusion of in-context future information provided by TSCA. Moreover, we present a streaming automatic speech recognition pipeline that facilitates the integration of TSCA with minimal user-perceived latency, while also enabling batch processing capability, making it practical for various applications."
   ],
   "p1": 4478,
   "pn": 4482,
   "doi": "10.21437/Interspeech.2024-1971",
   "url": "interspeech_2024/le24_interspeech.html"
  },
  "hoang24b_interspeech": {
   "authors": [
    [
     "Vu",
     "Hoang"
    ],
    [
     "Viet Thanh",
     "Pham"
    ],
    [
     "Hoa Nguyen",
     "Xuan"
    ],
    [
     "Pham",
     "Nhi"
    ],
    [
     "Phuong",
     "Dat"
    ],
    [
     "Thi Thu Trang",
     "Nguyen"
    ]
   ],
   "title": "VSASV: a Vietnamese Dataset for Spoofing-Aware Speaker Verification",
   "original": "1972",
   "resource": "https://doi.org/10.5281/zenodo.12802021",
   "order": 882,
   "page_count": 5,
   "abstract": [
    "Recent research in improving speaker verification systems to detect spoofed speech has seen a concentrated focus on English language, while the performance of such systems in other languages remains unexplored. This paper introduces the VSASV dataset for Spoofing-Aware Speaker Verification (SASV) in Vietnamese language. The dataset comprises over 174,000 spoofed utterances and 164,000 authentic utterances from 1,382 speakers, which were generated with the latest spoofing techniques to encourage the development of SASV systems in this language. We also provide experimental results on the efficacy of the different state-of-the-art anti-spoofing systems on Vietnamese language."
   ],
   "p1": 4288,
   "pn": 4292,
   "doi": "10.21437/Interspeech.2024-1972",
   "url": "interspeech_2024/hoang24b_interspeech.html"
  },
  "tran24b_interspeech": {
   "authors": [
    [
     "Tuyen",
     "Tran"
    ],
    [
     "Khanh",
     "Le"
    ],
    [
     "Ngoc Dang",
     "Nguyen"
    ],
    [
     "Minh",
     "Vu"
    ],
    [
     "Huyen",
     "Ngo"
    ],
    [
     "Woomyoung",
     "Park"
    ],
    [
     "Thi Thu Trang",
     "Nguyen"
    ]
   ],
   "title": "VN-SLU: A Vietnamese Spoken Language Understanding Dataset",
   "original": "1976",
   "resource": "https://doi.org/10.5281/zenodo.12734274",
   "order": 276,
   "page_count": 5,
   "abstract": [
    "Spoken Language Understanding (SLU) is a crucial task in spoken language processing. Despite the availability of numerous English datasets for research, there is a scarcity of resources for low-resource languages like Vietnamese. This paper introduces VN-SLU, the first dataset explicitly designed for Vietnamese SLU. VN-SLU includes 17,321 utterances from 240 Vietnamese speakers, obtained through novel crowd-sourcing methods. We propose a web tool for scenario generation and label validation, ensuring dataset quality and diversity. This tool prompts participants to confirm intents and slot values in smart home and virtual assistant dialogues, ensuring precise alignment. Experimental results highlight the challenging nature of the chosen test set sampling strategy in intent accuracy, SLU-F1, and utterance accuracy. Additionally, we explore the integration of pitch information into the Vietnamese SLU system. Results show improved performance compared to the baseline model."
   ],
   "p1": 1335,
   "pn": 1339,
   "doi": "10.21437/Interspeech.2024-1976",
   "url": "interspeech_2024/tran24b_interspeech.html"
  },
  "yadav24b_interspeech": {
   "authors": [
    [
     "Hemant",
     "Yadav"
    ],
    [
     "Sunayana",
     "Sitaram"
    ],
    [
     "Rajiv Ratn",
     "Shah"
    ]
   ],
   "title": "MS-HuBERT: Mitigating Pre-training and Inference Mismatch in Masked Language Modelling methods for learning Speech Representations",
   "original": "1978",
   "order": 1035,
   "page_count": 5,
   "abstract": [
    "In recent years, self-supervised pre-training methods have gained significant traction in learning high-level information from raw speech. Among these methods, HuBERT has demonstrated SOTA performance in automatic speech recognition (ASR). However, HuBERT's performance lags behind data2vec due to disparities in pre-training strategies. In this paper, we propose (i) a Swap method to address pre-training and inference mismatch observed in HuBERT and (ii) incorporates Multicluster masked prediction loss for more effective utilization of the models capacity. The resulting method is, MS-HuBERT, an end-to-end self-supervised pre-training method for learning robust speech representations. It beats vanilla HuBERT on the ASR Librispeech benchmark on average by a 5% margin when evaluated on different finetuning splits. Additionally, we demonstrate that the learned embeddings obtained during pre-training encode essential information for improving performance of content based tasks such as ASR."
   ],
   "p1": 5053,
   "pn": 5057,
   "doi": "10.21437/Interspeech.2024-1978",
   "url": "interspeech_2024/yadav24b_interspeech.html"
  },
  "nilsson24_interspeech": {
   "authors": [
    [
     "Mattias",
     "Nilsson"
    ],
    [
     "Riccardo",
     "Miccini"
    ],
    [
     "Clement",
     "Laroche"
    ],
    [
     "Tobias",
     "Piechowiak"
    ],
    [
     "Friedemann",
     "Zenke"
    ]
   ],
   "title": "Resource-Efficient Speech Quality Prediction through Quantization Aware Training and Binary Activation Maps",
   "original": "1979",
   "order": 611,
   "page_count": 5,
   "abstract": [
    "As speech processing systems in mobile and edge devices become more commonplace, the demand for unintrusive speech quality monitoring increases. Deep learning methods provide high-quality estimates of objective and subjective speech quality metrics. However, their significant computational requirements are often prohibitive on resource-constrained devices. To address this issue, we investigated binary activation maps (BAMs) for speech quality prediction on a convolutional architecture based on DNSMOS. We show that the binary activation model with quantization aware training matches the predictive performance of the baseline model. It further allows using other compression techniques. Combined with 8-bit weight quantization, our approach results in a 25-fold memory reduction during inference, while replacing almost all dot products with summations. Our findings show a path toward substantial resource savings by supporting mixed-precision binary multiplication in hard- and software."
   ],
   "p1": 2975,
   "pn": 2979,
   "doi": "10.21437/Interspeech.2024-1979",
   "url": "interspeech_2024/nilsson24_interspeech.html"
  },
  "sigurgeirsson24_interspeech": {
   "authors": [
    [
     "Atli",
     "Sigurgeirsson"
    ],
    [
     "Eddie L.",
     "Ungless"
    ]
   ],
   "title": "Just Because We Camp, Doesn't Mean We Should: The Ethics of Modelling Queer Voices.",
   "original": "1982",
   "order": 626,
   "page_count": 5,
   "abstract": [
    "Modern voice cloning models claim to be able to capture a diverse range of voices. We test the ability of a typical pipeline to capture the style known colloquially as “gay voice” and notice a homogenisation effect: synthesised speech is rated as sounding significantly “less gay” (by LGBTQ+ participants) than its corresponding ground-truth for speakers with “gay voice”, but ratings actually increase for control speakers. Loss of “gay voice” has implications for accessibility. We also find that for speakers with “gay voice”, loss of “gay voice” corresponds to lower similarity ratings. However, we caution that improving the ability of such models to synthesise “gay voice” comes with a great number of risks. We use this pipeline as a starting point for a discussion on the ethics of modelling queer voices more broadly. Collecting “clean” queer data has safety and fairness ramifications, and the resulting technology may cause harms from mockery to death."
   ],
   "p1": 3050,
   "pn": 3054,
   "doi": "10.21437/Interspeech.2024-1982",
   "url": "interspeech_2024/sigurgeirsson24_interspeech.html"
  },
  "ibrahim24_interspeech": {
   "authors": [
    [
     "Adham",
     "Ibrahim"
    ],
    [
     "Shady",
     "Shehata"
    ],
    [
     "Ajinkya",
     "Kulkarni"
    ],
    [
     "Mukhtar",
     "Mohamed"
    ],
    [
     "Muhammad",
     "Abdul-Mageed"
    ]
   ],
   "title": "What Does it Take to Generalize SER Model Across Datasets? A Comprehensive Benchmark",
   "original": "1983",
   "order": 327,
   "page_count": 5,
   "abstract": [
    "Speech emotion recognition (SER) is essential for enhancing human-computer interaction in speech-based applications. Despite improvements in specific emotional datasets, there is still a research gap in SER's capability to generalize across real-world situations. In this paper, we investigate approaches to generalize the SER system across different emotion datasets. In particular, incorporate 11 emotional speech datasets and illustrate a comprehensive benchmark on the SER task. We also address the challenge of imbalanced data distribution using oversampling methods when combining SER datasets for training. Furthermore, we explore various evaluation protocols for adeptness in the generalization of SER. Building on this, we explore the potential of Whisper for SER, emphasizing the importance of thorough evaluation. Our approach is designed to advance SER technology by integrating speaker-independent methods. "
   ],
   "p1": 1590,
   "pn": 1594,
   "doi": "10.21437/Interspeech.2024-1983",
   "url": "interspeech_2024/ibrahim24_interspeech.html"
  },
  "blatt24_interspeech": {
   "authors": [
    [
     "Alexander",
     "Blatt"
    ],
    [
     "Aravind",
     "Krishnan"
    ],
    [
     "Dietrich",
     "Klakow"
    ]
   ],
   "title": "Joint vs Sequential Speaker-Role Detection and Automatic Speech Recognition for Air-traffic Control",
   "original": "1987",
   "order": 775,
   "page_count": 5,
   "abstract": [
    "Utilizing air-traffic control (ATC) data for downstream natural-language processing tasks requires preprocessing steps. Key steps are the transcription of the data via automatic speech recognition (ASR) and speaker diarization, respectively speaker role detection (SRD) to divide the transcripts into pilot and air-traffic controller (ATCO) transcripts. While traditional approaches take on these tasks separately, we propose a transformer-based joint ASR-SRD system that solves both tasks jointly while relying on a standard ASR architecture. We compare this joint system against two cascaded approaches for ASR and SRD on multiple ATC datasets. Our study shows in which cases our joint system can outperform the two traditional approaches and in which cases the other architectures are preferable. We additionally evaluate how acoustic and lexical differences influence all architectures and show how to overcome them for our joint architecture."
   ],
   "p1": 3759,
   "pn": 3763,
   "doi": "10.21437/Interspeech.2024-1987",
   "url": "interspeech_2024/blatt24_interspeech.html"
  },
  "li24na_interspeech": {
   "authors": [
    [
     "Weiqin",
     "Li"
    ],
    [
     "Peiji",
     "Yang"
    ],
    [
     "Yicheng",
     "Zhong"
    ],
    [
     "Yixuan",
     "Zhou"
    ],
    [
     "Zhisheng",
     "Wang"
    ],
    [
     "Zhiyong",
     "Wu"
    ],
    [
     "Xixin",
     "Wu"
    ],
    [
     "Helen",
     "Meng"
    ]
   ],
   "title": "Spontaneous Style Text-to-Speech Synthesis with Controllable Spontaneous Behaviors Based on Language Models",
   "original": "1989",
   "order": 366,
   "page_count": 5,
   "abstract": [
    "Spontaneous style speech synthesis, which aims to generate human-like speech, often encounters challenges due to the scarcity of high-quality data and limitations in model capabilities. Recent language model-based TTS systems can be trained on large, diverse, and low-quality speech datasets, resulting in highly natural synthesized speech. However, they are limited by the difficulty of simulating various spontaneous behaviors and capturing prosody variations in spontaneous speech. In this paper, we propose a novel spontaneous speech synthesis system based on language models. We systematically categorize and uniformly model diverse spontaneous behaviors. Moreover, fine-grained prosody modeling is introduced to enhance the model's ability to capture subtle prosody variations in spontaneous speech. Experimental results show that our proposed method significantly outperforms the baseline methods in terms of prosody naturalness and spontaneous behavior naturalness."
   ],
   "p1": 1785,
   "pn": 1789,
   "doi": "10.21437/Interspeech.2024-1989",
   "url": "interspeech_2024/li24na_interspeech.html"
  },
  "raybarman24_interspeech": {
   "authors": [
    [
     "Sneha",
     "Ray Barman"
    ],
    [
     "Shakuntala",
     "Mahanta"
    ],
    [
     "Neeraj Kumar",
     "Sharma"
    ]
   ],
   "title": "Deciphering Assamese Vowel Harmony with Featural InfoWaveGAN",
   "original": "1990",
   "resource": "https://doi.org/10.5281/zenodo.12798919",
   "order": 311,
   "page_count": 5,
   "abstract": [
    "Traditional approaches for understanding phonological learning have predominantly relied on curated text data. Although insightful, such approaches limit the knowledge captured in textual representations of the spoken language. To overcome this limitation, we investigate the potential of the Featural InfoWaveGAN model to learn iterative long-distance vowel harmony using raw speech data. We focus on Assamese, a language known for its phonologically regressive and word-bound vowel harmony. We demonstrate that the model is adept at grasping the intricacies of Assamese phonotactics, particularly iterative long-distance harmony with regressive directionality. It also produced non-iterative illicit forms resembling speech errors during human language acquisition. Our statistical analysis reveals a preference for a specific [+high,+ATR] vowel as a trigger across novel items, indicative of feature learning. More data and control could improve model proficiency, contrasting the universality of learning."
   ],
   "p1": 1510,
   "pn": 1514,
   "doi": "10.21437/Interspeech.2024-1990",
   "url": "interspeech_2024/raybarman24_interspeech.html"
  },
  "phukan24_interspeech": {
   "authors": [
    [
     "Orchid Chetia",
     "Phukan"
    ],
    [
     "Priyabrata",
     "Mallick"
    ],
    [
     "Swarup Ranjan",
     "Behera"
    ],
    [
     "Aalekhya Satya",
     "Narayani"
    ],
    [
     "Arun Balaji",
     "Buduru"
    ],
    [
     "Rajesh",
     "Sharma"
    ]
   ],
   "title": "Towards Multilingual Audio-Visual Question Answering",
   "original": "1993",
   "order": 831,
   "page_count": 5,
   "abstract": [
    "In this paper, we work towards extending Audio-Visual Question Answering (AVQA) to multilingual settings. Existing AVQA research has predominantly revolved around English and replicating it for addressing AVQA in other languages requires a substantial allocation of resources. As a scalable solution, we leverage machine translation and present two multilingual AVQA datasets for eight languages created from existing benchmark AVQA datasets. This prevents extra human annotation efforts of collecting questions and answers manually. To this end, we propose, MERA framework, by leveraging state-of-the-art (SOTA) video, audio, and textual foundation models for AVQA in multiple languages. We introduce a suite of models namely MERA-L, MERA-C, MERA-T with varied model architectures to benchmark the proposed datasets. We believe our work will open new research directions and act as a reference benchmark for future works in multilingual AVQA."
   ],
   "p1": 4039,
   "pn": 4043,
   "doi": "10.21437/Interspeech.2024-1993",
   "url": "interspeech_2024/phukan24_interspeech.html"
  },
  "wu24n_interspeech": {
   "authors": [
    [
     "Hongchen",
     "Wu"
    ],
    [
     "Jiwon",
     "Yun"
    ]
   ],
   "title": "Influences of Morphosyntax and Semantics on the Intonation of Mandarin Chinese Wh-indeterminates",
   "original": "2004",
   "resource": "https://doi.org/10.5281/zenodo.12771270",
   "order": 429,
   "page_count": 5,
   "abstract": [
    "This study utilized a speech production test to examine how different components of the language faculty interact, focusing on the impact of morphosyntax and semantics on the intonation of Mandarin Chinese wh-indeterminates. The analysis of the hour-long speech production data from 33 speakers indicates that both morphosyntax and semantics play a crucial role in shaping the intonation of wh-indeterminates. We found that wh-indeterminates in complex clause structures effectively utilize all prosodic features to convey interrogative meaning. Moreover, when a morphosyntactic marker suggests a preference for a specific interpretation of wh-indeterminates, there is a reduced prosodic contrast (although still significantly different) between wh-interrogatives and wh-indefinites, underlying an interference effect of morphosyntactic cues in speech production."
   ],
   "p1": 2070,
   "pn": 2074,
   "doi": "10.21437/Interspeech.2024-2004",
   "url": "interspeech_2024/wu24n_interspeech.html"
  },
  "yang24q_interspeech": {
   "authors": [
    [
     "Jinhyeok",
     "Yang"
    ],
    [
     "Junhyeok",
     "Lee"
    ],
    [
     "Hyeong-Seok",
     "Choi"
    ],
    [
     "Seunghoon",
     "Ji"
    ],
    [
     "Hyeongju",
     "Kim"
    ],
    [
     "Juheon",
     "Lee"
    ]
   ],
   "title": "DualSpeech: Enhancing Speaker-Fidelity and Text-Intelligibility Through Dual Classifier-Free Guidance",
   "original": "2005",
   "order": 909,
   "page_count": 5,
   "abstract": [
    "Text-to-Speech (TTS) models have advanced significantly, aiming to accurately replicate human speech's diversity, including unique speaker identities and linguistic nuances. Despite these advancements, achieving an optimal balance between speaker-fidelity and text-intelligibility remains a challenge, particularly when diverse control demands are considered. Addressing this, we introduce DualSpeech, a TTS model that integrates phoneme-level latent diffusion with dual classifier-free guidance. This approach enables exceptional control over speaker-fidelity and text-intelligibility. Experimental results demonstrate that by utilizing the sophisticated control, DualSpeech surpasses existing state-of-the-art TTS models in performance. Demos are available at https://bit.ly/48Ewoib."
   ],
   "p1": 4423,
   "pn": 4427,
   "doi": "10.21437/Interspeech.2024-2005",
   "url": "interspeech_2024/yang24q_interspeech.html"
  },
  "liu24m_interspeech": {
   "authors": [
    [
     "Tianchi",
     "Liu"
    ],
    [
     "Lin",
     "Zhang"
    ],
    [
     "Rohan Kumar",
     "Das"
    ],
    [
     "Yi",
     "Ma"
    ],
    [
     "Ruijie",
     "Tao"
    ],
    [
     "Haizhou",
     "Li"
    ]
   ],
   "title": "How Do Neural Spoofing Countermeasures Detect Partially Spoofed Audio?",
   "original": "2009",
   "order": 230,
   "page_count": 5,
   "abstract": [
    "Partially manipulating a sentence can greatly change its meaning. Recent work shows that countermeasures (CMs) trained on partially spoofed audio can effectively detect such spoofing. However, the current understanding of the decision-making process of CMs is limited. We utilize Grad-CAM and introduce a quantitative analysis metric to interpret CMs' decisions. We find that CMs prioritize the artifacts of transition regions created when concatenating bona fide and spoofed audio. This focus differs from that of CMs trained on fully spoofed audio, which concentrate on the pattern differences between bona fide and spoofed parts. Our further investigation explains the varying nature of  CMs' focus while making correct or incorrect predictions. These insights provide a basis for the design of CM models and the creation of datasets. Moreover, this work lays a foundation of interpretability in the field of partial spoofed audio detection that has not been well explored previously."
   ],
   "p1": 1105,
   "pn": 1109,
   "doi": "10.21437/Interspeech.2024-2009",
   "url": "interspeech_2024/liu24m_interspeech.html"
  },
  "pistor24_interspeech": {
   "authors": [
    [
     "Tillmann",
     "Pistor"
    ],
    [
     "Adrian",
     "Leemann"
    ]
   ],
   "title": "Echoes of Implicit Bias Exploring Aesthetics and Social Meanings of Swiss German Dialect Features",
   "original": "2013",
   "order": 91,
   "page_count": 5,
   "abstract": [
    "This study investigates the phonaesthetics and perceptual dynamics of Swiss German dialects, focusing on how particular sound features influence subjective assessments and, in doing so, contribute to dialect stereotypes. By examining 24 linguistic features of Bern and Zurich German, including nine vowels and 15 consonants in single-word utterances, we aim to fill a research gap that has been previously overlooked, despite suggestions of importance. In an online perception study, we gathered evaluations from three distinct groups of raters (N = 46) from Bern, Zurich, and Hessen, Germany, across six categories from aesthetic dimensions to stereotypical dialect attributions. The findings reveal that rater origin determines the levels of importance on evaluation categories and that certain linguistic features can be identified that are closely linked with specific perceptions (e.g., stupid or arrogant), which may foster negative biases against dialect speakers."
   ],
   "p1": 447,
   "pn": 451,
   "doi": "10.21437/Interspeech.2024-2013",
   "url": "interspeech_2024/pistor24_interspeech.html"
  },
  "chaudhary24b_interspeech": {
   "authors": [
    [
     "Aryan",
     "Chaudhary"
    ],
    [
     "Vinayak",
     "Abrol"
    ]
   ],
   "title": "QGAN: Low Footprint Quaternion Neural Vocoder for Speech Synthesis",
   "original": "2014",
   "order": 798,
   "page_count": 5,
   "abstract": [
    "Neural vocoders have recently evolved to achieve superior synthesis quality by leveraging advancements in methods like diffusion, flow, transformers, GANs, etc. However, such models have grown vastly in terms of space and time complexity, leading to challenges in the deployment of speech synthesis systems in resource-constraint scenarios. To address this, we present a novel low-footprint Quaternion Generative Adversarial Network (QGAN) for efficient and high-fidelity speech synthesis without compromising on the audio quality. QGAN achieves structural model compression over conventional GAN with quaternion convolutions in the generator and a modified multi-scale/period discriminator. To ensure model stability, we also propose weight-normalization in the quaternion domain. We show the effectiveness of QGAN with large-scale experiments on English and Hindi language datasets. In addition, using loss landscape visualization, we provide an analysis of the learning behaviour of the proposed QGAN model."
   ],
   "p1": 3874,
   "pn": 3878,
   "doi": "10.21437/Interspeech.2024-2014",
   "url": "interspeech_2024/chaudhary24b_interspeech.html"
  },
  "casanova24_interspeech": {
   "authors": [
    [
     "Edresson",
     "Casanova"
    ],
    [
     "Kelly",
     "Davis"
    ],
    [
     "Eren",
     "Gölge"
    ],
    [
     "Görkem",
     "Göknar"
    ],
    [
     "Iulian",
     "Gulea"
    ],
    [
     "Logan",
     "Hart"
    ],
    [
     "Aya",
     "Aljafari"
    ],
    [
     "Joshua",
     "Meyer"
    ],
    [
     "Reuben",
     "Morais"
    ],
    [
     "Samuel",
     "Olayemi"
    ],
    [
     "Julian",
     "Weber"
    ]
   ],
   "title": "XTTS: a Massively Multilingual Zero-Shot Text-to-Speech Model",
   "original": "2016",
   "order": 1020,
   "page_count": 5,
   "abstract": [
    "Most Zero-shot Multi-speaker TTS (ZS-TTS) systems support only a single language. Although models like YourTTS, VALL-E X, Mega-TTS 2, and Voicebox explored  Multilingual ZS-TTS they are limited to just a few high/medium resource languages, limiting the applications of these models in most of the low/medium resource languages. In this paper, we aim to alleviate this issue by proposing and making publicly available the XTTS system. Our method builds upon the Tortoise model and adds several novel modifications to enable multilingual training, improve voice cloning, and enable faster training and inference. XTTS was trained in 16 languages and achieved state-of-the-art (SOTA) results in most of them."
   ],
   "p1": 4978,
   "pn": 4982,
   "doi": "10.21437/Interspeech.2024-2016",
   "url": "interspeech_2024/casanova24_interspeech.html"
  },
  "chen24v_interspeech": {
   "authors": [
    [
     "Nan",
     "Chen"
    ],
    [
     "Yonghe",
     "Wang"
    ],
    [
     "Feilong",
     "Bao"
    ]
   ],
   "title": "Sign Value Constraint Decomposition for Efficient 1-Bit Quantization of Speech Translation Tasks",
   "original": "2022",
   "order": 170,
   "page_count": 5,
   "abstract": [
    "Speech-to-text translation is vital in converting speech input to text output in different languages. While combining speech and machine translation pre-trained models enhances translation quality, it also escalates the number of parameters, resulting in substantial hardware costs for model training and deployment. We propose a 1-bit quantized model based on Sign Value Constraint Decomposition (SVCD) for linear layers to address this challenge. SVCD approximates the weight matrix of the linear layer as a sign matrix and two trainable vectors, preserving higher information capacity at a minor space cost. Additionally, we utilize knowledge distillation to transfer the capability of the original fine-tuned model to the quantized model. The experimental results demonstrate the critical importance of the decoder's attention module in the performance of the quantized speech translation model. Our code is available at https://github.com/myaxxxxx/onebit-st."
   ],
   "p1": 842,
   "pn": 846,
   "doi": "10.21437/Interspeech.2024-2022",
   "url": "interspeech_2024/chen24v_interspeech.html"
  },
  "kulshreshtha24_interspeech": {
   "authors": [
    [
     "Devang",
     "Kulshreshtha"
    ],
    [
     "Nikolaos",
     "Pappas"
    ],
    [
     "Brady",
     "Houston"
    ],
    [
     "Saket",
     "Dingliwal"
    ],
    [
     "Srikanth",
     "Ronanki"
    ]
   ],
   "title": "Sequential Editing for Lifelong Training of Speech Recognition Models",
   "original": "2027",
   "order": 807,
   "page_count": 5,
   "abstract": [
    "Automatic Speech Recognition (ASR) traditionally assumes known domains, but adding data from a new domain raises concerns about computational inefficiencies linked to retraining models on both existing and new domains. Fine-tuning solely on new domain risks Catastrophic Forgetting (CF). To address this, Lifelong Learning (LLL) algorithms have been proposed for ASR. Prior research has explored techniques such as Elastic Weight Consolidation, Knowledge Distillation, and Replay, all of which necessitate either additional parameters or access to prior domain data. We propose Sequential Model Editing as a novel method to continually learn new domains in ASR systems. Different than previous methods, our approach does not necessitate access to prior datasets or the introduction of extra parameters. Our study demonstrates up to 15% Word Error Rate Reduction (WERR) over fine-tuning baseline, and superior efficiency over other LLL techniques on CommonVoice English multi-accent dataset."
   ],
   "p1": 3919,
   "pn": 3923,
   "doi": "10.21437/Interspeech.2024-2027",
   "url": "interspeech_2024/kulshreshtha24_interspeech.html"
  },
  "vitale24_interspeech": {
   "authors": [
    [
     "Vincenzo Norman",
     "Vitale"
    ],
    [
     "Loredana",
     "Schettino"
    ],
    [
     "Francesco",
     "Cutugno"
    ]
   ],
   "title": "Rich speech signal: exploring and exploiting  end-to-end automatic speech recognizers’ ability to model hesitation phenomena",
   "original": "2029",
   "order": 46,
   "page_count": 5,
   "abstract": [
    "Modern automatic speech recognition systems can achieve remarkable performances. However, they usually neglect speech characteristic phenomena such as fillers (<eeh> <ehm>) or segmental prolongations (the<ee>) which are still only considered as disrupting objects to be detected and removed, despite their acknowledged regularity and procedural value. This study investigates the ability of state-of-the-art systems based on end-to-end models (E2E-ASRs) to model distinctive features of hesitation phenomena. Two types of pre-trained systems with the same Conformer-based encoding architecture but different decoders are evaluated: the Connectionist Temporal Classification (CTC) decoder and a Transducer decoder. E2E-ASRs ability to model the acoustic information tied to such phenomena can be exploited rather than disregarded as a noise source, which would not only improve transcription and support linguistic annotation processes, but also deepen our understanding of the systems’ working."
   ],
   "p1": 222,
   "pn": 226,
   "doi": "10.21437/Interspeech.2024-2029",
   "url": "interspeech_2024/vitale24_interspeech.html"
  },
  "favaro24_interspeech": {
   "authors": [
    [
     "Anna",
     "Favaro"
    ],
    [
     "Tianyu",
     "Cao"
    ],
    [
     "Najim",
     "Dehak"
    ],
    [
     "Laureano",
     "Moro-Velazquez"
    ]
   ],
   "title": "Leveraging Universal Speech Representations for Detecting and Assessing the Severity of Mild Cognitive Impairment Across Languages",
   "original": "2030",
   "order": 196,
   "page_count": 5,
   "abstract": [
    "This study examines the suitability of language-agnostic features for automatically detecting Mild Cognitive Impairment (MCI) and predicting Mini-Mental State Examination (MMSE) scores in a multilingual framework. We explored two methods for feature extraction: traditional feature engineering and pre-trained feature representation. We developed our models using the Interspeech 2024 Taukadial challenge data set, containing audios from subjects with MCI and controls in Chinese and English. Our top ensemble model achieved 75% accuracy in MCI detection and an RMSE of 2.44 in MMSE prediction in the testing set. Our results reveal the complementary nature of acoustic and linguistic representations and the existence of universal features that can be used cross-lingually. However, a statistical analysis of interpretable features did not show any shared speech patterns between the two languages, which can be attributed to differences in disease severity between the two cohorts of participants."
   ],
   "p1": 972,
   "pn": 976,
   "doi": "10.21437/Interspeech.2024-2030",
   "url": "interspeech_2024/favaro24_interspeech.html"
  },
  "sasindran24_interspeech": {
   "authors": [
    [
     "Zitha",
     "Sasindran"
    ],
    [
     "Harsha",
     "Yelchuri"
    ],
    [
     "T. V.",
     "Prabhakar"
    ]
   ],
   "title": "SeMaScore: A new evaluation metric for automatic speech recognition tasks",
   "original": "2033",
   "order": 936,
   "page_count": 5,
   "abstract": [
    "In this study, we present SeMaScore, generated using a segment-wise mapping and scoring algorithm that serves as an evaluation metric for automatic speech recognition tasks. SeMaScore leverages both the error rate and a more robust similarity score. We show that our algorithm’s score generation improves upon the state-of-the-art BERTScore. Our experimental results show that SeMaScore corresponds well with expert human assessments, signal-to-noise ratio levels, and other natural language metrics. We outperform BERTScore by 41x in metric computation speed. Overall, we demonstrate that SeMaScore serves as a more dependable evaluation metric, particularly in real-world situations involving atypical speech patterns."
   ],
   "p1": 4558,
   "pn": 4562,
   "doi": "10.21437/Interspeech.2024-2033",
   "url": "interspeech_2024/sasindran24_interspeech.html"
  },
  "kc24_interspeech": {
   "authors": [
    [
     "Bhasi",
     "K. C."
    ],
    [
     "Rajeev",
     "Rajan"
    ],
    [
     "Noumida",
     "A"
    ]
   ],
   "title": "Attention-augmented X-vectors for the Evaluation of Mimicked Speech Using Sparse Autoencoder-LSTM framework",
   "original": "2036",
   "order": 784,
   "page_count": 5,
   "abstract": [
    "This paper evaluates the quality of mimicked speech by computing the speaker embeddings. We propose an attention-augmented encoded speaker embedding for mimicking speaker evaluation. X-vector embeddings extracted from the spectral features are passed through a 1-D convolutional neural network (CNN) with an attention module. The resulting output is  fed into a sparse autoencoder. Later, the encoded vector is fed to a long short-term memory (LSTM) -based scoring mechanism. The best mimicking artist is initially identified by a perception test. Later, the we investigate whether the LSTM-based self-attention model predicts the same artist. When the model identifies the mean opinion score(MOS)-identified artist with the highest probability (rank-1), we assume that one hit occurs. The performance evaluation is carried out with a mimicry dataset using top-X criteria. The experiment demonstrates efficacy in the proposed vector representation in competency evaluation of voice mimicking."
   ],
   "p1": 3804,
   "pn": 3808,
   "doi": "10.21437/Interspeech.2024-2036",
   "url": "interspeech_2024/kc24_interspeech.html"
  },
  "gudmundsson24_interspeech": {
   "authors": [
    [
     "Vidar Freyr",
     "Gudmundsson"
    ],
    [
     "Keve Márton",
     "Gönczi"
    ],
    [
     "Malin",
     "Svensson Lundmark"
    ],
    [
     "Donna",
     "Erickson"
    ],
    [
     "Oliver",
     "Niebuhr"
    ]
   ],
   "title": "The MARRYS helmet: A new device for researching and training “jaw dancing”",
   "original": "2039",
   "order": 289,
   "page_count": 5,
   "abstract": [
    "The paper introduces a new device for analyzing, teaching, and training jaw movements: the MARRYS helmet. We outline the motivation for the development of the helmet, describe its key advantages and features relative to those of the Electromagnetic Articulograph (EMA) and illustrate by means of selected study portraits the possible uses of the MARRYS helmet in the various fields of the empirical and applied speech sciences."
   ],
   "p1": 1400,
   "pn": 1404,
   "doi": "10.21437/Interspeech.2024-2039",
   "url": "interspeech_2024/gudmundsson24_interspeech.html"
  },
  "jakhar24_interspeech": {
   "authors": [
    [
     "Nikhil",
     "Jakhar"
    ],
    [
     "Sudhanshu",
     "Srivastava"
    ],
    [
     "Arun",
     "Baby"
    ]
   ],
   "title": "A Unified Approach to Multilingual Automatic Speech Recognition with Improved Language Identification for Indic Languages",
   "original": "2043",
   "order": 813,
   "page_count": 5,
   "abstract": [
    "Multilingual Automatic Speech Recognition (ASR) presents several difficulties, especially when multiple languages are being spoken in the same audio. Traditional multilingual ASR systems often rely on low-resource Indic language data and language-specific models, which limits their scalability and efficiency. Creating individual models is difficult due to the lack of Indic language data, while the need for an accurate language identification (LID) model further affects the downstream task. Our method integrates LID and multilingual ASR in a unified framework, leveraging their symbiotic relationship to overcome limitations. This study presents an approach to multilingual ASR incorporating LID capabilities using Whisper as the baseline architecture. Experimental results on benchmark datasets demonstrate our method’s effectiveness, which shows an absolute 19.1% improvement in Word Error Rate (WER) while enhancing LID performance by 6% in terms of Diarization Error Rate (DER)."
   ],
   "p1": 3949,
   "pn": 3953,
   "doi": "10.21437/Interspeech.2024-2043",
   "url": "interspeech_2024/jakhar24_interspeech.html"
  },
  "english24_interspeech": {
   "authors": [
    [
     "Patrick Cormac",
     "English"
    ],
    [
     "John D.",
     "Kelleher"
    ],
    [
     "Julie",
     "Carson-Berndsen"
    ]
   ],
   "title": "Searching for Structure: Appraising the Organisation of Speech Features in wav2vec 2.0 Embeddings",
   "original": "2047",
   "order": 947,
   "page_count": 5,
   "abstract": [
    "Recent advancements in speech recognition have been driven by large transformer models trained on extensive unlabelled speech corpora. These models generate speech representations that potentially encapsulate key speech features, yet the organisation of these features within the model's embedding space and their alignment with phonetic and phonological theories remains unclear. This paper aims to bridge this gap by applying probing methods to explore the structure of phonetic information within embeddings, thereby uncovering linguistically significant relationships within the latent representations. We introduce a novel approach that probes the speech embeddings for independent features and then applies association rule mining to identify relationships and organisational structure within the data. Our research seeks to enhance the understanding of the speech embeddings of transformer models, ultimately contributing to the explainability of these systems."
   ],
   "p1": 4613,
   "pn": 4617,
   "doi": "10.21437/Interspeech.2024-2047",
   "url": "interspeech_2024/english24_interspeech.html"
  },
  "deoliveira24_interspeech": {
   "authors": [
    [
     "Danilo",
     "de Oliveira"
    ],
    [
     "Simon",
     "Welker"
    ],
    [
     "Julius",
     "Richter"
    ],
    [
     "Timo",
     "Gerkmann"
    ]
   ],
   "title": "The PESQetarian: On the Relevance of Goodhart's Law for Speech Enhancement",
   "original": "2051",
   "order": 794,
   "page_count": 5,
   "abstract": [
    "To obtain improved speech enhancement models, researchers often focus on increasing performance according to specific instrumental metrics. However, when the same metric is used in a loss function to optimize models, it may be detrimental to aspects that the given metric does not see. The goal of this paper is to illustrate the risk of overfitting a speech enhancement model to the metric used for evaluation. For this, we introduce enhancement models that exploit the widely used PESQ measure. Our “PESQetarian” model achieves 3.82 PESQ on VB-DMD while scoring very poorly in a listening experiment. While the obtained PESQ value of 3.82 would imply “state-of-the-art” PESQ-performance on the VB-DMD benchmark, our examples show that when optimizing w.r.t. a metric, an isolated evaluation on the same metric may be misleading. Instead, other metrics should be included in the evaluation and the resulting performance predictions should be confirmed by listening."
   ],
   "p1": 3854,
   "pn": 3858,
   "doi": "10.21437/Interspeech.2024-2051",
   "url": "interspeech_2024/deoliveira24_interspeech.html"
  },
  "kelley24_interspeech": {
   "authors": [
    [
     "Liam",
     "Kelley"
    ],
    [
     "Diego",
     "Di Carlo"
    ],
    [
     "Aditya Arie",
     "Nugraha"
    ],
    [
     "Mathieu",
     "Fontaine"
    ],
    [
     "Yoshiaki",
     "Bando"
    ],
    [
     "Kazuyoshi",
     "Yoshii"
    ]
   ],
   "title": "RIR-in-a-Box: Estimating Room Acoustics from 3D Mesh Data through Shoebox Approximation",
   "original": "2053",
   "order": 667,
   "page_count": 5,
   "abstract": [
    "This paper describes a method for estimating the room impulse response (RIR) for a microphone and a sound source located at arbitrary positions from the 3D mesh data of the room. Simulating realistic RIRs with pure physics-driven methods often fails the balance between physical consistency and computational efficiency, hindering application to real time speech processing. Alternatively, one can use MESH2IR, a fast black-box estimator that consists of an encoder extracting latent code from mesh data with a graph convolutional network (GCN) and a decoder generating the RIR from the latent code. Combining these two approaches, we propose a fast yet physically coherent estimator with interpretable latent code based on differentiable digital signal processing (DDSP). Specifically, the encoder estimates a virtual shoebox room scene that acoustically approximates the real scene, accelerating physical simulation with the differentiable image-source model in the decoder. Our experiments showed that our method outperformed MESH2IR for real mesh data obtained with the depth scanner of Microsoft HoloLens 2, and can provide correct spatial consistency for binaural RIRs."
   ],
   "p1": 3255,
   "pn": 3259,
   "doi": "10.21437/Interspeech.2024-2053",
   "url": "interspeech_2024/kelley24_interspeech.html"
  },
  "maurya24_interspeech": {
   "authors": [
    [
     "Himanshu",
     "Maurya"
    ],
    [
     "Atli",
     "Sigurgeirsson"
    ]
   ],
   "title": "A Human-in-the-Loop Approach to Improving Cross-Text Prosody Transfer",
   "original": "2055",
   "order": 474,
   "page_count": 5,
   "abstract": [
    "Text-To-Speech (TTS) prosody transfer models can generate varied prosodic renditions, for the same text, by conditioning on a reference utterance. These models are trained with a reference that is identical to the target utterance. But when the reference utterance differs from the target text, as in cross-text prosody transfer, these models struggle to separate prosody from text, resulting in reduced perceived naturalness. To address this, we propose a Human-in-the-Loop (HitL) approach. HitL users adjust salient correlates of prosody to make the prosody more appropriate for the target text, while maintaining the overall reference prosodic effect. Human adjusted renditions maintain the reference prosody while being rated as more appropriate for the target text 57.8% of the time. Our analysis suggests that limited user effort suffices for these improvements, and that closeness in the latent reference space is not a reliable prosodic similarity metric for the cross-text condition."
   ],
   "p1": 2295,
   "pn": 2299,
   "doi": "10.21437/Interspeech.2024-2055",
   "url": "interspeech_2024/maurya24_interspeech.html"
  },
  "stafylakis24_interspeech": {
   "authors": [
    [
     "Themos",
     "Stafylakis"
    ],
    [
     "Anna",
     "Silnova"
    ],
    [
     "Johan",
     "Rohdin"
    ],
    [
     "Oldřich",
     "Plchot"
    ],
    [
     "Lukáš",
     "Burget"
    ]
   ],
   "title": "Challenging margin-based speaker embedding extractors by using the variational information bottleneck",
   "original": "2058",
   "order": 660,
   "page_count": 5,
   "abstract": [
    "Speaker embedding extractors are typically trained using a classification loss over the training speakers. During the last few years, the standard softmax/cross-entropy loss has been replaced by the margin-based losses, yielding significant improvements in speaker recognition accuracy. Motivated by the fact that the margin merely reduces the logit of the target speaker during training, we consider a probabilistic framework that has a similar effect. The variational information bottleneck provides a principled mechanism for making deterministic nodes stochastic, resulting in an implicit reduction of the posterior of the target speaker. We experiment with a wide range of speaker recognition benchmarks and scoring methods and report competitive results to those obtained with the state-of-the-art Additive Angular Margin loss."
   ],
   "p1": 3220,
   "pn": 3224,
   "doi": "10.21437/Interspeech.2024-2058",
   "url": "interspeech_2024/stafylakis24_interspeech.html"
  },
  "loiacono24_interspeech": {
   "authors": [
    [
     "Federico",
     "Lo Iacono"
    ],
    [
     "Valentina",
     "Colonna"
    ],
    [
     "Antonio",
     "Romano"
    ]
   ],
   "title": "Preservation, conservation and phonetic study of the voices of Italian poets: A study on the seven years of the VIP archive",
   "original": "2060",
   "order": 756,
   "page_count": 5,
   "abstract": [
    "Poetic speech, like other oral traditions, is a fragile cultural heritage that requires proper conservation to prevent dispersal. Therefore, since the early 20th century, there has been a growing global recognition of the need to handle such oral data appropriately. This awareness has led to the establishment of both digital and analog archives dedicated to preserving poetic readings. Noteworthy is Voices of Italian Poets (VIP), a prominent project focused on conserving and studying contemporary Italian poetic speech. This paper aims to outline the value of the VIP vocal archive by providing a descriptive analysis of its data, seven years after its creation. The study presents the structure and purposes of the online platform and highlights the large number and variety of recordings and poets involved in the project. The VIP-Platform results as a crucial hub for preserving culturally significant materials and a crucial case study for nurturing a shared cultural inheritance."
   ],
   "p1": 3664,
   "pn": 3668,
   "doi": "10.21437/Interspeech.2024-2060",
   "url": "interspeech_2024/loiacono24_interspeech.html"
  },
  "ng24_interspeech": {
   "authors": [
    [
     "Si-Ioi",
     "Ng"
    ],
    [
     "Lingfeng",
     "Xu"
    ],
    [
     "Kimberly D.",
     "Mueller"
    ],
    [
     "Julie",
     "Liss"
    ],
    [
     "Visar",
     "Berisha"
    ]
   ],
   "title": "Segmental and Suprasegmental Speech Foundation Models for Classifying Cognitive Risk Factors: Evaluating Out-of-the-Box Performance",
   "original": "2063",
   "order": 185,
   "page_count": 5,
   "abstract": [
    "Speech foundation models are remarkably successful in various consumer applications, prompting their extension to clinical use-cases. This is challenged by small clinical datasets, which precludes effective fine-tuning. We tested the efficacy of two models to classify participants by segmental (Wav2Vec2.0) and suprasegmental (Trillsson) speech analysis windows. Analysis at both time scales has shown differences in the context of cognitive decline. Speakers were classified as healthy controls (HC), Amyloid-β+ (Aβ+), mild cognitive impairment (MCI), or dementia. A subset of W2V2 and Trillsson representations showed large effect size between HC and each risk factor. Cross-validation showed W2V2 consistently outperforms Trillsson. Mean macro-F1 of 54.1%, 63.5%, and 72.0% in were found for classifying Aβ+, MCI, and dementia from HC. Repeatability of Trillsson and W2V2 showed intraclass correlations of 0.30 and 0.41. Reliability of such models must be enhanced for clinical speech analysis and longitudinal tracking."
   ],
   "p1": 917,
   "pn": 921,
   "doi": "10.21437/Interspeech.2024-2063",
   "url": "interspeech_2024/ng24_interspeech.html"
  },
  "nishihara24_interspeech": {
   "authors": [
    [
     "Miku",
     "Nishihara"
    ],
    [
     "Dan",
     "Wells"
    ],
    [
     "Korin",
     "Richmond"
    ],
    [
     "Aidan",
     "Pine"
    ]
   ],
   "title": "Low-dimensional Style Token Control for Hyperarticulated Speech Synthesis",
   "original": "2074",
   "order": 693,
   "page_count": 5,
   "abstract": [
    "Global style tokens (GSTs) allow for rich modelling of the variation in a speech corpus and subsequent control of text-tospeech synthesis (TTS). However, certain styles of speech may be marked by variation along multiple dimensions, complicating the interpretation and control of learned style tokens. One example is hyperarticulated or ‘clear’ speech, for example as directed toward listeners with hearing impairments or language learners in the classroom, which in English is characterised by reduced speaking rate, increased F0, more careful articulation of vowels and plosive consonants, and other factors. We present a method for simplifying control of style tokens by applying principal components analysis (PCA) to GST weights from a TTS system trained on both plain and clear speech. We identify the axes of variation in PCA space with the acoustic correlates of clear speech in English and show that we can synthesise either style by moving along a single dimension in that space."
   ],
   "p1": 3385,
   "pn": 3389,
   "doi": "10.21437/Interspeech.2024-2074",
   "url": "interspeech_2024/nishihara24_interspeech.html"
  },
  "ebbers24_interspeech": {
   "authors": [
    [
     "Janek",
     "Ebbers"
    ],
    [
     "François G.",
     "Germain"
    ],
    [
     "Gordon",
     "Wichern"
    ],
    [
     "Jonathan",
     "Le Roux"
    ]
   ],
   "title": "Sound Event Bounding Boxes",
   "original": "2075",
   "order": 114,
   "page_count": 5,
   "abstract": [
    "Sound event detection is the task of recognizing sounds and determining their extent (onset/offset times) within an audio clip. Existing systems commonly predict sound presence posteriors in short time frames. Then, thresholding produces binary frame-level presence decisions, with the extent of individual events determined by merging presence in consecutive frames. In this paper, we show that frame-level thresholding deteriorates event extent prediction by coupling it with the system’s sound presence confidence. We propose to decouple the prediction of event extent and confidence by introducing sound event bounding boxes (SEBBs), which format each sound event prediction as a combination of a class type, extent, and overall confidence. We also propose a change-detection-based algorithm to convert frame-level posteriors into SEBBs. We find the algorithm significantly improves the performance of DCASE 2023 Challenge systems, boosting the state of the art from .644 to .686 PSDS1."
   ],
   "p1": 562,
   "pn": 566,
   "doi": "10.21437/Interspeech.2024-2075",
   "url": "interspeech_2024/ebbers24_interspeech.html"
  },
  "nafea24_interspeech": {
   "authors": [
    [
     "Youssef",
     "Nafea"
    ],
    [
     "Shady",
     "Shehata"
    ],
    [
     "Zeerak",
     "Talat"
    ],
    [
     "Ahmed",
     "Aboeitta"
    ],
    [
     "Ahmed",
     "Sharshar"
    ],
    [
     "Preslav",
     "Nakov"
    ]
   ],
   "title": "AraOffence: Detecting Offensive Speech Across Dialects in Arabic Media",
   "original": "2077",
   "order": 885,
   "page_count": 5,
   "abstract": [
    "Natural language processing (NLP) has made efforts towards identifying toxicity and offensive content for the text and image modalities. Despite sharing similar concerns with text and images, such as increased access to online abuse using speech, speech offensiveness research trails behind. While NLP has primarily considered English language data, speech has emphasized under-represented languages such as Swahili and Wolof. In this work, we introduce ARAOFFENSE, a dataset of scripted media in Arabic dialects labelled for offensiveness. ARAOFFENSE contains 2146 instances, of which 475 are labelled as offensive, spanning 1.55 hours of audio. We assess the capabilities of speech models to detect offensive content and present a hard-to-beat multi-modal text and audio model which outperforms the baselines by 26+% in terms of the Matthews Correlation Coefficient. Our work thus presents the first benchmark for offensive speech detection in dialectical Arabic."
   ],
   "p1": 4303,
   "pn": 4307,
   "doi": "10.21437/Interspeech.2024-2077",
   "url": "interspeech_2024/nafea24_interspeech.html"
  },
  "khokhlov24_interspeech": {
   "authors": [
    [
     "Yuri",
     "Khokhlov"
    ],
    [
     "Tatiana",
     "Prisyach"
    ],
    [
     "Anton",
     "Mitrofanov"
    ],
    [
     "Dmitry",
     "Dutov"
    ],
    [
     "Igor",
     "Agafonov"
    ],
    [
     "Tatiana",
     "Timofeeva"
    ],
    [
     "Aleksei",
     "Romanenko"
    ],
    [
     "Maxim",
     "Korenevsky"
    ]
   ],
   "title": "Classification of Room Impulse Responses and its application for channel verification and diarization",
   "original": "2081",
   "order": 666,
   "page_count": 5,
   "abstract": [
    "This paper describes experiments on a classification of Room Impulse Responses (RIRs) from recordings where acoustic channel is described by corresponding RIRs. The classifiers are trained on large sets of synthetic RIRs and then used as extractors of RIR embeddings bearing information about acoustic characteristic of a room as well as the locations of sound source and microphone. Experiments on different datasets demonstrate that RIR embeddings can be used for verification of acoustic channel and for the diarization in meeting-like scenarios where speakers' positions are fixed. The verification experiments on VoxCeleb1 and BUT Speech@FIT Reverb show reasonable performance of RIR embeddings. The diarization results with RIR embeddings on LibriCSS dataset are better than those with state-of-the-art speaker embeddings that shows the potential of the proposed approach."
   ],
   "p1": 3250,
   "pn": 3254,
   "doi": "10.21437/Interspeech.2024-2081",
   "url": "interspeech_2024/khokhlov24_interspeech.html"
  },
  "deluca24_interspeech": {
   "authors": [
    [
     "Alessandro",
     "De Luca"
    ],
    [
     "Andrew",
     "Clark"
    ],
    [
     "Volker",
     "Dellwo"
    ]
   ],
   "title": "NumberLie: a game-based experiment to understand the acoustics of deception and truthfulness",
   "original": "2082",
   "resource": "https://doi.org/10.5281/zenodo.12743994",
   "order": 755,
   "page_count": 5,
   "abstract": [
    "To record clearly defined natural deceptive speech with precise knowledge of the ground truth and immediate consequences for the lying subject we present here the NumberLie game. The NumberLie design enables simultaneous and isolated audio recording of five players in our state-of-the-art laboratory, or adapted to any number of players in an online setting, playing against each other in a number-based game revolving around deception and trustworthiness. We describe the technical solutions employed to guarantee precise labelling of statements as truths or lies and immediate consequences to each interaction, backed by a performance-based financial reward to motivate participants. The design is easily manipulated to tailor to specific research questions, maintaining constant or eliminating completely additional sources of variability."
   ],
   "p1": 3659,
   "pn": 3663,
   "doi": "10.21437/Interspeech.2024-2082",
   "url": "interspeech_2024/deluca24_interspeech.html"
  },
  "wu24o_interspeech": {
   "authors": [
    [
     "Rongshuai",
     "Wu"
    ],
    [
     "Debasish Ray",
     "Mohapatra"
    ],
    [
     "Sidney",
     "Fels"
    ]
   ],
   "title": "Modeling Vocal Tract Like Acoustic Tubes Using the Immersed Boundary Method",
   "original": "2087",
   "order": 699,
   "page_count": 5,
   "abstract": [
    "The standard finite-difference time-domain (FDTD) wave solver employs a regular grid that approximates vocal tract boundaries in a stair-stepped manner, making it less effective for modeling complex and dynamic geometry. We present a novel 2D wave solver that integrates a unique immersed boundary method with FDTD, enabling precise approximation of wave propagation in acoustic tubes. The solver uses Lagrangian points to define vocal tract boundaries on a regular grid, eliminating their stair-stepped discretization. The boundary and the flow equations interact via additional forcing terms characterized by boundary immittances. The results show that the formant frequencies of a single-segment tube closely align with its analytical solutions, exhibiting a percentage deviation of less than 3%. Similarly, the frequency responses of a two-segment tube and actual vocal tract geometries show good agreement with existing state-of-the-art 2D and 3D wave solvers, except for a few discrepancies."
   ],
   "p1": 3415,
   "pn": 3419,
   "doi": "10.21437/Interspeech.2024-2087",
   "url": "interspeech_2024/wu24o_interspeech.html"
  },
  "cronenberg24_interspeech": {
   "authors": [
    [
     "Johanna",
     "Cronenberg"
    ],
    [
     "Ioana",
     "Chitoran"
    ],
    [
     "Lori",
     "Lamel"
    ],
    [
     "Ioana",
     "Vasilescu"
    ]
   ],
   "title": "Crosslinguistic Comparison of Acoustic Variation in the Vowel Sequences /ia/ and /io/ in Four Romance Languages",
   "original": "2090",
   "order": 761,
   "page_count": 5,
   "abstract": [
    "According to theoretical accounts, Romance languages differ with respect to the pronunciation of vowel sequences such as /ia/ and /io/. Italian produces these sequences as diphthongs, i.e. /ja/ and /jo/, while Portuguese prefers hiatuses. Spanish and Romanian are claimed to use a mix of diphthongs and hiatuses. These accounts are based on phonological criteria or on small samples of carefully read isolated words. This study proposes to investigate the realisation of /ia/ and /io/ in large corpora of fluent speech, focusing on their acoustic properties along the whole formant trajectory. The results of the functional and statistical analyses show extensive acoustic variation with respect to the duration of the sequences as well as their formant dynamics. The languages clearly differ from one another, but the analysis shows that there remains more to learn about the distinction between diphthongs and hiatuses. We discuss the inclusion of further factors in future investigations."
   ],
   "p1": 3689,
   "pn": 3693,
   "doi": "10.21437/Interspeech.2024-2090",
   "url": "interspeech_2024/cronenberg24_interspeech.html"
  },
  "gusev24_interspeech": {
   "authors": [
    [
     "Aleksei",
     "Gusev"
    ],
    [
     "Anastasia",
     "Avdeeva"
    ]
   ],
   "title": "Improvement Speaker Similarity for Zero-Shot Any-to-Any Voice Conversion of Whispered and Regular Speech",
   "original": "2091",
   "order": 563,
   "page_count": 5,
   "abstract": [
    "Zero-shot voice conversion aims to transfer the voice of a source speaker to that of a speaker unseen during training, while preserving the content information. Although various methods have been proposed to reconstruct speaker information in generated speech, there is still room for improvement in achieving high similarity between generated and ground truth recordings. Furthermore, zero-shot voice conversion for speech in specific domains, such as whispered, remains an unexplored area. To address this problem, we propose a SpeakerVC model that can effectively perform zero-shot speech conversion in both voiced and whispered domains, while being lightweight and capable of running in streaming mode without significant quality degradation. In addition, we explore methods to improve the quality of speaker identity transfer and demonstrate their effectiveness for a variety of voice conversion systems."
   ],
   "p1": 2735,
   "pn": 2739,
   "doi": "10.21437/Interspeech.2024-2091",
   "url": "interspeech_2024/gusev24_interspeech.html"
  },
  "wu24p_interspeech": {
   "authors": [
    [
     "Haibin",
     "Wu"
    ],
    [
     "Yuan",
     "Tseng"
    ],
    [
     "Hung-yi",
     "Lee"
    ]
   ],
   "title": "CodecFake: Enhancing Anti-Spoofing Models Against Deepfake Audios from Codec-Based Speech Synthesis Systems",
   "original": "2093",
   "order": 363,
   "page_count": 5,
   "abstract": [
    "Current state-of-the-art (SOTA) codec-based audio synthesis systems can mimic anyone's voice with just a 3-second sample from that specific unseen speaker.  Unfortunately, malicious attackers may exploit these technologies, causing misuse and security issues. Anti-spoofing models have been developed to detect fake speech.  However, the open question of whether current SOTA anti-spoofing models can effectively counter deepfake audios from codec-based speech synthesis systems remains unanswered. In this paper, we curate an extensive collection of contemporary SOTA codec models, employing them to re-create synthesized speech.  This endeavor leads to the creation of CodecFake, the first codec-based deepfake audio dataset.  Additionally, we verify that anti-spoofing models trained on commonly used datasets cannot detect synthesized speech from current codec-based speech generation systems. The proposed CodecFake dataset empowers these models to counter this challenge effectively."
   ],
   "p1": 1770,
   "pn": 1774,
   "doi": "10.21437/Interspeech.2024-2093",
   "url": "interspeech_2024/wu24p_interspeech.html"
  },
  "coffey24_interspeech": {
   "authors": [
    [
     "Joseph",
     "Coffey"
    ],
    [
     "Okko",
     "Räsänen"
    ],
    [
     "Camila",
     "Scaff"
    ],
    [
     "Alejandrina",
     "Cristia"
    ]
   ],
   "title": "The Difficulty and Importance of Estimating the Lower and Upper Bounds of Infant Speech Exposure",
   "original": "2102",
   "order": 739,
   "page_count": 5,
   "abstract": [
    "Estimates of infants' language exposure are necessary for computational studies that attempt to model and learn from infant language experiences. However, there are no well-established input estimates usable for this purpose. This paper explores empirical data on infant language exposure across various cultural settings to derive plausible limits on the speech exposure infants might receive during their first years of life. First, we argue that several assumptions lack unanimous agreement and demonstrate that existing data are problematic in multiple ways. Integrating these uncertainties and published information, we find estimates that range from 1 to 3,300 hours per year. We end by discussing how such a large possible range may impact evaluation of the plausibility and benchmarking of computational models."
   ],
   "p1": 3615,
   "pn": 3619,
   "doi": "10.21437/Interspeech.2024-2102",
   "url": "interspeech_2024/coffey24_interspeech.html"
  },
  "murzaku24_interspeech": {
   "authors": [
    [
     "John",
     "Murzaku"
    ],
    [
     "Adil",
     "Soubki"
    ],
    [
     "Owen",
     "Rambow"
    ]
   ],
   "title": "Multimodal Belief Prediction",
   "original": "2103",
   "order": 224,
   "page_count": 5,
   "abstract": [
    "Recognizing a speaker's level of commitment to a belief is a difficult task; humans do not only interpret the meaning of the words in context, but also understand cues from intonation and other aspects of the audio signal. Many papers and corpora in the NLP community have approached the belief prediction task using text-only approaches. We are the first to frame and present results on the multimodal belief prediction task. We use the CB-Prosody corpus (CBP), containing aligned text and audio with speaker belief annotations. We first report baselines and significant features using acoustic-prosodic features and traditional machine learning methods. We then present text and audio baselines for the CBP corpus fine-tuning on BERT and Whisper respectively. Finally, we present our multimodal architecture which fine-tunes on BERT and Whisper and uses multiple fusion methods, improving on both modalities alone."
   ],
   "p1": 1075,
   "pn": 1079,
   "doi": "10.21437/Interspeech.2024-2103",
   "url": "interspeech_2024/murzaku24_interspeech.html"
  },
  "wagner24_interspeech": {
   "authors": [
    [
     "Dominik",
     "Wagner"
    ],
    [
     "Ilja",
     "Baumann"
    ],
    [
     "Korbinian",
     "Riedhammer"
    ],
    [
     "Tobias",
     "Bocklet"
    ]
   ],
   "title": "Outlier Reduction with Gated Attention for Improved Post-training Quantization in Large Sequence-to-sequence Speech Foundation Models",
   "original": "2105",
   "order": 949,
   "page_count": 5,
   "abstract": [
    "This paper explores the improvement of post-training quantization (PTQ) after knowledge distillation in the Whisper speech foundation model family.  We address the challenge of outliers in weights and activation tensors, known to impede quantization quality in transformer-based language and vision models.  Extending this observation to Whisper, we demonstrate that these outliers are also present when transformer-based models are trained to perform automatic speech recognition, necessitating mitigation strategies for PTQ.  We show that outliers can be reduced by a recently proposed gating mechanism in the attention blocks of the student model, enabling effective 8-bit quantization, and lower word error rates compared to student models without the gating mechanism in place."
   ],
   "p1": 4623,
   "pn": 4627,
   "doi": "10.21437/Interspeech.2024-2105",
   "url": "interspeech_2024/wagner24_interspeech.html"
  },
  "tapo24_interspeech": {
   "authors": [
    [
     "Allahsera",
     "Tapo"
    ],
    [
     "Éric",
     "Le Ferrand"
    ],
    [
     "Zoey",
     "Liu"
    ],
    [
     "Christopher",
     "Homan"
    ],
    [
     "Emily",
     "Prud'hommeaux"
    ]
   ],
   "title": "Leveraging Speech Data Diversity to Document Indigenous Heritage and Culture",
   "original": "2107",
   "order": 1042,
   "page_count": 5,
   "abstract": [
    "The majority of the world's 7,000 languages lack a standardized writing system. In this paper we consider one such language, Bambara, which is rarely written down but is widely spoken in Mali and neighboring countries. We explore the task of using automatic speech recognition (ASR) to transcribe culturally significant recordings focused on two domains: archival linguistic and anthropological fieldwork and contemporary oral histories performed by griots, the traditional Mande history keepers. We describe our two 6.5-hour corpora then experiment with different data configurations and multi-stage tuning from pretrained multilingual models within two neural ASR architectures. We find that while the diversity in content, style, and recording quality across the two corpora presents challenges, their commonalities can sometimes be leveraged to improve ASR accuracy. We note, however, that the diverse qualities of these corpora diminish their utility for cross-domain ASR training."
   ],
   "p1": 5088,
   "pn": 5092,
   "doi": "10.21437/Interspeech.2024-2107",
   "url": "interspeech_2024/tapo24_interspeech.html"
  },
  "abdullah24_interspeech": {
   "authors": [
    [
     "Badr M.",
     "Abdullah"
    ],
    [
     "Mohammed Maqsood",
     "Shaik"
    ],
    [
     "Dietrich",
     "Klakow"
    ]
   ],
   "title": "Wave to Interlingua: Analyzing Representations of Multilingual Speech Transformers for Spoken Language Translation",
   "original": "2109",
   "order": 74,
   "page_count": 5,
   "abstract": [
    "In Transformer-based Speech-to-Text (S2T) translation, an encoder-decoder model is trained end-to-end  to take as input  an untranscribed acoustic signal in the source language and directly generate a text translation in the target language. S2T translation models can also be trained in multilingual setups where a single front-end speech encoder is shared across multiple languages. A lingering question, however, is whether the encoder represents spoken utterances in a language-neutral space. In this paper, we present an interpretability study of encoder representations in a multilingual speech translation Transformer via various probing tasks. Our main findings show that while encoder representations are not entirely language-neutral, there exists a semantic subspace that is shared across different languages. Furthermore, we discuss our findings and the implication of our study on cross-lingual learning for spoken language understanding tasks."
   ],
   "p1": 362,
   "pn": 366,
   "doi": "10.21437/Interspeech.2024-2109",
   "url": "interspeech_2024/abdullah24_interspeech.html"
  },
  "liu24n_interspeech": {
   "authors": [
    [
     "Mingshuai",
     "Liu"
    ],
    [
     "Zhuangqi",
     "Chen"
    ],
    [
     "Xiaopeng",
     "Yan"
    ],
    [
     "Yuanjun",
     "Lv"
    ],
    [
     "Xianjun",
     "Xia"
    ],
    [
     "Chuanzeng",
     "Huang"
    ],
    [
     "Yijian",
     "Xiao"
    ],
    [
     "Lei",
     "Xie"
    ]
   ],
   "title": "RaD-Net 2: A causal two-stage repairing and denoising speech enhancement network with knowledge distillation and complex axial self-attention",
   "original": "2114",
   "order": 349,
   "page_count": 5,
   "abstract": [
    "In real-time speech communication systems, speech signals are often degraded by multiple distortions. Recently, a two-stage Repair-and-Denoising network (RaD-Net) was proposed with superior speech quality improvement in the ICASSP 2024 Speech Signal Improvement (SSI) Challenge. However, failure to use future information and constraint receptive field of convolution layers limit the system's performance. To mitigate these problems, we extend RaD-Net to its upgraded version, RaD-Net 2. Specifically, a causality-based knowledge distillation is introduced in the first stage to use future information in a causal way. We use the non-causal repairing network as the teacher to improve the performance of the causal repairing network. In addition, in the second stage, complex axial self-attention is applied in the denoising network's complex feature encoder/decoder. Experimental results on the ICASSP 2024 SSI Challenge blind test set show that RaD-Net 2 brings 0.10 OVRL DNSMOS improvement compared to RaD-Net."
   ],
   "p1": 1700,
   "pn": 1704,
   "doi": "10.21437/Interspeech.2024-2114",
   "url": "interspeech_2024/liu24n_interspeech.html"
  },
  "pereztoro24_interspeech": {
   "authors": [
    [
     "Paula Andrea",
     "Pérez-Toro"
    ],
    [
     "Tomas",
     "Arias-Vergara"
    ],
    [
     "Philipp",
     "Klumpp"
    ],
    [
     "Tobias",
     "Weise"
    ],
    [
     "Maria",
     "Schuster"
    ],
    [
     "Elmar",
     "Noeth"
    ],
    [
     "Juan Rafael",
     "Orozco-Arroyave"
    ],
    [
     "Andreas",
     "Maier"
    ]
   ],
   "title": "Multilingual Speech and Language Analysis for the Assessment of Mild Cognitive Impairment: Outcomes from the Taukadial Challenge",
   "original": "2115",
   "order": 198,
   "page_count": 5,
   "abstract": [
    "Cognitive decline, a hallmark of several neurological conditions, including dementia and Alzheimer's disease, often manifests in noticeable changes in speech patterns and language use. Speech analysis in this context can serve as a valuable tool for early detection and monitoring of cognitive impairment. In this paper, we present the results of our attempts at the TAUKADIAL Challenge for automatically detecting people with mild cognitive impairment and predicting a cognitive score on English and Chinese speakers. In the classification task, we achieved a UAR of 83% using two language-dependent classifiers trained with timing and acoustic features. In the regression task, we obtained an RMSE of 1.87 using English speakers to train the base model with timing, acoustic, and language-dependent features."
   ],
   "p1": 982,
   "pn": 986,
   "doi": "10.21437/Interspeech.2024-2115",
   "url": "interspeech_2024/pereztoro24_interspeech.html"
  },
  "yakovlev24_interspeech": {
   "authors": [
    [
     "Ivan",
     "Yakovlev"
    ],
    [
     "Rostislav",
     "Makarov"
    ],
    [
     "Andrei",
     "Balykin"
    ],
    [
     "Pavel",
     "Malov"
    ],
    [
     "Anton",
     "Okhotnikov"
    ],
    [
     "Nikita",
     "Torgashov"
    ]
   ],
   "title": "Reshape Dimensions Network for Speaker Recognition",
   "original": "2116",
   "order": 663,
   "page_count": 5,
   "abstract": [
    "In this paper, we present Reshape Dimensions Network (ReDimNet), a novel neural network architecture for extracting utterance-level speaker representations. Our approach leverages dimensionality reshaping of 2D feature maps to 1D signal representation and vice versa, enabling the joint usage of 1D and 2D blocks. We propose an original network topology that preserves the volume of channel-timestep-frequency outputs of 1D and 2D blocks, facilitating efficient residual feature maps aggregation. Moreover, ReDimNet is efficiently scalable, and we introduce a range of model sizes, varying from 1 to 15 M parameters and from 0.5 to 20 GMACs. Our experimental results demonstrate that ReDimNet achieves state-of-the-art performance in speaker recognition while reducing computational complexity and the number of model parameters."
   ],
   "p1": 3235,
   "pn": 3239,
   "doi": "10.21437/Interspeech.2024-2116",
   "url": "interspeech_2024/yakovlev24_interspeech.html"
  },
  "maciejewski24_interspeech": {
   "authors": [
    [
     "Matthew",
     "Maciejewski"
    ],
    [
     "Dominik",
     "Klement"
    ],
    [
     "Ruizhe",
     "Huang"
    ],
    [
     "Matthew",
     "Wiesner"
    ],
    [
     "Sanjeev",
     "Khudanpur"
    ]
   ],
   "title": "Evaluating the Santa Barbara Corpus: Challenges of the Breadth of Conversational Spoken Language",
   "original": "2119",
   "order": 446,
   "page_count": 5,
   "abstract": [
    "As speech technology has matured, there has been a push towards systems that can process conversational speech, reflecting the so-called “cocktail party problem,” which includes not only more challenging acoustic conditions, but also necessitates solutions to new problems, such as identifying who spoke when and processing multiple concurrent streams of speech. Such problems have been approached primarily via corpora comprising business meetings and dinner parties, overlooking the broad range of conversational dynamics and speaker demographics that fall under the category of multi-talker speech. To this end, we introduce the use of the Santa Barbara Corpus of Spoken American English for evaluation of speech technology—including preparing the corpus and annotations for automatic processing, demonstrating the failure of state-of-the-art systems to withstand the heterogeneity of conditions, and highlighting the situations where standard methods struggle to perform at all."
   ],
   "p1": 2155,
   "pn": 2159,
   "doi": "10.21437/Interspeech.2024-2119",
   "url": "interspeech_2024/maciejewski24_interspeech.html"
  },
  "wagner24b_interspeech": {
   "authors": [
    [
     "Dominik",
     "Wagner"
    ],
    [
     "Sebastian P.",
     "Bayerl"
    ],
    [
     "Ilja",
     "Baumann"
    ],
    [
     "Elmar",
     "Noeth"
    ],
    [
     "Korbinian",
     "Riedhammer"
    ],
    [
     "Tobias",
     "Bocklet"
    ]
   ],
   "title": "Large Language Models for Dysfluency Detection in Stuttered Speech",
   "original": "2120",
   "order": 1048,
   "page_count": 5,
   "abstract": [
    "Accurately detecting dysfluencies in spoken language can help to improve the performance of automatic speech and language processing components and support the development of more inclusive speech and language technologies. Inspired by the recent trend towards the deployment of large language models (LLMs) as universal learners and processors of non-lexical inputs, such as audio and video, we approach the task of multi-label dysfluency detection as a language modeling problem. We present hypotheses candidates generated with an automatic speech recognition system and acoustic representations extracted from an audio encoder model to an LLM, and finetune the system to predict dysfluency labels on three datasets containing English and German stuttered speech. The experimental results show that our system effectively combines acoustic and lexical information and achieves competitive results on the multi-label stuttering detection task. "
   ],
   "p1": 5118,
   "pn": 5122,
   "doi": "10.21437/Interspeech.2024-2120",
   "url": "interspeech_2024/wagner24b_interspeech.html"
  },
  "omahony24_interspeech": {
   "authors": [
    [
     "Johannah",
     "O'Mahony"
    ],
    [
     "Catherine",
     "Lai"
    ],
    [
     "Éva",
     "Székely"
    ]
   ],
   "title": "Well, what can you do with messy data? Exploring the prosody and pragmatic function of the discourse marker &quot;well&quot; with found data and speech synthesis",
   "original": "2122",
   "order": 840,
   "page_count": 5,
   "abstract": [
    "Recently, there has been growing interest in the synthesis of conversational speech prosody. Conversational prosody is variable and carries many pragmatic functions. As speech synthesis research moves to using large amounts of untranscribed data, it is crucial that we understand the subtle pragmatic differences prosody can make. This study focuses on discourse markers, which are linguistic elements that perform various communicative functions, with their specific roles often linked to their prosodic realisation. In this paper, we explore the prosodic realisation of well using an unlabelled corpus of conversational speech. We use clustering to explore the variation in its prosodic realisation and identify common patterns in a data-driven manner. We synthesise the cluster centroids using controllable speech synthesis. Finally, we evaluate how the prosodic realisation of well affects the meaning of an utterance. "
   ],
   "p1": 4084,
   "pn": 4088,
   "doi": "10.21437/Interspeech.2024-2122",
   "url": "interspeech_2024/omahony24_interspeech.html"
  },
  "xu24i_interspeech": {
   "authors": [
    [
     "Zhongweiyang",
     "Xu"
    ],
    [
     "Ali",
     "Aroudi"
    ],
    [
     "Ke",
     "Tan"
    ],
    [
     "Ashutosh",
     "Pandey"
    ],
    [
     "Jung-Suk",
     "Lee"
    ],
    [
     "Buye",
     "Xu"
    ],
    [
     "Francesco",
     "Nesta"
    ]
   ],
   "title": "FoVNet: Configurable Field-of-View Speech Enhancement with Low Computation and Distortion for Smart Glasses",
   "original": "2124",
   "order": 686,
   "page_count": 5,
   "abstract": [
    "This paper presents a novel multi-channel speech enhancement approach, FoVNet, that enables highly efficient speech enhancement within a configurable field of view (FoV) of a smart-glasses user without needing specific target-talker(s) directions. It advances over prior works by enhancing all speakers within any given FoV, with a hybrid signal processing and deep learning approach designed with high computational efficiency. The neural network component is designed with ultra-low computation (about 50 MMACS). A multi-channel Wiener filter and a post-processing module are further used to improve perceptual quality. We evaluate our algorithm with a microphone array on smart glasses, providing a configurable, efficient solution for augmented hearing on energy-constrained devices. FoVNet excels in both computational efficiency and speech quality across multiple scenarios, making it a promising solution for smart glasses applications."
   ],
   "p1": 3350,
   "pn": 3354,
   "doi": "10.21437/Interspeech.2024-2124",
   "url": "interspeech_2024/xu24i_interspeech.html"
  },
  "baumann24_interspeech": {
   "authors": [
    [
     "Ilja",
     "Baumann"
    ],
    [
     "Nicole",
     "Unger"
    ],
    [
     "Dominik",
     "Wagner"
    ],
    [
     "Korbinian",
     "Riedhammer"
    ],
    [
     "Tobias",
     "Bocklet"
    ]
   ],
   "title": "Automatic Evaluation of a Sentence Memory Test for Preschool Children",
   "original": "2125",
   "order": 1056,
   "page_count": 5,
   "abstract": [
    "Assessment of memory capabilities in preschool-aged children is crucial for early detection of potential speech development impairments or delays. We present an approach for the automatic evaluation of a standardized sentence memory test specifically for preschool children. Our methodology leverages automatic transcription of recited sentences and evaluation based on natural language processing techniques. We demonstrate the effectiveness of our approach on a dataset comprised of recited sentences from preschool-aged children, incorporating ratings of semantic and syntactic correctness. The best performing systems achieve an F1 score of 91.7% for semantic correctness and 86.1% for syntactic correctness using automatic transcripts. Our results showcase the potential of automated evaluation systems in providing reliable and efficient assessments of memory capabilities in early childhood, facilitating timely interventions and support for children with language development needs."
   ],
   "p1": 5158,
   "pn": 5162,
   "doi": "10.21437/Interspeech.2024-2125",
   "url": "interspeech_2024/baumann24_interspeech.html"
  },
  "zhong24c_interspeech": {
   "authors": [
    [
     "Jinzuomu",
     "Zhong"
    ],
    [
     "Yang",
     "Li"
    ],
    [
     "Hui",
     "Huang"
    ],
    [
     "Korin",
     "Richmond"
    ],
    [
     "Jie",
     "Liu"
    ],
    [
     "Zhiba",
     "Su"
    ],
    [
     "Jing",
     "Guo"
    ],
    [
     "Benlai",
     "Tang"
    ],
    [
     "Fengjie",
     "Zhu"
    ]
   ],
   "title": "Multi-Modal Automatic Prosody Annotation with Contrastive Pretraining of Speech-Silence and Word-Punctuation",
   "original": "2133",
   "order": 476,
   "page_count": 5,
   "abstract": [
    "In expressive and controllable Text-to-Speech (TTS), explicit prosodic features significantly improve the naturalness and controllability of synthesised speech. However, manual prosody annotation is labor-intensive and inconsistent. To address this issue, a two-stage automatic annotation pipeline is novelly proposed in this paper. In the first stage, we use contrastive pretraining of Speech-Silence and Word-Punctuation (SSWP) pairs to enhance prosodic information in latent representations. In the second stage, we build a multi-modal prosody annotator, comprising pretrained encoders, a text-speech fusing scheme, and a sequence classifier. Experiments on English prosodic boundaries demonstrate that our method achieves state-of-the-art (SOTA) performance with 0.72 and 0.93 f1 score for Prosodic Word and Prosodic Phrase boundary respectively, while bearing remarkable robustness to data scarcity."
   ],
   "p1": 2305,
   "pn": 2309,
   "doi": "10.21437/Interspeech.2024-2133",
   "url": "interspeech_2024/zhong24c_interspeech.html"
  },
  "baumann24b_interspeech": {
   "authors": [
    [
     "Ilja",
     "Baumann"
    ],
    [
     "Dominik",
     "Wagner"
    ],
    [
     "Maria",
     "Schuster"
    ],
    [
     "Korbinian",
     "Riedhammer"
    ],
    [
     "Elmar",
     "Noeth"
    ],
    [
     "Tobias",
     "Bocklet"
    ]
   ],
   "title": "Towards Self-Attention Understanding for Automatic Articulatory Processes Analysis in Cleft Lip and Palate Speech",
   "original": "2134",
   "order": 501,
   "page_count": 5,
   "abstract": [
    "Cleft lip and palate (CLP) speech presents unique challenges for automatic phoneme analysis due to its distinct acoustic characteristics and articulatory anomalies. We perform phoneme analysis in CLP speech using a pre-trained wav2vec 2.0 model with a multi-head self-attention classification module to capture long-range dependencies within the speech signal, thereby enabling better contextual understanding of phoneme sequences. We demonstrate the effectiveness of our approach in the classification of various articulatory processes in CLP speech. Furthermore, we investigate the interpretability of self-attention to gain insights into the model's understanding of CLP speech characteristics. Our findings highlight the potential of the self-attention mechanisms for improving automatic phoneme analysis in CLP speech, paving the way for enhanced diagnostics, adding interpretability for therapists and affected patients."
   ],
   "p1": 2430,
   "pn": 2434,
   "doi": "10.21437/Interspeech.2024-2134",
   "url": "interspeech_2024/baumann24b_interspeech.html"
  },
  "mousavi24_interspeech": {
   "authors": [
    [
     "Pooneh",
     "Mousavi"
    ],
    [
     "Jarod",
     "Duret"
    ],
    [
     "Salah",
     "Zaiem"
    ],
    [
     "Luca",
     "Della Libera"
    ],
    [
     "Artem",
     "Ploujnikov"
    ],
    [
     "Cem",
     "Subakan"
    ],
    [
     "Mirco",
     "Ravanelli"
    ]
   ],
   "title": "How Should We Extract Discrete Audio Tokens from Self-Supervised Models?",
   "original": "2135",
   "order": 526,
   "page_count": 5,
   "abstract": [
    "Discrete audio tokens have recently gained attention for their potential to bridge the gap between audio and language processing. Ideal audio tokens must preserve content, paralinguistic elements, speaker identity, and many other audio details. Current audio tokenization methods fall into two categories: Semantic tokens, acquired through quantization of Self-Supervised Learning (SSL) models, and Neural compression-based tokens (codecs). Although previous studies have benchmarked codec models to identify optimal configurations, the ideal setup for quantizing pretrained SSL models remains unclear. This paper explores the optimal configuration of semantic tokens across discriminative and generative tasks. We propose a scalable solution to train a universal vocoder across multiple SSL layers. Furthermore, an attention mechanism is employed to identify task-specific influential layers, enhancing the adaptability and performance of semantic tokens in diverse audio applications."
   ],
   "p1": 2554,
   "pn": 2558,
   "doi": "10.21437/Interspeech.2024-2135",
   "url": "interspeech_2024/mousavi24_interspeech.html"
  },
  "woszczyk24_interspeech": {
   "authors": [
    [
     "Dominika",
     "Woszczyk"
    ],
    [
     "Ranya",
     "Aloufi"
    ],
    [
     "Soteris",
     "Demetriou"
    ]
   ],
   "title": "Prosody-Driven Privacy-Preserving Dementia Detection",
   "original": "2137",
   "order": 623,
   "page_count": 5,
   "abstract": [
    "Speaker embeddings extracted from voice recordings have been proven valuable for dementia detection. However, by their nature, these embeddings contain identifiable information which raises privacy concerns. In this work, we aim to anonymize embeddings while preserving the diagnostic utility for dementia detection. Previous studies rely on adversarial learning and models trained on the target attribute and struggle in limited-resource settings. We propose a novel approach that leverages domain knowledge to disentangle prosody features relevant to dementia from speaker embeddings without relying on a dementia classifier. Our experiments show the effectiveness of our approach in preserving speaker privacy (speaker recognition F1-score .01%) while maintaining high dementia detection score F1-score of 74% on the ADReSS dataset. Our results are also on par with a more constrained classifier-dependent system on ADReSSo (.01% and .66%), and have no impact on synthesized speech naturalness."
   ],
   "p1": 3035,
   "pn": 3039,
   "doi": "10.21437/Interspeech.2024-2137",
   "url": "interspeech_2024/woszczyk24_interspeech.html"
  },
  "nguyen24c_interspeech": {
   "authors": [
    [
     "Minh",
     "Nguyen"
    ],
    [
     "Toan Quoc",
     "Nguyen"
    ],
    [
     "Kishan",
     "KC"
    ],
    [
     "Zeyu",
     "Zhang"
    ],
    [
     "Thuy",
     "Vu"
    ]
   ],
   "title": "Reinforcement Learning from Answer Reranking Feedback for Retrieval-Augmented Answer Generation",
   "original": "2147",
   "order": 832,
   "page_count": 5,
   "abstract": [
    "Retrieval-augmented generation (RAG) is a method to improve accuracy and reliability of large language models (LLMs) for open-domain question answering (ODQA). Traditional approaches rely on supervised learning, which can result in misaligned user intent and system output. Reinforcement learning from human feedback (RLHF) addresses this issue by training a reward model using human preference feedback. In this work, we introduce a novel RLHF framework for ODQA, leveraging existing large-scale answer reranking datasets for training a reward model. In particular, our reward model for ODQA plays two complementary roles: (i) providing ranking scores as rewards for PPO, and (ii) retrieving relevant facts that enable the ODQA system to formulate a factual answer. Experimental results indicate that our proposed framework is effective for RLHF, leading to near-expert performance for ODQA."
   ],
   "p1": 4044,
   "pn": 4048,
   "doi": "10.21437/Interspeech.2024-2147",
   "url": "interspeech_2024/nguyen24c_interspeech.html"
  },
  "elbanna24_interspeech": {
   "authors": [
    [
     "Gasser",
     "Elbanna"
    ],
    [
     "Zohreh",
     "Mostaani"
    ],
    [
     "Mathew",
     "Magimai.-Doss"
    ]
   ],
   "title": "Predicting Heart Activity from Speech using Data-driven and Knowledge-based features",
   "original": "2150",
   "order": 976,
   "page_count": 5,
   "abstract": [
    "Accurately predicting heart activity and other biological signals is crucial for diagnosis and monitoring. Given that speech is an outcome of multiple physiological systems, a significant body of work studied the acoustic correlates of heart activity. Recently, self-supervised models have excelled in speech-related tasks compared to traditional acoustic methods. However, the robustness of data-driven representations in predicting heart activity remained unexplored. In this study, we demonstrate that self-supervised speech models outperform acoustic features in predicting heart activity parameters. We also emphasize the impact of individual variability on model generalizability. These findings underscore the value of data-driven representations in such tasks and the need for more speech-based physiological data to mitigate speaker-related challenges."
   ],
   "p1": 4758,
   "pn": 4762,
   "doi": "10.21437/Interspeech.2024-2150",
   "url": "interspeech_2024/elbanna24_interspeech.html"
  },
  "kwon24_interspeech": {
   "authors": [
    [
     "Jinuk",
     "Kwon"
    ],
    [
     "David",
     "Harwath"
    ],
    [
     "Debadatta",
     "Dash"
    ],
    [
     "Paul",
     "Ferrari"
    ],
    [
     "Jun",
     "Wang"
    ]
   ],
   "title": "Direct Speech Synthesis from Non-Invasive, Neuromagnetic Signals",
   "original": "2153",
   "resource": "https://doi.org/10.5281/zenodo.12798054",
   "order": 84,
   "page_count": 5,
   "abstract": [
    "Direct speech synthesis from neural activity can enable individuals to communicate without articulatory movement or vocalization. A number of recent speech braincomputer interface (BCI) studies have been conducted using invasive neuroimaging techniques, which require neurosurgery to implant electrodes in the brain. In this study, we investigated the feasibility of direct speech synthesis from non-invasive, magnetoencephalography (MEG) signals acquired while participants performed overt speech production tasks. We used a transformer-based framework (Squeezeformer) to convert neural signals into Mel-spectrograms followed by a neural vocoder to generate speech. Our approach achieved an average correlation coefficient of 0.95 between the target and the generated Mel spectrograms, indicating high fidelity. To the best of our knowledge, this is the first demonstration of synthesizing intelligible speech directly from non-invasive brain signals."
   ],
   "p1": 412,
   "pn": 416,
   "doi": "10.21437/Interspeech.2024-2153",
   "url": "interspeech_2024/kwon24_interspeech.html"
  },
  "li24oa_interspeech": {
   "authors": [
    [
     "Menglu",
     "Li"
    ],
    [
     "Xiao-Ping",
     "Zhang"
    ]
   ],
   "title": "Interpretable Temporal Class Activation Representation for Audio Spoofing  Detection",
   "original": "2156",
   "order": 233,
   "page_count": 5,
   "abstract": [
    "Explaining the decisions made by audio spoofing detection models is crucial for fostering trust in detection outcomes. However, current research on the interpretability of detection models is limited to applying XAI tools to post-trained models. In this paper, we utilize the wav2vec 2.0 model and attentive utterance-level features to integrate interpretability directly into the model's architecture, thereby enhancing transparency of the decision-making process. Specifically, we propose a class activation representation to localize the discriminative frames contributing to detection. Furthermore, we demonstrate that multi-label training based on spoofing types, rather than binary labels as bonafide and spoofed, enables the model to learn distinct characteristics of different attacks, significantly improving detection performance. Our model achieves state-of-the-art results, with an EER of 0.51% and a min t-DCF of 0.0165 on the ASVspoof2019-LA set."
   ],
   "p1": 1120,
   "pn": 1124,
   "doi": "10.21437/Interspeech.2024-2156",
   "url": "interspeech_2024/li24oa_interspeech.html"
  },
  "mihajlik24_interspeech": {
   "authors": [
    [
     "Peter",
     "Mihajlik"
    ],
    [
     "Yan",
     "Meng"
    ],
    [
     "Mate S",
     "Kadar"
    ],
    [
     "Julian",
     "Linke"
    ],
    [
     "Barbara",
     "Schuppler"
    ],
    [
     "Katalin",
     "Mády"
    ]
   ],
   "title": "On Disfluency and Non-lexical Sound Labeling for End-to-end Automatic Speech Recognition",
   "original": "2157",
   "order": 263,
   "page_count": 5,
   "abstract": [
    "Spontaneous speech contains a significant amount of disfluencies and non-lexical sounds (e.g., backchannels, filled pauses), which are often difficult to transcribe. Disfluency labeling for automatic speech recognition (ASR) aims at editing these phenomena in the transcription to improve overall recognition accuracy. Such labeling techniques typically delete non-lexical/disfluent labels from the prediction, where classical ASR techniques either ignore or treat them as lexical items. Our results, obtained by systematic comparison and detailed evaluation of various disfluency labeling methods on two different language conversational corpora, suggest that neither of the previous approaches are optimal. We propose to distinguish between filled pauses and meaningful conversational grunts and show that keeping the non-lexical labels is not only possible but as low as 7% label error rates can be achieved for highly important categories (including 'mhm') while preserving a decent WER."
   ],
   "p1": 1270,
   "pn": 1274,
   "doi": "10.21437/Interspeech.2024-2157",
   "url": "interspeech_2024/mihajlik24_interspeech.html"
  },
  "looney24_interspeech": {
   "authors": [
    [
     "David",
     "Looney"
    ],
    [
     "Nikolay D.",
     "Gaubitch"
    ]
   ],
   "title": "Robust spread spectrum speech watermarking using linear prediction and deep spectral shaping",
   "original": "2165",
   "order": 559,
   "page_count": 5,
   "abstract": [
    "We consider the problem of robust watermarking of speech signals using the spread spectrum method. To date, it has primarily been applied to music signals. Here we discuss differences between speech and music, and the implications this has on the use of spread spectrum watermarking. Moreover, we propose enhancements to the watermarking of speech for the detection of deepfake attacks at call centres using classical signal processing techniques and deep learning. "
   ],
   "p1": 2715,
   "pn": 2719,
   "doi": "10.21437/Interspeech.2024-2165",
   "url": "interspeech_2024/looney24_interspeech.html"
  },
  "pandey24_interspeech": {
   "authors": [
    [
     "Ashutosh",
     "Pandey"
    ],
    [
     "Sanha",
     "Lee"
    ],
    [
     "Juan",
     "Azcarreta"
    ],
    [
     "Daniel",
     "Wong"
    ],
    [
     "Buye",
     "Xu"
    ]
   ],
   "title": "All Neural Low-latency Directional Speech Extraction",
   "original": "2168",
   "order": 890,
   "page_count": 5,
   "abstract": [
    "We introduce a novel all neural model for low-latency directional speech extraction. The model uses direction of arrival (DOA) embeddings from a predefined spatial grid, which are transformed and fused into a recurrent neural network based speech extraction model. This process enables the model to effectively extract speech from a specified DOA. Unlike previous methods that relied on hand-crafted directional features, the proposed model trains DOA embeddings from scratch using speech enhancement loss, making it suitable for low-latency scenarios. Additionally, it operates at a high frame rate, taking in DOA with each input frame, which brings in the capability of quickly adapting to changing scene in highly dynamic real-world scenarios. We provide extensive evaluation to demonstrate the model's efficacy in directional speech extraction, robustness to DOA mismatch, and its capability to quickly adapt to abrupt changes in DOA."
   ],
   "p1": 4328,
   "pn": 4332,
   "doi": "10.21437/Interspeech.2024-2168",
   "url": "interspeech_2024/pandey24_interspeech.html"
  },
  "mumtaz24_interspeech": {
   "authors": [
    [
     "Benazir",
     "Mumtaz"
    ],
    [
     "Miriam",
     "Butt"
    ]
   ],
   "title": "Urdu Alternative Questions: A Hat Pattern",
   "original": "2172",
   "order": 430,
   "page_count": 5,
   "abstract": [
    "This study explores the prosody of alternative questions (AltQs) in Urdu. We found that AltQs are characterized by a hat pattern that is linked to double contrast sentences, featuring two contrastive foci, similar to German. However, unlike German, the selection of the phrasal accent (PA) is fixed. In Urdu the first PA features an L*(+H)Ha while the second accent has an L*(+H). The hat pattern's full realization depends on the spacing between PAs: greater spacing leads to a prolonged high peak, while reduced spacing results in a compressed hat pattern. Moreover, both the accentual phrase (AP) and the hat pattern carry an L*H contour, with differences in H tone realization: with the hat pattern, the H is significantly higher than that of the AP. We explain the presence of this shared phonological L*H contour by noting that the basic intonational phonology of Urdu is defined by a series of L*H contours, that can then be realized in different ways to show, for example, focus or AltQs."
   ],
   "p1": 2075,
   "pn": 2079,
   "doi": "10.21437/Interspeech.2024-2172",
   "url": "interspeech_2024/mumtaz24_interspeech.html"
  },
  "huang24i_interspeech": {
   "authors": [
    [
     "Kevin",
     "Huang"
    ],
    [
     "Jack",
     "Goldberg"
    ],
    [
     "Louis",
     "Goldstein"
    ],
    [
     "Shrikanth",
     "Narayanan"
    ]
   ],
   "title": "Analysis of articulatory setting for L1 and L2 English speakers using MRI data",
   "original": "2175",
   "order": 213,
   "page_count": 5,
   "abstract": [
    "This paper investigates the extent to which the geographical region (country) where a speaker acquired their English language affects the articulatory setting in their speech. To obtain accurate measurements for evaluating articulatory setting, we utilized a large real-time MRI corpus of vocal tract articulation. The corpus was obtained from speakers from a variety of linguistic backgrounds producing continuous English speech. We use an automated pipeline to process and extract articulatory positional information from the MRI video data. This data is used to draw comparisons between English language speakers from the United States and speakers who acquired their English in India, Korea, and China. Analysis of the speaker groups reveals statistically significant articulatory setting posture differences in multiple places of articulation."
   ],
   "p1": 1020,
   "pn": 1024,
   "doi": "10.21437/Interspeech.2024-2175",
   "url": "interspeech_2024/huang24i_interspeech.html"
  },
  "yuan24_interspeech": {
   "authors": [
    [
     "Kuang",
     "Yuan"
    ],
    [
     "Shuo",
     "Han"
    ],
    [
     "Swarun",
     "Kumar"
    ],
    [
     "Bhiksha",
     "Raj"
    ]
   ],
   "title": "DeWinder: Single-Channel Wind Noise Reduction using Ultrasound Sensing",
   "original": "2180",
   "order": 127,
   "page_count": 5,
   "abstract": [
    "The quality of audio recordings in outdoor environments is often degraded by the presence of wind. Mitigating the impact of wind noise on the perceptual quality of single-channel speech remains a significant challenge due to its non-stationary characteristics. Prior work in noise suppression treats wind noise as a general background noise without explicit modeling of its characteristics. In this paper, we leverage ultrasound as an auxiliary modality to explicitly sense the airflow and characterize the wind noise. We propose a multi-modal deep-learning framework to fuse the ultrasonic Doppler features and speech signals for wind noise reduction. Our results show that DeWinder can significantly improve the noise reduction capabilities of state-of-the-art speech enhancement models."
   ],
   "p1": 627,
   "pn": 631,
   "doi": "10.21437/Interspeech.2024-2180",
   "url": "interspeech_2024/yuan24_interspeech.html"
  },
  "sayeed24_interspeech": {
   "authors": [
    [
     "Mohammad Amaan",
     "Sayeed"
    ],
    [
     "Hanan",
     "Aldarmaki"
    ]
   ],
   "title": "Spoken Word2Vec: Learning Skipgram Embeddings from Speech",
   "original": "2181",
   "order": 600,
   "page_count": 5,
   "abstract": [
    "Text word embeddings that encode distributional semantics work by modeling contextual similarities of frequently occurring words. Acoustic word embeddings, on the other hand, typically encode low-level phonetic similarities. Semantic embeddings for spoken words have been previously explored using analogous algorithms to Word2Vec, but the resulting vectors still mainly encoded phonetic rather than semantic features. In this paper, we examine the assumptions and architectures used in previous works and show experimentally how shallow skipgram-like algorithms fail to encode distributional semantics when the input units are acoustically correlated. We illustrate the potential of an alternative deep end-to-end variant of the model and examine the effects on the resulting  embeddings, showing positive results of semantic relatedness in the embedding space. "
   ],
   "p1": 2920,
   "pn": 2924,
   "doi": "10.21437/Interspeech.2024-2181",
   "url": "interspeech_2024/sayeed24_interspeech.html"
  },
  "sanguedolce24_interspeech": {
   "authors": [
    [
     "Giulia",
     "Sanguedolce"
    ],
    [
     "Sophie",
     "Brook"
    ],
    [
     "Dragos C.",
     "Gruia"
    ],
    [
     "Patrick A.",
     "Naylor"
    ],
    [
     "Fatemeh",
     "Geranmayeh"
    ]
   ],
   "title": "When Whisper Listens to Aphasia: Advancing Robust Post-Stroke Speech Recognition",
   "original": "2183",
   "order": 408,
   "page_count": 5,
   "abstract": [
    "Despite recent advancements in Automatic Speech Recognition (ASR), its accuracy remains low for pathological speech, thereby limiting AI-based healthcare interventions in such settings. This work addresses this challenge by fine-tuning Whisper, an ASR known for its ability to capture high-dimensional features in healthy speech. Using our comprehensive dataset of patients with stroke, we fine-tuned Whisper and significantly reduced Word Error Rate (WER), surpassing previous work on severe aphasia. To demonstrate its generalisability, we tested the model on a separate database, AphasiaBank, and observed a lower WER despite variations in dialect, linguistics, and test protocols. Our result on the AphasiaBank was superior to previous ASRs trained on this database, confirming the generalisability of our approach. These outcomes not only address ASR limitations in impaired speech but also establish the foundations for standardised and versatile AI solutions for remote speech monitoring for timely diagnosis and intervention."
   ],
   "p1": 1995,
   "pn": 1999,
   "doi": "10.21437/Interspeech.2024-2183",
   "url": "interspeech_2024/sanguedolce24_interspeech.html"
  },
  "yarga24_interspeech": {
   "authors": [
    [
     "Sidi Yaya Arnaud",
     "Yarga"
    ],
    [
     "Sean U N",
     "Wood"
    ]
   ],
   "title": "Neuromorphic Keyword Spotting with Pulse Density Modulation MEMS Microphones",
   "original": "2185",
   "order": 671,
   "page_count": 5,
   "abstract": [
    "The Keyword Spotting (KWS) task involves continuous audio stream monitoring to detect predefined words, requiring low energy devices for continuous processing. Neuromorphic devices effectively address this energy challenge. However, the general neuromorphic KWS pipeline, from microphone to Spiking Neural Network (SNN), entails multiple processing stages. Leveraging the popularity of Pulse Density Modulation (PDM) microphones in modern devices and their similarity to spiking neurons, we propose a direct microphone-to-SNN connection. This approach eliminates intermediate stages, notably reducing computational costs. The system achieved an accuracy of 91.54\\% on the Google Speech Command (GSC) dataset, surpassing the state-of-the-art for the Spiking Speech Command (SSC) dataset which is a bio-inspired encoded GSC. Furthermore, the observed sparsity in network activity and connectivity indicates potential for remarkably low energy consumption in a neuromorphic device implementation."
   ],
   "p1": 3275,
   "pn": 3279,
   "doi": "10.21437/Interspeech.2024-2185",
   "url": "interspeech_2024/yarga24_interspeech.html"
  },
  "wang24ka_interspeech": {
   "authors": [
    [
     "Yuzhe",
     "Wang"
    ],
    [
     "Anna",
     "Favaro"
    ],
    [
     "Thomas",
     "Thebaud"
    ],
    [
     "Jesus",
     "Villalba"
    ],
    [
     "Najim",
     "Dehak"
    ],
    [
     "Laureano",
     "Moro-Velazquez"
    ]
   ],
   "title": "Exploring the Complementary Nature of Speech and Eye Movements for Profiling Neurological Disorders",
   "original": "2186",
   "order": 304,
   "page_count": 5,
   "abstract": [
    "Subtle changes in both speech and ocular movements often characterize neurodegenerative diseases (NDs). This study aims to profile various NDs using speech and ocular movements collected during the Cookie Thief Picture description task. We designed new multi-modal features centered around the Areas of Interest (AoIs) to quantify attention focus and search patterns. Through statistical analysis, we examined which features differentiated controls (CTLs) from subjects with various NDs, including Alzheimer’s disease (AD), Parkinson’s disease (PD), and Parkinson’s disease mimics (PDM). Results showed that subjects with AD and PDM had reduced AoI visits, indicating diminished search behavior. In contrast, subjects with PD displayed increased variability in visual search patterns, possibly reflecting disruptions in integrating visual information with verbal expression and loss of attention during the trial. These features effectively differentiated ND groups from CTLs and among themselves."
   ],
   "p1": 1475,
   "pn": 1479,
   "doi": "10.21437/Interspeech.2024-2186",
   "url": "interspeech_2024/wang24ka_interspeech.html"
  },
  "prasad24_interspeech": {
   "authors": [
    [
     "Amrutha",
     "Prasad"
    ],
    [
     "Srikanth",
     "Madikeri"
    ],
    [
     "Driss",
     "Khalil"
    ],
    [
     "Petr",
     "Motlicek"
    ],
    [
     "Christof",
     "Schuepbach"
    ]
   ],
   "title": "Speech and Language Recognition with Low-rank Adaptation of Pretrained Models",
   "original": "2187",
   "order": 581,
   "page_count": 5,
   "abstract": [
    "Finetuning large pretrained models demands considerable computational resources, posing practical constraints. Majority of the total number of parameters in these models are used by fully connected layers. In this work, we consider applying a semi-orthogonal constraint, followed by full finetuning to the fully connected layers reduces model parameters significantly without sacrificing efficacy in downstream tasks. Specifically, we consider wav2vec2.0 XLS-R and Whisper models for Automatic Speech Recognition and Language Recognition. Our results show that we can reduce the model size by approximately 24% during both training and inference time with 0.7% absolute drop in performance for XLS-R and no drop in performance for Whisper for ASR. In combination with performance-efficient training with low-rank adapters, the resource requirements for training can be further reduced by up to 90%."
   ],
   "p1": 2825,
   "pn": 2829,
   "doi": "10.21437/Interspeech.2024-2187",
   "url": "interspeech_2024/prasad24_interspeech.html"
  },
  "kim24s_interspeech": {
   "authors": [
    [
     "Kwangyoun",
     "Kim"
    ],
    [
     "Suwon",
     "Shon"
    ],
    [
     "Yi-Te",
     "Hsu"
    ],
    [
     "Prashant",
     "Sridhar"
    ],
    [
     "Karen",
     "Livescu"
    ],
    [
     "Shinji",
     "Watanabe"
    ]
   ],
   "title": "Convolution-Augmented Parameter-Efficient Fine-Tuning for Speech Recognition",
   "original": "2188",
   "order": 582,
   "page_count": 5,
   "abstract": [
    "Parameter-efficient fine-tuning (PEFT) methods, which train only a part of a model, yield efficient and effective models. Bottleneck approaches, such as adapters and low-rank adaptation (LoRA), have been found to be beneficial in numerous studies and are widely utilized. In this work, we propose and investigate an enhanced PEFT method that adds convolution to linear projection-based bottleneck approaches. We experiment with HuBERT, a representative speech model pre-trained with self-supervised learning, and fine-tune it for the automatic speech recognition (ASR) task to examine how the proposed PEFT method impacts training and inference. We demonstrate consistent performance improvements with a minimal increase in parameters and computational complexity."
   ],
   "p1": 2830,
   "pn": 2834,
   "doi": "10.21437/Interspeech.2024-2188",
   "url": "interspeech_2024/kim24s_interspeech.html"
  },
  "lee24l_interspeech": {
   "authors": [
    [
     "Jonathan Him Nok",
     "Lee"
    ],
    [
     "Mark",
     "Liberman"
    ],
    [
     "Martin",
     "Salzmann"
    ]
   ],
   "title": "Do we EXPECT TO find phonetic traces for syntactic traces?",
   "original": "2190",
   "order": 876,
   "page_count": 5,
   "abstract": [
    "In syntactic theories, the existence of silent/inaudible syntactic elements, such as movement traces, has been hypothesized to impact phonetic outcomes. One classic example involves the contraction of “want to” into “wanna,” where intervening syntactic traces are thought to prevent such contraction. This study extends this inquiry to “expect to,” a similar construction but without established morpho-phonological contractions, and investigates whether there are any phonetic effects of the presence of traces. Drawing on findings from wanna- contraction, syntactic theories predict that intervening traces will hinder phonetic reduction in “expect to.” To investigate, we randomly sampled 300 utterances containing “expect to” from a corpus of NPR podcasts. Multivariate linear regression shows no relationship between the presence of intervening syntactic traces and the duration of any phones in “expect to.” However, multinomial logistic regression showed that the lenition of “expect to” (“to” /tu:/ → [tʌ] or [tɪ]) was significantly more likely to occur across intervening syntactic traces. Our findings contradict the predictions posited by syntactic theories, suggesting a more intricate interplay between syntax and phonetics."
   ],
   "p1": 4258,
   "pn": 4262,
   "doi": "10.21437/Interspeech.2024-2190",
   "url": "interspeech_2024/lee24l_interspeech.html"
  },
  "fan24c_interspeech": {
   "authors": [
    [
     "Chaofei",
     "Fan"
    ],
    [
     "Jaimie M.",
     "Henderson"
    ],
    [
     "Chris",
     "Manning"
    ],
    [
     "Francis R.",
     "Willett"
    ]
   ],
   "title": "Towards a Quantitative Analysis of Coarticulation with a Phoneme-to-Articulatory Model",
   "original": "2191",
   "order": 635,
   "page_count": 5,
   "abstract": [
    "Prior coarticulation studies focus mainly on limited phonemic sequences and specific articulators, providing only approximate descriptions of the temporal extent and magnitude of coarticulation. This paper is an initial attempt to comprehensively investigate coarticulation. We leverage existing Electromagnetic Articulography (EMA) datasets to develop and train a phoneme-to-articulatory (P2A) model that can generate realistic EMA for novel phoneme sequences and replicate known coarticulation patterns. We use model-generated EMA on 9K minimal word pairs to analyze coarticulation magnitude and extent up to eight phonemes from the coarticulation trigger, and compare coarticulation resistance across different consonants. Our findings align with earlier studies and suggest a longer-range coarticulation effect than previously found. This model-based approach can potentially compare coarticulation between adults and children and across languages, offering new insights into speech production."
   ],
   "p1": 3095,
   "pn": 3099,
   "doi": "10.21437/Interspeech.2024-2191",
   "url": "interspeech_2024/fan24c_interspeech.html"
  },
  "ortiztandazo24_interspeech": {
   "authors": [
    [
     "Angelo",
     "Ortiz Tandazo"
    ],
    [
     "Thomas",
     "Schatz"
    ],
    [
     "Thomas",
     "Hueber"
    ],
    [
     "Emmanuel",
     "Dupoux"
    ]
   ],
   "title": "Simulating articulatory trajectories with phonological feature interpolation",
   "original": "2192",
   "resource": "https://doi.org/10.5281/zenodo.12796067",
   "order": 735,
   "page_count": 5,
   "abstract": [
    "As a first step towards a complete computational model of speech learning involving perception-production loops, we investigate the forward mapping between pseudo-motor commands and articulatory trajectories. Two phonological feature sets, based respectively on generative and articulatory phonology, are used to encode a phonetic target sequence. Different interpolation techniques are compared to generate smooth trajectories in these feature spaces, with a potential optimisation of the target value and timing to capture co-articulation effects. We report the Pearson correlation between a linear projection of the generated trajectories and articulatory data derived from a multi-speaker dataset of electromagnetic articulography (EMA) recordings. A correlation of 0.67 is obtained with an extended feature set based on generative phonology and a linear interpolation technique. We discuss the implications of our results for our understanding of the dynamics of biological motion."
   ],
   "p1": 3595,
   "pn": 3599,
   "doi": "10.21437/Interspeech.2024-2192",
   "url": "interspeech_2024/ortiztandazo24_interspeech.html"
  },
  "srivastava24_interspeech": {
   "authors": [
    [
     "Tejes",
     "Srivastava"
    ],
    [
     "Jiatong",
     "Shi"
    ],
    [
     "William",
     "Chen"
    ],
    [
     "Shinji",
     "Watanabe"
    ]
   ],
   "title": "EFFUSE: Efficient Self-Supervised Feature Fusion for E2E ASR in Low Resource and Multilingual Scenarios",
   "original": "2199",
   "order": 821,
   "page_count": 5,
   "abstract": [
    "Self-Supervised Learning (SSL) models have demonstrated exceptional performance in various speech tasks, particularly in low-resource and multilingual domains. Recent works show that fusing diverse SSL models could achieve superior performance compared to using one SSL model. However, fusing models increases the overall parameter size, leading to higher computational costs. We propose EFFUSE, a novel approach that uses a single SSL model to mimic the features of multiple SSL models via prediction, resulting in a lightweight framework with competitive performance. Our experiments show that EFFUSE outperforms individual SSL models in multilingual speech recognition tasks. Our best performing model achieves an average SUPERB score increase of 63.5 (6.3%) from the SSL baselines in Multilingual Speech Universal PERformance Benchmark (ML-SUPERB), while decreasing parameter size on average by 317M parameters (49%) from the fusion models."
   ],
   "p1": 3989,
   "pn": 3993,
   "doi": "10.21437/Interspeech.2024-2199",
   "url": "interspeech_2024/srivastava24_interspeech.html"
  },
  "meyer24b_interspeech": {
   "authors": [
    [
     "David",
     "Meyer"
    ],
    [
     "Eitan",
     "Abecassis"
    ],
    [
     "Clara",
     "Fernandez-Labrador"
    ],
    [
     "Christopher",
     "Schroers"
    ]
   ],
   "title": "RAST: A Reference-Audio Synchronization Tool for Dubbed Content",
   "original": "2203",
   "order": 15,
   "page_count": 5,
   "abstract": [
    "In the film industry, audio-video synchronization issues are considered major quality defects and key drivers of viewer disengagement. This is especially true for dubbed content, which is more prone to these errors due to the added manual process of replacing the original speech with a translated version. Despite their potential benefit for dubbed media production, automatic sync detection methods are seldom explored. In this paper, we propose a Transformer-based Siamese network for dubbed audio synchronization detection. Based on a large dataset of dubbed entertainment, we demonstrate that, compared to previous methods, our approach is more robust in detecting the misalignment introduced by translated speech segments. While our method addresses the previously studied constant synchronization errors, our model is the first to handle the frequent issue of intermittent offsets."
   ],
   "p1": 67,
   "pn": 71,
   "doi": "10.21437/Interspeech.2024-2203",
   "url": "interspeech_2024/meyer24b_interspeech.html"
  },
  "morozova24_interspeech": {
   "authors": [
    [
     "Natalia",
     "Morozova"
    ],
    [
     "Guanghao",
     "You"
    ],
    [
     "Sabine",
     "Stoll"
    ],
    [
     "Adrian",
     "Bangerter"
    ]
   ],
   "title": "Measuring acoustic dissimilarity of hierarchical markers in task-oriented dialogue with MFCC-based dynamic time warping",
   "original": "2204",
   "order": 977,
   "page_count": 5,
   "abstract": [
    "Joint activities (e.g. building a LEGO model) unfold in a hierarchy of subprojects. Navigating them implies horizontally elaborating on a subproject (placing one block) and vertically moving to a new subproject (next block). Interactants coordinate horizontal and vertical transitions with project markers (okay, yeah). We suggest that vertical vs. horizontal transitions are distinguished both lexically and acoustically. We predicted that acoustic features of identical markers used for different transitions (okay-vertical vs. okay-horizontal) would exhibit more dissimilarity than markers used for same transitions (okay-vertical vs. okay-vertical). We used MFCC-based dynamic time warping to measure dissimilarity between vocalisations and analysed them with a Bayesian regression model. We find that Vietnamese speakers use both lexical and acoustic cues to mark transitions, and paired same-horizontal markers are acoustically more similar than same-vertical and different-transition markers."
   ],
   "p1": 4763,
   "pn": 4767,
   "doi": "10.21437/Interspeech.2024-2204",
   "url": "interspeech_2024/morozova24_interspeech.html"
  },
  "escobargrisales24_interspeech": {
   "authors": [
    [
     "Daniel",
     "Escobar-Grisales"
    ],
    [
     "Cristian David",
     "Ríos-Urrego"
    ],
    [
     "Ilja",
     "Baumann"
    ],
    [
     "Korbinian",
     "Riedhammer"
    ],
    [
     "Elmar",
     "Noeth"
    ],
    [
     "Tobias",
     "Bocklet"
    ],
    [
     "Adolfo M.",
     "Garcia"
    ],
    [
     "Juan Rafael",
     "Orozco-Arroyave"
    ]
   ],
   "title": "It’s Time to Take Action: Acoustic Modeling of Motor Verbs to Detect Parkinson’s Disease",
   "original": "2205",
   "order": 402,
   "page_count": 5,
   "abstract": [
    "Pre-trained models generate speech representations that are used in different tasks, including the automatic detection of Parkinson’s disease (PD). Although these models can yield high accuracy, their interpretation is still challenging. This paper used a pre-trained Wav2vec 2.0 model to represent speech frames of 25ms length and perform a frame-by-frame discrimination between PD patients and healthy control (HC) subjects. This fine granularity prediction enabled us to identify specific linguistic segments with high discrimination capability. Speech representations of all produced verbs were compared w.r.t. nouns and the first ones yielded higher accuracies. To gain a deeper understanding of this pattern, representations of motor and non-motor verbs were compared and the first ones yielded better results, with accuracies of around 83% in an independent test set. These findings support well-established neurocognitive models about action-related language highlighted as key drivers of PD."
   ],
   "p1": 1965,
   "pn": 1969,
   "doi": "10.21437/Interspeech.2024-2205",
   "url": "interspeech_2024/escobargrisales24_interspeech.html"
  },
  "fathan24_interspeech": {
   "authors": [
    [
     "Abderrahim",
     "Fathan"
    ],
    [
     "Xiaolin",
     "Zhu"
    ],
    [
     "Jahangir",
     "Alam"
    ]
   ],
   "title": "On the impact of several regularization techniques on label noise robustness of self-supervised speaker verification systems",
   "original": "2206",
   "order": 550,
   "page_count": 5,
   "abstract": [
    "Clustering-based Pseudo-Labels (PLs) are widely used to optimize Speaker Embedding (SE) networks and train Self-Supervised (SS) Speaker Verification (SV) systems. However, this SS training scheme relies on highly accurate PLs. In this paper, we perform a large investigative study of the effect of several regularization techniques (mixup, label smoothing, employing sub-centers) on the label noise robustness of SSSV systems. We study these techniques and apply them on various recent metric learning loss functions for better generalization of  SSSV systems. In particular, we investigate the effect of these losses and regularizations on the robustness of the self-supervised SV task against label noise using the CAMSAT clustering model to generate PLs. We provide a thorough comparative analysis of the performance of these techniques using different numbers of clusters and show that some of them are effective against label noise and lead to considerable improvements in SV performance."
   ],
   "p1": 2670,
   "pn": 2674,
   "doi": "10.21437/Interspeech.2024-2206",
   "url": "interspeech_2024/fathan24_interspeech.html"
  },
  "krishnan24_interspeech": {
   "authors": [
    [
     "Aravind",
     "Krishnan"
    ],
    [
     "Badr M.",
     "Abdullah"
    ],
    [
     "Dietrich",
     "Klakow"
    ]
   ],
   "title": "On the Encoding of Gender in Transformer-based ASR Representations",
   "original": "2209",
   "order": 634,
   "page_count": 5,
   "abstract": [
    "While existing literature relies on performance differences to uncover gender biases in ASR models, a deeper analysis is essential to understand how gender is encoded and utilized during transcript generation. This work investigates the encoding and utilization of gender in the latent representations of two transformer-based ASR models, Wav2Vec2 and HuBERT. Using linear erasure, we demonstrate the feasibility of removing gender information from each layer of an ASR model and show that such an intervention has minimal impacts on the ASR performance. Additionally, our analysis reveals a concentration of gender information within the first and last frames in the final layers, explaining the ease of erasing gender in these layers. Our findings suggest the prospect of creating gender-neutral embeddings that can be integrated into ASR frameworks without compromising their efficacy."
   ],
   "p1": 3090,
   "pn": 3094,
   "doi": "10.21437/Interspeech.2024-2209",
   "url": "interspeech_2024/krishnan24_interspeech.html"
  },
  "zheng24d_interspeech": {
   "authors": [
    [
     "Xianrui",
     "Zheng"
    ],
    [
     "Guangzhi",
     "Sun"
    ],
    [
     "Chao",
     "Zhang"
    ],
    [
     "Philip C.",
     "Woodland"
    ]
   ],
   "title": "SOT Triggered Neural Clustering for Speaker Attributed ASR",
   "original": "2211",
   "order": 145,
   "page_count": 5,
   "abstract": [
    "This paper introduces a novel approach to speaker-attributed ASR transcription using a neural clustering method. With a parallel processing mechanism, diarisation and ASR can be applied simultaneously, helping to prevent the accumulation of errors from one sub-system to the next in a cascaded system. This is achieved by the use of ASR, trained using a serialised output training method, together with segment-level discriminative neural clustering (SDNC) to assign speaker labels. With SDNC, our system does not require an extra non-neural clustering method to assign speaker labels, thus allowing the entire system to be based on neural networks. Experimental results on the AMI meeting dataset demonstrate that SDNC outperforms spectral clustering (SC) by a 19% relative diarisation error rate (DER) reduction on the AMI Eval set. When compared with the cascaded system with SC, the parallel system with SDNC gives a 7%/4% relative improvement in cpWER on the Dev/Eval set."
   ],
   "p1": 717,
   "pn": 721,
   "doi": "10.21437/Interspeech.2024-2211",
   "url": "interspeech_2024/zheng24d_interspeech.html"
  },
  "reitsema24_interspeech": {
   "authors": [
    [
     "Ariëlle",
     "Reitsema"
    ],
    [
     "Chenxin",
     "Li"
    ],
    [
     "Leanne",
     "van Lambalgen"
    ],
    [
     "Laura",
     "Preining"
    ],
    [
     "Saskia",
     "Galindo Jong"
    ],
    [
     "Qing",
     "Yang"
    ],
    [
     "Xinyi",
     "Wen"
    ],
    [
     "Yiya",
     "Chen"
    ]
   ],
   "title": "Perceptual Learning in Lexical Tone: Phonetic Similarity vs. Phonological Categories",
   "original": "2212",
   "order": 874,
   "page_count": 5,
   "abstract": [
    "In speech comprehension, listeners recalibrate their interpretation of variable speech signals through exposure and disambiguating information. Recalibration is attested both segmentally and suprasegmentally, but little is known about what constrains it in lexical tone. This project investigated the effects of phonological categories and phonetic similarity on perceptual learning. We exposed Chinese listeners to pitch contours ambiguous between two tone categories (realised with a level or rising pitch contour) and lexically biased their perception to one interpretation. Crucially, the rising pitch contour could be from two different phonological tone categories. Perceptual learning was observed not only in the rising contours used for exposure but also across phonetically similar but phonologically different rising contours, suggesting that perceptual learning in tone is not constrained by phonological tone categories but is facilitated by the phonetic similarity of pitch contours."
   ],
   "p1": 4248,
   "pn": 4252,
   "doi": "10.21437/Interspeech.2024-2212",
   "url": "interspeech_2024/reitsema24_interspeech.html"
  },
  "kang24d_interspeech": {
   "authors": [
    [
     "Wonjune",
     "Kang"
    ],
    [
     "Deb",
     "Roy"
    ]
   ],
   "title": "Prompting Large Language Models with Audio for General-Purpose Speech Summarization",
   "original": "2213",
   "order": 400,
   "page_count": 5,
   "abstract": [
    "In this work, we introduce a framework for speech summarization that leverages the processing and reasoning capabilities of large language models (LLMs). We propose an end-to-end system that combines an instruction-tuned LLM with an audio encoder that converts speech into token representations that the LLM can interpret. Using a dataset with paired speech-text data, the overall system is trained to generate consistent responses to prompts with the same semantic information regardless of the input modality. The resulting framework allows the LLM to process speech inputs in the same way as text, enabling speech summarization by simply prompting the LLM. Unlike prior approaches, our method is able to summarize spoken content from any arbitrary domain, and it can produce summaries in different styles by varying the LLM prompting strategy. Experiments demonstrate that our approach outperforms a cascade baseline of speech recognition followed by LLM text processing."
   ],
   "p1": 1955,
   "pn": 1959,
   "doi": "10.21437/Interspeech.2024-2213",
   "url": "interspeech_2024/kang24d_interspeech.html"
  },
  "li24pa_interspeech": {
   "authors": [
    [
     "Zehua Kcriss",
     "Li"
    ],
    [
     "Meiying Melissa",
     "Chen"
    ],
    [
     "Yi",
     "Zhong"
    ],
    [
     "Pinxin",
     "Liu"
    ],
    [
     "Zhiyao",
     "Duan"
    ]
   ],
   "title": "GTR-Voice: Articulatory Phonetics Informed Controllable Expressive Speech Synthesis",
   "original": "2216",
   "order": 364,
   "page_count": 5,
   "abstract": [
    "Expressive speech synthesis aims to generate speech that captures a wide range of para-linguistic features, including emotion and articulation, though current research primarily emphasizes emotional aspects over the nuanced articulatory features mastered by professional voice actors. Inspired by this, we explore expressive speech synthesis through the lens of articulatory phonetics. Specifically, we define a framework with three dimensions: Glottalization, Tenseness, and Resonance (GTR), to guide the synthesis at the voice production level. With this framework, we record a high-quality speech dataset named GTR-Voice, featuring 20 Chinese sentences articulated by a professional voice actor across 125 distinct GTR combinations. We verify the framework and GTR annotations through automatic classification and listening tests, and demonstrate precise controllability along the GTR dimensions on two fine-tuned expressive TTS models. We open-source the dataset and TTS models."
   ],
   "p1": 1775,
   "pn": 1779,
   "doi": "10.21437/Interspeech.2024-2216",
   "url": "interspeech_2024/li24pa_interspeech.html"
  },
  "shahin24_interspeech": {
   "authors": [
    [
     "Mostafa",
     "Shahin"
    ],
    [
     "Beena",
     "Ahmed"
    ]
   ],
   "title": "Phonological-Level Mispronunciation Detection and Diagnosis",
   "original": "2217",
   "order": 63,
   "page_count": 5,
   "abstract": [
    "The automatic identification and analysis of pronunciation errors, known as mispronunciation detection and diagnosis (MDD), is vital in computer-aided pronunciation learning (CAPL) tools for second-language (L2) learning. Existing MDD methods focus on analyzing phonemes, but they can only detect categorical errors for phonemes with sufficient training data. Due to the unpredictable nature of non-native speakers’ pronunciation errors and limited training datasets, modelling all mispronunciations becomes impractical. Additionally, phoneme-level MDD approaches provide limited diagnostic information. In our proposed approach, we detect phonological features, breaking down phoneme production into elementary components related to the articulatory system, offering more informative feedback to learners. Applied to L2 English speech data, it outperformed traditional phoneme-level methods, reducing false acceptance rate (FAR), false rejection rate (FRR), and diagnostic error rate (DER)."
   ],
   "p1": 307,
   "pn": 311,
   "doi": "10.21437/Interspeech.2024-2217",
   "url": "interspeech_2024/shahin24_interspeech.html"
  },
  "takagi24_interspeech": {
   "authors": [
    [
     "Tatsunari",
     "Takagi"
    ],
    [
     "Yukoh",
     "Wakabayashi"
    ],
    [
     "Atsunori",
     "Ogawa"
    ],
    [
     "Norihide",
     "Kitaoka"
    ]
   ],
   "title": "Text-only Domain Adaptation for CTC-based Speech Recognition through Substitution of Implicit Linguistic Information in the Search Space",
   "original": "2222",
   "order": 59,
   "page_count": 5,
   "abstract": [
    "Domain adaptation using only language models in Automatic Speech Recognition (ASR) has been widely studied because of its practicality. Still, it remains challenging for non-autoregressive ASR models such as Connectionist Temporal Classification (CTC)-based ones. Against this background, this study addresses a text-only domain adaptation method for CTC-based ASR models by leveraging the Density Ratio Approach (DRA). Our method combines a beam search algorithm for substituting linguistic information in DRA, accommodated to the CTC decoding procedure, and a language model adaptation method considered the conditional independence assumption of CTC. We conducted domain adaptation experiments for character-level ASR with the Corpus of Spontaneous Japanese (CSJ) and sub-word ASR with the English-language LibriSpeech and GigaSpeech corpora. The experimental results confirmed that our proposed method achieved improved accuracy in Japanese and English compared to the Shallow Fusion method."
   ],
   "p1": 287,
   "pn": 291,
   "doi": "10.21437/Interspeech.2024-2222",
   "url": "interspeech_2024/takagi24_interspeech.html"
  },
  "jain24_interspeech": {
   "authors": [
    [
     "Rishi",
     "Jain"
    ],
    [
     "Bohan",
     "Yu"
    ],
    [
     "Peter",
     "Wu"
    ],
    [
     "Tejas",
     "Prabhune"
    ],
    [
     "Gopala",
     "Anumanchipalli"
    ]
   ],
   "title": "Multimodal Segmentation for Vocal Tract Modeling",
   "original": "2223",
   "resource": "https://doi.org/10.5281/zenodo.12789209",
   "order": 86,
   "page_count": 5,
   "abstract": [
    "Accurate modeling of the vocal tract is necessary to construct articulatory representations for interpretable speech processing and linguistics. However, vocal tract modeling is challenging because many internal articulators are occluded from external motion capture technologies. Real-time magnetic resonance imaging (RT-MRI) allows measuring precise movements of internal articulators during speech, but annotated datasets of MRI are limited in size due to time-consuming and computationally expensive labeling methods. We first present a deep labeling strategy for the RT-MRI video using a vision-only segmentation approach. We then introduce a multimodal algorithm using audio to improve segmentation of vocal articulators. Together, we set a new benchmark for vocal tract modeling in MRI video segmentation and use this to release labels for a 75-speaker RT-MRI dataset, increasing the amount of labeled public RT-MRI data of the vocal tract by over a factor of 9. The code and dataset labels can be found at rishiraij.github.io/multimodal-mri-avatar/."
   ],
   "p1": 422,
   "pn": 426,
   "doi": "10.21437/Interspeech.2024-2223",
   "url": "interspeech_2024/jain24_interspeech.html"
  },
  "premananth24_interspeech": {
   "authors": [
    [
     "Gowtham",
     "Premananth"
    ],
    [
     "Yashish M.",
     "Siriwardena"
    ],
    [
     "Philip",
     "Resnik"
    ],
    [
     "Sonia",
     "Bansal"
    ],
    [
     "Deanna",
     "L.Kelly"
    ],
    [
     "Carol",
     "Espy-Wilson"
    ]
   ],
   "title": "A Multimodal Framework for the Assessment of the Schizophrenia Spectrum",
   "original": "2224",
   "order": 303,
   "page_count": 5,
   "abstract": [
    "This paper presents a novel multimodal framework to distinguish between different symptom classes of subjects in the schizophrenia spectrum and healthy controls using audio, video, and text modalities. We implemented Convolution Neural Network and Long Short Term Memory based unimodal models and experimented on various multimodal fusion approaches to come up with the proposed framework. We utilized a minimal Gated multimodal unit (mGMU) to obtain a bi-modal intermediate fusion of the features extracted from the input modalities before finally fusing the outputs of the bimodal fusions to perform subject-wise classifications. The use of mGMU units in the multimodal framework improved the performance in both weighted f1-score and weighted AUC-ROC scores."
   ],
   "p1": 1470,
   "pn": 1474,
   "doi": "10.21437/Interspeech.2024-2224",
   "url": "interspeech_2024/premananth24_interspeech.html"
  },
  "phukan24b_interspeech": {
   "authors": [
    [
     "Orchid Chetia",
     "Phukan"
    ],
    [
     "Gautam Siddharth",
     "Kashyap"
    ],
    [
     "Arun Balaji",
     "Buduru"
    ],
    [
     "Rajesh",
     "Sharma"
    ]
   ],
   "title": "Are Paralinguistic Representations all that is needed for Speech Emotion Recognition?",
   "original": "2233",
   "order": 964,
   "page_count": 5,
   "abstract": [
    "Availability of representations from pre-trained models (PTMs) have facilitated substantial progress in speech emotion recognition (SER). Particularly, representations from PTM trained for paralinguistic speech processing have shown state-of-the-art (SOTA) performance for SER. However, such paralinguistic PTM representations haven’t been evaluated for SER in linguistic environments other than English. Also, paralinguistic PTM representations haven’t been investigated in benchmarks such as SUPERB, EMO-SUPERB, ML-SUPERB for SER. This makes it difficult to access the efficacy of paralinguistic PTM representations for SER in multiple languages. To fill this gap, we perform a comprehensive comparative study of five SOTA PTM representations. Our results shows that paralinguistic PTM (TRILLsson) representations performs the best and this performance can be attributed to its effectiveness in capturing pitch, tone and other speech characteristics more effectively than other PTM representations."
   ],
   "p1": 4698,
   "pn": 4702,
   "doi": "10.21437/Interspeech.2024-2233",
   "url": "interspeech_2024/phukan24b_interspeech.html"
  },
  "lovelace24_interspeech": {
   "authors": [
    [
     "Justin",
     "Lovelace"
    ],
    [
     "Soham",
     "Ray"
    ],
    [
     "Kwangyoun",
     "Kim"
    ],
    [
     "Kilian Q.",
     "Weinberger"
    ],
    [
     "Felix",
     "Wu"
    ]
   ],
   "title": "Sample-Efficient Diffusion for Text-To-Speech Synthesis",
   "original": "2235",
   "order": 905,
   "page_count": 5,
   "abstract": [
    "This work introduces Sample-Efficient Speech Diffusion (SESD), an algorithm for effective speech synthesis in modest data regimes through latent diffusion. It is based on a novel diffusion architecture, that we call U-Audio Transformer (U-AT), that efficiently scales to long sequences and operates in the latent space of a pre-trained audio autoencoder. Conditioned on character-aware language model representations, SESD achieves impressive results despite training on less than 1k hours of speech – far less than current state-of-the-art systems. In fact, it synthesizes more intelligible speech than thestate-of-the-art auto-regressive model, VALL-E, while using less than 2% the training data. Our implementation is available at https://github.com/justinlovelace/SESD."
   ],
   "p1": 4403,
   "pn": 4407,
   "doi": "10.21437/Interspeech.2024-2235",
   "url": "interspeech_2024/lovelace24_interspeech.html"
  },
  "ariasvergara24_interspeech": {
   "authors": [
    [
     "Tomas",
     "Arias-Vergara"
    ],
    [
     "Paula Andrea",
     "Pérez-Toro"
    ],
    [
     "Xiaofeng",
     "Liu"
    ],
    [
     "Fangxu",
     "Xing"
    ],
    [
     "Maureen",
     "Stone"
    ],
    [
     "Jiachen",
     "Zhuo"
    ],
    [
     "Jerry L.",
     "Prince"
    ],
    [
     "Maria",
     "Schuster"
    ],
    [
     "Elmar",
     "Noeth"
    ],
    [
     "Jonghye",
     "Woo"
    ],
    [
     "Andreas",
     "Maier"
    ]
   ],
   "title": "Contrastive Learning Approach for Assessment of Phonological Precision in Patients with Tongue Cancer Using MRI Data",
   "original": "2236",
   "order": 187,
   "page_count": 5,
   "abstract": [
    "Magnetic Resonance Imaging (MRI) allows analyzing speech production by capturing high-resolution images of the dynamic processes in the vocal tract. In clinical applications, combining MRI with synchronized speech recordings leads to improved patient outcomes, especially if a phonological-based approach is used for assessment. However, when audio signals are unavailable, the recognition accuracy of sounds is decreased when using only MRI data. We propose a contrastive learning approach to improve the detection of phonological classes from MRI data when acoustic signals are not available at inference time. We demonstrate that frame-wise recognition of phonological classes improves from an f1 of 0.74 to 0.85 when the contrastive loss approach is implemented. Furthermore, we show the utility of our approach in the clinical application of using such phonological classes to assess speech disorders in patients with tongue cancer, yielding promising results in the recognition task."
   ],
   "p1": 927,
   "pn": 931,
   "doi": "10.21437/Interspeech.2024-2236",
   "url": "interspeech_2024/ariasvergara24_interspeech.html"
  },
  "kodali24_interspeech": {
   "authors": [
    [
     "Manila",
     "Kodali"
    ],
    [
     "Sudarsana Reddy",
     "Kadiri"
    ],
    [
     "Paavo",
     "Alku"
    ]
   ],
   "title": "Fine-tuning of Pre-trained Models for Classification of Vocal Intensity Category from Speech Signals",
   "original": "2237",
   "order": 98,
   "page_count": 5,
   "abstract": [
    "Speakers regulate vocal intensity on many occasions for example to be heard over a long distance or to express vocal emotions. Humans can regulate vocal intensity over a wide sound pressure level (SPL) range  and therefore speech can be categorized into different vocal intensity categories. Recent machine learning experiments have studied classification of vocal intensity category from speech signals which have been recorded without SPL information and which are represented on arbitrary amplitude scales. By fine-tuning four pre-trained models (wav2vec2-BASE, wav2vec2-LARGE, HuBERT, audio speech transformers), this paper studies classification of speech into four intensity categories (soft, normal, loud, very loud), when speech is presented on such arbitrary amplitude scale. The fine-tuned model embeddings showed absolute improvements of 5% and 10-12% in accuracy compared to baselines for the target intensity category label and the SPL-based intensity category label, respectively."
   ],
   "p1": 482,
   "pn": 486,
   "doi": "10.21437/Interspeech.2024-2237",
   "url": "interspeech_2024/kodali24_interspeech.html"
  },
  "kadambi24_interspeech": {
   "authors": [
    [
     "Prad",
     "Kadambi"
    ],
    [
     "Tristan",
     "Mahr"
    ],
    [
     "Lucas",
     "Annear"
    ],
    [
     "Henry",
     "Nomeland"
    ],
    [
     "Julie",
     "Liss"
    ],
    [
     "Katherine",
     "Hustad"
    ],
    [
     "Visar",
     "Berisha"
    ]
   ],
   "title": "How Does Alignment Error Affect Automated Pronunciation Scoring in Children's Speech?",
   "original": "2239",
   "order": 1051,
   "page_count": 5,
   "abstract": [
    "Automated goodness of pronunciation scores measure deviation from typical adult speech by first phonetically segmenting speech using forced alignment and then computing phoneme likelihoods. Care must be taken to distinguish between the impact of alignment error (a spurious signal) and true acoustic deviation on the automated score. Using mixed effects modeling, we predict ∆PLLR, the difference between pronunciation scores computed using manual alignment (PLLRm) versus computed using automatic forced alignments (PLLRa). Pronunciation deviations and alignment error are both magnified in children’s speech and may be influenced by factors such as phoneme position and phoneme type. Our methodology shows that alignment error has a moderate effect on ∆PLLR, and other variables have small to no effect. Manual PLLR closely matches automatically calculated PLLR following cross utterance averaging. Thus, practical comparisons between child speakers should be very comparable across the two methods."
   ],
   "p1": 5133,
   "pn": 5137,
   "doi": "10.21437/Interspeech.2024-2239",
   "url": "interspeech_2024/kadambi24_interspeech.html"
  },
  "xu24j_interspeech": {
   "authors": [
    [
     "Chenzi",
     "Xu"
    ],
    [
     "Jessica",
     "Wormald"
    ],
    [
     "Paul",
     "Foulkes"
    ],
    [
     "Philip",
     "Harrison"
    ],
    [
     "Vincent",
     "Hughes"
    ],
    [
     "Poppy",
     "Welch"
    ],
    [
     "Finnian",
     "Kelly"
    ],
    [
     "David",
     "van der Vloed"
    ]
   ],
   "title": "Voice quality in telephone speech: Comparing acoustic measures between VoIP telephone and high-quality recordings",
   "original": "2240",
   "order": 323,
   "page_count": 5,
   "abstract": [
    "Implementing objective voice quality analysis in a forensic context is challenging. Forensic samples often involve telephone transmission, yet little is known about the impact of telecommunication channels on the acoustic measures of voice quality. This study compares the acoustics of laryngeal voice qualities (breathy, creaky, and modal) in controlled production of continuous English speech under two recording conditions: studio (headband microphone) and VoIP (simultaneously over a telephone line). A wide range of voice quality measures were extracted, including spectral tilts and harmonics-to-noise ratios, cepstral peak prominence (CPP), f0, and formants. Through comparative acoustic and linear discriminant analysis, this study identifies measures susceptible to recording conditions and those that robustly contribute to the differentiation of voice qualities in telephone recordings. Harmonic amplitudes H1H2c and H1c, CPP, and f0 are most reliable voice quality measures across studio and VoIP conditions."
   ],
   "p1": 1570,
   "pn": 1574,
   "doi": "10.21437/Interspeech.2024-2240",
   "url": "interspeech_2024/xu24j_interspeech.html"
  },
  "zang24_interspeech": {
   "authors": [
    [
     "Yongyi",
     "Zang"
    ],
    [
     "Jiatong",
     "Shi"
    ],
    [
     "You",
     "Zhang"
    ],
    [
     "Ryuichi",
     "Yamamoto"
    ],
    [
     "Jionghao",
     "Han"
    ],
    [
     "Yuxun",
     "Tang"
    ],
    [
     "Shengyuan",
     "Xu"
    ],
    [
     "Wenxiao",
     "Zhao"
    ],
    [
     "Jing",
     "Guo"
    ],
    [
     "Tomoki",
     "Toda"
    ],
    [
     "Zhiyao",
     "Duan"
    ]
   ],
   "title": "CtrSVDD: A Benchmark Dataset and Baseline Analysis for Controlled Singing Voice Deepfake Detection",
   "original": "2242",
   "order": 981,
   "page_count": 5,
   "abstract": [
    "Recent singing voice synthesis and conversion advancements necessitate robust singing voice deepfake detection (SVDD) models. Current SVDD datasets face challenges due to limited controllability, diversity in deepfake methods, and licensing restrictions. Addressing these gaps, we introduce CtrSVDD, a large-scale, diverse collection of bonafide and deepfake singing vocals. These vocals are synthesized using state-of-the-art methods from publicly accessible singing voice datasets. CtrSVDD includes 47.64 hours of bonafide and 260.34 hours of deepfake singing vocals, spanning 14 deepfake methods and involving 164 singer identities. We also present a baseline system with flexible front-end features, evaluated against a structured train/dev/eval split. The experiments show the importance of feature selection and highlight a need for generalization towards deepfake methods that deviate further from training distribution. The CtrSVDD dataset and baseline model weights are publicly accessible."
   ],
   "p1": 4783,
   "pn": 4787,
   "doi": "10.21437/Interspeech.2024-2242",
   "url": "interspeech_2024/zang24_interspeech.html"
  },
  "liu24o_interspeech": {
   "authors": [
    [
     "Xi",
     "Liu"
    ],
    [
     "John H.L.",
     "Hansen"
    ]
   ],
   "title": "DNN-based monaural speech enhancement using alternate analysis windows for phase and magnitude modification",
   "original": "2244",
   "order": 350,
   "page_count": 5,
   "abstract": [
    "In recent decades, considerable research has been devoted to speech enhancement leveraging the short-term Fourier transform (STFT) analysis. As speech processing technology evolves, the significance of phase information in enhancing speech intelligibility becomes more noticeable. Typically, the Hanning window has been widely employed as analysis window in STFT. In this study, we propose the Chebyshev window for phase analysis, and the Hanning window for magnitude analysis. Next, we introduce a novel cepstral domain enhancement approach designed to robustly reinforce the harmonic structure of speech. The performance of our model is evaluated using the DNS challenge test set as well as the naturalistic APOLLO Fearless Steps evaluation set. Experimental results demonstrate that the Chebyshev-based phase solution outperforms the Hanning option for in phase-aware speech enhancement. Furthermore, the incorporation of quefrency emphasis proves effective in enhancing overall speech quality."
   ],
   "p1": 1705,
   "pn": 1709,
   "doi": "10.21437/Interspeech.2024-2244",
   "url": "interspeech_2024/liu24o_interspeech.html"
  },
  "mujtaba24_interspeech": {
   "authors": [
    [
     "Dena",
     "Mujtaba"
    ],
    [
     "Nihar R.",
     "Mahapatra"
    ],
    [
     "Megan",
     "Arney"
    ],
    [
     "J. Scott",
     "Yaruss"
    ],
    [
     "Caryn",
     "Herring"
    ],
    [
     "Jia",
     "Bin"
    ]
   ],
   "title": "Inclusive ASR for Disfluent Speech: Cascaded Large-Scale Self-Supervised Learning with Targeted Fine-Tuning and Data Augmentation",
   "original": "2246",
   "order": 264,
   "page_count": 5,
   "abstract": [
    "Automatic speech recognition (ASR) systems often falter while processing stuttering-related disfluencies---such as involuntary blocks and word repetitions---yielding inaccurate transcripts.  A critical barrier to progress is the scarcity of large, annotated disfluent speech datasets. Therefore, we present an inclusive ASR design approach, leveraging large-scale self-supervised learning on standard speech followed by targeted fine-tuning and data augmentation on a smaller, curated dataset of disfluent speech. Our data augmentation technique enriches training datasets with various disfluencies, enhancing ASR processing of these speech patterns. Results show that fine-tuning wav2vec 2.0 with even a relatively small, labeled dataset, alongside data augmentation, can significantly reduce word error rates for disfluent speech. Our approach not only advances ASR inclusivity for people who stutter, but also paves the way for ASRs that can accommodate wider speech variations."
   ],
   "p1": 1275,
   "pn": 1279,
   "doi": "10.21437/Interspeech.2024-2246",
   "url": "interspeech_2024/mujtaba24_interspeech.html"
  },
  "chan24_interspeech": {
   "authors": [
    [
     "May Pik Yu",
     "Chan"
    ],
    [
     "Jianjing",
     "Kuang"
    ]
   ],
   "title": "Pitch-driven adjustments in tongue positions: Insights from ultrasound imaging",
   "original": "2247",
   "order": 646,
   "page_count": 5,
   "abstract": [
    "As an important singing technique, trained singers tend to adjust their resonance space based on pitch, especially for higher pitches. The mechanism behind vowel modification remains unclear - is it learned only for certain acoustic goals, or inherently required for articulatory ease? To address this question, we explore whether speakers adjust their resonance spaces when targeting different pitches; and if so, how. 23 participants participated in a production experiment involving ultrasound tongue imaging technique. Participants were asked to sing vowels across their pitch range rising by semitone. Results show that there is a link between pitch range and tongue positional adjustments - participants with a larger pitch range tended to adjust their tongue positions as they reached higher pitches, including some individuals who reported no history of vocal training. We discuss our results with respect to interactions between the source and filter in the human voice production system."
   ],
   "p1": 3150,
   "pn": 3154,
   "doi": "10.21437/Interspeech.2024-2247",
   "url": "interspeech_2024/chan24_interspeech.html"
  },
  "shi24g_interspeech": {
   "authors": [
    [
     "Jiatong",
     "Shi"
    ],
    [
     "Shih-Heng",
     "Wang"
    ],
    [
     "William",
     "Chen"
    ],
    [
     "Martijn",
     "Bartelds"
    ],
    [
     "Vanya",
     "Bannihatti Kumar"
    ],
    [
     "Jinchuan",
     "Tian"
    ],
    [
     "Xuankai",
     "Chang"
    ],
    [
     "Dan",
     "Jurafsky"
    ],
    [
     "Karen",
     "Livescu"
    ],
    [
     "Hung-yi",
     "Lee"
    ],
    [
     "Shinji",
     "Watanabe"
    ]
   ],
   "title": "ML-SUPERB 2.0: Benchmarking Multilingual Speech Models Across Modeling Constraints, Languages, and Datasets",
   "original": "2248",
   "order": 255,
   "page_count": 5,
   "abstract": [
    "ML-SUPERB evaluates self-supervised learning (SSL) models on the tasks of language identification and automatic speech recognition (ASR). This benchmark treats the models as feature extractors and uses a single shallow downstream model, which can be fine-tuned for a downstream task. However, real-world use cases may require different configurations. This paper presents ML-SUPERB 2.0, which is a new benchmark for evaluating pre-trained SSL and supervised speech models across downstream models, fine-tuning setups, and efficient model adaptation approaches. We find performance improvements over the setup of ML-SUPERB. However, performance depends on the downstream model design. Also, we find large performance differences between languages and datasets, suggesting the need for more targeted approaches to improve multilingual ASR performance."
   ],
   "p1": 1230,
   "pn": 1234,
   "doi": "10.21437/Interspeech.2024-2248",
   "url": "interspeech_2024/shi24g_interspeech.html"
  },
  "leduc24_interspeech": {
   "authors": [
    [
     "Khai",
     "Le-Duc"
    ],
    [
     "Khai-Nguyen",
     "Nguyen"
    ],
    [
     "Long",
     "Vo-Dang"
    ],
    [
     "Truong-Son",
     "Hy"
    ]
   ],
   "title": "Real-time Speech Summarization for Medical Conversations",
   "original": "2250",
   "order": 401,
   "page_count": 5,
   "abstract": [
    "In doctor-patient conversations, identifying medically relevant information is crucial, posing the need for conversation summarization. In this work, we propose the first deployable real-time speech summarization system for real-world applications in industry, which generates a local summary after every N speech utterances within a conversation and a global summary after the end of a conversation. Our system could enhance user experience from a business standpoint, while also reducing computational costs from a technical perspective. Secondly, we present VietMed-Sum which, to our knowledge, is the first speech summarization dataset for medical conversations. Thirdly, we are the first to utilize LLM and human annotators collaboratively to create gold standard and synthetic summaries for medical conversation summarization. Finally, we present baseline results of state-of-the-art models on VietMed-Sum. All code, data (English-translated and Vietnamese) and models are available online."
   ],
   "p1": 1960,
   "pn": 1964,
   "doi": "10.21437/Interspeech.2024-2250",
   "url": "interspeech_2024/leduc24_interspeech.html"
  },
  "shi24h_interspeech": {
   "authors": [
    [
     "Jiatong",
     "Shi"
    ],
    [
     "Xutai",
     "Ma"
    ],
    [
     "Hirofumi",
     "Inaguma"
    ],
    [
     "Anna",
     "Sun"
    ],
    [
     "Shinji",
     "Watanabe"
    ]
   ],
   "title": "MMM: Multi-Layer Multi-Residual Multi-Stream Discrete Speech Representation from Self-supervised Learning Model",
   "original": "2251",
   "order": 529,
   "page_count": 5,
   "abstract": [
    "Speech discrete representation has proven effective in various downstream applications due to its superior compression rate of the waveform, fast convergence during training, and compatibility with other modalities. Discrete units extracted from self-supervised learning (SSL) models have emerged as a prominent approach for obtaining speech discrete representation. However, while discrete units have shown effectiveness compared to spectral features, they still lag behind continuous SSL representations. In this work, we propose MMM, a multi-layer multi-residual multi-stream discrete units extraction method from SSL. Specifically, we introduce iterative residual vector quantization with K-means for different layers in an SSL model to extract multi-stream speech discrete representation. Through extensive experiments in speech recognition, speech resynthesis, and text-to-speech, we demonstrate the proposed MMM can surpass or on-par with neural codec's performance under various conditions."
   ],
   "p1": 2569,
   "pn": 2573,
   "doi": "10.21437/Interspeech.2024-2251",
   "url": "interspeech_2024/shi24h_interspeech.html"
  },
  "bukhari24_interspeech": {
   "authors": [
    [
     "Hazim",
     "Bukhari"
    ],
    [
     "Soham",
     "Deshmukh"
    ],
    [
     "Hira",
     "Dhamyal"
    ],
    [
     "Bhiksha",
     "Raj"
    ],
    [
     "Rita",
     "Singh"
    ]
   ],
   "title": "SELM: Enhancing Speech Emotion Recognition for Out-of-Domain Scenarios",
   "original": "2257",
   "order": 484,
   "page_count": 5,
   "abstract": [
    "Speech Emotion Recognition (SER) has been traditionally formulated as a classification task. However, emotions are generally a spectrum whose distribution varies from situation to situation leading to poor Out-of-Domain (OOD) performance. We take inspiration from statistical formulation of Automatic Speech Recognition (ASR) and formulate the SER task as generating the most likely sequence of text tokens to infer emotion. The formulation breaks SER into predicting acoustic model features weighted by language model prediction. As an instance of this approach, we present SELM, an audio-conditioned language model for SER that predicts different emotion views. We train SELM on curated speech emotion corpus and test it on three OOD datasets (RAVDESS, CREMAD, IEMOCAP) not used in training. SELM achieves significant improvements over the state-of-the-art baselines, with 17\\% and 7\\% relative accuracy gains for RAVDESS and CREMA-D, respectively. Moreover, SELM can further boost its performance by FewShot Learning using a few annotated examples. The results highlight the effectiveness of our SER formulation, especially to improve performance in OOD scenarios. "
   ],
   "p1": 2345,
   "pn": 2349,
   "doi": "10.21437/Interspeech.2024-2257",
   "url": "interspeech_2024/bukhari24_interspeech.html"
  },
  "best24_interspeech": {
   "authors": [
    [
     "Paul",
     "Best"
    ],
    [
     "Santiago",
     "Cuervo"
    ],
    [
     "Ricard",
     "Marxer"
    ]
   ],
   "title": "Transfer Learning from Whisper for Microscopic Intelligibility Prediction",
   "original": "2258",
   "order": 791,
   "page_count": 5,
   "abstract": [
    "Macroscopic intelligibility models predict the expected human word-error-rate for a given speech-in-noise stimulus. In contrast, microscopic intelligibility models aim to make fine-grained predictions about listeners' perception, e.g. predicting phonetic or lexical responses. State-of-the-art macroscopic models use transfer learning from large scale deep learning models for speech processing, whereas such methods have rarely been used for microscopic modeling. In this paper, we study the use of transfer learning from Whisper, a state-of-the-art deep learning model for automatic speech recognition, for microscopic intelligibility prediction at the level of lexical responses. Our method outperforms the considered baselines, even in a zero-shot setup, and yields a relative improvement of up to 66% when fine-tuned to predict listeners' responses. Our results showcase the promise of large scale deep learning based methods for microscopic intelligibility prediction."
   ],
   "p1": 3839,
   "pn": 3843,
   "doi": "10.21437/Interspeech.2024-2258",
   "url": "interspeech_2024/best24_interspeech.html"
  },
  "smith24_interspeech": {
   "authors": [
    [
     "Irene",
     "Smith"
    ],
    [
     "Morgan",
     "Sonderegger"
    ],
    [
     "The",
     "Spade Consortium"
    ]
   ],
   "title": "Modelled Multivariate Overlap: A method for measuring vowel merger",
   "original": "2260",
   "resource": "https://doi.org/10.5281/zenodo.12774559",
   "order": 93,
   "page_count": 5,
   "abstract": [
    "This paper introduces a novel method for quantifying vowel overlap. There is a tension in previous work between using multivariate measures, such as those derived from empirical distri- butions, and the ability to control for unbalanced data and extraneous factors, as is possible when using fitted model parameters. The method presented here resolves this tension by jointly modelling all acoustic dimensions of interest and by simulating distributions from the model to compute a measure of vowel overlap. An additional benefit of this method is that computation of uncertainty becomes straightforward. We evaluate this method on corpus speech data targeting the PIN-PEN merger in four dialects of English and find that using modelled distributions to calculate Bhattacharyya affinity substantially improves results compared to empirical distributions, while the difference between multivariate and univariate modelling is subtle."
   ],
   "p1": 457,
   "pn": 461,
   "doi": "10.21437/Interspeech.2024-2260",
   "url": "interspeech_2024/smith24_interspeech.html"
  },
  "afonja24_interspeech": {
   "authors": [
    [
     "Tejumade",
     "Afonja"
    ],
    [
     "Tobi",
     "Olatunji"
    ],
    [
     "Sewade",
     "Ogun"
    ],
    [
     "Naome A.",
     "Etori"
    ],
    [
     "Abraham",
     "Owodunni"
    ],
    [
     "Moshood",
     "Yekini"
    ]
   ],
   "title": "Performant ASR Models for Medical Entities in Accented Speech",
   "original": "2261",
   "order": 478,
   "page_count": 5,
   "abstract": [
    "Recent strides in automatic speech recognition (ASR) have accelerated their application in the medical domain where their performance on accented medical named entities (NE) such as drug names, diagnoses, and lab results, is largely unknown. We rigorously evaluate multiple ASR models on a clinical English dataset of 93 African accents. Our analysis reveals that despite some models achieving low overall word error rates (WER), errors in clinical entities are higher, potentially posing substantial risks to patient safety. To empirically demonstrate this, we extract clinical entities from transcripts, develop a novel algorithm to align ASR predictions with these entities, and compute medical NE Recall, medical WER, and character error rate. Our results show that fine-tuning on accented clinical speech improves medical WER by a wide margin (25-34 % relative), improving their practical applicability in healthcare environments."
   ],
   "p1": 2315,
   "pn": 2319,
   "doi": "10.21437/Interspeech.2024-2261",
   "url": "interspeech_2024/afonja24_interspeech.html"
  },
  "kumar24b_interspeech": {
   "authors": [
    [
     "Prakash",
     "Kumar"
    ],
    [
     "Ye",
     "Tian"
    ],
    [
     "Yongwan",
     "Lim"
    ],
    [
     "Sophia X.",
     "Cui"
    ],
    [
     "Christina",
     "Hagedorn"
    ],
    [
     "Dani",
     "Byrd"
    ],
    [
     "Uttam K.",
     "Sinha"
    ],
    [
     "Shrikanth",
     "Narayanan"
    ],
    [
     "Krishna S.",
     "Nayak"
    ]
   ],
   "title": "State-of-the-art speech production MRI protocol for new 0.55 Tesla scanners",
   "original": "2263",
   "resource": "https://doi.org/10.5281/zenodo.12751561",
   "order": 534,
   "page_count": 5,
   "abstract": [
    "Real-time magnetic resonance imaging (RT-MRI) is a safe and powerful tool for studying vocal tract dynamics during speech production. Emerging low- and mid-field MRI platforms bring new capability including higher frame rates, improved tissue contrast, and reduced blurring compared to conventional 1.5T and 3T MRI. Here, we present a state-of-the-art speech production MRI protocol tailored to the 0.55T platform. This includes 2D mid-sagittal vocal tract RT-MRI, simultaneous multislice RT-MRI with 3 parallel slices, tagged RT-MRI for functional evaluation of internal tongue deformation, biofeedback that allows participants to see their scan in real-time, and 3D static imaging. Imaging performance is comparable and, in several cases, superior to 1.5T and 3T MRI, making 0.55T an exciting new platform for speech research."
   ],
   "p1": 2590,
   "pn": 2594,
   "doi": "10.21437/Interspeech.2024-2263",
   "url": "interspeech_2024/kumar24b_interspeech.html"
  },
  "shao24_interspeech": {
   "authors": [
    [
     "Yiwen",
     "Shao"
    ],
    [
     "Shi-Xiong",
     "Zhang"
    ],
    [
     "Dong",
     "Yu"
    ]
   ],
   "title": "RIR-SF: Room Impulse Response Based Spatial Feature for Target Speech Recognition in Multi-Channel Multi-Speaker Scenarios",
   "original": "2264",
   "order": 1022,
   "page_count": 5,
   "abstract": [
    "Automatic speech recognition (ASR) on multi-talker recordings is challenging. Current methods using 3D spatial data from multi-channel audio and visual cues focus mainly on direct waves from the target speaker, overlooking reflection wave impacts, which hinders performance in reverberant environments. Our research introduces RIR-SF, a novel spatial feature based on room impulse response (RIR) that leverages the speaker's position, room acoustics, and reflection dynamics. RIR-SF significantly outperforms traditional 3D spatial features, showing superior theoretical and empirical performance. We also propose an optimized all-neural multi-channel ASR framework for RIR-SF, achieving a relative 21.3\\% reduction in CER for target speaker ASR in multi-channel settings. RIR-SF enhances recognition accuracy and demonstrates robustness in high-reverberation scenarios, overcoming the limitations of previous methods."
   ],
   "p1": 4988,
   "pn": 4992,
   "doi": "10.21437/Interspeech.2024-2264",
   "url": "interspeech_2024/shao24_interspeech.html"
  },
  "kroll24_interspeech": {
   "authors": [
    [
     "Margaret",
     "Kroll"
    ],
    [
     "Kelsey",
     "Kraus"
    ]
   ],
   "title": "Optimizing the role of human evaluation in LLM-based spoken document summarization systems",
   "original": "2268",
   "order": 396,
   "page_count": 5,
   "abstract": [
    "The emergence of powerful LLMs has led to a paradigm shift in abstractive summarization of spoken documents. The properties that make LLMs so valuable for this task -- creativity, ability to produce fluent speech, and ability to abstract information from large corpora -- also present new challenges to evaluating their content. Quick, cost-effective automatic evaluations such as ROUGE and BERTScore offer promise, but do not yet show competitive performance when compared to human evaluations. We draw on methodologies from the social sciences to propose an evaluation paradigm for spoken document summarization explicitly tailored for generative AI content. We provide detailed evaluation criteria and best practices guidelines to ensure robustness in the experimental design, replicability, and trustworthiness of human evaluation studies. We additionally include two case studies that show how these human-in-the-loop evaluation methods have been implemented at a major U.S. technology company."
   ],
   "p1": 1935,
   "pn": 1939,
   "doi": "10.21437/Interspeech.2024-2268",
   "url": "interspeech_2024/kroll24_interspeech.html"
  },
  "netzorg24_interspeech": {
   "authors": [
    [
     "Robin",
     "Netzorg"
    ],
    [
     "Alyssa",
     "Cote"
    ],
    [
     "Sumi",
     "Koshin"
    ],
    [
     "Klo Vivienne",
     "Garoute"
    ],
    [
     "Gopala Krishna",
     "Anumanchipalli"
    ]
   ],
   "title": "Speech After Gender: A Trans-Feminine Perspective on Next Steps for Speech Science and Technology",
   "original": "2269",
   "order": 631,
   "page_count": 5,
   "abstract": [
    "As experts in voice modification, trans-feminine gender-affirming voice teachers have unique perspectives on voice that confound current understandings of speaker identity. To demonstrate this, we present the Versatile Voice Dataset (VVD), a collection of three speakers modifying their voices along gendered axes. The VVD illustrates that current approaches in speaker modeling, based on categorical notions of gender and a static understanding of vocal texture, fail to account for the flexibility of the vocal tract. Utilizing publicly-available speaker embeddings, we demonstrate that gender classification systems are highly sensitive to voice modification, and speaker verification systems fail to identify voices as coming from the same speaker as voice modification becomes more drastic. As one path towards moving beyond categorical and static notions of speaker identity, we propose modeling individual qualities of vocal texture such as pitch, resonance, and weight."
   ],
   "p1": 3075,
   "pn": 3079,
   "doi": "10.21437/Interspeech.2024-2269",
   "url": "interspeech_2024/netzorg24_interspeech.html"
  },
  "veliche24_interspeech": {
   "authors": [
    [
     "Irina-Elena",
     "Veliche"
    ],
    [
     "Zhuangqun",
     "Huang"
    ],
    [
     "Vineeth",
     "Ayyat Kochaniyan"
    ],
    [
     "Fuchun",
     "Peng"
    ],
    [
     "Ozlem",
     "Kalinli"
    ],
    [
     "Michael L.",
     "Seltzer"
    ]
   ],
   "title": "Towards measuring fairness in speech recognition: Fair-Speech dataset",
   "original": "2273",
   "order": 286,
   "page_count": 5,
   "abstract": [
    "The current public datasets for speech recognition (ASR) tend not to focus specifically on the fairness aspect, such as performance across different demographic groups. This paper introduces a novel dataset, Fair-Speech, a publicly released corpus to help researchers evaluate their ASR models for accuracy across a diverse set of self-reported demographic information, such as age, gender, ethnicity, geographic variation and whether the participants consider themselves native English speakers. Our dataset includes approximately 26.5K utterances in recorded speech by 593 people in the United States, who were paid to record and submit audios of themselves saying voice commands. We also provide ASR baselines, including on models trained on transcribed and untranscribed social media videos and open source models."
   ],
   "p1": 1385,
   "pn": 1389,
   "doi": "10.21437/Interspeech.2024-2273",
   "url": "interspeech_2024/veliche24_interspeech.html"
  },
  "shao24b_interspeech": {
   "authors": [
    [
     "Yiwen",
     "Shao"
    ],
    [
     "Shi-Xiong",
     "Zhang"
    ],
    [
     "Yong",
     "Xu"
    ],
    [
     "Meng",
     "Yu"
    ],
    [
     "Dong",
     "Yu"
    ],
    [
     "Daniel",
     "Povey"
    ],
    [
     "Sanjeev",
     "Khudanpur"
    ]
   ],
   "title": "Multi-Channel Multi-Speaker ASR Using Target Speaker’s Solo Segment",
   "original": "2274",
   "order": 1023,
   "page_count": 5,
   "abstract": [
    "In the field of multi-channel, multi-speaker Automatic Speech Recognition (ASR), the task of discerning and accurately transcribing a target speaker’s speech within background noise remains a formidable challenge. Traditional approaches often rely on microphone array configurations and the information of the target speaker’s location or voiceprint. This study introduces the Solo Spatial Feature (Solo-SF), an innovative method that utilizes a target speaker’s isolated speech segment to enhance ASR performance, thereby circumventing the need for conventional inputs like microphone array layouts. We explore effective strategies for selecting optimal solo segments, a crucial aspect for Solo-SF’s success. Through evaluations conducted on the AliMeeting dataset and AISHELL-1 simulations, Solo-SF demonstrates superior performance over existing techniques, significantly lowering Character Error Rates (CER) in various test conditions. Our findings highlight Solo-SF’s potential as an effective solution for addressing the complexities of multi-channel, multi-speaker ASR tasks."
   ],
   "p1": 4993,
   "pn": 4997,
   "doi": "10.21437/Interspeech.2024-2274",
   "url": "interspeech_2024/shao24b_interspeech.html"
  },
  "akani24_interspeech": {
   "authors": [
    [
     "Eunice",
     "Akani"
    ],
    [
     "Frederic",
     "Bechet"
    ],
    [
     "Benoît",
     "Favre"
    ],
    [
     "Romain",
     "Gemignani"
    ]
   ],
   "title": "Unified Framework for Spoken Language Understanding and Summarization in Task-Based Human Dialog processing",
   "original": "2276",
   "order": 723,
   "page_count": 5,
   "abstract": [
    "Dialogue summarization aims to create a concise and coherent overview of a conversation between two or more people. Recent advances in language models have significantly improved this process, but accurately summarizing dialogues is still challenging due to the need to understand the interactions between speakers to capture the most relevant information. This study focuses on goal-oriented human-human dialogues, incorporating task-related information into the summarization process to produce summaries that are more semantically accurate. It explores multitask approaches that combine summarization with language comprehension tasks and introduces new methods for summary selection and evaluation based on semantic analysis. The study tests these methods on the DECODA corpus, a collection of French spoken dialogues from a call center, showing that integrating models and task-related information improves the accuracy of summaries, even with varying levels of word error rates."
   ],
   "p1": 3535,
   "pn": 3539,
   "doi": "10.21437/Interspeech.2024-2276",
   "url": "interspeech_2024/akani24_interspeech.html"
  },
  "ogun24_interspeech": {
   "authors": [
    [
     "Sewade",
     "Ogun"
    ],
    [
     "Abraham T.",
     "Owodunni"
    ],
    [
     "Tobi",
     "Olatunji"
    ],
    [
     "Eniola",
     "Alese"
    ],
    [
     "Babatunde",
     "Oladimeji"
    ],
    [
     "Tejumade",
     "Afonja"
    ],
    [
     "Kayode",
     "Olaleye"
    ],
    [
     "Naome A.",
     "Etori"
    ],
    [
     "Tosin",
     "Adewumi"
    ]
   ],
   "title": "1000 African Voices: Advancing inclusive multi-speaker multi-accent speech synthesis",
   "original": "2281",
   "resource": "https://doi.org/10.5281/zenodo.11630151",
   "order": 380,
   "page_count": 5,
   "abstract": [
    "Recent advances in speech synthesis have enabled many useful applications like audio directions in Google Maps, screen readers, and automated content generation on platforms like TikTok. However, these systems are mostly dominated by voices sourced from data-rich geographies with personas representative of their source data. Although 3000 of the world's languages are domiciled in Africa, African voices and personas are under-represented in these systems. As speech synthesis becomes increasingly democratized, it is desirable to increase the representation of African English accents. We present Afro-TTS, the first pan-African accented English speech synthesis system able to generate speech in 86 African accents, with 1000 personas representing the rich phonological diversity across the continent for downstream application in Education, Public Health, and Automated Content Creation. Speaker interpolation retains naturalness and accentedness, enabling the creation of new voices."
   ],
   "p1": 1855,
   "pn": 1859,
   "doi": "10.21437/Interspeech.2024-2281",
   "url": "interspeech_2024/ogun24_interspeech.html"
  },
  "niu24d_interspeech": {
   "authors": [
    [
     "Minxue",
     "Niu"
    ],
    [
     "Mimansa",
     "Jaiswal"
    ],
    [
     "Emily",
     "Mower Provost"
    ]
   ],
   "title": "From Text to Emotion: Unveiling the Emotion Annotation Capabilities of LLMs",
   "original": "2282",
   "order": 546,
   "page_count": 5,
   "abstract": [
    "Training emotion recognition models has relied heavily on human annotated data, which present diversity, quality, and cost challenges. In this paper, we explore the potential of Large Language Models (LLMs), specifically GPT-4, in automating or assisting emotion annotation. We compare GPT-4 with supervised models and/or humans in three aspects: agreement with human annotations, alignment with human perception, and impact on model training. We find that common metrics that use aggregated human annotations as ground truth can underestimate GPT-4's performance, and our human evaluation experiment reveals a consistent preference for GPT-4 annotations over humans across multiple datasets and evaluators. Further, we investigate the impact of using GPT-4 as an annotation filtering process to improve model training. Together, our findings highlight the great potential of LLMs in emotion annotation tasks and underscore the need for refined evaluation methodologies."
   ],
   "p1": 2650,
   "pn": 2654,
   "doi": "10.21437/Interspeech.2024-2282",
   "url": "interspeech_2024/niu24d_interspeech.html"
  },
  "liang24_interspeech": {
   "authors": [
    [
     "Dawei",
     "Liang"
    ],
    [
     "Alice",
     "Zhang"
    ],
    [
     "David",
     "Harwath"
    ],
    [
     "Edison",
     "Thomaz"
    ]
   ],
   "title": "Improving Audio Classification with Low-Sampled Microphone Input: An Empirical Study Using Model Self-Distillation",
   "original": "2285",
   "resource": "https://doi.org/10.5281/zenodo.12802621",
   "order": 19,
   "page_count": 5,
   "abstract": [
    "Acoustic scene and event classification is gaining traction in mobile health and wearable applications. Traditionally, relevant research focused on high-quality inputs (sampling rates >= 16 kHz). However, lower sampling rates (e.g., 1 kHz - 2 kHz) offer enhanced privacy and reduced power consumption, crucial for continuous mobile use.  This study introduces efficient methods for optimizing pre-trained audio neural networks (PANNs) targeting low-quality audio, employing Born-Again self-distillation (BASD) and a cross-sampling-rate self-distillation (CSSD) strategy. Testing three PANNs with diverse mobile datasets reveals that both strategies boost model inference performance, yielding an absolute accuracy / F1 gain ranging from 1% to 6% compared to a baseline without distillation, while sampling at very low rates (1 kHz - 2 kHz). Notably, CSSD shows greater benefits, suggesting models trained on high-quality audio adapt better to lower resolutions, despite the shift in input quality."
   ],
   "p1": 87,
   "pn": 91,
   "doi": "10.21437/Interspeech.2024-2285",
   "url": "interspeech_2024/liang24_interspeech.html"
  },
  "ahn24d_interspeech": {
   "authors": [
    [
     "Emily P.",
     "Ahn"
    ],
    [
     "Eleanor",
     "Chodroff"
    ],
    [
     "Myriam",
     "Lapierre"
    ],
    [
     "Gina-Anne",
     "Levow"
    ]
   ],
   "title": "The Use of Phone Categories and Cross-Language Modeling for Phone Alignment of Panãra",
   "original": "2286",
   "order": 310,
   "page_count": 5,
   "abstract": [
    "Automating the time-alignment of phonetic labels in speech facilitates research in language documentation, yet such phonetic forced alignment requires pretrained acoustic models. For low-resource languages, this raises the question as to how and on which data the acoustic model should be trained. To align data from Panãra, an Amazonian indigenous language of Brazil, we investigated three approaches for forced alignment of low-resource languages using the Montreal Forced Aligner. First, we implemented a novel approach of manipulating the acoustic model granularity from phone-specific to increasingly broader natural class categories in training language-specific Panãra models. Second, we trained cross-language English models under two granularity settings. Third, we compared these models to a large, pretrained Global English acoustic model. Results showed that broadening phone categories can improve language-specific modeling, but cross-language modeling performed the best."
   ],
   "p1": 1505,
   "pn": 1509,
   "doi": "10.21437/Interspeech.2024-2286",
   "url": "interspeech_2024/ahn24d_interspeech.html"
  },
  "colgiu24_interspeech": {
   "authors": [
    [
     "Ioana",
     "Colgiu"
    ],
    [
     "Laura",
     "Spinu"
    ],
    [
     "Rajiv",
     "Rao"
    ],
    [
     "Yasaman",
     "Rafat"
    ]
   ],
   "title": "Bilingual Rhotic Production Patterns: A Generational Comparison of Spanish-English Bilingual Speakers in Canada",
   "original": "2287",
   "order": 214,
   "page_count": 5,
   "abstract": [
    "This paper considers the differences in producing the acoustic correlates of the phonemic alveolar tap /ɾ/, phonemic canonical trill /r/, and alveolar approximant /ɹ/ in two groups of Spanish-English bilinguals: early (heritage Spanish) and late (native Spanish) bilingual speakers. In particular, the study explores cross-linguistic influence (CLI) and the phenomenon of phonetic drift across two generations of Colombian Spanish-English bilinguals residing in Canada. The data suggest that early bilinguals exhibit shorter trill durations with fewer closures, while late bilinguals show variation in their realizations of the approximant, producing taps, trills, and fricatives instead. Both groups display evidence of phonetic drift and CLI, with early bilinguals showing influence from English and late bilinguals favouring Spanish-like patterns. Our investigation adds to the body of work on CLI by examining its bidirectional effects in both early and late bilinguals."
   ],
   "p1": 1025,
   "pn": 1029,
   "doi": "10.21437/Interspeech.2024-2287",
   "url": "interspeech_2024/colgiu24_interspeech.html"
  },
  "lin24l_interspeech": {
   "authors": [
    [
     "Nana",
     "Lin"
    ],
    [
     "Youxiang",
     "Zhu"
    ],
    [
     "Xiaohui",
     "Liang"
    ],
    [
     "John A.",
     "Batsis"
    ],
    [
     "Caroline",
     "Summerour"
    ]
   ],
   "title": "Analyzing Multimodal Features of Spontaneous Voice Assistant Commands for Mild Cognitive Impairment Detection",
   "original": "2288",
   "order": 622,
   "page_count": 5,
   "abstract": [
    "Mild cognitive impairment (MCI) is a major public health concern due to its high risk of progressing to dementia. This study investigates the potential of detecting MCI with spontaneous voice assistant (VA) commands from 35 older adults in a controlled setting. Specifically, a command-generation task is designed with pre-defined intents for participants to freely generate commands that are more associated with cognitive ability than read commands. We develop MCI classification and regression models with audio, textual, intent, and multimodal fusion features. We find the command-generation task outperforms the command-reading task with an average classification accuracy of 82%, achieved by leveraging multimodal fusion features. In addition, generated commands correlate more strongly with memory and attention subdomains than read commands. Our results confirm the effectiveness of the command-generation task and imply the promise of using longitudinal in-home commands for MCI detection. "
   ],
   "p1": 3030,
   "pn": 3034,
   "doi": "10.21437/Interspeech.2024-2288",
   "url": "interspeech_2024/lin24l_interspeech.html"
  },
  "prajwal24_interspeech": {
   "authors": [
    [
     "K R",
     "Prajwal"
    ],
    [
     "Triantafyllos",
     "Afouras"
    ],
    [
     "Andrew",
     "Zisserman"
    ]
   ],
   "title": "Speech Recognition Models are Strong Lip-readers",
   "original": "2290",
   "order": 500,
   "page_count": 5,
   "abstract": [
    "In this work, we show that a large pre-trained ASR model can be adapted to perform lip-reading. Our method enables an ASR model like Whisper to interpret lip movements in a video and output text transcriptions. We achieve this by learning a cross-modal mapping from a lip sequence to a speech sequence, allowing a pre-trained ASR model to directly perform lip-reading. The mapping can be learnt simply by backpropagating the cross-entropy loss on the text labels through the pre-trained, frozen ASR model. We achieve an impressive gain of 5.7 WER in the low data regime on the LRS3 benchmark over previous lip-reading methods. Finally, we demonstrate that the same strategy can be extended to other visual speech tasks, such as identifying the spoken language in silent videos."
   ],
   "p1": 2425,
   "pn": 2429,
   "doi": "10.21437/Interspeech.2024-2290",
   "url": "interspeech_2024/prajwal24_interspeech.html"
  },
  "tang24c_interspeech": {
   "authors": [
    [
     "Yuxun",
     "Tang"
    ],
    [
     "Yuning",
     "Wu"
    ],
    [
     "Jiatong",
     "Shi"
    ],
    [
     "Qin",
     "Jin"
    ]
   ],
   "title": "SingOMD: Singing Oriented Multi-resolution Discrete Representation Construction from Speech Models",
   "original": "2291",
   "order": 528,
   "page_count": 5,
   "abstract": [
    "Discrete representation has shown advantages in speech generation tasks, wherein discrete tokens are derived by discretizing hidden features from self-supervised learning (SSL) pre-trained models. However, the direct application of speech SSL models to singing generation encounters domain gaps between speech and singing. Furthermore, singing generation necessitates a more refined representation than typical speech. To address these challenges, we introduce SingOMD, a novel method to extract singing-oriented multi-resolution discrete representations from speech SSL models. Specifically, we first adapt the features from speech SSL through a resynthesis task and incorporate multi-resolution modules based on resampling to better serve singing generation. These adapted multi-resolution features are then discretized via clustering. Extensive experiments demonstrate the robustness, efficiency, and effectiveness of these representations in singing vocoders and singing voice synthesis."
   ],
   "p1": 2564,
   "pn": 2568,
   "doi": "10.21437/Interspeech.2024-2291",
   "url": "interspeech_2024/tang24c_interspeech.html"
  },
  "changawala24_interspeech": {
   "authors": [
    [
     "Vrushank",
     "Changawala"
    ],
    [
     "Frank",
     "Rudzicz"
    ]
   ],
   "title": "Whister: Using Whisper’s representations for Stuttering detection",
   "original": "2293",
   "order": 181,
   "page_count": 5,
   "abstract": [
    "In this paper, we empirically investigate the influence of different factors on the performance of dysfluency detection. Specifically, we examine the impact of data splits, data quality, and learned representations of large pre-trained models. To conduct our experiments, we use the frozen Whisper model with two trainable heads, along with MFCC features extracted from input audio. We train on different data splits and evaluate performance using a cross-corpora testing strategy. We find that longer audio segments, specifically 5 seconds as opposed to the conventional 3-second segments, leads to improved performance. We also show that our architecture design is generalizable for multilingual data. We attain a 9.3% and 22% relative improvement in the average F1 score for FluencyBank and KSoF-test datasets, respectively, surpassing the previous state-of-the-art."
   ],
   "p1": 897,
   "pn": 901,
   "doi": "10.21437/Interspeech.2024-2293",
   "url": "interspeech_2024/changawala24_interspeech.html"
  },
  "puvvada24_interspeech": {
   "authors": [
    [
     "Krishna C.",
     "Puvvada"
    ],
    [
     "Piotr",
     "Żelasko"
    ],
    [
     "He",
     "Huang"
    ],
    [
     "Oleksii",
     "Hrinchuk"
    ],
    [
     "Nithin Rao",
     "Koluguri"
    ],
    [
     "Kunal",
     "Dhawan"
    ],
    [
     "Somshubra",
     "Majumdar"
    ],
    [
     "Elena",
     "Rastorgueva"
    ],
    [
     "Zhehuai",
     "Chen"
    ],
    [
     "Vitaly",
     "Lavrukhin"
    ],
    [
     "Jagadeesh",
     "Balam"
    ],
    [
     "Boris",
     "Ginsburg"
    ]
   ],
   "title": "Less is More: Accurate Speech Recognition &amp; Translation without Web-Scale Data",
   "original": "2294",
   "order": 816,
   "page_count": 5,
   "abstract": [
    "Recent advances in speech recognition and translation rely on hundreds of thousands of hours of Internet speech data. We argue that state-of-the art accuracy can be reached without relying on web-scale data. Canary - multilingual ASR and speech translation model, outperforms current state-of-the-art models – Whisper, OWSM, and Seamless-M4T on English, French, Spanish, and German languages, while being trained on an order of magnitude less data than these models. Three key factors enables such dataefficient model: (1) a FastConformer-based attention encoderdecoder architecture (2) training on synthetic data generated with machine translation and (3) advanced training techniques: data-balancing, dynamic data blending, dynamic bucketing and noise-robust fine-tuning. The model, weights, and training code will be open-sourced."
   ],
   "p1": 3964,
   "pn": 3968,
   "doi": "10.21437/Interspeech.2024-2294",
   "url": "interspeech_2024/puvvada24_interspeech.html"
  },
  "yuan24b_interspeech": {
   "authors": [
    [
     "Junming",
     "Yuan"
    ],
    [
     "Ying",
     "Shi"
    ],
    [
     "LanTian",
     "Li"
    ],
    [
     "Dong",
     "Wang"
    ],
    [
     "Askar",
     "Hamdulla"
    ]
   ],
   "title": "Few-Shot Keyword Spotting from Mixed Speech",
   "original": "2296",
   "order": 1037,
   "page_count": 5,
   "abstract": [
    "Few-shot keyword spotting (KWS) aims to detect unknown keywords with limited training samples. A commonly used approach is the pre-training and fine-tuning framework. While effective in clean conditions, this approach struggles with mixed keyword spotting – simultaneously detecting multiple keywords blended in an utterance, which is crucial in real-world applications. Previous research has proposed a Mix-Training (MT) approach to solve the problem, however, it has never been tested in the few-shot scenario. In this paper, we investigate the possibility of using MT and other relevant methods to solve the two practical challenges together: few-shot and mixed speech. Experiments conducted on the LibriSpeech and Google Speech Command corpora demonstrate that MT is highly effective on this task when employed in either the pre-training phase or the fine-tuning phase. Moreover, combining SSL-based large-scale pre-training (HuBert) and MT fine-tuning yields very strong results in all the test conditions."
   ],
   "p1": 5063,
   "pn": 5067,
   "doi": "10.21437/Interspeech.2024-2296",
   "url": "interspeech_2024/yuan24b_interspeech.html"
  },
  "wang24la_interspeech": {
   "authors": [
    [
     "Xintong",
     "Wang"
    ],
    [
     "Mingqian",
     "Shi"
    ],
    [
     "Ye",
     "Wang"
    ]
   ],
   "title": "Pitch-Aware RNN-T for Mandarin Chinese Mispronunciation Detection and Diagnosis",
   "original": "2297",
   "order": 60,
   "page_count": 5,
   "abstract": [
    "Mispronunciation Detection and Diagnosis (MDD) systems, leveraging Automatic Speech Recognition (ASR), face two main challenges in Mandarin Chinese: 1) The two-stage models create an information gap between the phoneme or tone classification stage and the MDD stage. 2) The scarcity of Mandarin MDD datasets limits model training. In this paper, we introduce a stateless RNN-T model for Mandarin MDD, utilizing HuBERT features with pitch embedding through a Pitch Fusion Block. Our model, trained solely on native speaker data, shows a 3% improvement in Phone Error Rate and a 7% increase in False Acceptance Rate over the state-of-the-art baseline in non-native scenarios."
   ],
   "p1": 292,
   "pn": 296,
   "doi": "10.21437/Interspeech.2024-2297",
   "url": "interspeech_2024/wang24la_interspeech.html"
  },
  "heuser24_interspeech": {
   "authors": [
    [
     "Annika",
     "Heuser"
    ],
    [
     "Tyler",
     "Kendall"
    ],
    [
     "Miguel",
     "del Rio"
    ],
    [
     "Quinn",
     "McNamara"
    ],
    [
     "Nishchal",
     "Bhandari"
    ],
    [
     "Corey",
     "Miller"
    ],
    [
     "Migüel",
     "Jetté"
    ]
   ],
   "title": "Quantification of stylistic differences in human- and ASR-produced transcripts of African American English",
   "original": "2300",
   "order": 932,
   "page_count": 5,
   "abstract": [
    "Common measures of accuracy used to assess the performance of automatic speech recognition (ASR) systems, as well as human transcribers, conflate multiple sources of error. Stylistic differences, such as verbatim vs non-verbatim, can play a significant role in ASR performance evaluation when differences exist between training and test datasets. The problem is compounded for speech from underrepresented varieties, where the speech to orthography mapping is not as standardized. We categorize the kinds of stylistic differences between 6 transcription versions, 4 human- and 2 ASR-produced, of 10 hours of African American English (AAE) speech. Focusing on verbatim features and AAE morphosyntactic features, we investigate the interactions of these categories with how well transcripts can be compared via word error rate (WER). The results, and overall analysis, help clarify how ASR outputs are a function of the decisions made by the training data’s human transcribers."
   ],
   "p1": 4538,
   "pn": 4542,
   "doi": "10.21437/Interspeech.2024-2300",
   "url": "interspeech_2024/heuser24_interspeech.html"
  },
  "tavernor24_interspeech": {
   "authors": [
    [
     "James",
     "Tavernor"
    ],
    [
     "Yara",
     "El-Tawil"
    ],
    [
     "Emily",
     "Mower Provost"
    ]
   ],
   "title": "The Whole Is Bigger Than the Sum of Its Parts: Modeling Individual Annotators to Capture Emotional Variability",
   "original": "2307",
   "order": 655,
   "page_count": 5,
   "abstract": [
    "Emotion expression and perception are nuanced, complex, and highly subjective processes. When multiple annotators label emotional data, the resulting labels contain high variability. Most speech emotion recognition tasks address this by averaging annotator labels as ground truth. However, this process omits the nuance of emotion and inter-annotator variability, which are important signals to capture. Previous work has attempted to learn distributions to capture emotion variability, but these methods also lose information about the individual annotators. We address these limitations by learning to predict individual annotators and by introducing a novel method to create distributions from continuous model outputs that permit the learning of emotion distributions during model training.  We show that this combined approach can result in emotion distributions that are more accurate than those seen in prior work, in both within- and cross-corpus settings."
   ],
   "p1": 3195,
   "pn": 3199,
   "doi": "10.21437/Interspeech.2024-2307",
   "url": "interspeech_2024/tavernor24_interspeech.html"
  },
  "kalyan24_interspeech": {
   "authors": [
    [
     "Pavan",
     "Kalyan"
    ],
    [
     "Preeti",
     "Rao"
    ],
    [
     "Preethi",
     "Jyothi"
    ],
    [
     "Pushpak",
     "Bhattacharyya"
    ]
   ],
   "title": "Emotion Arithmetic: Emotional Speech Synthesis via Weight Space Interpolation",
   "original": "2311",
   "order": 370,
   "page_count": 5,
   "abstract": [
    "While the idea of task arithmetic has been shown to be useful to steer the behaviour of neural models for NLP and vision tasks, it has not yet been used for speech. Moreover the tasks studied have been restricted to text classification and generation, and image classification. We extend the idea of task vectors to emotional speech synthesis in this work. We build emotion vectors by subtracting the weights of a pre-trained model from the weights of the same model after fine-tuning for a given emotion. These emotion vectors can be modified or combined through arithmetic operations such as negation and addition, with the hope of steering the behaviour of the resulting model accordingly in the generation of emotional speech. We also show that the emotion vector can achieve the desired transfer of emotion to a speaker not seen during training."
   ],
   "p1": 1805,
   "pn": 1809,
   "doi": "10.21437/Interspeech.2024-2311",
   "url": "interspeech_2024/kalyan24_interspeech.html"
  },
  "liu24p_interspeech": {
   "authors": [
    [
     "Rui",
     "Liu"
    ],
    [
     "Jiatian",
     "Xi"
    ],
    [
     "Ziyue",
     "Jiang"
    ],
    [
     "Haizhou",
     "Li"
    ]
   ],
   "title": "FluentEditor: Text-based Speech Editing by Considering Acoustic and Prosody Consistency",
   "original": "2320",
   "order": 703,
   "page_count": 5,
   "abstract": [
    "The current Text-based Speech Editing (TSE) techniques have focused on reducing the difference between the generated speech segment and the reference target in the editing region, ignoring its local and global fluency in the context and original utterance. To maintain the speech fluency, we propose a fluency speech editing model, termed FluentEditor, by considering fluency-aware training criterion in the TSE training. Specifically, the acoustic consistency constraint aims to smooth the transition between the edited region and its neighboring acoustic segments consistent with the ground truth, while the prosody consistency constraint seeks to ensure that the prosody attributes within the edited regions remain consistent with the overall style of the original utterance. The subjective and objective experimental results on VCTK demonstrate that our FluentEditor outperforms all advanced baselines in terms of naturalness and fluency. The audio samples and code are available at https://github.com/AI-S2-Lab/FluentEditor."
   ],
   "p1": 3435,
   "pn": 3439,
   "doi": "10.21437/Interspeech.2024-2320",
   "url": "interspeech_2024/liu24p_interspeech.html"
  },
  "zhang24p_interspeech": {
   "authors": [
    [
     "Yiru",
     "Zhang"
    ],
    [
     "Linyu",
     "Yao"
    ],
    [
     "Qun",
     "Yang"
    ]
   ],
   "title": "OR-TSE: An Overlap-Robust Speaker Encoder for Target Speech Extraction",
   "original": "2322",
   "order": 119,
   "page_count": 5,
   "abstract": [
    "Mainstream Target Speech Extraction (TSE) systems extract target speech from a mixture using pre-enrolled reference speech. The extraction performance heavily depends on the quality of the reference speech. However, the speech signal of the same speaker may vary under different conditions, leading to a decrease in extraction performance, particularly in speech overlap. Therefore, we propose an overlap robust speaker encoder for TSE to obtain stable speaker embeddings even when using signals with overlapping interference. Our approach combines attentive statistics pooling with contrastive learning to make the model focus on the feature of main speaker while disregarding interfering information. Based on our proposed speaker encoder, we introduce a TSE framework, which derive speaker embeddings from non-overlapping regions of mixture input. The experiments shows that our speaker encoder improves the performance of TSE in different conditions of reference speech."
   ],
   "p1": 587,
   "pn": 591,
   "doi": "10.21437/Interspeech.2024-2322",
   "url": "interspeech_2024/zhang24p_interspeech.html"
  },
  "kim24t_interspeech": {
   "authors": [
    [
     "Hyung Yong",
     "Kim"
    ],
    [
     "Byeong-Yeol",
     "Kim"
    ],
    [
     "Yunkyu",
     "Lim"
    ],
    [
     "Jihwan",
     "Park"
    ],
    [
     "Shukjae",
     "Choi"
    ],
    [
     "Yooncheol",
     "Ju"
    ],
    [
     "Jinseok",
     "Park"
    ],
    [
     "Youshin",
     "Lim"
    ],
    [
     "Seung Woo",
     "Yu"
    ],
    [
     "Hanbin",
     "Lee"
    ],
    [
     "Shinji",
     "Watanabe"
    ]
   ],
   "title": "Self-training ASR Guided by Unsupervised ASR Teacher",
   "original": "2323",
   "order": 589,
   "page_count": 5,
   "abstract": [
    "Self-training has gained increasing attention due to its notable performance improvement in speech recognition. However, conventional self-training techniques have two key limitations: (1) labeled dataset is required for training a teacher to produce a pseudo-target, and (2) the first teacher trained with the small labeled dataset faces an over-fitting issue, generating noisy pseudo-targets for unseen datasets. Our approach adopts an unsupervised automatic speech recognition model as the teacher, thus solely utilizing the unlabeled dataset. As the proposed model also learns phonetic information from the UASR teacher at the intermediate layer, the pseudo-target at the higher layer contains more ASR-related information than that of Data2vec2. Experimental results on the LibriSpeech show that our model outperforms Data2vec2, the state-of-the-art self-supervised learning model, achieving 8.9% and 4.3% relative word error rate reduction on test-clean and test-other."
   ],
   "p1": 2865,
   "pn": 2869,
   "doi": "10.21437/Interspeech.2024-2323",
   "url": "interspeech_2024/kim24t_interspeech.html"
  },
  "liu24q_interspeech": {
   "authors": [
    [
     "Suyuan",
     "Liu"
    ],
    [
     "Molly",
     "Babel"
    ],
    [
     "Jian",
     "Zhu"
    ]
   ],
   "title": "A comparison of voice similarity through acoustics, human perception and deep neural network (DNN) speaker verification systems",
   "original": "2331",
   "order": 758,
   "page_count": 5,
   "abstract": [
    "Voice similarity can be assessed through acoustic analysis, perceptual judgments by human listeners, and the recent addition of automatic speaker verification systems. However, a comparison across the similarity judgments made from acoustics, listener perception, and deep neural network (DNN) based speaker verification systems has not yet been made. This project fills this gap by comparing acoustic similarity scores generated from 24 acoustic dimensions and verification scores generated by seven pretrained speaker verification models using the Wespeaker toolkit to perceptual similarity assessed by human listeners in an AX discrimination task and a (dis)similarity rating task. Results suggest verification similarities correlate with acoustic similarities, but not with human perceptual similarities when controlled for talker pair, indicating the correlation between listeners and speaker verification models happens at a gross-phonetic level rather than a fine phonetic level."
   ],
   "p1": 3674,
   "pn": 3678,
   "doi": "10.21437/Interspeech.2024-2331",
   "url": "interspeech_2024/liu24q_interspeech.html"
  },
  "tokac24_interspeech": {
   "authors": [
    [
     "Zuheyra",
     "Tokac"
    ],
    [
     "Jennifer",
     "Cole"
    ]
   ],
   "title": "Phonological Symmetry Does Not Predict Generalization of Perceptual Adaptation to Vowels",
   "original": "2334",
   "order": 873,
   "page_count": 5,
   "abstract": [
    "Speech perception is inherently adaptive, with context-dependent rather than fixed perceptual phoneme boundaries. Exposure to novel vowel variants in lexically biasing contexts induces lasting boundary shifts toward the novel variant. Studies have found mixed results as to whether perceptual adaptation generalizes to phonologically related vowels and suggest that phonological symmetry might predict generalization. We test this hypothesis in Turkish, which has a fully symmetrical 8-vowel inventory. Listeners were exposed to words with either lowered /i/s or raised /ɛ/s and identified vowels on /i-e/ and /u-o/ continua to assess perceptual adaptation and generalization. We found perceptual adaptation to only lowered /i/s in /i-e/ identification and no generalization of perceptual adaptation in /u-o/ identification. We argue that phonological symmetry might not be sufficient for generalization, and that vowel inventory size and organization might also play a role."
   ],
   "p1": 4243,
   "pn": 4247,
   "doi": "10.21437/Interspeech.2024-2334",
   "url": "interspeech_2024/tokac24_interspeech.html"
  },
  "feng24d_interspeech": {
   "authors": [
    [
     "Jingyi",
     "Feng"
    ],
    [
     "Yusuke",
     "Yasuda"
    ],
    [
     "Tomoki",
     "Toda"
    ]
   ],
   "title": "Exploring the Robustness of Text-to-Speech Synthesis Based on Diffusion Probabilistic Models to Heavily Noisy Transcriptions",
   "original": "2337",
   "order": 906,
   "page_count": 5,
   "abstract": [
    "Large data volumes can benefit text-to-speech (TTS), but speech data with high-quality annotation is limited. Automatic transcription enables the transcription of found speech data to enhance the data volume for TTS, but TTS training suffers from transcription errors. In this paper, we investigate the robustness of typical TTS models against heavily noisy transcripts, including diffusion, flow, and autoregressive-based TTS models, in terms of objective intelligibility and subjective naturalness. Our experimental results show that diffusion-based TTS is extremely robust to heavily noisy transcriptions, mitigating about 30% of the word error rate compared to autoregressive and flow-based models. We also show that iterative inference with a long diffusion time is key to the robustness of diffusionbased TTS based on likelihood analysis."
   ],
   "p1": 4408,
   "pn": 4412,
   "doi": "10.21437/Interspeech.2024-2337",
   "url": "interspeech_2024/feng24d_interspeech.html"
  },
  "heuser24b_interspeech": {
   "authors": [
    [
     "Annika",
     "Heuser"
    ],
    [
     "Jianjing",
     "Kuang"
    ]
   ],
   "title": "Information-theoretic hypothesis generation of relative cue weighting for the voicing contrast",
   "original": "2340",
   "order": 733,
   "page_count": 5,
   "abstract": [
    "To learn the voicing contrast, children must identify which of the available perceptual cues are helpful in different contexts. Using Standard American English (SAE) as a case study, we generated hypotheses of which cues are the most informative for different contexts, such as onsets vs. codas. More specifically, we classified SAE obstruents as voiced vs. voiceless using decision trees trained and tested on TIMIT. We validated the feature importances of different contexts against the findings of previous perceptual studies and we gleaned more specific hypotheses to help design future experiments on children’s acquisition of the voicing contrast."
   ],
   "p1": 3585,
   "pn": 3589,
   "doi": "10.21437/Interspeech.2024-2340",
   "url": "interspeech_2024/heuser24b_interspeech.html"
  },
  "delafuente24_interspeech": {
   "authors": [
    [
     "Anton",
     "de la Fuente"
    ],
    [
     "Dan",
     "Jurafsky"
    ]
   ],
   "title": "A layer-wise analysis of Mandarin and English suprasegmentals in SSL speech models",
   "original": "2341",
   "order": 267,
   "page_count": 5,
   "abstract": [
    "This study asks how self-supervised speech models represent suprasegmental categories like Mandarin lexical tone, English lexical stress, and English phrasal accents. Through a series of probing tasks, we make layer-wise comparisons of English and Mandarin 12 layer monolingual models. Our findings suggest that 1) English and Mandarin wav2vec 2.0 models learn contextual representations of abstract suprasegmental categories which are strongest in the middle third of the network. 2) Models are better at representing features that exist in the language of their training data, and this difference is driven by enriched context in transformer blocks, not local acoustic representation. 3) Fine-tuned wav2vec 2.0 improves performance in later layers compared to pre-trained models mainly for lexically contrastive features like tone and stress, 4) HuBERT and WavLM learn similar representations to wav2vec 2.0, differing mainly in later layer performance. Our results extend previous understanding of how models represent suprasegmentals and offer new insights into the language-specificity and contextual nature of these representations."
   ],
   "p1": 1290,
   "pn": 1294,
   "doi": "10.21437/Interspeech.2024-2341",
   "url": "interspeech_2024/delafuente24_interspeech.html"
  },
  "ma24d_interspeech": {
   "authors": [
    [
     "Linhan",
     "Ma"
    ],
    [
     "Dake",
     "Guo"
    ],
    [
     "Kun",
     "Song"
    ],
    [
     "Yuepeng",
     "Jiang"
    ],
    [
     "Shuai",
     "Wang"
    ],
    [
     "Liumeng",
     "Xue"
    ],
    [
     "Weiming",
     "Xu"
    ],
    [
     "Huan",
     "Zhao"
    ],
    [
     "Binbin",
     "Zhang"
    ],
    [
     "Lei",
     "Xie"
    ]
   ],
   "title": "WenetSpeech4TTS: A 12,800-hour Mandarin TTS Corpus for Large Speech Generation Model Benchmark",
   "original": "2343",
   "order": 377,
   "page_count": 5,
   "abstract": [
    "With the development of large text-to-speech (TTS) models and scale-up of the training data, state-of-the-art TTS systems have achieved impressive performance. In this paper, we present WenetSpeech4TTS, a multi-domain Mandarin corpus derived from the open-sourced WenetSpeech dataset. Tailored for the text-to-speech tasks, we refined WenetSpeech by adjusting segment boundaries, enhancing the audio quality, and eliminating speaker mixing within each segment. Following a more accurate transcription process and quality-based data filtering process, the obtained WenetSpeech4TTS corpus contains 12, 800 hours of paired audio-text data. Furthermore, we have created subsets of varying sizes, categorized by segment quality scores to allow for TTS model training and fine-tuning. VALLE and NaturalSpeech 2 systems are trained and fine-tuned on these subsets to validate the usability of WenetSpeech4TTS, establishing baselines on benchmark for fair comparison of TTS systems. The corpus and corresponding benchmarks are publicly available on huggingface."
   ],
   "p1": 1840,
   "pn": 1844,
   "doi": "10.21437/Interspeech.2024-2343",
   "url": "interspeech_2024/ma24d_interspeech.html"
  },
  "dumpala24b_interspeech": {
   "authors": [
    [
     "Sri Harsha",
     "Dumpala"
    ],
    [
     "Katerina",
     "Dikaios"
    ],
    [
     "Abraham",
     "Nunes"
    ],
    [
     "Frank",
     "Rudzicz"
    ],
    [
     "Rudolf",
     "Uher"
    ],
    [
     "Sageev",
     "Oore"
    ]
   ],
   "title": "Self-Supervised Embeddings for Detecting Individual Symptoms of Depression",
   "original": "2344",
   "order": 299,
   "page_count": 5,
   "abstract": [
    "Depression, a prevalent mental health disorder impacting millions globally, demands reliable assessment systems. Unlike previous studies that focus solely on either detecting depression or predicting its severity, our work identifies individual symptoms of depression while also predicting its severity using speech input. We leverage self-supervised learning (SSL)-based speech models to better utilize the small-sized datasets that are frequently encountered in this task. Our study demonstrates notable performance improvements by utilizing SSL embeddings compared to conventional speech features. We compare various types of SSL pretrained models to elucidate the type of speech information (semantic, speaker, or prosodic) that contributes the most in identifying different symptoms. Additionally, we evaluate the impact of combining multiple SSL embeddings on performance. Furthermore, we show the significance of multi-task learning for identifying depressive symptoms effectively."
   ],
   "p1": 1450,
   "pn": 1454,
   "doi": "10.21437/Interspeech.2024-2344",
   "url": "interspeech_2024/dumpala24b_interspeech.html"
  },
  "chen24w_interspeech": {
   "authors": [
    [
     "Nan",
     "Chen"
    ],
    [
     "Yonghe",
     "Wang"
    ],
    [
     "Feilong",
     "Bao"
    ]
   ],
   "title": "Knowledge-Preserving Pluggable Modules for Multilingual Speech Translation Tasks",
   "original": "2346",
   "order": 75,
   "page_count": 5,
   "abstract": [
    "Multilingual speech translation tasks typically employ retraining, regularization, or resampling methods to add new languages. Retraining the model significantly increases training time and cost. Moreover, using existing regularization or resampling methods to balance performance between new and original languages might lead to catastrophic forgetting. This can degrade the translation performance of the existing languages. To mitigate the above issues, we store the knowledge of new languages in additional models. We then introduce them as pluggable modules into existing multilingual speech translation models. This approach does not significantly increase training costs and affect the translation performance of existing models. The experimental results demonstrate that our method improves the translation performance of new languages without affecting existing translation tasks. Our code is available at https://github.com/myaxxxxx/transfer-st."
   ],
   "p1": 367,
   "pn": 371,
   "doi": "10.21437/Interspeech.2024-2346",
   "url": "interspeech_2024/chen24w_interspeech.html"
  },
  "baser24_interspeech": {
   "authors": [
    [
     "Oguzhan",
     "Baser"
    ],
    [
     "Kaan",
     "Kale"
    ],
    [
     "Sandeep P.",
     "Chinchali"
    ]
   ],
   "title": "SecureSpectra: Safeguarding Digital Identity from Deep Fake Threats via Intelligent Signatures",
   "original": "2349",
   "resource": "https://doi.org/10.5281/zenodo.12741022",
   "order": 232,
   "page_count": 5,
   "abstract": [
    "Advancements in DeepFake (DF) audio models pose a significant threat to voice authentication systems, leading to unauthorized access and the spread of misinformation. We introduce a defense mechanism, SecureSpectra, addressing DF threats by embedding orthogonal, irreversible signatures within audio. SecureSpectra leverages the inability of DF models to replicate high-frequency content, which we empirically identify across diverse datasets and DF models. Integrating differential privacy into the pipeline protects signatures from reverse engineering and strikes a delicate balance between enhanced security and minimal performance compromises. Our evaluations on Mozilla Common Voice, LibriSpeech, and VoxCeleb datasets showcase SecureSpectra's superior performance, outperforming recent works by up to 71% in detection accuracy. We open-source SecureSpectra to benefit the research community."
   ],
   "p1": 1115,
   "pn": 1119,
   "doi": "10.21437/Interspeech.2024-2349",
   "url": "interspeech_2024/baser24_interspeech.html"
  },
  "shi24i_interspeech": {
   "authors": [
    [
     "Xiaohan",
     "Shi"
    ],
    [
     "Xingfeng",
     "Li"
    ],
    [
     "Tomoki",
     "Toda"
    ]
   ],
   "title": "Multimodal Fusion of Music Theory-Inspired and Self-Supervised Representations for Improved Emotion Recognition",
   "original": "2350",
   "order": 768,
   "page_count": 5,
   "abstract": [
    "Multimodal emotion recognition (MER) is a rapidly evolving field aimed at integrating information from various modalities, such as speech and text, to deepen our understanding of emotions. However, challenges in feature extraction and fusion hinder further advancements in MER performance. To address these challenges, we propose a MER method using self-supervised representations and handcrafted music theory-inspired representations across different modalities to comprehensively capture emotional information. Additionally, we introduce a novel multimodal fusion method to explore modality-specific and modality-invariant relationships, thereby reducing distribution gaps between different modalities in MER. Extensive experimental validation underscores the effectiveness of our approach, with state-of-the-art results showing a 3.55% improvement compared with the baseline. These results validate the effectiveness of our proposed method, signifying a notable enhancement in MER performance."
   ],
   "p1": 3724,
   "pn": 3728,
   "doi": "10.21437/Interspeech.2024-2350",
   "url": "interspeech_2024/shi24i_interspeech.html"
  },
  "morrison24_interspeech": {
   "authors": [
    [
     "Max",
     "Morrison"
    ],
    [
     "Cameron",
     "Churchwell"
    ],
    [
     "Nathan",
     "Pruyne"
    ],
    [
     "Bryan",
     "Pardo"
    ]
   ],
   "title": "Fine-Grained and Interpretable Neural Speech Editing",
   "original": "2351",
   "order": 39,
   "page_count": 5,
   "abstract": [
    "Fine-grained editing of speech attributes - such as prosody (i.e., the pitch, loudness, and phoneme durations), pronunciation, speaker identity, and formants - is useful for fine-tuning and fixing imperfections in human and AI-generated speech recordings for creation of podcasts, film dialogue, and video game dialogue. Existing speech synthesis systems use representations that entangle two or more of these attributes, prohibiting their use in fine-grained, disentangled editing. In this paper, we demonstrate the first disentangled and interpretable representation of speech with comparable subjective and objective vocoding reconstruction accuracy to Mel spectrograms. Our interpretable representation, combined with our proposed data augmentation method, enables training an existing neural vocoder to perform fast, accurate, and high-quality editing of pitch, duration, volume, timbral correlates of volume, pronunciation, speaker identity, and spectral balance."
   ],
   "p1": 187,
   "pn": 191,
   "doi": "10.21437/Interspeech.2024-2351",
   "url": "interspeech_2024/morrison24_interspeech.html"
  },
  "mcneill24_interspeech": {
   "authors": [
    [
     "Matthew",
     "McNeill"
    ],
    [
     "Rivka",
     "Levitan"
    ]
   ],
   "title": "Autoregressive cross-interlocutor attention scores meaningfully capture conversational dynamics",
   "original": "2352",
   "order": 605,
   "page_count": 5,
   "abstract": [
    "This paper analyzes attention scores over a conversational partner's historical turns trained with an autoregressive prosodic objective. Following a qualitative observation that these attention scores seem to organize dialogue history into topic segments, we demonstrate that they capture meaningful dialogue structure based on several quantitative measures. This finding has implications for spoken dialogue system design and analysis of entrainment and conversational dynamics in human-human and human-machine communication."
   ],
   "p1": 2945,
   "pn": 2949,
   "doi": "10.21437/Interspeech.2024-2352",
   "url": "interspeech_2024/mcneill24_interspeech.html"
  },
  "wu24q_interspeech": {
   "authors": [
    [
     "Yuning",
     "Wu"
    ],
    [
     "Chunlei",
     "Zhang"
    ],
    [
     "Jiatong",
     "Shi"
    ],
    [
     "Yuxun",
     "Tang"
    ],
    [
     "Shan",
     "Yang"
    ],
    [
     "Qin",
     "Jin"
    ]
   ],
   "title": "TokSing: Singing Voice Synthesis based on Discrete Tokens",
   "original": "2360",
   "order": 525,
   "page_count": 5,
   "abstract": [
    "Recent advancements in speech synthesis witness significant benefits by leveraging discrete tokens extracted from selfsupervised learning (SSL) models. Discrete tokens offer higher storage efficiency and greater operability in intermediate representations compared to traditional continuous Mel spectrograms. However, when it comes to singing voice synthesis (SVS), achieving higher levels of melody expression poses a great challenge for utilizing discrete tokens. In this paper, we introduce TokSing, a discrete-based SVS system equipped with a token formulator that offers flexible token blendings. We observe a melody degradation during discretization, prompting us to integrate a melody signal with the discrete token and incorporate a specially-designed melody enhancement strategy in the musical encoder. Extensive experiments demonstrate that our TokSing achieves better performance against the Mel spectrogram baselines while offering advantages in intermediate representation space cost and convergence speed."
   ],
   "p1": 2549,
   "pn": 2553,
   "doi": "10.21437/Interspeech.2024-2360",
   "url": "interspeech_2024/wu24q_interspeech.html"
  },
  "terhiija24_interspeech": {
   "authors": [
    [
     "Viyazonuo",
     "Terhiija"
    ],
    [
     "Priyankoo",
     "Sarmah"
    ]
   ],
   "title": "Voiced and voiceless laterals in Angami",
   "original": "2361",
   "order": 764,
   "page_count": 5,
   "abstract": [
    "Voiceless laterals are relatively rare cross-linguistically, with Angami being an exception where they exhibit aspirated characteristics. This investigation delves into the distinctive traits of both voiced and voiceless laterals in the language, which are produced in three contexts. The result of the study showed that there is a clear distinction between voiced and voiceless laterals, and the acoustic-phonetic properties that are significant in voicing distinction are formants (F1- F2), Harmonics to Noise Ratio (HNR), and amount of voicing. The study also shows that voiceless laterals differ in the contexts in which they are spoken. Statistical analyses provide further support for the observed distinctions. "
   ],
   "p1": 3704,
   "pn": 3708,
   "doi": "10.21437/Interspeech.2024-2361",
   "url": "interspeech_2024/terhiija24_interspeech.html"
  },
  "ma24e_interspeech": {
   "authors": [
    [
     "Linhan",
     "Ma"
    ],
    [
     "Xinfa",
     "Zhu"
    ],
    [
     "Yuanjun",
     "Lv"
    ],
    [
     "Zhichao",
     "Wang"
    ],
    [
     "Ziqian",
     "Wang"
    ],
    [
     "Wendi",
     "He"
    ],
    [
     "Hongbin",
     "Zhou"
    ],
    [
     "Lei",
     "Xie"
    ]
   ],
   "title": "Vec-Tok-VC+: Residual-enhanced Robust Zero-shot Voice Conversion with Progressive Constraints in a Dual-mode Training Strategy",
   "original": "2362",
   "order": 565,
   "page_count": 5,
   "abstract": [
    "Zero-shot voice conversion (VC) aims to transform source speech into arbitrary unseen target voice while keeping the linguistic content unchanged. Recent VC methods have made significant progress, but semantic losses in the decoupling process as well as training-inference mismatch still hinder conversion performance. In this paper, we propose Vec-Tok-VC+, a novel prompt-based zero-shot VC model improved from Vec-Tok Codec, achieving voice conversion given only a 3s target speaker prompt. We design a residual-enhanced K-Means decoupler to enhance the semantic content extraction with a two-layer clustering process. Besides, we employ teacherguided refinement to simulate the conversion process to eliminate the training-inference mismatch, forming a dual-mode training strategy. Furthermore, we design a multi-codebook progressive loss function to constrain the layer-wise output of the model from coarse to fine to improve speaker similarity and content accuracy. Objective and subjective evaluations demonstrate that Vec-Tok-VC+ outperforms the strong baselines in naturalness, intelligibility, and speaker similarity."
   ],
   "p1": 2745,
   "pn": 2749,
   "doi": "10.21437/Interspeech.2024-2362",
   "url": "interspeech_2024/ma24e_interspeech.html"
  },
  "kyung24_interspeech": {
   "authors": [
    [
     "Jehyun",
     "Kyung"
    ],
    [
     "Serin",
     "Heo"
    ],
    [
     "Joon-Hyuk",
     "Chang"
    ]
   ],
   "title": "Enhancing Multimodal Emotion Recognition through ASR Error Compensation and LLM Fine-Tuning",
   "original": "2364",
   "order": 961,
   "page_count": 5,
   "abstract": [
    "Multimodal emotion recognition (MER), particularly using speech and text, is promising for enhancing human-computer interaction. However, the efficacy of such systems is often compromised by inaccuracies introduced during the automatic speech recognition (ASR) process. Addressing this, we present a comprehensive MER system that incorporates ways to make up for errors in ASR-generated text. Our system capitalizes on the strengths of speech signals and ASR-generated text, employing a cross-modal transformer (CMT) to blend these modalities effectively. We introduce a novel error compensation technique to counteract the detrimental effects of ASR inaccuracies and employ preference learning to fine-tune a large language model (LLM), thus improving its ability to distinguish slight emotional nuances in text. Performance of our proposed MER system is evaluated on the IEMOCAP dataset, demonstrating significant advancements in emotion recognition accuracy over conventional methods."
   ],
   "p1": 4683,
   "pn": 4687,
   "doi": "10.21437/Interspeech.2024-2364",
   "url": "interspeech_2024/kyung24_interspeech.html"
  },
  "shechtman24_interspeech": {
   "authors": [
    [
     "Slava",
     "Shechtman"
    ],
    [
     "Avihu",
     "Dekel"
    ]
   ],
   "title": "Low Bitrate High-Quality RVQGAN-based Discrete Speech Tokenizer",
   "original": "2366",
   "order": 858,
   "page_count": 5,
   "abstract": [
    "Discrete Audio codecs (or audio tokenizers) have recently regained interest due to the ability of Large Language Models (LLMs) to learn their compressed acoustic representations. Various publicly available trainable discrete tokenizers recently demonstrated impressive results for audio tokenization, yet they mostly require high token rates to gain high-quality reconstruction. In this study, we fine-tuned an open-source general audio RVQGAN model using diverse open-source speech data, considering various recording conditions and quality levels. The resulting wideband (24kHz) speech-only model achieves speech reconstruction, which is nearly indistinguishable from PCM (pulse-code modulation) with a rate of 150-300 tokens per second (1500-3000 bps). The evaluation used comprehensive English speech data encompassing different recording conditions, including studio settings. Speech samples and details on the reproducible trained models are made publicly available."
   ],
   "p1": 4174,
   "pn": 4178,
   "doi": "10.21437/Interspeech.2024-2366",
   "url": "interspeech_2024/shechtman24_interspeech.html"
  },
  "kim24u_interspeech": {
   "authors": [
    [
     "Junghun",
     "Kim"
    ],
    [
     "Ka Hyun",
     "Park"
    ],
    [
     "Hoyoung",
     "Yoon"
    ],
    [
     "U",
     "Kang"
    ]
   ],
   "title": "Domain-Aware Data Selection for Speech Classification via Meta-Reweighting",
   "original": "2368",
   "order": 161,
   "page_count": 5,
   "abstract": [
    "Given speeches from diverse domains, how can we train an accurate classifier for a specific target domain utilizing the other source domains? The problem commonly arises in real-world scenarios, such as identifying the intents of speeches from individuals with a specific speech disorder using those of other disorders. However, existing data selection methods for utilizing the source instances encounter two main challenges: they cannot consider the diversities of source domains, and their hard selection schemes may ignore helpful source instances if the given information of the target domain is insufficient. In this work, we propose DOREME, a domain-aware data selection method for accurate speech classification on a target domain. The key idea is to softly select source instances by dynamically assigning importance scores to each instance based on two similarities: instance-scores and domain-scores. Various experiments show that DOREME achieves the best classification accuracy."
   ],
   "p1": 797,
   "pn": 801,
   "doi": "10.21437/Interspeech.2024-2368",
   "url": "interspeech_2024/kim24u_interspeech.html"
  },
  "cheng24c_interspeech": {
   "authors": [
    [
     "Jiali",
     "Cheng"
    ],
    [
     "Mohamed",
     "Elgaar"
    ],
    [
     "Nidhi",
     "Vakil"
    ],
    [
     "Hadi",
     "Amiri"
    ]
   ],
   "title": "CogniVoice: Multimodal and Multilingual Fusion Networks for Mild Cognitive Impairment Assessment from Spontaneous Speech",
   "original": "2370",
   "order": 886,
   "page_count": 5,
   "abstract": [
    "Mild Cognitive Impairment (MCI) is a medical condition characterized by noticeable declines in memory and cognitive abilities, potentially affecting individual’s daily activities. In this paper, we introduce CogniVoice, a novel multilingual and multimodal framework to detect MCI and estimate Mini-Mental State Examination (MMSE) scores by analyzing speech data and its textual transcriptions. The key component of CogniVoice is an ensemble multimodal and multilingual network based on “Product of Experts” that mitigates reliance on shortcut solutions. Using a comprehensive dataset containing both English and Chinese languages from TAUKADIAL challenge, CogniVoice outperforms the best performing baseline model on MCI classification and MMSE regression tasks by 2.8 and 4.1 points in F1 and RMSE respectively, and can effectively reduce the performance gap across different language groups by 0.7 points in F1."
   ],
   "p1": 4308,
   "pn": 4312,
   "doi": "10.21437/Interspeech.2024-2370",
   "url": "interspeech_2024/cheng24c_interspeech.html"
  },
  "chen24x_interspeech": {
   "authors": [
    [
     "Shaowen",
     "Chen"
    ],
    [
     "Tomoki",
     "Toda"
    ]
   ],
   "title": "QHM-GAN: Neural Vocoder based on Quasi-Harmonic Modeling",
   "original": "2371",
   "order": 801,
   "page_count": 5,
   "abstract": [
    "Neural vocoder has been studied for years, aiming at modeling speech signals and enabling speech signal reconstruction from acoustic features. Unfortunately, the existing end-to-end neural vocoders lack revealing the intrinsic structure of speech due to their black-box nature, thus losing the ability to flexibly synthesize or modify the speech with high quality. Moreover, they usually require complicated networks to generate speech with substantial time consumption. In this paper, we are inspired by the quasi-harmonic model (QHM) and propose a neural vocoder incorporating QHM for network architectures. In this way, speech signals can be parameterized into quasi-harmonic components and be arbitrarily resynthesized with a high quality where the time consumption and network size prominently decrease. The experiments indicate that the proposed method combines the advantages of QHM and neural vocoders and outperforms other methods, such as HiFi-GAN, in terms of generation speed and quality."
   ],
   "p1": 3889,
   "pn": 3893,
   "doi": "10.21437/Interspeech.2024-2371",
   "url": "interspeech_2024/chen24x_interspeech.html"
  },
  "zhang24q_interspeech": {
   "authors": [
    [
     "Tian-Hao",
     "Zhang"
    ],
    [
     "Xinyuan",
     "Qian"
    ],
    [
     "Feng",
     "Chen"
    ],
    [
     "Xu-Cheng",
     "Yin"
    ]
   ],
   "title": "Transmitted and Aggregated Self-Attention for Automatic Speech Recognition",
   "original": "2374",
   "order": 47,
   "page_count": 5,
   "abstract": [
    "Transformer based models have recently achieved outstanding progress in ASR system. The attention maps are generated in self-attention to capture temporal relationships among input tokens and heavily influence transformer performance. Many works demonstrate that attention maps of different layers incorporate various contextual scopes of information. We believe that the information from diverse attention maps is valuable and complementary. This inspires us with a novel proposal, namely Transmitted and Aggregated Self-Attention (TASA), which leverages the information of attention maps in each layer to improve the overall performance. In particular, we design Residual-TASA and Dense-TASA which are distinguished by using attention maps of the previous layer or all previous layers, respectively. Extensive experiments demonstrate that the proposed method achieving up to 10.62% CER and 7.36% WER relative reduction conducted on AISHELL-1 and LibriSpeech datasets, respectively."
   ],
   "p1": 227,
   "pn": 231,
   "doi": "10.21437/Interspeech.2024-2374",
   "url": "interspeech_2024/zhang24q_interspeech.html"
  },
  "li24qa_interspeech": {
   "authors": [
    [
     "Bohan",
     "Li"
    ],
    [
     "Feiyu",
     "Shen"
    ],
    [
     "Yiwei",
     "Guo"
    ],
    [
     "Shuai",
     "Wang"
    ],
    [
     "Xie",
     "Chen"
    ],
    [
     "Kai",
     "Yu"
    ]
   ],
   "title": "On the Effectiveness of Acoustic BPE in Decoder-Only TTS",
   "original": "2375",
   "order": 850,
   "page_count": 5,
   "abstract": [
    "Discretizing speech into tokens and generating them by a decoder-only model have been a promising direction for text-to-speech (TTS) and spoken language modeling (SLM). To shorten the sequence length of speech tokens, acoustic byte-pair encoding (BPE) has emerged in SLM that treats speech tokens from self-supervised semantic representations as characters to further compress the token sequence. But the gain in TTS has not been fully investigated, and the proper choice of acoustic BPE remains unclear. In this work, we conduct a comprehensive study on various settings of acoustic BPE to explore its effectiveness in decoder-only TTS models with semantic speech tokens. Experiments on LibriTTS verify that acoustic BPE uniformly increases the intelligibility and diversity of synthesized speech, while showing different features across BPE settings. Hence, acoustic BPE is a favorable tool for decoder-only TTS."
   ],
   "p1": 4134,
   "pn": 4138,
   "doi": "10.21437/Interspeech.2024-2375",
   "url": "interspeech_2024/li24qa_interspeech.html"
  },
  "javed24_interspeech": {
   "authors": [
    [
     "Tahir",
     "Javed"
    ],
    [
     "Janki",
     "Nawale"
    ],
    [
     "Sakshi",
     "Joshi"
    ],
    [
     "Eldho",
     "George"
    ],
    [
     "Kaushal",
     "Bhogale"
    ],
    [
     "Deovrat",
     "Mehendale"
    ],
    [
     "Mitesh M.",
     "Khapra"
    ]
   ],
   "title": "LAHAJA: A Robust Multi-accent Benchmark for Evaluating Hindi ASR Systems",
   "original": "2376",
   "resource": "https://doi.org/10.5281/zenodo.12742424",
   "order": 479,
   "page_count": 5,
   "abstract": [
    "Hindi, one of the most spoken language of India, exhibits a diverse array of accents due to its usage among individuals from diverse linguistic origins. To enable a robust evaluation of Hindi ASR systems on multiple accents, we create a benchmark, LAHAJA, which contains read and extempore speech on a diverse set of topics and use cases, with a total of 12.5 hours of Hindi audio, sourced from 132 speakers spanning 83 districts of India. We evaluate existing open-source and commercial models on LAHAJA and find their performance to be poor. We then train models using different datasets and find that our model trained on multilingual data with good speaker diversity outperforms existing models by a significant margin. We also present a fine-grained analysis which shows that the performance declines for speakers from North-East and South India, especially with content heavy in named entities and specialized terminology. "
   ],
   "p1": 2320,
   "pn": 2324,
   "doi": "10.21437/Interspeech.2024-2376",
   "url": "interspeech_2024/javed24_interspeech.html"
  },
  "demaere24_interspeech": {
   "authors": [
    [
     "Alexis",
     "DeMaere"
    ],
    [
     "Nicole",
     "van Rootselaar"
    ],
    [
     "Fangfang",
     "Li"
    ],
    [
     "Robbin",
     "Gibb"
    ],
    [
     "Claudia L. R.",
     "Gonzalez"
    ]
   ],
   "title": "On the relationship between speech production and vocabulary size in 3-5 year olds",
   "original": "2377",
   "order": 863,
   "page_count": 5,
   "abstract": [
    "Language acquisition is a multifaceted process, including distinct yet complimentary elements such as word comprehension and speech production . There is evidence that these skills develop together. We studied a group of preschool children to measure the strength of the relationship between receptive vocabulary and speech production at the start and the end of a six-month period, as well as to document any improvement in these domains. A linear regression revealed that vocabulary size at the initial testing time predicted speech production accuracy at the final session, and vice versa. We discuss the null result of no sex differences for either speech or vocabulary. Our results highlight the inter-connectedness of speech production and vocabulary."
   ],
   "p1": 4194,
   "pn": 4198,
   "doi": "10.21437/Interspeech.2024-2377",
   "url": "interspeech_2024/demaere24_interspeech.html"
  },
  "heitkaemper24_interspeech": {
   "authors": [
    [
     "Jens",
     "Heitkaemper"
    ],
    [
     "Joe",
     "Caroselli"
    ],
    [
     "Arun",
     "Narayanan"
    ],
    [
     "Nathan",
     "Howard"
    ]
   ],
   "title": "TfCleanformer: A streaming, array-agnostic, full- and sub-band modeling front-end for robust ASR",
   "original": "2378",
   "order": 919,
   "page_count": 5,
   "abstract": [
    "Multiple recent publications have demonstrated the benefits of neural network based enhancement in the time-frequency domain. This paper builds on those findings to improve upon a recently published streaming, array agnostic multi-channel enhancement system called Cleanformer. The proposed streaming enhancement system achieves competitive results against a non-causal state-of-the-art model on a source separation task, outperforming Cleanformer. Additionally, the presented model improves upon Cleanformer enhancement results in multiple challenging environments without introducing further latency.  A short ablation study is performed to evaluate the influence of the proposed changes on the improved performance."
   ],
   "p1": 4473,
   "pn": 4477,
   "doi": "10.21437/Interspeech.2024-2378",
   "url": "interspeech_2024/heitkaemper24_interspeech.html"
  },
  "lee24m_interspeech": {
   "authors": [
    [
     "Jaeuk",
     "Lee"
    ],
    [
     "Sohee",
     "Jang"
    ],
    [
     "Joon-Hyuk",
     "Chang"
    ]
   ],
   "title": "Neural ATSM: Fully Neural Network-based Adaptive Time-Scale Modification Using Sentence-Specific Dynamic Control",
   "original": "2380",
   "order": 1005,
   "page_count": 5,
   "abstract": [
    "Adaptive time-scale modification (ATSM) adaptively adjusts audio speed and improves upon previous systems by tailoring the scale for each phoneme in two steps: phoneme positioning via Montreal forced aligner (MFA) and reconstruction with adaptive speaking rate. However, ATSM’s phoneme-specific rate is constant regardless of sentences, and MFA struggles with precise phoneme alignment in synthetic speech. Driven by this, we propose a fully neural networks-based ATSM (Neural ATSM) that dynamically controls each phoneme’s speaking rate to vary from sentence to sentence. It predicts phonemelevel rates using a speaking rate predictor and flexibly modifies the scales to fit sentence context using Gaussian upsampling and attention mechanism, ensuring feature similarity with Softdynamic time warping (DTW) loss. We also integrate a variational autoencoder (VAE) and flow models for enhanced timescaled signals. Experimental results show that Neural ATSM outperforms ATSM for real and synthesized speech."
   ],
   "p1": 4903,
   "pn": 4907,
   "doi": "10.21437/Interspeech.2024-2380",
   "url": "interspeech_2024/lee24m_interspeech.html"
  },
  "ng24b_interspeech": {
   "authors": [
    [
     "Sara",
     "Ng"
    ],
    [
     "Gina-Anne",
     "Levow"
    ],
    [
     "Mari",
     "Ostendorf"
    ],
    [
     "Richard",
     "Wright"
    ]
   ],
   "title": "Investigating the Influence of Stance-Taking on Conversational Timing of Task-Oriented Speech",
   "original": "2381",
   "resource": "https://doi.org/10.5281/zenodo.12747360",
   "order": 727,
   "page_count": 5,
   "abstract": [
    "In task-oriented conversations, speakers express their thoughts and attitudes about task objectives through the act of stance-taking. Previous work has shown that stance-taking influences prosodic behaviors such as pitch, intensity, and utterance duration. However, less is known about how stance-taking affects the negotiation of turn-taking behaviors in dyadic conversation. In this work, we analyze the relationship between speaker stance and conversational timing in Pacific Northwest English. We show that there are statistically significant differences in the duration of speech units, intra-speaker pauses and inter-speaker floor transfers when speech contains stance-taking behaviors. Our findings suggest that in addition to other known sources of variation in the timing of turn-taking behaviors, speaker stance measurably influences the time course of conversation."
   ],
   "p1": 3555,
   "pn": 3559,
   "doi": "10.21437/Interspeech.2024-2381",
   "url": "interspeech_2024/ng24b_interspeech.html"
  },
  "prabhu24_interspeech": {
   "authors": [
    [
     "Darshan",
     "Prabhu"
    ],
    [
     "Yifan",
     "Peng"
    ],
    [
     "Preethi",
     "Jyothi"
    ],
    [
     "Shinji",
     "Watanabe"
    ]
   ],
   "title": "MULTI-CONVFORMER: Extending Conformer with Multiple Convolution Kernels",
   "original": "2384",
   "order": 48,
   "page_count": 5,
   "abstract": [
    "Convolutions have become essential in state-of-the-art end-to-end Automatic Speech Recognition (ASR) systems due to their efficient modelling of local context. Notably, its use in Conformers has led to superior performance compared to vanilla Transformer-based ASR systems. While components other than the convolution module in the Conformer have been reexamined, altering the convolution module itself has been far less explored. Towards this, we introduce MULTI-CONVFORMER that uses multiple convolution kernels within the convolution module of the Conformer in conjunction with gating. This helps in improved modeling of local dependencies at varying granularities. Our model rivals existing Conformer variants such as CgMLP and E-Branchformer in performance, while being more parameter efficient. We empirically compare our approach with Conformer and its variants across four different datasets and three different modelling paradigms and show up to 8% relative word error rate (WER) improvements."
   ],
   "p1": 232,
   "pn": 236,
   "doi": "10.21437/Interspeech.2024-2384",
   "url": "interspeech_2024/prabhu24_interspeech.html"
  },
  "gao24f_interspeech": {
   "authors": [
    [
     "Yuan",
     "Gao"
    ],
    [
     "Hao",
     "Shi"
    ],
    [
     "Chenhui",
     "Chu"
    ],
    [
     "Tatsuya",
     "Kawahara"
    ]
   ],
   "title": "Speech Emotion Recognition with Multi-level Acoustic and Semantic Information Extraction and Interaction",
   "original": "2385",
   "order": 221,
   "page_count": 5,
   "abstract": [
    "Speech emotion recognition (SER) systems can learn linguistic information by integrating automatic speech recognition (ASR). However, existing SER systems fall short in explicitly learning semantic emotional information from ASR predictions. Our proposed system addresses this problem by incorporating a semantic feature extractor for explicit emotional information extraction. Furthermore, a cross attention-based information interaction module is proposed to learn the complementary emotional information in the embeddings from both feature extractors. Within the interaction module, a temporal-aware gate fusion network is incorporated to dynamically integrate the embeddings from acoustic and semantic feature extractors and mitigate the impact of ASR errors in SER. Experimental results on IEMOCAP show that our system outperforms the existing SER systems by improving the unweighted accuracy by 3.32%."
   ],
   "p1": 1060,
   "pn": 1064,
   "doi": "10.21437/Interspeech.2024-2385",
   "url": "interspeech_2024/gao24f_interspeech.html"
  },
  "duan24_interspeech": {
   "authors": [
    [
     "Junwen",
     "Duan"
    ],
    [
     "Fangyuan",
     "Wei"
    ],
    [
     "Hong-Dong",
     "Li"
    ],
    [
     "Jin",
     "Liu"
    ]
   ],
   "title": "Pre-trained Feature Fusion and Matching for Mild Cognitive Impairment Detection",
   "original": "2386",
   "order": 194,
   "page_count": 5,
   "abstract": [
    "Effective diagnosis of Mild Cognitive Impairment (MCI), a preclinical stage of cognitive decline, is significant for delaying disease progression. While most current spontaneous speech-based diagnostic methods focus on English speech, the Interspeech 2024 TAUKADIAL Challenge proposed an innovative research direction to develop a language-agnostic approach to diagnose MCI. This paper proposes an MCI diagnosis method by analyzing and combining linguistic and acoustic features using the bilingual Chinese-English speech dataset provided by the challenge. We employed a pre-trained multilingual model and expressivity encoder to extract language-agnostic speech features. To overcome the challenges of data scarcity and language diversity, we implemented data augmentation and alignment to enhance the model's generalization. Our approach achieved 77.5% accuracy, demonstrating its effectiveness and potential on cross-lingual data. "
   ],
   "p1": 962,
   "pn": 966,
   "doi": "10.21437/Interspeech.2024-2386",
   "url": "interspeech_2024/duan24_interspeech.html"
  },
  "kaneko24_interspeech": {
   "authors": [
    [
     "Takuhiro",
     "Kaneko"
    ],
    [
     "Hirokazu",
     "Kameoka"
    ],
    [
     "Kou",
     "Tanaka"
    ],
    [
     "Yuto",
     "Kondo"
    ]
   ],
   "title": "FastVoiceGrad: One-step Diffusion-Based Voice Conversion with Adversarial Conditional Diffusion Distillation",
   "original": "2387",
   "order": 40,
   "page_count": 5,
   "abstract": [
    "Diffusion-based voice conversion (VC) techniques such as VoiceGrad have attracted interest because of their high VC performance in terms of speech quality and speaker similarity. However, a notable limitation is the slow inference caused by the multi-step reverse diffusion. Therefore, we propose FastVoiceGrad, a novel one-step diffusion-based VC that reduces the number of iterations from dozens to one while inheriting the high VC performance of the multi-step diffusion-based VC. We obtain the model using adversarial conditional diffusion distillation (ACDD), leveraging the ability of generative adversarial networks and diffusion models while reconsidering the initial states in sampling. Evaluations of one-shot any-to-any VC demonstrate that FastVoiceGrad achieves VC performance superior to or comparable to that of previous multi-step diffusion-based VC while enhancing the inference speed."
   ],
   "p1": 192,
   "pn": 196,
   "doi": "10.21437/Interspeech.2024-2387",
   "url": "interspeech_2024/kaneko24_interspeech.html"
  },
  "ryu24_interspeech": {
   "authors": [
    [
     "Sangwon",
     "Ryu"
    ],
    [
     "Heejin",
     "Do"
    ],
    [
     "Yunsu",
     "Kim"
    ],
    [
     "Gary Geunbae",
     "Lee"
    ],
    [
     "Jungseul",
     "Ok"
    ]
   ],
   "title": "Key-Element-Informed sLLM Tuning for Document Summarization",
   "original": "2389",
   "order": 397,
   "page_count": 5,
   "abstract": [
    "Remarkable advances in large language models (LLMs) have enabled high-quality text summarization. However, this capability is currently accessible only through LLMs of substantial size or proprietary LLMs with usage fees. In response, smaller-scale LLMs (sLLMs) of easy accessibility and low costs have been extensively studied, yet they often suffer from missing key information and entities, i.e., low relevance, in particular, when input documents are long. We hence propose a key-element-informed instruction tuning for summarization, so-called KEITSum, which identifies key elements in documents and instructs sLLM to generate summaries capturing these key elements. Experimental results on dialogue and news datasets demonstrate that sLLM with KEITSum indeed provides high-quality summarization with higher relevance and less hallucinations, competitive to proprietary LLM."
   ],
   "p1": 1940,
   "pn": 1944,
   "doi": "10.21437/Interspeech.2024-2389",
   "url": "interspeech_2024/ryu24_interspeech.html"
  },
  "kim24v_interspeech": {
   "authors": [
    [
     "Haechan",
     "Kim"
    ],
    [
     "Junho",
     "Myung"
    ],
    [
     "Seoyoung",
     "Kim"
    ],
    [
     "Sungpah",
     "Lee"
    ],
    [
     "Dongyeop",
     "Kang"
    ],
    [
     "Juho",
     "Kim"
    ]
   ],
   "title": "LearnerVoice: A Dataset of Non-Native English Learners’ Spontaneous Speech",
   "original": "2392",
   "order": 480,
   "page_count": 5,
   "abstract": [
    "Prevalent ungrammatical expressions and disfluencies in spontaneous speech from second language (L2) learners pose unique challenges to Automatic Speech Recognition (ASR) systems. However, few datasets are tailored to L2 learner speech. We publicly release LearnerVoice, a dataset consisting of 50.04 hours of audio and transcriptions of L2 learners’ spontaneous speech. Our linguistic analysis reveals that transcriptions in our dataset contain L2S (L2 learner’s Spontaneous speech) features, consisting of ungrammatical expressions and disfluencies (e.g., filler words, word repetitions, self-repairs, false starts), significantly more than native speech datasets. Fine-tuning whisper-small.en with LearnerVoice achieves a WER of 10.26%, 44.2% lower than vanilla whisper-small.en. Furthermore, our qualitative analysis indicates that 54.2% of errors from the vanilla model on LearnerVoice are attributable to L2S features, with 48.1% of them being reduced in the fine-tuned model."
   ],
   "p1": 2325,
   "pn": 2329,
   "doi": "10.21437/Interspeech.2024-2392",
   "url": "interspeech_2024/kim24v_interspeech.html"
  },
  "chang24d_interspeech": {
   "authors": [
    [
     "Kalvin",
     "Chang"
    ],
    [
     "Yi-Hui",
     "Chou"
    ],
    [
     "Jiatong",
     "Shi"
    ],
    [
     "Hsuan-Ming",
     "Chen"
    ],
    [
     "Nicole",
     "Holliday"
    ],
    [
     "Odette",
     "Scharenborg"
    ],
    [
     "David R.",
     "Mortensen"
    ]
   ],
   "title": "Self-supervised Speech Representations Still Struggle with African American Vernacular English",
   "original": "2394",
   "order": 953,
   "page_count": 5,
   "abstract": [
    "Underperformance of ASR systems for speakers of African American Vernacular English (AAVE) and other marginalized language varieties is a welldocumented phenomenon, and one that reinforces the stigmatization of these varieties. We investigate whether or not the recent wave of Self-Supervised Learning (SSL) speech models can close the gap in ASR performance between AAVE and Mainstream American English (MAE). We evaluate four SSL models (wav2vec 2.0, HuBERT, WavLM, and XLS-R) on zero-shot Automatic Speech Recognition (ASR) for these two varieties and find that these models perpetuate the bias in performance against AAVE. Additionally, the models have higher word error rates on utterances with more phonological and morphosyntactic features of AAVE. Despite the success of SSL speech models in improving ASR for low resource varieties, SSL pre-training alone may not bridge the gap between AAVE and MAE."
   ],
   "p1": 4643,
   "pn": 4647,
   "doi": "10.21437/Interspeech.2024-2394",
   "url": "interspeech_2024/chang24d_interspeech.html"
  },
  "bhogale24_interspeech": {
   "authors": [
    [
     "Kaushal Santosh",
     "Bhogale"
    ],
    [
     "Deovrat",
     "Mehendale"
    ],
    [
     "Niharika",
     "Parasa"
    ],
    [
     "Sathish Kumar Reddy",
     "G"
    ],
    [
     "Tahir",
     "Javed"
    ],
    [
     "Pratyush",
     "Kumar"
    ],
    [
     "Mitesh M.",
     "Khapra"
    ]
   ],
   "title": "Empowering Low-Resource Language ASR via Large-Scale Pseudo Labeling",
   "original": "2396",
   "order": 519,
   "page_count": 5,
   "abstract": [
    "In this study, we tackle the challenge of limited labeled data for low-resource languages in ASR, focusing on Hindi. Specifically, we explore pseudo-labeling, by proposing a generic framework combining multiple ideas from existing works. Our framework integrates multiple base models for transcription and evaluators for assessing audio-transcript pairs, resulting in robust pseudo-labeling for low resource languages. We validate our approach with a new benchmark, IndicYT, comprising diverse YouTube audio files from multiple content categories. Our findings show that augmenting pseudo labeled data from YouTube with existing training data leads to significant performance improvements on IndicYT, without affecting performance on out-of-domain benchmarks, demonstrating the efficacy of pseudo-labeled data in enhancing ASR capabilities for low-resource languages. The benchmark, code and models developed as a part of this work will be made publicly available."
   ],
   "p1": 2519,
   "pn": 2523,
   "doi": "10.21437/Interspeech.2024-2396",
   "url": "interspeech_2024/bhogale24_interspeech.html"
  },
  "aluru24_interspeech": {
   "authors": [
    [
     "Sai Harshitha",
     "Aluru"
    ],
    [
     "Jhansi",
     "Mallela"
    ],
    [
     "Chiranjeevi",
     "Yarra"
    ]
   ],
   "title": "Post-Net: A linguistically inspired sequence-dependent transformed neural architecture for automatic syllable stress detection",
   "original": "2400",
   "order": 684,
   "page_count": 5,
   "abstract": [
    "Automatic syllable stress detection methods typically consider syllable-level features as independent. However, as per linguistic studies, there is a dependency among the syllables within a word. In this work, we address this issue by proposing a Post-Net approach using Time-Delay Neural Networks to exploit the syllable dependency in a word for stress detection task. For this, we propose a loss function to incorporate the dependency by ensuring only one stressed syllable in a word. The proposed Post-Net leverages the existing SOTA sequence-independent stress detection models and learns in both supervised and unsupervised settings. We compare the Post-Net with three existing SOTA sequence-independent models and also with sequential model (LSTMs).  Experiments conducted on ISLE corpus show the highest relative accuracy improvement of  2.1% and 20.28%  with the proposed Post-Net compared to the best sequence-independent SOTA model in supervised and unsupervised manners, respectively."
   ],
   "p1": 3340,
   "pn": 3344,
   "doi": "10.21437/Interspeech.2024-2400",
   "url": "interspeech_2024/aluru24_interspeech.html"
  },
  "atkins24_interspeech": {
   "authors": [
    [
     "Conor",
     "Atkins"
    ],
    [
     "Ian",
     "Wood"
    ],
    [
     "Mohamed Ali",
     "Kaafar"
    ],
    [
     "Hassan",
     "Asghar"
    ],
    [
     "Nardine",
     "Basta"
    ],
    [
     "Michal",
     "Kepkowski"
    ]
   ],
   "title": "ConvoCache: Smart Re-Use of Chatbot Responses",
   "original": "2402",
   "resource": "https://doi.org/10.5281/zenodo.12735947",
   "order": 606,
   "page_count": 5,
   "abstract": [
    "We present ConvoCache, a conversational caching system that solves the problem of slow and expensive generative AI models in spoken chatbots. ConvoCache finds a semantically similar prompt in the past and reuses the response. In this paper we evaluate ConvoCache on the DailyDialog dataset. We find that ConvoCache can apply a UniEval coherence threshold of 90% and respond to 89% of prompts using the cache with an average latency of 214ms, replacing LLM and voice synthesis that can take over 1s. To further reduce latency we test prefetching and find limited usefulness. Prefetching with 80% of a request leads to a 63% hit rate, and a drop in overall coherence. ConvoCache can be used with any chatbot to reduce costs by reducing usage of generative AI by up to 89%."
   ],
   "p1": 2950,
   "pn": 2954,
   "doi": "10.21437/Interspeech.2024-2402",
   "url": "interspeech_2024/atkins24_interspeech.html"
  },
  "ratsep24_interspeech": {
   "authors": [
    [
     "Liisa",
     "Rätsep"
    ],
    [
     "Rasmus",
     "Lellep"
    ],
    [
     "Mark",
     "Fishel"
    ]
   ],
   "title": "Enabling Conversational Speech Synthesis using Noisy Spontaneous Data",
   "original": "2403",
   "order": 1009,
   "page_count": 5,
   "abstract": [
    "In recent years, the quality of text-to-speech models has increased significantly, but most text-to-speech solutions are trained on datasets of read speech and do not cover conversational speaking styles due to the lack of suitable training data. This paper explores options for creating multi-style speech synthesis using speech recognition datasets that contain samples of spontaneous speech and dialogues but may also include background noise and an insufficient number of samples per speaker. We develop an Estonian multi-speaker TTS system that increases prosodic variability on conversational inputs while still being able to synthesize read speech. We show that our proposed approach can be used to train models that can be controlled to produce conversational speech with little compromise on audio quality. We also highlight a potential multilingual use case to achieve cross-lingual speaker and style transfer to low-resource languages that lack stylistically diverse speech corpora."
   ],
   "p1": 4923,
   "pn": 4927,
   "doi": "10.21437/Interspeech.2024-2403",
   "url": "interspeech_2024/ratsep24_interspeech.html"
  },
  "mallela24_interspeech": {
   "authors": [
    [
     "Jhansi",
     "Mallela"
    ],
    [
     "Sai Harshitha",
     "Aluru"
    ],
    [
     "Chiranjeevi",
     "Yarra"
    ]
   ],
   "title": "A comparative analysis of sequential models that integrate syllable dependency for automatic syllable stress detection",
   "original": "2404",
   "order": 789,
   "page_count": 5,
   "abstract": [
    "Automatic syllable stress detection is typically operated at syllable level with stress-related acoustic features.  The stress placed on a syllable is influenced not only by its own characteristics but also by its context in the word.  However, traditional methods for stress detection overlook the contextual acoustic factors that influence stress placement. By addressing this issue, we study sequential modeling approaches by integrating the syllable dependency for automatic syllable stress detection using a masking strategy. This approach considers a sequence of syllables at the word level and identifies its stress label sequence. We explore various sequential models, such as RNNs, LSTMs, GRUs, and Attention networks. We conduct experiments on the ISLE corpus comprising non-native speakers speaking English. From the experiments, we observe a significant improvement in the performance with all sequential models compared to the state-of-the-art non-sequential baseline (DNN)."
   ],
   "p1": 3829,
   "pn": 3833,
   "doi": "10.21437/Interspeech.2024-2404",
   "url": "interspeech_2024/mallela24_interspeech.html"
  },
  "lv24_interspeech": {
   "authors": [
    [
     "Yuanjun",
     "Lv"
    ],
    [
     "Hai",
     "Li"
    ],
    [
     "Ying",
     "Yan"
    ],
    [
     "Junhui",
     "Liu"
    ],
    [
     "Danming",
     "Xie"
    ],
    [
     "Lei",
     "Xie"
    ]
   ],
   "title": "FreeV: Free Lunch For Vocoders Through Pseudo Inversed Mel Filter",
   "original": "2407",
   "order": 797,
   "page_count": 5,
   "abstract": [
    "Vocoders reconstruct speech waveforms from acoustic features and play a pivotal role in modern TTS systems. Frequentdomain GAN vocoders like Vocos and APNet2 have recently seen rapid advancements, outperforming time-domain models in inference speed while achieving comparable audio quality. However, these frequency-domain vocoders suffer from large parameter sizes, thus introducing extra memory burden. Inspired by PriorGrad and SpecGrad, we employ pseudo-inverse to estimate the amplitude spectrum as the initialization roughly. This simple initialization significantly mitigates the parameter demand for vocoder. Based on APNet2 and our streamlined Amplitude prediction branch, we propose our FreeV, compared with its counterpart APNet2, our FreeV achieves 1.8× inference speed improvement with nearly half parameters. Meanwhile, our FreeV outperforms APNet2 in resynthesis quality, marking a step forward in pursuing real-time, highfidelity speech synthesis. Code and checkpoints is available at: https://github.com/BakerBunker/FreeV"
   ],
   "p1": 3869,
   "pn": 3873,
   "doi": "10.21437/Interspeech.2024-2407",
   "url": "interspeech_2024/lv24_interspeech.html"
  },
  "lin24m_interspeech": {
   "authors": [
    [
     "Jiayan",
     "Lin"
    ],
    [
     "Shenghui",
     "Lu"
    ],
    [
     "Hukai",
     "Huang"
    ],
    [
     "Wenhao",
     "Guan"
    ],
    [
     "Binbin",
     "Xu"
    ],
    [
     "Hui",
     "Bu"
    ],
    [
     "Qingyang",
     "Hong"
    ],
    [
     "Lin",
     "Li"
    ]
   ],
   "title": "MinSpeech: A Corpus of Southern Min Dialect for Automatic Speech Recognition",
   "original": "2414",
   "resource": "https://doi.org/10.5281/zenodo.12903814",
   "order": 481,
   "page_count": 5,
   "abstract": [
    "This paper presents MinSpeech, a speech corpus of Southern Min (also known as Hokkien), to propel research in dialect speech recognition. Despite the linguistic and cultural importance of Southern Min, there is still a notable scarcity of publicly accessible speech corpus for this dialect. MinSpeech provides 2237 hours of unlabeled audio and 1778 hours of labeled audio, sourced diversely and encompassing various contexts. Mandarin text is employed as labels to enable cross-linguistic alignment and transformation. Using this corpus, we have developed baseline systems, including supervised models (Kaldi Chain and Conformer) and two self-supervised models (Wav2vec 2.0 and HuBERT). These systems were assessed on an automatic speech recognition (ASR) task to the Southern Min dialect. Experiments illustrate that the corpus offers practical assistance and resources for speech processing of this dialect. MinSpeech dataset is available at https://minspeech.github.io/."
   ],
   "p1": 2330,
   "pn": 2334,
   "doi": "10.21437/Interspeech.2024-2414",
   "url": "interspeech_2024/lin24m_interspeech.html"
  },
  "sharma24_interspeech": {
   "authors": [
    [
     "Chetan",
     "Sharma"
    ],
    [
     "Vaishnavi",
     "Chandwanshi"
    ],
    [
     "Prasanta Kumar",
     "Ghosh"
    ]
   ],
   "title": "A comparative study of the impact of voiceless alveolar and palato-alveolar sibilants in English on lip aperture and protrusion during VCV production",
   "original": "2415",
   "order": 636,
   "page_count": 5,
   "abstract": [
    "Lip rounding and protrusion during sibilant production are known. Whether these features are discriminative among sibilants and how the discrimination changes in different vowels context are not well explored. In this work, we consider two voiceless sibilants, namely, /s/ (alveolar sibilant) and /ʃ/ (palato-alveolar sibilant) in English during VCV production and show that lip aperture (LA) and lip protrusion (LP) are significantly higher in case of /ʃ/ than /s/ irrespective of the vowels (/a/, /i/, /u/) context. Using the USC Speech MRI database comprising 74 subjects speaking VCV sequence, we also show that, when used for /s/ vs /ʃ/ automatic classification, LA provides the highest classification accuracy of 90.57%(±5.91%) in case of /i/ followed by 85.09%(±5.19%) and 82.38%(±8.76%) in case of /u/ and /a/, respectively. The change in LA and LP from /s/ to /ʃ/ are seen as an effect of higher displacement of lower lip than that of upper lip for /a/ and /i/ unlike that for /u/."
   ],
   "p1": 3100,
   "pn": 3104,
   "doi": "10.21437/Interspeech.2024-2415",
   "url": "interspeech_2024/sharma24_interspeech.html"
  },
  "anand24_interspeech": {
   "authors": [
    [
     "Srija",
     "Anand"
    ],
    [
     "Praveen",
     "Srinivasa Varadhan"
    ],
    [
     "Ashwin",
     "Sankar"
    ],
    [
     "Giri",
     "Raju"
    ],
    [
     "Mitesh M.",
     "Khapra"
    ]
   ],
   "title": "Enhancing Out-of-Vocabulary Performance of Indian TTS Systems for Practical Applications through Low-Effort Data Strategies",
   "original": "2418",
   "order": 249,
   "page_count": 5,
   "abstract": [
    "Publicly available TTS datasets for low-resource languages like Hindi and Tamil typically contain 10-20 hours of data, leading to poor vocabulary coverage. This limitation becomes evident in downstream applications where domain-specific vocabulary coupled with frequent code-mixing with English, results in many OOV words. To highlight this problem, we create a benchmark containing OOV words from several real-world applications. Indeed, state-of-the-art Hindi and Tamil TTS systems perform poorly on this OOV benchmark, as indicated by intelligibility tests. To improve the model’s OOV performance, we propose a low-effort and economically viable strategy to obtain more training data. Specifically, we propose using volunteers as opposed to high quality voice artists to record words containing character bigrams unseen in the training data. We show that using such inexpensive data, the model's performance improves on OOV words, while not affecting voice quality and in-domain performance."
   ],
   "p1": 1200,
   "pn": 1204,
   "doi": "10.21437/Interspeech.2024-2418",
   "url": "interspeech_2024/anand24_interspeech.html"
  },
  "tang24d_interspeech": {
   "authors": [
    [
     "Changli",
     "Tang"
    ],
    [
     "Wenyi",
     "Yu"
    ],
    [
     "Guangzhi",
     "Sun"
    ],
    [
     "Xianzhao",
     "Chen"
    ],
    [
     "Tian",
     "Tan"
    ],
    [
     "Wei",
     "Li"
    ],
    [
     "Jun",
     "Zhang"
    ],
    [
     "Lu",
     "Lu"
    ],
    [
     "Zejun",
     "Ma"
    ],
    [
     "Yuxuan",
     "Wang"
    ],
    [
     "Chao",
     "Zhang"
    ]
   ],
   "title": "Can Large Language Models Understand Spatial Audio?",
   "original": "2419",
   "order": 853,
   "page_count": 5,
   "abstract": [
    "This paper explores enabling large language models (LLMs) to understand spatial information from multichannel audio, a skill currently lacking in auditory LLMs. By leveraging LLMs’ advanced cognitive and inferential abilities, the aim is to enhance understanding of 3D environments via audio. We study 3 spatial audio tasks: sound source localization (SSL), far-field speech recognition (FSR), and localisation-informed speech extraction (LSE), achieving notable progress in each task. For SSL, our approach achieves an MAE of 2.70◦ on the Spatial LibriSpeech dataset, substantially surpassing the prior benchmark of about 6.60◦. Moreover, our model can employ spatial cues to improve FSR accuracy and execute LSE by selectively attending to sounds originating from a specified direction via text prompts, even amidst overlapping speech. These findings highlight the potential of adapting LLMs to grasp physical audio concepts, paving the way for LLM-based agents in 3D environments."
   ],
   "p1": 4149,
   "pn": 4153,
   "doi": "10.21437/Interspeech.2024-2419",
   "url": "interspeech_2024/tang24d_interspeech.html"
  },
  "srinivasavaradhan24_interspeech": {
   "authors": [
    [
     "Praveen",
     "Srinivasa Varadhan"
    ],
    [
     "Ashwin",
     "Sankar"
    ],
    [
     "Giri",
     "Raju"
    ],
    [
     "Mitesh M",
     "Khapra"
    ]
   ],
   "title": "Rasa: Building Expressive Speech Synthesis Systems for Indian Languages in Low-resource Settings",
   "original": "2421",
   "resource": "https://doi.org/10.5281/zenodo.12772958",
   "order": 375,
   "page_count": 5,
   "abstract": [
    "We release Rasa, the first multilingual expressive TTS dataset for any Indian language, which contains 10 hours of neutral speech and 1-3 hours of expressive speech for each of the 6 Ekman emotions covering 3 languages: Assamese, Bengali, & Tamil. Our ablation studies reveal that just 1 hour of neutral and 30 minutes of expressive data can yield a Fair system as indicated by MUSHRA scores. Increasing neutral data to 10 hours, with minimal expressive data, significantly enhances expressiveness. This offers a practical recipe for resource-constrained languages, prioritizing easily obtainable neutral data alongside smaller amounts of expressive data. We show the importance of syllabically balanced data and pooling emotions to enhance expressiveness. We also highlight challenges in generating specific emotions, e.g., fear and surprise."
   ],
   "p1": 1830,
   "pn": 1834,
   "doi": "10.21437/Interspeech.2024-2421",
   "url": "interspeech_2024/srinivasavaradhan24_interspeech.html"
  },
  "tan24b_interspeech": {
   "authors": [
    [
     "Haotian",
     "Tan"
    ],
    [
     "Sakriani",
     "Sakti"
    ]
   ],
   "title": "Contrastive Feedback Mechanism for Simultaneous Speech Translation",
   "original": "2426",
   "order": 172,
   "page_count": 5,
   "abstract": [
    "Recent advances in simultaneous speech translation (SST) focus on the decision policies that enable the use of offline-trained ST models for simultaneous inference. These decision policies not only control the quality-latency trade-off in SST but also mitigate the impact of unstable predictions on translation quality by delaying translation for more context or discarding these predictions through stable hypothesis detection. However, these policies often overlook the potential benefits of utilizing unstable predictions. We introduce the contrastive feedback mechanism (CFM) for SST, a novel method that leverages these unstable predictions as feedback to improve translation quality. CFM guides the system to eliminate undesired model behaviors from these predictions through a contrastive objective. The experiments on 3 state-of-the-art decision policies across 8 languages in the MuST-C v1.0 dataset show that CFM effectively improves the performance of SST."
   ],
   "p1": 852,
   "pn": 856,
   "doi": "10.21437/Interspeech.2024-2426",
   "url": "interspeech_2024/tan24b_interspeech.html"
  },
  "tammen24_interspeech": {
   "authors": [
    [
     "Marvin",
     "Tammen"
    ],
    [
     "Tsubasa",
     "Ochiai"
    ],
    [
     "Marc",
     "Delcroix"
    ],
    [
     "Tomohiro",
     "Nakatani"
    ],
    [
     "Shoko",
     "Araki"
    ],
    [
     "Simon",
     "Doclo"
    ]
   ],
   "title": "Array Geometry-Robust Attention-Based Neural Beamformer for Moving Speakers",
   "original": "2427",
   "order": 685,
   "page_count": 5,
   "abstract": [
    "Although mask-based beamforming is a powerful speech enhancement approach, it often requires manual parameter tuning to handle moving speakers. Recently, this approach was augmented with an attention-based spatial covariance matrix aggregator (ASA) module, enabling accurate tracking of moving speakers without manual tuning. However, the deep neural network model used in this module is limited to specific microphone arrays, necessitating a different model for varying channel permutations, numbers, or geometries. To improve the robustness of the ASA module against such variations, in this paper we investigate three approaches: training with random channel configurations, employing the transform-average-concatenate method to process multi-channel input features, and utilizing robust input features. Our experiments on the CHiME-3 and DEMAND datasets show that these approaches enable the ASA-augmented beamformer to track moving speakers across different microphone arrays unseen in training."
   ],
   "p1": 3345,
   "pn": 3349,
   "doi": "10.21437/Interspeech.2024-2427",
   "url": "interspeech_2024/tammen24_interspeech.html"
  },
  "lu24f_interspeech": {
   "authors": [
    [
     "Yi",
     "Lu"
    ],
    [
     "Yuankun",
     "Xie"
    ],
    [
     "Ruibo",
     "Fu"
    ],
    [
     "Zhengqi",
     "Wen"
    ],
    [
     "Jianhua",
     "Tao"
    ],
    [
     "Zhiyong",
     "Wang"
    ],
    [
     "Xin",
     "Qi"
    ],
    [
     "Xuefei",
     "Liu"
    ],
    [
     "Yongwei",
     "Li"
    ],
    [
     "Yukun",
     "Liu"
    ],
    [
     "Xiaopeng",
     "Wang"
    ],
    [
     "Shuchen",
     "Shi"
    ]
   ],
   "title": "Codecfake: An Initial Dataset for Detecting LLM-based Deepfake Audio",
   "original": "2428",
   "order": 287,
   "page_count": 5,
   "abstract": [
    "With the proliferation of Large Language Model (LLM) based deepfake audio, there is an urgent need for effective detection methods. Previous deepfake audio generation methods typically involve a multi-step generation process, with the final step using a vocoder to predict the waveform from hand-crafted features. However, LLM-based audio is directly generated from discrete neural codecs in an end-to-end generation process, skipping the final step of vocoder processing. This poses a significant challenge for current audio deepfake detection (ADD) models based on vocoder artifacts. To effectively detect LLM-based deepfake audio, we focus on the core of the generation process, the conversion from neural codec to waveform. We propose Codecfake dataset, which is generated by seven representative neural codec methods.  Experiment results show that codec-trained ADD models exhibit a 41.406% reduction in average equal error rate compared to vocoder-trained ADD models on the Codecfake test set."
   ],
   "p1": 1390,
   "pn": 1394,
   "doi": "10.21437/Interspeech.2024-2428",
   "url": "interspeech_2024/lu24f_interspeech.html"
  },
  "prabhu24b_interspeech": {
   "authors": [
    [
     "Darshan",
     "Prabhu"
    ],
    [
     "Abhishek",
     "Gupta"
    ],
    [
     "Omkar",
     "Nitsure"
    ],
    [
     "Preethi",
     "Jyothi"
    ],
    [
     "Sriram",
     "Ganapathy"
    ]
   ],
   "title": "Improving Self-supervised Pre-training using Accent-Specific Codebooks",
   "original": "2438",
   "order": 477,
   "page_count": 5,
   "abstract": [
    "Speech accents present a serious challenge to the performance of state-of-the-art end-to-end Automatic Speech Recognition (ASR) systems. Even with self-supervised learning and pre-training of ASR models, accent invariance is seldom achieved. In this work, we propose an accent-aware adaptation technique for self-supervised learning that introduces a trainable set of accent-specific codebooks to the self-supervised architecture. These learnable codebooks enable the model to capture accent specific information during pre-training, that is further refined during ASR finetuning. On the Mozilla Common Voice dataset, our proposed approach outperforms all other accent-adaptation approaches on both seen and unseen English accents, with up to 9% relative reduction in word error rate (WER). "
   ],
   "p1": 2310,
   "pn": 2314,
   "doi": "10.21437/Interspeech.2024-2438",
   "url": "interspeech_2024/prabhu24b_interspeech.html"
  },
  "osman24_interspeech": {
   "authors": [
    [
     "Mohamed",
     "Osman"
    ],
    [
     "Daniel Z.",
     "Kaplan"
    ],
    [
     "Tamer",
     "Nadeem"
    ]
   ],
   "title": "SER Evals: In-domain and Out-of-domain benchmarking for speech emotion recognition",
   "original": "2440",
   "order": 288,
   "page_count": 5,
   "abstract": [
    "Speech emotion recognition (SER) has made significant strides with the advent of powerful self-supervised learning (SSL) models. However, the generalization of these models to diverse languages and emotional expressions remains a challenge. We propose a large-scale  benchmark to evaluate the robustness and adaptability of state-of-the-art SER models in both in-domain and out-of-domain settings. Our benchmark includes a diverse set of multilingual datasets, focusing on less commonly used corpora to assess generalization to new data. We employ logit adjustment to account for varying class distributions and establish a single dataset cluster for systematic evaluation. Surprisingly, we find that the Whisper model, primarily designed for automatic speech recognition, outperforms dedicated SSL models in cross-lingual SER. Our results highlight the need for more robust and generalizable SER models, and our benchmark serves as a valuable resource to drive future research in this direction."
   ],
   "p1": 1395,
   "pn": 1399,
   "doi": "10.21437/Interspeech.2024-2440",
   "url": "interspeech_2024/osman24_interspeech.html"
  },
  "vakirtzian24_interspeech": {
   "authors": [
    [
     "Socrates",
     "Vakirtzian"
    ],
    [
     "Chara",
     "Tsoukala"
    ],
    [
     "Stavros",
     "Bompolas"
    ],
    [
     "Katerina",
     "Mouzou"
    ],
    [
     "Vivian",
     "Stamou"
    ],
    [
     "Georgios",
     "Paraskevopoulos"
    ],
    [
     "Antonios",
     "Dimakis"
    ],
    [
     "Stella",
     "Markantonatou"
    ],
    [
     "Angela",
     "Ralli"
    ],
    [
     "Antonios",
     "Anastasopoulos"
    ]
   ],
   "title": "Speech Recognition for Greek Dialects: A Challenging Benchmark",
   "original": "2443",
   "order": 818,
   "page_count": 5,
   "abstract": [
    "Language technologies should be judged on their usefulness in real-world use cases. Despite recent impressive progress in automatic speech recognition (ASR), an often overlooked aspect in ASR research and evaluation is language variation in the form of non-standard dialects or language varieties.  To this end, this work introduces a challenging benchmark that focuses on four varieties of Greek (Aivaliot, Cretan, Griko, Messenian) encompassing challenges related to data availability, orthographic conventions, and complexities arising from language contact. Initial experiments with state-of-the-art models and established cross-lingual transfer techniques highlight the difficulty of adapting to such low-resource varieties."
   ],
   "p1": 3974,
   "pn": 3978,
   "doi": "10.21437/Interspeech.2024-2443",
   "url": "interspeech_2024/vakirtzian24_interspeech.html"
  },
  "li24ra_interspeech": {
   "authors": [
    [
     "Vivian G.",
     "Li"
    ]
   ],
   "title": "In search of structure and correspondence in intra-speaker trial-to-trial variability",
   "original": "2456",
   "order": 92,
   "page_count": 5,
   "abstract": [
    "Intra-speaker variability is present even when the talker is uttering the same words in the same social and linguistic context. Studies have revealed that such intra-speaker trial-to-trial variability is connected to speech perception and is actively regulated during speech production. However, the relevant parameters in the variability that are under active regulation remain largely unclear. This study contributes to the discussion by examining the distributional properties of intra-speaker variability. Following up on a study that showed formants on hundreds of repetitions of the same word, measured at different points along the trajectory, are all normally distributed, we ask if those normal distributions correspond, i.e. whether a particular repetition would hold a stable position in the distributions across measurement points. Our analysis of 300 repetitions of /i, oU/ showed that strong correspondence typically spans one to two measurement points, and the strength of correspondence is phonemedependent and position sensitive."
   ],
   "p1": 452,
   "pn": 456,
   "doi": "10.21437/Interspeech.2024-2456",
   "url": "interspeech_2024/li24ra_interspeech.html"
  },
  "ciaperoni24_interspeech": {
   "authors": [
    [
     "Martino",
     "Ciaperoni"
    ],
    [
     "Athanasios",
     "Katsamanis"
    ],
    [
     "Aristides",
     "Gionis"
    ],
    [
     "Panagiotis",
     "Karras"
    ]
   ],
   "title": "Beam-search SIEVE for low-memory speech recognition",
   "original": "2457",
   "order": 56,
   "page_count": 5,
   "abstract": [
    "A capacity to recognize speech offline eliminates privacy concerns and the need for an internet connection. Despite efforts to reduce the memory demands of speech recognition systems, these demands remain formidable and thus popular tools such as Kaldi run best via cloud computing. The key bottleneck arises form the fact that a bedrock of such tools, the Viterbi algorithm, requires memory that grows linearly with utterance length even when contained via beam search. A recent recasting of the Viterbi algorithm, SIEVE, eliminates the path length factor from space complexity, but with a significant practical runtime overhead. In this paper, we develop a variant of SIEVE that lessens this runtime overhead via beam search, retains the decoding quality of standard beam search, and waives its linearly growing memory bottleneck. This space-complexity reduction is orthogonal to decoding quality and complementary to memory savings in model representation and training."
   ],
   "p1": 272,
   "pn": 276,
   "doi": "10.21437/Interspeech.2024-2457",
   "url": "interspeech_2024/ciaperoni24_interspeech.html"
  },
  "kalda24_interspeech": {
   "authors": [
    [
     "Joonas",
     "Kalda"
    ],
    [
     "Tanel",
     "Alumae"
    ],
    [
     "Martin",
     "Lebourdais"
    ],
    [
     "Hervé",
     "Bredin"
    ],
    [
     "Séverin",
     "Baroudi"
    ],
    [
     "Ricard",
     "Marxer"
    ]
   ],
   "title": "TalTech-IRIT-LIS Speaker and Language Diarization Systems for DISPLACE 2024",
   "original": "2462",
   "order": 336,
   "page_count": 5,
   "abstract": [
    "This paper describes the submissions of team TalTech-IRIT-LIS to the DISPLACE 2024 challenge. Our team participated in the speaker diarization and language diarization tracks of the challenge. In the speaker diarization track, our best submission was an ensemble of systems based on the pyannote.audio speaker diarization pipeline utilizing powerset training and our recently proposed PixIT method that performs joint diarization and speech separation. We improve upon PixIT by using the separation outputs for speaker embedding extraction. Our ensemble achieved a diarization error rate of 27.1% on the evaluation dataset. In the language diarization track, we fine-tuned a pre-trained Wav2Vec2-BERT language embedding model on in-domain data, and clustered short segments using AHC and VBx, based on similarity scores from LDA/PLDA. This led to a language diarization error rate of 27.6% on the evaluation data. Both results were ranked first in their respective challenge tracks."
   ],
   "p1": 1635,
   "pn": 1639,
   "doi": "10.21437/Interspeech.2024-2462",
   "url": "interspeech_2024/kalda24_interspeech.html"
  },
  "tan24c_interspeech": {
   "authors": [
    [
     "Zhenxiong",
     "Tan"
    ],
    [
     "Xinyin",
     "Ma"
    ],
    [
     "Gongfan",
     "Fang"
    ],
    [
     "Xinchao",
     "Wang"
    ]
   ],
   "title": "LiteFocus: Accelerated Diffusion Inference for Long Audio Synthesis",
   "original": "2467",
   "order": 1000,
   "page_count": 5,
   "abstract": [
    "Latent diffusion models have shown promising results in audio generation, making notable advancements over traditional methods. However, their performance, while impressive with short audio clips, faces challenges when extended to longer audio sequences. These challenges are due to model’s selfattention mechanism and training predominantly on 10-second clips, which complicates the extension to longer audio without adaptation. In response to these issues, we introduce a novel approach, LiteFocus that enhances the inference of existing audio latent diffusion models in long audio synthesis. Observed the attention pattern in self-attention, we employ a dual sparse form for attention calculation, designated as same-frequency focus and cross-frequency compensation, which curtails the attention computation under same-frequency constraints, while enhancing audio quality through cross-frequency refillment. LiteFocus demonstrates substantial reduction on inference time with diffusion-based TTA model by 1.99× in synthesizing 80-second audio clips while also obtaining improved audio quality."
   ],
   "p1": 4878,
   "pn": 4882,
   "doi": "10.21437/Interspeech.2024-2467",
   "url": "interspeech_2024/tan24c_interspeech.html"
  },
  "wang24ma_interspeech": {
   "authors": [
    [
     "Tianhao",
     "Wang"
    ],
    [
     "Lantian",
     "Li"
    ],
    [
     "Dong",
     "Wang"
    ]
   ],
   "title": "SE/BN Adapter: Parametric Efficient Domain Adaptation for Speaker Recognition",
   "original": "2476",
   "order": 444,
   "page_count": 5,
   "abstract": [
    "Deploying a well-optimized pre-trained speaker recognition model in a new domain often leads to a significant decline in performance. While fine-tuning is a commonly employed solution, it demands ample adaptation data and suffers from parameter inefficiency, rendering it impractical for real-world applications with limited data available for model adaptation. Drawing inspiration from the success of adapters in self-supervised pre-trained models, this paper introduces a SE/BN adapter to address this challenge. By freezing the core speaker encoder and adjusting the feature maps’ weights and activation distributions, we introduce a novel adapter utilizing trainable squeeze-and-excitation (SE) blocks and batch normalization (BN) layers, termed SE/BN adapter. Our experiments, conducted using VoxCeleb for pre-training and 4 genres from CN-Celeb for adaptation, demonstrate that the SE/BN adapter offers significant performance improvement over the baseline and competes with the vanilla fine-tuning approach by tuning just 1% of the parameters."
   ],
   "p1": 2145,
   "pn": 2149,
   "doi": "10.21437/Interspeech.2024-2476",
   "url": "interspeech_2024/wang24ma_interspeech.html"
  },
  "zhou24f_interspeech": {
   "authors": [
    [
     "Zhenyu",
     "Zhou"
    ],
    [
     "Shibiao",
     "Xu"
    ],
    [
     "Shi",
     "Yin"
    ],
    [
     "Lantian",
     "Li"
    ],
    [
     "Dong",
     "Wang"
    ]
   ],
   "title": "A Comprehensive Investigation on Speaker Augmentation for Speaker Recognition",
   "original": "2478",
   "order": 447,
   "page_count": 5,
   "abstract": [
    "Data augmentation (DA) has played a pivotal role in the success of deep speaker recognition. Current DA techniques primarily focus on speaker-preserving augmentation, which does not change the speaker trait of the speech and does not create new speakers. Recent research has shed light on the potential of speaker augmentation, which generates new speakers to enrich the training dataset. In this study, we delve into two speaker augmentation approaches: speed perturbation (SP) and vocal tract length perturbation (VTLP). Despite the empirical utilization of both methods, a comprehensive investigation into their efficacy is lacking. Our study, conducted using two public datasets, VoxCeleb and CN-Celeb, revealed that both SP and VTLP are proficient at generating new speakers, leading to significant performance improvements in speaker recognition. Furthermore, they exhibit distinct properties in sensitivity to perturbation factors and data complexity, hinting at the potential benefits of their fusion. Our research underscores the substantial potential of speaker augmentation, highlighting the importance of in-depth exploration and analysis."
   ],
   "p1": 2160,
   "pn": 2164,
   "doi": "10.21437/Interspeech.2024-2478",
   "url": "interspeech_2024/zhou24f_interspeech.html"
  },
  "sukhadia24_interspeech": {
   "authors": [
    [
     "Vrunda N.",
     "Sukhadia"
    ],
    [
     "Shammur Absar",
     "Chowdhury"
    ]
   ],
   "title": "Children’s Speech Recognition through Discrete Token Enhancement",
   "original": "2481",
   "order": 1053,
   "page_count": 5,
   "abstract": [
    "Children’s speech recognition is considered a low-resource task mainly due to the lack of publicly available data. There are several reasons for such data scarcity, including expensive data collection and annotation processes, and data privacy, among others. Transforming speech signals into discrete tokens that do not carry sensitive information but capture both linguistic and acoustic information could be a solution for privacy concerns. In this study, we investigate the integration of discrete speech tokens into children’s speech recognition systems as input without significantly degrading the ASR performance. Additionally, we explored single-view and multi-view strategies for creating these discrete labels. Furthermore, we tested the models for generalization capabilities with unseen domain and nativity dataset. Results reveal that the discrete token ASR for children achieves nearly equivalent performance with an approximate 83% reduction in parameters."
   ],
   "p1": 5143,
   "pn": 5147,
   "doi": "10.21437/Interspeech.2024-2481",
   "url": "interspeech_2024/sukhadia24_interspeech.html"
  },
  "tao24b_interspeech": {
   "authors": [
    [
     "Dehua",
     "Tao"
    ],
    [
     "Tan",
     "Lee"
    ],
    [
     "Harold",
     "Chui"
    ],
    [
     "Sarah",
     "Luk"
    ]
   ],
   "title": "Learning Representation of Therapist Empathy in Counseling Conversation Using Siamese Hierarchical Attention Network",
   "original": "2483",
   "order": 226,
   "page_count": 5,
   "abstract": [
    "Counseling is an activity of conversational speaking between a therapist and a client. Therapist empathy is an essential indicator of counseling quality and assessed subjectively by considering the entire conversation. This paper proposes to encode long counseling conversation using a hierarchical attention network. Conversations with extreme values of empathy rating are used to train a Siamese network based encoder with contrastive loss. Two-level attention mechanisms are applied to learn the importance weights of individual speaker turns and groups of turns in the conversation. Experimental results show that the use of contrastive loss is effective in encouraging the conversation encoder to learn discriminative embeddings that are related to therapist empathy. The distances between conversation embeddings positively correlate with the differences in the respective empathy scores. The learned conversation embeddings can be used to predict the subjective rating of therapist empathy."
   ],
   "p1": 1085,
   "pn": 1089,
   "doi": "10.21437/Interspeech.2024-2483",
   "url": "interspeech_2024/tao24b_interspeech.html"
  },
  "li24sa_interspeech": {
   "authors": [
    [
     "Shu",
     "Li"
    ],
    [
     "Peng",
     "Zhang"
    ],
    [
     "Ye",
     "Li"
    ]
   ],
   "title": "Robust Voice Activity Detection using Locality-Sensitive Hashing and Residual Frequency-Temporal Attention",
   "original": "2489",
   "order": 50,
   "page_count": 5,
   "abstract": [
    "For voice activity detection (VAD), recent works focus on learning the attention distribution on contextual information of speech to reduce the impact of irrelevant noise. However, contextual frames selected with specific steps may not be relevant, and these attention mechanisms can not fully discover the structure and characteristics of speech. In this paper, we explore a self-attention-inspired locality-sensitive hashing algorithm (SALSH) for dynamic and efficient contextual frame selection to enrich the frame-level features into a 2D partial spectrogram. Then, we propose a residual frequency-temporal attention model (FTAM) for VAD, consisting of an interval branch, an analogous hourglass structure with channel attention, and an attention learning mechanism for speech based on frequency-temporal attention. On the LibriSpeech and TIMIT datasets, the proposed method outperforms the others in terms of area under the curve (AUC), even under extremely low signal-to-noise ratio of -15dB."
   ],
   "p1": 242,
   "pn": 246,
   "doi": "10.21437/Interspeech.2024-2489",
   "url": "interspeech_2024/li24sa_interspeech.html"
  },
  "deheerkloots24_interspeech": {
   "authors": [
    [
     "Marianne",
     "de Heer Kloots"
    ],
    [
     "Willem",
     "Zuidema"
    ]
   ],
   "title": "Human-like Linguistic Biases in Neural Speech Models: Phonetic Categorization and Phonotactic Constraints in Wav2Vec2.0",
   "original": "2490",
   "order": 943,
   "page_count": 5,
   "abstract": [
    "What do deep neural speech models know about phonology? Existing work has examined the encoding of individual linguistic units such as phonemes in these models. Here we investigate interactions between units. Inspired by classic experiments on human speech perception, we study how Wav2Vec2 resolves phonotactic constraints. We synthesize sounds on an acoustic continuum between /l/ and /r/ and embed them in controlled contexts where only /l/, only /r/, or neither occur in English. Like humans, Wav2Vec2 models show a bias towards the phonotactically admissable category in processing such ambiguous sounds. Using simple measures to analyze model internals on the level of individual stimuli, we find that this bias emerges in early layers of the model's Transformer module. This effect is amplified by ASR finetuning but also present in fully self-supervised models. Our approach demonstrates how controlled stimulus designs can help localize specific linguistic knowledge in neural speech models."
   ],
   "p1": 4593,
   "pn": 4597,
   "doi": "10.21437/Interspeech.2024-2490",
   "url": "interspeech_2024/deheerkloots24_interspeech.html"
  },
  "kulkarni24_interspeech": {
   "authors": [
    [
     "Ajinkya",
     "Kulkarni"
    ],
    [
     "Atharva",
     "Kulkarni"
    ],
    [
     "Miguel",
     "Couceiro"
    ],
    [
     "Isabel",
     "Trancoso"
    ]
   ],
   "title": "Unveiling Biases while Embracing Sustainability: Assessing the Dual Challenges of Automatic Speech Recognition Systems",
   "original": "2494",
   "order": 950,
   "page_count": 5,
   "abstract": [
    "In this paper, we present a bias and sustainability focused investigation of Automatic Speech Recognition (ASR) systems, namely Whisper and Massively Multilingual Speech (MMS), which have achieved state-of-the-art (SOTA) performances. Despite their improved performance in controlled settings, there remains a critical gap in understanding their efficacy and equity in real-world scenarios. We analyze ASR biases w.r.t. gender, accent, and age group, as well as their effect on downstream tasks. In addition, we examine the environmental impact of ASR systems, scrutinizing the use of large acoustic models on carbon emission and energy consumption. We also provide insights into our empirical analyses, offering a valuable contribution to the claims surrounding bias and sustainability in ASR systems."
   ],
   "p1": 4628,
   "pn": 4632,
   "doi": "10.21437/Interspeech.2024-2494",
   "url": "interspeech_2024/kulkarni24_interspeech.html"
  },
  "braun24_interspeech": {
   "authors": [
    [
     "Franziska",
     "Braun"
    ],
    [
     "Sebastian P.",
     "Bayerl"
    ],
    [
     "Florian",
     "Hönig"
    ],
    [
     "Hartmut",
     "Lehfeld"
    ],
    [
     "Thomas",
     "Hillemacher"
    ],
    [
     "Tobias",
     "Bocklet"
    ],
    [
     "Korbinian",
     "Riedhammer"
    ]
   ],
   "title": "Infusing Acoustic Pause Context into Text-Based Dementia Assessment",
   "original": "2496",
   "order": 405,
   "page_count": 5,
   "abstract": [
    "Speech pauses, alongside content and structure, offer a valuable and non-invasive biomarker for detecting dementia. This work investigates the use of pause-enriched transcripts in transformer-based language models to differentiate the cognitive states of subjects with no cognitive impairment, mild cognitive impairment, and Alzheimer's dementia based on their speech from a clinical assessment. We address three binary classification tasks: Onset, monitoring, and dementia exclusion. The performance is evaluated through experiments on a German Verbal Fluency Test and a Picture Description Test, comparing the model's effectiveness across different speech production contexts. Starting from a textual baseline, we investigate the effect of incorporation of pause information and acoustic context. We show the test should be chosen depending on the task, and similarly, lexical pause information and acoustic cross-attention contribute differently."
   ],
   "p1": 1980,
   "pn": 1984,
   "doi": "10.21437/Interspeech.2024-2496",
   "url": "interspeech_2024/braun24_interspeech.html"
  },
  "li24ta_interspeech": {
   "authors": [
    [
     "Xiaolou",
     "Li"
    ],
    [
     "Zehua",
     "Liu"
    ],
    [
     "Chen",
     "Chen"
    ],
    [
     "Lantian",
     "Li"
    ],
    [
     "Li",
     "Guo"
    ],
    [
     "Dong",
     "Wang"
    ]
   ],
   "title": "Zero-Shot Fake Video Detection by Audio-Visual Consistency",
   "original": "2497",
   "order": 603,
   "page_count": 5,
   "abstract": [
    "Recent studies have advocated the detection of fake videos as a one-class detection task, predicated on the hypothesis that the consistency between audio and visual modalities of genuine data is more significant than that of fake data. This methodology, which solely relies on genuine audio-visual data while negating the need for forged counterparts, is thus delineated as a ‘zero-shot’ detection paradigm. This paper introduces a novel zero-shot detection approach anchored in content consistency across audio and video. By employing pre-trained ASR and VSR models, we recognize the audio and video content sequences, respectively. Then, the edit distance between the two sequences is computed to assess whether the claimed video is genuine. Experimental results indicate that, compared to two mainstream approaches based on semantic consistency and temporal consistency, our approach achieves superior generalizability across various deepfake techniques and demonstrates strong robustness against audio-visual perturbations. Finally, state-of the-art performance gains can be achieved by simply integratingthe decision scores of these three systems."
   ],
   "p1": 2935,
   "pn": 2939,
   "doi": "10.21437/Interspeech.2024-2497",
   "url": "interspeech_2024/li24ta_interspeech.html"
  },
  "do24_interspeech": {
   "authors": [
    [
     "Heejin",
     "Do"
    ],
    [
     "Wonjun",
     "Lee"
    ],
    [
     "Gary Geunbae",
     "Lee"
    ]
   ],
   "title": "Acoustic Feature Mixup for Balanced Multi-aspect Pronunciation Assessment",
   "original": "2498",
   "order": 64,
   "page_count": 5,
   "abstract": [
    "In automated pronunciation assessment, recent emphasis progressively lies on evaluating multiple aspects to provide enriched feedback. However, acquiring multi-aspect-score labeled data for non-native language learners' speech poses challenges; moreover, it often leads to score-imbalanced distributions. In this paper, we propose two Acoustic Feature Mixup strategies, linearly and non-linearly interpolating with the in-batch averaged feature, to address data scarcity and score-label imbalances. Primarily using goodness-of-pronunciation as an acoustic feature, we tailor mixup designs to suit pronunciation assessment. Further, we integrate fine-grained error-rate features by comparing speech recognition results with the original answer phonemes, giving direct hints for mispronunciation. Effective mixing of the acoustic features notably enhances overall scoring performances on the speechocean762 dataset, and detailed analysis highlights our potential to predict unseen distortions."
   ],
   "p1": 312,
   "pn": 316,
   "doi": "10.21437/Interspeech.2024-2498",
   "url": "interspeech_2024/do24_interspeech.html"
  },
  "mittal24_interspeech": {
   "authors": [
    [
     "Ashish",
     "Mittal"
    ],
    [
     "Darshan",
     "Prabhu"
    ],
    [
     "Sunita",
     "Sarawagi"
    ],
    [
     "Preethi",
     "Jyothi"
    ]
   ],
   "title": "SALSA: Speedy ASR-LLM Synchronous Aggregation",
   "original": "2499",
   "order": 713,
   "page_count": 5,
   "abstract": [
    "Harnessing pre-trained LLMs to improve ASR systems, particularly for low-resource languages, is now an emerging area of research. Existing methods range from using LLMs for ASR error correction to tightly coupled systems that replace the ASR decoder with the LLM. These approaches either increase decoding time or require expensive training of the cross-attention layers. We propose SALSA, which couples the decoder layers of the ASR to the LLM decoder, while synchronously advancing both decoders. Such coupling is performed with a simple projection of the last decoder state, and is thus significantly more training efficient than earlier approaches. A challenge of our proposed coupling is handling the mismatch between the tokenizers of the LLM and ASR systems. We handle this mismatch using cascading tokenization with respect to the LLM and ASR vocabularies. We evaluate SALSA on 8 low-resource languages in the FLEURS benchmark, yielding substantial WER reductions of up to 38%. "
   ],
   "p1": 3485,
   "pn": 3489,
   "doi": "10.21437/Interspeech.2024-2499",
   "url": "interspeech_2024/mittal24_interspeech.html"
  },
  "okamoto24_interspeech": {
   "authors": [
    [
     "Takuma",
     "Okamoto"
    ],
    [
     "Yamato",
     "Ohtani"
    ],
    [
     "Sota",
     "Shimizu"
    ],
    [
     "Tomoki",
     "Toda"
    ],
    [
     "Hisashi",
     "Kawai"
    ]
   ],
   "title": "Challenge of Singing Voice Synthesis Using Only Text-To-Speech Corpus With FIRNet Source-Filter Neural Vocoder",
   "original": "2504",
   "order": 383,
   "page_count": 5,
   "abstract": [
    "Singing voice synthesis (SVS) corpora are more costly to collect than TTS corpora. SVS using only a TTS corpus is challenging because the ranges of fundamental frequency (fo) and phoneme duration in SVS are wider than those in TTS. Although a melody-unsupervised method prototyped SVS using only a TTS corpus, some problems remain. To improve duration and fo controllability, this paper proposes a unified TTS and SVS framework. It is based on the FastSpeech-2-based duration-expansion-robust TTS acoustic model with phoneme embedding skip connection (PESC) and FIRNet source-filter neural vocoder with source-filter acoustic features. In the inference for SVS, the input text, fo, and phoneme duration are obtained from lyrics and notes in a musical score. Additionally, input fo shift is proposed. Experiments using the JSUT corpus confirm that the PESC-based acoustic model using input fo shift and FIRNet can improve the SVS quality compared with that using HiFi-GAN."
   ],
   "p1": 1870,
   "pn": 1874,
   "doi": "10.21437/Interspeech.2024-2504",
   "url": "interspeech_2024/okamoto24_interspeech.html"
  },
  "jiang24d_interspeech": {
   "authors": [
    [
     "Yuepeng",
     "Jiang"
    ],
    [
     "Tao",
     "Li"
    ],
    [
     "Fengyu",
     "Yang"
    ],
    [
     "Lei",
     "Xie"
    ],
    [
     "Meng",
     "Meng"
    ],
    [
     "Yujun",
     "Wang"
    ]
   ],
   "title": "Towards Expressive Zero-Shot Speech Synthesis with Hierarchical Prosody Modeling",
   "original": "2506",
   "order": 475,
   "page_count": 5,
   "abstract": [
    "Recent research in zero-shot speech synthesis has made significant progress in speaker similarity. However, current efforts focus on timbre generalization rather than prosody modeling, which results in limited naturalness and expressiveness. To address this, we introduce a novel speech synthesis model trained on large-scale datasets, including both timbre and hierarchical prosody modeling. As timbre is a global attribute closely linked to expressiveness, we adopt a global vector to model speaker timbre while guiding prosody modeling. Besides, given that prosody contains both global consistency and local variations, we introduce a diffusion model as the pitch predictor and employ a prosody adaptor to model prosody hierarchically, further enhancing the prosody quality of the synthesized speech. Experimental results show that our model not only maintains comparable timbre quality to the baseline but also exhibits better naturalness and expressiveness. The synthesized samples can be found at: https://rxy-j.github.io/HPMD-TTS/"
   ],
   "p1": 2300,
   "pn": 2304,
   "doi": "10.21437/Interspeech.2024-2506",
   "url": "interspeech_2024/jiang24d_interspeech.html"
  },
  "chen24y_interspeech": {
   "authors": [
    [
     "Chen",
     "Chen"
    ],
    [
     "Zehua",
     "Liu"
    ],
    [
     "Xiaolou",
     "Li"
    ],
    [
     "Lantian",
     "Li"
    ],
    [
     "Dong",
     "Wang"
    ]
   ],
   "title": "CNVSRC 2023: The First Chinese Continuous Visual Speech Recognition Challenge",
   "original": "2509",
   "order": 395,
   "page_count": 5,
   "abstract": [
    "The first Chinese Continuous Visual Speech Recognition Challenge aimed to probe the performance of Large Vocabulary Continuous Visual Speech Recognition (LVC-VSR) on two tasks: (1) Single-speaker VSR for a particular speaker and (2) Multi-speaker VSR for a set of registered speakers. The challenge yielded highly successful results, with the best submission significantly outperforming the baseline, particularly in the single-speaker task. This paper comprehensively reviews the challenge, encompassing the data profile, task specifications, and baseline system construction. It also summarises the representative techniques employed by the submitted systems, highlighting the most effective approaches. Additional information and resources about this challenge can be accessed through the official website at http://cnceleb.org/competition."
   ],
   "p1": 1930,
   "pn": 1934,
   "doi": "10.21437/Interspeech.2024-2509",
   "url": "interspeech_2024/chen24y_interspeech.html"
  },
  "chakraborty24_interspeech": {
   "authors": [
    [
     "Joyshree",
     "Chakraborty"
    ],
    [
     "Leena",
     "Dihingia"
    ],
    [
     "Priyankoo",
     "Sarmah"
    ],
    [
     "Rohit",
     "Sinha"
    ]
   ],
   "title": "On Comparing Time- and Frequency-Domain Rhythm Measures in Classifying Assamese Dialects",
   "original": "2513",
   "order": 427,
   "page_count": 5,
   "abstract": [
    "The rhythm measures are one of the primary means of differentiating languages and their dialects. They are broadly classified into two groups: time domain and frequency domain measures. The time-domain rhythm measures include temporal metrics like vocalic and non-vocalic durations and their derivatives, etc, and the frequency-domain rhythm measures comprise amplitude-modulated rhythm formant trajectory frequency and magnitude. To conduct this research, we focused on four Assamese regional varieties spoken in four districts of the Indian state of Assam. Data used in this study consists of read speech data of native Assamese speakers reading the Assamese translation of the \"North Wind and the Sun\" passage. The speech data was obtained from a total of 10 speakers for each of the four varieties. The average accuracy of dialect classification using quadratic discriminant analysis turns out to be 42% and 35%, respectively, for time- and frequency-domain rhythm measures."
   ],
   "p1": 2060,
   "pn": 2064,
   "doi": "10.21437/Interspeech.2024-2513",
   "url": "interspeech_2024/chakraborty24_interspeech.html"
  },
  "liu24r_interspeech": {
   "authors": [
    [
     "Rui",
     "Liu"
    ],
    [
     "Zening",
     "Ma"
    ]
   ],
   "title": "Emotion-Aware Speech Self-Supervised Representation Learning with Intensity Knowledge",
   "original": "2518",
   "order": 652,
   "page_count": 5,
   "abstract": [
    "Speech Self-Supervised Learning (SSL) has demonstrated considerable efficacy in various downstream tasks. Nevertheless, prevailing self-supervised models often overlook the incorporation of emotion-related prior information, thereby neglecting the potential enhancement of emotion task comprehension through emotion prior knowledge in speech.  In this paper, we propose an emotion-aware speech representation learning with intensity knowledge. Specifically, we extract frame-level emotion intensities using an established speech-emotion understanding model. Subsequently, we propose a novel emotional masking strategy (EMS) to incorporate emotion intensities into the masking process. We selected two representative models based on Transformer and CNN, namely MockingJay and Non-autoregressive Predictive Coding (NPC), and conducted experiments on IEMOCAP dataset. Experiments have demonstrated that the representations derived from our proposed method outperform the original model in SER task. We release the source code at https://github.com/AI-S2-Lab/EMS."
   ],
   "p1": 3180,
   "pn": 3184,
   "doi": "10.21437/Interspeech.2024-2518",
   "url": "interspeech_2024/liu24r_interspeech.html"
  },
  "lodagala24_interspeech": {
   "authors": [
    [
     "Vasista Sai",
     "Lodagala"
    ],
    [
     "Abhishek",
     "Biswas"
    ],
    [
     "Shoutrik",
     "Das"
    ],
    [
     "Jordan",
     "F"
    ],
    [
     "S",
     "Umesh"
    ]
   ],
   "title": "All Ears: Building Self-Supervised Learning based ASR models for Indian Languages at scale",
   "original": "2520",
   "order": 812,
   "page_count": 5,
   "abstract": [
    "The abundance of unlabeled speech and its ease of collection calls for the development of self-supervised learning (SSL) based speech foundation models, which have been effective across several downstream speech tasks. As a part of this work, we curate 29.5K hours of raw speech data across 24 Indian languages and multiple domains, to pre-train SSL models over 5 different architectures. We then fine-tune these models for the downstream Automatic Speech Recognition (ASR) task on 13 Indian languages and evaluate them over diverse benchmarks. In addition we measure the efficacy of these models by evaluating them over the SUPERB benchmark. Our work signifies the need for careful choice of the SSL objectives while emphasizing the benefits of multilingual pretraining. Our pre-trained models out-perform baseline models such as MMS-300M and IndicWav2Vec by 17.3% and 36.0% relative WER improvements respectively, on Indian language ASR."
   ],
   "p1": 3944,
   "pn": 3948,
   "doi": "10.21437/Interspeech.2024-2520",
   "url": "interspeech_2024/lodagala24_interspeech.html"
  },
  "park24b_interspeech": {
   "authors": [
    [
     "Yo-Han",
     "Park"
    ],
    [
     "Wencke",
     "Liermann"
    ],
    [
     "Yong-Seok",
     "Choi"
    ],
    [
     "Seung Hi",
     "Kim"
    ],
    [
     "Jeong-Uk",
     "Bang"
    ],
    [
     "Seung",
     "Yun"
    ],
    [
     "Kong Joo",
     "Lee"
    ]
   ],
   "title": "Backchannel prediction, based on who, when and what",
   "original": "2523",
   "order": 730,
   "page_count": 5,
   "abstract": [
    "Backchannels are fundamental elements within conversations that serve as essential tools for effective communication and interpersonal dynamics. A typical backchannel prediction model primarily utilizes audio signal and text information. But backchanneling can exhibit different patterns depending on who I am, who I talk to, when I talk to them, and what I talk about. Therefore, we propose to employ three related pieces of information to enhance the quality of backchannel prediction models: speaker & listener characteristics, conversation progress, and topic. In our experiments with Korean counseling data, incorporating the suggested information into the model resulted in a performance improvement of 4.1% compared to the baseline model, increasing the F1 score from 50.1% to 54.2% ."
   ],
   "p1": 3570,
   "pn": 3574,
   "doi": "10.21437/Interspeech.2024-2523",
   "url": "interspeech_2024/park24b_interspeech.html"
  },
  "sun24e_interspeech": {
   "authors": [
    [
     "Haoqin",
     "Sun"
    ],
    [
     "Shiwan",
     "Zhao"
    ],
    [
     "Xiangyu",
     "Kong"
    ],
    [
     "Xuechen",
     "Wang"
    ],
    [
     "Hui",
     "Wang"
    ],
    [
     "Jiaming",
     "Zhou"
    ],
    [
     "Yong",
     "Qin"
    ]
   ],
   "title": "Iterative Prototype Refinement for Ambiguous Speech Emotion Recognition",
   "original": "2525",
   "order": 656,
   "page_count": 5,
   "abstract": [
    "Recognizing emotions from speech is a daunting task due to the subtlety and ambiguity of expressions. Traditional speech emotion recognition (SER) systems, which typically rely on a singular, precise emotion label, struggle with this complexity. Therefore, modeling the inherent ambiguity of emotions is an urgent problem. In this paper, we propose an iterative prototype refinement framework (IPR) for ambiguous SER. IPR comprises two interlinked components: contrastive learning and class prototypes. The former provides an efficient way to obtain high-quality representations of ambiguous samples. The latter are dynamically updated based on ambiguous labels—the similarity of the ambiguous data to all prototypes. These refined embeddings yield precise pseudo labels, thus reinforcing representation quality. Experimental evaluations conducted on the IEMOCAP dataset validate the superior performance of IPR over state-of-the-art methods, thus proving the effectiveness of our proposed method."
   ],
   "p1": 3200,
   "pn": 3204,
   "doi": "10.21437/Interspeech.2024-2525",
   "url": "interspeech_2024/sun24e_interspeech.html"
  },
  "ta24_interspeech": {
   "authors": [
    [
     "Bao Thang",
     "Ta"
    ],
    [
     "Minh Tu",
     "Le"
    ],
    [
     "Van Hai",
     "Do"
    ],
    [
     "Huynh Thi",
     "Thanh Binh"
    ]
   ],
   "title": "Enhancing No-Reference Speech Quality Assessment with Pairwise, Triplet Ranking Losses, and ASR Pretraining",
   "original": "2527",
   "order": 556,
   "page_count": 5,
   "abstract": [
    "Speech Quality Assessment (SQA) without reference signals has garnered attention due to its wide applications. Current SQA methods often rely on the Mean Square Error (MSE) loss to approximate human subjective ratings. However, MSE treats all deviations from the ground truth symmetrically, ignoring their direction and relative quality distinctions among speech samples. Therefore, predictions learned through MSE have limited correlations. This paper introduces a novel approach that leverages the relative quality distinctions among speech samples. By enforcing relative ranking using Pairwise and Triplet Ranking Losses, our method encourages the SQA model to learn not only the absolute quality of individual speech samples but also their quality in comparison to others, addressing the limitations of MSE-based approaches. Additionally, we suggest pretraining the SQA encoder with an ASR task to enhance generalization. Experiments on NISQA test sets confirm our approach's effectiveness."
   ],
   "p1": 2700,
   "pn": 2704,
   "doi": "10.21437/Interspeech.2024-2527",
   "url": "interspeech_2024/ta24_interspeech.html"
  },
  "neelabh24_interspeech": {
   "authors": [
    [
     "Kumar",
     "Neelabh"
    ],
    [
     "Vishnu",
     "Sreekumar"
    ]
   ],
   "title": "From Sound to Meaning in the Auditory Cortex: A Neuronal Representation and Classification Analysis",
   "original": "2531",
   "order": 307,
   "page_count": 5,
   "abstract": [
    "The neural mechanisms underlying the comprehension of meaningful sounds are yet to be fully understood. While previous work has shown that the auditory cortex can classify auditory stimuli into distinct semantic categories, the specific functions of the primary (A1) and the secondary auditory cortex (A2) are not well understood. We analyzed the neural responses of songbirds as they listened to their entire vocal repertoire. We first show that the distances between semantic categories in acoustic and neural representations are correlated in both A1 and A2. We then show that while both A1 and A2 are equally informative of the acoustic category of the vocalizations, A2 is significantly more informative of their semantic category. Additionally, we show that the semantic categories are more separated in A2. These findings suggest that as the incoming signal moves downstream within the auditory cortex, its acoustic information is preserved, while its semantic information is enhanced."
   ],
   "p1": 1490,
   "pn": 1494,
   "doi": "10.21437/Interspeech.2024-2531",
   "url": "interspeech_2024/neelabh24_interspeech.html"
  },
  "hartanto24_interspeech": {
   "authors": [
    [
     "Roland",
     "Hartanto"
    ],
    [
     "Sakriani",
     "Sakti"
    ],
    [
     "Koichi",
     "Shinoda"
    ]
   ],
   "title": "MSDET: Multitask Speaker Separation and Direction-of-Arrival Estimation Training",
   "original": "2537",
   "order": 449,
   "page_count": 5,
   "abstract": [
    "The information on the spatial location of speakers can be effectively used for multi-channel speaker separation. For example, Location-Based Training (LBT) uses the order of azimuth angles and distances of speakers to solve the permutation ambiguity problem. This location information can be used to improve the separation performance further. This paper proposes a multitask learning approach, Multitask Speaker Separation and Direction-of-Arrival Estimation Training (MSDET), jointly optimizing speaker separation and Direction-of-Arrival (DoA) estimation. In our evaluation using SMS-WSJ dataset, it outperforms LBT by 0.13 points in SI-SDR and 0.35 points in ESTOI."
   ],
   "p1": 2170,
   "pn": 2174,
   "doi": "10.21437/Interspeech.2024-2537",
   "url": "interspeech_2024/hartanto24_interspeech.html"
  },
  "ta24b_interspeech": {
   "authors": [
    [
     "Bao Thang",
     "Ta"
    ],
    [
     "Van Hai",
     "Do"
    ],
    [
     "Huynh Thi",
     "Thanh Binh"
    ]
   ],
   "title": "Enhancing Non-Matching Reference Speech Quality Assessment through Dynamic Weight Adaptation",
   "original": "2538",
   "order": 795,
   "page_count": 5,
   "abstract": [
    "Non-Matching Reference (NMR) is a promising approach in Speech Quality Assessment (SQA), enabling the use of references without the need for exact pristine versions of audio signals. However, NMR-SQA often relies on manually fixed weights for its multitask learning components. This approach requires significant expert knowledge and also rigidly assigns the role of each task in supporting training models. Certain tasks may be more beneficial and should be given greater attention at specific stages of the training process. Fixed weights do not accommodate such variations. To address this limitation, we propose an adaptation in NMR-SQA, utilizing a novel probability distribution and success history memory, which allows weights to change dynamically, vary, and provide multiple points to roll back during the training process. Experiments on NISQA test sets demonstrate the efficacy of our approach compared to other advanced methods."
   ],
   "p1": 3859,
   "pn": 3863,
   "doi": "10.21437/Interspeech.2024-2538",
   "url": "interspeech_2024/ta24b_interspeech.html"
  },
  "min24_interspeech": {
   "authors": [
    [
     "Anna",
     "Min"
    ],
    [
     "Chenxu",
     "Hu"
    ],
    [
     "Yi",
     "Ren"
    ],
    [
     "Hang",
     "Zhao"
    ]
   ],
   "title": "A Unit-based System and Dataset for Expressive Direct Speech-to-Speech Translation",
   "original": "2548",
   "order": 78,
   "page_count": 5,
   "abstract": [
    "Current research in speech-to-speech translation (S2ST) primarily concentrates on translation accuracy and speech naturalness, often overlooking key elements like paralinguistic information, which is essential for conveying emotions and attitudes in communication. To address this, our research introduces a novel, carefully curated multilingual dataset from various movie audio tracks. Each dataset pair is precisely matched for paralinguistic features and duration. We enhance this by integrating multiple prosody transfer techniques, aiming for translations that are accurate, natural-sounding, and rich in paralinguistic details. Our experimental results confirm that our model retains more paralinguistic information from the source speech while maintaining high standards of translation accuracy and naturalness."
   ],
   "p1": 382,
   "pn": 386,
   "doi": "10.21437/Interspeech.2024-2548",
   "url": "interspeech_2024/min24_interspeech.html"
  },
  "jia24_interspeech": {
   "authors": [
    [
     "Bonian",
     "Jia"
    ],
    [
     "Huiyao",
     "Chen"
    ],
    [
     "Yueheng",
     "Sun"
    ],
    [
     "Meishan",
     "Zhang"
    ],
    [
     "Min",
     "Zhang"
    ]
   ],
   "title": "LLM-Driven Multimodal Opinion Expression Identification",
   "original": "2550",
   "order": 602,
   "page_count": 5,
   "abstract": [
    "Opinion Expression Identification (OEI) is essential in NLP for applications ranging from voice assistants to depression diagnosis. This study extends OEI to encompass multimodal inputs, underlining the significance of auditory cues in delivering emotional subtleties beyond the capabilities of text. We introduce a novel multimodal OEI (MOEI) task, integrating text and speech to mirror real-world scenarios. Utilizing CMU MOSEI and IEMOCAP datasets, we construct the CI-MOEI dataset. Additionally, Text-to-Speech (TTS) technology is applied to the MPQA dataset to obtain the CIM-OEI dataset. We design a template for the OEI task to take full advantage of the generative power of large language models (LLMs). Advancing further, we propose an LLM-driven method STOEI, which combines speech and text modal to identify opinion expressions. Our experiments demonstrate that MOEI significantly improves the performance while our method outperforms existing methods by 9.20% and obtains SOTA results."
   ],
   "p1": 2930,
   "pn": 2934,
   "doi": "10.21437/Interspeech.2024-2550",
   "url": "interspeech_2024/jia24_interspeech.html"
  },
  "arai24_interspeech": {
   "authors": [
    [
     "Takayuki",
     "Arai"
    ],
    [
     "Ryohei",
     "Suzuki"
    ],
    [
     "Chandler",
     "Earp"
    ],
    [
     "Shinya",
     "Tsuji"
    ],
    [
     "Keiko",
     "Ochi"
    ]
   ],
   "title": "Production of phrases by mechanical models of the human vocal tract",
   "original": "5001",
   "order": 199,
   "page_count": 2,
   "abstract": [
    "Three types of mechanical models of the human vocal tract were used to produce utterances in English and Japanese. The first model, VTM-S24-1, is a type of sliding three-tube model in which the inner column only moves inside the outer straight pipe back and forth. In spite of this simple structure, we were able to produce “How are you?” with this model and a single actuator. The second model, VTM-UT-D11, is based on the model that Umeda and Teranishi made. Eleven actuators control the positions of ten plastic bars in the main vocal tract and the velopharyngeal opening to adjust the vocal-tract configuration dynamically. Not only the English phrase “How are you?” but also Japanese phrase for “Good morning” were successfully produced. Finally, the third model, VTM-UT-D6, was newly designed. In this model, there are only six movable blocks inserted in the main vocal tract and no side branch is attached. With this model, a linear cam mechanism successfully\nproduced five vowels continuously. In addition, a rotating cam mechanism achieved “How are you?” with high intelligibility."
   ],
   "p1": 987,
   "pn": 988
  },
  "gourav24_interspeech": {
   "authors": [
    [
     "Vishal",
     "Gourav"
    ],
    [
     "Ankit",
     "Tyagi"
    ],
    [
     "Phanindra",
     "Mankale"
    ]
   ],
   "title": "Faster Vocoder: a multi threading approach to achieve low latency during TTS Inference",
   "original": "5002",
   "order": 200,
   "page_count": 2,
   "abstract": [
    "The first question by any customer who wants to buy or rent any service/product is always, ”How fast is it?”. So how fast is our Text-To-Speech(TTS) we get asked, how fast can it turn my text into speech, what is the Customer Perceived Latency (CPL) i.e. total time taken from the time a customer submits text to the time they get the audio file for that text? Like any other service, TTS also comprises of a number of moving parts contributing to CPL, for example, SSML parser, Text Normaliser, G2P, Models (Acoustic Model and Vocoder) and post processing. Each layer contributes in some way to the overall processing time. The goal is to reduce this processing time without any deterioration in the audio quality."
   ],
   "p1": 989,
   "pn": 990
  },
  "mohan24_interspeech": {
   "authors": [
    [
     "Aanchan",
     "Mohan"
    ],
    [
     "Monideep",
     "Chakraborti"
    ],
    [
     "Katelyn",
     "Eng"
    ],
    [
     "Nailia",
     "Kushaeva"
    ],
    [
     "Mirjana",
     "Prpa"
    ],
    [
     "Jordan",
     "Lewis"
    ],
    [
     "Tianyi",
     "Zhang"
    ],
    [
     "Vince",
     "Geisler"
    ],
    [
     "Carol",
     "Geisler"
    ]
   ],
   "title": "A powerful and modern AAC composition tool for impaired speakers",
   "original": "5005",
   "order": 201,
   "page_count": 2,
   "abstract": [
    "Augmentative and alternative communication (AAC) software assists impaired speakers to communicate. Enabling context awareness, authenticity and ease of use goes a long way in empowering communication. Our work presents an easy to use AAC tool that allows for message composition with text or emoji input by typing as well as speaking with contextually relevant word-level suggestions. Any transcription errors are corrected, and contextually relevant phrases with the appropriately chosen emotional tone are suggested by large-language models. The user is then able to use modern text-to-speech synthesis to be able to synthesize the composed message in the user’s own voice. Our system additionally maintains features from AAC software such as word-cards, pre-composed and frequently used messages."
   ],
   "p1": 991,
   "pn": 992
  },
  "mika24_interspeech": {
   "authors": [
    [
     "Grzegorz P.",
     "Mika"
    ],
    [
     "Konrad",
     "Zieli´nski"
    ],
    [
     "Paweł",
     "Cyrta"
    ],
    [
     "Marek",
     "Grzelec"
    ]
   ],
   "title": "VoxFlow AI: wearable voice converter for atypical speech",
   "original": "5009",
   "order": 202,
   "page_count": 2,
   "abstract": [
    "The central challenges of the embedded voice conversion system lie in the transformation of atypical speech into one resembling the original voice without changing the linguistic content. While various atypical speech input techniques have been developed to aid voice inter-\nactions, they are typically challenging to use daily, and the research on their usability could be more robust. In this Show&Tell paper, we\npresent VoxFlow AI, a wearable loudspeaker integrated with a voice converter designed to improve real-life interactions. During the interactive demonstration, we will show how this device changes interactions between people with oncological and neurological speech disorders, who are the direct beneficiaries of the solution."
   ],
   "p1": 993,
   "pn": 994
  },
  "akarsh24_interspeech": {
   "authors": [
    [
     "Sai",
     "Akarsh"
    ],
    [
     "Vamshiraghusimha",
     "Narasinga"
    ],
    [
     "Anil Kumar",
     "Vuppala"
    ]
   ],
   "title": "Stress transfer in speech-to-speech machine translation",
   "original": "5010",
   "order": 203,
   "page_count": 2,
   "abstract": [
    "India’s education sector faces a significant challenge due to its linguistic diversity, hindering inclusivity. The dominance of English on the internet underscores the critical need for translating educational content into Indian languages to enhance accessibility. Although Speech-to-Speech Machine Translation (SSMT) technologies exist, their deficiency in reproducing intonation results in monotonous translations, diminishing audience engagement and interest in the content. To address this issue, this paper demonstrates an SSMT application with a Text-to-Speech (TTS) architecture capable of incorporating stress into synthesized speech to give a more engaging experience. The SSMT pipeline also has components like a stress classifier that captures the stress in the source speech and allows it to be utilized during speech generation. The application takes in a speech file and gives a translated speech file with stress transferred from the source."
   ],
   "p1": 995,
   "pn": 996
  },
  "okamoto24b_interspeech": {
   "authors": [
    [
     "Takuma",
     "Okamoto"
    ],
    [
     "Yamato",
     "Ohtani"
    ],
    [
     "Hisashi",
     "Kawai"
    ]
   ],
   "title": "Mobile PresenTra: NICT fast neural text-to-speech system on smartphones with incremental inference of MS-FC-HiFi-GAN for law-latency synthesis",
   "original": "5016",
   "order": 204,
   "page_count": 2,
   "abstract": [
    "For achieving fast and high-fidelity neural text-to-speech on edge smartphone devices without network connection, we NICT prototyped Mobile PresenTra by introducing non-autoregressive acoustic model with Transformer encoder and ConvNeXt decoder, and MS-FC-HiFi-GAN neural vocoder.\nAdditionally, the incremental inference is applied only to neural vocoder for low-latency synthesis without performance degradation. Compared with a previous NICT system with Transformer encoder, Transforme decoder and MS-HiFi-GAN neural vocoder, the proposed Mobile PresenTra can realize high-fidelity and fast synthesis on a middle-range smartphone with a real-time factor of about 0.3 for batch inference, and a latency of less than 0.5 s for incremental inference. In the Show & Tell, attendees can freely experience the demonstration of Mobile PresenTra systems implemented on actual smartphones for English, Japanese and Chinese with arbitrary text input."
   ],
   "p1": 997,
   "pn": 998
  },
  "peirolilja24_interspeech": {
   "authors": [
    [
     "Alex",
     "Peiró-Lilja"
    ],
    [
     "José",
     "Giraldo"
    ],
    [
     "Martí",
     "Llopart-Font"
    ],
    [
     "Carme",
     "Armentano-Oller"
    ],
    [
     "Baybars",
     "Külebi"
    ],
    [
     "Mireia",
     "Farrús"
    ]
   ],
   "title": "Multi-speaker and multi-dialectal Catalan TTS models for video gaming",
   "original": "5020",
   "order": 205,
   "page_count": 2,
   "abstract": [
    "Recently, we explored and trained different state-of-the-art text-to-speech (TTS) architectures for Catalan. We used existing datasets but also produced a new Catalan multi-accent dataset to train these architectures. The objective of our work is to improve the quality of current TTS systems in Catalan and export the resulting models for potential interactive applications and video games. For this reason, our set of multi-speaker and multi-accent Catalan TTS models are presented within a demo made in Unity. The users are able to interact with game characters which are attached to our Catalan TTS. While generated Catalan speech replies are reproduced, execution time, real-time\nfactor and transcription are shown on screen. Exported weights, such as data and demo source code, are released to the public."
   ],
   "p1": 999,
   "pn": 1000
  },
  "francis24_interspeech": {
   "authors": [
    [
     "Juliana",
     "Francis"
    ],
    [
     "Éva",
     "Székely"
    ],
    [
     "Joakim",
     "Gustafson"
    ]
   ],
   "title": "ConnecTone: a modular AAC system prototype with contextual generative text prediction and style-adaptive conversational TTS",
   "original": "5023",
   "order": 206,
   "page_count": 2,
   "abstract": [
    "Recent developments in generative language modeling and conversational Text-to-Speech present transformative potential for enhancing Augmentative and Alternative Communication (AAC) devices. Practical application of these technologies requires extensive research and testing. To address this, we introduce ConnecTone, a modular platform designed for rapid integration and testing of language generation and speech technology. ConnecTone implements context-sensitive generative text prediction, using conversational context from Automatic\nSpeech Recognition inputs. The system incorporates a neural TTS that supports interpolation between reading and spontaneous conversational styles, along with adjustable prosodic features. These speech characteristics are predicted using Large Language Models, but can be adjusted by users for individual needs. We anticipate ConnecTone will enable us to rapidly evaluate and implement innovations, thereby contributing to faster benefit delivery to AAC users."
   ],
   "p1": 1001,
   "pn": 1002
  },
  "rohmatillah24_interspeech": {
   "authors": [
    [
     "Mahdin",
     "Rohmatillah"
    ],
    [
     "Bryan Gautama",
     "Ngo"
    ],
    [
     "Willianto",
     "Sulaiman"
    ],
    [
     "Po-Chuan",
     "Chen"
    ],
    [
     "Jen-Tzung",
     "Chien"
    ]
   ],
   "title": "Reliable dialogue system for facilitating student-counselor communication",
   "original": "5024",
   "order": 207,
   "page_count": 2,
   "abstract": [
    "The mental well-being of university students is a critical concern, mainly due to the high student-to-counselor ratio in a university. This often results in lengthy waiting periods for students to receive in-person consultations with counselors, and suboptimal treatment due to counselors’ unmanageable workloads. This paper proposes the advancement of a mental health dialogue system named NYCUKA to support both students and counselors. NYCUKA provides immediate aid by acting as an active listener to encourage students to share more about themselves during the waiting period. NYCUKA focuses primarily on the traditional Chinese language. All components are thoughtfully developed to ensure reliability aspect leading to safe communication. Conversation histories are stored in the database and analyzed using an advanced continual reasoning prompt for large language models. Conversation histories and analyses are displayed on the counselor dashboard to monitor\nstudents’ mental health effectively."
   ],
   "p1": 1003,
   "pn": 1004
  },
  "chaudhuri24_interspeech": {
   "authors": [
    [
     "Yashwardhan",
     "Chaudhuri"
    ],
    [
     "Paridhi",
     "Mundra"
    ],
    [
     "Arnesh",
     "Batra"
    ],
    [
     "Orchid Chetia",
     "Phukan"
    ],
    [
     "Arun Balaji",
     "Buduru"
    ]
   ],
   "title": "ASGIR: audio spectrogram transformer guided classification and information retrieval for birds",
   "original": "5017",
   "order": 745,
   "page_count": 2,
   "abstract": [
    "Recognition and interpretation of bird vocalizations are pivotal in ornithological research and ecological conservation efforts due to their significance in understanding avian behaviour, performing habitat assessment and judging ecological health. This paper presents an audio spectrogram-guided classification framework called ASGIR for improved bird sound recognition and information retrieval. Our work is accompanied by a simple-to-use, two-step information retrieval system that uses geographical location and bird sounds to localize and retrieve relevant bird information by scraping Wikipedia page information of recognized birds. ASGIR offers a substantial performance on a random subset of 51 classes of Xeno-Canto dataset Bird sounds from European countries with a median of 100% performance on F1, Precision and Sensitivity metrics. Our code is available as follows: https://github.com/MainSample1234/AS-GIR"
   ],
   "p1": 3636,
   "pn": 3637
  },
  "koshal24_interspeech": {
   "authors": [
    [
     "Devyani",
     "Koshal"
    ],
    [
     "Orchid Chetia",
     "Phukan"
    ],
    [
     "Sarthak",
     "Jain"
    ],
    [
     "Arun Balaji",
     "Buduru"
    ],
    [
     "Rajesh",
     "Sharma"
    ]
   ],
   "title": "PERSONA: an application for emotion recognition, gender recognition and age estimation",
   "original": "5018",
   "order": 746,
   "page_count": 2,
   "abstract": [
    "Emotion Recognition (ER), Gender Recognition (GR), and Age Estimation (AE) constitute paralinguistic tasks that rely not on the spoken content but primarily on speech characteristics such as pitch and tone. While previous research has made significant strides in developing models for each task individually, there has been comparatively less emphasis on concurrently learning these tasks, despite their inherent interconnectedness. As such in this demonstration, we present PERSONA, an application for predicting ER, GR, and AE with a single model in the backend. One notable point is we show that representations from speaker recognition pre-trained model (PTM) is better suited for such a multi-task learning format than the state-of-the-art (SOTA) self-supervised (SSL) PTM by carrying out a comparative study. Our methodology obviates the need for deploying separate models for each task and can potentially conserve resources and time during the training and deployment phases."
   ],
   "p1": 3638,
   "pn": 3639
  },
  "v24_interspeech": {
   "authors": [
    [
     "Kesavaraj",
     "V"
    ],
    [
     "Charan",
     "Devarkonda"
    ],
    [
     "Vamshiraghusimha",
     "Narasinga"
    ],
    [
     "Anil Kumar",
     "Vuppala"
    ]
   ],
   "title": "Custom wake word detection",
   "original": "5003",
   "order": 415,
   "page_count": 2,
   "abstract": [
    "In personalizing interactions with smart devices, identifying keywords in an open-vocabulary context is crucial. Previous methods for open-vocabulary keyword spotting relied on a shared embedding space created by audio and text encoders. However, they suffered from heterogeneous modality representations, causing audio-text mismatch. To tackle this issue, our proposed framework utilizes knowledge from a pre-trained text-to-speech (TTS) system. This knowledge transfer incorporates awareness of audio projections into text representations derived from the text encoder. Consequently, this approach aids in preventing false triggers in scenarios such as closely related pronunciations of audio-text pairs. Additionally, our proposed approach benefits from the keyword embedding calculation only during keyword enrollment phase. The proposed system gives consistent performance across all the word lengths."
   ],
   "p1": 2030,
   "pn": 2031
  },
  "chen24z_interspeech": {
   "authors": [
    [
     "Song",
     "Chen"
    ],
    [
     "Mandar",
     "Gogate"
    ],
    [
     "Kia",
     "Dashtipour"
    ],
    [
     "Jasper",
     "Kirton-Wingate"
    ],
    [
     "Adeel",
     "Hussain"
    ],
    [
     "Faiyaz",
     "Doctor"
    ],
    [
     "Tughrul",
     "Arslan"
    ],
    [
     "Amir",
     "Hussain"
    ]
   ],
   "title": "Edged based audio-visual speech enhancement demonstrator",
   "original": "5007",
   "order": 416,
   "page_count": 2,
   "abstract": [
    "Difficulty understanding speech in noisy environments presents a significant challenge for individuals with hearing loss and is a primary factor contributing to non-adherence to hearing aid use. Recent technological advancements integrating artificial intelligence, machine learning, and smartphone technology hold promise in advancing and customizing hearing healthcare. A proposed solution is a portable hearing assistive system designed for speech enhancement in noisy settings. We anticipate that this system will enhance the auditory experience of hearing aid users. The system leverages a mobile phone’s camera, microphone, and speaker, ensuring ease of portability. Raw video and audio data are stored locally on the phone and processed by the device’s processor alongside an audio-visual speech enhancement algorithm. This algorithm is capable of identifying voice signals and lip movements using a lightweight deep neural network model, thereby optimizing memory efficiency required for real-time processing."
   ],
   "p1": 2032,
   "pn": 2033
  },
  "anway24_interspeech": {
   "authors": [
    [
     "Arif Reza",
     "Anway"
    ],
    [
     "Bryony",
     "Buck"
    ],
    [
     "Mandar",
     "Gogate"
    ],
    [
     "Kia",
     "Dashtipour"
    ],
    [
     "Michael",
     "Akeroyd"
    ],
    [
     "Amir",
     "Hussain"
    ]
   ],
   "title": "Real-Time Gaze-directed speech enhancement for audio-visual hearing-aids",
   "original": "5008",
   "order": 417,
   "page_count": 2,
   "abstract": [
    "This study introduces a novel real-time, gaze-directed audio-visual speech enhancement (AVSE) framework for hearing aids designed to improve speech intelligibility for individuals with hearing loss (pHL) in noisy environments. Existing gaze estimation methods often rely solely on eye angle, leading to reduced accuracy. Our approach addresses this limitation by combining eye angle and nose position with head pose estimation to enhance target speaker identification and facilitate noise reduction within the AVSE framework. We utilize a novel eye gaze estimation algorithm that leverages the listener’s nose position for improved accuracy. Head pose estimation is also used to capture the overall direction of attention. This combined information is utilized in real-time to steer a beamformer towards the target speaker, effectively enhancing their voice and suppressing background noise. Pilot trials with pHL users demonstrated high accuracy (99.55% - 99.88%) in estimating target speaker direction using the proposed algorithm. This research presents a promising approach for improving communication\naccessibility and social interaction for pIH users by potentially enhancing speech recognition in challenging listening situations. Future studies will quantify the improvement in speech intelligibility achieved by the gaze directed AVSE framework."
   ],
   "p1": 2034,
   "pn": 2035
  },
  "kumar24c_interspeech": {
   "authors": [
    [
     "Abhishek",
     "Kumar"
    ],
    [
     "Srikanth",
     "Konjeti"
    ],
    [
     "Jithendra",
     "Vepa"
    ]
   ],
   "title": "Detection of background agents speech in contact centers",
   "original": "5013",
   "order": 418,
   "page_count": 2,
   "abstract": [
    "In a typical contact center environment, multiple agents often handle calls simultaneously and they are frequently in close proximity to one another. Consequently, there is a possibility that conversations of nearby agents may inadvertently be recorded during calls. This represents instances of background agents speech being captured during agent-customer interactions. Such unintended background speech may not only impact the quality of conversation but may also contain some sensitive information which may pose security concerns in contact centers. Therefore, contact centers are interested in identifying such scenarios. This knowledge can assist them to implement appropriate mitigating strategies and enhance the quality of audio conversations, thereby improving the overall customer experience. In this work, we utilise the pauses and gaps in the agent speech to clearly identify the background speech. Our approach that is based on speech features is simple, tuneable, computationally efficient and cost effective."
   ],
   "p1": 2036,
   "pn": 2037
  },
  "jain24b_interspeech": {
   "authors": [
    [
     "Sarthak",
     "Jain"
    ],
    [
     "Orchid Chetia",
     "Phukan"
    ],
    [
     "Arun Balaji",
     "Buduru"
    ],
    [
     "Rajesh",
     "Sharma"
    ]
   ],
   "title": "The reasonable effectiveness of speaker embeddings for violence detection",
   "original": "5022",
   "order": 749,
   "page_count": 2,
   "abstract": [
    "In this paper, we focus on audio violence detection (AVD). AVD is necessary for several reasons, especially in the context of maintaining safety, preventing harm, and ensuring security in various environments. This calls for accurate AVD systems. Like many related applications in audio processing, the most common approach for improving the performance, would be by leveraging self-supervised (SSL) pre-trained models (PTMs). However, as these SSL models are very large models with million of parameters and this can hinder real-world deployment especially in compute-constraint environment. To resolve this, we propose the usage of speaker recognition models which are much smaller compared to the SSL models. Experimentation with speaker recognition model embeddings with SVM & Random Forest as classifiers, we show that speaker recognition model embeddings perform the best in comparison to state-of-the-art (SOTA) SSL models and achieve SOTA results."
   ],
   "p1": 3644,
   "pn": 3645
  },
  "morrone24_interspeech": {
   "authors": [
    [
     "Giovanni",
     "Morrone"
    ],
    [
     "Enrico",
     "Zovato"
    ],
    [
     "Fabio",
     "Brugnara"
    ],
    [
     "Enrico",
     "Sartori"
    ],
    [
     "Leonardo",
     "Badino"
    ]
   ],
   "title": "A toolkit for joint speaker diarization and identification with application to speaker-attributed ASR",
   "original": "5030",
   "order": 753,
   "page_count": 2,
   "abstract": [
    "We present a modular toolkit to perform joint speaker diarization and speaker identification. The toolkit can leverage on multiple models and algorithms which are defined in a configuration file. Such flexibility allows our system to work properly in various conditions (e.g., multiple registered speakers’ sets, acoustic conditions and languages) and across application domains (e.g. media monitoring, institutional, speech analytics). In this demonstration we show a practical use-case in which speaker-related information is used jointly with automatic speech recognition engines to generate speaker-attributed transcriptions. To achieve that, we employ a user-friendly web-based interface to process audio and video inputs with the chosen configuration."
   ],
   "p1": 3652,
   "pn": 3653
  },
  "schade24_interspeech": {
   "authors": [
    [
     "Leonie",
     "Schade"
    ],
    [
     "Nico",
     "Dallmann"
    ],
    [
     "Olcay",
     "Tük"
    ],
    [
     "Stefan",
     "Lazarov"
    ],
    [
     "Petra",
     "Wagner"
    ]
   ],
   "title": "Understanding “understanding”: presenting a richly annotated multimodal corpus of dyadic interaction",
   "original": "5031",
   "order": 420,
   "page_count": 2,
   "abstract": [
    "This paper presents the MUNDEX corpus (MUltimodal UNDerstanding of EXplanations) together with past and current investigations using its data. The corpus is constructed to observe the dynamics of co-constructed communication and the understanding of explanations on multiple modalities in dyadic interactions. These modalities are annotated on several levels, including orthographic transcriptions, acoustic information, annotations of head movement, gaze, manual gestures, and further non-verbal behaviour as well as discourse annotations. Present\nand past projects are also concerned with adding further to these annotations. The interlocutors’ level of understanding is currently investigated in regard to several verbal and non-verbal behaviour markers of both the explaining and listening side."
   ],
   "p1": 2040,
   "pn": 2041
  },
  "possamaidemenezes24_interspeech": {
   "authors": [
    [
     "Joao Vitor",
     "Possamai de Menezes"
    ],
    [
     "Arne-Lukas",
     "Fietkau"
    ],
    [
     "Tom",
     "Diener"
    ],
    [
     "Steffen",
     "Kurbis"
    ],
    [
     "Peter",
     "Birkholz"
    ]
   ],
   "title": "A demonstrator for articulation-based command word recognition",
   "original": "5038",
   "order": 421,
   "page_count": 2,
   "abstract": [
    "This paper proposes a demonstration system capable of recognizing command words with a custom optopalatographic device that measures speech articulation. Beyond the measuring device, the proposed system is composed of two interacting computer programs - one for recording and one for the actual word recognition. Operating offline, the recording software can be used to record large datasets, which in turn are used by the recognition software to train and validate classification models. Operating online, the recording software can be used to record single words, which are classified using pre-trained classification models. The system was developed modularly, enabling different devices for measuring speech articulation and different classification models to be easily integrated. The current classification system employs dynamic time warping in combination with a pattern matching algorithm and achieves 98.38% accuracy on a single speaker 40-word corpus."
   ],
   "p1": 2042,
   "pn": 2043
  },
  "ward24b_interspeech": {
   "authors": [
    [
     "Nigel G.",
     "Ward"
    ],
    [
     "Andres",
     "Segura"
    ]
   ],
   "title": "Pragmatically similar utterance finder demonstration",
   "original": "5041",
   "order": 422,
   "page_count": 2,
   "abstract": [
    "Models for estimating the similarity between two utterances are fundamental in speech technology. While fairly good automatic measures exist for semantic similarity, we only recently built a model of pragmatic similarity, the first. We propose to present this model by letting participants try out our Pragmatically Similar Utterance Finder. This system listens to one side of a live conversation, identifies the utterances, and for each retrieves the most similar utterances, according to our model, from a large corpus. Participants and viewers will then be able to hear these utterances, and judge for themselves the prospects for pragmatic-similarity modeling."
   ],
   "p1": 2044,
   "pn": 2045
  },
  "ryumina24_interspeech": {
   "authors": [
    [
     "Elena",
     "Ryumina"
    ],
    [
     "Dmitry",
     "Ryumin"
    ],
    [
     "Alexey",
     "Karpov"
    ]
   ],
   "title": "OCEAN-AI: open multimodal framework for personality traits assessment and HR-processes automatization",
   "original": "5006",
   "order": 742,
   "page_count": 2,
   "abstract": [
    "Human personality traits (PT) reflect individual differences in patterns of thinking, feeling, and behaving. Knowledge on PT may be useful in many applied tasks in our everyday live. In this paper, we present a first open-source multimodal framework called OCEAN-AI for PT assessment (PTA) and HR-processes automatization. Our framework performs PTA analyzing three modalities, including audio, video, and text, and includes three processing modules. All the modules extract heterogeneous (deep neural and hand-crafted) features and use them for a com-\nplex analysis of human’s behavior. The final fourth module aggregates these six feature sets by a Siamese neural network with a gated attention mechanism. Our framework was tested on two free-available corpora, including First Impressions v2 and our MuPTA, and achieved the best results. Applying our framework, a user can automate solutions of some practical applied tasks, such as ranking potential candidates by professional responsibilities, forming efficient work teams and so on."
   ],
   "p1": 3630,
   "pn": 3631
  },
  "mundra24_interspeech": {
   "authors": [
    [
     "Paridhi",
     "Mundra"
    ],
    [
     "Manik",
     "Sharma"
    ],
    [
     "Yashwardhan",
     "Chaudhuri"
    ],
    [
     "Orchid Chetia",
     "Phukan"
    ],
    [
     "Arun Balaji",
     "Buduru"
    ]
   ],
   "title": "VoxMed: one-step respiratory disease classifier using digital stethoscope sounds",
   "original": "5011",
   "order": 743,
   "page_count": 2,
   "abstract": [
    "As respiratory illnesses become more common, it’s crucial to quickly and accurately detect them to improve patient care. There is a need for improved diagnostic methods for immediate medical assessments for optimal patient outcomes. This paper introduces VoxMed—a UI-assisted one-step classifier that uses digital stethoscope recordings to diagnose respiratory diseases. It employs an Audio Spectrogram Transformer(AST) for feature extraction and a 1-D CNN-based architecture to classify respiratory disease, offering professionals information regarding their patients’ respiratory health in seconds. We use the ICBHI dataset, which includes stethoscope recordings collected from patients in Greece and Portugal, to classify respiratory diseases. github repository: https://github.com/Sample-User131001/VoxMed"
   ],
   "p1": 3632,
   "pn": 3633
  },
  "sharma24b_interspeech": {
   "authors": [
    [
     "Sarthak",
     "Sharma"
    ],
    [
     "Orchid Chetia",
     "Phukan"
    ],
    [
     "Drishti",
     "Singh"
    ],
    [
     "Arun Balaji",
     "Buduru"
    ],
    [
     "Rajesh",
     "Sharma"
    ]
   ],
   "title": "AVR: synergizing foundation models for audio-visual humor detection",
   "original": "5015",
   "order": 744,
   "page_count": 2,
   "abstract": [
    "In this work, we present, AVR application for audio-visual humor detection. While humor detection has traditionally centered around textual analysis, recent advancements have spotlighted multimodal approaches. However, these methods lean on textual cues as a modality, necessitating the use of ASR systems for transcribing the audio-data. This heavy reliance on ASR accuracy can pose challenges in real-world applications. To address this bottleneck, we propose an innovative audio-visual humor detection system that circumvents textual reliance, eliminating the need for ASR models. Instead, the proposed approach hinges on the intricate interplay between audio and visual content for effective humor detection."
   ],
   "p1": 3634,
   "pn": 3635
  },
  "lameris24_interspeech": {
   "authors": [
    [
     "Harm",
     "Lameris"
    ],
    [
     "Joakim",
     "Gustafson"
    ],
    [
     "Éva",
     "Székely"
    ]
   ],
   "title": "CreakVC: a voice conversion tool for modulating creaky voice",
   "original": "5025",
   "order": 208,
   "page_count": 2,
   "abstract": [
    "We introduce a human-in-the-loop one-shot voice conversion tool called CreakVC designed to modulate the level of creaky voice in the converted speech. Creaky voice, often used by speakers to convey sociolinguistic cues, presents challenges to speech processing due to its complex phonation characteristics. The primary goal of CreakVC is to enable in-depth research into how these cues are perceived, using systematic perceptual studies. CreakVC provides access to a diverse range of voice identities exhibiting creaky voice, while maintaining consistency in other parameters. We developed a spectrogram-frame level creak representation using CreaPy and finetuned FreeVC,\na one-shot voice conversion tool, by conditioning the speaker embedding and the self-supervised audio representation with the creak representation. An integrated plotting feature allows users to visualize and manipulate portions of speech for precise adjustments of creaky phonation levels. Beyond research, CreakVC has potential applications in voice-interactive systems and multimedia production."
   ],
   "p1": 1005,
   "pn": 1006
  },
  "tsao24_interspeech": {
   "authors": [
    [
     "Yu-Sheng",
     "Tsao"
    ],
    [
     "Yung-Chang",
     "Hsu"
    ],
    [
     "Jiun-Ting",
     "Li"
    ],
    [
     "Siang-Hong",
     "Weng"
    ],
    [
     "Tien-Hong",
     "Lo"
    ],
    [
     "Berlin",
     "Chen"
    ]
   ],
   "title": "EZTalking: English assessment platform for teachers and students",
   "original": "5028",
   "order": 209,
   "page_count": 2,
   "abstract": [
    "EZTalking is an AI-powered English learning platform designed to provide an effective and comfortable learning experience. Based on computer-assisted pronunciation training (CAPT) techniques, our platform features two main functions: (1) A CAPT system that quickly detects mispronunciations and provides immediate feedback to aid students in their pronunciation. (2) A learning portfolio platform that allows teachers to grasp each student’s strengths and weaknesses, letting them create tailored exercises with real-time feedback on areas needing improvement. As students work through exercises in the EZTalking, they receive phoneme-level feedback, enabling them to refine their pronunciation and monitor their progress. Additionally, advanced students can take mock exams to assess their language proficiency thoroughly. With these features, EZTalking streamlines the teaching process and empowers students to reach their language learning goals with confidence."
   ],
   "p1": 1007,
   "pn": 1008
  },
  "koilakuntla24_interspeech": {
   "authors": [
    [
     "Bramhendra",
     "Koilakuntla"
    ],
    [
     "Prajesh",
     "Rana"
    ],
    [
     "Paras",
     "Ahuja"
    ],
    [
     "Srikanth",
     "Konjeti"
    ],
    [
     "Jithendra",
     "Vepa"
    ]
   ],
   "title": "Leveraging large language models for post-transcription correction in contact centers",
   "original": "5014",
   "order": 419,
   "page_count": 2,
   "abstract": [
    "Contact centers depend on Automatic Speech Recognition (ASR) to power their downstream tasks. However, any mis-transcription in the ASR can have a significant impact on their downstream tasks. This issue is compounded by the extensive array of diverse brand and business names. Traditional transcription correction methods have a long development cycle and require skilled resources. Most of the time these errors will\nhave a context, suggesting a search and replace solution in the post-call analytics platform. But identifying these contexts is time-consuming and tedious. Moreover, these words may get recognized in various similar forms, further complicating the situation. To tackle this, we propose a post transcription correction module by employing Large Language Models (LLMs) to detect these contexts, termed ‘anchors’ and to correct phonetically similar misrecognised words. By leveraging anchor phrases, we can pinpoint the error occurrences and correct the\nmisrecognized."
   ],
   "p1": 2038,
   "pn": 2039
  },
  "obukhov24_interspeech": {
   "authors": [
    [
     "Dmitrii",
     "Obukhov"
    ],
    [
     "Marcel",
     "de Korte"
    ],
    [
     "Andrey",
     "Adaschik"
    ]
   ],
   "title": "ATTEST: an analytics tool for the testing and evaluation of speech technologies",
   "original": "5026",
   "order": 750,
   "page_count": 2,
   "abstract": [
    "Advances in speech technology have led to the development of a large variety of powerful speech models. With the increased number of models released comes the need to quickly and effectively compare new (versions of) models with existing ones on selected metrics. Acknowledging this need, we introduce ATTEST, a powerful, user-friendly, no-code Streamlit framework that provides a large range of objective metrics for\ncomprehensive speech evaluation, covering the dimensions of Speech Intelligibility, Speech Prosody, Speaker Similarity, Signal Quality, and MOS Prediction. This paper details the framework’s architecture, user interface, and application scenarios. To encourage easy integration of newly-developed metrics, thesource code for this framework is released alongside this paper."
   ],
   "p1": 3646,
   "pn": 3647
  },
  "masson24_interspeech": {
   "authors": [
    [
     "Margot",
     "Masson"
    ],
    [
     "Erfan A.",
     "Shams"
    ],
    [
     "Iona",
     "Gessinger"
    ],
    [
     "Julie",
     "Carson-Berndsen"
    ]
   ],
   "title": "PhoneViz: exploring alignments at a glance",
   "original": "5029",
   "order": 751,
   "page_count": 2,
   "abstract": [
    "This Show and Tell presents PhoneViz, a phone alignment visualiser which facilitates a deeper analysis of the phone alignments typically used to compare a reference transcription and a concrete speaker pronunciation. PhoneViz provides an interactive environment where aligned phones are displayed in the IPA chart helping users to explore phonetic variation beyond symbol substitution, insertion and deletion. We showcase the functionality of the visualiser using samples of Spanish-accented English, where users can see at a glance where there are phonetic similarities between substituted sounds."
   ],
   "p1": 3648,
   "pn": 3649
  },
  "pages24_interspeech": {
   "authors": [
    [
     "Clément",
     "Pages"
    ],
    [
     "Hervé",
     "Bredin"
    ]
   ],
   "title": "Gryannote open-source speaker diarization labeling tool",
   "original": "5032",
   "order": 752,
   "page_count": 2,
   "abstract": [
    "gryannote is a collection of Gradio custom components focusing on the labeling of speaker diarization data. Integrated with the pyannote speaker diarization ecosystem, it allows to build web applications to load pretrained pyannote pipelines and customize their hyper-parameters, upload or record an audio file, process it with the pipeline, visualize and interact with its outputs, correct them if needed, and export the final annotation in RTTM format. Each of these components can be used independently from each other."
   ],
   "p1": 3650,
   "pn": 3651
  },
  "liu24s_interspeech": {
   "authors": [
    [
     "Kai",
     "Liu"
    ],
    [
     "Ziqing",
     "Du"
    ],
    [
     "Zhou",
     "Huan"
    ],
    [
     "Xucheng",
     "Wan"
    ],
    [
     "Naijun",
     "Zheng"
    ]
   ],
   "title": "Real-time scheme for rapid extraction of speaker embeddings in challenging recording conditions",
   "original": "5043",
   "order": 423,
   "page_count": 2,
   "abstract": [
    "Speaker embedding plays a crucial role in the realm of speaker-related tasks like speaker verification, diarization, target speaker extraction and voice conversion. Variant networks have been suggested for generating speaker embedding from the enrollment speech of the target speaker. Nevertheless, in real-time speaker-related tasks, it is typical that a pristine recording environment is unavailable. Consequently, enrollment speech may be tainted by background noises and interference from non-target speakers, potentially compromising task \nperformance. In this study, we present a three-stage progressive filtering scheme for rapidly extracting speaker embeddings in noisy recording scenarios, and validate its effectiveness and efficiency through an real-time target speaker extraction task conducted on real meeting data."
   ],
   "p1": 2046,
   "pn": 2047
  },
  "sirigiraju24_interspeech": {
   "authors": [
    [
     "Meenakshi",
     "Sirigiraju"
    ],
    [
     "Arjun",
     "Rajasekar"
    ],
    [
     "Abhishikth",
     "Meejuri"
    ],
    [
     "Chiranjeevi",
     "Yarra"
    ]
   ],
   "title": "IIITH Ucchar e-Sudharak: an automatic English pronunciation corrector for school-going children with a teacher in the loop",
   "original": "5004",
   "order": 1064,
   "page_count": 2,
   "abstract": [
    "The language acquisition skills are predominant during childhood. Due to the lack of resources, a gap often arises in the language communication abilities of students. To bridge this gap, it is necessary to consider automatic methods for improving the children language skills. Our IIITH Ucchar e-Sudharak web based tool, deployed in a real environment, helps school-going children to practice English reading skills with a teacher in the loop. The choice of English as the target language is strategic, given its status as a global lingua franca. Within the tool, children are provided with class and chapter-specific English sentences for practice tailored by their teachers. During practice sessions, the tool provides real-time feedback in the form of sentence and word-level scores, which are computed by comparing against teacher (expert) audio recordings. These scores offer insights into word-level pronunciation correctness and sentence-level quality by measuring fluency. Moreover, the tool provides a key feature where teachers can assign specific sentences to individual students and assess their performance. This functionality empowers teachers to personalize their instruction, catering to the specific needs of each student. As far as we know, this is a one-of-a-kind tool specifically designed to meet the needs of school-going children."
   ],
   "p1": 5198,
   "pn": 5199
  },
  "yap24_interspeech": {
   "authors": [
    [
     "Boon Peng",
     "Yap"
    ],
    [
     "Kok Liang",
     "Tan"
    ],
    [
     "Zhenghao",
     "Li"
    ],
    [
     "Rong",
     "Tong"
    ]
   ],
   "title": "Speech enabled visual acuity test",
   "original": "5012",
   "order": 1065,
   "page_count": 2,
   "abstract": [
    "This paper introduces a novel automatic visual acuity test system employing automatic speech recognition and image recognition. The system provides users a private and self-paced eye assessment experience. During such visual acuity test, the system communicates with user via speech interaction: the system prompts user to speak out the letters shown and recognizes user’s speech input to assess user’s eye sights. The system is also able to automatically detect user’s posture and notify user when incorrect posture is detected. When the testing process is completed, the system will analyze the result automatically and record the details in database."
   ],
   "p1": 5200,
   "pn": 5201
  },
  "aiba24_interspeech": {
   "authors": [
    [
     "Mayuko",
     "Aiba"
    ],
    [
     "Daisuke",
     "Saito"
    ],
    [
     "Nobuaki",
     "Minematsu"
    ]
   ],
   "title": "A ChatGPT-based oral Q&A practice system for first-time student participants in international conferences",
   "original": "5034",
   "order": 1066,
   "page_count": 2,
   "abstract": [
    "A ChatGPT-based system was developed to enhance students’ oral Q&A skills at international conferences. By uploading their original papers to ChatGPT in the system, it generates oral questions to the students, who can practice Q&A. We examined six configurations for question generation: 1) whether reference papers should also be explicitly uploaded to the system, and 2) whether or not the questions should be generated for each keyword or each section in sequence. Surprisingly, all the students and teachers who participated in the experiment chose the simplest configuration – no explicit upload of reference papers and question generation from the entire scope of the paper. This finding led us to combine the simply-configured ChatGPT with a speech interface to create an interactive practice environment. Our system orally poses questions to a student about their papers, who responds verbally. After using this environment, all the participants strongly affirmed its effectiveness of practicing Q&A to attend future international conferences."
   ],
   "p1": 5202,
   "pn": 5203
  },
  "chen24aa_interspeech": {
   "authors": [
    [
     "Szu-Yu",
     "Chen"
    ],
    [
     "Tien-Hong",
     "Lo"
    ],
    [
     "Yao-Ting",
     "Sung"
    ],
    [
     "Ching-Yu",
     "Tseng"
    ],
    [
     "Berlin",
     "Chen"
    ]
   ],
   "title": "TEEMI: a speaking practice tool for L2 English learners",
   "original": "5035",
   "order": 424,
   "page_count": 2,
   "abstract": [
    "With the influence that globalization brought along and with hopes of linking up with the world, having a second foreign language has become crucial. Being the language spoken by the most people worldwide, English is a popular choice for foreign language acquisition. While modern technology has realized automated speaking assessment in recent years, there still has been limited research and development focusing of automated speaking assessment platforms for native Chinese speakers. This paper presents the speaking practice tool on TEEMI (Test\nof English for English Medium Instruction). This tool provides the opportunity for users to practice English speaking, and for teachers to adjust their teaching approach accordingly. TEEMI speaking tasks allows users to showcase and enhance their English-speaking proficiency. All TEEMI speaking tasks are in open-speech format. Additionally, TEEMI can be accessed on all kinds of devices, i.e. PC, laptop, tablet, and smartphone. The total user count of TEEMI speaking tasks has surpassed 20,000 in Taiwan."
   ],
   "p1": 2048,
   "pn": 2049
  },
  "sridaran24_interspeech": {
   "authors": [
    [
     "Karthik Venkat",
     "Sridaran"
    ],
    [
     "Raja",
     "Praveen"
    ],
    [
     "Reuben T",
     "Varghese"
    ],
    [
     "Ajish K",
     "Abraham"
    ],
    [
     "Shankar",
     "R"
    ],
    [
     "Winnie Rachel",
     "Cherian"
    ]
   ],
   "title": "Visual scene display application for augmentative and alternative communication",
   "original": "5036",
   "order": 1067,
   "page_count": 2,
   "abstract": [
    "Augmentative and Alternative Communication (AAC) refers to various strategies and tools used to enhance communication for individuals with limited speech or language abilities. Visual Scene Displays (VSDs) have emerged as a promising approach within AAC, leveraging visual cues and contextualized scenes to support communication and language development. This show and tell presentation describes the development of a Visual Scene Display application, implemented as a webpage. The application offers a user-friendly interface that allows individuals to create and customize their own scenes by uploading images or capturing real-life scenes using the device's camera. The system incorporates a robust symbol library, including pictograms, icons, and audio to support comprehension and expression. The development process involved iterative design, usability testing, and feedback from AAC professionals, individuals with communication challenges, and their caregivers. This collaborative approach ensured that the VSD system addressed the diverse needs and abilities of its target users. Preliminary evaluations of the VSD system have shown promising results, indicating its potential to enhance communication and language development among individuals with limited speech or language abilities."
   ],
   "p1": 5204,
   "pn": 5205
  },
  "masudakatsuse24_interspeech": {
   "authors": [
    [
     "Ikuyo",
     "Masuda-Katsuse"
    ],
    [
     "Ayako",
     "Shirose"
    ]
   ],
   "title": "CALL system using pitch-accent feature representations reflecting listeners’ subjective adequacy",
   "original": "5037",
   "order": 1068,
   "page_count": 2,
   "abstract": [
    "This paper presents a CALL system that implements a method for automatically and quantitatively evaluating the pitch accents of spoken words of Japanese learners in terms of how well they are accepted by native speakers."
   ],
   "p1": 5206,
   "pn": 5207
  },
  "preston24_interspeech": {
   "authors": [
    [
     "Jonathan L",
     "Preston"
    ],
    [
     "Nina R",
     "Benway"
    ],
    [
     "Nathan",
     "Prestopnik"
    ],
    [
     "Nathan",
     "Preston"
    ]
   ],
   "title": "The speech motor chaining web app for speech motor learning",
   "original": "5039",
   "order": 1069,
   "page_count": 2,
   "abstract": [
    "Systematic review has indicated that existing computerized speech learning apps show unclear potential to invoke speech change, highlighting the need to incorporate tested theoretical principles in app design. As speech is a motor skill, this paper illustrates how the principles of motor learning underlie the Speech Motor Chaining speech therapy web app. Features such as game design, telepractice, and speech analysis are discussed."
   ],
   "p1": 5208,
   "pn": 5209
  },
  "yoder24_interspeech": {
   "authors": [
    [
     "Charlotte",
     "Yoder"
    ],
    [
     "Karrie",
     "Karahalios"
    ],
    [
     "Mark",
     "Hasegawa-Johnson"
    ],
    [
     "Shreyansh",
     "Agrawal"
    ]
   ],
   "title": "Visualization for improving foreign language pronunciation",
   "original": "5040",
   "order": 1070,
   "page_count": 2,
   "abstract": [
    "Speaking coherently in a second language requires knowledge and execution of pronunciation. Often, pronunciation training is seldom emphasized in language learning. However, explicitly teaching pronunciation can increase confidence and attitudes towards pronunciation in learners. We present a visual feedback system for vowels using a speaker calibrated vowel chart. The demo includes vowel chart calibration, an interactive tutorial for learning how to read a vowel chart, and a practice page for three vowel minimal pairs."
   ],
   "p1": 5210,
   "pn": 5211
  },
  "phan24b_interspeech": {
   "authors": [
    [
     "Nhan",
     "Phan"
    ],
    [
     "Anna",
     "von Zansen"
    ],
    [
     "Maria",
     "Kautonen"
    ],
    [
     "Tamás",
     "Grósz"
    ],
    [
     "Mikko",
     "Kurimo"
    ]
   ],
   "title": "CaptainA self-study mobile app for practising speaking: task completion assessment and feedback with generative AI",
   "original": "5042",
   "order": 1071,
   "page_count": 2,
   "abstract": [
    "We introduce the CaptainA mobile app, designed to meet the needs of second language (L2) learners engaged in self-study of Finnish, with potential applicability to other languages. Our app can provide automatic speaking assessment (ASA) of task completion in picture-based tasks, along with grading explanations and corrective feedback. It can also automatically generate pictures for visual tasks, providing users with unlimited practice opportunities. The mobile app is based on our framework that combines visual natural language generation\n(NLG), automatic speech recognition (ASR), and prompting large language model (LLM) for low-resource language. Our goal is to promote the development of next-generation speech-based computer-assisted language learning (CALL) systems capable of providing automatic scoring with feedback for learners, even when minimal speech data of L2 learners is available. While the mobile app demonstration is designed for Finnish, the app can also be tested in English."
   ],
   "p1": 5212,
   "pn": 5213
  },
  "akhtar24_interspeech": {
   "authors": [
    [
     "Mohd Mujtaba",
     "Akhtar"
    ],
    [
     "",
     "Girish"
    ],
    [
     "Orchid Chetia",
     "Phukan"
    ],
    [
     "Muskaan",
     "Singh"
    ]
   ],
   "title": "NeuRO: an application for code-switched autism detection in children",
   "original": "5019",
   "order": 747,
   "page_count": 2,
   "abstract": [
    "Code-switching is a common communication phenomenon where individuals alternate between two or more languages or linguistic styles within a single conversation. Autism Spectrum Disorder(ASD) is a developmental disorder posing challenges in social interaction, communication, and repetitive behaviors. Detecting ASD in individuals with code-switch scenario presents unique challenges. In this paper, we address this problem by building an application NeuRO which aims to detect potential signs of autism in code-switched conversations, facilitating early intervention and support for individuals with ASD."
   ],
   "p1": 3640,
   "pn": 3641
  },
  "phukan24c_interspeech": {
   "authors": [
    [
     "Orchid Chetia",
     "Phukan"
    ],
    [
     "Sarthak",
     "Jain"
    ],
    [
     "Shubham",
     "Singh"
    ],
    [
     "Muskaan",
     "Singh"
    ],
    [
     "Arun Balaji",
     "Buduru"
    ],
    [
     "Rajesh",
     "Sharma"
    ]
   ],
   "title": "ComFeAT: combination of neural and spectral features for improved depression detection",
   "original": "5021",
   "order": 748,
   "page_count": 2,
   "abstract": [
    "In this work, we focus on the detection of depression through speech analysis. Previous research has widely explored features extracted from pre-trained models (PTMs) primarily trained for paralinguistic tasks. Although these features have led to sufficient advances in speech-based depression detection, their performance declines in real-world settings. To address this, in this paper, we introduce ComFeAT, an application that employs a CNN model trained on a combination of features extracted from PTMs, a.k.a. neural features and spectral features to enhance depression detection. Spectral features are robust to domain variations, but, they are not as good as neural features in per-\nformance, suprisingly, combining them shows complementary behavior and improves over both neural and spectral features individually. The proposed method also improves over previous state-of-the-art (SOTA) works on E-DAIC benchmark."
   ],
   "p1": 3642,
   "pn": 3643
  },
  "trancoso24_interspeech": {
   "authors": [
    [
     "Isabel",
     "Trancoso"
    ]
   ],
   "title": "Towards Responsible Speech Processing",
   "original": "8001",
   "order": 1,
   "page_count": 1,
   "abstract": [
    "Responsible AI may not be a consensuous concept and the list of the so called pillars may not be uniquely defined either. Nonetheless, their message is clear and urgent. In this talk, I’ll address some of the pillars of responsible speech processing, focusing on privacy, explainability (namely for health applications), fairness/inclusion and sustainability. Rather than attempting a comprehensive survey of all the efforts in these directions, I will present my own perspective of how these pillars should inform the next generation of speech research."
   ],
   "p1": 1,
   "pn": 1
  },
  "araki24_interspeech": {
   "authors": [
    [
     "Shoko",
     "Araki"
    ]
   ],
   "title": "Frontier of Frontend for Conversational Speech Processing",
   "original": "8002",
   "order": 210,
   "page_count": 1,
   "abstract": [
    "To deepen and enrich our daily communications, researchers have made significant efforts over several decades to develop technologies that can recognize and understand natural human conversations. Despite significant progress in both speech/language processing and speech enhancement technology, conversational speech processing remains challenging. Recordings of conversations with distant microphones contain ambient noise, reverberation, and speaker overlap that changes as the conversation progresses. Consequently, recognizing conversational speech is much more challenging than single-talker speech recognition, and frontend technologies such as speech enhancement and speaker diarization are essential to achieving highly accurate conversational speech processing. \nFor more than two decades, the presenter‘s research group has explored frontend techniques (source separation, dereverberation, noise reduction, and diarization) for handling realistic natural conversations with distant microphones. In this talk, I would like to talk about the evolution and frontier of frontend technologies for conversational signal processing. Specifically, we will trace the evolution of multichannel signal processing and neural network techniques, including beamforming and target speaker tracking and extraction, which have always played an important role in successive cutting-edge frontends, along with the latest achievements."
   ],
   "p1": 1009,
   "pn": 1009
  },
  "noeth24_interspeech": {
   "authors": [
    [
     "Elmar",
     "Noeth"
    ]
   ],
   "title": "Analysis of Pathological Speech – Pitfalls along the Way",
   "original": "8003",
   "order": 531,
   "page_count": 1,
   "abstract": [
    "In this talk, I focus on speech as an easy-to-extract biomarker for various diseases and congenital defects. I discuss the motivation and information gain of the analysis of pathological speech as well as various aspects which are more or less important when compared to the analysis of regular speech. Examples for these aspects are small data collections, data privacy, and explainability of the automatic decisions."
   ],
   "p1": 2579,
   "pn": 2579
  },
  "tillmann24_interspeech": {
   "authors": [
    [
     "Barbara",
     "Tillmann"
    ]
   ],
   "title": "Perception of music and speech: Focus on rhythm processing",
   "original": "8004",
   "order": 859,
   "page_count": 1,
   "abstract": [
    "Research in cognitive neuroscience has revealed similarities in neural and cognitive correlates of music and language processing. Investigations focusing on temporal processing, in particular, rhythmic and metrical processing, have revealed interesting connections between music and speech. These observations have led to several theoretical frameworks and hypotheses about underlying mechanisms and neural functioning, and has motivated applications to clinical research. I will present research that has demonstrated beneficial effects of rhythmic stimulation or training to improve language processing in populations of adults and children with typical development and with developmental language disorder or dyslexia. A recent hypothesis highlights the potential value of early detection of atypical rhythmic processing as indicative of increased risk for language disorders. This research domain provides perspectives for creating rhythm-based training programs for rehabilitation and also for early intervention, aiming to decrease language deficits during development."
   ],
   "p1": 4179,
   "pn": 4179
  }
 },
 "sessions": [
  {
   "title": "Keynote 1 ISCA Medallist",
   "papers": [
    "trancoso24_interspeech"
   ]
  },
  {
   "title": "L2 Speech, Bilingualism and Code-Switching",
   "papers": [
    "wesolek24_interspeech",
    "chi24_interspeech",
    "xue24_interspeech",
    "mohapatra24b_interspeech"
   ]
  },
  {
   "title": "Speaker Diarization 1",
   "papers": [
    "chowdhury24_interspeech",
    "li24x_interspeech",
    "huang24d_interspeech",
    "harkonen24_interspeech",
    "yin24_interspeech",
    "arya24_interspeech"
   ]
  },
  {
   "title": "Speech and Audio Analysis and Representations",
   "papers": [
    "zhao24h_interspeech",
    "niizumi24_interspeech",
    "fujita24_interspeech",
    "meyer24b_interspeech",
    "li24ja_interspeech",
    "ullah24_interspeech",
    "pieper24_interspeech"
   ]
  },
  {
   "title": "Acoustic Event Detection and Classification 2",
   "papers": [
    "liang24_interspeech",
    "mu24_interspeech",
    "nam24_interspeech",
    "ho24_interspeech",
    "jiang24c_interspeech",
    "xie24d_interspeech",
    "ghaffarzadegan24_interspeech"
   ]
  },
  {
   "title": "Detection and Classification of Bioacoustic Signals",
   "papers": [
    "kumar24_interspeech",
    "jing24_interspeech",
    "cauzinille24_interspeech",
    "qiu24_interspeech",
    "lin24_interspeech"
   ]
  },
  {
   "title": "Acoustic Echo Cancellation",
   "papers": [
    "nayak24_interspeech",
    "khanagha24_interspeech",
    "gao24b_interspeech",
    "ni24_interspeech",
    "schwartz24_interspeech",
    "zhao24b_interspeech"
   ]
  },
  {
   "title": "Speech Synthesis: Voice Conversion 1",
   "papers": [
    "seki24_interspeech",
    "baade24_interspeech",
    "morrison24_interspeech",
    "kaneko24_interspeech",
    "ning24_interspeech",
    "qi24_interspeech"
   ]
  },
  {
   "title": "Neural Network Architectures for ASR 2",
   "papers": [
    "nakagome24_interspeech",
    "meng24_interspeech",
    "flynn24b_interspeech",
    "vitale24_interspeech",
    "zhang24q_interspeech",
    "prabhu24_interspeech",
    "miyazaki24_interspeech",
    "li24sa_interspeech",
    "wan24_interspeech",
    "gupta24_interspeech",
    "gong24b_interspeech"
   ]
  },
  {
   "title": "Decoding Algorithms",
   "papers": [
    "wang24k_interspeech",
    "zou24_interspeech",
    "ciaperoni24_interspeech",
    "galvez24_interspeech",
    "wang24w_interspeech",
    "takagi24_interspeech"
   ]
  },
  {
   "title": "Pronunciation Assessment",
   "papers": [
    "wang24la_interspeech",
    "chen24c_interspeech",
    "cao24b_interspeech",
    "shahin24_interspeech",
    "do24_interspeech",
    "phan24_interspeech"
   ]
  },
  {
   "title": "Spoken Language Processing",
   "papers": [
    "wang24c_interspeech",
    "jung24_interspeech",
    "jin24d_interspeech",
    "li24r_interspeech",
    "yen24_interspeech",
    "monteiro24_interspeech",
    "peng24b_interspeech"
   ]
  },
  {
   "title": "Spoken Machine Translation 2",
   "papers": [
    "chen24m_interspeech",
    "abdullah24_interspeech",
    "chen24w_interspeech",
    "rabatin24_interspeech",
    "wang24aa_interspeech",
    "min24_interspeech",
    "oneata24_interspeech",
    "khurana24_interspeech"
   ]
  },
  {
   "title": "Biosignal-enabled Spoken Communication",
   "papers": [
    "li24ca_interspeech",
    "wu24k_interspeech",
    "bras24_interspeech",
    "kwon24_interspeech",
    "yang24o_interspeech",
    "jain24_interspeech",
    "bandekar24_interspeech",
    "yan24b_interspeech",
    "lin24f_interspeech",
    "pahuja24_interspeech"
   ]
  },
  {
   "title": "Individual and Social Factors in Phonetics",
   "papers": [
    "pistor24_interspeech",
    "li24ra_interspeech",
    "smith24_interspeech",
    "ochi24_interspeech",
    "tanner24_interspeech",
    "taylor24_interspeech"
   ]
  },
  {
   "title": "Paralinguistics",
   "papers": [
    "parragallego24_interspeech",
    "kodali24_interspeech",
    "kathan24_interspeech",
    "bhattacharya24_interspeech"
   ]
  },
  {
   "title": "Speaker Recognition: Adversarial and Spoofing Attacks",
   "papers": [
    "rosello24_interspeech",
    "zhang24j_interspeech",
    "wu24b_interspeech",
    "lu24b_interspeech",
    "kan24_interspeech",
    "chen24p_interspeech",
    "chen24_interspeech",
    "li24g_interspeech",
    "truong24b_interspeech"
   ]
  },
  {
   "title": "Audio Event Detection and Classification 1",
   "papers": [
    "feng24b_interspeech",
    "dinkel24b_interspeech",
    "yadav24_interspeech",
    "cai24_interspeech",
    "ebbers24_interspeech",
    "li24k_interspeech"
   ]
  },
  {
   "title": "Source Separation 2",
   "papers": [
    "taherian24_interspeech",
    "ewert24_interspeech",
    "pan24_interspeech",
    "zhang24p_interspeech",
    "hsieh24b_interspeech",
    "lin24g_interspeech",
    "wang24g_interspeech",
    "saijo24_interspeech"
   ]
  },
  {
   "title": "Noise Reduction, Dereverberation, and Echo Cancellation",
   "papers": [
    "zhao24_interspeech",
    "guo24_interspeech",
    "bahrman24_interspeech",
    "yuan24_interspeech",
    "barnhill24_interspeech",
    "nayak24b_interspeech",
    "wang24o_interspeech"
   ]
  },
  {
   "title": "Computationally-Efficient Speech Enhancement",
   "papers": [
    "bae24_interspeech",
    "gholami24_interspeech",
    "zhang24o_interspeech",
    "zhao24c_interspeech",
    "zhang24k_interspeech",
    "lin24h_interspeech",
    "cheng24_interspeech"
   ]
  },
  {
   "title": "Zero-shot TTS",
   "papers": [
    "xue24c_interspeech",
    "wang24v_interspeech",
    "fujita24b_interspeech",
    "pankov24_interspeech"
   ]
  },
  {
   "title": "Noise Robustness, Far-Field, and Multi-Talker ASR",
   "papers": [
    "jin24_interspeech",
    "xing24_interspeech",
    "shi24e_interspeech",
    "zheng24d_interspeech",
    "bando24_interspeech",
    "masumura24_interspeech"
   ]
  },
  {
   "title": "Contextual Biasing and Adaptation",
   "papers": [
    "shamsian24_interspeech",
    "manhtienanh24_interspeech",
    "wang24q_interspeech",
    "yang24j_interspeech",
    "huang24f_interspeech",
    "andrusenko24_interspeech",
    "wei24_interspeech",
    "liu24e_interspeech",
    "liu24d_interspeech",
    "zhao24d_interspeech",
    "ando24_interspeech",
    "khassanov24_interspeech",
    "yusuf24_interspeech",
    "kim24u_interspeech"
   ]
  },
  {
   "title": "Spoken Language Understanding",
   "papers": [
    "futami24_interspeech",
    "porjazovski24_interspeech",
    "laperriere24_interspeech",
    "lee24i_interspeech",
    "li24b_interspeech",
    "koudounas24b_interspeech"
   ]
  },
  {
   "title": "Spoken Machine Translation 1",
   "papers": [
    "huang24h_interspeech",
    "hirschkind24_interspeech",
    "chen24v_interspeech",
    "lee24h_interspeech",
    "tan24b_interspeech",
    "macaire24_interspeech"
   ]
  },
  {
   "title": "Hearing Disorders",
   "papers": [
    "lee24e_interspeech",
    "ahn24_interspeech",
    "huckvale24_interspeech",
    "fagniart24_interspeech",
    "irino24_interspeech",
    "niu24c_interspeech",
    "monaghan24_interspeech"
   ]
  },
  {
   "title": "Speech Disorders 2",
   "papers": [
    "changawala24_interspeech",
    "xiong24_interspeech",
    "chen24i_interspeech",
    "ilias24_interspeech",
    "ng24_interspeech",
    "papadimitriou24_interspeech",
    "ariasvergara24_interspeech",
    "zhang24l_interspeech",
    "zhou24e_interspeech",
    "gosztolya24c_interspeech"
   ]
  },
  {
   "title": "TAUKADIAL Challenge: Speech-Based Cognitive Assessment in Chinese and English (Special Session)",
   "papers": [
    "luz24_interspeech",
    "ortizperez24_interspeech",
    "gosztolya24_interspeech",
    "duan24_interspeech",
    "barreraaltuna24_interspeech",
    "favaro24_interspeech",
    "hoang24_interspeech",
    "pereztoro24_interspeech"
   ]
  },
  {
   "title": "Show and Tell 1",
   "papers": [
    "arai24_interspeech",
    "gourav24_interspeech",
    "mohan24_interspeech",
    "mika24_interspeech",
    "akarsh24_interspeech",
    "okamoto24b_interspeech",
    "peirolilja24_interspeech",
    "francis24_interspeech",
    "rohmatillah24_interspeech",
    "lameris24_interspeech",
    "tsao24_interspeech"
   ]
  },
  {
   "title": "Keynote 2",
   "papers": [
    "araki24_interspeech"
   ]
  },
  {
   "title": "Phonetics and Phonology of Second Language Acquisition",
   "papers": [
    "tuttosi24_interspeech",
    "popescu24_interspeech",
    "huang24i_interspeech",
    "colgiu24_interspeech",
    "coulange24_interspeech",
    "wu24m_interspeech"
   ]
  },
  {
   "title": "Corpora-based Approaches in Automatic Emotion Recognition",
   "papers": [
    "ranjan24_interspeech",
    "mote24_interspeech",
    "wang24ja_interspeech",
    "xi24_interspeech",
    "gao24f_interspeech"
   ]
  },
  {
   "title": "Analysis of Speakers States and Traits",
   "papers": [
    "niebuhr24_interspeech",
    "li24ma_interspeech",
    "murzaku24_interspeech",
    "chen24f_interspeech",
    "tao24b_interspeech",
    "kunmei24_interspeech",
    "gerczuk24_interspeech"
   ]
  },
  {
   "title": "Spoofing and Deepfake Detection",
   "papers": [
    "klein24_interspeech",
    "liu24m_interspeech",
    "wang24l_interspeech",
    "baser24_interspeech",
    "li24oa_interspeech",
    "ge24_interspeech"
   ]
  },
  {
   "title": "Audio Captioning, Tagging, and Audio-Text Retrieval",
   "papers": [
    "sun24c_interspeech",
    "liu24_interspeech",
    "xin24b_interspeech",
    "dinkel24_interspeech",
    "chaudhary24_interspeech",
    "jing24b_interspeech",
    "xu24e_interspeech"
   ]
  },
  {
   "title": "Generative Speech Enhancement",
   "papers": [
    "scheibler24_interspeech",
    "yang24h_interspeech",
    "jukic24_interspeech",
    "trachu24_interspeech",
    "yang24k_interspeech",
    "kim24o_interspeech"
   ]
  },
  {
   "title": "Speech Synthesis: Evaluation",
   "papers": [
    "yin24b_interspeech",
    "anand24_interspeech",
    "edlund24_interspeech",
    "adigwe24_interspeech",
    "wang24s_interspeech",
    "saget24_interspeech"
   ]
  },
  {
   "title": "Multilingual ASR",
   "papers": [
    "kwok24_interspeech",
    "shi24g_interspeech",
    "pineiromartin24_interspeech",
    "saif24_interspeech",
    "li24s_interspeech",
    "houston24_interspeech"
   ]
  },
  {
   "title": "General Topics in ASR",
   "papers": [
    "suh24_interspeech",
    "li24c_interspeech",
    "zusag24_interspeech",
    "mihajlik24_interspeech",
    "mujtaba24_interspeech",
    "tan24_interspeech",
    "lehecka24_interspeech",
    "delafuente24_interspeech",
    "leivaditi24_interspeech",
    "hsieh24_interspeech",
    "wang24x_interspeech",
    "zheng24_interspeech",
    "li24h_interspeech",
    "wang24n_interspeech"
   ]
  },
  {
   "title": "Spoken Language Understanding",
   "papers": [
    "phung24_interspeech",
    "li24_interspeech",
    "tran24b_interspeech",
    "kando24_interspeech",
    "yue24_interspeech",
    "johnson24_interspeech"
   ]
  },
  {
   "title": "Speech and Multimodal Resources",
   "papers": [
    "pesan24_interspeech",
    "turetzky24_interspeech",
    "wang24b_interspeech",
    "kong24_interspeech",
    "anderer24_interspeech",
    "sungbin24_interspeech",
    "veliche24_interspeech",
    "lu24f_interspeech",
    "osman24_interspeech"
   ]
  },
  {
   "title": "Pathological Speech Analysis 1",
   "papers": [
    "gudmundsson24_interspeech",
    "laquatra24_interspeech",
    "triantafyllopoulos24_interspeech",
    "amiri24_interspeech",
    "kim24q_interspeech"
   ]
  },
  {
   "title": "Speech and Language in Health: from Remote Monitoring to Medical Conversations - 1 (Special Session)",
   "papers": [
    "kadkhodaieelyaderani24_interspeech",
    "mun24_interspeech",
    "despotovic24_interspeech",
    "goria24_interspeech",
    "rameau24_interspeech",
    "dumpala24b_interspeech",
    "mehta24_interspeech",
    "williams24_interspeech",
    "demir24_interspeech",
    "premananth24_interspeech"
   ]
  },
  {
   "title": "Speech and Brain",
   "papers": [
    "wang24ka_interspeech",
    "li24l_interspeech",
    "wang24ba_interspeech",
    "neelabh24_interspeech",
    "feng24_interspeech",
    "lee24c_interspeech"
   ]
  },
  {
   "title": "Innovative Methods in Phonetics and Phonology",
   "papers": [
    "ahn24d_interspeech",
    "raybarman24_interspeech",
    "tadavarthy24_interspeech",
    "kaland24_interspeech",
    "rousso24_interspeech",
    "kim24l_interspeech",
    "lambropoulos24_interspeech",
    "chung24_interspeech",
    "weise24_interspeech",
    "oura24_interspeech"
   ]
  },
  {
   "title": "Voice, Tones and F0",
   "papers": [
    "li24f_interspeech",
    "xiaowang24_interspeech",
    "weirich24_interspeech",
    "xu24j_interspeech",
    "gessinger24_interspeech"
   ]
  },
  {
   "title": "Emotion Recognition: Resources and Benchmarks",
   "papers": [
    "ma24b_interspeech",
    "triantafyllopoulos24b_interspeech",
    "ibrahim24_interspeech",
    "naini24_interspeech",
    "latif24_interspeech",
    "wang24ia_interspeech"
   ]
  },
  {
   "title": "Speaker and Language Identification and Diarization",
   "papers": [
    "rahou24_interspeech",
    "boeddeker24_interspeech",
    "mariotte24_interspeech",
    "pirlogeanu24_interspeech",
    "kalluri24_interspeech",
    "kalda24_interspeech",
    "hao24b_interspeech",
    "valente24_interspeech",
    "paturi24_interspeech",
    "su24_interspeech",
    "makishima24_interspeech",
    "munakata24_interspeech"
   ]
  },
  {
   "title": "Audio-Text Retrieval",
   "papers": [
    "xin24_interspeech",
    "yan24_interspeech",
    "deshmukh24_interspeech",
    "paissan24_interspeech",
    "kim24f_interspeech",
    "tang24b_interspeech"
   ]
  },
  {
   "title": "Speech Enhancement",
   "papers": [
    "liu24n_interspeech",
    "liu24o_interspeech",
    "li24aa_interspeech",
    "zhang24_interspeech",
    "li24w_interspeech",
    "zhang24n_interspeech",
    "guan24_interspeech",
    "mawalim24_interspeech",
    "zhang24i_interspeech"
   ]
  },
  {
   "title": "Speech Coding",
   "papers": [
    "zhang24g_interspeech",
    "zhang24m_interspeech",
    "gupta24c_interspeech",
    "muller24c_interspeech",
    "ai24b_interspeech",
    "wu24p_interspeech"
   ]
  },
  {
   "title": "Speech Synthesis: Expressivity and Emotion",
   "papers": [
    "li24pa_interspeech",
    "seong24b_interspeech",
    "li24na_interspeech",
    "guo24d_interspeech",
    "bott24_interspeech",
    "xue24b_interspeech",
    "kalyan24_interspeech",
    "cho24_interspeech",
    "li24da_interspeech",
    "yu24b_interspeech"
   ]
  },
  {
   "title": "Speech Synthesis: Tools and Data",
   "papers": [
    "saito24_interspeech",
    "srinivasavaradhan24_interspeech",
    "ma24c_interspeech",
    "ma24d_interspeech",
    "yang24d_interspeech",
    "kawamura24_interspeech",
    "ogun24_interspeech",
    "take24_interspeech"
   ]
  },
  {
   "title": "Speech Synthesis: Singing Voice Synthesis",
   "papers": [
    "kim24i_interspeech",
    "okamoto24_interspeech",
    "kim24p_interspeech",
    "shi24_interspeech",
    "hwang24_interspeech",
    "gao24e_interspeech"
   ]
  },
  {
   "title": "LLM in ASR",
   "papers": [
    "baskar24_interspeech",
    "seide24_interspeech",
    "li24t_interspeech",
    "tang24_interspeech"
   ]
  },
  {
   "title": "Vision and Speech",
   "papers": [
    "kim24g_interspeech",
    "ghosh24b_interspeech",
    "li24v_interspeech",
    "chen24y_interspeech"
   ]
  },
  {
   "title": "Spoken Document Summarization",
   "papers": [
    "kroll24_interspeech",
    "ryu24_interspeech",
    "matsuura24_interspeech",
    "shang24_interspeech",
    "kang24d_interspeech",
    "leduc24_interspeech"
   ]
  },
  {
   "title": "Speech and Language in Health: from Remote Monitoring to Medical Conversations - 2 (Special Sessions)",
   "papers": [
    "escobargrisales24_interspeech",
    "maisonneuve24_interspeech",
    "botelho24_interspeech",
    "braun24_interspeech",
    "roesler24_interspeech",
    "barberis24_interspeech",
    "sanguedolce24_interspeech",
    "wang24e_interspeech",
    "kothare24_interspeech",
    "spiesberger24_interspeech",
    "dineley24_interspeech",
    "labrak24_interspeech",
    "jiang24b_interspeech"
   ]
  },
  {
   "title": "Show and Tell 2",
   "papers": [
    "v24_interspeech",
    "chen24z_interspeech",
    "anway24_interspeech",
    "kumar24c_interspeech",
    "koilakuntla24_interspeech",
    "schade24_interspeech",
    "possamaidemenezes24_interspeech",
    "ward24b_interspeech",
    "liu24s_interspeech",
    "chen24aa_interspeech"
   ]
  },
  {
   "title": "Prosody",
   "papers": [
    "hu24b_interspeech",
    "geng24_interspeech",
    "chakraborty24_interspeech",
    "riegger24_interspeech",
    "wu24n_interspeech",
    "mumtaz24_interspeech"
   ]
  },
  {
   "title": "Foundational Models for Deepfake and Spoofed Speech Detection",
   "papers": [
    "tran24_interspeech",
    "martindonas24_interspeech",
    "pan24c_interspeech",
    "wu24c_interspeech",
    "liu24b_interspeech",
    "doan24_interspeech"
   ]
  },
  {
   "title": "Speaker Recognition 1",
   "papers": [
    "peng24_interspeech",
    "yu24_interspeech",
    "liou24_interspeech",
    "kim24j_interspeech",
    "nam24b_interspeech",
    "mosner24_interspeech",
    "li24la_interspeech",
    "wang24ma_interspeech",
    "xie24b_interspeech",
    "maciejewski24_interspeech",
    "zhou24f_interspeech"
   ]
  },
  {
   "title": "Source Separation 1",
   "papers": [
    "wang24i_interspeech",
    "hartanto24_interspeech",
    "kealey24_interspeech",
    "chen24h_interspeech",
    "kim24m_interspeech",
    "yip24_interspeech"
   ]
  },
  {
   "title": "Audio-Visual and Generative Speech Enhancement",
   "papers": [
    "li24d_interspeech",
    "wang24m_interspeech",
    "lay24_interspeech",
    "jung24b_interspeech",
    "chen24g_interspeech",
    "li24m_interspeech",
    "hu24c_interspeech"
   ]
  },
  {
   "title": "Speech Privacy and Bandwidth Expansion",
   "papers": [
    "vali24_interspeech",
    "singh24_interspeech",
    "fan24_interspeech",
    "muller24_interspeech",
    "zhou24b_interspeech",
    "liu24g_interspeech",
    "moussa24_interspeech",
    "lin24c_interspeech",
    "lu24_interspeech",
    "li24ea_interspeech"
   ]
  },
  {
   "title": "Speech Synthesis: Prosody",
   "papers": [
    "korotkova24_interspeech",
    "mehta24b_interspeech",
    "eskimez24_interspeech",
    "maurya24_interspeech",
    "jiang24d_interspeech",
    "zhong24c_interspeech"
   ]
  },
  {
   "title": "Accented Speech, Prosodic Features, Dialect, Emotion, Sound Classification",
   "papers": [
    "prabhu24b_interspeech",
    "afonja24_interspeech",
    "javed24_interspeech",
    "kim24v_interspeech",
    "lin24m_interspeech",
    "hu24e_interspeech",
    "goel24_interspeech",
    "bukhari24_interspeech",
    "bentum24_interspeech",
    "nguyen24b_interspeech",
    "mogridge24_interspeech",
    "chen24s_interspeech"
   ]
  },
  {
   "title": "Neural Network Adaptation",
   "papers": [
    "rolland24b_interspeech",
    "huo24_interspeech",
    "shim24_interspeech",
    "kang24_interspeech",
    "li24ka_interspeech",
    "chen24n_interspeech"
   ]
  },
  {
   "title": "ASR and LLMs",
   "papers": [
    "yoon24_interspeech",
    "yang24f_interspeech",
    "choi24_interspeech",
    "ai24_interspeech",
    "rouditchenko24_interspeech",
    "prajwal24_interspeech"
   ]
  },
  {
   "title": "Pathological Speech Analysis 3",
   "papers": [
    "baumann24b_interspeech",
    "liu24f_interspeech",
    "lin24k_interspeech",
    "dang24b_interspeech",
    "hsu24_interspeech",
    "kalabakov24_interspeech",
    "neumann24_interspeech"
   ]
  },
  {
   "title": "Speech Disorders 3",
   "papers": [
    "gao24c_interspeech",
    "shah24_interspeech",
    "um24_interspeech",
    "chen24b_interspeech",
    "zheng24c_interspeech",
    "jiang24_interspeech",
    "leung24_interspeech",
    "gosztolya24b_interspeech"
   ]
  },
  {
   "title": "Speech Recognition with Large Pretrained Speech Models for Under-represented Languages (Special Session)",
   "papers": [
    "shih24_interspeech",
    "xu24d_interspeech",
    "li24ia_interspeech",
    "bhogale24_interspeech",
    "li24i_interspeech",
    "udupa24_interspeech",
    "xu24h_interspeech",
    "getman24b_interspeech",
    "qian24_interspeech"
   ]
  },
  {
   "title": "Speech Processing Using Discrete Speech Units (Special Session)",
   "papers": [
    "wu24q_interspeech",
    "mousavi24_interspeech",
    "chang24b_interspeech",
    "tang24c_interspeech",
    "shi24h_interspeech",
    "dhawan24_interspeech"
   ]
  },
  {
   "title": "Keynote 3",
   "papers": [
    "noeth24_interspeech"
   ]
  },
  {
   "title": "Databases and Progress in Methodology",
   "papers": [
    "ahn24b_interspeech",
    "nijat24_interspeech",
    "kumar24b_interspeech",
    "shi24c_interspeech",
    "zhong24b_interspeech",
    "zhao24e_interspeech"
   ]
  },
  {
   "title": "Articulation, Convergence and Perception",
   "papers": [
    "giroud24_interspeech",
    "shen24c_interspeech",
    "li24ga_interspeech",
    "loddo24_interspeech",
    "svenssonlundmark24_interspeech"
   ]
  },
  {
   "title": "Speech Emotion Recognition",
   "papers": [
    "amiriparian24_interspeech",
    "rittergutierrez24_interspeech",
    "mai24_interspeech",
    "niu24d_interspeech"
   ]
  },
  {
   "title": "Self-Supervised Models in Speaker Recognition",
   "papers": [
    "kim24c_interspeech",
    "miara24_interspeech",
    "lim24_interspeech",
    "fathan24_interspeech",
    "li24e_interspeech",
    "zhao24f_interspeech"
   ]
  },
  {
   "title": "Speech Quality Assessment",
   "papers": [
    "hu24d_interspeech",
    "udupa24b_interspeech",
    "wells24_interspeech",
    "ta24_interspeech"
   ]
  },
  {
   "title": "Privacy and Security in Speech Communication 1",
   "papers": [
    "muller24b_interspeech",
    "oiso24_interspeech",
    "looney24_interspeech",
    "chen24k_interspeech",
    "liu24i_interspeech",
    "yang24e_interspeech"
   ]
  },
  {
   "title": "Speech Synthesis: Voice Conversion 2",
   "papers": [
    "gusev24_interspeech",
    "um24b_interspeech",
    "ma24e_interspeech",
    "igarashi24_interspeech",
    "kanagawa24_interspeech",
    "xu24b_interspeech",
    "gengembre24_interspeech",
    "chen24e_interspeech"
   ]
  },
  {
   "title": "Speech Synthesis: Text Processing",
   "papers": [
    "roth24_interspeech",
    "dekel24_interspeech",
    "rezackova24_interspeech",
    "kurihara24_interspeech",
    "shirahata24_interspeech",
    "yang24_interspeech",
    "sun24_interspeech",
    "gupta24d_interspeech",
    "tannander24_interspeech"
   ]
  },
  {
   "title": "Training Methods, Self-Supervised Learning, Adaptation",
   "papers": [
    "fernandezlopez24_interspeech",
    "prasad24_interspeech",
    "kim24s_interspeech",
    "meghanani24_interspeech",
    "flynn24_interspeech",
    "vandereeckt24_interspeech",
    "shi24b_interspeech",
    "kusunoki24_interspeech",
    "hojo24_interspeech",
    "kim24t_interspeech",
    "gu24b_interspeech",
    "joseph24_interspeech",
    "lee24j_interspeech",
    "singh24c_interspeech",
    "lee24b_interspeech"
   ]
  },
  {
   "title": "Novel Architectures for ASR",
   "papers": [
    "honda24_interspeech",
    "kashiwagi24_interspeech",
    "shejwalkar24_interspeech",
    "kang24b_interspeech"
   ]
  },
  {
   "title": "Multimodality and Foundation Models",
   "papers": [
    "cui24_interspeech",
    "sayeed24_interspeech",
    "bujnowski24_interspeech",
    "jia24_interspeech",
    "li24ta_interspeech",
    "eungi24_interspeech"
   ]
  },
  {
   "title": "Spoken Dialogue Systems and Conversational Analysis 1",
   "papers": [
    "mcneill24_interspeech",
    "atkins24_interspeech",
    "qian24b_interspeech",
    "sahipjohn24_interspeech",
    "wang24t_interspeech",
    "shih24b_interspeech"
   ]
  },
  {
   "title": "Speech Technology",
   "papers": [
    "nilsson24_interspeech",
    "naderi24_interspeech",
    "meng24d_interspeech",
    "wu24l_interspeech",
    "sun24d_interspeech",
    "gudmalwar24_interspeech",
    "wu24g_interspeech",
    "svirsky24_interspeech"
   ]
  },
  {
   "title": "Pathological Speech Analysis 2",
   "papers": [
    "halpern24_interspeech",
    "maji24_interspeech",
    "talkar24_interspeech",
    "lin24l_interspeech",
    "woszczyk24_interspeech",
    "koudounas24_interspeech"
   ]
  },
  {
   "title": "Speech Science, Speech Technology, and Gender (Special Session)",
   "papers": [
    "schubert24_interspeech",
    "sigurgeirsson24_interspeech",
    "pelloin24_interspeech",
    "doukhan24_interspeech",
    "hughes24_interspeech",
    "szekely24_interspeech",
    "netzorg24_interspeech",
    "lai24_interspeech",
    "elie24b_interspeech",
    "krishnan24_interspeech"
   ]
  },
  {
   "title": "Speech Production and Perception",
   "papers": [
    "fan24c_interspeech",
    "sharma24_interspeech",
    "birkholz24_interspeech",
    "fang24_interspeech",
    "freixes24_interspeech",
    "friedrichs24_interspeech"
   ]
  },
  {
   "title": "Phonetics and Phonology: Segmentals and Suprasegmentals",
   "papers": [
    "miodonska24_interspeech",
    "hu24_interspeech",
    "watkins24_interspeech",
    "maselli24_interspeech",
    "erickson24_interspeech",
    "chan24_interspeech"
   ]
  },
  {
   "title": "Topics in Paralinguistics",
   "papers": [
    "bn24_interspeech",
    "wu24e_interspeech",
    "suda24_interspeech",
    "setoguchi24_interspeech",
    "hao24_interspeech",
    "liu24r_interspeech"
   ]
  },
  {
   "title": "Emotion Recognition: Fairness, Variability, Uncertainty",
   "papers": [
    "wu24_interspeech",
    "chou24_interspeech",
    "tavernor24_interspeech",
    "sun24e_interspeech",
    "chien24_interspeech",
    "schrufer24_interspeech",
    "garcia24_interspeech"
   ]
  },
  {
   "title": "Speaker Verification",
   "papers": [
    "stafylakis24_interspeech",
    "chien24c_interspeech",
    "benamor24_interspeech",
    "yakovlev24_interspeech",
    "jung24d_interspeech",
    "chen24l_interspeech"
   ]
  },
  {
   "title": "Spatial Audio and Acoustics",
   "papers": [
    "khokhlov24_interspeech",
    "kelley24_interspeech",
    "ahn24c_interspeech",
    "tao24_interspeech",
    "bayestehtashk24_interspeech",
    "yarga24_interspeech",
    "bitterman24_interspeech"
   ]
  },
  {
   "title": "Generative Models for Speech and Audio",
   "papers": [
    "bai24b_interspeech",
    "paissan24b_interspeech",
    "gupta24b_interspeech",
    "moschopoulos24_interspeech",
    "kim24n_interspeech",
    "choi24c_interspeech",
    "cappellazzo24_interspeech",
    "deshmukh24b_interspeech"
   ]
  },
  {
   "title": "Speech and Audio Modelling",
   "papers": [
    "gao24_interspeech",
    "guillaume24_interspeech",
    "wang24u_interspeech",
    "aluru24_interspeech"
   ]
  },
  {
   "title": "Multi-Channel Speech Enhancement",
   "papers": [
    "tammen24_interspeech",
    "xu24i_interspeech",
    "aziz24_interspeech",
    "lee24g_interspeech",
    "wang24_interspeech",
    "zhou24d_interspeech"
   ]
  },
  {
   "title": "Speech Synthesis: Paradigms and Methods 1",
   "papers": [
    "mcghee24_interspeech",
    "murata24_interspeech",
    "nishihara24_interspeech",
    "li24ba_interspeech",
    "dang24_interspeech",
    "kim24h_interspeech",
    "janiczek24_interspeech",
    "chien24b_interspeech",
    "wu24o_interspeech"
   ]
  },
  {
   "title": "Speech Synthesis: Paradigms and Methods 2",
   "papers": [
    "lemerle24_interspeech",
    "neekhara24_interspeech",
    "lai24b_interspeech",
    "liu24p_interspeech",
    "zhou24_interspeech",
    "lee24f_interspeech",
    "lenglet24_interspeech"
   ]
  },
  {
   "title": "Neural Network Architectures for ASR 1",
   "papers": [
    "yang24g_interspeech",
    "parcollet24_interspeech",
    "moriya24_interspeech",
    "kundu24_interspeech",
    "vankeirsbilck24_interspeech",
    "zhang24e_interspeech"
   ]
  },
  {
   "title": "Error Correction and Rescoring",
   "papers": [
    "mittal24_interspeech",
    "yoon24c_interspeech",
    "wang24j_interspeech",
    "shu24_interspeech",
    "kang24c_interspeech",
    "yang24b_interspeech"
   ]
  },
  {
   "title": "Spoken Language Understanding",
   "papers": [
    "yang24p_interspeech",
    "zheng24b_interspeech",
    "aimaiti24_interspeech",
    "christ24_interspeech",
    "akani24_interspeech",
    "anderson24_interspeech",
    "chang24_interspeech"
   ]
  },
  {
   "title": "Spoken Dialogue Systems and Conversational Analysis 2",
   "papers": [
    "chen24d_interspeech",
    "ng24b_interspeech",
    "uro24_interspeech",
    "watanabe24_interspeech",
    "park24b_interspeech",
    "hutin24_interspeech",
    "ohagi24_interspeech"
   ]
  },
  {
   "title": "Computational Models of Human Language Acquisition, Perception, and Production (Special Session)",
   "papers": [
    "heuser24b_interspeech",
    "hovsepyan24_interspeech",
    "ortiztandazo24_interspeech",
    "onda24_interspeech",
    "bonafos24_interspeech",
    "elie24_interspeech",
    "coffey24_interspeech",
    "vanniekerk24_interspeech",
    "mohamed24_interspeech"
   ]
  },
  {
   "title": "Show and Tell 3",
   "papers": [
    "ryumina24_interspeech",
    "mundra24_interspeech",
    "sharma24b_interspeech",
    "chaudhuri24_interspeech",
    "koshal24_interspeech",
    "akhtar24_interspeech",
    "phukan24c_interspeech",
    "jain24b_interspeech",
    "obukhov24_interspeech",
    "masson24_interspeech",
    "pages24_interspeech",
    "morrone24_interspeech"
   ]
  },
  {
   "title": "Phonetics, Phonology and Prosody",
   "papers": [
    "kinnunen24_interspeech",
    "deluca24_interspeech",
    "loiacono24_interspeech",
    "audibert24_interspeech",
    "liu24q_interspeech",
    "jones24_interspeech",
    "tulchynska24_interspeech"
   ]
  },
  {
   "title": "Segmentals",
   "papers": [
    "cronenberg24_interspeech",
    "vegarodriguez24_interspeech",
    "kye24_interspeech",
    "terhiija24_interspeech",
    "yang24n_interspeech"
   ]
  },
  {
   "title": "New Avenues in Emotion Recognition",
   "papers": [
    "wu24d_interspeech",
    "zhao24g_interspeech",
    "shi24i_interspeech",
    "triantafyllopoulos24c_interspeech",
    "leem24_interspeech",
    "lu24e_interspeech"
   ]
  },
  {
   "title": "Speaker Diarization 2",
   "papers": [
    "zhang24b_interspeech",
    "choi24d_interspeech",
    "wang24h_interspeech",
    "blatt24_interspeech",
    "plaquet24_interspeech",
    "baroudi24_interspeech"
   ]
  },
  {
   "title": "Speaker Recognition 2",
   "papers": [
    "loweimi24_interspeech",
    "jin24b_interspeech",
    "shen24_interspeech",
    "zhang24c_interspeech",
    "li24u_interspeech",
    "nguyen24_interspeech",
    "kc24_interspeech"
   ]
  },
  {
   "title": "Speech and Audio Analysis",
   "papers": [
    "almudevar24_interspeech",
    "koriyama24_interspeech",
    "karan24_interspeech",
    "dumpala24_interspeech",
    "mallela24_interspeech",
    "li24p_interspeech"
   ]
  },
  {
   "title": "Speech Quality and Intelligibility: Prediction and Enhancement",
   "papers": [
    "best24_interspeech",
    "zezario24_interspeech",
    "wang24y_interspeech",
    "deoliveira24_interspeech",
    "ta24b_interspeech",
    "chen24j_interspeech"
   ]
  },
  {
   "title": "Speech Synthesis: Vocoders",
   "papers": [
    "lv24_interspeech",
    "chaudhary24b_interspeech",
    "cho24b_interspeech",
    "shen24b_interspeech",
    "chen24x_interspeech",
    "du24_interspeech"
   ]
  },
  {
   "title": "ASR Model Training Methods",
   "papers": [
    "raissi24_interspeech",
    "gaur24_interspeech",
    "shakeel24_interspeech",
    "chiu24_interspeech",
    "kulshreshtha24_interspeech",
    "yeh24_interspeech"
   ]
  },
  {
   "title": "Cross-Lingual and Multilingual Processing",
   "papers": [
    "liu24l_interspeech",
    "song24_interspeech",
    "zanonboito24_interspeech",
    "lodagala24_interspeech",
    "jakhar24_interspeech",
    "dong24_interspeech",
    "tian24_interspeech",
    "puvvada24_interspeech",
    "paraskevopoulos24_interspeech",
    "vakirtzian24_interspeech",
    "liu24k_interspeech",
    "kummervold24_interspeech",
    "srivastava24_interspeech",
    "hussein24_interspeech",
    "ye24_interspeech"
   ]
  },
  {
   "title": "Speech Assessment",
   "papers": [
    "wu24i_interspeech",
    "cheng24b_interspeech",
    "gothi24_interspeech",
    "lun24_interspeech",
    "tomita24_interspeech",
    "robertson24_interspeech"
   ]
  },
  {
   "title": "Question Answering from Speech and Spoken Dialogue Systems",
   "papers": [
    "rajkhowa24_interspeech",
    "phukan24_interspeech",
    "nguyen24c_interspeech",
    "noroozi24_interspeech",
    "dibratto24_interspeech",
    "baihaqi24_interspeech",
    "zhou24c_interspeech"
   ]
  },
  {
   "title": "Spoken Dialogue Systems and Conversational Analysis 3",
   "papers": [
    "huang24b_interspeech",
    "shi24d_interspeech",
    "suzuki24_interspeech",
    "omahony24_interspeech",
    "shinoda24_interspeech",
    "hoscilowicz24_interspeech",
    "liu24c_interspeech"
   ]
  },
  {
   "title": "Dysarthric Speech Assessment",
   "papers": [
    "zaheera24_interspeech",
    "wan24b_interspeech",
    "samptur24_interspeech",
    "perez24_interspeech",
    "daoudi24_interspeech",
    "chen24t_interspeech"
   ]
  },
  {
   "title": "Spoken Language Models for Universal Speech Processing (Special Session)",
   "papers": [
    "li24qa_interspeech",
    "chang24c_interspeech",
    "kuan24_interspeech",
    "tang24d_interspeech",
    "shon24_interspeech",
    "lu24c_interspeech",
    "pan24b_interspeech",
    "messica24_interspeech",
    "shechtman24_interspeech"
   ]
  },
  {
   "title": "Keynote 4",
   "papers": [
    "tillmann24_interspeech"
   ]
  },
  {
   "title": "L1/L2 Acquisition and Cross-Linguistic Factors",
   "papers": [
    "hwang24b_interspeech",
    "li24fa_interspeech",
    "zaitova24_interspeech",
    "demaere24_interspeech",
    "polzehl24_interspeech",
    "li24n_interspeech",
    "truong24_interspeech"
   ]
  },
  {
   "title": "Speaker Stance, Emotion and Language-External Factors",
   "papers": [
    "hoffner24_interspeech",
    "wu24j_interspeech",
    "simantiraki24_interspeech",
    "hodoshima24_interspeech"
   ]
  },
  {
   "title": "Experimental Phonetics and Laboratory Phonology",
   "papers": [
    "zhao24i_interspeech",
    "kaland24b_interspeech",
    "tokac24_interspeech",
    "reitsema24_interspeech",
    "stein24_interspeech",
    "lee24l_interspeech"
   ]
  },
  {
   "title": "Speaker recognition evaluation and resources",
   "papers": [
    "lin24j_interspeech",
    "hutiri24_interspeech",
    "wang24fa_interspeech",
    "jung24c_interspeech",
    "huang24g_interspeech",
    "hoang24b_interspeech"
   ]
  },
  {
   "title": "Speech Type Classification",
   "papers": [
    "ma24_interspeech",
    "liu24h_interspeech",
    "nafea24_interspeech",
    "cheng24c_interspeech",
    "niu24b_interspeech",
    "chen24r_interspeech"
   ]
  },
  {
   "title": "Target Speaker Extraction",
   "papers": [
    "meng24b_interspeech",
    "pandey24_interspeech",
    "heo24_interspeech",
    "srinivas24_interspeech",
    "wu24h_interspeech",
    "liu24j_interspeech"
   ]
  },
  {
   "title": "Speech Synthesis: Voice Conversion 3",
   "papers": [
    "bai24_interspeech",
    "salman24_interspeech",
    "tanaka24_interspeech",
    "niu24_interspeech",
    "hai24_interspeech",
    "lee24d_interspeech",
    "siriwardena24_interspeech",
    "huang24e_interspeech",
    "kanagawa24b_interspeech"
   ]
  },
  {
   "title": "Speech Synthesis: Paradigms and Methods 3",
   "papers": [
    "yang24l_interspeech",
    "lovelace24_interspeech",
    "feng24d_interspeech",
    "kim24_interspeech",
    "sadekova24_interspeech",
    "yang24q_interspeech",
    "chen24q_interspeech",
    "song24b_interspeech"
   ]
  },
  {
   "title": "Privacy and Security in Speech Communication 2",
   "papers": [
    "ghosh24_interspeech",
    "wang24ha_interspeech",
    "meyer24_interspeech",
    "huang24_interspeech"
   ]
  },
  {
   "title": "Streaming ASR",
   "papers": [
    "yang24m_interspeech",
    "tsunoo24_interspeech",
    "chen24u_interspeech",
    "heitkaemper24_interspeech",
    "le24_interspeech",
    "wang24ea_interspeech"
   ]
  },
  {
   "title": "Computational Resource Constrained ASR",
   "papers": [
    "xiao24b_interspeech",
    "kim24k_interspeech",
    "gu24_interspeech",
    "li24o_interspeech",
    "rybakov24_interspeech",
    "lin24d_interspeech",
    "park24_interspeech",
    "wang24p_interspeech",
    "song24c_interspeech",
    "ling24_interspeech"
   ]
  },
  {
   "title": "Evaluation of Speech Technology Systems",
   "papers": [
    "heuser24_interspeech",
    "kuhn24_interspeech",
    "teleki24_interspeech",
    "lu24d_interspeech",
    "sasindran24_interspeech"
   ]
  },
  {
   "title": "Neural Network Training for Speech Recognition",
   "papers": [
    "xu24_interspeech",
    "hou24_interspeech",
    "huang24c_interspeech",
    "choi24b_interspeech",
    "han24_interspeech",
    "kim24d_interspeech"
   ]
  },
  {
   "title": "Leveraging Large Language Models and Contextual Features for Phonetic Analysis (Special Session)",
   "papers": [
    "deheerkloots24_interspeech",
    "lin24e_interspeech",
    "hao24c_interspeech",
    "shams24_interspeech",
    "english24_interspeech"
   ]
  },
  {
   "title": "Responsible Speech Foundation Models (Special Session)",
   "papers": [
    "wiepert24_interspeech",
    "wagner24_interspeech",
    "kulkarni24_interspeech",
    "lin24i_interspeech",
    "lin24b_interspeech",
    "chang24d_interspeech",
    "aldeneh24_interspeech",
    "meng24c_interspeech"
   ]
  },
  {
   "title": "Multimodal Paralinguistics",
   "papers": [
    "cai24b_interspeech",
    "li24z_interspeech",
    "zhu24_interspeech",
    "wang24r_interspeech",
    "chochlakis24_interspeech",
    "kyung24_interspeech",
    "goncalves24_interspeech"
   ]
  },
  {
   "title": "Automatic Emotion Recognition",
   "papers": [
    "upadhyay24_interspeech",
    "phukan24b_interspeech",
    "sun24b_interspeech",
    "khaertdinov24_interspeech"
   ]
  },
  {
   "title": "Self and Weakly-Labelled Speaker Verification",
   "papers": [
    "wang24z_interspeech",
    "li24q_interspeech",
    "jin24c_interspeech",
    "selvakumar24_interspeech"
   ]
  },
  {
   "title": "Acoustic Event Detection, Segmentation and Classification",
   "papers": [
    "behera24_interspeech",
    "xiao24_interspeech",
    "feng24c_interspeech",
    "omine24_interspeech",
    "lebourdais24_interspeech",
    "elbanna24_interspeech",
    "morozova24_interspeech",
    "buddi24_interspeech",
    "wang24ca_interspeech",
    "palaskar24_interspeech",
    "zang24_interspeech",
    "si24_interspeech",
    "a24_interspeech"
   ]
  },
  {
   "title": "Speech and Audio Modelling",
   "papers": [
    "li24ha_interspeech",
    "wang24d_interspeech",
    "xu24f_interspeech",
    "guan24b_interspeech",
    "cumlin24_interspeech",
    "boukun24_interspeech"
   ]
  },
  {
   "title": "Fake Audio Detection",
   "papers": [
    "pascu24_interspeech",
    "xie24_interspeech",
    "zhong24_interspeech",
    "chen24o_interspeech",
    "wang24ga_interspeech",
    "kim24b_interspeech"
   ]
  },
  {
   "title": "Deep Learning-Based Speech Enhancement: Approaches, Scalability, and Evaluation",
   "papers": [
    "cao24_interspeech",
    "parnamaa24_interspeech",
    "zhang24h_interspeech",
    "richter24_interspeech"
   ]
  },
  {
   "title": "Speech Synthesis: Other Topics 1",
   "papers": [
    "tan24c_interspeech",
    "kim24e_interspeech",
    "li24y_interspeech",
    "abel24_interspeech",
    "shi24f_interspeech",
    "lee24m_interspeech",
    "guo24c_interspeech",
    "kunesova24_interspeech",
    "ward24_interspeech"
   ]
  },
  {
   "title": "Speech Synthesis: Other Topics 2",
   "papers": [
    "ratsep24_interspeech",
    "yang24c_interspeech",
    "seong24_interspeech",
    "yang24i_interspeech",
    "saeki24_interspeech",
    "yoon24b_interspeech",
    "lee24_interspeech"
   ]
  },
  {
   "title": "Speech synthesis: Cross-lingual and multilingual aspects",
   "papers": [
    "lux24_interspeech",
    "gong24c_interspeech",
    "wu24f_interspeech",
    "xu24g_interspeech",
    "casanova24_interspeech",
    "guo24b_interspeech"
   ]
  },
  {
   "title": "Noise, Far-Field, Multi-Talker, Enhancement, Audio Classification",
   "papers": [
    "shao24_interspeech",
    "shao24b_interspeech",
    "ravenscroft24_interspeech",
    "vinnikov24_interspeech",
    "dissen24_interspeech",
    "haider24_interspeech",
    "wang24da_interspeech",
    "singh24b_interspeech",
    "sheikh24_interspeech",
    "sato24_interspeech",
    "borsdorf24_interspeech"
   ]
  },
  {
   "title": "Self-Supervised Learning for ASR",
   "papers": [
    "getman24_interspeech",
    "kato24_interspeech",
    "yadav24b_interspeech",
    "lee24k_interspeech"
   ]
  },
  {
   "title": "Spoken Term Detection and Speech Retrieval",
   "papers": [
    "yuan24b_interspeech",
    "yusuf24b_interspeech",
    "he24_interspeech",
    "xie24c_interspeech",
    "kim24r_interspeech",
    "tapo24_interspeech"
   ]
  },
  {
   "title": "Speech Disorders 1",
   "papers": [
    "mohapatra24_interspeech",
    "gong24_interspeech",
    "zulfikar24_interspeech",
    "zhang24f_interspeech",
    "nie24_interspeech",
    "wagner24b_interspeech"
   ]
  },
  {
   "title": "Connecting Speech-science and Speech-technology for Children’s Speech (Special Session)",
   "papers": [
    "demopoulos24_interspeech",
    "charuau24_interspeech",
    "kadambi24_interspeech",
    "benway24_interspeech",
    "sukhadia24_interspeech",
    "wang24f_interspeech",
    "gao24d_interspeech",
    "baumann24_interspeech",
    "li24j_interspeech",
    "blockmedin24_interspeech",
    "fan24b_interspeech",
    "rolland24_interspeech",
    "zhang24d_interspeech",
    "graave24_interspeech",
    "xu24c_interspeech"
   ]
  },
  {
   "title": "Show and Tell 4",
   "papers": [
    "sirigiraju24_interspeech",
    "yap24_interspeech",
    "aiba24_interspeech",
    "sridaran24_interspeech",
    "masudakatsuse24_interspeech",
    "preston24_interspeech",
    "yoder24_interspeech",
    "phan24b_interspeech"
   ]
  }
 ],
 "doi": "10.21437/Interspeech.2024"
}
