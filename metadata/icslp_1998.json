{
 "title": "5th International Conference on Spoken Language Processing (ICSLP 1998)",
 "location": "Sydney, Australia",
 "startDate": "30/11/1998",
 "endDate": "4/12/1998",
 "chair": "General Chair: Bruce Millar",
 "conf": "ICSLP",
 "year": "1998",
 "name": "icslp_1998",
 "series": "ICSLP",
 "SIG": "",
 "title1": "5th International Conference on Spoken Language Processing",
 "title2": "(ICSLP 1998)",
 "date": "30 November - 4 December 1998",
 "booklet": "icslp_1998.pdf",
 "papers": {
  "clark98_icslp": {
   "authors": [
    [
     "Graeme M.",
     "Clark"
    ]
   ],
   "title": "Cochlear implants in the second and third millennia",
   "original": "i98_1155",
   "page_count": 6,
   "order": 1,
   "p1": "paper 1155",
   "pn": "",
   "abstract": [
    "Much has been achieved in the Second Millennium in the development of cochlear implants for profoundly deaf people, but further advances in the Third Millennium should result in most severely to profoundly deaf people being able to communicate effectively in a hearing community.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-1"
  },
  "seneff98_icslp": {
   "authors": [
    [
     "Stephanie",
     "Seneff"
    ]
   ],
   "title": "The use of linguistic hierarchies in speech understanding",
   "original": "i98_0012",
   "page_count": 10,
   "order": 2,
   "p1": "paper 0012",
   "pn": "",
   "abstract": [
    "This paper describes two related systems which provide frameworks for encoding linguistic knowledge into formal rules within the context of a trainable probabilistic model. The first system, TINA, drives top-down from sentence level structure, terminating in either words or syllables. It's main purpose is to provide a meaning representation for the sentence. The other system, ANGIE, operates bottom-up from phonetic or orthographic units, characterizing word substructure. It provides a framework for both phonological rule modelling and letter-to-sound/sound-to-letter transformations. The two systems logically converge on the syllable or word layer. We have recently been successful in integrating their combined constraint into a recognizer search, achieving considerable improvement in understanding accuracy. In this paper, I will look both toward the past and the future, identifying and motivating the decisions that were made in the design of TINA and ANGIE and the associated rule formalisms, and contemplating various remaining open research issues.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-2"
  },
  "bagshaw98_icslp": {
   "authors": [
    [
     "Paul C.",
     "Bagshaw"
    ]
   ],
   "title": "Unsupervised training of phone duration and energy models for text-to-speech synthesis",
   "original": "i98_0132",
   "page_count": 4,
   "order": 3,
   "p1": "paper 0132",
   "pn": "",
   "abstract": [
    "A new model of phone duration and energy is presented. These parameters are modelled in two stages. The first stage builds a statistics tree that contains phone duration and energy mean and standard deviation values at each node. The branches of the tree are characterised by a set of factors related to phonetic context. The second stage considers phone duration and energy to be modified by two syllable-level prosodic coefficients. The duration and energy of the phones of a syllable are influenced to differing degrees by these coefficients. Weights are associated with the different phone positions in a syllable. A simulated annealing technique is used to find the set of weights that allow the prosodic coefficients to be calculated for all syllables and, in turn, minimise the error in predicting the phone duration and energy during synthesis. They are predicted with a mean squared error of 15.4ms and 6.8dB respectively. During synthesis, the syllable-level prosodic coefficients are predicted by regression trees from linguistic information. Manual prosodic labelling is not required at any stage.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-3"
  },
  "bellegarda98_icslp": {
   "authors": [
    [
     "Jerome R.",
     "Bellegarda"
    ],
    [
     "Kim E. A.",
     "Silverman"
    ]
   ],
   "title": "Improved duration modeling of English phonemes using a root sinusoidal transformation",
   "original": "i98_0135",
   "page_count": 4,
   "order": 4,
   "p1": "paper 0135",
   "pn": "",
   "abstract": [
    "Accurate duration modeling is necessary for synthetic speech to sound natural. Over the past few years, the sums-of-products framework has emerged as an effective way to account for contextual influences on phoneme duration. This approach is generally applied after log-transforming the durations. This paper presents empirical and theoretical evidence which suggests that this transformation is not optimal. A promising alternative solution is proposed, based on a root sinusoidal function. Preliminary experimental results were obtained on over 50,000 phonemes in varied prosodic contexts. Compared to the log transformation, this new transformation reduced the proportion of standard deviation unexplained by approximately 30%. Alternatively, for a given level of performance, the root sinusoidal transformation roughly halved the number of regression parameters required.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-4"
  },
  "shih98_icslp": {
   "authors": [
    [
     "Chilin",
     "Shih"
    ],
    [
     "Wentao",
     "Gu"
    ],
    [
     "Jan P. H. van",
     "Santen"
    ]
   ],
   "title": "Efficient adaptation of TTS duration model to new speakers",
   "original": "i98_0177",
   "page_count": 4,
   "order": 5,
   "p1": "paper 0177",
   "pn": "",
   "abstract": [
    "This paper discusses a methodology using a minimal set of sentences to adapt an existing TTS duration model to capture inter-speaker variations. The assumption is that the original duration database contains information of both language-specific and speaker-specific duration characteristics. In training a duration model for a new speaker, only the speaker-specific information needs to be modeled, therefore the size of the training data can be reduced drastically. Results from several experiments are compared and discussed.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-5"
  },
  "yoshimura98_icslp": {
   "authors": [
    [
     "Takayoshi",
     "Yoshimura"
    ],
    [
     "Keiichi",
     "Tokuda"
    ],
    [
     "Takashi",
     "Masuko"
    ],
    [
     "Takao",
     "Kobayashi"
    ],
    [
     "Tadashi",
     "Kitamura"
    ]
   ],
   "title": "Duration modeling for HMM-based speech synthesis",
   "original": "i98_0939",
   "page_count": 4,
   "order": 6,
   "p1": "paper 0939",
   "pn": "",
   "abstract": [
    "This paper proposes a new approach to state duration modeling for HMM-based speech synthesis. A set of state durations of each phoneme HMM is modeled by a multi-dimensional Gaussian distribution, and duration models are clustered using a decision tree based context clustering technique. In the synthesis stage, state durations are determined by using the state duration models. In this paper, we take account of contextual factors such as stress-related factors and locational factors in addition to phone identity factors. Experimental results show that we can synthesize good quality speech with natural timing, and the speaking rate can be varied easily.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-6"
  },
  "fordyce98_icslp": {
   "authors": [
    [
     "Cameron S.",
     "Fordyce"
    ],
    [
     "Mari",
     "Ostendorf"
    ]
   ],
   "title": "Prosody prediction for speech synthesis using transformational rule-based learning",
   "original": "i98_0682",
   "page_count": 4,
   "order": 7,
   "p1": "paper 0682",
   "pn": "",
   "abstract": [
    "Speech generation systems can benefit from the prediction of abstract prosodic labels from text input. Earlier methods of prosodic label prediction have relied on hand-written rules or on statistical methods such as decision trees. Statistical methods have the advantage of being automatically trainable and are portable to new domains. This research presents a new method for automatically training an abstract prosodic label predictor, transformational rule-based learning. This method is automatically trainable. Results will be presented for pitch accent location and phrase boundary prediction.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-7"
  },
  "fitt98_icslp": {
   "authors": [
    [
     "Susan",
     "Fitt"
    ],
    [
     "Stephen",
     "Isard"
    ]
   ],
   "title": "Representing the environments for phonological processes in an accent-independent lexicon for synthesis of English",
   "original": "i98_0850",
   "page_count": 4,
   "order": 8,
   "p1": "paper 0850",
   "pn": "",
   "abstract": [
    "This paper reports on work developing an accent-independent lexicon for use in synthesising speech in English. Developing a lexicon for a new accent is a long process, and one potential solution to this problem involves the encoding of regional variation by means of keywords; so, rather than transcribing different phonemes for 'pool' in RP and in Scottish accents, we can simply say that the word contains the same vowel as in the keyword GOOSE. However, there are a number of theoretical and practical issues, which are discussed here. It is proposed that phonemic variation within accents be encoded in the lexicon by use of keyword symbols, while allophonic differences be derived by accent-specific rules. If we wish to include some stylistic variation this makes the lexicon more comprehensive but more complex. Finally, it is noted that even in keyword synthesis exception lists cannot be avoided.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-8"
  },
  "faulkner98_icslp": {
   "authors": [
    [
     "Daniel",
     "Faulkner"
    ],
    [
     "Charles",
     "Bryant"
    ]
   ],
   "title": "Efficient lexical retrieval for English text-to-speech synthesis",
   "original": "i98_0091",
   "page_count": 4,
   "order": 9,
   "p1": "paper 0091",
   "pn": "",
   "abstract": [
    "We present a first version of a filter dictionary for use in a computer-telephony text-to-speech synthesis system. The aim of the filter dictionary was to provide a lexicon that was compact, fast and had broader coverage than the standard dictionary used to create it. Correct phonemic transcriptions and lexical stress assignment were both required for a transcription to be deemed accurate. The approach taken here guarantees 100% accurate coverage of the original dictionary, but also gives 93% accurate transcription of the expected coverage of novel words. Lexical stress and the phonemic transcription were retrieved in one pass, resulting in an extremely fast system. We also allowed user-definition to retain accuracy for non-standard transcriptions. This algorithm was developed for British English, but could be applied to other languages.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-9"
  },
  "donovan98_icslp": {
   "authors": [
    [
     "Robert E.",
     "Donovan"
    ],
    [
     "Ellen M.",
     "Eide"
    ]
   ],
   "title": "The IBM trainable speech synthesis system",
   "original": "i98_0166",
   "page_count": 4,
   "order": 10,
   "p1": "paper 0166",
   "pn": "",
   "abstract": [
    "The speech synthesis system described in this paper uses a set of speaker-dependent decision-tree state-clustered hidden Markov models to automatically generate a leaf level segmentation of a large single-speaker continuous-read-speech database. During synthesis, the phone sequence to be synthesised is converted to an acoustic leaf sequence by descending the HMM decision trees. Duration, energy and pitch values are predicted using separate trainable models. To determine the segment sequence to concatenate, a dynamic programming (d.p.) search is performed over all the waveform segments aligned to each leaf in training. The d.p. attempts to ensure that the selected segments join each other spectrally, and have durations, energies and pitches such that the amount of degradation introduced by the subsequent use of TD-PSOLA is minimised; the selected segments are concatenated and modified to have the required prosodic values using TD-PSOLA. The d.p. results in the system effectively selecting variable length units.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-10"
  },
  "hawkins98_icslp": {
   "authors": [
    [
     "Sarah",
     "Hawkins"
    ],
    [
     "Jill",
     "House"
    ],
    [
     "Mark",
     "Huckvale"
    ],
    [
     "John",
     "Local"
    ],
    [
     "Richard",
     "Ogden"
    ]
   ],
   "title": "Prosynth: an integrated prosodic approach to device-independent, natural-sounding speech synthesis",
   "original": "i98_0538",
   "page_count": 4,
   "order": 11,
   "p1": "paper 0538",
   "pn": "",
   "abstract": [
    "This paper outlines ProSynth, an approach to speech synthesis which takes a rich linguistic structure as central to the generation of natural-sounding speech. We start from the assumption that the speech signal is informationally rich, and that this acoustic richness reflects linguistic structural richness and underlies the percept of naturalness. Naturalness achieved by structural richness produces a perceptually robust signal intelligible in adverse listening conditions. ProSynth uses syntactic and phonological parses to model the fine acoustic-phonetic detail of real speech, segmentally, temporally and intonationally.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-11"
  },
  "zhang98_icslp": {
   "authors": [
    [
     "Jialu",
     "Zhang"
    ],
    [
     "Shiwei",
     "Dong"
    ],
    [
     "Ge",
     "Yu"
    ]
   ],
   "title": "Total quality evaluation of speech synthesis systems",
   "original": "i98_0060",
   "page_count": 4,
   "order": 12,
   "p1": "paper 0060",
   "pn": "",
   "abstract": [
    "Based on the performance assessment of speech synthesis systems for Chinese the total quality evaluation of them has been carried out regular since 1994. The total quality evaluation includes speech intelligibility test at different levels (syllable, word and sentence), speech naturalness test and anti-interference ability test for phonetic module and text processing ability test for linguistic module. The designing principle of testing materials and testing methods are briefly described and the test results of four text-to-speech (TTS) systems for Chinese are presented in this paper. It is shown that 1. All professional technicians joining the speech naturalness test with the testing crew together overestimated the overall quality of the four tested systems; 2. The word intelligibility and Semantically Unpredicted Sentence (SUS) score are good for evaluating speech synthesis systems; 3. The anti-interference ability of synthetic speech is rather weak about 20 per cent in syllable intelligibility lower than natural speech under condition of S/N=5dB.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-12"
  },
  "sonntag98_icslp": {
   "authors": [
    [
     "Gerit P.",
     "Sonntag"
    ],
    [
     "Thomas",
     "Portele"
    ]
   ],
   "title": "Comparative evaluation of synthetic prosody with the PURR method",
   "original": "i98_0018",
   "page_count": 4,
   "order": 13,
   "p1": "paper 0018",
   "pn": "",
   "abstract": [
    "In order to evaluate the prosodic output of a speech synthesis system independently from its segmental quality, we have developed a special way to delexicalize speech stimuli which we call PURR (Prosody Unveiling through Restricted Representation). We compared the use of PURR stimuli for the evaluation of prosodic naturalness in three different test designs: magnitude estimation (ME), categorical estimation (CE), and ranking order (RO). Sentences of different types were synthesized by six German synthesis systems. The synthetic utterances and one human voice were comparatively judged by experienced listeners. On the whole the results of all three methods are in good agreement. Choice of stimuli seems to be more important than the choice of method.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-13"
  },
  "sproat98_icslp": {
   "authors": [
    [
     "Richard",
     "Sproat"
    ],
    [
     "Andrew",
     "Hunt"
    ],
    [
     "Mari",
     "Ostendorf"
    ],
    [
     "Paul",
     "Taylor"
    ],
    [
     "Alan W.",
     "Black"
    ],
    [
     "Kevin",
     "Lenzo"
    ],
    [
     "Mike",
     "Eddington"
    ]
   ],
   "title": "SABLE: a standard for TTS markup",
   "original": "i98_0040",
   "page_count": 4,
   "order": 14,
   "p1": "paper 0040",
   "pn": "",
   "abstract": [
    "Currently, speech synthesizers are controlled by a multitude of proprietary tag sets. These tag sets vary substantially across synthesizers and are an inhibitor to the adoption of speech synthesis technology by developers. SABLE is an XML/SGML-based markup scheme for text-to-speech synthesis, developed to address the need for a common TTS control paradigm. This paper presents an overview of the SABLE v0.2 specification, and provides links to websites with further information on SABLE.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-14"
  },
  "bunnell98_icslp": {
   "authors": [
    [
     "H. Timothy",
     "Bunnell"
    ],
    [
     "Steve R.",
     "Hoskins"
    ],
    [
     "Debra",
     "Yarrington"
    ]
   ],
   "title": "Prosodic vs. segmental contributions to naturalness in a diphone synthesizer",
   "original": "i98_0857",
   "page_count": 4,
   "order": 15,
   "p1": "paper 0857",
   "pn": "",
   "abstract": [
    "The relative contributions of segmental versus prosodic factors to the perceived naturalness of synthetic speech was measured by transplanting prosody between natural speech and the output of a diphone synthesizer. A small corpus was created containing matched sentence pairs wherein one member of the pair was a natural utterance and the other was a synthetic utterance generated with diphone data from the same talker. Two additional sentences were formed from each sentence pair by transplanting the prosodic structure between the natural and synthetic members of each pair. In two listening experiments subjects were asked to (a) classify each sentence as \"natural\" or \"synthetic, or (b) rate the naturalness of each sentence. Results showed that the prosodic information was more important than segmental information in both classification and ratings of naturalness.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-15"
  },
  "acero98_icslp": {
   "authors": [
    [
     "Alex",
     "Acero"
    ]
   ],
   "title": "A mixed-excitation frequency domain model for time-scale pitch-scale modification of speech",
   "original": "i98_0072",
   "page_count": 4,
   "order": 16,
   "p1": "paper 0072",
   "pn": "",
   "abstract": [
    "This paper presents a time-scale pitch-scale modification technique for concatenative speech synthesis. The method is based on a frequency domain source-filter model, where the source is modeled as a mixed excitation. This model is highly coupled with a compression scheme that result in compact acoustic inventories. When compared to the approach in the Whistler system using no mixed excitation, the new method shows improvement in voiced fricatives and over-stretched voiced sounds. In addition, it allows for spectral manipulation such as smoothing of discontinuities at unit boundaries, voice transformations or loudness equalization.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-16"
  },
  "akamine98_icslp": {
   "authors": [
    [
     "Masami",
     "Akamine"
    ],
    [
     "Takehiko",
     "Kagoshima"
    ]
   ],
   "title": "Analytic generation of synthesis units by closed loop training for totally speaker driven text to speech system (TOS drive TTS)",
   "original": "i98_0139",
   "page_count": 4,
   "order": 17,
   "p1": "paper 0139",
   "pn": "",
   "abstract": [
    "This paper provides a new method for automatically generating speech synthesis units. The algorithm, called Closed-Loop Training (CLT), is based on evaluating and reducing the distortion in synthesized speech. It minimizes distortion caused by synthesis process such as prosodic modification in an analytic way. The distortion is measured by calculating the error between synthesized speech units and natural speech units in a large speech database (corpus). The CLT method effectively generates the synthesis units that are most resembling of natural speech after synthesis process. In this paper, CLT is applied to a waveform concatenation based synthesizer, whose basic unit is a diphone. By using CLT, the synthesizer generates clear and smooth synthetic speech even with a relatively small volume of synthesis units.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-17"
  },
  "vainio98_icslp": {
   "authors": [
    [
     "Martti",
     "Vainio"
    ],
    [
     "Toomas",
     "Altosaar"
    ]
   ],
   "title": "Modeling the microprosody of pitch and loudness for speech synthesis with neural networks",
   "original": "i98_0886",
   "page_count": 4,
   "order": 18,
   "p1": "paper 0886",
   "pn": "",
   "abstract": [
    "In this study of Finnish microprosody, two prosodic parameters --- pitch and loudness --- were modeled with artificial neural networks. The networks are of the general feed forward type trained with backpropagation. For each phoneme, the network predicts a series of either pitch or loudness values on the basis of information of the phoneme's phonologically motivated features and its phonetic environment. The tests we have run so far indicate that the neural networks are highly successful and accurate in modeling the micro-level behavior of both pitch and loudness. The tests were conducted on isolated word material but some preliminary results obtained from sentence material are also discussed.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-18"
  },
  "chappell98_icslp": {
   "authors": [
    [
     "David T.",
     "Chappell"
    ],
    [
     "John H. L.",
     "Hansen"
    ]
   ],
   "title": "Spectral smoothing for concatenative speech synthesis",
   "original": "i98_0849",
   "page_count": 4,
   "order": 19,
   "p1": "paper 0849",
   "pn": "",
   "abstract": [
    "This paper addresses the topic of performing effective concatenative speech synthesis with a limited database by proposing methods to smooth the transitions between speech segments. The objective is to produce natural-sounding speech via segment concatenation when formants and other spectral features do not align properly. We propose several methods for adjusting the spectra between waveform segments selected for concatenation. Techniques examined include optimal coupling, waveform interpolation, linear predictive pole shifting, and psychoacoustic closure. Several of these algorithms have been previously developed for either coding or synthesis, but our application of closure for segment processing is novel. After spectral smoothing, the final synthesized speech can better approximate the desired speech characteristics and is continuous in both the time domain and spectral structure.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-19"
  },
  "chen98_icslp": {
   "authors": [
    [
     "Aimin",
     "Chen"
    ],
    [
     "Saeed",
     "Vaseghi"
    ],
    [
     "Charles",
     "Ho"
    ]
   ],
   "title": "MIMIC : a voice-adaptive phonetic-tree speech synthesiser",
   "original": "i98_0204",
   "page_count": 4,
   "order": 20,
   "p1": "paper 0204",
   "pn": "",
   "abstract": [
    "This paper presents Mimic : a decision-tree based concatenative voice adaptive text to speech synthesiser. Mimic integrates text to speech synthesis (TTS) with speech recognition and speaker adaptation. Speech is synthesised from concatenation of triphone synthesis units. The triphone units are obtained from clusters of training examples modelled, labelled and segmented using clustered HMMs and Viterbi segmentation. The prosodic structure of pitch, duration and energy contours are captured using statistical training methods. The concept of a decision-tree based statistical micro-prosody model is introduced as a hierarchical method of modelling prosodic parameters. The voice adaptation component involves the adaptation of the spectral parameters as well as pitch, duration, and energy.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-20"
  },
  "jeon98_icslp": {
   "authors": [
    [
     "Jehun",
     "Jeon"
    ],
    [
     "Sunhwa",
     "Cha"
    ],
    [
     "Minhwa",
     "Chung"
    ],
    [
     "Jun",
     "Park"
    ],
    [
     "Kyuwoong",
     "Hwang"
    ]
   ],
   "title": "Automatic generation of Korean pronunciation variants by multistage applications of phonological rules",
   "original": "i98_0675",
   "page_count": 4,
   "order": 21,
   "p1": "paper 0675",
   "pn": "",
   "abstract": [
    "Phonetic transcriptions are often manually encoded in a pronunciation lexicon. This process is time consuming and requires linguistic expertise. Moreover, it is very difficult to maintain consistency. To handle these problems, we present a model that produces Korean pronunciation variants based on morphophonological analysis. By analyzing phonological variations frequently found in spoken Korean, we have derived about 800 phonemic contexts that would trigger the applications of the corresponding phonemic and allophonic rules. In generating pronunciation variants, morphological analysis is preceded to handle variations of phonological words. According to the morphological category, a set of finite state automata tables reflecting phonemic context is looked up to generate pronunciation variants. Our experiments show that the proposed model produces mostly correct pronunciation variants of phonological words consisting of several morphemes.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-21"
  },
  "cox98_icslp": {
   "authors": [
    [
     "Stephen",
     "Cox"
    ],
    [
     "Richard",
     "Brady"
    ],
    [
     "Peter",
     "Jackson"
    ]
   ],
   "title": "Techniques for accurate automatic annotation of speech waveforms",
   "original": "i98_0466",
   "page_count": 4,
   "order": 22,
   "p1": "paper 0466",
   "pn": "",
   "abstract": [
    "We describe techniques used in the development of a high-accuracy automatic annotation system designed to provide new voices for a concatenative speech synthesiser. We have used standard HMM-based \"forced alignment\" techniques and have concentrated on refining both acoustic and pronunciation modelling to achieve greater alignment accuracy. Acoustic models were improved by Bayesian speaker adaptation and the use of confidence measures from N-Best decodings to produce speaker dependent HMM's. Pronunciation modelling improvements involved the use of a large pronunciation dictionary containing multiple pronunciations for many words, use of pronunciation probabilities, accommodation of interword silences and using information derived from existing manual annotations to guide the recogniser during decoding. The system produces time-aligned phonetic alignments for UK accents in which the automatic and manual alignments agree on the segmental labelling 93% of the time and in which the boundaries have an r.m.s. error of 14.5 ms from the manual boundary.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-22"
  },
  "cronk98_icslp": {
   "authors": [
    [
     "Andrew",
     "Cronk"
    ],
    [
     "Michael W.",
     "Macon"
    ]
   ],
   "title": "Optimized stopping criteria for tree-based unit selection in concatenative synthesis",
   "original": "i98_0680",
   "page_count": 4,
   "order": 23,
   "p1": "paper 0680",
   "pn": "",
   "abstract": [
    "The lack of naturalness hampers the widespread application of speech synthesis. Increasing the size of the unit database in a concatenative speech synthesizer has been proposed as a method to increase the variety of units-thereby improving naturalness. However, expanding the unit database increases the computational cost of selecting the most appropriate unit and compounds the risk that a perceptually suboptimal unit is chosen. Clustering the unit database prior to synthesis is an effective method for reducing this cost and risk. In this study, a unit selection method based on tree-structured clustering of data is implemented and evaluated. This approach to tree construction differs from similar approaches used in both synthesis and recognition in that a \"right-sized\" tree is found automatically rather than using hand-tuned stopping criteria. The tree is grown to its maximum size, and its leaves are systematically recombined in order to determine the most suitable subtree. Trees are grown using the automatic stopping method and compared with those grown using thresholds. Cross validation shows that trees grown to their maximum size and systematically recombined produce fuller clusters with lower objective distortion measures than trees whose growth is arrested by a threshold. The study concludes with a discussion of how these results may affect the perceptual quality of a speech synthesizer.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-23"
  },
  "tournemire98_icslp": {
   "authors": [
    [
     "Stephanie de",
     "Tournemire"
    ]
   ],
   "title": "Automatic transcription of intonation using an identified prosodic alphabet",
   "original": "i98_1035",
   "page_count": 4,
   "order": 24,
   "p1": "paper 1035",
   "pn": "",
   "abstract": [
    "A solution is proposed for rapidly adapting prosodic models to a new voice or a new application. First, a prosodic alphabet that is supported by linguistic knowledge is identified at the acoustic level. The observation of the realisation of prosodic events on the acoustic corpus allows classes of breaks, F0 shapes and accents to be constructed and automatic transcription rules to be written. Then the transcribed corpus is used in the estimation of the parameters of a prosodic model for French. The good F0 contours and duration generated with the prosodic model verify the agreement of the identified alphabets to describe prosodic phenomena. Finally, the prosodic model is integrated in the CNET standard French Text-to-Speech Synthesis system. The quality of the generated prosody is considered by naive listeners as equivalent to the handcrafted system. This result verifies the appropriateness of the alphabet as prosodic descriptors.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-24"
  },
  "esquerra98_icslp": {
   "authors": [
    [
     "Ignasi",
     "Esquerra"
    ],
    [
     "Albert",
     "Febrer"
    ],
    [
     "Climent",
     "Nadeu"
    ]
   ],
   "title": "Frequency analysis of phonetic units for concatenative synthesis in catalan",
   "original": "i98_0817",
   "page_count": 4,
   "order": 25,
   "p1": "paper 0817",
   "pn": "",
   "abstract": [
    "Knowledge of phonetic unit frequency is very necessary for developing databases in both concatenative synthesis and continuous speech recognition. In the present work, a large corpus of text was processed and phonetically transcribed to obtain allophone and diphone frequencies for the Catalan language. The corpus was acquired from newspaper articles, in which there were a lot of foreign words that represented a problem in the normalisation process. After automatic transcription, units were counted to get their relative frequency and results were compared to other analysis. Finally, diphones found in the corpus were compared to units of a synthesis database to validate both the normalisation and transcription modules and the synthesis unit database.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-25"
  },
  "fang98_icslp": {
   "authors": [
    [
     "Alex Chengyu",
     "Fang"
    ],
    [
     "Jill",
     "House"
    ],
    [
     "Mark",
     "Huckvale"
    ]
   ],
   "title": "Investigating the syntactic characteristics of English tone units",
   "original": "i98_0273",
   "page_count": 4,
   "order": 26,
   "p1": "paper 0273",
   "pn": "",
   "abstract": [
    "This paper describes an investigation into the correspondence between grammatical units and English tone units. Our first aim is to provide some statistics based on scripted read speech since past studies mainly dealt with spontaneous speech. The second aim is to investigate whether the clause structure is a reliable indication of the tone unit. We start with a description of the annotation of transcribed speech data selected from the Spoken English Corpus (SEC), which is tagged for detailed wordclass information with AUTASYS and then parsed for rich syntactic description with the Survey Parser. Prosodic annotations in SEC, including both major and minor tone unit boundaries, were then mapped onto the parse trees. We then present our observations of tone units in the light of the clause structure. The paper will demonstrate that there is an overall correspondence between the clause structure and the tone unit in the sense that tone units generally co-start with the clause and that they seldom occur at major clause element junctures.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-26"
  },
  "bonafonte98_icslp": {
   "authors": [
    [
     "Antonio",
     "Bonafonte"
    ],
    [
     "Ignasi",
     "Esquerra"
    ],
    [
     "Albert",
     "Febrer"
    ],
    [
     "Jos√© A.R.",
     "Fonollosa"
    ],
    [
     "Francesc",
     "Vallverdu"
    ]
   ],
   "title": "The UPC text-to-speech system for Spanish and catalan",
   "original": "i98_1146",
   "page_count": 4,
   "order": 27,
   "p1": "paper 1146",
   "pn": "",
   "abstract": [
    "This paper summarizes the text-to-speech system that has been developed in the Speech Group of the Universitat Politecnica de Catalunya (UPC). The system is composed of a core and different interfaces so that it is compatible for research, for telephone applications (either CTI boards or standard ISDN PC cards supporting CAPI), and Windows applications developed using Microsoft SAPI. The paper reviews the system making emphasis in the parts of the system which are language dependent and which allow the reading of bilingual text (Spanish and Catalan). The paper also presents new approaches in prosodic modeling (segmental duration modeling) and generation of the database of speech segments, which have been introduced last year.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-27"
  },
  "ferencz98_icslp": {
   "authors": [
    [
     "Attila",
     "Ferencz"
    ],
    [
     "Istvan",
     "Nagy"
    ],
    [
     "Tunde-Csilla",
     "Kovacs"
    ],
    [
     "Maria",
     "Ferencz"
    ],
    [
     "Teodora",
     "Ratiu"
    ]
   ],
   "title": "The new version of the ROMVOX text-to-speech synthesis system based on a hybrid time domain-LPC synthesis technique",
   "original": "i98_0144",
   "page_count": 4,
   "order": 28,
   "p1": "paper 0144",
   "pn": "",
   "abstract": [
    "Through the years we developed several TTS systems for the Romanian language, each of them presenting some advantages and disadvantages [2]. Taking into account that waveform coding (time domain) methods assures a maximum level of intelligibility and naturalness of the synthesized speech, and that prosodic effects superimposing requires the alteration of pitch (frequency domain), we developed a hybrid time domain-LPC method, obtaining a better quality of the synthesized voice than those obtained with our former systems. This paper presents some theoretical considerations, signal processing and implementation aspects of this new synthesis method developed for the ROMVOX TTS system.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-28"
  },
  "kagoshima98_icslp": {
   "authors": [
    [
     "Takehiko",
     "Kagoshima"
    ],
    [
     "Masahiro",
     "Morita"
    ],
    [
     "Shigenobu",
     "Seto"
    ],
    [
     "Masami",
     "Akamine"
    ]
   ],
   "title": "An F0 contour control model for totally speaker driven text to speech system",
   "original": "i98_0214",
   "page_count": 4,
   "order": 29,
   "p1": "paper 0214",
   "pn": "",
   "abstract": [
    "Totally Speaker Driven Text to Speech System produces high quality and natural speech resembling the acoustic and prosodic characteristics of the original speech corpus. In the F0 contour control of this system, an F0 contour of a whole sentence is produced by concatenating segmental F0 contours generated by modifying vectors that are representatives of typical F0 contours. The representative vectors are selected from the F0 contour codebook, which is designed so as to minimize the approximation error between F0 contours generated by the proposed model and real F0 contours extracted from a speech corpus. It was confirmed by experiments with Japanese speech corpus that F0 contours can be modeled with small approximation errors by only 48 representative vectors, and the synthetic speech sounded very natural and resembled the prosodic characteristics of the original speaker.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-29"
  },
  "hirose98_icslp": {
   "authors": [
    [
     "Keikichi",
     "Hirose"
    ],
    [
     "Hiromichi",
     "Kawanami"
    ]
   ],
   "title": "On the relationship of speech rates with prosodic units in dialogue speech",
   "original": "i98_0730",
   "page_count": 4,
   "order": 30,
   "p1": "paper 0730",
   "pn": "",
   "abstract": [
    "For the purpose of constructing prosodic rules for dialogue speech synthesis, a comparative study was conducted on speech rates between dialogue speech and read speech. Based on the generation modeling of F0 contour, we can define 4 prosodic units, such as prosodic sentence, prosodic phrase and so on. Speech rate was analyzed with respect to these units. In a prosodic sentence, dialogue speech starts with a speech rate similar to that of read speech. The speech rate then gradually increases and, after passing through the middle of the unit, decreases towards the end. Similar tendencies were also observed in lower level units, but the degree of speech rate change in a unit was smaller. Based on the above results, a preliminary rules for speech rate control were developed for dialogue speech synthesis. A hearing test showed that the developed rules could make the synthetic speech sound more dialogue-like.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-30"
  },
  "klabbers98_icslp": {
   "authors": [
    [
     "Esther",
     "Klabbers"
    ],
    [
     "Raymond",
     "Veldhuis"
    ]
   ],
   "title": "On the reduction of concatenation artefacts in diphone synthesis",
   "original": "i98_0115",
   "page_count": 4,
   "order": 31,
   "p1": "paper 0115",
   "pn": "",
   "abstract": [
    "One well-known problem with diphone concatenation is the occurrence of audible discontinuities at diphone boundaries, which are most prominent in vowels and semi-vowels. Significant formant jumps at certain boundaries suggest that the problem is of a spectral nature. We have examined this hypothesis by correlating the results of a listening experiment with spectral distances measured across diphone boundaries. The aim is to find a spectral distance measure that best predicts when discontinuities are audible in order to find out how the diphone database can best be extended with context-sensitive diphones. The results show that the Kullback-Leibler measure is the best predictor.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-31"
  },
  "kuo98_icslp": {
   "authors": [
    [
     "Chih-Chung",
     "Kuo"
    ],
    [
     "Kun-Yuan",
     "Ma"
    ]
   ],
   "title": "Error analysis and confidence measure of Chinese word segmentation",
   "original": "i98_1078",
   "page_count": 4,
   "order": 32,
   "p1": "paper 1078",
   "pn": "",
   "abstract": [
    "Word segmentation for a Chinese sentence is essential for many applications in language and speech processing. There's no perfect method that could achieve word segmentation without any errors. We propose a confidence measure for the segmentation result to cope with the problem caused by the errors. The effective method depends mainly on the error analysis of the word segmentation. With the confidence measure the suspected errors can be identified such that manual inspection loads can be largely reduced for non-real-time applications. A soft-decision method and a composite-word approach for prosody generation are also designed for text-to-speech systems by exploiting the confidence measure, such that the wrong prosody caused by wrong word boundaries can be alleviated.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-32"
  },
  "lee98_icslp": {
   "authors": [
    [
     "Jungchul",
     "Lee"
    ],
    [
     "Donggyu",
     "Kang"
    ],
    [
     "Sanghoon",
     "Kim"
    ],
    [
     "Koengmo",
     "Sung"
    ]
   ],
   "title": "Energy contour generation for a sentence using a neural network learning method",
   "original": "i98_0404",
   "page_count": 4,
   "order": 33,
   "p1": "paper 0404",
   "pn": "",
   "abstract": [
    "Energy contour in a sentence is one of major factors that affect the naturalness of synthetic speech. In this paper. we propose a method to control the energy contour for the enhancement in the naturalness of Korean synthetic speech. Our algorithm adopts syllable as a basic unit and predicts the peak amplitude for each syllable in a word using a neural network (NN). We utilize indirect linguistic features as well as acoustic features of phonemes as input data to the NN to accommodate the grammatical effects of words in a sentence. The simulation results show that prediction error is less than 10% and our algorithm is very effective for analysis/synthesis of the energy contour of a sentence.. and generates a fairly good declarative contour for TTS.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-33"
  },
  "lee98b_icslp": {
   "authors": [
    [
     "Yong-Ju",
     "Lee"
    ],
    [
     "Sook-hyang",
     "Lee"
    ],
    [
     "Jong-Jin",
     "Kim"
    ],
    [
     "Hyun-Ju",
     "Ko"
    ],
    [
     "Young-Il",
     "Kim"
    ],
    [
     "Sang-Hun",
     "Kim"
    ],
    [
     "Jung-Cheol",
     "Lee"
    ]
   ],
   "title": "A computational algorithm for F0 contour generation in Korean developed with prosodically labeled databases using k-toBI system",
   "original": "i98_0704",
   "page_count": 4,
   "order": 34,
   "p1": "paper 0704",
   "pn": "",
   "abstract": [
    "This study describes an algorithm for the F0 contour generation system for Korean sentences and its evaluation results. 400 K-ToBI labeled utterances were used which were read by one male and one female announcers. F0 contour generation system uses two classification trees for prediction of K-ToBI labels for input text and 11 regression trees for prediction of F0 values for the labels. Evaluation results of the system showed 77.2% prediction accuracy for prediction of IP boundaries and 72.0% prediction accuracy for AP boundaries. Information of voicing and duration of the segments was not changed for F0 contour generation and its evaluation. Evaluation results showed 23.5Hz RMS error and 0.55 correlation coefficient in F0 generation experiment using labelling information from the original speech data.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-34"
  },
  "lenzo98_icslp": {
   "authors": [
    [
     "Kevin",
     "Lenzo"
    ],
    [
     "Christopher",
     "Hogan"
    ],
    [
     "Jeffrey",
     "Allen"
    ]
   ],
   "title": "Rapid-deployment text-to-speech in the DIPLOMAT system",
   "original": "i98_0868",
   "page_count": 4,
   "order": 35,
   "p1": "paper 0868",
   "pn": "",
   "abstract": [
    "The DIPLOMAT project at Carnegie Mellon University instantiates a program of rapid-deployment speech-to-speech machine translation; we have developed techniques for quickly producing text-to-speech (TTS) systems for new target languages to support this work. While the resulting systems are not immediately of comparable quality to commercial systems on unrestricted tasks in well-developed languages, they are more than adequate for limited-domain scenarios and rapid prototyping - they generalize to unseen data with some degradation, while quality in-domain can be quite good. Voices and engines for synthesizing new target languages may be developed in a period as short as two weeks after text corpus collection. We have successfully used these techniques to build a TTS module for English, Croatian, Spanish, Haitian Creole and Korean.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-35"
  },
  "mannell98_icslp": {
   "authors": [
    [
     "Robert H.",
     "Mannell"
    ]
   ],
   "title": "Formant diphone parameter extraction utilising a labelled single-speaker database",
   "original": "i98_0627",
   "page_count": 4,
   "order": 36,
   "p1": "paper 0627",
   "pn": "",
   "abstract": [
    "This paper examines a method for formant parameter extraction from a labeled single speaker database for use in a formant-parameter diphone-concatenation speech synthesis system. This procedure commences with an initial formant analysis of the labelled database, which is then used to obtain formant (F1-F5) probability spaces for each phoneme. These probability spaces guide a more careful speaker-specific extraction of formant frequencies. An analysis-by-synthesis procedure is then used to provide best-matching formant intensity and bandwidth parameters. The great majority of the parameters so extracted produce speech which is highly intelligible and which has a voice quality close to the original speaker.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-36"
  },
  "mizuno98_icslp": {
   "authors": [
    [
     "Osamu",
     "Mizuno"
    ],
    [
     "Shin'ya",
     "Nakajima"
    ]
   ],
   "title": "A new synthetic speech/sound control language",
   "original": "i98_1015",
   "page_count": 4,
   "order": 37,
   "p1": "paper 1015",
   "pn": "",
   "abstract": [
    "The Multi-layered Speech/Sound Synthesis Control Language (MSCL) proposed herein facilitates the synthesizing of several speech modes such as nuance, mental state and emotion, and allows speech to be synchronized to other media easily. MSCL is a multi-layered linguistic system and encompasses three layers: and semantic level layer (The S-layer), interpretation level layer (The I-layer), and parameter level layer (The P-layer). The S-layer is the description level of semantics such as emotional and emphasized speech. The I-layer is the description level of prosodic feature controls and interprets The S-layer scripts to for control on I-layer level. The P-layer represents prosodic parameters for speech synthesis. This multi-level description system is convenient for both laymen and professional users. MSCL also encompasses many effective prosodic feature control functions such as a time-varying pattern description function, absolute and relative control forms, and SDS(Speaker Dependent Scale). MSCL enables more emotional and expressive synthetic speech than conventional TTS systems. This paper describes these functions and the effective prosodic feature controls possible with MSCL.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-37"
  },
  "mochizuki98_icslp": {
   "authors": [
    [
     "Ryo",
     "Mochizuki"
    ],
    [
     "Yasuhiko",
     "Arai"
    ],
    [
     "Takashi",
     "Honda"
    ]
   ],
   "title": "A study on the natural-sounding Japanese phonetic word synthesis by using the VCV-balanced word database that consists of the words uttered forcibly in two types of pitch accent",
   "original": "i98_0247",
   "page_count": 4,
   "order": 38,
   "p1": "paper 0247",
   "pn": "",
   "abstract": [
    "In order to synthesize natural-sounding Japanese phonetic words, a novel VCV-concatenation synthesis with an advanced word database is proposed. The word database consists of VCV-balanced phonetic words which are uttered forcibly in type-0 and type-1 pitch accents. The advantage of using the advanced word database is that a variety of VCV-segments with the same phonetic chains and the different pitch patterns could be collected efficiently at the same time. The following pitch modification techniques are used to achieve the sound quality: (1) The optimal VCV-segment set which minimizes the pitch modification rate is selected. (2) Pitch waveforms are extracted by referring to excitation points. (3) Wavelengths of pitch waveforms are adjusted depending on the pitch modification rates. (4) Natural prosody in the VCV-segments in the database is effectively used. Superiority of the proposed database is ensured by means of the pitch pattern matching measurement and the subjective quality evaluation.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-38"
  },
  "pagel98_icslp": {
   "authors": [
    [
     "Vincent",
     "Pagel"
    ],
    [
     "Kevin",
     "Lenzo"
    ],
    [
     "Alan W.",
     "Black"
    ]
   ],
   "title": "Letter to sound rules for accented lexicon compression",
   "original": "i98_0561",
   "page_count": 4,
   "order": 39,
   "p1": "paper 0561",
   "pn": "",
   "abstract": [
    "This paper presents trainable methods for generating letter to sound rules from a given lexicon for use in pronouncing out-of-vocabulary words and as a method for lexicon compression. As the relationship between a string of letters and a string of phonemes representing its pronunciation for many languages is not trivial, we discuss two alignment procedures, one fully automatic and one hand-seeded which produce reasonable alignments of letters to phones. Top Down Induction Tree models are trained on the aligned entries. We show how combined phoneme/stress prediction is better than separate prediction processes, and still better when including in the model the last phonemes transcribed and part of speech information. For the lexicons we have tested, our models have a word accuracy (including stress) of 78% for OALD, 62% for CMU and 94% for BRULEX. The extremely high scores on the training sets allow substantial size reductions (more than 1/20). WWW site: http://tcts.fpms.ac.be/synthesis/mbrdico\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-39"
  },
  "roth98_icslp": {
   "authors": [
    [
     "Ze'ev",
     "Roth"
    ],
    [
     "Judith",
     "Rosenhouse"
    ]
   ],
   "title": "A name announcement algorithm with memory size and computational power constraints",
   "original": "i98_0280",
   "page_count": 4,
   "order": 40,
   "p1": "paper 0280",
   "pn": "",
   "abstract": [
    "This paper describes an algorithm for name (surnames and personal names) announcement in American English implemented on DSP Group's SmartCores (registered trade-mark) digital signal processor (dsp) core. The name announcement module is targeted for low cost applications therefore the amount of memory that can be allocated for dictionaries, program code, and runtime parameters is limited. The required response time of 0.5 seconds limits the computations performed in the linguistic analysis phase of each name. The synthesis scheme is limited by the real time capacity of the processor (since this task may be performed in parallel with other real time tasks).\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-40"
  },
  "sannier98_icslp": {
   "authors": [
    [
     "Frederique",
     "Sannier"
    ],
    [
     "Rabia",
     "Belrhali"
    ],
    [
     "V√©ronique",
     "Auberg√©"
    ]
   ],
   "title": "How a French TTS system can describe loanwords",
   "original": "i98_0497",
   "page_count": 4,
   "order": 41,
   "p1": "paper 0497",
   "pn": "",
   "abstract": [
    "We give a survey of the phonographical behaviour of the loanwords introduced into the French lexicon, through the observation of the systematic functioning of the French letter-to-phone TOPH system. We thus define sub-systems, isolated into lexicons. The processing of loans through the Toph TTS made it possible to give clues about the importance of one or other language in the French lexicon. The observation of the utterances graphonical functioning, made it thus possible to delimit classes. The second part of this study more specifically deals with the loanwords inflexion paradigms, for which as well different functionings are drawn.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-41"
  },
  "sef98_icslp": {
   "authors": [
    [
     "Tomaz",
     "Sef"
    ],
    [
     "Ales",
     "Dobnikar"
    ],
    [
     "Matjaz",
     "Gams"
    ]
   ],
   "title": "Improvements in slovene text-to-speech synthesis",
   "original": "i98_0128",
   "page_count": 4,
   "order": 42,
   "p1": "paper 0128",
   "pn": "",
   "abstract": [
    "This paper presents a new text-to-speech (TTS) system that is capable of synthesising continuous Slovenian speech. Input text is processed by a series of independent modules: text normalisation, grapheme-to-phoneme conversion, prosody generation and segmental concatenation. That enables easy improvements of separate parts of the system. In order to generate rules for our synthesis scheme, data was collected by analyzing the readings of ten speakers, five males and five females. Our system is used in several applications. It is built into an employment agent EMA that provides employment information through the Internet. Currently we are developing a system that will enable blind and partially sightless people to work in the Windows environment.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-42"
  },
  "seto98_icslp": {
   "authors": [
    [
     "Shigenobu",
     "Seto"
    ],
    [
     "Masahiro",
     "Morita"
    ],
    [
     "Takehiko",
     "Kagoshima"
    ],
    [
     "Masami",
     "Akamine"
    ]
   ],
   "title": "Automatic rule generation for linguistic features analysis using inductive learning technique: linguistic features analysis in TOS drive TTS system",
   "original": "i98_1059",
   "page_count": 4,
   "order": 43,
   "p1": "paper 1059",
   "pn": "",
   "abstract": [
    "The linguistic features analysis for input text plays an important role in achieving natural prosodic control in text-to-speech (TTS) systems. In a conventional scheme, experts refine suspicious if-then rules and change the tree structure manually to obtain correct analysis results when input texts that have been analyzed incorrectly. However, altering the tree structure drastically is difficult since attention is often paid only to the suspicious if-then rules. If earlier rule-tree structure is inappropriate, any attempt to improve the performance may be limited by the stiffness of the structure. To cope with these problems, the new development scheme generates analysis rules by using C4.5 [1], where an if-then rule-tree structure is generated by off-line training. The scheme has the advantage that since the generated rule-tree structure is simple, the rules are easier to maintain. The scheme is applied to generating four types of analysis rule-trees: rules for forming accent phrases, rules for determining accent position, rules for analyzing syntactic structure, and rules for pause insertion. An experimental evaluation was performed on these four rules. The accuracy was 96.5 percent for the accent phrase formation, 95.5 percent for the accent positioning, 87.0 percent for the pause insertion, and 88.3 percent for the syntactic analysis despite using small training data. These results indicate the validity of the scheme. The new scheme is used for developing linguistic features analysis rules in a Japanese TTS system, TOS Drive TTS [3].\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-43"
  },
  "shiga98_icslp": {
   "authors": [
    [
     "Yoshinori",
     "Shiga"
    ],
    [
     "Hiroshi",
     "Matsuura"
    ],
    [
     "Tsuneo",
     "Nitta"
    ]
   ],
   "title": "Segmental duration control based on an articulatory model",
   "original": "i98_0518",
   "page_count": 4,
   "order": 44,
   "p1": "paper 0518",
   "pn": "",
   "abstract": [
    "This paper proposes a new method that determines segmental duration for text-to-speech conversion based on the movement of articulatory organs which compose an articulatory model. The articulatory model comprises four time-variable articulatory parameters representing the conditions of articulatory organs whose physical restriction seems to significantly influence the segmental duration. The parameters are controlled according to an input sequence of phonetic symbols, following which segmental duration is determined based on the variation of the articulatory parameters. The proposed method is evaluated through an experiment using a Japanese speech database that consists of 150 phonetically balanced sentences. The results indicate that the mean square error of predicted segmental duration is approximately 15[ms] for the closed set and 15-17[ms] for the open set. The error is within 20[ms], the level of acceptability for distortion of segmental duration without loss of naturalness, and hence the method is proved to effectively predict segmental duration.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-44"
  },
  "tzoukermann98_icslp": {
   "authors": [
    [
     "Evelyne",
     "Tzoukermann"
    ]
   ],
   "title": "Text analysis for the bell labs French text-to-speech system",
   "original": "i98_0075",
   "page_count": 4,
   "order": 45,
   "p1": "paper 0075",
   "pn": "",
   "abstract": [
    "The Bell Labs text-to-speech synthesis system for French is part of a multilingual effort for text-to-speech generation. The text analysis component consists of four main parts: the morphological analysis module, the language models, the grapheme-to-phoneme conversion rules, and the prosodic module. The system is built in a pipeline architecture, the output of which feeds the subsequent synthesis modules. The originality of this work lies in the fact that we use weighted finite-state transducer technology to process the entire analysis of the French system. Moreover, the implementation not only accounts for most orthographic representations, such as numerals, abbreviations, dates, currencies, etc, but we also solve the hard questions of French liaison, mute e, and aspirated h using refined intermediate representations either in the form of traces or in the form of archigraphemes.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-45"
  },
  "venditti98_icslp": {
   "authors": [
    [
     "Jennifer J.",
     "Venditti"
    ],
    [
     "Jan P. H. van",
     "Santen"
    ]
   ],
   "title": "Modeling vowel duration for Japanese text-to-speech synthesis",
   "original": "i98_0786",
   "page_count": 4,
   "order": 46,
   "p1": "paper 0786",
   "pn": "",
   "abstract": [
    "Accurate estimation of segmental durations is crucial for natural-sounding text-to-speech (TTS) synthesis. This paper presents a model of vowel duration used in the Bell Labs Japanese TTS system. We describe the constraints on vowel devoicing, and effects of factors such as phone identity, surrounding phone identities, accentuation, syllabic structure, and phrasal position on the duration of both long and short vowels. A Sum-of-Products approach is used to model key interactions observed in the data, and to predict values of factor combinations not found in the speech database. We report root mean squared deviations between observed and predicted durations ranging from 8 to 15 ms, and an overall correlation of 0.89.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-46"
  },
  "wang98_icslp": {
   "authors": [
    [
     "Ren-Hua",
     "Wang"
    ],
    [
     "Qinfeng",
     "Liu"
    ],
    [
     "Yongsheng",
     "Teng"
    ],
    [
     "Deyu",
     "Xia"
    ]
   ],
   "title": "Towards a Chinese text-to-speech system with higher naturalness",
   "original": "i98_0172",
   "page_count": 4,
   "order": 47,
   "p1": "paper 0172",
   "pn": "",
   "abstract": [
    "This paper presents our research efforts on Chinese text-to-speech towards higher naturalness. The main results can be summarized as follows: 1. In the proposed TTS system the syllable-sized units were cut out from the real recorded speech, the synthetic speech was generated by concatenating these units back together. 2. The integration of units synthesized by rules with natural units was tested. A LMA filter based synthesizer was developed successfully to test and generate those units, which were difficult to be collected from the speech corpus. 3. A new efficient Chinese character coding scheme - \"Yin Xu Code\"(YX Code) has been developed to assist the GB Code . With the YX Code a new lexicon structure was designed up. The new dictionary system not only supplies with the pronunciation information, but also is much helpful for the words-segmentation. Based on above results, a Chinese text-to-speech system named as \"KD-863\" has been developed. The system converts any Chinese written text to speech in real time with high naturalness. In the national assessment of Chinese TTS systems held at the end of March 1998 in Beijing, the system achieved a first of the naturalness MOS (Mean Opinion Score).\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-47"
  },
  "breen98_icslp": {
   "authors": [
    [
     "Andrew P.",
     "Breen"
    ],
    [
     "Peter",
     "Jackson"
    ]
   ],
   "title": "A phonologically motivated method of selecting non-uniform units",
   "original": "i98_0389",
   "page_count": 4,
   "order": 48,
   "p1": "paper 0389",
   "pn": "",
   "abstract": [
    "This paper describes a method for selecting units from a database of recorded speech, for use in a concatenative speech synthesiser. The simplest approach is to store one example of every possible unit. A more powerful method is to have multiple examples of each unit. The challenge for such a method is to provide an efficient means of selecting units from a practical inventory, to give the best approximation to the desired sequence in some clearly specified way. The method used in BT's Laureate system uses mixed N-phone units. In theory such units could be of arbitrary size, but in practice they are constrained to a maximum of three phones. It dynamically generates the unit sequence based on a global cost. Units are selected using purely phonologically motivated criteria, without reference to acoustic features, either desired or available within the inventory.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-48"
  },
  "pearson98_icslp": {
   "authors": [
    [
     "Steve",
     "Pearson"
    ],
    [
     "Nick",
     "Kibre"
    ],
    [
     "Nancy",
     "Niedzielski"
    ]
   ],
   "title": "A synthesis method based on concatenation of demisyllables and a residual excited vocal tract model",
   "original": "i98_0648",
   "page_count": 4,
   "order": 49,
   "p1": "paper 0648",
   "pn": "",
   "abstract": [
    "This paper describes the back-end of a new, flexible, high-quality TTS system. Preliminary results have demonstrated a highly natural and intelligible output. Although the system follows some standard methodologies, such as concatenation, we have introduced a number of novel features and a combination of techniques that make our system unique. We will describe in detail many of the design decisions and compare them with other known systems. A demonstration of the speech quality with implanted prosody is available in waveform file ([WAVE stltts1.wav and stltts2.wav]) on the conference CD.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-49"
  },
  "syrdal98_icslp": {
   "authors": [
    [
     "Ann K.",
     "Syrdal"
    ],
    [
     "Alistair",
     "Conkie"
    ],
    [
     "Yannis",
     "Stylianou"
    ]
   ],
   "title": "Exploration of acoustic correlates in speaker selection for concatenative synthesis",
   "original": "i98_0882",
   "page_count": 4,
   "order": 50,
   "p1": "paper 0882",
   "pn": "",
   "abstract": [
    "It is often difficult to determine the suitability of a speaker to serve as a model for concatenative text-to-speech synthesis. The perceived quality of a speaker's natural voice is not necessarily predictive of its synthetic quality. The selection of female and male speakers on whom to base two synthetic voices for the new AT&T text-to-speech system was made empirically. Brief readings of identical text materials were recorded from professional speakers. Small-scale TTS systems were constructed with a minimal diphone inventory, suitable for synthesizing a limited number of test sentences. Synthesized sentences and their naturally spoken references were presented to listeners in a formal listening evaluation. In addition, a variety of acoustic measurements of the speakers were made in order to determine which acoustic characteristics correlated with subjective synthesis quality. The results have implications both for speaker selection and for improving concatenative synthesis methods.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-50"
  },
  "wouters98_icslp": {
   "authors": [
    [
     "Johan",
     "Wouters"
    ],
    [
     "Michael W.",
     "Macon"
    ]
   ],
   "title": "A perceptual evaluation of distance measures for concatenative speech synthesis",
   "original": "i98_0905",
   "page_count": 4,
   "order": 51,
   "p1": "paper 0905",
   "pn": "",
   "abstract": [
    "In concatenative synthesis, new utterances are created by concatenating segments (units) of recorded speech. When the segments are extracted from a large speech corpus, a key issue is to select segments that will sound natural in a given phonetic context. Distance measures are often used for this task. However, little is known about the perceptual relevance of these measures. More insight into the relationship between computed distances and perceptual differences is needed to develop accurate unit selection algorithms, and to improve the quality of the resulting computer speech. In this paper, we develop a perceptual test to measure subtle phonetic differences between speech units. We use the perceptual data to evaluate several popular distance measures. The results show that distance measures that use frequency warping perform better than those that do not, and minimal extra advantage is gained by using weighted distances or delta features.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-51"
  },
  "plumpe98_icslp": {
   "authors": [
    [
     "Mike",
     "Plumpe"
    ],
    [
     "Alex",
     "Acero"
    ],
    [
     "Hsiao-Wuen",
     "Hon"
    ],
    [
     "Xuedong",
     "Huang"
    ]
   ],
   "title": "HMM-based smoothing for concatenative speech synthesis",
   "original": "i98_0908",
   "page_count": 4,
   "order": 52,
   "p1": "paper 0908",
   "pn": "",
   "abstract": [
    "This paper will focus on our recent efforts to further improve the acoustic quality of the Whistler Text-to-Speech engine. We have developed an advanced smoothing system that a small pilot study indicates significantly improves quality. We represent speech as being composed of a number of frames, where each frame can be synthesized from a parameter vector. Each frame is represented by a state in an HMM, where the output distribution of each state is a Gaussian random vector consisting of x and Dx. The set of vectors that maximizes the HMM probability is the representation of the smoothed speech output. This technique follows our traditional goal of developing methods whose parameters are automatically learned from data with minimal human intervention. The general framework is demonstrated to be robust by maintaining improved quality with a significant reduction in data.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-52"
  },
  "holzapfel98_icslp": {
   "authors": [
    [
     "Martin",
     "Holzapfel"
    ],
    [
     "Nick",
     "Campbell"
    ]
   ],
   "title": "A nonlinear unit selection strategy for concatenative speech synthesis based on syllable level features",
   "original": "i98_0521",
   "page_count": 4,
   "order": 53,
   "p1": "paper 0521",
   "pn": "",
   "abstract": [
    "This paper describes an improved algorithm, motivated by fuzzy logic theory, for the selection of speech segments for concatenative synthesis from a huge database. Triphone HMM clustering is employed as an adaptive measure for articulatory similarity within a given database. Stress level contours are evaluated in the context of their surrounding vocalic peaks. The algorithm uses a beam search technique to optimise the suitability of each candidate unit to realise the desired target as well as continuity in concatenation.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-53"
  },
  "eklund98_icslp": {
   "authors": [
    [
     "Robert",
     "Eklund"
    ],
    [
     "Anders",
     "Lindstr√∂m"
    ]
   ],
   "title": "How to handle \"foreign\" sounds in Swedish text-to-speech conversion: approaching the 'xenophone' problem",
   "original": "i98_0514",
   "page_count": 4,
   "order": 54,
   "p1": "paper 0514",
   "pn": "",
   "abstract": [
    "This paper discusses the problem of handling 'foreign' speech sounds in Swedish speech technology systems, in particular speech synthesis. A production study is made, where it is shown that Swedish speakers add foreign speech sounds, here termed 'xenophones', to their phone repertoire when reading Swedish sentences with embedded English names and words. As a result of the observations, the phone set of a Swedish concatenative synthesizer is extended, and it is shown (by example) that this produces more natural-sounding synthetic speech.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-54"
  },
  "campbell98_icslp": {
   "authors": [
    [
     "Nick",
     "Campbell"
    ]
   ],
   "title": "Multi-lingual concatenative speech synthesis",
   "original": "i98_0024",
   "page_count": 5,
   "order": 55,
   "p1": "paper 0024",
   "pn": "",
   "abstract": [
    "This paper describes a method of concatenative speech synthesis that makes use of 3-dimensional labelling of speech, and shows how this can be applied to the synthesis of both mono-lingual and foreign-language speech. The dimensions encode phonetic, prosodic, and voice-quality information in order to fully describe the acoustic characteristics of each speech segment.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-55"
  },
  "saito98_icslp": {
   "authors": [
    [
     "Takashi",
     "Saito"
    ]
   ],
   "title": "On the use of F0 features in automatic segmentation for speech synthesis",
   "original": "i98_1044",
   "page_count": 4,
   "order": 56,
   "p1": "paper 1044",
   "pn": "",
   "abstract": [
    "This paper focuses on a method for automatically dividing speech utterances into phonemic segments, which are used for constructing synthesis unit inventories for speech synthesis. Here, we propose a new segmentation parameter called, \"F0 dynamics (DF0).\" In the fine structures of F0 contours, there exist phonemic events which are observed as local dips at phonemic transition regions, especially around voiced consonants. We apply this observation about F0 contours to a speech segmentation method. The DF0 segmentation parameter is used in the final stage of the segmentation procedure to refine the phonemic boundaries roughly obtained by DP alignment. We conduct experiments on the proposed automatic segmentation with a speech database prepared for unit inventory construction, and compare the obtained boundaries with those of manual segmentation to show the effectiveness of the proposed method. We also discuss the effects of the boundary refinement on the synthesized speech.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-56"
  },
  "sakurai98_icslp": {
   "authors": [
    [
     "Atsuhiro",
     "Sakurai"
    ],
    [
     "Takashi",
     "Natsume"
    ],
    [
     "Keikichi",
     "Hirose"
    ]
   ],
   "title": "A linguistic and prosodic database for data-driven Japanese TTS synthesis",
   "original": "i98_0735",
   "page_count": 4,
   "order": 57,
   "p1": "paper 0735",
   "pn": "",
   "abstract": [
    "We propose a method to generate a database that contains a parametric representation of F0 contours associated with linguistic and acoustic information, to be used by data-driven Japanese text-to-speech (TTS) systems. The configuration of the database includes recorded speech, F0 contours and their parametric labels, phonetic transcription with durations, and other linguistic information such as orthographic transcription, part-of-speech (POS) tags, and accent types. All information that is not available by dictionary lookup is obtained automatically. In this paper, we propose a method to automatically obtain parametric labels that describe F0 contours based on a superpositional model. Preliminary tests on a small data set show that the method can find the parametric representation of F0 contours with acceptable accuracy, and that accuracy can be improved by introducing additional linguistic information.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-57"
  },
  "kain98_icslp": {
   "authors": [
    [
     "Alexander",
     "Kain"
    ],
    [
     "Michael W.",
     "Macon"
    ]
   ],
   "title": "Text-to-speech voice adaptation from sparse training data",
   "original": "i98_0902",
   "page_count": 4,
   "order": 58,
   "p1": "paper 0902",
   "pn": "",
   "abstract": [
    "Voice adaptation describes the process of converting the output of a text-to-speech synthesizer voice to sound like a different voice after a training process in which only a small amount of the desired target speaker's speech is seen. We employ a locally linear conversion function based on Gaussian mixture models to map bark-scaled line spectral frequencies. We compare performance for three different estimation methods while varying the number of mixture components and the amount of data used for training. An objective evaluation revealed that all three methods yield similar test results. In perceptual tests, listeners judged the converted speech quality as acceptable and fairly successful in adapting to the target speaker.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-58"
  },
  "mohler98_icslp": {
   "authors": [
    [
     "Gregor",
     "M√∂hler"
    ]
   ],
   "title": "Describing intonation with a parametric model",
   "original": "i98_0205",
   "page_count": 4,
   "order": 59,
   "p1": "paper 0205",
   "pn": "",
   "abstract": [
    "In this study a data-based approach to intonation modeling is presented. The model incorporates knowledge from intonation theories like the expected types of F0 movements and syllable anchoring. The knowledge is integrated into the model using an appropriate approximation function for F0 parametrization. The F0 parameters that result from the parametrization are predicted from a set of features using neural nets. The quality of the generated contours is assessed by means of numerical measures and perception tests. They show that the basic hypotheses about intonation description and modeling are in principle correct and that they have the potential to be successfully applied to speech synthesis. We argue for a clear interface with a linguistic description (using pitch-accent and boundary labels as input) and discourse structure (using pitch-range normalized F0 parameters).\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-59"
  },
  "gustafson98_icslp": {
   "authors": [
    [
     "Joakim",
     "Gustafson"
    ],
    [
     "Patrik",
     "Elmberg"
    ],
    [
     "Rolf",
     "Carlson"
    ],
    [
     "Arne",
     "Jonsson"
    ]
   ],
   "title": "An educational dialogue system with a user controllable dialogue manager",
   "original": "i98_0504",
   "page_count": 4,
   "order": 60,
   "p1": "paper 0504",
   "pn": "",
   "abstract": [
    "We have developed an educational environment for a modular spoken dialogue system. The aim of the environment is to provide students, with different backgrounds, means to understand the behaviour of spoken dialogue systems. Focus in this paper is on dialogue and dialogue management. The dialogue is recorded in a dialogue tree whose nodes are dialogue objects. The dialogue objects model the constituents of the dialogue and consist of parameters for modelling dialogue structure, focus structure and a process description describing the actions of the dialogue system. Various dialogue system behaviours can be achieved by modifying these parameters. This is done using the educational environment, which is interactive and facilitates examination, expansion and modification of the dialogue object parameters and hence the system. The educational system has been used in a number of courses at various universities in Sweden.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-60"
  },
  "failenschmid98_icslp": {
   "authors": [
    [
     "Klaus",
     "Failenschmid"
    ],
    [
     "J.H. Simon",
     "Thornton"
    ]
   ],
   "title": "End-user driven dialogue system design: the reward experience",
   "original": "i98_0783",
   "page_count": 4,
   "order": 61,
   "p1": "paper 0783",
   "pn": "",
   "abstract": [
    "In the EC funded research project REWARD professionals in the target domain for a dialogue system were enabled to develop their own speech driven telephone service (dialogue system). Specialists in speech recognition technology provided them with graphical tools and technical advice. Guided by their in-depth understanding of the target domain and target user the four user partners developed four different dialogue systems. The shift of focus in the dialogue system design process from technology to end user driven design 'rationale' is influenced by research into the design process of dialogue systems and provides valuable data that informs user-centred-design processes of interactive systems. In this paper we report on observations with regard to this end-user driven design process. Dialogue features, which are a direct result of this different approach to designing speech recognition applications, are presented.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-61"
  },
  "lin98_icslp": {
   "authors": [
    [
     "Yi-Chung",
     "Lin"
    ],
    [
     "Tung-Hui",
     "Chiang"
    ],
    [
     "Heui-Ming",
     "Wang"
    ],
    [
     "Chung-Ming",
     "Peng"
    ],
    [
     "Chao-Huang",
     "Chang"
    ]
   ],
   "title": "The design of a multi-domain Mandarin Chinese spoken dialogue system",
   "original": "i98_0230",
   "page_count": 4,
   "order": 62,
   "p1": "paper 0230",
   "pn": "",
   "abstract": [
    "In some dialogue systems, the design of dialogue strategy is bound tightly to the domain by straightforwardly hard-coding the response actions into the system. Such a paradigm is quite easy to build up a prototype system, but makes it difficult to port the system across different domains. This paper presents a domain-transparent design of dialogue management to increase system portability. The basic idea of this framework is to extract the domain-dependent factors to form an external domain knowledge database, leaving the dialogue management component independent of the tasks. Based on the proposed framework, porting to another domain needs only to replace the domain knowledge database without changing the dialogue management module. This paper also proposed a task description table interface enabling system developers to design the dialogue strategy flexibly. With this approach, the effort of porting a spoken dialogue system across different domains can be relieved.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-62"
  },
  "georgila98_icslp": {
   "authors": [
    [
     "Kallirroi",
     "Georgila"
    ],
    [
     "Anastasios",
     "Tsopanoglou"
    ],
    [
     "Nikos",
     "Fakotakis"
    ],
    [
     "George",
     "Kokkinakis"
    ]
   ],
   "title": "An integrated dialogue system for the automation of call centre services",
   "original": "i98_0236",
   "page_count": 4,
   "order": 63,
   "p1": "paper 0236",
   "pn": "",
   "abstract": [
    "A human-machine dialogue system for the automation of call centre services is presented, which features two novel functions: a. It integrates spoken and written language processing through cooperation of a spoken dialogue component and an OCR which extracts handwritten information from application forms. b. It reads the forms and calls the applicants (instead of being called by them) in order to collect the missing information through a continuous speech, full sentence, machine-driven dialogue which allows the user to formulate his answers at will. The system has been developed for a Greek car insurance company in the framework of the EU-project: LE-1 1802, ACCeSS, but can be easily adapted to a different call centre application. The system is currently tested at the company's premises.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-63"
  },
  "wang98b_icslp": {
   "authors": [
    [
     "Kuansan",
     "Wang"
    ]
   ],
   "title": "An event driven model for dialogue systems",
   "original": "i98_0888",
   "page_count": 4,
   "order": 64,
   "p1": "paper 0888",
   "pn": "",
   "abstract": [
    "This paper reports our progress in building a mixed initiative, goal-oriented dialogue system for human machine interactions. The dialogue model embraces the spirits of the so-called plan-based approach in that the dialogue flow is not statically authored but dynamically generated by the system as a natural outcome of the semantic evaluation process. With multimodal applications in mind, the dialogue system is designed in an event driven architecture that is commonly seen in the core of a graphical user interface (GUI) environment. In the same manner that GUI events are handled by graphical objects, the proposed dialogue model assigns dialogue events to semantic objects that encapsulate the knowledge for handling events under various discourse contexts. Currently, we have found that four types of events, namely, dialogue object instantiation, semantic evaluation, dialogue repair, and discourse binding, are sufficient for a wide range of applications.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-64"
  },
  "popovici98_icslp": {
   "authors": [
    [
     "Cosmin",
     "Popovici"
    ],
    [
     "Paolo",
     "Baggia"
    ],
    [
     "Pietro",
     "Laface"
    ],
    [
     "Loreta",
     "Moisa"
    ]
   ],
   "title": "Automatic classification of dialogue contexts for dialogue predictions",
   "original": "i98_0552",
   "page_count": 4,
   "order": 65,
   "p1": "paper 0552",
   "pn": "",
   "abstract": [
    "This paper exploits the broad concept of dialogue predictions by linking a point in a human-machine dialogue with a specific language model which is used during the recognition of the next user utterance. The idea is to cluster several dialogue contexts into a class and to create for each class a specific language model. We present an automatic algorithm based on the minimal decrease of mutual information which clusters the dialogue contexts. Moreover the algorithm is able to guess an appropriate number of classes, that gives a good trade off between the mutual information and the amount of training data. Therefore the automatic classification allows the full automatic creation of context-dependent language models for a spoken dialogue system.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-65"
  },
  "ramaswamy98_icslp": {
   "authors": [
    [
     "Ganesh N.",
     "Ramaswamy"
    ],
    [
     "Jan",
     "Kleindienst"
    ]
   ],
   "title": "Automatic identification of command boundaries in a conversational natural language user interface",
   "original": "i98_0612",
   "page_count": 4,
   "order": 66,
   "p1": "paper 0612",
   "pn": "",
   "abstract": [
    "In this paper, we propose a trainable system that can automatically identify the command boundaries in a conversational natural language user interface. The proposed solution makes the conversational interface much more user friendly, and allows the user to speak naturally and continuously in a hands-free manner. The main ingredient of the system is the maximum entropy identification model, which is trained using data that has all the correct command boundaries marked. During training, a set of features and their weights are selected iteratively using the training data. The features consists of words and phrases, as well as their relative position to the potential command boundaries. Decoding is done by examining the product of the weights for the features that are present. We also propose several enhancements to the approach, such as combining it with a more effective language model at the speech recognition stage to generate additional tokens for the identification model. We conducted several experiments to evaluate the proposed approach, and the results are described.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-66"
  },
  "poesio98_icslp": {
   "authors": [
    [
     "Massimo",
     "Poesio"
    ],
    [
     "Andrei",
     "Mikheev"
    ]
   ],
   "title": "The predictive power of game structure in dialogue act recognition: experimental results using maximum entropy estimation",
   "original": "i98_0606",
   "page_count": 3,
   "order": 67,
   "p1": "paper 0606",
   "pn": "",
   "abstract": [
    "Recognizing the dialogue act(s) performed by means of an utterance involves combining top-down expectations about the next likely `move' in a dialogue with bottom-up information extracted from the speech signal. We compared two ways of generating expectations: one which makes the expectations depend only on the previous act (as in a bigram model), and one which also takes into account the fact that individual dialogue acts play a role as part of larger conversational structures (`games'). Our models were built by training over the HCRC MapTask corpus using the LTG implementation of maximum entropy estimation. We achieved an accuracy of 38.6% using bigrams, of 50.6% taking game structure into account; adding information about speaker change resulted in an accuracy of 41.8% with bigrams, 54% with game structure. These results indicate that exploiting game structure does lead to improved expectations.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-67"
  },
  "constantinides98_icslp": {
   "authors": [
    [
     "Paul C.",
     "Constantinides"
    ],
    [
     "Scott",
     "Hansma"
    ],
    [
     "Chris",
     "Tchou"
    ],
    [
     "Alexander I.",
     "Rudnicky"
    ]
   ],
   "title": "A schema based approach to dialog control",
   "original": "i98_0637",
   "page_count": 4,
   "order": 68,
   "p1": "paper 0637",
   "pn": "",
   "abstract": [
    "Frame-based approaches to spoken language interaction work well for limited tasks such as information access, given that the goal of the interaction is to construct a correct query then execute it. More complex tasks, however, can benefit from more active system participation. We describe two mechanisms that provide this, a modified stack that allows the system to track multiple topics, and form-specific schema that allow the system to deal with tasks that involve completion of multiple forms. Domain-dependent schema specify system behavior and are executed by a domain-independent engine. We describe implementations for a personal calendar system and for an air travel planning system.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-68"
  },
  "aist98_icslp": {
   "authors": [
    [
     "Gregory",
     "Aist"
    ]
   ],
   "title": "Expanding a time-sensitive conversational architecture for turn-taking to handle content-driven interruption",
   "original": "i98_0928",
   "page_count": 4,
   "order": 69,
   "p1": "paper 0928",
   "pn": "",
   "abstract": [
    "Turn taking in spoken language systems has generally been push-to-talk or strict alternation (user speaks, system speaks, user speaks, ...) with some systems such as telephone-based systems handling barge-in (interruption by the user.) In this paper we describe our time sensitive conversational architecture for turn taking that not only allows alternating turns and barge in, but other conversational behaviors as well. This architecture allows backchanneling, prompting the user by taking more than one turn if necessary, and overlapping speech. The architecture is implemented in a Reading Tutor that listens to children read aloud, and helps them. We extended this architecture to allow the Reading Tutor to interrupt the student based on a non-self-corrected mistake - \"content-driven interruption\". To the best of our knowledge, the Reading Tutor is thus the first spoken language system to intentionally interrupt the user based on the content of the utterance.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-69"
  },
  "swerts98_icslp": {
   "authors": [
    [
     "Marc",
     "Swerts"
    ],
    [
     "Hanae",
     "Koiso"
    ],
    [
     "Atsushi",
     "Shimojima"
    ],
    [
     "Yasuhiro",
     "Katagiri"
    ]
   ],
   "title": "On different functions of repetitive utterances",
   "original": "i98_0269",
   "page_count": 4,
   "order": 70,
   "p1": "paper 0269",
   "pn": "",
   "abstract": [
    "The study reported in this paper focuses on different functions of echoing in Japanese dialogues. Echoing is defined as a speaker's lexical repeat of (parts of) an utterance spoken by a conversation partner in a previous turn. The phenomenon was investigated in three task-oriented, informal dialogues. Repeats in this corpus were labeled in terms of a 5-point scale which expressed the level to which a speaker had integrated the other person's utterance into his/her own body of knowledge. Kappa statistics showed that the labels could reliably be reproduced by three independent subjects. The investigation brought to light that the level of integration is reflected in a number of lexical and prosodic correlates. These features are discussed regarding their information potential, i.e., their accuracy and comprehensiveness as signals.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-70"
  },
  "noguchi98_icslp": {
   "authors": [
    [
     "Hiroaki",
     "Noguchi"
    ],
    [
     "Yasuharu",
     "Den"
    ]
   ],
   "title": "Prosody-based detection of the context of backchannel responses",
   "original": "i98_0662",
   "page_count": 4,
   "order": 71,
   "p1": "paper 0662",
   "pn": "",
   "abstract": [
    "Current spoken dialogue systems lack positive feedback such as backchannels, which are common in human-human conversations. To develop more natural human-computer interfaces, the investigation of backchannel-responses are indispensable. In this paper, we propose a method for detecting the precise timing for backchannel responses in Japanese and aim at incorporating such method in future spoken dialogue systems. The proposed method is based on machine learning technique with a variety of prosodic features. It is shown to be effective in automatically deriving rules for detecting the contexts of backchannels. The performance of our method is considerably better than previous methods.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-71"
  },
  "stromback98_icslp": {
   "authors": [
    [
     "Lena",
     "Stromb√§ck"
    ],
    [
     "Arne",
     "Jonsson"
    ]
   ],
   "title": "Robust interpretation for spoken dialogue systems",
   "original": "i98_0478",
   "page_count": 4,
   "order": 72,
   "p1": "paper 0478",
   "pn": "",
   "abstract": [
    "Spoken dialogue systems must allow for robust and efficient interpretation of user utterances. This can be achieved by using shallow and partial interpretation. Partial interpretation is feasible together with a dialogue manager which provides information to guide the analysis. In this paper we present results on developing interfaces for information retrieval applications utilizing partial and information directed interpretation with unification-based formalisms, traditionally used for deep and complete analysis. The major advantage with our approach is that the time to develop the interpretation modules is reduced. Furthermore, the system will be fairly robust as large parts of the knowledge bases containing knowledge on ways in which a user can express a domain concept can be generated automatically or semi-automatically.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-72"
  },
  "okato98_icslp": {
   "authors": [
    [
     "Yohei",
     "Okato"
    ],
    [
     "Keiji",
     "Kato"
    ],
    [
     "Mikio",
     "Yamamoto"
    ],
    [
     "Shuichi",
     "Itahashi"
    ]
   ],
   "title": "System-user interaction and response strategy in spoken dialogue system",
   "original": "i98_0683",
   "page_count": 4,
   "order": 73,
   "p1": "paper 0683",
   "pn": "",
   "abstract": [
    "There are a number of restrictions in human-machine interactions, which continue to warrant a better control of response utterances in spoken dialogue systems. In this paper, we investigated how system-response strategies influence users, focusing on: interjectory responses to users' utterances; and on verbose responses which vary between brief and detailed expression used for confirmation. The dialogue task selected consists of telephone shopping, and human-machine dialogues using Wizard-of-OZ method were collected for analysis. The results show that back-channel feedbacks and brief confirmations by our spoken dialogue system, prompt more utterances from and give more satisfaction to users. Second, users tend to give back-channel feedbacks whenever the system gives them feedbacks. It follows that users are able to gauge the system's ability to handle interjections and to predict its behaviour.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-73"
  },
  "suzuki98_icslp": {
   "authors": [
    [
     "Noriko",
     "Suzuki"
    ],
    [
     "Kazuo",
     "Ishii"
    ],
    [
     "Michio",
     "Okada"
    ]
   ],
   "title": "Organizing self-motivated dialogue with autonomous creatures",
   "original": "i98_1101",
   "page_count": 4,
   "order": 74,
   "p1": "paper 1101",
   "pn": "",
   "abstract": [
    "This paper discusses the effectiveness of human-computer interaction in our prototype system \"Talking Eye\", which is based on social behaviors in self-motivated dialogue. Talking Eye system consists of autonomous creatures having the emergent computation architecture with the advantage of self- motivated dialogue. We performed an experiment using subject's impression to illustrate the effectiveness of self- motivated dialogue with the Talking Eye system. The result was obtained that they could find personal nature during interaction even if they could not accomplish their conversational purpose. The main goal of this work is to build a mechanism for autonomous creatures that promotes a more consensual feeling through its interaction with humans.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-74"
  },
  "hanrieder98_icslp": {
   "authors": [
    [
     "Gerhard",
     "Hanrieder"
    ],
    [
     "Paul",
     "Heisterkamp"
    ],
    [
     "Thomas",
     "Brey"
    ]
   ],
   "title": "Fly with the EAGLES: evaluation of the \"ACCeSS\" spoken language dialogue system",
   "original": "i98_0274",
   "page_count": 4,
   "order": 75,
   "p1": "paper 0274",
   "pn": "",
   "abstract": [
    "This paper reports the experiences we had in evaluating the ACCeSS system using the EAGLES evaluation metrics both at the input/output (black box evaluation) and component levels (glass box evaluation). Our primary motivation for using the EAGLES metrics was to apply a set of metrics that allow objective measurement of system performance. These measures are used both for diagnostic evaluation of the current version and for progress evaluation of successive system versions. The EAGLES metrics provided very valuable guidelines for our evaluation, but not all the metrics were unproblematic. Especially for diagnostics during development, the addition of subtask success rate proved very helpful.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-75"
  },
  "aretoulaki98_icslp": {
   "authors": [
    [
     "Maria",
     "Aretoulaki"
    ],
    [
     "Stefan",
     "Harbeck"
    ],
    [
     "Florian",
     "Gallwitz"
    ],
    [
     "Elmar",
     "N√∂th"
    ],
    [
     "Heinrich",
     "Niemann"
    ],
    [
     "Jozef",
     "Ivanecky"
    ],
    [
     "Ivo",
     "Ipsic"
    ],
    [
     "Nikola",
     "Pavesic"
    ],
    [
     "Vaclav",
     "Matousek"
    ]
   ],
   "title": "SQEL: a multilingual and multifunctional dialogue system",
   "original": "i98_0329",
   "page_count": 4,
   "order": 76,
   "p1": "paper 0329",
   "pn": "",
   "abstract": [
    "Within the EC SQEL project, the German EVAR spoken dialogue system has been extended to handle four different languages and domains: German, Slovak, and Czech (national train connections), and Slovenian (European flights). The SQEL demonstrator can also access databases on the WWW, enabling users without an internet connection to meet their information needs by just using the phone. When the system starts up, the caller is free to use any of the implemented languages. A multilingual word recognizer implicitly identifies the language, which is then associated with the appropriate domain and database. For the remainder of the dialogue, the corresponding monolingual recognizer is employed. The existence of language-independent parameters (e.g. goal/source location) has meant that porting the system to a new language does not involve an extensive restructuring of the interpretation process within the Dialogue Manager. This is sufficiently flexible to switch between the different domains and languages.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-76"
  },
  "kaspar98_icslp": {
   "authors": [
    [
     "Stefan",
     "Kaspar"
    ],
    [
     "Achim",
     "Hoffmann"
    ]
   ],
   "title": "Semi-automated incremental prototyping of spoken dialog systems",
   "original": "i98_0651",
   "page_count": 4,
   "order": 77,
   "p1": "paper 0651",
   "pn": "",
   "abstract": [
    "Cost-effective development and widespread deployment of spoken language systems is only possible if their development process is substantially supported by intelligent design tools. In this paper, we present PIA a new approach for fast prototyping of complex spoken dialog systems. Dialog structures are specified in a specialised fully declarative design language. Dialog design in PIA is considered a knowledge acquisition task, where knowledge about possible user input and the desired system reaction is incrementally derived from actual interactions. PIA strongly supports the designer to keep the fine balance between robustness of recognition and the naturalness of the dialog\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-77"
  },
  "heeman98_icslp": {
   "authors": [
    [
     "Peter A.",
     "Heeman"
    ],
    [
     "Michael",
     "Johnston"
    ],
    [
     "Justin",
     "Denney"
    ],
    [
     "Edward",
     "Kaiser"
    ]
   ],
   "title": "Beyond structured dialogues: factoring out grounding",
   "original": "i98_0933",
   "page_count": 4,
   "order": 78,
   "p1": "paper 0933",
   "pn": "",
   "abstract": [
    "Structured dialogue models are currently the only tools for easily building spoken dialogue systems. This approach, however, requires the dialogue designer to completely specify all dialogue behavior between the user and system, including how information is grounded between the user and the system. In this paper, we advocate factoring out the grounding behavior from structured dialogue models by using a general purpose dialogue manager that accounts for this behavior. This not only simplifies the specification of dialogue, but also allows more powerful mechanisms of grounding to be employed, which cannot be implemented within the framework of structured dialogues.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-78"
  },
  "araki98_icslp": {
   "authors": [
    [
     "Masahiro",
     "Araki"
    ],
    [
     "Shuji",
     "Doshita"
    ]
   ],
   "title": "A robust dialogue model for spoken dialogue processing",
   "original": "i98_0729",
   "page_count": 4,
   "order": 79,
   "p1": "paper 0729",
   "pn": "",
   "abstract": [
    "In this paper, we propose a robust processing model of spoken dialogue. Our dialogue model is a cognitive process model (1) which integrates stepwise processing from utterance understanding to response generation, (2) which specifies the interactions between the processing of each steps and two level dialogue management mechanism, and (3) which identifies the possible errors caused by speech recognition error and specifies the method of recovering from the error. Also, We examined the validity of this model using new evaluation paradigm: system-to-system dialogue with linguistic noise. By this evaluation, the robustness of proposed cognitive process model is shown in relatively low recognition error situation.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-79"
  },
  "brndsted98_icslp": {
   "authors": [
    [
     "Tom",
     "Br√∏ndsted"
    ],
    [
     "Bo Nygaard",
     "Bai"
    ],
    [
     "Jesper √òstergaard",
     "Olsen"
    ]
   ],
   "title": "The REWARD service creation environment. an overview",
   "original": "i98_0811",
   "page_count": 4,
   "order": 80,
   "p1": "paper 0811",
   "pn": "",
   "abstract": [
    "The paper describes the platform for building spoken language systems being designed and implemented within the EU-language engineering project REWARD. The platform collects and streamlines a set of software tools such that they together constitute the basic modules needed to enable dialogue developers to establish new dialogue applications with only minimal knowledge outside their own field of experience and within a minimum amount of time. The system differs from other platforms, as non-expert users have been strongly involved in the design phase.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-80"
  },
  "bull98_icslp": {
   "authors": [
    [
     "Matthew",
     "Bull"
    ],
    [
     "Matthew",
     "Aylett"
    ]
   ],
   "title": "An analysis of the timing of turn-taking in a corpus of goal-oriented dialogue",
   "original": "i98_0790",
   "page_count": 4,
   "order": 81,
   "p1": "paper 0790",
   "pn": "",
   "abstract": [
    "This paper presents a context-based analysis of the intervals between different speakers' utterances in a corpus of task-oriented dialogue (the Human Communication Research Centre's Map Task Corpus). In the analysis, we assessed the relationship between inter-speaker intervals and various contextual factors, such as the effects of eye contact, the presence of conversational game boundaries, the category of move in an utterance, and the degree of experience with the task in hand. The results of the analysis indicated that the main factors which gave rise to significant differences in inter-speaker intervals were those which related to decision-making and planning - the greater the amount of planning, the greater the inter-speaker interval. Differences between speakers were also found to be significant, although this effect did not necessarily interact with all other effects. These results provide unique and useful data for the improved effectiveness of dialogue systems.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-81"
  },
  "davies98_icslp": {
   "authors": [
    [
     "Sarah",
     "Davies"
    ],
    [
     "Massimo",
     "Poesio"
    ]
   ],
   "title": "The provision of corrective feedback in a spoken dialogue CALL system",
   "original": "i98_0813",
   "page_count": 4,
   "order": 82,
   "p1": "paper 0813",
   "pn": "",
   "abstract": [
    "In this paper we report on the development of a spoken dialogue system for computer aided language learning (CALL), and explore some of the issues involved in the incorporation of a corrective feedback module. We initially developed a small prototype system, and tested it for usability with visiting students of English as a foreign language. In the light of the positive results we obtained for this, we began to develop a more advanced system, with the aim of investigating how spoken dialogue systems might best be tailored to help language learning. The issue we focussed on was the kind of feedback on errors which might be most useful to the learner. We show the types of feedback we have considered, and highlight some of the problems associated with providing different types of feedback.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-82"
  },
  "devillers98_icslp": {
   "authors": [
    [
     "Laurence",
     "Devillers"
    ],
    [
     "Helene",
     "Bonneau-Maynard"
    ]
   ],
   "title": "Evaluation of dialog strategies for a tourist information retrieval system",
   "original": "i98_0378",
   "page_count": 4,
   "order": 83,
   "p1": "paper 0378",
   "pn": "",
   "abstract": [
    "In this paper, we describe the evaluation of the dialog management and response generation strategies being developed for retrieval of touristic information, selected as a common domain for the ARC-AUPELF-B2-action. Comparing and evaluating different strategies is a difficult task, which often remains unexplored, because in most cases evaluation approaches require a unified database structure and efficient integration of data from several disparate sources and forms. To avoid this problem, we implemented two dialog strategy versions within the same general platform. We investigate qualitative and quantitative criteria for evaluation of these dialog control strategies: in particular, by testing the efficiency of our system with and without automatic mechanisms for guiding the user via suggestive prompts. An evaluation phase has been carried out to assess the utility of guiding the user with 32 naive and experienced subjects. The experiments show that user guidance is appropriate for novices and appreciated by all users.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-83"
  },
  "furui98_icslp": {
   "authors": [
    [
     "Sadaoki",
     "Furui"
    ],
    [
     "Koh'ichiro",
     "Yamaguchi"
    ]
   ],
   "title": "Designing a multimodal dialogue system for information retrieval",
   "original": "i98_0036",
   "page_count": 4,
   "order": 84,
   "p1": "paper 0036",
   "pn": "",
   "abstract": [
    "This paper introduces a paradigm for designing multimodal dialogue systems. An example system task of the system is to retrieve particular information about different shops in the Tokyo Metropolitan area, such as their names, addresses and phone numbers. The system accepts speech and screen touching as input, and presents retrieved information on a screen display. The speech recognition part is modeled by the FSN (finite state network) consisting of keywords and fillers, both of which are implemented by the DAWG (directed acyclic word-graph) structure. The number of keywords is 306, consisting of district names and business names. The fillers accept roughly 100,000 non-keywords/phrases occuring in spontaneous speech. A variety of dialogue strategies are designed and evaluated based on an objective cost function having a set of actions and states as parameters. Expected dialogue cost is calculated for each strategy, and the best strategy is selected according to the keyword recognition accuracy.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-84"
  },
  "guan98_icslp": {
   "authors": [
    [
     "Dinghua",
     "Guan"
    ],
    [
     "Min",
     "Chu"
    ],
    [
     "Quan",
     "Zhang"
    ],
    [
     "Jian",
     "Liu"
    ],
    [
     "Xiangdong",
     "Zhang"
    ]
   ],
   "title": "The research project of man-computer dialogue system in Chinese",
   "original": "i98_0245",
   "page_count": 4,
   "order": 85,
   "p1": "paper 0245",
   "pn": "",
   "abstract": [
    "This paper gives a brief introduction about the five-year research project of \"Man-Computer Dialogue System in Chinese\", which was supported by the Chinese Academy of Sciences. The project is carried out in two steps. In the first step, research works undertook by several research groups separately on the core area such as speech recognition, speech synthesis, language understanding and dialogue organizing module. And in the second step, all techniques are assembled together to form a demo dialogue system of traveling information inquiry system. The current state of all above core areas and some evaluation results are discussed in the first part of this paper and the framework of the traveling information inquiry system is presented in the second part.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-85"
  },
  "hone98_icslp": {
   "authors": [
    [
     "Kate S.",
     "Hone"
    ],
    [
     "David",
     "Golightly"
    ]
   ],
   "title": "Interfaces for speech recognition systems: the impact of vocabulary constraints and syntax on performance",
   "original": "i98_0519",
   "page_count": 4,
   "order": 86,
   "p1": "paper 0519",
   "pn": "",
   "abstract": [
    "An experiment was conducted to investigate the effects of vocabulary constraints and syntax on human interactions with a speech interactive system. Three dialogue styles for a telephone banking application, all using constrained vocabularies, were compared: yes/no, menu and query prompts. These styles differ both in the degree of vocabulary constraint, and in how that constraint is communicated to the user. It was found that although i t involved more dialogue steps the yes/no interaction style was the most effective in terms of both task completion rates and performance time. The query strategy was least preferred by users.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-86"
  },
  "iwase98_icslp": {
   "authors": [
    [
     "Tatsuya",
     "Iwase"
    ],
    [
     "Nigel",
     "Ward"
    ]
   ],
   "title": "Pacing spoken directions to suit the listener",
   "original": "i98_0224",
   "page_count": 4,
   "order": 87,
   "p1": "paper 0224",
   "pn": "",
   "abstract": [
    "To make human-computer dialog as `natural' as human-human dialog requires paying attention to the timing of utterances. This is done with reference to responses from the listener, in particular back-channel feedback, questions and mumbles. On the basis of corpus analysis, We have made direction-giving dialog system which adjust the pace of dialog using only prosodic information without using speech recognition; no word recognition was used. We contrived a method to evaluate a dialog system talking to human naturally with prosodic information. To evaluate the naturalness of dialog made by our system, we made three experiment with 10 subjects each. The system accomplished natural dialog, and most of subjects weren't aware that it was a computer. This fact, that reasonably good performance was obtained by paying attention to prosodic information alone, indicates the utility of using prosody in producing appropriate timing in dialog. This confirms a commonly held belief.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-87"
  },
  "flychteriksson98_icslp": {
   "authors": [
    [
     "Annika",
     "Flycht-Eriksson"
    ],
    [
     "Arne",
     "Jonsson"
    ]
   ],
   "title": "A spoken dialogue system utilizing spatial information",
   "original": "i98_0479",
   "page_count": 4,
   "order": 88,
   "p1": "paper 0479",
   "pn": "",
   "abstract": [
    "Spatial reasoning plays an important role in many spoken dialogue systems. One application area where it is especially important is timetable information for local bus traffic. Users of such systems often request information based on vague spatial descriptions and a usable system must be able to handle this. We have extended a dialogue system with abilities to transform vague spatial expressions into a form that can be used to access the information base. In our approach we use the power of a Geographical Information System (GIS) for the spatial reasoning.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-88"
  },
  "kamm98_icslp": {
   "authors": [
    [
     "Candace A.",
     "Kamm"
    ],
    [
     "Diane J.",
     "Litman"
    ],
    [
     "Marilyn A.",
     "Walker"
    ]
   ],
   "title": "From novice to expert: the effect of tutorials on user expertise with spoken dialogue systems",
   "original": "i98_0883",
   "page_count": 4,
   "order": 89,
   "p1": "paper 0883",
   "pn": "",
   "abstract": [
    "One challenge for current spoken dialogue systems is how to make the limitations of the system (vocabulary, grammar, and application domain) apparent to users. This study explored the use of a 4-minute tutorial session to acquaint novice users with features of a spoken dialogue system for accessing email. On three scenario-based tasks, novice users who had the tutorial had task completion times and user satisfaction ratings that were comparable to those of expert users. Novices who did not experience the tutorial had significantly longer task completion times on the initial task, but similar completion times to the tutorial group on the final task. User satisfaction ratings of the no-tutorial group were consistently lower than the ratings of the other two groups. Evaluation using the PARADISE framework indicated that perceived task completion, mean recognition score, and number of help requests were significant predictors of user satisfaction with the system.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-89"
  },
  "kawabata98_icslp": {
   "authors": [
    [
     "Takeshi",
     "Kawabata"
    ]
   ],
   "title": "Emergent computational dialogue management architecture for task-oriented spoken dialogue systems",
   "original": "i98_0143",
   "page_count": 4,
   "order": 90,
   "p1": "paper 0143",
   "pn": "",
   "abstract": [
    "This paper proposes a new dialogue management architecture for human-machine speech communication systems. In our daily speech communication, incremental, non-deterministic and quick-response behaviors are required for effortless information interchange. Emergent computational architectures, proposed in the robot control domain, are promising to enable such features. The dialogue manager (ECL-DIALOG) consists of multiple \"phrase pattern\" detectors as input sensors. The CFG driven phrase detectors search for phrase patterns in user utterances and generates numerous emergent slot-filling signals. The system integrates them according to their \"phrase pattern\" priorities and updates the current task-completion context. When a slot value is updated, the system generates an appropriate response. For example, when the system finds a new slot value from user utterances, the system generates a chiming utterance \"yeah\". When the context slot is replaced by a different value \"Tuesday\" that has a lower priority, the system asks for confirmation \"On Tuesday?\".\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-90"
  },
  "kumamoto98_icslp": {
   "authors": [
    [
     "Tadahiko",
     "Kumamoto"
    ],
    [
     "Akira",
     "Ito"
    ]
   ],
   "title": "An analysis of dialogues with our dialogue system through a WWW page",
   "original": "i98_0493",
   "page_count": 4,
   "order": 91,
   "p1": "paper 0493",
   "pn": "",
   "abstract": [
    "Many researchers have been developing natural language dialogue systems as a human-friendly man-machine interface. The human factors in a man-machine dialogue, however, are not obvious enough to understand with regard to how people talk with a dialogue system. 141 dialogues which our dialogue system had in DiaLeague '97 were analyzed at the utterance and dialogue levels, where DiaLeague '97 was the second dialogue contest in which a dialogue system engaged in a dialogue with a human in order to solve a specific problem. For the analyses at the utterance level, we investigated the users' speaking styles, the richness of the users' utterances in a variety of surface patterns, and the influence of the system's utterance pattern on the users' utterance. For the analyses at the dialogue level, we investigated the instances of confusion observed in the 141 dialogues and we also show how the users behaved when the confusion occurred.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-91"
  },
  "mctear98_icslp": {
   "authors": [
    [
     "Michael F.",
     "McTear"
    ]
   ],
   "title": "Modelling spoken dialogues with state transition diagrams: experiences with the CSLU toolkit",
   "original": "i98_0545",
   "page_count": 4,
   "order": 92,
   "p1": "paper 0545",
   "pn": "",
   "abstract": [
    "The development of a spoken dialogue system is a complex process involving the integration of several component technologies. Various toolkits and authoring environments have been produced that provide assistance with this process. This paper reports on several projects involving CSLU's RAD (Rapid Application Developer) and critically evaluates the applicability of state transition diagrams for modelling different types of spoken dialogue. State transition methods have been recommended for dialogues that involve well-structured tasks that can be mapped directly on to a dialogue structure. However, other significant factors to be considered include the structure of the information to be transacted and the need for verification of the user's input as determined by the system's level of recognition accuracy. Examples of different types of dialogue are presented together with recommendations concerning the advantages and disadvantages of state transition based dialogue control.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-92"
  },
  "okada98_icslp": {
   "authors": [
    [
     "Michio",
     "Okada"
    ],
    [
     "Noriko",
     "Suzuki"
    ],
    [
     "Jacques",
     "Terken"
    ]
   ],
   "title": "Situated dialogue coordination for spoken dialogue systems",
   "original": "i98_0801",
   "page_count": 4,
   "order": 93,
   "p1": "paper 0801",
   "pn": "",
   "abstract": [
    "In this paper, we present a general framework and architecture for maintaining dialogue coordination in spoken dialogue systems, in which intended behaviors and goals are incrementally performed during the course of maintaining dialogue coordination. The dialogue structure emerges as a result from interaction between user and the dialogue system. The key feature of this design for the systems is to use multiple situated-agents for coordinating communicative acts that are realized as a hierarchy of autonomous behaviors by using a subsumption architecture. In this architecture it should be noted that the lower-level behaviors act autonomously for maintaining the dialogue coordination and are linked to the specifications from higher-level behaviors for dialogue management. In order to make the behavior of the system social, in general, the maintaining of dialogue coordination takes priority over the realization of intended goals of the system as a dialogue participant. We introduce an under-specification strategy for controlling the preference of the concurrent behaviors. This is in contrast to the classical, top-down approach to dialogue coordination.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-93"
  },
  "pouteau98_icslp": {
   "authors": [
    [
     "Xavier",
     "Pouteau"
    ],
    [
     "Luis",
     "Arevalo"
    ]
   ],
   "title": "Robust spoken dialogue systems for consumer products: a concrete application",
   "original": "i98_0368",
   "page_count": 4,
   "order": 94,
   "p1": "paper 0368",
   "pn": "",
   "abstract": [
    "In this paper, we report the significant results of a fully-implemented voice operated dialogue system, and particularly its main component: the Dialogue Manager (DM). Just like for other interfaces, spoken interfaces require a well-conducted design, implying a good analysis of the users' needs throughout the dialogue. The VODIS project 1 has led to the design and development of a spoken interface for the control of car equipment. Due to the workload caused by the task of driving the vehicle, spoken communication provides a potentially safe and efficient mode of operating the car equipment. To achieve this, we present the main characteristics of the task model specified during the design stage, and show how its specific features related to the spoken communication allowed to implement a robust dialogue.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-94"
  },
  "willett98_icslp": {
   "authors": [
    [
     "Daniel",
     "Willett"
    ],
    [
     "Arno",
     "Romer"
    ],
    [
     "J√∂rg",
     "Rottland"
    ],
    [
     "Gerhard",
     "Rigoll"
    ]
   ],
   "title": "A German dialogue system for scheduling dates and meetings by naturally spoken continuous speech",
   "original": "i98_0524",
   "page_count": 4,
   "order": 95,
   "p1": "paper 0524",
   "pn": "",
   "abstract": [
    "In this paper, we present the basic design principles and architecture of a dialogue system for scheduling appointments. This mixed-initiative dialogue system integrates an automatic speaker-independent speech recognition engine for continuously spoken German, a speech synthesizer and a scheduler database application to build up a scheduler that is purely driven by natural continuous speech and thus, does not need any visual display device. With these properties it is a prototype for a speech driven palm-size computer application and could be integrated in miniature computers that come along with no display device at all.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-95"
  },
  "wu98_icslp": {
   "authors": [
    [
     "Chung-Hsien",
     "Wu"
    ],
    [
     "Gwo-Lang",
     "Yan"
    ],
    [
     "Chien-Liang",
     "Lin"
    ]
   ],
   "title": "Spoken dialogue system using corpus-based hidden Markov model",
   "original": "i98_0219",
   "page_count": 4,
   "order": 96,
   "p1": "paper 0219",
   "pn": "",
   "abstract": [
    "In a spoken dialogue system, the intention is the most important component for speech understanding. In this paper, we propose a corpus-based hidden Markov model (HMM) to model the intention of a sentence. Each intention is represented by a sequence of word segment categories determined by a task-specific lexicon and a corpus. In the training procedure, five intention HMM's are defined, each representing one intention in our approach. In the intention identification process, the phrase sequence is fed to each intention HMM. Given a speech utterance, the Viterbi algorithm is used to find the most likely intention sequences. The intention HMM considers not only the phrase frequency but also the syntactic and semantic structure in a phrase sequence. In order to evaluate the proposed method, a spoken dialogue model for air travel information service is investigated. The experiments were carried out using a test database from 25 speakers (15 male and 10 female). There are 120 dialogues, which contain 725 sentences in the test database. The experimental results show that the correct response rate can achieve about 80.3% using intention HMM.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-96"
  },
  "wyard98_icslp": {
   "authors": [
    [
     "Peter",
     "Wyard"
    ],
    [
     "Gavin",
     "Churcher"
    ]
   ],
   "title": "A realistic wizard of oz simulation of a multimodal spoken language system",
   "original": "i98_0556",
   "page_count": 4,
   "order": 97,
   "p1": "paper 0556",
   "pn": "",
   "abstract": [
    "This paper describes a Wizard of Oz (WOZ) system that allows the realistic simulation of a multimodal spoken language system. A Wizard protocol has been drawn up which means that the WOZ system will simulate the limitations of an automatic system rather than allow the user to engage in the full range of human-human dialogue. In support of this protocol is a sophisticated Wizard response panel and underlying response generation functionality. This enables the Wizard to respond to complex multimodal inputs in near real-time. The chosen application is a 3D retail service, in which users can select furnishings from a database according to colour, pattern, fabric type, etc., transfer furnishings to objects in a virtual showroom, ask about prices and matching of fabrics, etc. The system includes a \"virtual assistant\", i.e. a synthetic persona which speaks the verbal system output. Users make their input by a combination of fluent speech and touchscreen input. The paper describes a formal trial carried out with the WOZ system, and discusses the results.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-97"
  },
  "yang98_icslp": {
   "authors": [
    [
     "Yen-Ju",
     "Yang"
    ],
    [
     "Lin-Shan",
     "Lee"
    ]
   ],
   "title": "A syllable-based Chinese spoken dialogue system for telephone directory services primarily trained with a corpus",
   "original": "i98_0528",
   "page_count": 4,
   "order": 98,
   "p1": "paper 0528",
   "pn": "",
   "abstract": [
    "This paper presents a syllable-based Chinese spoken dialogue system for telephone directory services primarily trained with a corpus. It integrates automatic phrase extraction, robust phrase spotting, statistics-based semantic parsing by phrase-concept joint language model as well as concept-based dialogue model, and intention identification by probabilistic finite state network to form a speech intention estimator. By applying the proposed techniques, the concept sequence with the maximum a-posteriori (MAP) probability based on intra and inter sentence consideration conveyed in the user's speech sentence, i.e. the speaker's intention, can be identified. This approach is convenient to be trained by a given corpus and flexible to be ported to different dialogue tasks. Incorporate a mixed-initiative goal-oriented dialogue manager, we have successfully developed a dialogue system for telephone directory service. Very promising results have been obtained in on-line tests.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-98"
  },
  "yano98_icslp": {
   "authors": [
    [
     "Hiroyuki",
     "Yano"
    ],
    [
     "Akira",
     "Ito"
    ]
   ],
   "title": "How disagreement expressions are used in cooperative tasks",
   "original": "i98_0719",
   "page_count": 4,
   "order": 99,
   "p1": "paper 0719",
   "pn": "",
   "abstract": [
    "Analysis was made of disagreement expressions in dialogues recorded in a cooperative task experiment. A disagreement expression is defined as the latter utterance of consecutive utterances, which shows disagreement with the former. Subjects used two types of disagreement expressions: to the partner's utterance, and to their own. These were classified into three subtypes according to part of speech: conjunction, interjection, and content word. The role of disagreement expressions in cooperative tasks was examined. It was found that subjects used disagreement expressions suitable to the occasion to maintain good relation with their partners. It was concluded that using expressions that disagree with one's own previous utterance is an effective strategy for expressing an opinion for which one lacks adequate evidence and for eliciting utterances from one's partner.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-99"
  },
  "rose98_icslp": {
   "authors": [
    [
     "Phil",
     "Rose"
    ]
   ],
   "title": "Tones of a tridialectal: acoustic and perceptual data on ten linguistic tonetic contrasts between lao, nyo and standard Thai",
   "original": "i98_0297",
   "page_count": 4,
   "order": 100,
   "p1": "paper 0297",
   "pn": "",
   "abstract": [
    "One way of investigating how tone languages and dialects differ with respect to their tonal phonetics is to use bilingual or bidialectal speakers. In this paper, the tones of a speaker who has native command of not just two, but three varieties of tone language are investigated acoustically and perceptually: Standard Thai, with 5 tones; Lao, with 7; and Nyo, with 4. This makes a maximum of 16 possible Linguistic-tonetically different tones. Mean fundamental frequency and duration values are presented for the allotones of the Lao, Standard Thai and Nyo tonemes on syllables with unstopped Rhymes. A perceptual experiment is described to determine how many of the 16 allotones are identifiable by Nyo tridialectal listeners. This approach enables us in a sense to see through the ears of both native speaker and native listener. The acoustic and perceptual data are used to demonstrate the existence of 10 linguistic-tonetically distinct tones. It is speculated that this might represent an upper limit for a multidialectal speaker.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-100"
  },
  "thompson98_icslp": {
   "authors": [
    [
     "Napier Guy Ian",
     "Thompson"
    ]
   ],
   "title": "Tone sandhi between complex tones in a seven-tone southern Thai dialect",
   "original": "i98_0309",
   "page_count": 4,
   "order": 101,
   "p1": "paper 0309",
   "pn": "",
   "abstract": [
    "This paper examines the behaviour of complex tones in tone sandhi, using data from a seven-tone Southern Thai dialect. Measurements of mean fundamental frequency and duration are presented for unstopped and stopped citation allotones and also for unstopped tones in combination. Tonological implications are drawn.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-101"
  },
  "coupe98_icslp": {
   "authors": [
    [
     "Alexander Robertson",
     "Coupe"
    ]
   ],
   "title": "The acoustic and perceptual features of tone in the tibeto-burman language ao naga",
   "original": "i98_0688",
   "page_count": 4,
   "order": 102,
   "p1": "paper 0688",
   "pn": "",
   "abstract": [
    "The tonemes of the Waromung Mongsen dialect of Ao Naga, a Tibeto-Burman of northeast India, are described with respect to their auditory and acoustic features. Even though rather small FO differences are found to separate each contrasting toneme, the results of a perception test nevertheless demonstrate that these small differences are perceptually salient to a native speaker and are readily identifiable.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-102"
  },
  "rose98b_icslp": {
   "authors": [
    [
     "Phil",
     "Rose"
    ]
   ],
   "title": "The differential status of semivowels in the acoustic phonetic realisation of tone",
   "original": "i98_0298",
   "page_count": 4,
   "order": 103,
   "p1": "paper 0298",
   "pn": "",
   "abstract": [
    "This paper examines one aspect of the phonetic mapping of tone onto segmental material. It asks whether prevocalic semivowels ([j- & w-]) can be said to constitute part of the material over which tonal F0 is distributed. The distribution of acoustical correlates of tone (F0, duration) is determined on syllables differing with respect to semivowels in the segmental structure in three of the contrasting contour tones (falling, convex and low rising) from the Chinese Wu dialect of Zhenhai. Four different syllable-structures are examined. It is concluded that (1) syllable-initial semivowels are not tonally relevant, and (2) semivowels behave differentially with respect to phonetic mapping depending on whether they are preceded by a syllable-initial consonant. Implications of the finding are explored for the phonological integration of semivowels into metrical syllable structure. The relevance of the finding is also pointed out for the measurement of tone (and intonation) acoustics in running speech.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-103"
  },
  "alter98_icslp": {
   "authors": [
    [
     "Kai",
     "Alter"
    ],
    [
     "Karsten",
     "Steinhauer"
    ],
    [
     "Angela D.",
     "Friederici"
    ]
   ],
   "title": "De-accentuation: linguistic environments and prosodic realizations",
   "original": "i98_0258",
   "page_count": 4,
   "order": 104,
   "p1": "paper 0258",
   "pn": "",
   "abstract": [
    "In this paper we present a preliminary speech production study concerning the prosodic realization of the syntactic and information structure in German. Firstly, we made predictions for the relative prominence and their assignment with tonal patterns. Secondly, exhaustive acoustic analysis were used to test the expectations. The data of a production experiment with seven non-instructed normal subjects were analyzed and then compared with the data of one patient with prosodic disorders.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-104"
  },
  "amir98_icslp": {
   "authors": [
    [
     "N.",
     "Amir"
    ],
    [
     "S.",
     "Ron"
    ]
   ],
   "title": "Towards an automatic classification of emotions in speech",
   "original": "i98_0199",
   "page_count": 4,
   "order": 105,
   "p1": "paper 0199",
   "pn": "",
   "abstract": [
    "In this paper we discuss a method for extracting emotional state from the speech signal. We describe a methodology for obtaining emotive speech. and a method for identifying the emotion present in the signal. This method is based on analysis of the signal over sliding windows. and extracting a representative parameter set. A set of basic emotions is defined, and for each such emotion a reference point is computed. At each instant the distance of the measured parameter set from the reference points is calculated. and used to compute a fuzzy membership index for each emotion, which we term the \"emotional index\". Preliminary results are presented, which demonstrate the discriminative abilities of this method when applied to a number of speakers.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-105"
  },
  "schroder98_icslp": {
   "authors": [
    [
     "Marc",
     "Schr√∂der"
    ],
    [
     "V√©ronique",
     "Auberg√©"
    ],
    [
     "Marie-Agnes",
     "Cathiard"
    ]
   ],
   "title": "Can we hear smile?",
   "original": "i98_0439",
   "page_count": 4,
   "order": 106,
   "p1": "paper 0439",
   "pn": "",
   "abstract": [
    "The amusement expression is both visual and audible in speech. After recording comparable spontaneous, acted, mechanical, reiterated and seduction stimuli, five perceptual experiments were held, mainly based on the hypothesis of prosodically controlled effects of amusement on speech. Results show that audio is partially independent from video, which is as performant as audio-video. Spontaneous speech (involuntary controlled) can be identified in front of acted speech (voluntary controlled). Amusement speech can be distinguished from seduction speech.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-106"
  },
  "aylett98_icslp": {
   "authors": [
    [
     "Matthew",
     "Aylett"
    ],
    [
     "Matthew",
     "Bull"
    ]
   ],
   "title": "The automatic marking of prominence in spontaneous speech using duration and part of speech information",
   "original": "i98_0825",
   "page_count": 4,
   "order": 107,
   "p1": "paper 0825",
   "pn": "",
   "abstract": [
    "The work reported in this paper was the result of the need to label a large corpus of spontaneous, task-oriented dialogue with prosodic prominences. A computational model using only word duration, part of speech and a dictionary lookup of each word's canonical phonemic contents was trained against the results of a human coder marking prominence. Because word durations were normalised, it was possible to set a common threshold for all members of a form class above which the lexically stressed syllables were classed as prominent. The method used is presented and the relative importance of duration information, phonemic contents, syllabic context and part of speech information is explored. The automatic coder was validated against unseen material and achieved a 58% agreement with a human coder. Further investigation showed that three humans coders agreed no better with each other than each agreed with the computational model. Thus, although the automatic system did not conform very well to the performance of any one human coder, it conformed as well as another human coder might.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-107"
  },
  "kim98_icslp": {
   "authors": [
    [
     "JongDeuk",
     "Kim"
    ],
    [
     "SeongJoon",
     "Baek"
    ],
    [
     "MyungJin",
     "Bae"
    ]
   ],
   "title": "On a pitch alteration technique in excited cepstral spectrum for high quality TTS",
   "original": "i98_1020",
   "page_count": 4,
   "order": 108,
   "p1": "paper 1020",
   "pn": "",
   "abstract": [
    "In the area of the speech synthesis techniques, the waveform coding methods maintain the intelligibility and naturalness of synthetic speech. In order to apply the waveform coding or hybrid coding techniques to synthesis by rule, we must be able to alter the pitches of synthetic speech. In this paper, we propose a new pitch alteration method that minimizes the spectrum distortion by using the behavior of cepstrum. This method splits the spectrum of speech signal into excitation spectrum and formant spectrum and transforms the excitation spectrum into cepstrum domain. The pitch of excitation cepstrum is altered by zero insertion or zero deletion and the pitch altered spectrum is reconstructed in spectrum domain. As a result of performance test, the average spectrum distortion was below 2.29% while that of conventional method is 2.47%.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-108"
  },
  "buckow98_icslp": {
   "authors": [
    [
     "Jan",
     "Buckow"
    ],
    [
     "Anton",
     "Batliner"
    ],
    [
     "Richard",
     "Huber"
    ],
    [
     "Elmar",
     "N√∂th"
    ],
    [
     "Volker",
     "Warnke"
    ],
    [
     "Heinrich",
     "Niemann"
    ]
   ],
   "title": "Dovetailing of acoustics and prosody in spontaneous speech recognition",
   "original": "i98_0336",
   "page_count": 4,
   "order": 109,
   "p1": "paper 0336",
   "pn": "",
   "abstract": [
    "In VERBMOBIL we previously augmented the output of a word recognizer with prosodic information. Here we present a new approach of interleaving word recognition and prosodic processing. While we still use the output of a word recognizer to determine phrase boundaries, we do not wait until the end of the utterance before we start processing. Instead we intercept chunks of word hypotheses during the forward search of the recognizer. Neural networks and language models are used to predict phrase boundaries. Those boundary hypotheses, in turn, are used by the recognizer to cut the stream of incoming speech into syntactic-prosodic phrases. Thus, incremental processing is possible. We investigate which features are suited for incremental prosodic processing and compare them w.r.t. classification performance and efficiency. We show that with a set of features that can be computed efficiently classification results are achieved which are almost as good as those with the previously used computationally more expensive features.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-109"
  },
  "cahn98_icslp": {
   "authors": [
    [
     "Janet E.",
     "Cahn"
    ]
   ],
   "title": "A computational memory and processing model for prosody",
   "original": "i98_0991",
   "page_count": 4,
   "order": 110,
   "p1": "paper 0991",
   "pn": "",
   "abstract": [
    "This paper links prosody to the information in the text and how it is processed by the speaker. It describes the operation and output of Loq, a text-to-speech implementation that includes a model of limited attention and working memory. Attentional limitations are key. Varying the attentional parameter in the simulations varies in turn what counts as given and new in a text, and therefore, the intonational contours with which it is uttered. Currently, the system produces prosody in three different styles: child-like, adult expressive, and knowledgeable. This prosody also exhibits differences within each style -- no two simulations are alike. The limited resource approach captures some of the stylistic and individual variety found in natural prosody.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-110"
  },
  "collins98_icslp": {
   "authors": [
    [
     "Belinda",
     "Collins"
    ]
   ],
   "title": "Convergence of fundamental frequencies in conversation: if it happens, does it matter?",
   "original": "i98_0695",
   "page_count": 4,
   "order": 111,
   "p1": "paper 0695",
   "pn": "",
   "abstract": [
    "This paper explores the existence and nature of accommodation processes within conversation, particularly convergence of fundamental frequency (Fo) of conversational participants over time. The study raises a nuumber of issues related to methodologies for analysing interactional (typically conversational) data. Most important is the issue of the applicability of statistical sampling methods which are independent of the interactional events occurring within the talk. It concludes with suggestions for a methodology that examines long term acoustic phenomena (long term Fo) and relates events at the micro acoustic level to interactional events within a conversation.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-111"
  },
  "fujisaki98_icslp": {
   "authors": [
    [
     "Hiroya",
     "Fujisaki"
    ],
    [
     "Sumio",
     "Ohno"
    ],
    [
     "Takashi",
     "Yagi"
    ],
    [
     "Takeshi",
     "Ono"
    ]
   ],
   "title": "Analysis and interpretation of fundamental frequency contours of british English in terms of a command-response model",
   "original": "i98_0830",
   "page_count": 4,
   "order": 112,
   "p1": "paper 0830",
   "pn": "",
   "abstract": [
    "In order to test the validity of authors' command-response model for the generation of F0 contours in the analysis and interpretation of F0 contours of British English, F0 contours of utterances containing sections that are not usually found in those of the common Japanese are analyzed. The results indicate that these F0 contours can actually be generated by selecting appropriate patterns of commands for phrase and accent in a way that is specific to British English, while using the same model for the mechanism of F0 control as for the common Japanese.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-112"
  },
  "holm98_icslp": {
   "authors": [
    [
     "Frode",
     "Holm"
    ],
    [
     "Kazue",
     "Hata"
    ]
   ],
   "title": "Common patterns in word level prosody",
   "original": "i98_1038",
   "page_count": 4,
   "order": 113,
   "p1": "paper 1038",
   "pn": "",
   "abstract": [
    "The task of generating natural human-sounding prosody for text-to-speech (TTS) has historically been one of the most challenging problems that researchers and developers have had to face. TTS systems have in general become infamous for their \"robotic\" intonations. This paper describes an approach to this problem which endeavors to capture as much detail as possible from speech data, but in a way that avoids the \"black boxes\" typical of neural networks and some vector clustering algorithms. Unlike these latter methods, our approach may give feedback as to exactly what the crucial parameters are that determine the successful choice of pattern. Focusing on the notion of prosody templates, we confirmed that a representative F0 and duration pattern can be extracted based on stress pattern for a target proper noun which occurs in sentence-initial position.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-113"
  },
  "horiuchi98_icslp": {
   "authors": [
    [
     "Yasuo",
     "Horiuchi"
    ],
    [
     "Akira",
     "Ichikawa"
    ]
   ],
   "title": "Prosodic structure in Japanese spontaneous speech",
   "original": "i98_0500",
   "page_count": 4,
   "order": 114,
   "p1": "paper 0500",
   "pn": "",
   "abstract": [
    "In this paper, we introduce a method of generating a prosodic tree structure from the F0 contour of an utterance in order to analyze the information expressed by prosody in Japanese spontaneous dialogue. The connection rate, which means the strength of the relationship between two prosodic units, is defined. By repeatedly combining the two adjacent prosodic units where the rate is high, the tree structure is gradually generated. To determine the parameters objectively, we applied the principal component analysis to 32 dialogues from the Chiba Map Task Dialogue Corpus. Then we applied our method to one dialogue. The results suggested that the prosodic tree based on the first principal component was concerned with the information telling what the speaker wanted to do next and that the prosodic tree based on the second principal component represented the syntactic and grammatical structure.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-114"
  },
  "ishihara98_icslp": {
   "authors": [
    [
     "Shunichi",
     "Ishihara"
    ]
   ],
   "title": "An acoustic-phonetic description of word tone in kagoshima Japanese",
   "original": "i98_0628",
   "page_count": 4,
   "order": 115,
   "p1": "paper 0628",
   "pn": "",
   "abstract": [
    "The Japanese dialect of Kagoshima (KJ) has two different surface pitch patterns, (L)0 HL and (L)0 H. In this study, the properties of these two surface pitch patterns of KJ will be acoustically-phonetically described by means of z-score normalisation. Words consisting of two, three, four and five syllables were used in this study (the syllable structure is a CV) as test words, and the F0 of each syllable nucleus was extracted and the raw F0 values were z-score normalised. Two native speakers of KJ (one male and one female) participated in this study. The tonal representation of KJ words will be discussed on the basis of the z-score normalised results.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-115"
  },
  "iwano98_icslp": {
   "authors": [
    [
     "Koji",
     "Iwano"
    ],
    [
     "Keikichi",
     "Hirose"
    ]
   ],
   "title": "Representing prosodic words using statistical models of moraic transition of fundamental frequency contours of Japanese",
   "original": "i98_0731",
   "page_count": 4,
   "order": 116,
   "p1": "paper 0731",
   "pn": "",
   "abstract": [
    "We have formerly proposed a statistical model of moraic transitions of fundamental frequency (F0) contours and showed its effectiveness for prosodic boundary detection and accent type recognition. This model represented F0 contours of prosodic words to simultaneously detect and recognize prosodic word boundaries and accent types. This paper proposes a method where prosodic word F0 contours are modeled separately according to their accent types and presence/absence of succeeding pauses. An utterance is regarded as a sequence of prosodic words under a simple grammar. Each moraic F0 contour is represented by a pair of codes; the original shape code and the newly introduced delta code representing the degree of F0 change between the mora in question and its preceding mora. Compared with earlier results, the boundary detection rate improves from 87.7% to 91.5%. Accent type recognition rate reached 76.0% (type 1 accent discrimination).\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-116"
  },
  "jang98_icslp": {
   "authors": [
    [
     "Tae-Yeoub",
     "Jang"
    ],
    [
     "Minsuck",
     "Song"
    ],
    [
     "Kiyeong",
     "Lee"
    ]
   ],
   "title": "Disambiguation of Korean utterances using automatic intonation recognition",
   "original": "i98_0547",
   "page_count": 4,
   "order": 117,
   "p1": "paper 0547",
   "pn": "",
   "abstract": [
    "The paper describes a research on a use of intonation for disambiguating utterance types of Korean spoken sentences. Based on tilt intonation theory (Taylor and Black 1994), two related but separate experiments were performed at speaker independent level, both using the Hidden Markov Model training technique. In the first experiment, a system is established so that rough boundary positions of major intonation events are detected. Subsequently the significant parameters are extracted from the products of the first experiment, which are directly used to train the final models for utterance type disambiguation. Results show that the intonation contour can be used as a significant meaning distinguisher in an automatic speech recognition system of Korean as well as in a natural human communication system.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-117"
  },
  "jokisch98_icslp": {
   "authors": [
    [
     "Oliver",
     "Jokisch"
    ],
    [
     "Diane",
     "Hirschfeld"
    ],
    [
     "Matthias",
     "Eichner"
    ],
    [
     "R√ºdiger",
     "Hoffmann"
    ]
   ],
   "title": "Multi-level rhythm control for speech synthesis using hybrid data driven and rule-based approaches",
   "original": "i98_0855",
   "page_count": 4,
   "order": 118,
   "p1": "paper 0855",
   "pn": "",
   "abstract": [
    "This paper presents: a multi-level concept to generate the speech rhythm in the Dresden TTS system for German (DreSS). The rhythm control includes the phrase, the syllabic and the phonemic level. The concept allows the alternative use of rule-based or statistical, but also data driven methods on these levels. To create the rules and to train a neural network, a new speech corpus from original speakers of the diphone-based inventories has been recorded. The corpus covers texts and single utterances and is subdivided into phrase, syllabic and phonemic databases. First results indicating that the rule-based and the train-based methods generate a comparable speech rhythm, if the databases are uniform. The stepwise duration control on several prosodic levels shows promise as a method of producing a flexible rhythm depending on the specific TTS application.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-118"
  },
  "kong98_icslp": {
   "authors": [
    [
     "Jiangping",
     "Kong"
    ]
   ],
   "title": "EGG model of ditoneme in Mandarin",
   "original": "i98_0104",
   "page_count": 8,
   "order": 119,
   "p1": "paper 0104",
   "pn": "",
   "abstract": [
    "This paper concerns with the study of EGG (electroglottalgram by laryngograph) model of ditoneme in Mandarin. The parameters for establishing models are fundamental frequency (F0), which is regarded as reference, speed quotient and open quotient, which are all extracted from the EGG signal by using the software EGG.exe, an option of CSL, Model 4300B, KAY. The result shows that speed quotient and open quotient have close relationships with the F0 in different ditonemes. In general, speed quotient and open quotient will decrease, when F0 increases in sustained vowels. But in the ditonemes, speed quotient and open quotient show different natures according to the position and environment. The conclusion is that EGG models of ditonemes are composed of the patterns of F0, speed quotient and open quotient in Mandarin.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-119"
  },
  "krishnan98_icslp": {
   "authors": [
    [
     "Geetha",
     "Krishnan"
    ],
    [
     "Wayne",
     "Ward"
    ]
   ],
   "title": "Temporal organization of speech for normal and fast rates",
   "original": "i98_0930",
   "page_count": 4,
   "order": 120,
   "p1": "paper 0930",
   "pn": "",
   "abstract": [
    "In this study predictors of speech rate that are sensitive to local and global rate changes, and relevant to different types of speakers, were examined. Two groups of subjects, normal and disfluent speakers (whose speech was clinically rated as \"slow\"), provided speech samples at normal and fast rates. Samples were segmented into interstress intervals (ISI) of varying length (i.e., varying number of syllables). The compressibility of components within ISIs of varying length provided information on local rate control strategies. The fast speech samples were useful for examining strategies used in global rate increases. Stressed vowels and intervowel intervals (IVI) showed similar trends in compression for both speakers, for local and global rate increases. We then investigated two measures of speech rate based on intervowel intervals: the ratio measure (IVI/ISI) and the average IVI. High correlation of average IVI with phone rate was found. Results of speech rate estimations are presented.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-120"
  },
  "kubozono98_icslp": {
   "authors": [
    [
     "Haruo",
     "Kubozono"
    ]
   ],
   "title": "A syllable-based generalization of Japanese accentuation",
   "original": "i98_0105",
   "page_count": 4,
   "order": 121,
   "p1": "paper 0105",
   "pn": "",
   "abstract": [
    "One of the major findings of the recent linguistic research on Japanese is that the syllable plays a pivotal role in a variety of phonological and morphological phenomena in the mora-based prosodic system of this language. This paper attempts to reinforce this argument by proposing a significant generalization of Japanese accentuation in terms of `syllable weight', an idea that each syllable has a certain weight according to its phonological configuration. Specifically, this analysis reveals that Japanese accentuation is strikingly similar to that of Latin and many languages with a Latin-type accent system, e.g. English. Moreover, a sociolinguistic analysis of the accentual changes currently in progress demonstrates that Japanese accentuation is becoming increasingly similar to the Latin-type accent system, where the syllable plays a primary role.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-121"
  },
  "lee98c_icslp": {
   "authors": [
    [
     "Hyuck-Joon",
     "Lee"
    ]
   ],
   "title": "Non-adjacent segmental effects in tonal realization of accentual phrase in seoul Korean",
   "original": "i98_0903",
   "page_count": 4,
   "order": 122,
   "p1": "paper 0903",
   "pn": "",
   "abstract": [
    "This paper investigates the degree to which an onset consonant of an accentual phrase affects the f0 of the following syllables within the phrase in Seoul Korean. Korean tense or aspirated onset consonants raise f0 values of the following adjacent vowel, and when they are positioned on the first syllable onset of an accentual phrase, they continuously raise f0 values of the following non-adjacent vowels. This f0 raising after aspirated or tense consonants supports the previous claim that the microprosody in Korean is phonologized in phrase initial position. The results also confirms the previous claim regarding the location of the underlying 4 tones of the accentual phrase and the interpolation hypothesis.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-122"
  },
  "lopez98_icslp": {
   "authors": [
    [
     "Eduardo",
     "L√≥pez"
    ],
    [
     "Javier",
     "Caminero"
    ],
    [
     "Ismael",
     "Cortazar"
    ],
    [
     "Luis A.",
     "Hern√°ndez"
    ]
   ],
   "title": "Improvement on connected numbers recognition using prosodic information",
   "original": "i98_0353",
   "page_count": 4,
   "order": 123,
   "p1": "paper 0353",
   "pn": "",
   "abstract": [
    "In this paper we propose a strategy to improve the performance of a connected number recognition system in Spanish using prosodic information. Prosodic information is included as the detection of pitch movements between what some studies of intonation in Spanish called melodic units. The basic linguistic background of our approach together with the specific strategies to detect and correct ambiguities and recognition errors are discussed. Experimental results show a 16% of reduction in recognition errors for our state-of-the-art connected number recognizer, and the possibility to solve ambiguities unable to be considered by the recognizer.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-123"
  },
  "maeda98_icslp": {
   "authors": [
    [
     "Kazuaki",
     "Maeda"
    ],
    [
     "Jennifer J.",
     "Venditti"
    ]
   ],
   "title": "Phonetic investigation of boundary pitch movements in Japanese",
   "original": "i98_0800",
   "page_count": 4,
   "order": 124,
   "p1": "paper 0800",
   "pn": "",
   "abstract": [
    "Pitch movements at the boundaries of sentence-medial and final phrases in Japanese can provide a cue to the speaker's intention. For example, the phrase /Nagano-de/ 'in Nagano' can be uttered with different rising and/or falling pitch movements on the the final mora /de/ to convey meanings such as clarification, incredulity, prominence in the discourse, insistence, etc. The identification of these movements is important not only for spoken language understanding systems, but also for natural-sounding speech synthesis. The current study examines F0 shape, height, and alignment characteristics of four distinct sentence-medial boundary rises. We compare these types with accented and focused unaccented words containing identical phonetic segments, and discuss a number of different possible phonological analyses of the pitch movements.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-124"
  },
  "maekawa98_icslp": {
   "authors": [
    [
     "Kikuo",
     "Maekawa"
    ]
   ],
   "title": "Phonetic and phonological characteristics of paralinguistic information in spoken Japanese",
   "original": "i98_0997",
   "page_count": 5,
   "order": 125,
   "p1": "paper 0997",
   "pn": "",
   "abstract": [
    "Three Japanese sentences were uttered repeatedly by three speakers with paralinguistic information indicating A(dmiration), D(issapointment), F(ocus), I(ndifference), S(uspicion), and N(eutral). A perception test using all recorded materials revealed that the average correct perception rate was higher than 80 percent. Acoustic analyses revealed the following phonetic characteristics: - Considerable elongation of utterance duration in types A, D, and S. - The first and last morae were more elastic in duration than the others. - Narrowed pitch range in type D and enlarged pitch range in A, F, and S. - Characteristic low pitch at the beginning of types A, D and S. - Delayed accentual peak location in types A, D, and S. - Systematic distributional difference of sentence-final vowels on the F1-F2 formant plane. - Seeming 'Laryngealization' in the initial low-pitched portion of types S, A, and D.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-125"
  },
  "maghbouleh98_icslp": {
   "authors": [
    [
     "Arman",
     "Maghbouleh"
    ]
   ],
   "title": "ToBI accent type recognition",
   "original": "i98_0632",
   "page_count": 4,
   "order": 126,
   "p1": "paper 0632",
   "pn": "",
   "abstract": [
    "This paper describes work in progress for recognizing a subset of ToBI intonation labels (H*, L+H*, L*, !H*, L+!H*, no accent). Initially, duration characteristics are used to classify syllables as accented or not. The accented syllables are then subclassified based on fundamental frequency, F0, values. Potential F0 intonation gestures are schematized by connected line segments within a window around a given syllable. The schematizations are found using spline-basis linear regression. The regression weights on F0 points are varied in order to discount segmental effects and F0 detection errors. Parameters based on the line segments are then used to perform the subclassification. This paper presents new results in recognizing L*, L+H*, and L+!H* accents. In addition, the models presented here perform comparably (80% overall, and 74% accent type recognition) to models which do not distinguish bitonal accents.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-126"
  },
  "mixdorff98_icslp": {
   "authors": [
    [
     "Hansjorg",
     "Mixdorff"
    ],
    [
     "Hiroya",
     "Fujisaki"
    ]
   ],
   "title": "The influence of syllable structure on the timing of intonational events in German",
   "original": "i98_0707",
   "page_count": 4,
   "order": 127,
   "p1": "paper 0707",
   "pn": "",
   "abstract": [
    "In earlier studies the authors developed a model of German intonation based on the quantitative Fujisaki-model. The present study examines the influence of the segmental structure of an accented syllable on the timing of accent commands. It aims at developing refined alignment rules for speech synthesis. The corpus consists of 67 three-syllable words of German with word-accent on the second syllable produced by three male speakers three times. The words differ as to the structure of the second syllable. It was observed that onsets of accent commands can be most accurately predicted relative to the duration of the nuclear vowel, with variations depending on the type of consonant preceding the vowel. Accent command offsets are generally aligned with the offset of the syllable. The effectiveness of the refined timing rules was confirmed by an informal perception experiment.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-127"
  },
  "mizuno98b_icslp": {
   "authors": [
    [
     "Osamu",
     "Mizuno"
    ],
    [
     "Shin'ya",
     "Nakajima"
    ]
   ],
   "title": "New prosodic control rules for expressive synthetic speech",
   "original": "i98_1014",
   "page_count": 4,
   "order": 128,
   "p1": "paper 1014",
   "pn": "",
   "abstract": [
    "This paper proposes new prosodic feature control rules for constructing semantic prosody control. Research was conducted into mental state tendencies using tests that examined the perceptions of the subject's sensibility to the control of synthetic speech prosody. The results showed the relationships between prosodic control rules and non-verbal expressions. Duration control reflects information processing state in spoken dialogues. Sentence final pitch contour control reflects the reliability of the information. Pitch contour dynamic range control indicates the speaker's excitement. The pitch contour control from start to peak pitch contour indicates the speaker's requirement for attention. Furthermore, for the Multi-layered Speech/Sound Synthesis Control Language(MSCL) we construct prosodic feature control commands using prosodic control rules and semantic control commands using the relationships. MSCL realizes expressive synthetic speech.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-128"
  },
  "nakai98_icslp": {
   "authors": [
    [
     "Mitsuru",
     "Nakai"
    ],
    [
     "Hiroshi",
     "Shimodaira"
    ]
   ],
   "title": "The use of F0 reliability function for prosodic command analysis on F0 contour generation model",
   "original": "i98_0998",
   "page_count": 4,
   "order": 129,
   "p1": "paper 0998",
   "pn": "",
   "abstract": [
    "This paper describes a method of utilizing an ``F0 Reliability Field'' (FRF), which we have proposed in our previous work, for estimating prosodic commands on F0 contour generation model. This FRF is the time-frequency representation of F0 likelihood, and an advantage of FRF is that it is not necessary to consider F0 errors that occur during an automatic F0 determination. Therefore, it is thought that FRF can be a more useful feature for automatic prosody analyses than F0 contour, and our previous paper has reported the validity of FRF on the analysis of detecting prosodic boundaries in Japanese continuous speech. Moreover, in this paper, we have examined the validity on the prosodic command estimation of superpositional model. Experimental results show that the accuracy of command estimation with FRF is well and it is close to the accuracy of command estimation with ideal F0 contour that has no F0 error.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-129"
  },
  "ohno98_icslp": {
   "authors": [
    [
     "Sumio",
     "Ohno"
    ],
    [
     "Hiroya",
     "Fujisaki"
    ],
    [
     "Hideyuki",
     "Taguchi"
    ]
   ],
   "title": "Analysis of effects of lexical accent, syntax, and global speech rate upon the local speech rate",
   "original": "i98_0935",
   "page_count": 4,
   "order": 130,
   "p1": "paper 0935",
   "pn": "",
   "abstract": [
    "The speech rate is one of the important prosodic parameters for the naturalness and intelligibility of an utterance. On the basis of the authors' definition of the relative local speech rate, the present paper describes an analysis of the effects of changes in global speech rate, syntactic constituency and lexical accent on the local speech rate, using short utterances in which these factors are systematically controlled. Preliminary results indicate that the span of changes in local speech rate is the syllable rather than mora, and also shows the interaction between these factors.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-130"
  },
  "ohno98b_icslp": {
   "authors": [
    [
     "Sumio",
     "Ohno"
    ],
    [
     "Hiroya",
     "Fujisaki"
    ],
    [
     "Yoshikazu",
     "Hara"
    ]
   ],
   "title": "On the effects of speech rate upon parameters of the command-response model for the fundamental frequency contours of speech",
   "original": "i98_0936",
   "page_count": 4,
   "order": 131,
   "p1": "paper 0936",
   "pn": "",
   "abstract": [
    "A command-response model for the process of F0 contour generation has been presented by Fujisaki and his coworkers. The present paper describes the results of a study on the variability and speech rate dependency of the model's parameters in utterances of a speaker of Japanese. It was found that parameters alpha and beta can be considered to be practically constant at a given speech rate, while Fb may vary slightly from utterance to utterance. Among these three parameters, only alpha was found to have a small but systematic tendency to increase with the speech rate.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-131"
  },
  "portele98_icslp": {
   "authors": [
    [
     "Thomas",
     "Portele"
    ],
    [
     "Barbara",
     "Heuft"
    ]
   ],
   "title": "The maximum-based description of F0 contours and its application to English",
   "original": "i98_0526",
   "page_count": 4,
   "order": 132,
   "p1": "paper 0526",
   "pn": "",
   "abstract": [
    "The maximum-based description is a linear and simple parametrization method of F0 contours. An F0 maximum is characterized by four parameters: its position, its height, its left and its right slope. An automatic parametrization algorithm was developed. A perceptual evaluation was carried out for German and for English. The perceptual equality between original and parametrized contours was confirmed.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-132"
  },
  "portele98b_icslp": {
   "authors": [
    [
     "Thomas",
     "Portele"
    ]
   ],
   "title": "Perceived prominence and acoustic parameters in american English",
   "original": "i98_0527",
   "page_count": 4,
   "order": 133,
   "p1": "paper 0527",
   "pn": "",
   "abstract": [
    "This paper describes the relationships between perceived prominence as a gradual value and some acoustic-prosodic parameters. Prominence is used as an intermediate parameter in a speech synthesis system. A corpus of American English utterances was constructed by measuring and annotating various linguistic, acoustic and perceptual parameters and features. The investigation of the corpus revealed some strong and some rather weak relations between prominence and acoustic-prosodic parameters that serve as a starting point for the development of prominence-based rules for the synthesis of American English prosody in a content-to-speech system.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-133"
  },
  "rank98_icslp": {
   "authors": [
    [
     "Erhard",
     "Rank"
    ],
    [
     "Hannes",
     "Pirker"
    ]
   ],
   "title": "Generating emotional speech with a concatenative synthesizer",
   "original": "i98_0975",
   "page_count": 4,
   "order": 134,
   "p1": "paper 0975",
   "pn": "",
   "abstract": [
    "We describe the attempt to synthesize emotional speech with a concatenative speech synthesizer using a parameter space covering not only f0, duration and amplitude, but also voice quality parameters, spectral energy distribution, harmonics-to-noise ratio, and articulatory precision. The application of these extended parameter set offers the possibility to combine the high segmental quality of concatenative synthesis with a wider range of control settings needed for the synthesis of natural affected speech.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-134"
  },
  "rilliard98_icslp": {
   "authors": [
    [
     "Albert",
     "Rilliard"
    ],
    [
     "V√©ronique",
     "Auberg√©"
    ]
   ],
   "title": "A perceptive measure of pure prosody linguistic functions with reiterant sentences",
   "original": "i98_1086",
   "page_count": 4,
   "order": 135,
   "p1": "paper 1086",
   "pn": "",
   "abstract": [
    "We present here a perceptual measure experiment of the linguistic segmentation hints carried by prosody. The selected paradigm is a dissociation test between couples of stimuli. The sentences are made of several segmentation variations in group and clause levels, and couples are made on all possible combinations of two sentences from the corpus. 20 listeners are able to associate at the same time the similar area and syntactic level frontiers. They admit a single syllable translation on the position of the major syntactic boundary in the reiterated stimuli. They distinguish the couples that do not show the same frontiers. The results also show that listeners are puzzled (random choices) when the proposed frontiers delimit complex segments.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-135"
  },
  "koike98_icslp": {
   "authors": [
    [
     "Kazuhito",
     "Koike"
    ],
    [
     "Hirotaka",
     "Suzuki"
    ],
    [
     "Hiroaki",
     "Saito"
    ]
   ],
   "title": "Prosodic parameters in emotional speech",
   "original": "i98_0996",
   "page_count": 4,
   "order": 136,
   "p1": "paper 0996",
   "pn": "",
   "abstract": [
    "Importance of speech prosody is on the increase as spontaneous interaction between human and machines is asked for. This paper examines how prosody contributes emotions to speech. Major elements which determine the emotion are pitch, tempo, and stress of speech. The last two elements correspond to duration and power of syllables, respectively. We choose five emotions to be tested; anger, surprise, sorrow, hate, and joy. To verify our analysis, we have implemented a speech synthesis module which can easily control prosodic parameters of output speech. Responses to the synthesized speech show that the parameters of anger, sorrow and hate are confirmed over 85 %. Experiment results also suggest that surprise and joy feelings tend to depend less on its prosody.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-136"
  },
  "streefkerk98_icslp": {
   "authors": [
    [
     "Barbertje M.",
     "Streefkerk"
    ],
    [
     "Louis C. W.",
     "Pols"
    ],
    [
     "Louis F.M. ten",
     "Bosch"
    ]
   ],
   "title": "Automatic detection of prominence (as defined by listeners' judgements) in read aloud dutch sentences",
   "original": "i98_0285",
   "page_count": 4,
   "order": 137,
   "p1": "paper 0285",
   "pn": "",
   "abstract": [
    "This paper describes a first step towards the automatic classification of prominence (as defined by naive listeners). As a result of a listening experiment each word in 500 sentences was marked with a rating scale between '0' (non-prominent) and '10' (very prominent). These prominence labels are compared with the following acoustical features: loudness of each vowel, and F0 range and duration of each syllable. A linear relationship between the rating scale of prominence and these acoustical features is found. These acoustical features then are used for a preliminary automatic classification to predict prominence.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-137"
  },
  "tamoto98_icslp": {
   "authors": [
    [
     "Masafumi",
     "Tamoto"
    ],
    [
     "Takeshi",
     "Kawabata"
    ]
   ],
   "title": "A schema for illocutionary act identification with prosodic feature",
   "original": "i98_1099",
   "page_count": 4,
   "order": 138,
   "p1": "paper 1099",
   "pn": "",
   "abstract": [
    "We propose a new discrimination schema for illocutionary acts using prosodic features based on experimental results. Given the transcribed sentence with contextual information, the subjects were able to identify correctly the sentence type of 85% of 290 sentences. With information about the intonation contour types, they could correctly identify 90% of illocutionary acts. We find evidence that illocutionary acts can be signaled by specific contour types. These typical contours are realized in the sentence final boundary tone; a neutral or falling tone for assertion and request, a rising tone for question. An intonation contour is then identified using an algorithm that calculates the range and slope of the upper and lower bounds of unwarped segmental contour, and matches these against predefined contour templates. This automated intonation contour classification, nearly 90% of illocutionary acts could be correctly identified. (URL: http://www.brl.ntt.co.jp/info/dug/)\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-138"
  },
  "tsukahara98_icslp": {
   "authors": [
    [
     "Wataru",
     "Tsukahara"
    ]
   ],
   "title": "An algorithm for choosing Japanese acknowledgments using prosodic cues and context",
   "original": "i98_0955",
   "page_count": 4,
   "order": 139,
   "p1": "paper 0955",
   "pn": "",
   "abstract": [
    "In human dialog a wide variety of acknowledgments are used. One function of this seems to be indicating attention, interest, and involvement to the other speaker, and we believe this is an important factor in encouraging him and keeping up his interest. Thus, in this paper we focus on the problem of choosing appropriate acknowledgments at each point. Based on study of Japanese memory game dialogs, we propose an algorithm for choosing among acknowledgment responses, including `hai' (yes), `so' (right), and `un' (mm). The primary factors involved are aspects of the speaker's internal state, including confidence and liveliness, as inferred from the context and the speaker's prosody. Evaluation of naturalness and helpfulness of dialog generated by this algorithm suggests that judges prefer rule-based responses to randomly chosen responses, confirming our hypothesis that `sensitive' and subtle choice of response may improve helpfulness and naturalness of man-machine spoken language interaction.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-139"
  },
  "wang98c_icslp": {
   "authors": [
    [
     "Chao",
     "Wang"
    ],
    [
     "Stephanie",
     "Seneff"
    ]
   ],
   "title": "A study of tones and tempo in continuous Mandarin digit strings and their application in telephone quality speech recognition",
   "original": "i98_0535",
   "page_count": 4,
   "order": 140,
   "p1": "paper 0535",
   "pn": "",
   "abstract": [
    "Prosodic cues (namely, fundamental frequency, energy and duration) provide important information for speech. For a tonal language such as Chinese, fundamental frequency (F0) plays a critical role in characterizing tone as well, which is an essential phonemic feature. In this paper, we describe our work on duration and tone modeling for telephone-quality continuous Mandarin digits, and the application of these models to improve recognition. The duration modeling includes a speaking-rate normalization scheme. A novel F0 extraction algorithm is developed, and parameters based on orthonormal decomposition of the F0 contour are extracted for tone recognition. Context dependency is expressed by ``tri-tone'' models clustered into broad classes. A 20.0% error rate is achieved for four-tone classification. Over a baseline recognition performance of 5.1% word error rate, we achieve 31.4% error reduction with duration models, 23.5% error reduction with tone models, and 39.2% error reduction with duration and tone models combined.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-140"
  },
  "whiteside98_icslp": {
   "authors": [
    [
     "Sandra P.",
     "Whiteside"
    ]
   ],
   "title": "Simulated emotions: an acoustic study of voice and perturbation measures",
   "original": "i98_0153",
   "page_count": 4,
   "order": 141,
   "p1": "paper 0153",
   "pn": "",
   "abstract": [
    "This brief study presents a set of acoustic correlates for a number of vocal emotions simulated by two actors. Five short sentences were used in the simulations. The emotions simulated were neutral, cold anger, hot anger, happiness, sadness, interest and elation. The seventy sentences were digitised and a number of acoustic analyses were carried out, which included a number of perturbation measures. The acoustic parameters investigated were: i) mean overall fundamental frequency (Hz); ii) overall mean energy (dB); iii) overall mean standard deviation of energy (dB); iv) mean overall jitter (%); and v) mean overall shimmer (dB). These acoustic parameters were used to profile the vocal emotions. Results showed that the actors displayed similarities in their acoustic profiles for some emotions like anger and sadness, for example. The results are presented and discussed in brief.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-141"
  },
  "zhang98b_icslp": {
   "authors": [
    [
     "Jin-song",
     "Zhang"
    ],
    [
     "Keikichi",
     "Hirose"
    ]
   ],
   "title": "A robust tone recognition method of Chinese based on sub-syllabic F0 contours",
   "original": "i98_0674",
   "page_count": 4,
   "order": 142,
   "p1": "paper 0674",
   "pn": "",
   "abstract": [
    "This paper proposes a scheme of using F0 contours of vowel nuclei to discriminate Chinese lexical tones. The authors suggest that the F0 contour fragment of a vowel nucleus of a syllable contributes most to tone perception of the syllable. To correlate the F0 contour with the phonemic components of a syllable, a tone-based syllabic structure is also proposed. Tone recognition experiments on a speaker independent dissyllable words task proved the effectiveness of the proposed method. Better performance over approaches observing full syllabic F0 contours indicates that the proposed method is a more robust tone recognition method.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-142"
  },
  "zhu98_icslp": {
   "authors": [
    [
     "Xiaonong Sean",
     "Zhu"
    ]
   ],
   "title": "The microprosodics of tone sandhi in shanghai disyllabic compounds",
   "original": "i98_0423",
   "page_count": 4,
   "order": 143,
   "p1": "paper 0423",
   "pn": "",
   "abstract": [
    "This paper examines the F0 variations during tone sandhi due to various prosodic factors such as phonation type, length, stress and pitch height. It will be shown that the F0 height and shape of the second syllable (S2) in disyllabic words are determined by the interaction of four conditions: the intervocalic consonant (C2) voicing, the S2 Truncation, the F0 height of S1, and stress assignment.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-143"
  },
  "bolfanstosic98_icslp": {
   "authors": [
    [
     "Natalija",
     "Bolfan-Stosic"
    ],
    [
     "Tatjana",
     "Prizl"
    ]
   ],
   "title": "Jitter and shimmer differences between pathological voices of school children",
   "original": "i98_0103",
   "page_count": 3,
   "order": 144,
   "p1": "paper 0103",
   "pn": "",
   "abstract": [
    "A study was undertaken to determine differences between jitter and shimmer in voices of children with different syndromes. Voices of 60 children, both sexes, aged 7-12 years were analysed by EZ Voice Analysis Software (program for jitter and shimmer measuring). The main purpose of this paper has diagnostic background. Obtained results show, which acoustical indicators of pathological voice are in certain group of children, and in which shapes they appear. In that way, we try to find easiest way to explain acoustical characteristics of different voice pathologies as help in diagnostics. The results indicate that the children with stuttering and disartric symptoms have higher values almost in all applied variables than the average values of children from other groups. Children with Down syndrome and hearing losses exhibited the most disordered voice quality. Finally, the mixed group (stuttering with dysphonia) and group of children with dysphonia exhibited the least pathological characteristics of voice. Obtained results of Analysis of Variance have shown significant statistical differences in all applied variables among the groups.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-144"
  },
  "zhu98b_icslp": {
   "authors": [
    [
     "Xiaonong Sean",
     "Zhu"
    ]
   ],
   "title": "What spreads, and how? tonal rightward spreading on shanghai disyllabic compounds",
   "original": "i98_0422",
   "page_count": 4,
   "order": 145,
   "p1": "paper 0422",
   "pn": "",
   "abstract": [
    "The present paper examines what kinds of Shanghai disyllabic lexical tone sandhi undergoes, especially in what sense and to what extent a disyllabic tone can be claimed to result from rightward spreading of the corresponding citation tone. It will be shown that F0 spreading occurs in the Long tone domains while Contour element spreading mainly in the Short tone domains.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-145"
  },
  "zhu98c_icslp": {
   "authors": [
    [
     "Sean",
     "Zhu"
    ],
    [
     "Phil",
     "Rose"
    ]
   ],
   "title": "Tonal complexity as a dialectal feature: 25 different citation tones from four zhejiang wu dialects",
   "original": "i98_0299",
   "page_count": 4,
   "order": 146,
   "p1": "paper 0299",
   "pn": "",
   "abstract": [
    "The Wu dialects of East-Central China are notorious for their tone sandhi, which is said to be the most complex in the world. This paper demonstrates that tonological complexity in Wu is not confined to tone sandhi, but is manifested also in citation tones. These show a both a large (7 - 8) number of contrasts and a very high percentage of contour and complex tones. Acoustic and auditory data are presented from an ongoing large-scale investigation into the tones and tone sandhi of the Wu dialects of Zhejiang province in East Central China. The citation tones from 4 sites (3 hitherto undescribed) in the little known Central Zhejiang area are described: Pujiang, Tonglu, Shengxian and Tiantai. Mean F0 and duration data are presented for the tones of these dialects. The data demonstrate a high degree of complexity, having no less than 25 Linguistic-tonetically different tones, including 3 different falling tones, and 4 different falling-level tones. The nature of the complexity of these forms is analysed.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-146"
  },
  "montero98_icslp": {
   "authors": [
    [
     "Juan Manuel",
     "Montero"
    ],
    [
     "Juana M.",
     "Gutierrez-Arriola"
    ],
    [
     "Sira",
     "Palazuelos"
    ],
    [
     "Emilia",
     "Enriquez"
    ],
    [
     "Santiago",
     "Aguilera"
    ],
    [
     "Jos√© Manuel",
     "Pardo"
    ]
   ],
   "title": "Emotional speech synthesis: from speech database to TTS",
   "original": "i98_1037",
   "page_count": 4,
   "order": 147,
   "p1": "paper 1037",
   "pn": "",
   "abstract": [
    "Modern Speech synthesisers have achieved a high degree of intelligibility, but can not be regarded as natural-sounding devices. In order to decrease the monotony of synthetic speech, the implementation of emotional effects is now being progressively considered. This paper presents a through study of emotional speech in Spanish, and its application to TTS, presenting a prototype system that simulates emotional speech using a commercial synthesiser. The design and recording of a Spanish database will be described and also the analysis of the emotional prosody (by fitting the data to a formal model). Using this collected data, a rule-based simulation of three primary emotions was implemented in the Text-to-Speech system. Finally, the assessment of the synthetic voice through perception experiments will classify the system as capable of producing quality voice with recognisable emotional effects.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-147"
  },
  "pereira98_icslp": {
   "authors": [
    [
     "Cecile",
     "Pereira"
    ],
    [
     "Catherine",
     "Watson"
    ]
   ],
   "title": "Some acoustic characteristics of emotion",
   "original": "i98_0684",
   "page_count": 3,
   "order": 148,
   "p1": "paper 0684",
   "pn": "",
   "abstract": [
    "This study presents an acoustic analysis of emotion. The material consisted of two semantically neutral utterances spoken by two actors, one male, one female, portraying three moods: anger, happiness and sadness; and a neutral tone. The duration, fundamental frequency (F0) and an estimate of the sound intensity (RMS) were analysed. The fundamental frequency parameter was the most revealing, showing differences between anger and happiness according to the shape of the contour, and between \"cold\" anger and \"hot\" anger on F0 mean. In addition, the study replicates previous findings showing hot anger and happiness having an F0 large range and high mean in contrast to the more subdued emotion of sadness, and the neutral voice.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-148"
  },
  "swerts98b_icslp": {
   "authors": [
    [
     "Marc",
     "Swerts"
    ]
   ],
   "title": "Intonative structure as a determinant of word order variation in dutch verbal endgroups",
   "original": "i98_0267",
   "page_count": 4,
   "order": 149,
   "p1": "paper 0267",
   "pn": "",
   "abstract": [
    "This paper looks into the question to what extent intonative structure determines word order variation in a particular type of syntactic structures in Dutch. Certain subordinate clauses in this language may contain verbal groups consisting of an auxiliary (aux) and a participle (part) that appear in sentence-final position. The order of these verbal elements is fundamentally free so that both aux+part and part+aux combinations occur. Analyses were based on a set of thirty spontaneous monologues, which contained 71 clauses with verbal endgroups, with the two orders about equally balanced. Distributional analyses revealed that prosodic features both inside the verbal group and in the immediately preceding and following contexts play a role in the choice for the two orders. First, a pitch accent on the participle mostly leads to a part+aux order. Second, an accent on the word immediately preceding the verbal endgroup under certain conditions favours an aux+part order, whereas a prosodic boundary after the endgroup favours a part+aux order. Results are discussed in terms of particular push principles, from the left and the right.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-149"
  },
  "caspers98_icslp": {
   "authors": [
    [
     "Johanneke",
     "Caspers"
    ]
   ],
   "title": "Experiments on the meaning of two pitch accent types: the 'pointed hat' versus the accent-lending fall in dutch",
   "original": "i98_0235",
   "page_count": 4,
   "order": 150,
   "p1": "paper 0235",
   "pn": "",
   "abstract": [
    "The aim of the present investigation is to find out more about the meaning of two Dutch melodic shapes: the default pitch accent or `pointed hat' and the accent-lending fall. Can the meaning difference between these pitch configurations be better described as a difference in information status or as a difference in attitude? Subjects were presented with the two contours on short sentences in specific contexts; the stimulus formed either the answer to a question (the focused information is new) or the completion of an enumeration (the focused information was already projected). In a pairwise comparison test subjects had to choose the contour best fitting the presented context. In a rating experiment subjects judged each combination of contour type and context on a number of semantic scales. Information status as well as attitude explain part of the results, indicating that both notions should be incorporated in the semantics of intonation.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-150"
  },
  "jun98_icslp": {
   "authors": [
    [
     "Sun-Ah",
     "Jun"
    ],
    [
     "Hyuck-Joon",
     "Lee"
    ]
   ],
   "title": "Phonetic and phonological markers of contrastive focus in Korean",
   "original": "i98_1087",
   "page_count": 4,
   "order": 151,
   "p1": "paper 1087",
   "pn": "",
   "abstract": [
    "Cross-linguistically, focus is often cued by suprasegmental features and changes in phrasing. In this paper, phonetic and phonological markers of contrastive focus in Korean are investigated. We find that, as a phonological marker, focus initiates an accentual phrase (AP), and tends to, but does not always, include the following words in the same AP. But regardless of whether the post-focus sequence is dephrased or not, there is a significant expansion of the focused peak compared to the peak on the following words, thus achieving the perceptual goal of focus: prominence of the focused word relative to the following items. As a phonetic marker, a focused AP has extra-strengthening on its left edge, and the sequence before and after focus tends to be shorter than that in a neutral sentence.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-151"
  },
  "krahmer98_icslp": {
   "authors": [
    [
     "Emiel",
     "Krahmer"
    ],
    [
     "Marc",
     "Swerts"
    ]
   ],
   "title": "Reconciling two competing views on contrastiveness",
   "original": "i98_0270",
   "page_count": 4,
   "order": 152,
   "p1": "paper 0270",
   "pn": "",
   "abstract": [
    "Some people claim that contrastive accents are more emphatic than newness accents and have a different melodic shape. Others, however, maintain that contrastiveness can only be determined by looking at how accents are distributed in an utterance. In this paper it is argued that these two competing views can be reconciled by showing that they apply on different levels. To this end, accent patterns were obtained via a dialogue game (Dutch) in which two participants had to describe coloured figures in consecutive turns. Target descriptions (``blue square\") were collected in four contexts: no contrast (all new), contrast in the adjective, contrast in the noun, all contrast. A distributional analysis revealed that both all new and all contrast situations correspond with double accents, whereas single accents on the adjective or the noun are used when these are contrastive. Single contrastive accents on the adjective are acoustically different from newness accents in the same syntactic position. The former have the shape of a `nuclear' accent, whereas the newness accents on the adjective are `prenuclear'. Contrastive accents stand out as perceptually more prominent than newness accents. This difference in salience tends to disappear if the accented word is heard in isolation.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-152"
  },
  "taylor98_icslp": {
   "authors": [
    [
     "Paul",
     "Taylor"
    ]
   ],
   "title": "The tilt intonation model",
   "original": "i98_0827",
   "page_count": 4,
   "order": 153,
   "p1": "paper 0827",
   "pn": "",
   "abstract": [
    "The tilt intonation model facilitates automatic analysis and synthesis of intonation. The analysis algorithm detects intonational events in F0 contours and parameterises them in terms of the continuously varying parameters. We describe the analysis system and give results for speaker independent spontaneous dialogue speech. We then describe a synthesis algorithm which can generate F0 contours given a tilt parameterisation of an utterance. We give results showing how well the automatically produced contours match natural ones. The paper concludes with a discussion of the linguistic relevance of the tilt parameters and show that this is both a useful and natural way of representing intonation.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-153"
  },
  "fujisaki98b_icslp": {
   "authors": [
    [
     "Hiroya",
     "Fujisaki"
    ],
    [
     "Sumio",
     "Ohno"
    ],
    [
     "Seiji",
     "Yamada"
    ]
   ],
   "title": "Analysis of occurrence of pauses and their durations in Japanese text reading",
   "original": "i98_0831",
   "page_count": 4,
   "order": 154,
   "p1": "paper 0831",
   "pn": "",
   "abstract": [
    "Pauses play important roles both for the intelligibility and the naturalness of speech. Their occurrences and durations in text reading are influenced by syntactic structures of the text as well as by physiological constraints of respiration on the part of the speaker. The present paper describes some of the preliminary findings on Japanese text reading, especially on the effects of the syntactic role of the preceding phrase on the rate of occurrence and the duration of a pause at a syntactic boundary.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-154"
  },
  "campione98_icslp": {
   "authors": [
    [
     "Estelle",
     "Campione"
    ],
    [
     "Jean",
     "V√©ronis"
    ]
   ],
   "title": "A statistical study of pitch target points in five languages",
   "original": "i98_0845",
   "page_count": 4,
   "order": 155,
   "p1": "paper 0845",
   "pn": "",
   "abstract": [
    "We present the results of a large-scale statistical study of pitch target points in five languages, on a corpus comprising 4 hours 20 minutes of speech and involving 50 different speakers. The entire corpus has been stylized automatically by a technique reducing the F0 contour to a series of target points representing the significant pitch changes. It was then entirely verified by experts using a resynthesis method, in order to ensure that there was no audible difference with the original. The set of ca. 50000 pitch target points thus obtained was then analyzed from a statistical point of view. In this paper we describe the main results of this study, in terms of frequency distribution of target points, pitch movements and relation of pitch movements to time interval. Our study reveals interesting differences across languages and sex.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-155"
  },
  "malfrere98_icslp": {
   "authors": [
    [
     "Fabrice",
     "Malfr√®re"
    ],
    [
     "Thierry",
     "Dutoit"
    ],
    [
     "Piet",
     "Mertens"
    ]
   ],
   "title": "Fully automatic prosody generator for text-to-speech",
   "original": "i98_0355",
   "page_count": 4,
   "order": 156,
   "p1": "paper 0355",
   "pn": "",
   "abstract": [
    "Text-to-Prosody systems based on the use of prosodic databases extracted from natural speech will be a key point for further development of new Text-to-Speech systems. This paper describes a system using such speech databases to generate the rhythm and the intonation of a French written text. The system is based on a very crude chinks 'n chunks prosodic phrasing algorithm and on a prosodic analysis of a natural speech database. The rhythm of the synthetic speech is generated with a CART tree trained on a large mono-speaker speech corpus. The acoustic aspect of the intonation is derived from a set of prosodic patterns automatically derived from the same speech corpus. The system has been tested on single sentences and news paragraphs. Informal listening tests have shown that the resulting prosody is convincing most of the time.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-156"
  },
  "vereecken98_icslp": {
   "authors": [
    [
     "Halewijn",
     "Vereecken"
    ],
    [
     "Jean-Pierre",
     "Martens"
    ],
    [
     "Cynthia",
     "Grover"
    ],
    [
     "Justin",
     "Fackrell"
    ],
    [
     "Bert Van",
     "Coile"
    ]
   ],
   "title": "Automatic prosodic labeling of 6 languages",
   "original": "i98_0045",
   "page_count": 4,
   "order": 157,
   "p1": "paper 0045",
   "pn": "",
   "abstract": [
    "This contribution describes a method for the automatic prosodic labeling of multi-lingual speech data. The automatic labeler assigns a boundary strength between 0 and 3 to each word boundary, and a word prominence between 0 and 9 to each word. The speech signal and its orthographic representation are first transformed to feature vectors comprising acoustic and linguistic features such as pitch, duration, energy, part-of-speech, punctuation, word frequency and stress. Next, the feature vectors are mapped to prosodic labels via a cascade of multi-layer perceptrons. Experiments on 6 different languages demonstrate that combining acoustic with linguistic features yields a better performance than obtainable on the basis of acoustic features alone. We also present experiments in which we assess the influence of the quality of the underlying phonetic segmentation and labeling on the prosodic labeling performance.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-157"
  },
  "wright98_icslp": {
   "authors": [
    [
     "Helen",
     "Wright"
    ]
   ],
   "title": "Automatic utterance type detection using suprasegmental features",
   "original": "i98_0575",
   "page_count": 4,
   "order": 158,
   "p1": "paper 0575",
   "pn": "",
   "abstract": [
    "The goal of the work presented here is to automatically predict the type of an utterance in spoken dialogue by using automatically extracted suprasegmental information. For this task we present and compare three stochastic algorithms: hidden Markov models, artificial neural nets, and classification and regression trees. These models are easily trainable, reasonably robust and fit into the probabilistic framework required for speech recognition. Utterance type detection is dependent on the assumption that different types of utterances have different suprasegmental characteristics. The categorisation of these utterance types is based on the theory of conversation games and consists of 12 move types (e.g. reply to a question, wh-question, acknowledgement). The system is speaker independent and is trained on spontaneous goal-directed dialogue collected from Canadian speakers. This utterance type detector is used in an automatic speech recognition system to reduce word error rate.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-158"
  },
  "low98_icslp": {
   "authors": [
    [
     "Ee Ling",
     "Low"
    ],
    [
     "Esther",
     "Grabe"
    ]
   ],
   "title": "A contrastive study of lexical stress placement in singapore English and british English",
   "original": "i98_0098",
   "page_count": 4,
   "order": 159,
   "p1": "paper 0098",
   "pn": "",
   "abstract": [
    "Singapore English and British English have been claimed to differ in lexical stress placement. Examples cited in the literature involve polysyllabic words such as 'hopelessly' and compounds such as 'blackboard'. Such words are stressed word-initially in BE, but are said to be stressed word-finally in SE. Two observations lead us to explore the claim that SE and BE differ in lexical stress placement. Firstly, observations about stress differences between SE and BE are based solely on auditory impressions by British English listeners. Acoustic evidence is not available. Secondly, the auditory evidence comes from realisations of test words in citation form, i.e. in nuclear, phrase-final position. If Singapore English has more phrase-final lengthening than British English, then this may account for the suggested differences in lexical stress placement. In the present paper, we investigate the acoustic evidence for the suggested cross-varietal difference.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-159"
  },
  "gallwitz98_icslp": {
   "authors": [
    [
     "Florian",
     "Gallwitz"
    ],
    [
     "Anton",
     "Batliner"
    ],
    [
     "Jan",
     "Buckow"
    ],
    [
     "Richard",
     "Huber"
    ],
    [
     "Heinrich",
     "Niemann"
    ],
    [
     "Elmar",
     "N√∂th"
    ]
   ],
   "title": "Integrated recognition of words and phrase boundaries",
   "original": "i98_0328",
   "page_count": 4,
   "order": 160,
   "p1": "paper 0328",
   "pn": "",
   "abstract": [
    "In this paper we present an integrated approach for recognizing both the word sequence and the syntactic-prosodic structure of a spontaneous utterance. We take into account the fact that a spontaneous utterance is not merely an unstructured sequence of words by incorporating phrase boundary information into the language model and by providing HMMs to model boundaries. This allows for a distinction between word transitions across phrase boundaries and transitions within a phrase. During recognition, the syntactic-prosodic structure of the utterance is determined implicitly. Without any increase in computational effort, this leads to a 4% reduction of word error rate, and, at the same time, syntactic-prosodic boundary labels are provided for subsequent processing. The boundaries are recognized with a precision and recall rate of about 75% each. They can be used to reduce drastically the computational effort for parsing spontaneous utterances. We also present a system architecture to incorporate additional prosodic information.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-160"
  },
  "arvaniti98_icslp": {
   "authors": [
    [
     "Amalia",
     "Arvaniti"
    ]
   ],
   "title": "Phrase accents revisited: comparative evidence from standard and cypriot greek",
   "original": "i98_0550",
   "page_count": 4,
   "order": 161,
   "p1": "paper 0550",
   "pn": "",
   "abstract": [
    "Phrase accents, one of the three tonal categories assumed by much recent research on intonation, are expected to associate with a prosodic boundary (e.g. the end of the utterance) but not to phonetically align with a specific tone-bearing unit (TBU), such as a stressed syllable. This paper presents experimental evidence on the intonation of Cypriot Greek polar questions suggesting that phrase accents prefer to associate with specific TBUs. Concretely, it is shown that in Cypriot Greek polar question intonation, autosegmentally described as L* H L%, the H phrase accent does not align with the final stressed vowel as in Standard Greek, but instead it aligns approximately 30ms from the onset of either the penultimate or the final vowel of the utterance. The data provide evidence that phrase accents, like other tonal categories, exhibit stable phonetic alignment and support Ladd, Arvaniti and Mennen's (1997) typology of stress-seeking and non-stress-seeking phrase accents.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-161"
  },
  "dogil98_icslp": {
   "authors": [
    [
     "Grzegorz",
     "Dogil"
    ],
    [
     "Gregor",
     "M√∂hler"
    ]
   ],
   "title": "Phonetic invariance and phonological stability: lithuanian pitch accents",
   "original": "i98_0206",
   "page_count": 4,
   "order": 162,
   "p1": "paper 0206",
   "pn": "",
   "abstract": [
    "We argue that phonetically invariant realizations of phonological categories imply the synchronic and diachronic imperviousness of such categories to phonological rules and sound laws. We claim that phonetic invariance is the foundation of phonological stability. The category we discuss in this contribution is the pitch-accent. We provide a parametric phonetic description of this phonological category. By means of a parametrization technique we apply this description to the contrastive pitch-accents of Lithuanian. The statistic differences between acute and circumflex pitch-accents derived by the parametrization provide a basis for the discussion of synchronic and diachronic behavior of the phonetically nonbalanced phonological contrasts.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-162"
  },
  "brindopke98_icslp": {
   "authors": [
    [
     "Christel",
     "Brind√∂pke"
    ],
    [
     "Gernot A.",
     "Fink"
    ],
    [
     "Franz",
     "Kummert"
    ],
    [
     "Gerhard",
     "Sagerer"
    ]
   ],
   "title": "A HMM-based recognition system for perceptive relevant pitch movements of spontaneous German speech",
   "original": "i98_0503",
   "page_count": 4,
   "order": 163,
   "p1": "paper 0503",
   "pn": "",
   "abstract": [
    "This paper presents an HMM-based recognition system for perceptive relevant pitch movements of spontaneous German speech. The pitch movements are defined according to the perceptively and phonetically motivated IPO-approach to intonation. For recognition we use a hybrid approach combining polynomial classification with Hidden Markov Modelling. The recognition is based only on the speech signal, its fundamental frequency and eleven derived features. We evaluate the system on a speaker independent recognition task.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-163"
  },
  "veronis98_icslp": {
   "authors": [
    [
     "Jean",
     "V√©ronis"
    ],
    [
     "Estelle",
     "Campione"
    ]
   ],
   "title": "Towards a reversible symbolic coding of intonation",
   "original": "i98_0846",
   "page_count": 4,
   "order": 164,
   "p1": "paper 0846",
   "pn": "",
   "abstract": [
    "This paper presents a two-step model for the symbolic coding and generation of intonation. First, the F0 curve is reduced to a series of pitch target points that capture the macroprosodic information of the utterance. Target points are then converted into a sequence of labels. Generation is achieved through the reverse steps. The model is language independent and requires no prior training on the data. We discuss the influence of the number of categories on the precision of fit, and show, by an evaluation on a large multilingual corpus (4 hours 20 minutes of speech, 50 speakers, 5 languages) that a model composed of three ascending and three descending categories, plus a category for small or null movements enables a regeneration of ca. 99% of points at less than 2 ST than the original. Given that the model is capable of various improvements, it seems a good candidate for practical applications.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-164"
  },
  "luo98_icslp": {
   "authors": [
    [
     "Xiaoqiang",
     "Luo"
    ],
    [
     "Frederick",
     "Jelinek"
    ]
   ],
   "title": "Nonreciprocal data sharing in estimating HMM parameters",
   "original": "i98_0365",
   "page_count": 4,
   "order": 165,
   "p1": "paper 0365",
   "pn": "",
   "abstract": [
    "Parameter tying is often used in large vocabulary continuous speech recognition (LVCSR) systems to balance the model resolution and generalizability. However, one consequence of tying is that the differences among tied constructs are ignored. Parameter tying can be alternatively viewed as reciprocal data sharing in that a tied construct uses data associated with all others in its tied-class. To capture the fine difference among tied constructs, we propose to use nonreciprocal data sharing (NRDS) when estimating HMM parameters. In particular, when estimating Gaussian parameters for a HMM state, contributions from other acoustically similar HMM states will be weighted, thus allowing different statistics to govern different states. Data sharing weights are optimized using cross-validation. It can be shown that the objective function for cross-validation is a sum of rational functions and can be efficiently optimized by the growth-transform. Our results on Switchboard show that NRDS reduces the word error rate (WER) significantly compared with a state-of-art baseline system using HMM state-tying.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-165"
  },
  "bilmes98_icslp": {
   "authors": [
    [
     "Jeff A.",
     "Bilmes"
    ]
   ],
   "title": "Data-driven extensions to HMM statistical dependencies",
   "original": "i98_0894",
   "page_count": 4,
   "order": 166,
   "p1": "paper 0894",
   "pn": "",
   "abstract": [
    "In this paper, a new technique is introduced that relaxes the HMM conditional independence assumption in a principled way. Without increasing the number of states, the modeling power of an HMM is increased by including only those additional probabilistic dependencies (to the surrounding observation context) that are believed to be both relevant and discriminative. Conditional mutual information is used to determine both relevance and discriminability. Extended Gaussian-mixture HMMs and new EM update equations are introduced. In an isolated word speech database, results show an average 34% word error improvement over an HMM with the same number of states, and a 15% improvement over an HMM with a comparable number of parameters.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-166"
  },
  "sun98_icslp": {
   "authors": [
    [
     "Jiping",
     "Sun"
    ],
    [
     "Li",
     "Deng"
    ]
   ],
   "title": "Use of high-level linguistic constraints for constructing feature-based phonological model in speech recognition",
   "original": "i98_0043",
   "page_count": 4,
   "order": 167,
   "p1": "paper 0043",
   "pn": "",
   "abstract": [
    "Modeling phonological units of speech is a critical issue in speech recognition. In this paper, we report our recent development of an overlapping feature-based phonological model which gives long-span contextual dependency. We extend our earlier work by incorporating high-level linguistic constraints in automatic construction of the feature overlapping patterns. The main linguistic information explored includes morpheme, syllable, syllable constituent categories and word stress markers. We describe a consistent computational framework developed for the construction of the feature-based model, and discuss use of the model as the HMM state topology for speech recognizers.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-167"
  },
  "lee98d_icslp": {
   "authors": [
    [
     "Steven C.",
     "Lee"
    ],
    [
     "James R.",
     "Glass"
    ]
   ],
   "title": "Real-time probabilistic segmentation for segment-based speech recognition",
   "original": "i98_0594",
   "page_count": 4,
   "order": 168,
   "p1": "paper 0594",
   "pn": "",
   "abstract": [
    "In this work, we investigate modifications to a probabilistic segmentation algorithm to achieve a real-time, and pipelined capability for a segment-based speech recognizer. The existing algorithm used a Viterbi and backwards A* search to hypothesize phonetic segments. We were able to reduce the computational requirements of this algorithm by reducing the effective search space to acoustic landmarks, and were able to achieve pipelined capability by executing the A* search in blocks defined by reliably detected phonetic boundaries. The new algorithm produces 30% fewer segments, and improves TIMIT phonetic recognition performance by 2.4% over an acoustic segmentation baseline. We were also able to produce 30% fewer segments on a word recognition task in a weather information domain.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-168"
  },
  "gravier98_icslp": {
   "authors": [
    [
     "Guillaume",
     "Gravier"
    ],
    [
     "Marc",
     "Sigelle"
    ],
    [
     "G√©rard",
     "Chollet"
    ]
   ],
   "title": "Toward Markov random field modeling of speech",
   "original": "i98_0560",
   "page_count": 4,
   "order": 169,
   "p1": "paper 0560",
   "pn": "",
   "abstract": [
    "In this paper, we present a new technique for statistical modeling of speech segments based on Markov random fields. Classical and multi-stream HMMs are particular cases of this more general family of models. However, the Random Field Model (RFM) proposed here can be seen as an extension of the multi-band HMM in which interactions between the frequency bands have been added. In a first experiment, samples are drawn from different models and compared to real observations. This experiment shows that the RFM is able to produce realistic samples but a single HMM still performs better. Isolated word recognition experiments stress the fact that more work must be done on the RFM in order to reach the performances of classical hidden Markov modeling techniques. For the moment, the RFM parameters are estimated using a heuristic. We believe that a real maximum likelihood parameter estimation algorithm should improve the results. The main advantage of this new model is that it can easily be extended since a model is defined by some local interactions and the Gibbs potential functions associated to those interactions.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-169"
  },
  "iyer98_icslp": {
   "authors": [
    [
     "Rukmini",
     "Iyer"
    ],
    [
     "Herbert",
     "Gish"
    ],
    [
     "Man-Hung",
     "Siu"
    ],
    [
     "George",
     "Zavaliagkos"
    ],
    [
     "Spyros",
     "Matsoukas"
    ]
   ],
   "title": "Hidden Markov models for trajectory modeling",
   "original": "i98_0891",
   "page_count": 4,
   "order": 170,
   "p1": "paper 0891",
   "pn": "",
   "abstract": [
    "Current state-of-the-art statistical speech recognition systems use hidden Markov models (HMM) for modeling the speech signal. However, it is well known that HMM's do not exploit the time-dependence in the speech process, since they are limited by the assumption of conditional independence of observations given the state sequence. Alternative techniques, such as segment modeling approaches, can effectively exploit time-dependencies in the acoustic signal by discarding the observation independence assumption. However, losing the basic HMM structure is often a high computational price to pay for improved acoustic models. In this paper, we introduce the parallel path HMM that exploits the time-dependence in speech via parametric trajectory models while maintaining the HMM framework. We present preliminary results on Switchboard, a large vocabulary conversational speech recognition task, demonstrating both improved modeling and potential for improved recognition performance.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-170"
  },
  "aizawa98_icslp": {
   "authors": [
    [
     "Katsura",
     "Aizawa"
    ],
    [
     "Chieko",
     "Furuichi"
    ]
   ],
   "title": "A statistical phonemic segment model for speech recognition based on automatic phonemic segmentation",
   "original": "i98_0544",
   "page_count": 4,
   "order": 171,
   "p1": "paper 0544",
   "pn": "",
   "abstract": [
    "This paper presents a method of constructing a statistical phonemic segment model (SPSM) for a speech recognition system based on speaker-independent context-independent automatic phonemic segmentation. In our recent research, we proposed the phoneme recognition system using the template matching method with the same segmentation, and confirmed that 5-frame-fixed time sequence of feature vectors used as a template represents features of phoneme effectively. This time, to improve a mass of these templates to a smarter model, we introduced a statistical method into modeling. The structure of SPSM connects 5 distributions of Gaussian N-mixture density in series. By the experiment of closed Japanese spoken word recognition, using VCV balanced 4920 words spoken by 10 male adults including 34430 phonemes in total, the rate of phoneme recognition using SPSM was up to 90.23 % compared with the rate using phoneme templates, 80.39 %.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-171"
  },
  "demuynck98_icslp": {
   "authors": [
    [
     "Kris",
     "Demuynck"
    ],
    [
     "Jacques",
     "Duchateau"
    ],
    [
     "Dirk Van",
     "Compernolle"
    ],
    [
     "Patrick",
     "Wambacq"
    ]
   ],
   "title": "Improved feature decorrelation for HMM-based speech recognition",
   "original": "i98_1081",
   "page_count": 4,
   "order": 172,
   "p1": "paper 1081",
   "pn": "",
   "abstract": [
    "Many HMM-based recognition systems use mixtures of diagonal covariance gaussians to model the observation density functions in the states. These mixtures are however only approximations of the real distributions. One of the approximations is the assumption that the off-diagonal elements of the covariance matrices of the gaussians are close to zero (diagonal covariance). To that end, most recognition systems have some kind of parameter decorrelation near the end of the preprocessing, e.g. the inverse cosine transform used with cepstral transformations. These transforms are however not optimal if it comes to decorrelating features on the gaussian level. This paper presents an optimal solution in a least-square sense to the decorrelation problem. It also demonstrates the link between the recently published maximum likelihood modelling for semi-tied covariance matrices and the presented least-squares optimisation. Evaluation on a large vocabulary recognition task shows a 10% relative improvement.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-172"
  },
  "preez98_icslp": {
   "authors": [
    [
     "J. A. du",
     "Preez"
    ],
    [
     "D. M.",
     "Weber"
    ]
   ],
   "title": "Efficient high-order hidden Markov modelling",
   "original": "i98_1073",
   "page_count": 4,
   "order": 173,
   "p1": "paper 1073",
   "pn": "",
   "abstract": [
    "We present two powerful tools which allow efficient training of arbitrary (including mixed and infinite) order hidden Markov models. The method rests on two parts: an algorithm which can convert high-order models to an equivalent first-order representation (ORder rEDucing), and a Fast (order) Incremental Training algorithm. We demonstrate that this method is more flexible, results in significantly faster training and improved generalisation compared to prior work. Order reducing is also shown to give insight into the language modelling capabilities of certain high-order HMM topologies.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-173"
  },
  "eide98_icslp": {
   "authors": [
    [
     "Ellen M.",
     "Eide"
    ],
    [
     "Lalit R.",
     "Bahl"
    ]
   ],
   "title": "A time-synchronous, tree-based search strategy in the acoustic fast match of an asynchronous speech recognition system",
   "original": "i98_0165",
   "page_count": 4,
   "order": 174,
   "p1": "paper 0165",
   "pn": "",
   "abstract": [
    "This paper describes the interaction between a synchronous fast match and an asynchronous detailed match search in an automatic speech recognition system. We show a considerable speed-up through this hybrid architecture over a fully-asynchronous approach and discuss the advantages of the hybrid system.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-174"
  },
  "fritsch98_icslp": {
   "authors": [
    [
     "J√ºrgen",
     "Fritsch"
    ],
    [
     "Michael",
     "Finke"
    ],
    [
     "Alex",
     "Waibel"
    ]
   ],
   "title": "Effective structural adaptation of LVCSR systems to unseen domains using hierarchical connectionist acoustic models",
   "original": "i98_0754",
   "page_count": 4,
   "order": 175,
   "p1": "paper 0754",
   "pn": "",
   "abstract": [
    "We present an approach to efficiently and effectively downsize and adapt the structure of large vocabulary conversational speech recognition (LVCSR) systems to unseen domains, requiring only small amounts of transcribed adaptation data. Our approach aims at bringing todays mostly task dependent systems closer to the aspired goal of domain independence. To achieve this, we rely on the ACID/HNN framework, a hierarchical connectionist modeling paradigm which allows to dynamically adapt a tree structured modeling hierarchy to differing specificity of phonetic context in new domains. Experimental validation of the proposed approach has been carried out by adapting size and structure of ACID/HNN based acoustic models trained on Switchboard to two quite different, unseen domains, Wall Street Journal and an English Spontaneous Scheduling Task. In both cases, our approach yields considerably downsized acoustic models with performance improvements of up to 18% over the unadapted baseline models.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-175"
  },
  "ganapathiraju98_icslp": {
   "authors": [
    [
     "Aravind",
     "Ganapathiraju"
    ],
    [
     "Jonathan",
     "Hamaker"
    ],
    [
     "Joseph",
     "Picone"
    ]
   ],
   "title": "Support vector machines for speech recognition",
   "original": "i98_0410",
   "page_count": 4,
   "order": 176,
   "p1": "paper 0410",
   "pn": "",
   "abstract": [
    "A Support Vector Machine (SVM) is a promising machine learning technique that has generated a lot of interest in the pattern recognition community in recent years. The greatest asset of an SVM is its ability to construct nonlinear decision regions in a discriminative fashion. This paper describes an application of SVMs to two speech data classification experiments: 11 vowels spoken in isolation and 16 phones extracted from spontaneous telephone speech. The best performance achieved on the spontaneous speech classification task is a 51% error rate using an RBF kernel. This is comparable to frame-level classification achieved by other nonlinear modeling techniques such as artificial neural networks (ANN).\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-176"
  },
  "gandhi98_icslp": {
   "authors": [
    [
     "Malan B.",
     "Gandhi"
    ]
   ],
   "title": "Natural number recognition using discriminatively trained inter-word context dependent hidden Markov models",
   "original": "i98_0090",
   "page_count": 4,
   "order": 177,
   "p1": "paper 0090",
   "pn": "",
   "abstract": [
    "Many automatic speech recognition telephony applications involve recognition of input containing some type of numbers. Traditionally, this has been achieved by using isolated or connected digit recognizers. However, as speech recognition finds a wider range of applications, it is often infeasible to impose restrictions on speaker behavior. This paper studies two model topologies for natural number recognition which use minimum classification error (MCE) trained inter-word context dependent acoustic models. One model topology uses triphone context units while another is of the head-body-tail (HBT) type. The performance of the models is evaluated on three natural number applications involving recognition of dates, time of day, and dollar amounts. Experimental results show that context dependent models reduce string error rates by as much as 50% over baseline context independent whole-word models. String accuracies of about 93% are obtained on these tasks while at the same time allowing users flexibility in speaking styles.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-177"
  },
  "hamaker98_icslp": {
   "authors": [
    [
     "Jonathan",
     "Hamaker"
    ],
    [
     "Aravind",
     "Ganapathiraju"
    ],
    [
     "Joseph",
     "Picone"
    ]
   ],
   "title": "Information theoretic approaches to model selection",
   "original": "i98_0653",
   "page_count": 4,
   "order": 178,
   "p1": "paper 0653",
   "pn": "",
   "abstract": [
    "The primary problem in large vocabulary conversational speech recognition (LVCSR) is poor acoustic-level matching due to large variability in pronunciations. There is much to explore about the \"quality\" of states in an HMM and the inter-relationships between inter-state and intra-state Gaussians used to model speech. Of particular interest is the variable discriminating power of the individual states. The fundamental concept addressed in this paper is to investigate means of exploiting such dependencies through model topology optimization based on the Bayesian Information Criterion (BIC) and the Minimum Description Length (MDL) principle.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-178"
  },
  "hanai98_icslp": {
   "authors": [
    [
     "Kengo",
     "Hanai"
    ],
    [
     "Kazumasa",
     "Yamamoto"
    ],
    [
     "Nobuaki",
     "Minematsu"
    ],
    [
     "Seiichi",
     "Nakagawa"
    ]
   ],
   "title": "Continuous speech recognition using segmental unit input HMMs with a mixture of probability density functions and context dependency",
   "original": "i98_1012",
   "page_count": 4,
   "order": 179,
   "p1": "paper 1012",
   "pn": "",
   "abstract": [
    "It is well-known that HMMs only of the basic structure cannot capture the correlations among successive frames adequately. In our previous work, to solve this problem, segmental unit HMMs were introduced and their effectiveness was shown. And the integration of delta- cepstrum and delta-delta- cepstrum into the segmental unit HMMs was also found to improve the recognition performance in the work. In this paper, we investigated further refinements of the models by using a mixture of PDFs and/or context dependency, where, for a given syllable, only a preceding vowel was treated as the context information. Recognition experiments showed that the accuracy rate was improved by 23 %, which clearly indicates the effectiveness of the refinements examined in this paper. The proposed syllable-based HMM outperformed a triphone model.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-179"
  },
  "simonin98_icslp": {
   "authors": [
    [
     "Jacques",
     "Simonin"
    ],
    [
     "Lionel",
     "Delphin-Poulat"
    ],
    [
     "Geraldine",
     "Damnati"
    ]
   ],
   "title": "Gaussian density tree structure in a multi-Gaussian HMM-based speech recognition system",
   "original": "i98_1063",
   "page_count": 4,
   "order": 180,
   "p1": "paper 1063",
   "pn": "",
   "abstract": [
    "This paper presents a Gaussian density tree structure usage which enables a computational cost reduction without a significant degradation of recognition performances, during a continuous speech recognition process. The Gaussian tree structure is built from successive Gaussian density merging. Each node of the tree is associated with a Gaussian density, and the actual HMM densities are associated to the leaves. We propose then a criterion to obtain good recognition performances with this Gaussian tree structure. This structure is evaluated with a continuous speech recognition system on a telephone database. The criterion allows a 75 to 85% computational cost reduction in terms of log-likelihood computations without any significant word error rate during the recognition process.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-180"
  },
  "kojima98_icslp": {
   "authors": [
    [
     "Hiroaki",
     "Kojima"
    ],
    [
     "Kazuyo",
     "Tanaka"
    ]
   ],
   "title": "Generalized phone modeling based on piecewise linear segment lattice",
   "original": "i98_0995",
   "page_count": 4,
   "order": 181,
   "p1": "paper 0995",
   "pn": "",
   "abstract": [
    "The goal of this work is to model phone-like units automatically from spoken word samples without using any transcriptions except for the lexical identification of the words. In order to implement this task, we have proposed the \"piecewise linear segment lattice (PLSL)\" model for phoneme representation. The structure of this model is a lattice of segments, each of which is represented as regression coefficients of feature vectors within the segment. In order to organize phone models, operations including division, concatenation, blocking and clustering are applied to the models. This paper mainly report on blocking and clustering. Experimental results for isolated word recognition task is that the recognition rate is significantly improved by blocking the segments and by clustering the segments within a block. We get sufficient performance for the task with the models consist of at most 128 clusters of segment patterns.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-181"
  },
  "koshiba98_icslp": {
   "authors": [
    [
     "Ryosuke",
     "Koshiba"
    ],
    [
     "Mitsuyoshi",
     "Tachimori"
    ],
    [
     "Hiroshi",
     "Kanazawa"
    ]
   ],
   "title": "A flexible method of creating HMM using block-diagonalization of covariance matrices",
   "original": "i98_0197",
   "page_count": 4,
   "order": 182,
   "p1": "paper 0197",
   "pn": "",
   "abstract": [
    "A new algorithm to reduce the amount of calculation in the likelihood computation of continuous mixture HMM(CMHMM) with block-diagonal covariance matrices while retaining high recognition rate is proposed. The block matrices are optimized by minimizing difference between the output probability calculated with full covariance matrices and that calculated with block-diagonal covariance matrices. The idea was implemented and tested on a continuous number recognition task.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-182"
  },
  "chesta98_icslp": {
   "authors": [
    [
     "C.",
     "Chesta"
    ],
    [
     "Pietro",
     "Laface"
    ],
    [
     "F.",
     "Ravera"
    ]
   ],
   "title": "HMM topology selection for accurate acoustic and duration modeling",
   "original": "i98_0149",
   "page_count": 4,
   "order": 183,
   "p1": "paper 0149",
   "pn": "",
   "abstract": [
    "In this paper we show that accurate HMMs for connected word recognition can be obtained without context dependent modeling and discriminative training. To account for different speaking rates, we define two HMMs for each word that must be trained. The two models have the same, standard, left to right topology with the possibility of skipping one state, but each model has a different number of states, automatically selected. Our simple modeling and training technique has been applied to connected digit recognition using the adult speaker portion of the TI/NIST corpus. The obtained results are comparable with the best ones reported in the literature for models with a larger number of densities.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-183"
  },
  "lee98e_icslp": {
   "authors": [
    [
     "Tan",
     "Lee"
    ],
    [
     "Rolf",
     "Carlson"
    ],
    [
     "Bj√∂rn",
     "Granstr√∂m"
    ]
   ],
   "title": "Context-dependent duration modelling for continuous speech recognition",
   "original": "i98_0441",
   "page_count": 4,
   "order": 184,
   "p1": "paper 0441",
   "pn": "",
   "abstract": [
    "This paper presents a trial study of using context-dependent segmental duration for continuous speech recognition in a domain-specific application. Different modelling strategies are proposed for function words and content words. Stress status, word position in utterance and phone position in word are identified to be the 3 most crucial factors affecting segmental duration in this particular application. In addition, speaking rate normalization is applied to further reduce the duration variabilities. Experimental results show that the normalized duration models can help improving the rank of the correct sentence in the N-best hypotheses list.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-184"
  },
  "mak98_icslp": {
   "authors": [
    [
     "Brian",
     "Mak"
    ],
    [
     "Enrico",
     "Bocchieri"
    ]
   ],
   "title": "Training of context-dependent subspace distribution clustering hidden Markov model",
   "original": "i98_0699",
   "page_count": 4,
   "order": 185,
   "p1": "paper 0699",
   "pn": "",
   "abstract": [
    "Training of continuous density hidden Markov models(CDHMMs) is usually time-consuming and tedious due to the large number of model parameters involved. Recently we proposed a new derivative of CDHMM, the subspace distribution clustering hidden Markov model(SDCHMM) which tie CDHMMs at the finer level of subspace distributions, resulting in many fewer model parameters. An SDCHMM training algorithm is also devised to train SDCHMMs directly from speech data without intermediate CDHMMs. On the ATIS task, speaker-independent context-independent(CI) SDCHMMs can be trained with as little as 8 minutes of speech with no loss in recognition accuracy --- a 25-fold reduction when compared with their CDHMM counterparts. In this paper, we extend our novel SDCHMM training to context-dependent(CD) modeling with the assumption of various prior knowledge. Despite the 30-fold increase of model parameters in the CD ATIS CDHMMs, their equivalent CD SDCHMMs can still be estimated with a few minutes of ATIS data.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-185"
  },
  "martindelalamo98_icslp": {
   "authors": [
    [
     "Cesar",
     "Martin del Alamo"
    ],
    [
     "Luis",
     "Villarrubia"
    ],
    [
     "Francisco Javier",
     "Gonzalez"
    ],
    [
     "Luis A.",
     "Hern√°ndez"
    ]
   ],
   "title": "Unsupervised training of HMMs with variable number of mixture components per state",
   "original": "i98_0443",
   "page_count": 4,
   "order": 186,
   "p1": "paper 0443",
   "pn": "",
   "abstract": [
    "In this work automatic methods for determining the number of gaussians per state in a set of Hidden Markov Models are studied. Four different mix-up criteria are proposed to decide how to increase the size of the states. These criteria, derived from Maximum Likelihood scores, are focused to increase the discrimination between states obtaining different number of gaussians per state. We compare these proposed methods with the common approach where the number of density functions used in every state is equal and pre-fixed by the designer. Experimental results demonstrate that performance can be maintained while reducing the total number of density functions by 17% (from 2046 down to 1705). These results are obtained in a flexible large vocabulary isolated word recognizer using context dependent models.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-186"
  },
  "szarvas98_icslp": {
   "authors": [
    [
     "Mate",
     "Szarvas"
    ],
    [
     "Shoichi",
     "Matsunaga"
    ]
   ],
   "title": "Acoustic observation context modeling in segment based speech recognition",
   "original": "i98_1098",
   "page_count": 4,
   "order": 187,
   "p1": "paper 1098",
   "pn": "",
   "abstract": [
    "This paper describes a novel method that models the correlation between acoustic observations in contiguous speech segments. The basic idea behind the method is that acoustic observations are conditioned not only on the phonetic context but also on the preceding acoustic segment observation. The correlation between consecutive acoustic observations is modeled by polynomial mean trajectory segment models. This method is an extension of conventional segment modeling approaches in that it not only describes the correlation of acoustic observations inside segments but also between contiguous segments. It is also a generalization of phonetic context (e.g., triphone) modeling approaches because it can model acoustic context and phonetic context at the same time. In a speaker-independent phoneme classification test, using the proposed method resulted in a 7-9% reduction in error rate as compared to the traditional triphone segmental model system and a 31% reduction as compared to a similar triphone HMM system.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-187"
  },
  "ming98_icslp": {
   "authors": [
    [
     "Ji",
     "Ming"
    ],
    [
     "Philip",
     "Hanna"
    ],
    [
     "Darryl",
     "Stewart"
    ],
    [
     "Saeed",
     "Vaseghi"
    ],
    [
     "F. Jack",
     "Smith"
    ]
   ],
   "title": "Capturing discriminative information using multiple modeling techniques",
   "original": "i98_0263",
   "page_count": 4,
   "order": 188,
   "p1": "paper 0263",
   "pn": "",
   "abstract": [
    "An acoustic model is a simplified mathematical representation of acoustic-phonetic information. The simplifying assumptions inherent to each model entail that it may only be capable of capturing a certain aspect of the available information. An effective combination of different types of model should therefore permit a combined model that can utilize all the information captured by the individual models. This paper reports some preliminary research in combining certain types of acoustic model for speech recognition. In particular, we designed and implemented a single HMM framework, which combines a segment-based modeling technique with the standard HMM technique. The recognition experiments, based on a speaker-independent E-set database, have shown that the combined model has the potential of producing a significantly higher performance than the individual models considered in isolation.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-188"
  },
  "molloy98_icslp": {
   "authors": [
    [
     "Laurence",
     "Molloy"
    ],
    [
     "Stephen",
     "Isard"
    ]
   ],
   "title": "Suprasegmental duration modelling with elastic constraints in automatic speech recognition",
   "original": "i98_1103",
   "page_count": 4,
   "order": 189,
   "p1": "paper 1103",
   "pn": "",
   "abstract": [
    "In this paper a method of integrating a model of suprasegmental duration with a HMM-based recogniser at the post-processing level is presented. The N-Best utterance output is rescored using a suitable linear combination of acoustic log-likelihood (provided by a set of tied-state triphone HMMs) and duration log-likelihood (provided by a set of durational models). The durational model used in the post-processing imposes syllable-level elastic constraints on the durational behaviour of speech segments. Results are presented for word accuracy on the Resource Management database after rescoring, using two different syllable-like constraint units, a fixed-size N-phone window and simple (no constraint) phone duration probability scoring.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-189"
  },
  "nogueirasrodriguez98_icslp": {
   "authors": [
    [
     "Albino",
     "Nogueiras-Rodriguez"
    ],
    [
     "Jos√© B.",
     "Mari√±o"
    ],
    [
     "Enric",
     "Monte"
    ]
   ],
   "title": "An adaptive gradient-search based algorithm for discriminative training of HMM's",
   "original": "i98_0769",
   "page_count": 4,
   "order": 190,
   "p1": "paper 0769",
   "pn": "",
   "abstract": [
    "Although having revealed to be a very powerful tool in acoustic modelling, discriminative training presents a major drawback: the lack of a formulation guaranteeing convergence in no matter which initial conditions, such as the Baum-Welch algorithm in maximum likelihood training. For this reason, a gradient descent search is usually used in this kind of problem. Unfortunately, standard gradient descent algorithms rely heavily on the election of the learning rates. This dependence is specially cumbersome because it represents that, at each run of the discriminative training procedure, a search should be carried out over the parameters ruling the algorithm. In this paper we describe an adaptive procedure for determining the optimal value of the step size at each iteration. While the calculus and memory overhead of the algorithm is negligible, results show less dependence on the initial learning rate than standard gradient descent and, using the same idea in order to apply self-scaling, it clearly outperforms it.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-190"
  },
  "nogueirasrodriguez98b_icslp": {
   "authors": [
    [
     "Albino",
     "Nogueiras-Rodriguez"
    ],
    [
     "Jos√© B.",
     "Mari√±o"
    ]
   ],
   "title": "Task adaptation of sub-lexical unit models using the minimum confusibility criterion on task independent databases",
   "original": "i98_0770",
   "page_count": 4,
   "order": 191,
   "p1": "paper 0770",
   "pn": "",
   "abstract": [
    "Discriminative training is a powerful tool in acoustic modeling for automatic speech recognition. Its strength is based on the direct minimisation of the number of errors committed by the system at recognition time. This is usually accomplished by defining an auxiliary function that characterises the behaviour of the system, and adjusting the parameters of the system in a way that this function is minimised. The main drawback of this approach is that a task specific training database is needed. In this paper an alternative procedure is proposed: task adaptation using task independent databases. It consists in the combination of acoustic information -estimated using a general purpose training database-, and linguistic information -taken from the definition of the task-. In the experiments carried out, this technique has led to great improvement in the recognition of two different tasks: clean speech digit strings in English and dates in Spanish over the telephone wire.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-191"
  },
  "ramsay98_icslp": {
   "authors": [
    [
     "Gordon",
     "Ramsay"
    ]
   ],
   "title": "Stochastic calculus, non-linear filtering, and the internal model principle: implications for articulatory speech recognition",
   "original": "i98_0671",
   "page_count": 4,
   "order": 192,
   "p1": "paper 0671",
   "pn": "",
   "abstract": [
    "A stochastic approach to modelling speech production and perception is discussed, based on Ito calculus. Speech is modelled by a system of non-linear stochastic differential equations evolving on a finite-dimensional state space, representing a partially-observed Markov process. The optimal non-linear filtering equations for the model are stated, and shown to exhibit a predictor-corrector structure, which mimics the structure of the original system. This is used to suggest a possible justification for the hypothesis that speakers and listeners make use of an ``internal model'' in producing and perceiving speech, and leads to a useful statistical framework for articulatory speech recognition.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-192"
  },
  "wellekens98_icslp": {
   "authors": [
    [
     "Christian J.",
     "Wellekens"
    ],
    [
     "Jussi",
     "Kangasharju"
    ],
    [
     "Cedric",
     "Milesi"
    ]
   ],
   "title": "The use of meta-HMM in multistream HMM training for automatic speech recognition",
   "original": "i98_0271",
   "page_count": 4,
   "order": 193,
   "p1": "paper 0271",
   "pn": "",
   "abstract": [
    "Among the different attempts to improve recognition scores and robustness to noise, the recognition of parallel streams of data, each one representing partial information on the test signal, and the fusion of the decisions have received a great deal of interest. The problem of training such models taking recombination constraints at the level of speech-subunits has not yet been rigorously addressed. This paper shows how equivalence with an extended meta-HMM solves the problem and how reestimation formulas have to be applied to guarantee equivalence between the multistream model and the meta-HMM. Experiments demonstrate the importance of transition probabilities in the meta-HMM which they have to meet some constraints in order to represent a multistream HMM.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-193"
  },
  "wellekens98b_icslp": {
   "authors": [
    [
     "Christian J.",
     "Wellekens"
    ]
   ],
   "title": "Enhanced ASR by acoustic feature filtering",
   "original": "i98_0272",
   "page_count": 4,
   "order": 194,
   "p1": "paper 0272",
   "pn": "",
   "abstract": [
    "Since long, the use of contextual features has been shown to improve the recognition scores: use of numerical estimations of speed and acceleration appended to the current feature vectors, predictive HMM or neural networks. All these implementations are particular case of FIR filtering of feature trajectories. This paper presents a new approach where the characteristics of filters are trained together with the HMM parameters resulting in improvements of the recognition in first tests. Reestimation formulas for the cut-off frequencies of ideal LP-filters are derived as well for the impulse response coefficients of a general FIR LP-filter. Filters can be either common to all feature vectors or dedicated to a given entry or a given HMM state.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-194"
  },
  "neukirchen98_icslp": {
   "authors": [
    [
     "Christoph",
     "Neukirchen"
    ],
    [
     "Daniel",
     "Willett"
    ],
    [
     "Gerhard",
     "Rigoll"
    ]
   ],
   "title": "Soft state-tying for HMM-based speech recognition",
   "original": "i98_0346",
   "page_count": 4,
   "order": 195,
   "p1": "paper 0346",
   "pn": "",
   "abstract": [
    "This paper introduces a method for regularization of HMM systems that avoids parameter overfitting caused by insufficient training data. Regularization is done by augmenting the EM training method by a penalty term that favors simple  and smooth HMM systems. The penalty term is constructed as a mixture model of negative exponential distributions that is assumed to generate the  state dependent emission probabilities of the HMMs. This new method is the successful transfer of a well known regularization approach in neural networks to the HMM domain and can be interpreted as a generalization of traditional state-tying for HMM systems. The effect of regularization is demonstrated for continuous speech recognition tasks by improving overfitted triphone models and by speaker adaptation with limited training data.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-195"
  },
  "witt98_icslp": {
   "authors": [
    [
     "Silke",
     "Witt"
    ],
    [
     "Steve",
     "Young"
    ]
   ],
   "title": "Estimation of models for non-native speech in computer-assisted language learning based on linear model combination",
   "original": "i98_1010",
   "page_count": 4,
   "order": 196,
   "p1": "paper 1010",
   "pn": "",
   "abstract": [
    "This paper investigates how to improve the acoustic modelling of non-native speech. For this purpose we present an adaptation technique to combine hidden Markov models of the source and the target language of a foreign language student. Such model combination requires a mapping of the mean vectors from target to source language. Therefore, three different mapping approaches, based on either phonetic knowledge and/or acoustical distance measures have been tested. The performance of this model combination method and several variations of it has been measured and compared with standard MLLR adaptation. For the baseline model combination small improvements of recognition accuracy compared to the results based on applying MLLR were obtained. Furthermore, slight improvements were found when using an a-priori approach, where the models were combined with predefined weights before applying any of the adaptation techniques.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-196"
  },
  "yang98b_icslp": {
   "authors": [
    [
     "Tae-Young",
     "Yang"
    ],
    [
     "Ji-Sung",
     "Kim"
    ],
    [
     "Chungyong",
     "Lee"
    ],
    [
     "Dae Hee",
     "Youn"
    ],
    [
     "Il-Whan",
     "Cha"
    ]
   ],
   "title": "Duration modeling using cumulative duration probability and speaking rate compensation",
   "original": "i98_0428",
   "page_count": 4,
   "order": 197,
   "p1": "paper 0428",
   "pn": "",
   "abstract": [
    "A duration modeling scheme and a speaking rate compensation technique are presented for the HMM based connected digit recognizer. The proposed duration modeling technique uses a cumulative duration probability. The cumulative duration probability also can be used to obtain the duration bounds for the bounded duration modeling. One of the advantages of proposed technique is that the cumulative duration probability can be applied directly to the Viterbi decoding procedure without additional postprocessing. Therefore, it rules the state and word transition at each frame. To alleviate the problems due to fast or slow speech, a modification to the bounded duration modeling which accounts for speaking rate is described. The experimental results on Korean connected digit recognition show the effectiveness of the proposed duration modeling scheme and the speaking rate compensation technique.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-197"
  },
  "zweig98_icslp": {
   "authors": [
    [
     "Geoffrey",
     "Zweig"
    ],
    [
     "Stuart",
     "Russell"
    ]
   ],
   "title": "Probabilistic modeling with Bayesian networks for automatic speech recognition",
   "original": "i98_0858",
   "page_count": 4,
   "order": 198,
   "p1": "paper 0858",
   "pn": "",
   "abstract": [
    "This paper describes the application of Bayesian networks to automatic speech recognition (ASR). Bayesian networks enable the construction of probabilistic models in which an arbitrary set of variables can be associated with each speech frame in order to explicitly model factors such as acoustic context, speaking rate, or articulator positions. Once the basic inference machinery is in place, a wide variety of models can be expressed and tested. We have implemented a Bayesian network system for isolated word recognition, and present experimental results on the PhoneBook database. These results indicate that performance improves when the observations are conditioned on an auxiliary variable modeling acoustic/articulatory context. The use of multivalued and multiple context variables further improves recognition accuracy.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-198"
  },
  "sivakumaran98_icslp": {
   "authors": [
    [
     "Perasiriyan",
     "Sivakumaran"
    ],
    [
     "Aladdin M.",
     "Ariyaeeinia"
    ],
    [
     "Jill A.",
     "Hewitt"
    ]
   ],
   "title": "Sub-band based speaker verification using dynamic recombination weights",
   "original": "i98_1055",
   "page_count": 4,
   "order": 199,
   "p1": "paper 1055",
   "pn": "",
   "abstract": [
    "The concept of splitting the entire frequency domain into sub-bands and processing the spectra in these bands independently in between every consecutive recombination stage to generate a final score has recently been proposed for speech recognition. Some of the aspects of this technique have also been studied for the task of speaker recognition. A remaining critical problem in this approach, however, is the determination of appropriate recombination weights. This paper presents a new method for generating these weights for sub-band based speaker verification. The approach is based on the use of background speaker models and aims to reduce the effect of any mismatch between the band-limited segments of the test utterance and the corresponding sections in the target speaker model. The paper also discusses a problem generally associated with the sub-band cepstral features and outlines a possible solution.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-199"
  },
  "barlow98_icslp": {
   "authors": [
    [
     "Michael",
     "Barlow"
    ],
    [
     "Michael",
     "Wagner"
    ]
   ],
   "title": "Measuring the dynamic encoding of speaker identity and dialect in prosodic parameters",
   "original": "i98_0979",
   "page_count": 4,
   "order": 200,
   "p1": "paper 0979",
   "pn": "",
   "abstract": [
    "This paper describes a methodology for analysing the dynamic encoding of identity and dialect in prosodic parameters. Properties of the well-known DTW (Dynamic Time Warping) path of best match allow the separation of dynamic from static properties of acoustic parameters. A database of 19 speakers of Australian English was recorded and F0, energy, zero crossing rate and voicing contours extracted. Discriminant analysis figures measured identity encoding, while correlation rates measured dialect. Dynamic encoding levels were found to be significantly higher than static for both speaker characteristics (identity: 75% versus 55%; dialect: 0.58 versus 0.45).Normalisation of acoustic parameters into the range 0--1, eliminating all static information, reduced encoding levels to 70% (identity) and 0.52 (dialect) showing the robustness of dynamic encoding. Contrasting DTW warp path properties with the DTW distance showed the warp path a significantly better extractor of encoded information (72% versus 54% for identity; 0.45 versus 0.30 for dialect).\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-200"
  },
  "beringer98_icslp": {
   "authors": [
    [
     "Nicole",
     "Beringer"
    ],
    [
     "Florian",
     "Schiel"
    ],
    [
     "Peter",
     "Regel-Brietzmann"
    ]
   ],
   "title": "German regional variants - a problem for automatic speech recognition?",
   "original": "i98_0201",
   "page_count": 4,
   "order": 201,
   "p1": "paper 0201",
   "pn": "",
   "abstract": [
    "A well known problem in automatic speech recognition (ASR) is robustness against the variability of speech between speakers. There are several ways to normalise different speakers; one of them is to deal with the problem of regional variation. In this paper we discuss the problem of whether moderate regional variants of German influence the ASR process and whether there is a way to improve performance through knowledge of the regional origin of the unknown speaker. The basic idea is to cluster test speakers into distinct dialectal regions and derive observations about the typical pronunciation within these regions from a classified training set. In a cheating experiment where the origin of the test speakers is known we verify whether the use of the dialect-specific pronunciation forms will improve the overall performance. It turns out that simply using dialect-specific pronunciation does not significantly improve word accuracy on the VERBMOBIL task.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-201"
  },
  "berkling98_icslp": {
   "authors": [
    [
     "Kay",
     "Berkling"
    ],
    [
     "Marc A.",
     "Zissman"
    ],
    [
     "Julie",
     "Vonwiller"
    ],
    [
     "Christopher",
     "Cleirigh"
    ]
   ],
   "title": "Improving accent identification through knowledge of English syllable structure",
   "original": "i98_0394",
   "page_count": 4,
   "order": 202,
   "p1": "paper 0394",
   "pn": "",
   "abstract": [
    "This paper studies the structure of foreign-accented read English speech. A system for accent identification is constructed by combining linguistic theory with statistical analysis. Results demonstrate that the linguistic theory is reflected in real speech data and its application improves accent identification. The work discussed here combines and applies previous research in language identification based on phonemic features with the analysis of the structure and function of the English language. Working with phonemically handlabelled data in three accented speaker groups of Australian English (Vietnamese, Lebanese, and native speakers), we show that accents of foreign speakers can be predicted and manifest themselves differently as a function of their position within the syllable. When applying this knowledge, English vs. Vietnamese accent identification improves from 86% to 93% (English vs. Lebanese improves from 78% to 84%). The described algorithm is also successfully applied to automatically aligned phonemes.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-202"
  },
  "bond98_icslp": {
   "authors": [
    [
     "Z. S.",
     "Bond"
    ],
    [
     "Donald",
     "Fucci"
    ],
    [
     "Verna",
     "Stockmal"
    ],
    [
     "Douglas",
     "McColl"
    ]
   ],
   "title": "Multi-dimensional scaling of listener responses to complex auditory stimuli",
   "original": "i98_0163",
   "page_count": 3,
   "order": 203,
   "p1": "paper 0163",
   "pn": "",
   "abstract": [
    "This study explored the attributes of languages to which listeners attend, using magnitude estimation and multidimensional scaling techniques. In magnitude estimation. listeners assign any numerical value to a set of stimuli. In response to the question: How similar is this language to English'? fifty college students assigned numerical values to spoken samples of foreign languages. The languages represented Europe, Asia and Africa. Differences between the mean ratings for each language and English were used to construct a proximity matrix which was submitted to MDS analysis. The optimum solution employed three dimensions. The first dimension was interpreted as \"familiarity,\" the second as \"speaker affect,\" and the third as \"prosodic pattern.\" The MDS maps suggest that listeners were using English as a standard of comparison to the acoustic-phonetic properties of other languages. The maps resulted from the relationship between each language and the standard. and speaker and language characteristics which listeners found salient.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-203"
  },
  "stockmal98_icslp": {
   "authors": [
    [
     "Verna",
     "Stockmal"
    ],
    [
     "Danny R.",
     "Moates"
    ],
    [
     "Z. S.",
     "Bond"
    ]
   ],
   "title": "Same talker, different language",
   "original": "i98_0164",
   "page_count": 4,
   "order": 204,
   "p1": "paper 0164",
   "pn": "",
   "abstract": [
    "Listeners can easily say whether a language they are hearing is familiar or foreign to them. Infants, young children, and adults are able to make same-language, different-language judgments at better than chance levels. In many of these studies, foreign language samples have been provided by different talkers so that language and talker characteristics have been confounded. We conducted three experiments using the same talker for different pairs of language. Listeners were able to discriminate between two languages they do not know even when spoken by the same talker, suggesting that listeners can distinguish talker characteristics from language characteristics.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-204"
  },
  "burger98_icslp": {
   "authors": [
    [
     "Susanne",
     "Burger"
    ],
    [
     "Daniela",
     "Oppermann"
    ]
   ],
   "title": "The impact of regional variety upon specific word categories in spontaneous German",
   "original": "i98_0508",
   "page_count": 4,
   "order": 205,
   "p1": "paper 0508",
   "pn": "",
   "abstract": [
    "The aim of the work to be reported here is to verify that pronunciation variety in German spontaneous speech appears in specific linguistic word categories (POS) while others are less affected. Additionally, the material (transliterations of spontaneous monologues of the RVG1-corpus [5]) allows a detailed view on pronunciation regarding the speakers' origin within the German-speaking regions and regarding the topic a speaker is talking about. Our results show that generally the most affected parts of speech are the auxiliary verbs. The regions with the highest deviation rates of standard German are Switzerland and Austria, and the regions in southern Germany. We can only add the vague answer to the question, whether the semantically topic of a story may have an influence upon the deviation of the standard language, that our results show a striking less frequency of pronunciation variants, when people speak about their jobs.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-205"
  },
  "genoud98_icslp": {
   "authors": [
    [
     "Dominique",
     "Genoud"
    ],
    [
     "G√©rard",
     "Chollet"
    ]
   ],
   "title": "Speech pre-processing against intentional imposture in speaker recognition",
   "original": "i98_0734",
   "page_count": 11,
   "order": 206,
   "p1": "paper 0734",
   "pn": "",
   "abstract": [
    "Recently, some large-scale text dependent speaker verification systems have been tested. They show that less than 1% Equal Error Rate can be obtained on a test set score distribution. So far, the majority of impostor tests are performed using speakers who don't really try to fool the system. This can be explained by the lack of databases recorded for this purpose, and the difficulty for a normal speaker to transform his voice characteristics. Nevertheless, actual automatic analysis/synthesis techniques, such as Harmonic plus Noise Model (H+N), allows very good speech/speaker transformations. Thus, it becomes possible to transform the voice of a speaker in the voice of another speaker in order to make voluntary impostures. This paper evaluates these kind of intrusive impostures and proposes a new speech pre-processing method, based on harmonic subtraction, making speaker verification less insensitive to these spectral transformations. A state-of-the-art Hidden Markov Model is used as reference system to assess the transformation results. The speech is parameterised by LPCC coefficients. The results are obtained on a database of telephone speech quality. The speaker verification system works in text dependent mode.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-206"
  },
  "lincoln98_icslp": {
   "authors": [
    [
     "Mike",
     "Lincoln"
    ],
    [
     "Stephen",
     "Cox"
    ],
    [
     "Simon",
     "Ringland"
    ]
   ],
   "title": "A comparison of two unsupervised approaches to accent identification",
   "original": "i98_0465",
   "page_count": 4,
   "order": 207,
   "p1": "paper 0465",
   "pn": "",
   "abstract": [
    "The ability to automatically identify a speaker's accent would be very useful for a speech recognition system as it would enable the system to use both a pronunciation dictionary and speech models specific to the accent, techniques which have been shown to improve accuracy. Here, we describe some experiments in unsupervised accent classification. Two techniques have been investigated to classify British- and American-accented speech: an acoustic approach, in which we analyse the pattern of usage of the distributions in the recogniser by a speaker to decide on his most probable accent, and a high-level approach in which we use a phonotactic model for classification of the accent. Results show that both techniques give excellent performance on this task which is maintained when testing is done on data from an independent dataset.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-207"
  },
  "dersch98_icslp": {
   "authors": [
    [
     "Dominik R.",
     "Dersch"
    ],
    [
     "Christopher",
     "Cleirigh"
    ],
    [
     "Julie",
     "Vonwiller"
    ]
   ],
   "title": "The influence of accents in australian English vowels and their relation to articulatory tract parameters",
   "original": "i98_0188",
   "page_count": 4,
   "order": 208,
   "p1": "paper 0188",
   "pn": "",
   "abstract": [
    "We analyse and compare a low dimensional linguistic representation of vowels with high dimensional prototypical vowel templates derived from native Australian English speaker. To simplify the problem, the study is restricted to a group of short and long vowels. In a low dimensional linguistic representation a vowel is represented by the horizontal and vertical position of the part of the tongue involved in the key articulation of a particular vowel, e.g., high or low and front or back. To this is added lip posture, spread or rounded. For a comparison we perform a multidimensional scaling transformation of high dimensional vowel clusters derived from speech samples. We further performed the same analysis on Lebanese and Vietnamese accented English to investigate how differences due to accents impact on such a representation.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-208"
  },
  "preez98b_icslp": {
   "authors": [
    [
     "J. A. du",
     "Preez"
    ],
    [
     "D. M.",
     "Weber"
    ]
   ],
   "title": "Automatic language recognition using high-order HMMs",
   "original": "i98_1074",
   "page_count": 4,
   "order": 209,
   "p1": "paper 1074",
   "pn": "",
   "abstract": [
    "We present automatic language recognition results using high-order hidden Markov models (HMM) and the recently developed ORder rEDucing (ORED) and Fast Incremental Training (FIT) HMM algorithms. We demonstrate the efficiency and accuracy of pseudo-phoneme context and duration modelling mixed-order HMMs as well as fixed order HMMs over conventional approaches. For a two language problem, we show that a third-order FIT trained HMM gives a test set accuracy of 97.4% compared to 89.7% for a conventionally trained third-order HMM. A first-order model achieved 82.1% accuracy on the same problem.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-209"
  },
  "faundezzanuy98_icslp": {
   "authors": [
    [
     "Marcos",
     "Faundez-Zanuy"
    ],
    [
     "Daniel",
     "Rodriguez-Porcheron"
    ]
   ],
   "title": "Speaker recognition using residual signal of linear and nonlinear prediction models",
   "original": "i98_1102",
   "page_count": 4,
   "order": 210,
   "p1": "paper 1102",
   "pn": "",
   "abstract": [
    "This Paper discusses the usefullness of the residual signal for speaker recognition. It is shown that the combination of both a measure defined over LPCC coefficients and a measure defined over the energy of the residual signal gives rise to an improvement over the classical method which considers only the LPCC coefficients. If the residual signal is obtained from a linear prediction analisys, the improvement is 2.63% (error rate drops from 6.31% to 3.68%) and if it is computed through a nonlinear predictive neural nets based model, the improvement is 3.68%.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-210"
  },
  "gu98_icslp": {
   "authors": [
    [
     "Yong",
     "Gu"
    ],
    [
     "Trevor",
     "Thomas"
    ]
   ],
   "title": "An implementation and evaluation of an on-line speaker verification system for field trials",
   "original": "i98_0070",
   "page_count": 4,
   "order": 211,
   "p1": "paper 0070",
   "pn": "",
   "abstract": [
    "This paper presents a HMM-based speaker verification system which was implemented for a field trial. One of the challenges for moving HMM from speech recognition to speaker verification is to understand the HMM score variation and to define a proper measurement which is comparable across speech samples. In this paper we define two basic verification measurements, a qualifier-based measurement and a competition-based measurement, and examine score normalisation approaches using these two measurements. This leads to some useful theoretical differentiation between cohort model and world model approaches used for HMM score normalisation. We adopted a world model method for score normalisation in the system. The adaptive variance flooring technique is also implemented in the system. The paper presents evaluation results of the implementation.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-211"
  },
  "hernando98_icslp": {
   "authors": [
    [
     "Javier",
     "Hernando"
    ],
    [
     "Climent",
     "Nadeu"
    ]
   ],
   "title": "Speaker verification on the polycost database using frequency filtered spectral energies",
   "original": "i98_0724",
   "page_count": 4,
   "order": 212,
   "p1": "paper 0724",
   "pn": "",
   "abstract": [
    "The spectral parameters that result from filtering the frequency sequence of log mel-scaled filter-bank energies with a first or second order FIR filter have proved to be competitive for speech recognition. Recently, the authors have shown that this frequency filtering can approximately equalize the cepstrum variance enhancing the oscillations of the spectral envelope curve that are most effective for discrimination between speakers. Even better speaker identification results than using mel-cepstrum were observed on the TIMIT database, especially when white noise was added. In this paper, the hybridization of both linear prediction and filter-bank spectral analysis using either cepstral transformation or the alternative frequency filtering is explored for speaker verification. This combination, that had shown to be able to outperform the conventional techniques in clean and noisy word recognition, has yield good text-dependent speaker verification results on the new speaker-oriented telephone-line POLYCOST database.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-212"
  },
  "jin98_icslp": {
   "authors": [
    [
     "Qin",
     "Jin"
    ],
    [
     "Luo",
     "Si"
    ],
    [
     "Qixiu",
     "Hu"
    ]
   ],
   "title": "A high-performance text-independent speaker identification system based on BCDM",
   "original": "i98_1112",
   "page_count": 4,
   "order": 213,
   "p1": "paper 1112",
   "pn": "",
   "abstract": [
    "This paper describes a Text-Independent Speaker Identification System of high performance. This system includes two subsystems, one is the close-set speaker identification system; the other is the open-set speaker identification system. In the implementation of the Text-Independent Speaker Identification System we introduce an advanced VQ method and a new distance estimation algorithm called BCDM (Based on Codes Distribution Method). In the close-set identification, the Correct Recognition Rate is 98.5% as there are 50 speakers in the training set. In the open-set identification, the Equal Error Rate is 5% as there are 40 speakers in the training set.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-213"
  },
  "kido98_icslp": {
   "authors": [
    [
     "Hiroshi",
     "Kido"
    ],
    [
     "Hideki",
     "Kasuya"
    ]
   ],
   "title": "Representation of voice quality features associated with talker individuality",
   "original": "i98_1005",
   "page_count": 4,
   "order": 214,
   "p1": "paper 1005",
   "pn": "",
   "abstract": [
    "As a first step toward development of a \"speech montage system\", this paper attempts to derive a core set of Japanese epithets which are commonly used in an everyday life to represent voice quality features associated with talker individuality. Perceptual experiments were conducted, where subjects were asked to evaluate sentence utterances recorded from a variety of male speakers in terms of 25 epithets which were derived in another experiment [1] to be indicative of voice quality relevant to talker individuality. The evaluation scores were subjected to a statistical clustering analysis. The analysis resulted in that the 25 epithets could be grouped into either eight categories for male or seven for female subjects. These categories were basically the same as those obtained in the previous experiment [1] where subjects were required to evaluate their own voice with the same set of 25 epithets. Agreement between the results from the two experiments guarantees reliability of the core epithet categories to represent voice quality associated with talker individuality.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-214"
  },
  "kim98b_icslp": {
   "authors": [
    [
     "Ji-Hwan",
     "Kim"
    ],
    [
     "Gil-Jin",
     "Jang"
    ],
    [
     "Seong-Jin",
     "Yun"
    ],
    [
     "Yung Hwan",
     "Oh"
    ]
   ],
   "title": "Candidate selection based on significance testing and its use in normalisation and scoring",
   "original": "i98_0261",
   "page_count": 4,
   "order": 215,
   "p1": "paper 0261",
   "pn": "",
   "abstract": [
    "Log likelihood ratio normalisation and scoring methods have been studied by many researchers and have improved the performance of speaker identification systems. However, these studies have disadvantages: the recognised distorted speech segments are different for each speaker. Also the background model in log likelihood ratio normalisation is changed in each speech segment even for the same speaker. This paper presents two techniques. Firstly, candidate selection based on significance testing, which designs the background speaker model more accurately. And secondly, the scoring method, which recognises the same distorted speech segments for every speaker. We perform a number of experiments with the SPIDRE database.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-215"
  },
  "kinoshita98_icslp": {
   "authors": [
    [
     "Yuko",
     "Kinoshita"
    ]
   ],
   "title": "Japanese forensic phonetics: non-contemporaneous within-speaker variation in natural and read-out speech",
   "original": "i98_0652",
   "page_count": 4,
   "order": 216,
   "p1": "paper 0652",
   "pn": "",
   "abstract": [
    "This paper aims to explore non-contemporaneous within-speaker variation of a Japanese male speaker, focusing on the difference between speech styles, viz.. natural speech and read-out speech. Recordings under forensic conditions are mostly of natural speech. A suspect[HEX 146]s recording to be compared are, however, sometimes read-out speech, but not natural speech, in order to obtain the similar phonological conditions to the original criminal speech. This paper aims to examine the validity of such a procedure.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-216"
  },
  "korkmazskiy98_icslp": {
   "authors": [
    [
     "Filipp",
     "Korkmazskiy"
    ],
    [
     "Biing-Hwang",
     "Juang"
    ]
   ],
   "title": "Statistical modeling of pronunciation and production variations for speech recognition",
   "original": "i98_0345",
   "page_count": 4,
   "order": 217,
   "p1": "paper 0345",
   "pn": "",
   "abstract": [
    "In this paper, we propose a procedure for training a pronunciation network with criteria consistent with the optimality objectives for speech recognition systems. In particular, we describe a framework for using maximum likelihood(ML) and minimum classification error(MCE) criteria for pronunciation network optimization. The ML criterion is used to obtain an optimal structure for the pronunciation network based on statistically-derived phonological rules. Discrimination among different pronunciation networks is achieved by weighting of the pronunciation networks, optimized by applying the MCE criterion. Experiment results demonstrate improvements in speech recognition accuracy after applying statistically derived phonological rules. It is shown that the impact of the pronunciation network weighting on the recognition performance is determined by the size of the recognition vocabulary.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-217"
  },
  "foldvik98_icslp": {
   "authors": [
    [
     "Arne Kjell",
     "Foldvik"
    ],
    [
     "Knut",
     "Kvale"
    ]
   ],
   "title": "Dialect maps and dialect research; useful tools for automatic speech recognition?",
   "original": "i98_0470",
   "page_count": 4,
   "order": 218,
   "p1": "paper 0470",
   "pn": "",
   "abstract": [
    "ABSTRACT Traditional dialect maps are based on data from carefully selected informants which usually results in clear-cut dialect borders, isoglosses, with one dialect characteristic present on one side of the isogloss and absent on the other. We illustrate some of the problems and pitfalls connected with using dialect maps for ASR by comparing results from traditional dialect research with investigations of the Norwegian part of the European SpeechDat database, centred on the two main types of /r/ pronunciation. Our analysis shows that traditional dialect maps and surveys may be of limited use in ASR. To what extent the Norwegian findings have parallels in other countries will depend on two main factors, dialect allegiance vs. a national standard pronunciation and the extent to which the population is sedentary or mobile. Results from traditional dialect research may therefore be more useful in ASR of other languages than Norwegian.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-218"
  },
  "kyung98_icslp": {
   "authors": [
    [
     "Youn-Jeong",
     "Kyung"
    ],
    [
     "Hwang-Soo",
     "Lee"
    ]
   ],
   "title": "Text independent speaker recognition using micro-prosody",
   "original": "i98_0407",
   "page_count": 4,
   "order": 219,
   "p1": "paper 0407",
   "pn": "",
   "abstract": [
    "The acoustic aspects that differentiate voices are difficult to separate from signal traits that reflect the identity of the sounds. There are two sources of variation among speakers: (1) differences in vocal cords and vocal tract shape, and (2) differences in speaking style. The latter includes variations in both target vocal tract positions for phonemes and dynamic aspects of speech, such as speaking rate. However, most parameters and features are in the former. In this paper, we propose the use of a prosodic feature that represents micro prosody of utterances. The robustness of the prosodic feature on noise environment becomes clear. Also we propose a combined model. The combined model uses both the spectral feature and the prosodic feature. In our experiments, this model provides robust speaker recognition in noise environments.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-219"
  },
  "cheng98_icslp": {
   "authors": [
    [
     "Yoik",
     "Cheng"
    ],
    [
     "Hong C.",
     "Leung"
    ]
   ],
   "title": "Speaker verification using fundamental frequency",
   "original": "i98_0228",
   "page_count": 4,
   "order": 220,
   "p1": "paper 0228",
   "pn": "",
   "abstract": [
    "This paper describes the use of speech fundamental frequency (F0) for speaker verification. Both Chinese and English have been included in this study, with Chinese representing a tonal language and English representing a non-tonal language. A HMM-based speaker verification system has been developed, using features based on cepstral coefficients and the F0 contour. Four different techniques have been investigated in our experiments on the YOHO database and a Chinese speech database similar to YOHO. It has been found that the pitch information results in a reduction of the equal error rates by 40.5% and 33.9% in Cantonese and English, respectively, suggesting that the pitch information is important for speaker verification and that it is more important for tonal languages. We have also found that the pitch information is even more effective when it is represented in the log domain, resulting in an ERR of 2.28% for Cantonese. This ERR corresponds to a reduction of the ERR by 54%.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-220"
  },
  "liu98_icslp": {
   "authors": [
    [
     "Weijie",
     "Liu"
    ],
    [
     "Toshihiro",
     "Isobe"
    ],
    [
     "Naoki",
     "Mukawa"
    ]
   ],
   "title": "On optimum normalization method used for speaker verification",
   "original": "i98_1045",
   "page_count": 4,
   "order": 221,
   "p1": "paper 1045",
   "pn": "",
   "abstract": [
    "Score normalization has become necessary for speaker verification systems, but general principles leading to optimum performance are lacking. In the paper, theoretical analyses to optimum normalization are given. Under the analyses, four existing methods based on likelihood ratio, cohort, a posteriori probability and pooled cohort are investigated. Performance of these methods in verification with known imposters, robustness for different imposters and separability of the optimal threshold from the imposter model are discussed after experiments based on a database of 100 speakers.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-221"
  },
  "lloydthomas98_icslp": {
   "authors": [
    [
     "Harvey",
     "Lloyd-Thomas"
    ],
    [
     "Eluned S.",
     "Parris"
    ],
    [
     "Jeremy H.",
     "Wright"
    ]
   ],
   "title": "Recurrent substrings and data fusion for language recognition",
   "original": "i98_1061",
   "page_count": 4,
   "order": 222,
   "p1": "paper 1061",
   "pn": "",
   "abstract": [
    "Recurrent phone substrings that are characteristic of a language are a promising technique for language recognition. In previous work on language recognition, building anti-models to normalise the scores from acoustic phone models for target languages, has been shown to reduce the Equal Error Rate (EER) by a third. Recurrent substrings and anti-models have now been applied alongside three other techniques (bigrams, usefulness and frequency histograms) to the NIST 1996 Language Recognition Evaluation, using data from the CALLFRIEND and OGI databases for training. By fusing scores from the different techniques using a multi-layer perceptron the ERR on the NIST data can be reduced further.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-222"
  },
  "markov98_icslp": {
   "authors": [
    [
     "Konstantin P.",
     "Markov"
    ],
    [
     "Seiichi",
     "Nakagawa"
    ]
   ],
   "title": "Text-independent speaker recognition using multiple information sources",
   "original": "i98_0744",
   "page_count": 4,
   "order": 223,
   "p1": "paper 0744",
   "pn": "",
   "abstract": [
    "In the speaker recognition, when the cepstral coefficients are calculated from the LPC analysis parameters, the LPC residual and pitch are usually ignored. This paper describes an approach to integrate the pitch and LPC-residual with the LPC-cepstrum in a Gaussian Mixture Model based speaker recognition system. The pitch and LPC-residual are represented as a logarithm of the F0 and as a MFCC vector respectively. The second task of this research is to verify whether the correlation between the different information sources is useful for the speaker recognition task. The results showed that adding the pitch gives significant improvement only when the correlation between the pitch and cepstral coefficients is used. Adding only LPC-residual also gives significant improvement, but using the correlation with the cepstral coefficients does not have big effect. The best achieved results are 98.5% speaker identification rate and 0.21% speaker verification equal error rate compared to 97.0% and 1.07% of the baseline system, respectively.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-223"
  },
  "markov98b_icslp": {
   "authors": [
    [
     "Konstantin P.",
     "Markov"
    ],
    [
     "Seiichi",
     "Nakagawa"
    ]
   ],
   "title": "Discriminative training of GMM using a modified EM algorithm for speaker recognition",
   "original": "i98_0745",
   "page_count": 4,
   "order": 224,
   "p1": "paper 0745",
   "pn": "",
   "abstract": [
    "In this paper, we present a new discriminative training method for Gaussian Mixture Models (GMM) and its application for the text-independent speaker recognition. The objective of this method is to maximize the frame level normalized likelihoods of the training data. That is why we call it the Maximum Normalized Likelihood Estimation (MNLE). In contrast to other discriminative algorithms, the objective function is optimized using a modified Expectation- Maximization (EM) algorithm which greatly simplifies the training procedure. The evaluation experiments using both clean and telephone speech showed improvement of the recognition rates compared to the Maximum Likelihood Estimation (MLE) trained speaker models, especially when the mismatch between the training and testing conditions is significant.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-224"
  },
  "matrouf98_icslp": {
   "authors": [
    [
     "Driss",
     "Matrouf"
    ],
    [
     "Martine",
     "Adda-Decker"
    ],
    [
     "Lori F.",
     "Lamel"
    ],
    [
     "Jean-Luc",
     "Gauvain"
    ]
   ],
   "title": "Language identification incorporating lexical information",
   "original": "i98_0990",
   "page_count": 4,
   "order": 225,
   "p1": "paper 0990",
   "pn": "",
   "abstract": [
    "In this paper we explore the use of lexical information for language identification (LID). Our reference LID system uses language-dependent acoustic phone models and phone-based bigram language models. For each language, lexical information is introduced by augmenting the phone vocabulary with the N most frequent words in the training data. Combined phone and word bigram models are used to provide linguistic constraints during acoustic decoding. Experiments were carried out on a 4-language telephone speech corpus. Using lexical information achieves a relative error reduction of about 20% on spontaneous and read speech compared to the reference phone-based system. Identification rates of 92%, 96% and 99% are achieved for spontaneous, read and task-specific speech segments respectively, with prior speech detection.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-225"
  },
  "monte98_icslp": {
   "authors": [
    [
     "Enric",
     "Monte"
    ],
    [
     "Ramon",
     "Arqu√©"
    ],
    [
     "Xavier",
     "Mir√≥"
    ]
   ],
   "title": "A VQ based speaker recognition system based in histogram distances. text independent and for noisy environments",
   "original": "i98_1145",
   "page_count": 5,
   "order": 226,
   "p1": "paper 1145",
   "pn": "",
   "abstract": [
    "In speaker recognition systems based on VQ, normally each speaker is assigned a codebook, and the classification is done by means of the a distortion distance of the utterance computed by means of each codebook. In [1] we proposed a system which instead of having a codebook for each speaker, had only one codebook for all the speakers, and for each speaker one histogram. This histogram was the occupancy rate of each codeword for a given speaker. This means that the information of the histogram of a given speaker is the probability that the speaker utters the information related to the codeword. So we approximated the pdf of each speaker by the normalized histogram. In this paper we present an exhaustive study of different measures for comparing histograms: Kullbach-Leiber, log-difference of each probability, geometrical distance, and the Euclidean distance. We have done also an exhaustive study of the properties of the system for each distance in the presence of noise (white and colored), and for different parameterizations: LPC, MFCC, LPC-Cepstrum-OSA (One sided autocorrelation sequence), LCP-Cepstrum. (Cepstrum with/without liftering). As the combination of experiments was high, the conclusions were drawn after an analysis of variance (ANOVA), and T-tests. Thus the conclusions, with significance levels, can be drawn about the differences and interactions between kind of. distance, parameterization, kind of noise and level of noise.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-226"
  },
  "moreno98_icslp": {
   "authors": [
    [
     "Asunci√≥n",
     "Moreno"
    ],
    [
     "Jos√© B.",
     "Mari√±o"
    ]
   ],
   "title": "Spanish dialects: phonetic transcription",
   "original": "i98_0598",
   "page_count": 4,
   "order": 227,
   "p1": "paper 0598",
   "pn": "",
   "abstract": [
    "It is well known that canonical Spanish, the dialectal variant `central' of Spain, so called Castilian, can be transcribed by rules. This paper deals with the automatic grapheme to phoneme transcription rules in several Spanish dialects from Latin America. Spanish is a language spoken by more than 300 million people, has an important geographical dispersion compared among other languages and has been historically influenced by many native languages. In this paper authors expand the Castilian transcription rules to a set of different dialectal variants of Latin America. Transcriptions are based on SAMPA symbols. The paper includes an identification of sounds that doesn't appear in Castilian, extend accepted SAMPA symbols for Spanish (Castilian) to different dialectal variants, describes the necessary rules to implement an automatic Orthographic to Phonetic transcription in several dialectal Spanish variants and show some quantitative results of dialectal differences.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-227"
  },
  "muramatsu98_icslp": {
   "authors": [
    [
     "Mieko",
     "Muramatsu"
    ]
   ],
   "title": "Acoustic analysis of Japanese English prosody: comparison between fukushima dialect speakers and tokyo dialect speakers in declarative sentences and yes-no questions",
   "original": "i98_1090",
   "page_count": 4,
   "order": 228,
   "p1": "paper 1090",
   "pn": "",
   "abstract": [
    "L1 transfer may explain prosodic errors in an L2. For Japanese English prosody, several comparative studies have been conducted. However, only oral reading texts have been used and not much attention has been paid to the effect of differences in the L1 dialect, especially in an \"accentless\" Japanese dialect. This preliminary study describes an investigation of the differences in L1 dialect prosodic transfer to English between the speakers of the Fukushima dialect (an accentless dialect) and the Tokyo dialect (an accent dialect) in declarative sentences and yes-no questions. A two-way communicative task was selected to induce natural utterances. The fundamental frequency at each point of twenty equally-spaced points of observation of three female voices from each dialect group was measured. The major finding is there do appear to be dialectal differences in L1 transfer of prosody. However, this preliminary study is not conclusive and more comprehensive investigation will be necessary.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-228"
  },
  "noda98_icslp": {
   "authors": [
    [
     "Hideki",
     "Noda"
    ],
    [
     "Katsuya",
     "Harada"
    ],
    [
     "Eiji",
     "Kawaguchi"
    ],
    [
     "Hidefumi",
     "Sawai"
    ]
   ],
   "title": "A context-dependent approach for speaker verification using sequential decision",
   "original": "i98_0108",
   "page_count": 4,
   "order": 229,
   "p1": "paper 0108",
   "pn": "",
   "abstract": [
    "This paper is concerned about speaker verification (SV) using the sequential probability ratio test (SPRT). In the SPRT input samples are usually assumed to be i.i.d. samples from a probability density function because an on-line probability computation is required. Feature vectors used in speech processing obviously do not satisfy the assumption and therefore the correlation between successive feature vectors has not been considered in conventional SV using the SPRT. The correlation can be modeled by the hidden Markov model (HMM) but unfortunately the HMM can not be directly applied to the SPRT because of statistical dependence of input samples. This paper proposes a method of HMM probability computation using the mean field approximation to resolve this problem, where the probability of whole input samples is nominally represented as the product of probability of each sample as if input samples were independent each other.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-229"
  },
  "ortegagarcia98_icslp": {
   "authors": [
    [
     "Javier",
     "Ortega-Garc√≠a"
    ],
    [
     "Santiago",
     "Cruz-Llanas"
    ],
    [
     "Joaquin",
     "Gonzalez-Rodriguez"
    ]
   ],
   "title": "Quantitative influence of speech variability factors for automatic speaker verification in forensic tasks",
   "original": "i98_1062",
   "page_count": 4,
   "order": 230,
   "p1": "paper 1062",
   "pn": "",
   "abstract": [
    "Regarding speaker identity in forensic conditions, several factors of variability must be taken into account, as peculiar intra-speaker variability, forced intra-speaker variability or channel-dependent external influences. Using 'AHUMADA' large speech database in Spanish, containing several recording sessions and channels, and including different tasks for 100 male speakers, automatic speaker verification experiments have been accomplished. Due to the inherent non-cooperative nature of speakers in forensic applications, only text-independent recognizers are used. In this sense, a GMM-based verification system is being used in order to obtain quantitative results. Maximum likelihood estimation of the models is performed, and LPC-cepstra, delta- and delta-delta-LPCC, are used at the parameterization stage. With this baseline verification system, we intend to determine how some variability sources included in 'AHUMADA' affect speaker identification. Results including speaking rate influence, single- and multi-session training, cross-channel testing, and kind of speech (read vs. spontaneous) are presented when likelihood-domain normalization is applied.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-230"
  },
  "pfau98_icslp": {
   "authors": [
    [
     "Thilo",
     "Pfau"
    ],
    [
     "Guenther",
     "Ruske"
    ]
   ],
   "title": "Creating hidden Markov models for fast speech",
   "original": "i98_0255",
   "page_count": 4,
   "order": 231,
   "p1": "paper 0255",
   "pn": "",
   "abstract": [
    "This paper deals with the problem of building hidden Markov models (HMMs) suitable for fast speech. First an automatic procedure is presented to split speech material into different categories according to the speaking rate. Then the problem of sparse data available for the estimation of HMMs for fast speech is discussed. A comparison of different methods to overcome this problem follows. The main emphasis here is set on robust reestimation techniques like maximum aposteriori estimation (MAP) as well as on methods to reduce the variability of the speech signal and therefore to be able to reduce the number of HMM parameters. Vocaltract length normalization (VTLN) is chosen for that purpose. Finally a comparison of various combinations of the methods discussed is presented basing on word error rates for fast speech. The best method (MAPVTLN) results in a decrease of the error rate of 10% relative to the baseline system.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-231"
  },
  "pham98_icslp": {
   "authors": [
    [
     "Tuan",
     "Pham"
    ],
    [
     "Michael",
     "Wagner"
    ]
   ],
   "title": "Speaker identification using relaxation labeling",
   "original": "i98_0949",
   "page_count": 4,
   "order": 232,
   "p1": "paper 0949",
   "pn": "",
   "abstract": [
    "A nonlinear probabilistic relaxation labeling for speaker identification is presented in this paper. This relaxation scheme, which is an iterative and parallel process, offers a flexible and effective framework for dealing with uncertainty inherently existing in the labeling of the speech feature vectors. Basic concepts and formulations of the relaxation algorithms are outlined. We then discuss how to model the relaxation scheme to the labeling of the speech feature vectors for the speaker identification task. The implementation is tested on a commercial speech corpus TI46. The results using several codebook sizes obtained from the proposed approach are more favorable than those from the conventional VQ (Vector Quantization)-based method.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-232"
  },
  "rodriguezlinares98_icslp": {
   "authors": [
    [
     "Leandro",
     "Rodriguez-Linares"
    ],
    [
     "Carmen",
     "Garc√≠a-Mateo"
    ]
   ],
   "title": "A novel technique for the combination of utterance and speaker verification systems in a text-dependent speaker verification task",
   "original": "i98_1084",
   "page_count": 4,
   "order": 233,
   "p1": "paper 1084",
   "pn": "",
   "abstract": [
    "In this paper we present a novel technique for combining a Speaker Verification System with an Utterance Verification System in a Speaker Authentication system over the telephone. Speaker Verification consists in accepting or rejecting the claimed identity of a speaker by processing samples of his/her voice. Usually, these systems are based on HMM's that try to represent the characteristics of the talkers' vocal tracts. Utterance Verification systems make use of a set of speaker-independent speech models to recognize a certain utterance and decide whether a speaker has uttered it or not. If the utterances consist of passwords, this can be used for identity verification purposes. Up to now, both techniques have been used separately. This paper is focused on the problem of how to combine these two sources of information. A new architecture is presented to join an utterance verification system and a speaker verification system in order to improve the performance in a text-dependent speaker verification task.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-233"
  },
  "rose98c_icslp": {
   "authors": [
    [
     "Phil",
     "Rose"
    ]
   ],
   "title": "A forensic phonetic investigation into non-contemporaneous variation in the f-pattern of similar-sounding speakers.",
   "original": "i98_0301",
   "page_count": 4,
   "order": 234,
   "p1": "paper 0301",
   "pn": "",
   "abstract": [
    "A forensic phonetic experiment is described which investigates the nature of non-contemporaneous within-speaker variation for six similar-sounding speakers. Between 8 and 10 intonationally varying tokens of the naturally produced single word utterance hello were elicited from six similar-sounding adult Australian males in two repeats separated by a reading of the \"rainbow\" passage. Both repeats are compared with a single batch of intonationally varying hello tokens recorded at least one year earlier. Within-speaker variation is quantified by ANOVA on mean non-contemporaneous differences and Scheffe's F for centre frequencies of the first 4 formants at 7 well-defined points in the word. Values for non-contemporaneous within-speaker between-token differences are also given, and their contribution to a Bayesian Likelihood Ratio is exemplified.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-234"
  },
  "schmidtnielsen98_icslp": {
   "authors": [
    [
     "Astrid",
     "Schmidt-Nielsen"
    ],
    [
     "Thomas H.",
     "Crystal"
    ]
   ],
   "title": "Human vs. machine speaker identification with telephone speech",
   "original": "i98_0148",
   "page_count": 4,
   "order": 235,
   "p1": "paper 0148",
   "pn": "",
   "abstract": [
    "An experiment compared the speaker recognition performance of human listeners to that of computer algorithms/systems. Listening protocols were developed analogous to procedures used in the algorithm evaluation run by the U.S. National Institute of Standards and Technology (NIST), and the same telephone conversation data were used. For \"same number\" testing, with three-second samples, listener panels and the best algorithm had the same equal-error rate (EER) of 8%. Listeners were better than typical algorithms. For \"different number\" testing, EER's increased but humans had a 40% lower equal-error rate. Other observations on human listening performance and robustness to \"degradations\" were made.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-235"
  },
  "slomka98_icslp": {
   "authors": [
    [
     "Stefan",
     "Slomka"
    ],
    [
     "Sridha",
     "Sridharan"
    ],
    [
     "Vinod",
     "Chandran"
    ]
   ],
   "title": "A comparison of fusion techniques in mel-cepstral based speaker identification",
   "original": "i98_0123",
   "page_count": 4,
   "order": 236,
   "p1": "paper 0123",
   "pn": "",
   "abstract": [
    "Input level fusion and output level fusion methods are compared for fusing Mel-frequency Cepstral Coefficients with their corresponding delta coefficients. A 49 speaker subset of the King database is used under wideband and telephone conditions. The best input level fusion system is more computationally complex than the output level fusion system. Both input and output fusion systems were able to outperform the best purely MFCC based system for wideband data. For King telephone data, only the output level fusion based system was able to outperform the best purely MFCC based system. Further experiments using NIST'96 data under matched and mismatched conditions were also performed. Provided it was well tuned, we found that the output level fused system always outperformed the input level fused system under all experimental conditions.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-236"
  },
  "soltau98_icslp": {
   "authors": [
    [
     "Hagen",
     "Soltau"
    ],
    [
     "Alex",
     "Waibel"
    ]
   ],
   "title": "On the influence of hyperarticulated speech on recognition performance",
   "original": "i98_0736",
   "page_count": 4,
   "order": 237,
   "p1": "paper 0736",
   "pn": "",
   "abstract": [
    "Since we cannot exclude that speech recognizers fail sometimes, it is important to examine how users react to recognition errors. In correction situations, speaking style becomes more accentuated to disambiguate the original mistake. We examine the effect of speaking style in such situations on speech recognition performance. Our results indicate that hyperarticulated effects occur in correction situations and decrease word accuracy significantly.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-237"
  },
  "ward98_icslp": {
   "authors": [
    [
     "Nuala C.",
     "Ward"
    ],
    [
     "Dominik R.",
     "Dersch"
    ]
   ],
   "title": "Text-independent speaker identification and verification using the TIMIT database",
   "original": "i98_0291",
   "page_count": 4,
   "order": 238,
   "p1": "paper 0291",
   "pn": "",
   "abstract": [
    "This paper presents a neural network inspired approach to speaker recognition using speaker models constructed from full data sets. A similarity measure between data sets is used for text-independent speaker identification and verification. In order to reduce the computational effort in calculating the similarity measure, a fuzzy Vector Quantisation procedure is applied. This method has previously been successfully applied to a database of 108 Australian English speakers. The purpose of this paper is to apply this method to a larger benchmark database of 630 speakers (TIMIT Database). Using the full 630-speaker database, an accuracy of 98.2% (one test sentence) and 99.7% (two test sentences) was achieved for text-independent speaker identification. On a 462-speaker subset of the database a 98.5% successful acceptance and 96.9% successful rejection rate for text-independent speaker verification was achieved.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-238"
  },
  "yanguas98_icslp": {
   "authors": [
    [
     "Lisa R.",
     "Yanguas"
    ],
    [
     "Gerald C.",
     "O'Leary"
    ],
    [
     "Marc A.",
     "Zissman"
    ]
   ],
   "title": "Incorporating linguistic knowledge into automatic dialect identification of Spanish",
   "original": "i98_1136",
   "page_count": 4,
   "order": 239,
   "p1": "paper 1136",
   "pn": "",
   "abstract": [
    "In this paper we exploit linguistic knowledge to aid in automatic dialect identification in Spanish. Segments of extemporaneous Cuban and Peruvian Spanish dialect data from the Miami Corpus were analyzed and 49 linguistic features that occur with different rates in each of the two dialects identified and hand-labelled. We evaluate the expected performance of the dialect detection system based on a theoretical model and compute the systems' performance. Using a Gaussian classifier we show that a subset of the 49 originally-identified features obtains nearly perfect performance for discriminating between the two dialects. We compare these results with those from an automatic recognition system (PRLM-P). We then test this system in the limited domain of read digits from 0 through 10 using an orthographic transcription and hand-marked data for phone extraction and alignment. Initial experiments on phone-level segments show that phone duration and energy computations prove discriminatory for dialect discrimination.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-239"
  },
  "zhang98c_icslp": {
   "authors": [
    [
     "Yiying",
     "Zhang"
    ],
    [
     "Xiaoyan",
     "Zhu"
    ]
   ],
   "title": "A novel text-independent speaker verification method using the global speaker model",
   "original": "i98_1144",
   "page_count": 4,
   "order": 240,
   "p1": "paper 1144",
   "pn": "",
   "abstract": [
    "In this paper a new text-independent speaker verification method is proposed based on likelihood score normalization and the global speaker model, which is established to represent the universal features of speech and environment, and to normalize the likelihood score. As a result the equal error rates are decreased significantly, verification procedure is accelerated and system adaptability is improved. Two possible ways of establishing the global speaker model, one of which can meet the real-time requirement, are also suggested and discussed. Experiments demonstrate the effectiveness of this novel verification method and its improvement over the conventional method and other normalization methods.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-240"
  },
  "rosenberg98_icslp": {
   "authors": [
    [
     "Aaron E.",
     "Rosenberg"
    ],
    [
     "Ivan",
     "Magrin-Chagnolleau"
    ],
    [
     "S.",
     "Parthasarathy"
    ],
    [
     "Qian",
     "Huang"
    ]
   ],
   "title": "Speaker detection in broadcast speech databases",
   "original": "i98_0202",
   "page_count": 4,
   "order": 241,
   "p1": "paper 0202",
   "pn": "",
   "abstract": [
    "Experiments have been carried out to assess the feasibility of detecting target speaker segments in multi-speaker broadcast databases, The experimental database consists of NBC Nightly News broadcasts. The target speaker is the news anchor, Tom Brokaw. Gaussian mixture models are constructed from labelled training data for the target speaker as well as background models for other speakers,, commercials, and music. Four labelled 30-min. broadcasts are used ffor testing. Mel-frequency cepstral features, augmented by delta cepstral features, are calculated over 20 msec. windows shifted every 10 msecs. through a broadcast. Likelihood ratio scores are calculated for each test frame averaged over blocks of frames with a specified duration. The block scores are input to a detection routine which returns estimates of target segment boundaries. The range of best results obtained over the test broadcasts is 82% to 100% detection of target segments with segment frame accuracy ranging from 86% to 95%. 0 to 2 false alarm segments are detected over each 30 min. broadcast.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-241"
  },
  "parris98_icslp": {
   "authors": [
    [
     "Eluned S.",
     "Parris"
    ],
    [
     "Michael J.",
     "Carey"
    ]
   ],
   "title": "Multilateral techniques for speaker recognition",
   "original": "i98_0444",
   "page_count": 4,
   "order": 242,
   "p1": "paper 0444",
   "pn": "",
   "abstract": [
    "Speaker recognition is usually accomplished by building a set of models from speech of a known speaker, training data, and subsequently using a pattern matching algorithm to score the speech from an unknown speaker, test data. In this paper we discard the notion of train and test data in speaker recognition and introduce the multilateral scoring technique. This technique comprises building speaker models on material for the known speaker and matching the unknown speaker data to these models, the traditional approach to speaker recognition. The resultant scores are fused with an equivalent set of scores produced by matching the known speaker utterance to models built on the unknown speaker data. Significant improvements have been achieved using this technique on the NIST 1996, 1997 and 1998 Speaker Recognition Evaluation data. Results are presented for two speaker recognition systems, the first based on Hidden Markov models and the second based on Gaussian Mixture models.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-242"
  },
  "nishida98_icslp": {
   "authors": [
    [
     "Masafumi",
     "Nishida"
    ],
    [
     "Yasuo",
     "Ariki"
    ]
   ],
   "title": "Real time speaker indexing based on subspace method - application to TV news articles and debate",
   "original": "i98_0125",
   "page_count": 4,
   "order": 243,
   "p1": "paper 0125",
   "pn": "",
   "abstract": [
    "In this paper, we propose a method to extract and verify individual speaker utterance using a subspace method. This method can extract speech section of the same speaker by repeating speaker verification between the present speech section and the immediately previous speech section. The speaker models are automatically trained in the verification process without constructing speaker templates in advance. As a result, this speaker verification method is applied to speaker indexing. In this study, announcer utterances are automatically extracted from news speech data which includes reporter or interviewer utterances. Also extracted automatically are the utterances of each participator in debate program broadcasted on TV.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-243"
  },
  "doddington98_icslp": {
   "authors": [
    [
     "George",
     "Doddington"
    ],
    [
     "Walter",
     "Liggett"
    ],
    [
     "Alvin",
     "Martin"
    ],
    [
     "Mark",
     "Przybocki"
    ],
    [
     "Douglas A.",
     "Reynolds"
    ]
   ],
   "title": "SHEEP, GOATS, LAMBS and WOLVES: a statistical analysis of speaker performance in the NIST 1998 speaker recognition evaluation",
   "original": "i98_0608",
   "page_count": 4,
   "order": 244,
   "p1": "paper 0608",
   "pn": "",
   "abstract": [
    "Performance variability in speech and speaker recognition systems can be attributed to many factors. One major factor, which is acknowledged but seldom analyzed, is differences in the recognizability of different speakers. In speaker recognition systems such differences are characterized by the use of animal names for different types of speakers, including sheep, goats, lambs and wolves, depending on their behavior with respect to automatic recognition systems. In this paper we propose statistical tests for the existance of these animals and hunt for such animals using results from the 1998 NIST speaker recognition evaluation.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-244"
  },
  "corradaemmanuel98_icslp": {
   "authors": [
    [
     "Andres",
     "Corrada-Emmanuel"
    ],
    [
     "Michael",
     "Newman"
    ],
    [
     "Barbara",
     "Peskin"
    ],
    [
     "Lawrence",
     "Gillick"
    ],
    [
     "Robert",
     "Roth"
    ]
   ],
   "title": "Progress in speaker recognition at dragon systems",
   "original": "i98_1017",
   "page_count": 4,
   "order": 245,
   "p1": "paper 1017",
   "pn": "",
   "abstract": [
    "We present a new algorithm for speaker recognition (the Sequential Non-Parametric system, or SNP) that has the potential to overcome two limitations of the current approaches. It uses sequences of frames instead of one frame at a time; and it avoids the need to model a speaker with mixtures of Gaussians by scoring the data non-parametrically. Although at an early stage in its development, SNP's output can be interpolated with that of our GMM system to outperform state-of-the-art GMM's. Comparative results are presented for the 1998 NIST Speaker Recognition Evaluation test set.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-245"
  },
  "nordstrom98_icslp": {
   "authors": [
    [
     "Tomas",
     "Nordstr√∂m"
    ],
    [
     "Haakan",
     "Melin"
    ],
    [
     "Johan",
     "Lindberg"
    ]
   ],
   "title": "A comparative study of speaker verification systems using the polycost database",
   "original": "i98_0773",
   "page_count": 4,
   "order": 246,
   "p1": "paper 0773",
   "pn": "",
   "abstract": [
    "This paper reports on a comparative study of several automatic speaker verification systems using the Polycost database. Polycost is a multi-lingual database with non-native English and mother-tongue speech by subjects from 14 countries. We present results for the first three baseline experiments defined for the database as well as explore the multi-lingual aspects of Polycost in a number of experiments where we compare cross-language and same-language impostor attempts. Our results then lead us to suggest a revised set of baseline experiments.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-246"
  },
  "matsui98_icslp": {
   "authors": [
    [
     "Tomoko",
     "Matsui"
    ],
    [
     "Kiyoaki",
     "Aikawa"
    ]
   ],
   "title": "Robust speaker verification insensitive to session-dependent utterance variation and handset-dependent distortion",
   "original": "i98_0714",
   "page_count": 4,
   "order": 247,
   "p1": "paper 0714",
   "pn": "",
   "abstract": [
    "This paper investigates a method of creating robust speaker models that are not sensitive to session-dependent (SD) utterance-variation and handset-dependent (HD) distortion for HMM-based speaker verification systems in a real telephone network. We recently reported a method of creating session-independent (SI) speaker-HMMs that are not sensitive to SD utterance-variation. In that method, the distortion function that transforms SI speaker-HMMs to SD speaker-HMMs is introduced, and the parameters in the function and the speaker-HMM parameters are jointly estimated using a speaker adaptive training algorithm. This paper proposes a method that is less sensitive to SD utterance-variation and HD distortion than the previous method. This new idea focuses on different difficulties in estimating parameters in distortion functions for SD utterance-variation and HD distortion. In text-independent verification experiments using telephone speech data, the error reduction rate of the improved method compared with that of the conventional method of cepstral mean normalization is 24%.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-247"
  },
  "melin98_icslp": {
   "authors": [
    [
     "Haakan",
     "Melin"
    ],
    [
     "Johan W.",
     "Koolwaaij"
    ],
    [
     "Johan",
     "Lindberg"
    ],
    [
     "Fr√©d√©ric",
     "Bimbot"
    ]
   ],
   "title": "A comparative evaluation of variance flooring techniques in HMM-based speaker verification",
   "original": "i98_0467",
   "page_count": 4,
   "order": 248,
   "p1": "paper 0467",
   "pn": "",
   "abstract": [
    "The problem of how to train variance parameters on scarce data is addressed in the context of text-dependent, HMM-based, automatic speaker verification. Three variations of variance flooring is explored as a means to prevent over-fitting. With the best performing one, the floor to a variance vector of a client model is proportional to the corresponding variance vector in a non-client multi-speaker model. It is also found that creating a client-model by adapting the means and mixture weights from the non-client model while keeping variances constant works comparably to variance flooring and is much simpler. Comparisons are made on three large telephone quality corpora: Gandalf, SESP and Polycost.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-248"
  },
  "petrovskadelacretaz98_icslp": {
   "authors": [
    [
     "Dijana",
     "Petrovska-Delacretaz"
    ],
    [
     "Jan",
     "Cernocky"
    ],
    [
     "Jean",
     "Hennebert"
    ],
    [
     "G√©rard",
     "Chollet"
    ]
   ],
   "title": "Text-independent speaker verification using automatically labelled acoustic segments",
   "original": "i98_0536",
   "page_count": 4,
   "order": 249,
   "p1": "paper 0536",
   "pn": "",
   "abstract": [
    "Most of the current text-independent speaker veri,cation techniques are based on modelling the global probability distribution function of speakers in the acoustic vector space. We present an alternative approach based on class-dependent veri,cation systems using automatically determined segmental units, obtained with temporal decomposition and labelled through unsupervised clustering. The core of the system is a set of multi-layer perceptrons (MLP) trained to discriminate between client and an independent set of world speakers. Each MLP is dedicated to work with data segments that are previously selected as belonging to a particular class. Issues and potential advantages of the segmental approach are presented. Performances of global and segmental approaches are tested on the NIST'98 database (250 female and 250 male speakers), showing promising results for the proposed new segmental approach. Comparison with a state of the art system, based on Gaussian Mixture Modelling is also included.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-249"
  },
  "li98_icslp": {
   "authors": [
    [
     "Qi",
     "Li"
    ]
   ],
   "title": "A fast decoding algorithm based on sequential detection of the changes in distribution",
   "original": "i98_0815",
   "page_count": 4,
   "order": 250,
   "p1": "paper 0815",
   "pn": "",
   "abstract": [
    "A fast algorithm for left-to-right HMM decoding is proposed in this paper. The algorithm is developed based on a sequential detection scheme which is asymptotically optimal in the sense of detecting a possible change in distribution as reliably and quickly as possible. The scheme is extended to HMM decoding in determining the state segmentations for likelihood or other score computations. As a sequential scheme, it can determine a state boundary in a few time steps after it occurs. The examples in this paper show that the proposed algorithm is 5 to 9 times faster than the Viterbi algorithm while it still can provide the same or similar decoding results. The proposed algorithm can be applied to speaker recognition, audio segmentation, voice/silence detection, and many other applications, where an assumption of the algorithm is usually satisfied.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-250"
  },
  "olsen98_icslp": {
   "authors": [
    [
     "Jesper √òstergaard",
     "Olsen"
    ]
   ],
   "title": "Speaker verification with ensemble classifiers based on linear speech transforms",
   "original": "i98_0334",
   "page_count": 4,
   "order": 251,
   "p1": "paper 0334",
   "pn": "",
   "abstract": [
    "For most classifier architectures realistic training schemes only allow classifiers corresponding to local optima of the training criteria to be constructed. One way of dealing with this problem is to work with classifier ensembles: multiple classifiers are trained for the same classification problem and combined into one ``super'' classifier. The problem addressed in this paper is text prompted speaker verification by means of phoneme dependent Radial Basis Function networks trained by gradient descent error minimisation. In this context ensemble techniques are introduced by combining different classifiers that classify feature vectors, which have been pre-processed using different linear transforms. Four different types of linear transforms are studied: the Fisher transform, the LDA transform, the PCA transform and the cosine transform. The verification system is evaluated on the Gandalf database, where the equal error rate is reduced from 3.6% to 3.2% when ensemble techniques are introduced.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-251"
  },
  "olsen98b_icslp": {
   "authors": [
    [
     "Jesper √òstergaard",
     "Olsen"
    ]
   ],
   "title": "Speaker recognition based on discriminative projection models",
   "original": "i98_0335",
   "page_count": 4,
   "order": 252,
   "p1": "paper 0335",
   "pn": "",
   "abstract": [
    "A new discriminant speaker model is introduced in this paper. The model is text dependent and relies on characterising speakers in terms of the angular distance between ``projection vectors'', which allow good discrimination between individual speakers. The projection models require only little enrollment data to be available per target speaker, but at the same time require a set of ``cohort speakers'' to be available for which a relatively large amount of training speech is available per cohort speaker. The projection model technique is evaluated on the Gandalf database and compared to conventional Gaussian Mixture Models (GMMs). It is found that the projection models require less storage per target speaker, while at the same time achieving lower error rates, particularly when applied for speaker identification and recognition under mismatched conditions.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-252"
  },
  "moody98_icslp": {
   "authors": [
    [
     "James",
     "Moody"
    ],
    [
     "Stefan",
     "Slomka"
    ],
    [
     "Jason",
     "Pelecanos"
    ],
    [
     "Sridha",
     "Sridharan"
    ]
   ],
   "title": "On the convergence of Gaussian mixture models: improvements through vector quantization",
   "original": "i98_0667",
   "page_count": 4,
   "order": 253,
   "p1": "paper 0667",
   "pn": "",
   "abstract": [
    "This paper studies the reliance of a Gaussian Mixture Model (GMM) based closed-set Speaker Identification system on model convergence and describes methods to improve this convergence. It shows that the reason why the Vector Quantisation GMMs (VQGMMs) outperform a simple GMM is mainly due to decreasing the complexity of the data during training. In addition, it is shown that the VQGMM system is less computationally complex than the traditional GMM, yielding a system which is quicker to train and which gives higher performance. We also investigate four different VQ distance measures which can be used in the training of a VQGMM and compare their respective performances. It is found that the improvements gained by the VQGMM is only marginally dependant on the distance measure.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-253"
  },
  "sonmez98_icslp": {
   "authors": [
    [
     "Kemal",
     "S√∂nmez"
    ],
    [
     "Elizabeth",
     "Shriberg"
    ],
    [
     "Larry",
     "Heck"
    ],
    [
     "Mitchel",
     "Weintraub"
    ]
   ],
   "title": "Modeling dynamic prosodic variation for speaker verification",
   "original": "i98_0920",
   "page_count": 4,
   "order": 254,
   "p1": "paper 0920",
   "pn": "",
   "abstract": [
    "Statistics of frame-level pitch have recently been used in speaker recognition systems with good results. Although they convey useful long-term information about a speaker's distribution of f0 values, such statistics fail to capture information about local dynamics in intonation that characterize an individual's speaking style. In this work, we take a first step toward capturing such suprasegmental patterns for automatic speaker verification. Specifically, we model the speaker's f0 movements by fitting a piecewise linear model to the f0 track to obtain a stylized f0 contour. Parameters of the model are then used as statistical features for speaker verification. We report results on 1998 NIST speaker verification evaluation. Prosody modeling improves the verification performance of a cepstrum-based Gaussian mixture model system (as measured by a task-specific Bayes risk) by 10%.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-254"
  },
  "reynolds98_icslp": {
   "authors": [
    [
     "Douglas A.",
     "Reynolds"
    ],
    [
     "Elliot",
     "Singer"
    ],
    [
     "Beth A.",
     "Carlson"
    ],
    [
     "Gerald C.",
     "O'Leary"
    ],
    [
     "Jack J.",
     "McLaughlin"
    ],
    [
     "Marc A.",
     "Zissman"
    ]
   ],
   "title": "Blind clustering of speech utterances based on speaker and language characteristics",
   "original": "i98_0610",
   "page_count": 4,
   "order": 255,
   "p1": "paper 0610",
   "pn": "",
   "abstract": [
    "Classical speaker and language recognition techniques can be applied to the classification of unknown utterances by computing the likelihoods of the utterances given a set of well trained target models. This paper addresses the problem of grouping unknown utterances when no information is available regarding the speaker or language classes or even the total number of classes. Approaches to blind message clustering are presented based on conventional hierarchical clustering techniques and an integrated cluster generation and selection method called the d* algorithm. Results are presented using message sets derived from the Switchboard and Callfriend corpora. Potential applications include automatic indexing of recorded speech corpora by speaker/language tags and automatic or semiautomatic selection of speaker specific speech utterances for speaker recognition adaptation.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-255"
  },
  "caseiro98_icslp": {
   "authors": [
    [
     "Diamantino",
     "Caseiro"
    ],
    [
     "Isabel M.",
     "Trancoso"
    ]
   ],
   "title": "Spoken language identification using the speechdat corpus",
   "original": "i98_1093",
   "page_count": 4,
   "order": 256,
   "p1": "paper 1093",
   "pn": "",
   "abstract": [
    "Current language identification systems vary significantly in their complexity. The systems that use higher level linguistic information have the best performance. Nevertheless, that information is hard to collect for each new language. The system presented in this paper is easily extendable to new languages because it uses very little linguistic information. In fact, the presented system needs only one language specific phone recogniser (in our case the Portuguese one), and is trained with speech from each of the other languages. With the SpeechDat-M corpus, with 6 European languages (English, French, German, Italian, Portuguese and Spanish) our system achieved an identification rate of 83.4% on 5-second utterances, this result shows an improvement of 5% over our previous version, mainly through the use of a neural network classifier. Both the baseline and the full system were implemented in realtime.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-256"
  },
  "braun98_icslp": {
   "authors": [
    [
     "Jerome",
     "Braun"
    ],
    [
     "Haim",
     "Levkowitz"
    ]
   ],
   "title": "Automatic language identification with perceptually guided training and recurrent neural networks",
   "original": "i98_0405",
   "page_count": 4,
   "order": 257,
   "p1": "paper 0405",
   "pn": "",
   "abstract": [
    "We present a novel approach to Automatic Language Identification (LID). We propose Perceptually Guided Training (PGT), a novel LID training method, involving identification of utterance parts which are particularly significant perceptually for the language identification process, and exploitation of these Perceptually Significant Regions (PSRs) to guide the LID training process. Our approach involves a Recurrent Neural Network (RNN) as the main mechanism. We propose that, because of the long-range intra-utterance acoustical context significance in LID, RNNs are particularly suitable for the LID task. Our approach does not require phonetic labeling or transcription of the training corpus. LIREN/PGT, the LID system we developed, incorporates our approach. Our LID experiments were on English, German, and Mandarin Chinese, using the OGI-TS corpus.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-257"
  },
  "vuuren98_icslp": {
   "authors": [
    [
     "Sarel van",
     "Vuuren"
    ],
    [
     "Hynek",
     "Hermansky"
    ]
   ],
   "title": "On the importance of components of the modulation spectrum for speaker verification",
   "original": "i98_0631",
   "page_count": 4,
   "order": 258,
   "p1": "paper 0631",
   "pn": "",
   "abstract": [
    "We provide an analysis of the relative importance of components of the modulation spectrum for speaker verification. The aim is to remove less relevant components and reduce system sensitivity to acoustic disturbances while improving verification accuracy. Spectral components between about 0.1Hz and 10Hz are found to contain the most useful speaker information. We discuss this result in the context of RASTA processing and cepstral mean subtraction. When compared to cepstral mean subtraction that retains components up to 50Hz, lowpass filtering to 10Hz with downsampling by 75 percent is found to significantly improve robustness in mismatched conditions. The downsampling results in a large computational savings.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-258"
  },
  "breen98b_icslp": {
   "authors": [
    [
     "Andrew P.",
     "Breen"
    ],
    [
     "O.",
     "Gloaguen"
    ],
    [
     "P.",
     "Stern"
    ]
   ],
   "title": "A fast method of producing talking head mouth shapes from real speech",
   "original": "i98_0390",
   "page_count": 4,
   "order": 259,
   "p1": "paper 0390",
   "pn": "",
   "abstract": [
    "The subject of computer generated virtual characters is a diverse and rapidly developing field, with a wide variety of applications in industries as varied as entertainment, education and advertising. Many of these applications require or would be greatly enhanced by having the virtual characters speak with the recorded voice of a real person. Such an ability is particularly useful in applications where users are interacting via avatars in real time in a virtual world. There are three basic problems which need to be addressed when developing an interface which has this functionality. *) The process must be capable of animating mouth shapes in real time. *) The process should not mouth extraneous sounds such as music, doors slamming etc. To do so would diminish the effectiveness of the illusion. *) The mouth shapes produced by the avatar should approximate that of the speaker. This paper describes a series of experiments which attempt to address each of the points outlined above. The experimental procedures are based around a real time low computation approach which relies on a particular variety of neural network known as the Single Layer Look Up Perceptron (SLLUP).\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-259"
  },
  "cohen98_icslp": {
   "authors": [
    [
     "Phil R.",
     "Cohen"
    ],
    [
     "Michael",
     "Johnston"
    ],
    [
     "David",
     "McGee"
    ],
    [
     "Sharon L.",
     "Oviatt"
    ],
    [
     "Joshua",
     "Clow"
    ],
    [
     "Ira",
     "Smith"
    ]
   ],
   "title": "The efficiency of multimodal interaction: a case study",
   "original": "i98_0571",
   "page_count": 4,
   "order": 260,
   "p1": "paper 0571",
   "pn": "",
   "abstract": [
    "This paper reports on a case study comparison of a direct-manipulation-based graphical user interface (GUI) with the QuickSet pen/voice multimodal interface for supporting the task of military force \"laydown.\" In this task, a user places military units and \"control measures,\" such as various types of lines, obstacles, objectives, etc., on a map. A military expert designed his own scenario and entered it via both interfaces. Usage of QuickSet led to a speed improvement of 3.2 to 8.7-fold, depending on the kind of object being created. These results suggest that there may be substantial efficiency advantages to using multimodal interaction over GUIs for map-based tasks.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-260"
  },
  "czap98_icslp": {
   "authors": [
    [
     "Laszlo",
     "Czap"
    ]
   ],
   "title": "Audio and audio-visual perception of consonants disturbed by white noise and 'cocktail party'",
   "original": "i98_0445",
   "page_count": 4,
   "order": 261,
   "p1": "paper 0445",
   "pn": "",
   "abstract": [
    "Some research questions regarding the speech perception can only be answered with natural speech stimuli especially in noisy environment. In this paper we are going to answer a couple of questions on visual support of audio signal at speech recognition. How much support can give the video signal for the audio one? The impact of nature of the noise. How can help the visual information to identify the place of articulation? Does the voices of different class of excitation get the same visual support? In order to answer these questions we have performed intelligibility study on consonants between the same vowel supported or not by the speaker's image with different signal to noise ratio. The noise is either white noise or a mix of other speakers' voice.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-261"
  },
  "downey98_icslp": {
   "authors": [
    [
     "Simon",
     "Downey"
    ],
    [
     "Andrew P.",
     "Breen"
    ],
    [
     "Maria",
     "Fern√°ndez"
    ],
    [
     "Edward",
     "Kaneen"
    ]
   ],
   "title": "Overview of the maya spoken language system",
   "original": "i98_0391",
   "page_count": 4,
   "order": 262,
   "p1": "paper 0391",
   "pn": "",
   "abstract": [
    "Recent developments in distributed system processing have opened the doors to the running of highly complex systems over a number of networked computers. This enables the complexity of a system to be hidden behind a small, lightweight user interface - for example a downloadable web page. The Maya system makes use of such interfaces to combine the functionality of speech recognition, synthesis, robust parsing, text generation and dialogue management into a highly flexible multimodal architecture, working in real time. This paper describes the development of the architecture and interfaces to each system component. The configuration of the system to particular tasks is discussed, making use of an email secretary task as an example. Once configured, the system is able to adopt all the functionality of a conventional email system and extend these capabilities by allowing complex queries to be made about mail messages.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-262"
  },
  "cettolo98_icslp": {
   "authors": [
    [
     "Mauro",
     "Cettolo"
    ],
    [
     "Daniele",
     "Falavigna"
    ]
   ],
   "title": "Automatic recognition of spontaneous speech dialogues",
   "original": "i98_0332",
   "page_count": 4,
   "order": 263,
   "p1": "paper 0332",
   "pn": "",
   "abstract": [
    "The work reported in the paper will concern the assessment of a set of modifications applied to the continuous speech recognizer developed at IRST for dictation tasks. The objective of the proposed modifications is to improve the recognizer performance on a corpus of human-human dialogues, spontaneously uttered. Some solutions will be given to increase Automatic Speech Recognition (ASR) robustness with respect to typical spontaneous speech phenomena such as: breaths, coughs, filled and silent pauses and speaking rate variations. Both gender independent and dependent models are used. Specific models of extra-linguistic phenomena are trained and a method for coping with speaking rate variations will be proposed. Different recognizers, corresponding to males, females and to various speaking rate factors, are combined together so as a unique search space is defined. Best performance, obtained on a corpus of hundreds of person-to-person spontaneously uttered dialogues, gives 26.1% Word Error Rate (WER).\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-263"
  },
  "fries98_icslp": {
   "authors": [
    [
     "Georg",
     "Fries"
    ],
    [
     "Stefan",
     "Feldes"
    ],
    [
     "Alfred",
     "Corbet"
    ]
   ],
   "title": "Using an animated talking character in a web-based city guide demonstrator",
   "original": "i98_1024",
   "page_count": 4,
   "order": 264,
   "p1": "paper 1024",
   "pn": "",
   "abstract": [
    "Taking into account that the user acceptance of an animated agent is influenced by different criteria, including appropriateness of the application domain, quality of application design, and quality of character design, we have developed a demo application in a domain where we found the agent substantially helpful - a web-based city guide. The animated character interactively guides the user through some sights of the city of Darmstadt. It can display and explain how to get to different places of interest by moving around and pointing at locations on a map. The system allows input via mouse clicks, speech and typed text. Output modalities of the agent are speech, gesture, text (cartoon word balloons) and some facial expression. Further we have designed two new 3D characters. We describe experiences gained during system development and discuss design aspects concerning application as well as character animation.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-264"
  },
  "kanzaki98_icslp": {
   "authors": [
    [
     "Rika",
     "Kanzaki"
    ],
    [
     "Takashi",
     "Kato"
    ]
   ],
   "title": "Influence of facial views on the mcgurk effect in auditory noise",
   "original": "i98_0243",
   "page_count": 4,
   "order": 265,
   "p1": "paper 0243",
   "pn": "",
   "abstract": [
    "To investigate the nature of facial information involved in the integration of audiovisual speech perception, we examined the influence of facial views on the McGurk effect under two auditory conditions. While the speech perception of most of the audiovisual syllables used was little affected by the facial views and the auditory noise, a stronger McGurk effect was obtained for the 3/4-view image uttering labial sounds presented with auditory syllables of alveolars under the auditory noise. However, the facial view did not affect the visual identification of labials in the same way. These results suggest that the information about labial or nonlabial is not the only facial information involved in the McGurk effect. It appears some other information available only in the 3/4-view image might also be involved in the McGurk effect. Implications for the processing of visual, auditory and audiovisual speech are discussed.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-265"
  },
  "brndsted98b_icslp": {
   "authors": [
    [
     "Tom",
     "Br√∏ndsted"
    ],
    [
     "Lars Bo",
     "Larsen"
    ],
    [
     "Michael",
     "Manthey"
    ],
    [
     "Paul",
     "McKevitt"
    ],
    [
     "Thomas B.",
     "Moeslund"
    ],
    [
     "Kristian G.",
     "Olesen"
    ]
   ],
   "title": "The intellimedia workbench - a generic environment for multimodal systems",
   "original": "i98_0767",
   "page_count": 4,
   "order": 266,
   "p1": "paper 0767",
   "pn": "",
   "abstract": [
    "The present paper presents a generic environment for intelligent multi media applications, denoted \"The Intellimedia Work-Bench\". The aim of the workbench is to facilitate development and research within the field of multi modal user interaction. Physically it is a table with various devices mounted above and around. These include: A camera and a laser projector mounted above the workbench, a microphone array mounted on the walls of the room, a speech recogniser and a speech synthesiser. The camera is attached to a vision system capable of locating various objects placed on the workbench. The paper presents two applications utilising the workbench. One is a campus information system, allowing the user to ask for directions within a part of the university campus. The second application is a pool trainer, intended to provide guidance to novice players.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-266"
  },
  "clow98_icslp": {
   "authors": [
    [
     "Joshua",
     "Clow"
    ],
    [
     "Sharon L.",
     "Oviatt"
    ]
   ],
   "title": "STAMP: a suite of tools for analyzing multimodal system processing",
   "original": "i98_0050",
   "page_count": 4,
   "order": 267,
   "p1": "paper 0050",
   "pn": "",
   "abstract": [
    "In this paper we describe a new automated suite of tools for capturing and analyzing data on multimodal systems called STAMP.1 STAMP is designed to support research and development efforts for advancing next-generation multimodal systems. STAMP permits researchers to analyze multimodal system performance by: (1) recording data on users' multimodal input and the system's responding, (2) supporting flexible replay of these multimodal commands, along with n-best recognition lists for the individual modalities and their combined multimodal interpretation, and (3) supporting automated analysis using different metrics of multimodal system performance. This collection of tools currently is being used to conduct basic research on the characteristics of multimodal systems, and also to iterate different aspects of the Quickset multimodal architecture.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-267"
  },
  "shigeno98_icslp": {
   "authors": [
    [
     "Sumi",
     "Shigeno"
    ]
   ],
   "title": "Cultural similarities and differences in the recognition of audio-visual speech stimuli",
   "original": "i98_1057",
   "page_count": 4,
   "order": 268,
   "p1": "paper 1057",
   "pn": "",
   "abstract": [
    "Cultural similarities and differences were compared between Japanese and North American subjects in the recognition of emotion. Seven native Japanese and five native North Americans (four Americans and one Canadian) subjects participated in the experiments. The materials were five meaningful words or short-sentences in Japanese and American English. Japanese and American actors made vocal and facial expression in order to transmit six basic emotions- happiness, surprise, anger, disgust, fear, and sadness. Three presentation conditions were used-auditory, visual, and audio-visual. The audio-visual stimuli were made by dubbing the auditory stimuli on to the visual stimuli. The results show: (1) subjects can more easily recognize the vocal expression of a speaker who belongs to their own culture, (2) Japanese subjects are not good at recognizing 'fear' in both the auditory-alone and visual-alone conditions, (3) and both Japanese and American subjects identify the audio-visually incongruent stimuli more often as a visual label rather than as an auditory label. These results suggest that it is difficult to identify the emotion of a speaker from a different culture and that people will predominantly use visual information to identify emotion.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-268"
  },
  "takezawa98_icslp": {
   "authors": [
    [
     "Toshiyuki",
     "Takezawa"
    ],
    [
     "Tsuyoshi",
     "Morimoto"
    ]
   ],
   "title": "A multimodal-input multimedia-output guidance system: MMGS",
   "original": "i98_0958",
   "page_count": 4,
   "order": 269,
   "p1": "paper 0958",
   "pn": "",
   "abstract": [
    "We have built a multimodal-input multimedia-output guidance system called MMGS. The input of a user can be a combination of speech and hand-written gestures. The system, on the other hand, outputs a response that combines speech, three-dimensional graphics, and/or other information. This system can interact cooperatively with the user by resolving ellipses/anaphora and various ambiguities such as those caused by speech recognition errors. It is currently implemented on a SGI workstation and achieves nearly real-time processing.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-269"
  },
  "vanegas98_icslp": {
   "authors": [
    [
     "Oscar",
     "Vanegas"
    ],
    [
     "Akiji",
     "Tanaka"
    ],
    [
     "Keiichi",
     "Tokuda"
    ],
    [
     "Tadashi",
     "Kitamura"
    ]
   ],
   "title": "HMM-based visual speech recognition using intensity and location normalization",
   "original": "i98_0789",
   "page_count": 4,
   "order": 270,
   "p1": "paper 0789",
   "pn": "",
   "abstract": [
    "This paper describes intensity and location normalization techniques for improving the performance of visual speech recognizers used in audio-visual speech recognition. For auditory speech recognition, there exist many methods for dealing with channel characteristics and speaker individualities, e.g., CMN (cepstral mean normalization), SAT (speaker adaptive training). We present two techniques similar to CMN and SAT, respectively, for intensity and location normalization in visual speech recognition. For the intensity normalization, the mean value over the image sequence is subtracted from each pixel of the image secuence. For the location normalization, the training and the testing processes are carried out by finding the lip position with the highest likelihood of each utterance for HMMs. Word recognition experiments based on HMM show that a significant improvement in recognition performance is achieved by combining the two techniques.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-270"
  },
  "xu98_icslp": {
   "authors": [
    [
     "Yanjun",
     "Xu"
    ],
    [
     "Limin",
     "Du"
    ],
    [
     "Guoqiang",
     "Li"
    ],
    [
     "Ziqiang",
     "Hou"
    ]
   ],
   "title": "A hierarchy probability-based visual features extraction method for speechreading",
   "original": "i98_0187",
   "page_count": 4,
   "order": 271,
   "p1": "paper 0187",
   "pn": "",
   "abstract": [
    "Visual feature extraction method now becomes the key technique in automatic speechreading systems. However it still remains a difficult problem due to large inter-person and intra-person appearance variabilities. In this paper, we extend the normal active shape model to a hierarchy probability-based framework, which can model a complex shape, such as human face. It decomposes the complex shape into two layers: the global shape including the position, scale and rotation of local shapes (such as eyes, nose, mouth and chin); the local simple shape in normal form. The two layers describe the global variation and local variation respectively, and are combined into a probability framework. It can perform fully automatic facial features locating in speechreading, or face recognition.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-271"
  },
  "ostermann98_icslp": {
   "authors": [
    [
     "J√∂rn",
     "Ostermann"
    ],
    [
     "Mark C.",
     "Beutnagel"
    ],
    [
     "Ariel",
     "Fischer"
    ],
    [
     "Yao",
     "Wang"
    ]
   ],
   "title": "Integration of talking heads and text-to-speech synthesizers for visual TTS",
   "original": "i98_0931",
   "page_count": 4,
   "order": 272,
   "p1": "paper 0931",
   "pn": "",
   "abstract": [
    "The integration of text-to-speech (TTS) synthesis and animation of synthetic faces allows new applications like visual human computer interfaces using agents or avatars. The TTS informs the talking head when phonemes are spoken. The appropriate mouth shapes are animated and rendered while the TTS produces the sound. We call this integrated system of TTS and animation a Visual TTS (VTTS). This paper describes the architecture on an integrated VTTS synthesizer that allows defining facial expressions as bookmarks in the text that will be animated while the model is talking. The position of a bookmark in the text defines the start time for the facial expression. The bookmark itself names the expression, its amplitude and the duration during which the amplitude has to be reached by the face. A bookmark to face animation parameter (FAP) converter creates a curve defining the amplitude for the given FAP over time using Hermite functions of 3rd order [http://www.research.att.com/info/osterman].\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-272"
  },
  "arslan98_icslp": {
   "authors": [
    [
     "Levent M.",
     "Arslan"
    ],
    [
     "David",
     "Talkin"
    ]
   ],
   "title": "Speech driven 3-d face point trajectory synthesis algorithm",
   "original": "i98_0110",
   "page_count": 4,
   "order": 273,
   "p1": "paper 0110",
   "pn": "",
   "abstract": [
    "A novel algorithm is proposed which generates three-dimensional face point trajectories for a given speech file. The proposed algorithm first employs an off-line training phase. In this phase, recorded face point trajectories along with their speech data and phonetic labels are used to generate phonetic codebooks. These codebooks consist of both acoustic and visual features. During the synthesis stage, speech input is rated in terms of its similarity to the codebook entries, and a weight is assigned to each codebook entry. If the phonetic information about the test speech is available, this is utilized in restricting the codebook search to only several codebook entries which are visually closest to the current phoneme. These weights are then used to synthesize the principal components of the face point trajectory. The performance of the algorithm is tested on held-out data, and the synthesized face point trajectories showed a correlation of 0.73 with true face point trajectories.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-273"
  },
  "yamamoto98_icslp": {
   "authors": [
    [
     "Eli",
     "Yamamoto"
    ],
    [
     "Satoshi",
     "Nakamura"
    ],
    [
     "Kiyohiro",
     "Shikano"
    ]
   ],
   "title": "Speech-to-lip movement synthesis based on the EM algorithm using audio-visual HMMs",
   "original": "i98_0756",
   "page_count": 4,
   "order": 274,
   "p1": "paper 0756",
   "pn": "",
   "abstract": [
    "This paper proposes a method to re-estimate output visual parameters for speech-to-lip movement synthesis using audio-visual Hidden Markov Models (HMMs) under the Expectation-Maximization(EM) algorithm. In the conventional methods for speech-to-lip movement synthesis, there is a synthesis method estimating a visual parameter sequence through the Viterbi alignment of an input acoustic speech signal using audio HMMs. However, the HMM-Viterbi method involves a substantial problem that incorrect HMM state alignment may output incorrect visual parameters. The problem in the HMM-Viterbi method is caused by the deterministic synthesis process to assign a single HMM state for an input audio frame. The proposed method avoids the deterministic process by re-estimating non-deterministic visual parameters while maximizing the likelihood of the audio-visual observation sequence under the EM algorithm.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-274"
  },
  "roy98_icslp": {
   "authors": [
    [
     "Deb",
     "Roy"
    ],
    [
     "Alex",
     "Pentland"
    ]
   ],
   "title": "Learning words from natural audio-visual input",
   "original": "i98_0551",
   "page_count": 4,
   "order": 275,
   "p1": "paper 0551",
   "pn": "",
   "abstract": [
    "We present a computational model of sensory-grounded language acquisition. Words are learned from naturally spoken multiword utterances paired with color images of objects. Speech recognition and computer vision algorithms are used to build representations of the input speech and images. Words are learned by first clustering images along shape and color dimensions. A search algorithm then finds speech segments within the continuous multiword input speech which co-occur with each visual cluster. The learned words can be used in a speech understanding task to request images based on spoken descriptions and in a speech generation task to automatically generate spoken descriptions of images. Although simple in its current form, this model is a first step towards a more complete, fully-grounded model of language acquisition. Practical applications include adaptive human-machine interfaces based on spoken language for information browsing, assistive technologies, education, and entertainment.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-275"
  },
  "dupont98_icslp": {
   "authors": [
    [
     "St√©phane",
     "Dupont"
    ],
    [
     "Juergen",
     "Luettin"
    ]
   ],
   "title": "Using the multi-stream approach for continuous audio-visual speech recognition: experiments on the M2VTS database",
   "original": "i98_0582",
   "page_count": 4,
   "order": 276,
   "p1": "paper 0582",
   "pn": "",
   "abstract": [
    "The Multi-Stream automatic speech recognition approach was investigated in this work as a framework for Audio-Visual data fusion and speech recognition. This method presents many potential advantages for such a task. It particularly allows for synchronous decoding of continuous speech while still allowing for some asynchrony of the visual and acoustic information streams. First, the Multi-Stream formalism is briefly recalled. Then, on top of the Multi-Stream motivations, experiments on the M2VTS multimodal database are presented and discussed. To our knowledge, these are the first experiments addressing multi-speaker continuous Audio-Visual Speech Recognition (AVSR). It is shown that the Multi-Stream approach can yield improved Audio-Visual speech recognition performance when the acoustic signal is corrupted by noise as well as for clean speech.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-276"
  },
  "oviatt98_icslp": {
   "authors": [
    [
     "Sharon L.",
     "Oviatt"
    ],
    [
     "Karen",
     "Kuhn"
    ]
   ],
   "title": "Referential features and linguistic indirection in multimodal language",
   "original": "i98_0048",
   "page_count": 4,
   "order": 277,
   "p1": "paper 0048",
   "pn": "",
   "abstract": [
    "The present report outlines differences between multimodal and unimodal communication patterns in linguistic features associated with ease of dialogue tracking and ambiguity resolution. A simulation method was used to collect data while participants used spoken, pen-based, or multimodal input during spatial tasks with a dynamic system. Users' linguistic constructions were analyzed for differences in the rates of reference, co-reference, definite and indefinite referring expressions, and deictic terms. Differences also were summarized in the prevalence of linguistic indirection. Results indicate that spoken language contains substantially higher levels of referring and co-referring expressions and also linguistic indirection, compared with multimodal language communicated by the same users completing the same task. In contrast, multimodal language not only has fewer referential expressions and relatively little anaphora, it also specifically lacks the regular use of determiners observed in spoken definite and indefinite noun phrases. In addition, multimodal language is distinct in its high levels of deictic reference. Implications of these findings are discussed for the relative ease of natural language processing for speech-only versus multimodal systems.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-277"
  },
  "johnston98_icslp": {
   "authors": [
    [
     "Michael",
     "Johnston"
    ]
   ],
   "title": "Multimodal language processing",
   "original": "i98_0893",
   "page_count": 4,
   "order": 278,
   "p1": "paper 0893",
   "pn": "",
   "abstract": [
    "Multimodal interfaces enable more natural and effective human-computer interaction by providing multiple channels through which input or output may pass. In order to realize their full potential, they need to support not just input from multiple modes, but synchronized integration of modes. This paper describes a multimodal language processing architecture which allows for declarative statement of multimodal integration strategies in a unification-based grammar formalism. The architecture is currently deployed in a working system supporting interaction with dynamic maps using speech and pen, but the approach is more general and extends to a wide variety of other potential multimodal interfaces.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-278"
  },
  "hirasawa98_icslp": {
   "authors": [
    [
     "Jun-ichi",
     "Hirasawa"
    ],
    [
     "Noboru",
     "Miyazaki"
    ],
    [
     "Mikio",
     "Nakano"
    ],
    [
     "Takeshi",
     "Kawabata"
    ]
   ],
   "title": "Implementation of coordinative nodding behavior on spoken dialogue systems",
   "original": "i98_0158",
   "page_count": 4,
   "order": 279,
   "p1": "paper 0158",
   "pn": "",
   "abstract": [
    "This paper proposes a mechanism that contributes to the implementation of a spoken dialogue system with which a user can communicate effortlessly. In a dialogue, exchanges between participants promote the establishment of shared information and this leads to effortless communication. This is called \"dialogue coordination\". In particular, revealing the respondent's internal state, such as through nodding and back-channel feedback, promotes the establishment of shared information. This is called \"manifestation\", which is one aspect of coordinative behavior, and a mechanism for handling manifestation is introduced. In a human-human dialogue, the listener's manifestative behavior often occurs during a speaker's utterance. However, systems using conventional speech recognition technologies cannot respond during the speaker's utterance. In order to solve this problem, the proposed mechanism, ISTAR protocol transmission, utilizes the intermediate speech recognition results without waiting for the end of the speaker's utterance. This realizes a system with flexible manifestative behavior.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-279"
  },
  "yokoyama98_icslp": {
   "authors": [
    [
     "Masao",
     "Yokoyama"
    ],
    [
     "Kazumi",
     "Aoyama"
    ],
    [
     "Hideaki",
     "Kikuchi"
    ],
    [
     "Katsuhiko",
     "Shirai"
    ]
   ],
   "title": "Use of non-verbal information in communication between human and robot",
   "original": "i98_0491",
   "page_count": 4,
   "order": 280,
   "p1": "paper 0491",
   "pn": "",
   "abstract": [
    "In this research, we consider the use of non-verbal information in human-robot dialogue to draw the communication ability of robots close to that of human beings. This paper describes analysis of output timing of non-verbal information for the interactive dialogue between human beings. Moreover, we analyzed influences of output timing by controlling it in dialogue with a CG robot. As a result, we clarify the strength of constraint and naturalness of various types of non-verbal information. We also confirm that appropriate output timing of non-verbal information is the start of utterances. This is the same as in human-human dialogue. As a result, non-verbal information made speaker-change smooth for the CG robot.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-280"
  },
  "whittaker98_icslp": {
   "authors": [
    [
     "Steve",
     "Whittaker"
    ],
    [
     "John",
     "Choi"
    ],
    [
     "Julia",
     "Hirschberg"
    ],
    [
     "Christine H.",
     "Nakatani"
    ]
   ],
   "title": "What you see is (almost) what you hear: design principles for user interfaces for accessing speech archives",
   "original": "i98_1002",
   "page_count": 5,
   "order": 281,
   "p1": "paper 1002",
   "pn": "",
   "abstract": [
    "Despite the recent growth and potential utility of speech archives, we currently lack tools for effective archival access. Previous research on search of textual archives has assumed that the system goal should be to retrieve sets of relevant documents, leaving users to visually scan through those documents to identify relevant information. However, in previous work we show that in accessing real speech archives, it is insufficient to only retrieve \"document\" sets [9,10]. Users experience huge problems of local navigation in attempting to extract relevant information from within speech \"documents\". These studies also show that users address these problems by taking handwritten notes. These notes detail both the content of the speech and serve as indices to help access relevant regions of the archive. From these studies we derive a new principle for the design of speech access systems: What You See Is (Almost) What You Hear. We present a new user interface to a broadcast news archive, designed on that principle.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-281"
  },
  "azzopardi98_icslp": {
   "authors": [
    [
     "Daniel",
     "Azzopardi"
    ],
    [
     "Shahram",
     "Semnani"
    ],
    [
     "Ben",
     "Milner"
    ],
    [
     "Richard",
     "Wiseman"
    ]
   ],
   "title": "Improving accuracy of telephony-based, speaker-independent speech recognition",
   "original": "i98_0543",
   "page_count": 4,
   "order": 282,
   "p1": "paper 0543",
   "pn": "",
   "abstract": [
    "A combination of techniques for increasing recognition accuracy has been developed for an automated corporate directory system with 120,000 entries. Using a traditional recogniser an accuracy of around 60% has previously been obtained for both a 156 town name task and 1108 road name task. Techniques presented in this paper comprise front-end modifications, context dependent models, improved lexicon and noise modelling. This resulted in an increased recognition accuracy of around 90%.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-282"
  },
  "bayya98_icslp": {
   "authors": [
    [
     "Aruna",
     "Bayya"
    ]
   ],
   "title": "Rejection in speech recognition systems with limited training",
   "original": "i98_0572",
   "page_count": 4,
   "order": 283,
   "p1": "paper 0572",
   "pn": "",
   "abstract": [
    "In this paper, we propose a new rejection criterion applicable specifically to limited-training speech recognition systems such as Speaker-Dependent (SD) recognition systems. The new criterion uses confidence measures as well as speaker-specific out-of-vocabulary (OOV) models. The OOV models are created from the same training data that is available to create the in-vocabulary (IV) word models. We describe the method for creating these speaker-specific out-of-vocabulary models from limited training data. We also define a fairly robust confidence measure to reject the OOV words. The results presented in this paper demonstrate the effectiveness of the new criterion in a SD recognition task under various conditions.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-283"
  },
  "chen98b_icslp": {
   "authors": [
    [
     "Ruxin",
     "Chen"
    ],
    [
     "Miyuki",
     "Tanaka"
    ],
    [
     "Duanpei",
     "Wu"
    ],
    [
     "Lex",
     "Olorenshaw"
    ],
    [
     "Mariscela",
     "Amador"
    ]
   ],
   "title": "A four layer sharing HMM system for very large vocabulary isolated word recognition",
   "original": "i98_0583",
   "page_count": 4,
   "order": 284,
   "p1": "paper 0583",
   "pn": "",
   "abstract": [
    "This paper reports on a large vocabulary speaker independent isolated word recognizer targeting 50,000 words. The system supports a unique four-layer sharing structure for either continuous HMM or discrete HMM. Evaluation is performed using a dictionary of 5000 US city names, a dictionary of the 5000 English most frequent words, a dictionary of 50,000 English words, and the 110,000 word CMU English dictionary. For these dictionaries, recognition accuracy ranges from 90% to 93% for the top 3 results.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-284"
  },
  "chengalvarayan98_icslp": {
   "authors": [
    [
     "Rathinavelu",
     "Chengalvarayan"
    ]
   ],
   "title": "A comparative study of hybrid modelling techniques for improved telephone speech recognition",
   "original": "i98_0022",
   "page_count": 4,
   "order": 285,
   "p1": "paper 0022",
   "pn": "",
   "abstract": [
    "This paper presents a new technique for modelling heterogeneous data sources such as speech signals received via distinctly different channels which arises when an automatic speech recognition is deployed in wireless telephony in which highly heterogenous channels coexist and interoperate. The key problem is that a simple model may become inadequate to describe accurately the diversity of the signal, resulting in an unsatisfactory recognition performance. To cope up with this problem, different hybrid modelling techniques have been proposed and investigated in this paper by intelligently combining models from two different wireline and wireless environments.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-285"
  },
  "choi98_icslp": {
   "authors": [
    [
     "Jae-Seung",
     "Choi"
    ],
    [
     "Jong-Seok",
     "Lee"
    ],
    [
     "Hee-Youn",
     "Lee"
    ]
   ],
   "title": "Smoothing and tying for Korean flexible vocabulary isolated word recognition",
   "original": "i98_0623",
   "page_count": 4,
   "order": 286,
   "p1": "paper 0623",
   "pn": "",
   "abstract": [
    "For large vocabulary recognition system, as well as for flexible vocabulary applications using hidden Markov model(HMM), parameter smoothing and tying have been used to increase the reliability of models. This paper describes bottom-up and top-down clustering techniques for state level tying. This paper also describes a method of applying parameter smoothing to the clustered states and covariance matrix of semicontinuous hidden Markov model(SCHMM). We applied co-occurrence smoothing method(CSM) for senone smoothing. We present a new parameter smoothing method and apply it to the distribution of discrete hidden Markov model(DHMM) in the training procedure. A new model composition method for unseen triphone modeling in bottom-up clustering is also proposed and compared with traditional context-independent model backing-off method.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-286"
  },
  "ferreiros98_icslp": {
   "authors": [
    [
     "Javier",
     "Ferreiros"
    ],
    [
     "Javier",
     "Macias-Guarasa"
    ],
    [
     "Ascensi√≥n",
     "Gallardo"
    ],
    [
     "Jos√©",
     "Col√°s"
    ],
    [
     "Ricardo",
     "C√≥rdoba"
    ],
    [
     "Jos√© Manuel",
     "Pardo"
    ],
    [
     "Luis",
     "Villarrubia"
    ]
   ],
   "title": "Recent work on a preselection module for a flexible large vocabulary speech recognition system in telephone environment",
   "original": "i98_0987",
   "page_count": 4,
   "order": 287,
   "p1": "paper 0987",
   "pn": "",
   "abstract": [
    "At ICSLP'96 we presented a flexible, large vocabulary, speaker independent, isolated-word preselection system in a telephone environment, using a two stage, bottom-up strategy. We achieved reasonable performance in large and very large vocabulary tasks, ranging from 1200 to 10000 words. In this paper, we will describe recent studies we have carried out on the system, aimed in two directions: handling of non speech sounds in the speech signal (we consider lips, respiration and click noises); and making the preselection lists dynamic in length, to reduce computational load, in the average. In the first case, we want to model non speech sounds, as these effects are crucial in real-life situations, leading to wrong endpointing and increasing error rates. In the second, we are interested in integrating any available system parameter to calculate the preselection list length to use, having applied both parametric and non parametric methods.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-287"
  },
  "hoshimi98_icslp": {
   "authors": [
    [
     "Masakatsu",
     "Hoshimi"
    ],
    [
     "Maki",
     "Yamada"
    ],
    [
     "Katsuyuki",
     "Niyada"
    ],
    [
     "Shozo",
     "Makino"
    ]
   ],
   "title": "A study of noise robustness for speaker independent speech recognition method using phoneme similarity vector",
   "original": "i98_0257",
   "page_count": 4,
   "order": 288,
   "p1": "paper 0257",
   "pn": "",
   "abstract": [
    "As an input method for rapidly spreading small portable information devices, development of speaker independent speech recognition technology which can be embedded on a single DSP is now urgently requested. We have reported a speech recognition method using phoneme similarity vector as a feature vector, which is quite robust for reduction of precision of the feature parameter. We've also developed a recognition board with a single DSP, which works 100-word vocabulary using only the internal memory inside the DSP. [1][2] In this report, we propose a new technique which makes our recognition method more robust, where a newly introduced noise standard template together with traditional phoneme standard templates for calculating phoneme similarity vector realizes precise word-spotting. When the newly proposed noise robustness method was tested with 100 isolated word vocabulary speech of 50 subjects, recognition accuracy of 94.7% was obtained under various noisy environments.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-288"
  },
  "jian98_icslp": {
   "authors": [
    [
     "Fran H. L.",
     "Jian"
    ]
   ],
   "title": "Classification of taiwanese tones based on pitch and energy movements",
   "original": "i98_0146",
   "page_count": 4,
   "order": 289,
   "p1": "paper 0146",
   "pn": "",
   "abstract": [
    "This paper addresses the difficulties associated with automatically distinguishing the seven Taiwanese tones. The tone recogniser is an essential component of any automatic speech recognition system customised for tone languages such as Taiwanese. We show that it is difficult to distinguish between the Taiwanese tones simply employing the fundamental frequency contours and that the task is simplified by employing energy contour features besides the fundamental frequency features.  To allow energy to be accommodated into the classification model an energy-contour feature extraction approach is presented. The proposed approach is inspired by the ADSR model employed in musical instrument synthesis where the envelopes of complex sounds are modeled employing only a few parameters. Our experiments demonstrate that the inclusion of energy into the recognition model allows the seven Taiwanese tones to be discriminated successfully. The paper also presents acoustical measurements of the fundamental frequency and energy features described\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-289"
  },
  "johansen98_icslp": {
   "authors": [
    [
     "Finn Tore",
     "Johansen"
    ]
   ],
   "title": "Phoneme-based recognition for the norwegian speechdat(II) database",
   "original": "i98_0889",
   "page_count": 4,
   "order": 290,
   "p1": "paper 0889",
   "pn": "",
   "abstract": [
    "This paper presents results from a number of flexible vocabulary recognition experiments on the Norwegian SpeechDat(II) database. A common phoneme-based recogniser design procedure is tested on five different tasks, and for five different training sets. Results verify that reasonably accurate recognisers can be built with the database, using standard HMM techniques. They also quantify the importance of training set selection for small and medium vocabulary tasks.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-290"
  },
  "karnjanadecha98_icslp": {
   "authors": [
    [
     "Montri",
     "Karnjanadecha"
    ],
    [
     "Stephen A.",
     "Zahorian"
    ]
   ],
   "title": "Robust feature extraction for alphabet recognition",
   "original": "i98_1110",
   "page_count": 4,
   "order": 291,
   "p1": "paper 1110",
   "pn": "",
   "abstract": [
    "Spectral/temporal segment features are adapted for isolated word recognition and tested with the entire English alphabet set using Hidden Markov Models. The ISOLET database from OGI and the HTK toolkit from Cambridge university were used to test our feature extraction technique. With our feature set we were able to achieve 97.3% recognition accuracy on test data with one pass using a whole word based recognizer. Gaussian noise was also added to evaluate robustness of the feature set. We were able to obtain recognition accuracies of 49.6% and 84.3% at SNR of -10dB and 0dB, respectively. Linear discriminant analysis was also applied to the initial feature set for a number of feature configurations and noise levels but, generally, the performance was not improved. We conclude that the initial feature computations used are both very efficient (best results obtained with 50 total features) and robust in the presence of noise.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-291"
  },
  "kawai98_icslp": {
   "authors": [
    [
     "Hisashi",
     "Kawai"
    ],
    [
     "Norio",
     "Higuchi"
    ]
   ],
   "title": "Recognition of connected digit speech in Japanese collected over the telephone network",
   "original": "i98_0694",
   "page_count": 4,
   "order": 292,
   "p1": "paper 0694",
   "pn": "",
   "abstract": [
    "This paper describes experimental results on whole word HMM-based speech recognition of connected digits in Japanese collected through the telephone network. The training data comprises 756860 digits uttered by 1963 speakers, while the testing data comprises 304212 digits uttered by 852 speakers. The best performance was a word error rate of 0.42% for known length strings obtained using context dependent models. The word error rate was measured as a function of the training data size. The result showed that at least 3302 samples per speaker and 344 speakers are necessary and sufficient for context independent training. Error analysis was conducted on a fraction of the population bearing the major part of recognition errors. The results suggested that such speakers arise not simply from speaker characteristics but from a combination of speaker characteristics and environmental conditions.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-292"
  },
  "koizumi98_icslp": {
   "authors": [
    [
     "Takuya",
     "Koizumi"
    ],
    [
     "Shuji",
     "Taniguchi"
    ],
    [
     "Kazuhiro",
     "Kohtoh"
    ]
   ],
   "title": "Improving the speaker-dependency of subword-unit-based isolated word recognition",
   "original": "i98_0051",
   "page_count": 4,
   "order": 293,
   "p1": "paper 0051",
   "pn": "",
   "abstract": [
    "This paper deals with a subword-unit-based isolated word recognition system with enhanced speaker-independency. The subword is defined as a part of word whose central portion has rather stationary or time-invariant short-time spectra with its portions near its ends having rapidly varying short-time spectra. In this system each isolated word is decomposed into a sequence of subwords, each of which is identified by means of a particular semi-continuous hidden Markov model that is named a subword HMM. Each isolated word is recognized by a particular set of concatenated subword HMMs that is designated as a word HMM. Subword boundaries within a word are detected by finding peaks of the magnitude of delta cepstral vectors obtained from the word. The system attains average word recognition rates over 87 % for a number of Japanese words uttered by ten native male speakers.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-293"
  },
  "konuma98_icslp": {
   "authors": [
    [
     "Tomohiro",
     "Konuma"
    ],
    [
     "Tetsu",
     "Suzuki"
    ],
    [
     "Maki",
     "Yamada"
    ],
    [
     "Yoshio",
     "Ohno"
    ],
    [
     "Masakatsu",
     "Hoshimi"
    ],
    [
     "Katsuyuki",
     "Niyada"
    ]
   ],
   "title": "Speaker independent speech recognition method using constrained time alignment near phoneme discriminative frame",
   "original": "i98_0198",
   "page_count": 4,
   "order": 294,
   "p1": "paper 0198",
   "pn": "",
   "abstract": [
    "We present constrained time alignment acoustic models based on phonetic knowledge and a speaker independent speech recognition method using our proposed models. Japanese syllable and isolated word recognition experiments show that the models have robustness to intra- and inter- speaker varieties such as acoustic diversity. Furthermore we experiment with word recognition tests under the condition such as noise environments and endpoints free matching, it reveals the feasibility of our proposed models.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-294"
  },
  "lee98f_icslp": {
   "authors": [
    [
     "Ki Yong",
     "Lee"
    ],
    [
     "Joohun",
     "Lee"
    ]
   ],
   "title": "A nonstationary autoregressive HMM with gain adaptation for speech recognition",
   "original": "i98_0408",
   "page_count": 4,
   "order": 295,
   "p1": "paper 0408",
   "pn": "",
   "abstract": [
    "In this paper, a time domain approach for speech recognition is developed. The nonstationary autoregressive (AR) hidden markov model (HMM) with gain contour is proposed for modeling the statistical characteristics of the speech signal. The parameter of nonstationary AR model was modeled by the polynomial function with linear combination of M known basis functions. In this proposed model, speech signal is blocked by samples into fixed-length frames and modeled by nonstationary AR model controlled by markov switching sequences at each frame. Given the HMM parameter set of the speech, the gain-adapted recognition algorithm is developed for speech recognition.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-295"
  },
  "lyu98_icslp": {
   "authors": [
    [
     "Ren-yuan",
     "Lyu"
    ],
    [
     "Yuang-jin",
     "Chiang"
    ],
    [
     "Wen-ping",
     "Hsieh"
    ]
   ],
   "title": "A large-vocabulary taiwanese (MIN-NAN) multi-syllabic word recognition system based upon right-context-dependent phones with state clustering by acoustic decision tree",
   "original": "i98_0080",
   "page_count": 4,
   "order": 296,
   "p1": "paper 0080",
   "pn": "",
   "abstract": [
    "In this paper, we apply context dependent phonetic modeling on the task of large vocabulary (with 20 thousand words) Taiwanese multi-syllabic word recognition. Considering the phonetic characteristics of Taiwanese, the right context dependent (RCD) phones instead of the general tri-phones are used. The RCDs are further clustered at the sub-phone or state level using a decision tree with a set of context-split questions specially designed for Taiwanese speech according to the acoustic/phonetic knowledge. For the speaker dependent case, 7.18% word error rate is achieved. A real-time prototype system implemented on a Pentium-II personal computer running MS-Windows95/ NT is also shown to validate the approaches proposed here.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-296"
  },
  "tanaka98_icslp": {
   "authors": [
    [
     "Kazuyo",
     "Tanaka"
    ],
    [
     "Hiroaki",
     "Kojima"
    ]
   ],
   "title": "Speech recognition based on the distance calculation between intermediate phonetic code sequences in symbolic domain",
   "original": "i98_0966",
   "page_count": 4,
   "order": 297,
   "p1": "paper 0966",
   "pn": "",
   "abstract": [
    "This paper proposes a speech recognition method alternative to the conventional sample-based statistical methods which are characterized by the necessity of large amounts of training speech data. To resolve this type of heavy processing, the proposed method employs an intermediate phonetic code system and the calculation of distance between phonetic code sequences in symbolic domain. It realizes high efficiency when compared with direct processing of acoustic correlates, although some deterioration will be expected in recognition scores. We first describe the distance calculation method and present specific procedures for obtaining the intermediate code sequence from input utterances and for spotting words using the calculation of distance in the symbolic domain. Preliminary experiments were examined on isolated word recognition and phrase spotting in continuous speech. Word recognition results indicate that the recognition scores obtained by the proposed method are comparable compared with ordinary phone-HMM-based speech recognition.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-297"
  },
  "yang98c_icslp": {
   "authors": [
    [
     "York Chung-Ho",
     "Yang"
    ],
    [
     "June-Jei",
     "Kuo"
    ]
   ],
   "title": "High accuracy Chinese speech recognition approach with Chinese input technology for telecommunication use",
   "original": "i98_0505",
   "page_count": 4,
   "order": 298,
   "p1": "paper 0505",
   "pn": "",
   "abstract": [
    "Phoneme-oriented input with synchronised auto-revision to pictographic nature of written Chinese and Chinese speech recognition are two subjects not often brought together in the same article nor even the same proposition. This paper explores the growing relations between these two entities and, in particular, investigates what is found in integration the Chinese phonetic input with its auto-revision methods (Kuo; 1986, 1987, 1995, 1996) and Chinese isolated word, continuous speech recognition for portable device such as mobile telephone. Chinese phonetic input with a synchronised auto-revision approach integrates with a small size, high recognition rate Chinese speech recognition kernel for DSP single chip application will be introduced in this paper. Chinese phrase taxonomy has been defined and the definition is ready to be obtained from the system's dictionary.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-298"
  },
  "roberts98_icslp": {
   "authors": [
    [
     "William J.J.",
     "Roberts"
    ],
    [
     "Yariv",
     "Ephraim"
    ]
   ],
   "title": "Robust speech recognition using HMM's with toeplitz state covariance matrices",
   "original": "i98_0141",
   "page_count": 4,
   "order": 299,
   "p1": "paper 0141",
   "pn": "",
   "abstract": [
    "Hidden Markov modeling of speech waveforms is studied and applied to speech recognition of clean and noisy signals. Signal vectors in each state are assumed Gaussian with zero mean and a Toeplitz covariance matrix. This model allows short signal vectors and thus is useful for speech signals with rapidly changing second order statistics. It can also be straightforwardly adapted to noisy signals especially when the noise is additive and independent of the signal. Since no closed form solution exists for the maximum likelihood estimate of the Toeplitz covariance matrices, an expectation-maximization procedure was used and efficiently implemented. HMM's with Toeplitz as well as asymptotically Toeplitz (e.g., circulant, autoregressive) covariance matrices are theoretically and experimentally studied. While asymptotically all of these matrices provide similar performance, they differ significantly when the frame length is finite. Recognition results are provided for clean and noisy signals at 0-30dB SNR.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-299"
  },
  "thambiratnam98_icslp": {
   "authors": [
    [
     "David",
     "Thambiratnam"
    ],
    [
     "Sridha",
     "Sridharan"
    ]
   ],
   "title": "Modeling of output probability distribution to improve small vocabulary speech recognition in adverse environments",
   "original": "i98_0308",
   "page_count": 4,
   "order": 300,
   "p1": "paper 0308",
   "pn": "",
   "abstract": [
    "This paper presents a solution to the adverse environment, open microphone problem, by using the information stored in HMM output probability distributions to obtain a confidence measure of the results. This information can also be used to perform a secondary classification and improve recognition results. The system was tested on data from the TI46 database that had been corrupted by noise from the NOISEX-92 database, as well as on real-world data, and shows promising results.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-300"
  },
  "morin98_icslp": {
   "authors": [
    [
     "Philippe",
     "Morin"
    ],
    [
     "Ted H.",
     "Applebaum"
    ],
    [
     "Robert",
     "Boman"
    ],
    [
     "Yi",
     "Zhao"
    ],
    [
     "Jean-Claude",
     "Junqua"
    ]
   ],
   "title": "Robust and compact multilingual word recognizers using features extracted from a phoneme similarity front-end",
   "original": "i98_0402",
   "page_count": 4,
   "order": 301,
   "p1": "paper 0402",
   "pn": "",
   "abstract": [
    "In this paper we characterize the sensitivity of two speaker-dependent isolated word recognizers toward several kinds of variability and distortions; namely noise, channels, distance to microphone and target language. Both recognizers use a phoneme similarity acoustic front-end as a rich representation for speech from which reliable features are extracted. A cross-correlation test showed that a phoneme similarity front-end is more robust to variability and distortions (especially intra-speaker variability) than a LPC cepstral front-end. The first recognizer (Condor) uses a frame-based approach while the second (Pasha) uses the phoneme similarity information contained in a small number of speech segments. The two recognition methods are presented with a special emphasis on the robustness improvements and computational trade-offs that have been made. Experimental results are reported for car noise at different speeds, speakerphone versus handset input in an office environment and several target languages. Recognition accuracy greater than 94% was achieved in a car environment at 60 mph (Condor) and recognition accuracy greater than 95% was achieved for speakerphone input at a distance of 50 cm. in an office environment.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-301"
  },
  "yamada98_icslp": {
   "authors": [
    [
     "Takeshi",
     "Yamada"
    ],
    [
     "Satoshi",
     "Nakamura"
    ],
    [
     "Kiyohiro",
     "Shikano"
    ]
   ],
   "title": "An effect of adaptive beamforming on hands-free speech recognition based on 3-d viterbi search",
   "original": "i98_0484",
   "page_count": 4,
   "order": 302,
   "p1": "paper 0484",
   "pn": "",
   "abstract": [
    "To integrate the microphone array processing into speech recognition, we have proposed a speech recognition algorithm based on 3-D Viterbi search, which localizes a target talker considering the likelihood of HMMs (Hidden Markov Models) while performing speech recognition. The performance of the 3-D Viterbi search method depends on the improvement of the SNR (Signal to Noise Ratio) by the beamforming technique. This paper proposes a novel method based on an adaptive beamforming technique instead of the delay-and-sum beamformer used in our previous study. The speaker-dependent isolated-word recognition experiments were carried out on real environment data to evaluate the effect of the adaptive beamformer. These results showed that the adaptive beamformer drastically improves the recognition performance both for a fixed-position talker and for a moving-talker.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-302"
  },
  "gonzalezrodriguez98_icslp": {
   "authors": [
    [
     "Joaquin",
     "Gonzalez-Rodriguez"
    ],
    [
     "Santiago",
     "Cruz-Llanas"
    ],
    [
     "Javier",
     "Ortega-Garc√≠a"
    ]
   ],
   "title": "Coherence-based subband decomposition for robust speech and speaker recognition in noisy and reverberant rooms",
   "original": "i98_0064",
   "page_count": 4,
   "order": 303,
   "p1": "paper 0064",
   "pn": "",
   "abstract": [
    "In this paper, the acoustic characteristics of sound fields in enclosed rooms are studied in the joint presence of speech and noise, in order to design a broadband microphone array system capable of coping with both coherent and diffuse noises. Several state-of-the-art speech enhancement array structures are presented and compared to our new system in terms of correct word recognition rates in a simple command and control task. The proposed structure, based on a broadband subband-nested array, performs real-time estimations of the spatial coherence in order to determine the coherent/diffuse nature of the different subbands, using different filters in each case, improving also the classical Wiener post-filter, typically used for diffuse noise supression, for proper cancellation of coherent noises. The results obtained with a 15-channel simultaneous recording database in different reverberation and noise conditions show better performance than other structures previously proposed.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-303"
  },
  "jiang98_icslp": {
   "authors": [
    [
     "Hui",
     "Jiang"
    ],
    [
     "Keikichi",
     "Hirose"
    ],
    [
     "Qiang",
     "Huo"
    ]
   ],
   "title": "A minimax search algorithm for CDHMM based robust continuous speech recognition",
   "original": "i98_0693",
   "page_count": 4,
   "order": 304,
   "p1": "paper 0693",
   "pn": "",
   "abstract": [
    "In this paper, we propose a novel implementation of a minimax decision rule for continuous density hidden Markov model based robust speech recognition. By combining the idea of the minimax decision rule with a normal Viterbi search, we derive a recursive minimax search algorithm, where the minimax decision rule is repetitively applied to determine the partial paths during the search procedure. Because of its intrinsic nature of a recursive search, the proposed method can be easily extended to perform continuous speech recognition. Experimental results on Japanese isolated digits and TIDIGITS, where the mismatch between training and testing conditions is caused by additive white Gaussian noise, show the viability and efficiency of the proposed minimax search algorithm.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-304"
  },
  "wu98b_icslp": {
   "authors": [
    [
     "Su-Lin",
     "Wu"
    ],
    [
     "Brian E. D.",
     "Kingsbury"
    ],
    [
     "Nelson",
     "Morgan"
    ],
    [
     "Steven",
     "Greenberg"
    ]
   ],
   "title": "Performance improvements through combining phone- and syllable-scale information in automatic speech recognition",
   "original": "i98_0854",
   "page_count": 4,
   "order": 305,
   "p1": "paper 0854",
   "pn": "",
   "abstract": [
    "Combining knowledge derived from both syllable- (100-250 ms) and phone-length (40-100 ms) intervals in the automatic speech recognition process can yield performance superior to that obtained using information derived from a single time scale alone. The results are particularly pronounced for reverberant test conditions that have not been incorporated into the training set. In the present study, phone- and syllable-based systems are combined at three distinct levels of the recognition process --- the frame, the syllable and the entire utterance. Each strategy successfully integrates the complementary strengths of the individual systems, yielding a significant improvement in accuracy on a small-vocabulary, naturally spoken, telephone speech corpus. The syllable-level combination outperformed the other two methods under both relatively pristine and moderately reverberant acoustic conditions, yielding a 20-40% relative improvement over the baseline.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-305"
  },
  "surendran98_icslp": {
   "authors": [
    [
     "Arun C.",
     "Surendran"
    ],
    [
     "Chin-Hui",
     "Lee"
    ]
   ],
   "title": "Predictive adaptation and compensation for robust speech recognition",
   "original": "i98_0859",
   "page_count": 4,
   "order": 306,
   "p1": "paper 0859",
   "pn": "",
   "abstract": [
    "Earlier work in parametric modeling of distortions for robust speech recognition has focussed on estimating the distortion parameter using maximum likelihood and other techniques as a point in the parameter space, and treating this estimate as if it is the true value in a plug-in maximum a posteriori(MAP) decoder. This approach is deficient in most real environments where, due to many reasons, the value of the distortion parameter varies significantly. In this paper we introduce an approach which combines the power of parametric transformation and Bayesian prediction to solve this problem. Instead of approximating the distortion parameter with a point estimate, we average over its variation, thus taking into consideration the distribution of the parameter as well. This approach provides more robust performance than the conventional maximum-likelihood approach. It also provides the solution that minimizes the overall error given the distribution of the parameter. We present results to demonstrate the robustness and effectiveness of the predictive approach.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-306"
  },
  "junqua98_icslp": {
   "authors": [
    [
     "Jean-Claude",
     "Junqua"
    ],
    [
     "Steven",
     "Fincke"
    ],
    [
     "Ken",
     "Field"
    ]
   ],
   "title": "Influence of the speaking style and the noise spectral tilt on the lombard reflex and automatic speech recognition",
   "original": "i98_0374",
   "page_count": 4,
   "order": 307,
   "p1": "paper 0374",
   "pn": "",
   "abstract": [
    "To study the Lombard reflex, more realistic databases representing real world conditions need to be recorded and analyzed. In this paper we 1) propose a procedure to record Lombard data which provides a good approximation of realistic conditions and 2) present a comparison between two sets of experiments where subjects are in communication with a device while listening to noise through open-ear headphones and where subjects are reading a list. By studying acoustic correlates of the Lombard reflex and performing off-line speaker-independent recognition experiments it is shown that the communication factor affects the Lombard reflex. We also show evidence that several types of noise differing mainly by their spectral tilt induce different acoustic changes. This result reinforces the notion that it is difficult to separate the speaker from the environment stressor (in this case the noise) when studying the Lombard reflex.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-307"
  },
  "crafa98_icslp": {
   "authors": [
    [
     "Stefano",
     "Crafa"
    ],
    [
     "Luciano",
     "Fissore"
    ],
    [
     "Claudio",
     "Vair"
    ]
   ],
   "title": "Data-driven PMC and Bayesian learning integration for fast model adaptation in noisy conditions",
   "original": "i98_1140",
   "page_count": 4,
   "order": 308,
   "p1": "paper 1140",
   "pn": "",
   "abstract": [
    "In this paper, we present an integration of Data Driven Parallel Model Combination (DPMC) and Bayesian Learning into a fast and accurate framework which can be easily integrated in standard training and recognition systems. The original DPMC technique has been enhanced to avoid any modification of the acoustic models, as required by the original method. The Bayesian Learning estimation has been used in order to specialize a general noisy speech model (the a priori model) to the target acoustic environment, where the DPMC-generated observations are used as adaptation data. Thanks to these innovations, the proposed method can achieve better performance than the original DPMC, while consuming far less computational resources.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-308"
  },
  "hunke98_icslp": {
   "authors": [
    [
     "Martin",
     "Hunke"
    ],
    [
     "Meeran",
     "Hyun"
    ],
    [
     "Steve",
     "Love"
    ],
    [
     "Thomas",
     "Holton"
    ]
   ],
   "title": "Improving the noise and spectral robustness of an isolated-word recognizer using an auditory-model front end",
   "original": "i98_0715",
   "page_count": 4,
   "order": 309,
   "p1": "paper 0715",
   "pn": "",
   "abstract": [
    "In this study, the performance of an auditory-model feature-extraction 'front end' was assessed in an isolated-word speech recognition task using a common hidden Markov model (HMM) 'back end', and compared with the performance of other feature representation front-end methods including mel-frequency cepstral coefficients (MFCC) and two variants (J- and L-) of the relative spectral amplitude (RASTA) technique. The recognition task was performed in the presence of varying levels and types of additive noise and spectral distortion using standard HMM whole-word models with the Bellcore Digit database as a corpus. While all front ends achieved comparable recognition performance in clean speech, the performance of the auditory-model front end was generally significantly higher than other methods in recognition tasks involving background noise or spectral distortion. Training HMMs with speech processed by the auditory-model or L-RASTA front end in one type of noise also improved the recognition performance with other kinds of noise. This 'cross-training' effect did not occur with the MFCC or J-RASTA front end.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-309"
  },
  "kenny98_icslp": {
   "authors": [
    [
     "Owen P.",
     "Kenny"
    ],
    [
     "Douglas J.",
     "Nelson"
    ]
   ],
   "title": "A model for speech reverberation and intelligibility restoring filters",
   "original": "i98_0863",
   "page_count": 4,
   "order": 310,
   "p1": "paper 0863",
   "pn": "",
   "abstract": [
    "The problem of removing channel effects from speech has generally been attacked by attempting to recover a time-varying filter which inverts the entire channel impulse response. We show that human listeners are insensitive to many channel conditions and that the human ear seems to respond primarily to discontinu-ities of the channel. As a result of these observations, a partial equalization is proposed in which the channel effects to which the ear is sensitive may be removed, without full inversion of the channel. In addition, it is shown that it is possible to build filters of arbitrary length which do not reduce speech intelligibility and do not produce annoying artifacts.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-310"
  },
  "zhou98_icslp": {
   "authors": [
    [
     "Guojun",
     "Zhou"
    ],
    [
     "John H. L.",
     "Hansen"
    ],
    [
     "James F.",
     "Kaiser"
    ]
   ],
   "title": "Linear and nonlinear speech feature analysis for stress classification",
   "original": "i98_0840",
   "page_count": 4,
   "order": 311,
   "p1": "paper 0840",
   "pn": "",
   "abstract": [
    "Many stressful environments can deteriorate the performance of speech recognition systems such as aircraft cockpits or high workload task stress/emotional situations. To address this, we investigate a number of linear and nonlinear features and processing methods for stressed speech classification. The linear features include properties of pitch, duration, intensity, glottal source, and the vocal tract spectrum. Nonlinear processing is based on our newly proposed Teager Energy Operator speech feature which incorporates frequency domain critical band filters and properties of the resulting TEO autocorrelation envelope. In this study, we employ a Bayesian hypothesis testing and a hidden Markov model processor as classification methods. Evaluations focused on speech under loud, angry, and the Lombard effect from the SUSAS database. Results using ROC curves and EER based detection show that pitch is the best of the five linear features for stress classification; while the new nonlinear TEO-based feature outperforms the best linear feature by +5.2%, with a reduction in classification rate variability from 8.66 to 3.90.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-311"
  },
  "boughazale98_icslp": {
   "authors": [
    [
     "Sahar E.",
     "Bou-Ghazale"
    ],
    [
     "John H. L.",
     "Hansen"
    ]
   ],
   "title": "Speech feature modeling for robust stressed speech recognition",
   "original": "i98_0918",
   "page_count": 4,
   "order": 312,
   "p1": "paper 0918",
   "pn": "",
   "abstract": [
    "It is well known that the performance of speech recognition algorithms degrade in the presence of adverse environments where a speaker is under stress, emotion, or Lombard effect. This study evaluates the effectiveness of traditional features in recognition of speech under stress and formulates new features which are shown to improve stressed speech recognition. The focus is on formulating robust features which are less dependent on the speaking conditions rather than applying compensation or adaptation techniques. The stressed speaking styles considered are simulated angry and loud, Lombard effect speech, and noisy actual stressed speech from the SUSAS database. In addition, this study investigates the immunity of LP and FFT power spectrum to the presence of stress. Our results show that unlike FFT's immunity to noise, the LP power spectrum is more effective than the FFT to stress as well as to a combination of a noisy and stressful environment. Two alternative frequency partitioning methods (M-MFCC, ExpoLog) are proposed and compared with traditional MFCC features for stressed speech recognition. It is shown that the alternate filterbank frequency partitions are more effective for recognition of speech under both simulated and actual stressed conditions.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-312"
  },
  "kirchhoff98_icslp": {
   "authors": [
    [
     "Katrin",
     "Kirchhoff"
    ]
   ],
   "title": "Combining articulatory and acoustic information for speech recognition in noisy and reverberant environments",
   "original": "i98_0873",
   "page_count": 4,
   "order": 313,
   "p1": "paper 0873",
   "pn": "",
   "abstract": [
    "Robust speech recognition under varying acoustic conditions may be achieved by exploiting multiple sources of information in the speech signal. In addition to an acoustic signal representation, we use an articulatory representation consisting of pseudo-articulatory features as an additional information source. Hybrid ANN/HMM recognizers using either of these representations are evaluated on a continuous numbers recognition task (OGI Numbers95) under clean, reverberant and noisy conditions. An error analysis of preliminary recognition results shows that the different representations produce qualitatively different errors, which suggests a combination of both representations. We investigate various combination possibilities at the phoneme estimation level and show that significant improvements can been achieved under all three acoustic conditions.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-313"
  },
  "wark98_icslp": {
   "authors": [
    [
     "Timothy",
     "Wark"
    ],
    [
     "Sridha",
     "Sridharan"
    ]
   ],
   "title": "Improving speaker identification performance in reverberant conditions using lip information",
   "original": "i98_0294",
   "page_count": 4,
   "order": 314,
   "p1": "paper 0294",
   "pn": "",
   "abstract": [
    "This paper considers the improvement of speaker identification performance in reverberant conditions using additional lip information. Automatic speaker identification (ASI) using speech characteristics alone can be highly successful, however problems occur with mismatches between training and testing conditions. In particular, we find that ASI performance drops dramatically when given anechoic training but reverberant test speech. Previous work [1][2] has shown that speaker dependant information can be extracted from the static and dynamic qualities of moving lips. Given that lip information is unaffected by reverberation, we choose to fuse this additional information with speech data. We propose a new method for estimating confidence levels to allow adaptive fusion of the audio and visual data. Identification results are presented for increasing levels of artificially reverberated data, where lip information is shown to provide excellent ASI performance improvement.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-314"
  },
  "akagi98_icslp": {
   "authors": [
    [
     "Masato",
     "Akagi"
    ],
    [
     "Mamoru",
     "Iwaki"
    ],
    [
     "Noriyoshi",
     "Sakaguchi"
    ]
   ],
   "title": "Spectral sequence compensation based on continuity of spectral sequence",
   "original": "i98_0028",
   "page_count": 4,
   "order": 315,
   "p1": "paper 0028",
   "pn": "",
   "abstract": [
    "Humans have an excellent ability to select a particular sound source from a noisy environment, called the ``Cocktail-Party Effect'' and to compensate for physically missing sound, called the ``Illusion of Continuity.'' This paper proposes a spectral peak tracker as a model of the illusion of continuity (or phonemic restoration) and a spectral sequence prediction method using a spectral peak tracker. Although some models have already been proposed, they treat only spectral peak frequencies and often generate wrong predicted spectra. We introduce a peak representation of log-spectrum with four parameters: amplitude, frequency, bandwidth, and asymmetry, using the spectral shape analysis method described by the wavelet transformation. And we devise a time-varying second-order system for formulating the trajectories of the parameters. We demonstrate that the model can estimate and track the parameters for connected vowels whose transition section has been partially replaced by white noise.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-315"
  },
  "bayya98b_icslp": {
   "authors": [
    [
     "Aruna",
     "Bayya"
    ],
    [
     "B.",
     "Yegnanarayana"
    ]
   ],
   "title": "Robust features for speech recognition systems",
   "original": "i98_1121",
   "page_count": 4,
   "order": 316,
   "p1": "paper 1121",
   "pn": "",
   "abstract": [
    "ABSTRACT In this paper we propose a set of features based on group delay spectrum for speech recognition systems. These features appear to be more robust to channel variations and environmental changes compared to features based on Melspectral coefficients. The main idea is to derive cepstrum-like features from group delay spectrum instead of deriving them from power spectrum. The group delay spectrum is computed from modified auto-correlation-like function. The effectiveness of the new feature set is demonstrated by the results of both speaker-independent (SI) and speaker-dependent (SD) recognition tasks. Preliminary results indicate that using the new features, we can obtain results comparable to Mel cepstra and PLP cepstra in most of the cases and a slight improvement in noisy cases. More optimization of the parameters is needed to fully exploit the nature of the new features.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-316"
  },
  "berthommier98_icslp": {
   "authors": [
    [
     "Fr√©d√©ric",
     "Berthommier"
    ],
    [
     "Herv√©",
     "Glotin"
    ],
    [
     "Emmanuel",
     "Tessier"
    ],
    [
     "Herv√©",
     "Bourlard"
    ]
   ],
   "title": "Interfacing of CASA and partial recognition based on a multistream technique",
   "original": "i98_0600",
   "page_count": 4,
   "order": 317,
   "p1": "paper 0600",
   "pn": "",
   "abstract": [
    "We propose a running demonstration of coupling between an intermediate processing step (named CASA), based on the harmonicity cue, and partial recognition, implemented with a HMM/ANN multistream technique [2]. The model is able to recognise words corrupted with narrow band noise, either stationary or having variable center frequency. The principle is to identify frame by frame the most noisy subband within four subbands by analysing a SNR-dependent representation. A static partial recogniser is fed with the remaining subbands. We establish on Numbers93 the noisy-band identification (NBI) performance as well as the word error rate (WER), and alter the correlation between these two indexes by changing the distribution of the noise.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-317"
  },
  "chang98_icslp": {
   "authors": [
    [
     "Sen-Chia",
     "Chang"
    ],
    [
     "Shih-Chieh",
     "Chien"
    ],
    [
     "Chih-Chung",
     "Kuo"
    ]
   ],
   "title": "AN RNN-based compensation method for Mandarin telephone speech recognition",
   "original": "i98_1077",
   "page_count": 4,
   "order": 318,
   "p1": "paper 1077",
   "pn": "",
   "abstract": [
    "In this paper, a novel architecture, which integrates the recurrent neural network (RNN) based compensation process and the hidden Markov model (HMM) based recognition process into a unified framework, is proposed. The RNN is employed to estimate the additive bias, which represents the telephone channel effect, in the cepstral domain. Compensation of telephone channel effects is implemented by subtracting the additive bias from the cepstral coefficients of the input utterance. The integrated recognition system is trained based upon MCE/GPD (minimum classification error/generalized probabilistic descent) method with an objective function that is designed to minimize recognition error rate. Experimental results for speaker-independent Mandarin polysyllabic word recognition show an error rate reduction of 21.5% compared to the baseline system.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-318"
  },
  "chu98_icslp": {
   "authors": [
    [
     "Stephen M.",
     "Chu"
    ],
    [
     "Yunxin",
     "Zhao"
    ]
   ],
   "title": "Robust speech recognition using discriminative stream weighting and parameter interpolation",
   "original": "i98_0690",
   "page_count": 4,
   "order": 319,
   "p1": "paper 0690",
   "pn": "",
   "abstract": [
    "This paper presents a method to improve the robustness of speech recognition in noisy conditions. It has been shown that using dynamic features in addition to static features can improve the noise robustness of speech recognizers. In this work we show that in a continuous-density Hidden Markov Model (HMM) based speech recognition system, weighting the contribution of the dynamic features according to SNR levels can further improve the performance, and we propose a two-step scheme to adapt the weights for a given Signal to Noise Ratio (SNR). The first step is to obtain the optimal weights for a set of selected SNR levels by discriminative training. The Generalized Probabilistic Decent (GPD) framework is used in our experiments. The second step is to interpolate the set of SNR-specific weights obtained in step one for a new SNR condition. Experimental results obtained by the proposed technique is encouraging. Evaluation using speaker independent digits with added white Gaussian noise shows significant reduction in error rate at various SNR levels.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-319"
  },
  "veth98_icslp": {
   "authors": [
    [
     "Johan de",
     "Veth"
    ],
    [
     "Bert",
     "Cranen"
    ],
    [
     "Louis",
     "Boves"
    ]
   ],
   "title": "Acoustic backing-off in the local distance computation for robust automatic speech recognition",
   "original": "i98_0360",
   "page_count": 4,
   "order": 320,
   "p1": "paper 0360",
   "pn": "",
   "abstract": [
    "In this paper we propose to introduce backing-off in the acoustic contributions of the local distance functions used during Viterbi decoding as an operationalisation of missing feature theory for increased recognition robustness. Acoustic backing-off effectively removes the detrimental influence of outlier values from the local decisions in the Viterbi algorithm. It does so without the need for prior knowledge that specific features are missing. Acoustic backing-off avoids any kind of explicit outlier detection. This paper provides a proof of concept of acoustic backing-off in the context of connected digit recognition over the telephone, using artificial distortions of the acoustic observations. It is shown that the word error rate can be maintained at the level of 2.5 % obtained for undisturbed features, even in the case where a conventional local distance computation without backing-off leads to a word error rate > 80.0 %.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-320"
  },
  "dociofernandez98_icslp": {
   "authors": [
    [
     "Laura",
     "Docio-Fern√°ndez"
    ],
    [
     "Carmen",
     "Garc√≠a-Mateo"
    ]
   ],
   "title": "Noise model selection for robust speech recognition",
   "original": "i98_0579",
   "page_count": 4,
   "order": 321,
   "p1": "paper 0579",
   "pn": "",
   "abstract": [
    "This paper addresses the problem of mismatch between training and testing conditions in a HMM-based speech recognizer. Parallel Model Combination (PMC) has demonstrated to be an efficient technique for reducing the effects of additive noise. In order to apply this technique, a noise HMM must be trained at the recognition phase. Approaches that estimate the noise model based on the Expectation-Maximization (EM) or Baum-Welch algorithms are widely used. In these methods the recorded environmental noise data are used, and their major drawback is that they need a long sequence of noise data to estimate properly the model parameters. In some real life applications the amount of noise data can be too small, so from a practical point of view, the needed amount of noise is a critical parameter which should be as short as possible. We propose a novel method for obtaining a more reliable noise model than training it from scratch by using a short noise sequence.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-321"
  },
  "doclo98_icslp": {
   "authors": [
    [
     "Simon",
     "Doclo"
    ],
    [
     "Ioannis",
     "Dologlou"
    ],
    [
     "Marc",
     "Moonen"
    ]
   ],
   "title": "A novel iterative signal enhancement algorithm for noise reduction in speech",
   "original": "i98_0131",
   "page_count": 4,
   "order": 322,
   "p1": "paper 0131",
   "pn": "",
   "abstract": [
    "This paper presents an iterative signal enhancement algorithm for noise reduction in speech. The algorithm is based on a truncated singular value decomposition (SVD) procedure, which has already been used as a tool for signal enhancement [1][2]. Compared to the classical algorithms, the novel algorithm gives rise to comparable improvements in signal-to-noise ratio (SNR). Moreover the algorithm has an improved frequency selectivity for filtering out the noise and performs better with respect to the higher formants of the speech. It can also be extended easily to multiple channels.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-322"
  },
  "dupont98b_icslp": {
   "authors": [
    [
     "St√©phane",
     "Dupont"
    ]
   ],
   "title": "Missing data reconstruction for robust automatic speech recognition in the framework of hybrid HMM/ANN systems",
   "original": "i98_0581",
   "page_count": 4,
   "order": 323,
   "p1": "paper 0581",
   "pn": "",
   "abstract": [
    "In this paper, we propose to use the missing data theory to allow the reconstruction of missing spectro-temporal parameters in the framework of hybrid HMM/ANN systems. A simple signal-to-noise ratio estimator is used to automatically detect the components that are unavailable or corrupted by noise (missing components). A limited number of multidimensional gaussian distributions are then used to reconstruct those missing components solely on the basis of the present data. The reconstructed vectors are then used as input to an artificial neural network estimating the HMM state probabilities. Continuous speech recognition experiments have been done on filtered speech. In this case, filtered components carry few or no information at all, and hence, should probably be ignored. The results presented in this paper illustrate this point of view. Complementary experiments also suggest the interest of the proposed approach in the case of noisy speech.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-323"
  },
  "gallardoantolin98_icslp": {
   "authors": [
    [
     "Ascensi√≥n",
     "Gallardo-Antolin"
    ],
    [
     "Fernando",
     "Diaz-de-Maria"
    ],
    [
     "Francisco J.",
     "Valverde-Albacete"
    ]
   ],
   "title": "Recognition from GSM digital speech",
   "original": "i98_0584",
   "page_count": 4,
   "order": 324,
   "p1": "paper 0584",
   "pn": "",
   "abstract": [
    "This paper addresses the problem of speech recognition in the GSM environment. In this context, new sources of distortion, such as transmission errors or speech coding itself, significantly degrade the performance of speech recognizers. While conventional approaches deal with these types of distortion after decoding speech, we propose to recognize from the digital speech representation of GSM. In particular, our work focuses on the 13 kbit/s RPE-LTP GSM standard speech coder. In order to test our recognizer we have compared it to a conventional recognizer in several simulated situations, which allow us to gain insight into more practical ones. Specifically, besides recognizing from clean digital speech and evaluating the influence of speech coding distortion, the proposed recognizer is faced with speech degraded by random errors, burst errors and frame substitutions. The results are very encouraging: the worse the transmission conditions are, the more recognizing from digital speech outperforms the conventional approach.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-324"
  },
  "geutner98_icslp": {
   "authors": [
    [
     "Petra",
     "Geutner"
    ],
    [
     "Matthias",
     "Denecke"
    ],
    [
     "Uwe",
     "Meier"
    ],
    [
     "Martin",
     "Westphal"
    ],
    [
     "Alex",
     "Waibel"
    ]
   ],
   "title": "Conversational speech systems for on-board car navigation and assistance",
   "original": "i98_0772",
   "page_count": 4,
   "order": 325,
   "p1": "paper 0772",
   "pn": "",
   "abstract": [
    "This paper describes our latest efforts in building a speech recognizer for operating a navigation system through speech instead of typed input. Compared to conventional speech recognition for navigation systems, where the input is usually restricted to a fixed set of keywords and keyword phrases, complete spontaneous sentences are allowed as speech input. We will present the interaction of speech input, parsing and the reactions to the requested queries. Our system has been trained on German spontaneous speech data and has been adapted to navigation queries using MLLR. As the system is not restricted to command word input, a parser further processes the recognized utterance. We show that within a lab environment our system is able to handle arbitrary spontaneous sentences as input to a navigation system successfully. The performance of the recognizer measured in word error rate gives a result of 18%. Evaluation of the parser yields an error rate of 20%.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-325"
  },
  "girin98_icslp": {
   "authors": [
    [
     "Laurent",
     "Girin"
    ],
    [
     "Laurent",
     "Varin"
    ],
    [
     "Gang",
     "Feng"
    ],
    [
     "Jean-Luc",
     "Schwartz"
    ]
   ],
   "title": "A signal processing system for having the sound \"pop-out\" in noise thanks to the image of the speaker's lips: new advances using multi-layer perceptrons",
   "original": "i98_0431",
   "page_count": 4,
   "order": 326,
   "p1": "paper 0431",
   "pn": "",
   "abstract": [
    "This paper deals with the improvement of a noisy speech enhancement system based on the fusion of auditory and visual information. The system was presented in previous papers and implemented in the context of vowel to vowel and vowel to consonant transitions corrupted with white noise. Its principle consists in an analysis-enhancement-synthesis process based on a linear prediction (LP) model of the signal: the LP filter is enhanced thanks to associative tools that estimate LP cleaned parameters from both noisy audio and visual information. The detailed structure of the system is reminded and we focus on the improvement that concerns precisely the associators: basic neural networks (multi-layers perceptrons) are used instead of linear regression. It is shown that in the context of VCV transitions corrupted with white noise, neural networks can improve the performances of the system in terms of intelligibility gain, distance measures and classification tests.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-326"
  },
  "sarikaya98_icslp": {
   "authors": [
    [
     "Ruhi",
     "Sarikaya"
    ],
    [
     "John H. L.",
     "Hansen"
    ]
   ],
   "title": "Robust speech activity detection in the presence of noise",
   "original": "i98_0922",
   "page_count": 4,
   "order": 327,
   "p1": "paper 0922",
   "pn": "",
   "abstract": [
    "This study presents a new approach for robust speech activity detection (SAD). Our framework is based on HMM recognition of speech versus silence. We model speech as one of fourteen large phone classes whereas silence is represented as a separate model. Individual test utterances are concatenated to simulate read continuous speech for testing. The HMM-based algorithm is compared to both an energy based, as well as speech enhancement based, SAD algorithms for clean, 5 dB and 0 dB SNR levels under white Gaussian noise (WGN), aircraft cockpit noise (AIR) and automobile highway noise (HWY). We found that our algorithm provides lower frame error rates than the other two methods especially for HWY noise. Unlike other studies, we evaluate our algorithm on the core test set of the standard TIMIT database. Hence, results can be used as benchmarks to evaluate future systems.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-327"
  },
  "heon98_icslp": {
   "authors": [
    [
     "Michel",
     "H√©on"
    ],
    [
     "Hesham",
     "Tolba"
    ],
    [
     "Douglas",
     "O'Shaughnessy"
    ]
   ],
   "title": "Robust automatic speech recognition by the application of a temporal-correlation-based recurrent multilayer neural network to the mel-based cepstral coefficients",
   "original": "i98_0807",
   "page_count": 4,
   "order": 328,
   "p1": "paper 0807",
   "pn": "",
   "abstract": [
    "In this paper, the problem of robust speech recognition has been considered. Our approach is based on the noise reduction of the parameters that we use for recognition, that is, the Mel-based cepstral coefficients. A Temporal-Correlation-Based Recurrent Multilayer Neural Network (TCRMNN) for noise reduction in the cepstral domain is used in order to get less-variant parameters to be useful for robust recognition in noisy environments. Experiments show that the use of the enhanced parameters using such an approach increases the recognition rate of the continuous speech recognition (CSR) process. The HTK Hidden Markov Model Toolkit was used throughout. Experiments were done on a noisy version of the TIMIT database. With such a pre-processing noise reduction technique in the front-end of the HTK-based continuous speech recognition system (CSR) system, improvements in the recognition accuracy of about 17.77% and 18.58% using single mixture monophones and triphones, respectively, have been obtained at a moderate SNR of 20 dB.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-328"
  },
  "huerta98_icslp": {
   "authors": [
    [
     "Juan M.",
     "Huerta"
    ],
    [
     "Richard M.",
     "Stern"
    ]
   ],
   "title": "Speech recognition from GSM codec parameters",
   "original": "i98_0626",
   "page_count": 4,
   "order": 329,
   "p1": "paper 0626",
   "pn": "",
   "abstract": [
    "Speech coding affects speech recognition performance, with recognition accuracy deteriorating as the coded bit rate decreases. Virtually all systems that recognize coded speech reconstruct the speech waveform from the coded parameters, and then perform recognition (after possible noise and/or channel compensation) using conventional techniques. In this paper we compare the recognition accuracy of coded speech obtained by reconstructing the speech waveform with the speech recognition accuracy obtained when using cepstral features derived from the coding parameters. We focus our efforts on speech that has been coded using the 13-kbps full-rate GSM codec, a Regular Pulse Excited Long Term Prediction (RPE-LTP) codec. The GSM codec develops separate representations for the linear prediction (LPC) filter and the residual signal components of the coded speech. We measure the effects of quantization and coding on the accuracy with which these parameters are represented, and present two different methods for recombining them for speech recognition purposes. We observe that by selectively combining the cepstral streams repre senting the LPC parameters and the residual signal it is possible to obtain recognition accuracy directly from the coded parameters that equals or exceeds the recognition accuracy obtained from the reconstructed waveforms.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-329"
  },
  "hung98_icslp": {
   "authors": [
    [
     "Jeih-Weih",
     "Hung"
    ],
    [
     "Jia-Lin",
     "Shen"
    ],
    [
     "Lin-Shan",
     "Lee"
    ]
   ],
   "title": "Improved parallel model combination based on better domain transformation for speech recognition under noisy environments",
   "original": "i98_0231",
   "page_count": 4,
   "order": 330,
   "p1": "paper 0231",
   "pn": "",
   "abstract": [
    "The parallel model combination (PMC) technique has been shown to achieve very good performance for speech recognition under noisy conditions. However, there still exist some problems based on the PMC formula. In this paper, we first investigated these problems and some modifications on the transformation process of PMC were proposed. Experimental results show that this modified PMC can provide significant improvements over the original PMC in the recognition accuracies. Error rate reduction on the order of 12.92% was achieved.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-330"
  },
  "karray98_icslp": {
   "authors": [
    [
     "Lamia",
     "Karray"
    ],
    [
     "Jean",
     "Monne"
    ]
   ],
   "title": "Robust speech/non-speech detection in adverse conditions based on noise and speech statistics",
   "original": "i98_0430",
   "page_count": 4,
   "order": 331,
   "p1": "paper 0430",
   "pn": "",
   "abstract": [
    "Recognition performance decreases when recognition systems are used over the telephone network, especially wireless network and noisy environments. It appears that non efficient speech/non-speech detection is a very important source of this degradation. Therefore, speech detector robustness to noise is a challenging problem to be examined, in order to improve recognition performance for the very noisy communications. Speech collected in GSM environment gives an example of such very noisy speech to be recognized. Several studies were conducted aiming to improve the robustness of speech/non-speech detection used for speech recognition in adverse conditions. This paper introduces a robust word boundary detection algorithm reliable in the very noisy cellular network environment. The algorithm is based on the statistics of noise and speech in the observed signal. In order to decide on the binary hypotheses of noise only versus speech plus noise, we use a likelihood ratio criterion.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-331"
  },
  "song98_icslp": {
   "authors": [
    [
     "Myung Gyu",
     "Song"
    ],
    [
     "Hoi In",
     "Jung"
    ],
    [
     "Kab-Jong",
     "Shim"
    ],
    [
     "Hyung Soon",
     "Kim"
    ]
   ],
   "title": "Speech recognition in car noise environments using multiple models according to noise masking levels",
   "original": "i98_1065",
   "page_count": 4,
   "order": 332,
   "p1": "paper 1065",
   "pn": "",
   "abstract": [
    "In speech recognition for real-world applications, the performance degradation due to the mismatch introduced between training and testing environments should be overcome. In this paper, to reduce this mismatch, we provide a hybrid method of spectral subtraction and residual noise masking. We also employ multiple model approach to obtain improved robustness over various noise environments. In this approach, multiple model sets are made according to several noise masking levels and then a model set appropriate for the estimated noise level is selected automatically in recognition phase. According to speaker independent isolated word recognition experiments in car noise environments, the proposed method using model sets with only two masking levels reduces average word error rate by 60% in comparison with spectral subtraction method.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-332"
  },
  "linhard98_icslp": {
   "authors": [
    [
     "Klaus",
     "Linhard"
    ],
    [
     "Tim",
     "Haulick"
    ]
   ],
   "title": "Spectral noise subtraction with recursive gain curves",
   "original": "i98_0109",
   "page_count": 4,
   "order": 333,
   "p1": "paper 0109",
   "pn": "",
   "abstract": [
    "Spectral noise subtraction has the drawback of generating residual noise with musical character, the so-called musical noise. We propose a simple modification of the filter coefficients calculation in form of a recursion to the previous coefficient. This recursion results in a switching mechanism of the spectral subtraction gain, in the way that speech pauses are processed with a nearly constant very low gain and speech components are processed with a nearly constant high gain. Because of the switching mechanism the generation of musical noise is almost completely avoided. The well known approach of Ephraim and Malah has a similar mechanism, but the new recursive scheme is much easier for implementation yet yielding comparable or sometimes better performance than the other approach.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-333"
  },
  "pan98_icslp": {
   "authors": [
    [
     "Shengxi",
     "Pan"
    ],
    [
     "Jia",
     "Liu"
    ],
    [
     "Jintao",
     "Jiang"
    ],
    [
     "Zuoying",
     "Wang"
    ],
    [
     "Dajin",
     "Lu"
    ]
   ],
   "title": "A novel robust speech recognition algorithm based on multi-models and integrated decision method",
   "original": "i98_0413",
   "page_count": 4,
   "order": 334,
   "p1": "paper 0413",
   "pn": "",
   "abstract": [
    "In this paper, a new robust speech recognition algorithm of multi-models and integrated decision(MMID) is proposed. A parallel MMID(PMMID) algorithm is developed. By using this new algorithm the advantages of different models can be integrated into one system. This algorithm uses different acoustic models at the same time based on DDBHMM (duration distribution based Hidden Markov Model). These different models include the channel-mismatch-correct (CMC) model, more-alternative-pronunciation model, tone and non-tone models of Chinese Mandarin speech, voice activity detection(VAD) model and state-skip model. The speech recognition accuracy of the multi-model system is better than that of single-model system in the adverse environments. The experimental results show that the error rate of the recognition system is 2.9% and reduced by 81% compared with the baseline system of the single-model.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-334"
  },
  "macho98_icslp": {
   "authors": [
    [
     "Dusan",
     "Macho"
    ],
    [
     "Climent",
     "Nadeu"
    ]
   ],
   "title": "On the interaction between time and frequency filtering of speech parameters for robust speech recognition",
   "original": "i98_1137",
   "page_count": 4,
   "order": 335,
   "p1": "paper 1137",
   "pn": "",
   "abstract": [
    "One of the great today's challenges in speech recognition is to ensure the robustness of the used speech representation. Usually, the recognition rate is strongly reduced when the speech is corrupted, e.g. by convolutional or additive noise, and the speech features are not designed to be robust. In this paper we study the effect of additive noise on the logarithmic filter-bank energy representation. We use time and frequency filtering techniques to emphasize the discriminative information and to reduce the mismatch between noisy and clean speech representation. A 2-D spectral representation is introduced to see the regions most affected by noise in the 2-D quefrency-modulation frequency domain and to help to design the frequency and time filter shapes. Experiments with one and two dynamic feature sets show the usefulness of the combination of time and frequency filtering for both, white and low-pass noise speech recognition. At the end the power time and frequency filtering technique is presented.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-335"
  },
  "raj98_icslp": {
   "authors": [
    [
     "Bhiksha",
     "Raj"
    ],
    [
     "Rita",
     "Singh"
    ],
    [
     "Richard M.",
     "Stern"
    ]
   ],
   "title": "Inference of missing spectrographic features for robust speech recognition",
   "original": "i98_1152",
   "page_count": 4,
   "order": 336,
   "p1": "paper 1152",
   "pn": "",
   "abstract": [
    "Two types of algorithms are introduced that recover missing time-frequency regions of log-spectral representations of speech. These compensation algorithms modify the incoming feature vector without any changes to the speech recognition system, in contrast to previously-described approaches. The first approach clusters the log-spectral vectors representing clean speech. Missing data are recovered by estimating the spectral cluster in each analysis frame on the basis of the feature values that are present. The second approach uses MAP procedures to estimate the values of missing data elements based on their correlation with the features that are present. Greatest recognition accuracy was obtained using the correlation-based approach, presumably because of its ability to exploit the temporal as well as spectral structure of speech. The recognition accuracy provided by these algorithms approaches but does not exceed that obtained by traditional marginalization. Nevertheless, it is believed that these algorithms provide greater computational efficiency and enable greater flexibility in recognition system structure.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-336"
  },
  "schless98_icslp": {
   "authors": [
    [
     "Volker",
     "Schless"
    ],
    [
     "Fritz",
     "Class"
    ]
   ],
   "title": "SNR-dependent flooring and noise overestimation for joint application of spectral subtraction and model combination",
   "original": "i98_0138",
   "page_count": 4,
   "order": 337,
   "p1": "paper 0138",
   "pn": "",
   "abstract": [
    "We present an approach to joint application of spectral subtraction (SPS) and model combination (PMC) for speech recognition in noisy environments. Contrary to previous solutions distortion introduced by SPS is not modeled in PMC. Instead we ensure compatibility of the two methods by adapting parameters of SPS (spectral floor and overestimation factor) according to the present signal-to-noise-ratio (SNR). Parameter setting should be done to subtract a maximum of noise while minimizing distortion. Experiments suggest that for each noise level different parameter sets yield optimal performance. Setting the parameters adaptively according to the noise level leads to undegraded results at high SNR while in low SNR regions the benefits of the noise reduction process are significant. This scheme leaves the model combination process unchanged which simplifies parameter estimation and reduces computation time. Experiments show significant improvements when using PMC with modified SPS instead of standard SPS.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-337"
  },
  "shen98_icslp": {
   "authors": [
    [
     "Jia-Lin",
     "Shen"
    ],
    [
     "Jeih-Weih",
     "Hung"
    ],
    [
     "Lin-Shan",
     "Lee"
    ]
   ],
   "title": "Improved robust speech recognition considering signal correlation approximated by taylor series",
   "original": "i98_0442",
   "page_count": 4,
   "order": 338,
   "p1": "paper 0442",
   "pn": "",
   "abstract": [
    "In this paper, an improved mismatch function by considering signal correlation between speech and noise is proposed to better estimate the noisy speech HMM's. A linearized model based on Taylor series expansion approach is used to approximate the proposed mismatch function. The parameters of the noisy speech HMM's can be estimated more precisely by combining the parameters of the clean speech and noise HMM's in the log-spectral domain or cepstral domain. Experimental results show that improved robustness for speech recognition in the presence of white noise as well as colored noise can be obtained.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-338"
  },
  "shin98_icslp": {
   "authors": [
    [
     "Won-Ho",
     "Shin"
    ],
    [
     "Weon-Goo",
     "Kim"
    ],
    [
     "Chungyong",
     "Lee"
    ],
    [
     "Il-Whan",
     "Cha"
    ]
   ],
   "title": "Speech recognition in noisy environment using weighted projection-based likelihood measure",
   "original": "i98_0434",
   "page_count": 4,
   "order": 339,
   "p1": "paper 0434",
   "pn": "",
   "abstract": [
    "This paper investigates a projection-based likelihood measure that improves speech recognition performance in noisy environment. The projection-based likelihood measure is modified to give the weighting and projection effect and to reduce computational complexity. It is evaluated in sub-model based word recognition using semi-continuous hidden Markov model with speaker independent mode. Experimental results using proposed measure are reported for several performance factors: additive noise and noisy channel environment, various noise signals, and combination with other compensation method. In various noisy environments, performance improvements were achieved compared to the previously existing methods.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-339"
  },
  "takiguchi98_icslp": {
   "authors": [
    [
     "Tetsuya",
     "Takiguchi"
    ],
    [
     "Satoshi",
     "Nakamura"
    ],
    [
     "Kiyohiro",
     "Shikano"
    ],
    [
     "Masatoshi",
     "Morishima"
    ],
    [
     "Toshihiro",
     "Isobe"
    ]
   ],
   "title": "Evaluation of model adaptation by HMM decomposition on telephone speech recognition",
   "original": "i98_0698",
   "page_count": 4,
   "order": 340,
   "p1": "paper 0698",
   "pn": "",
   "abstract": [
    "In this paper, we evaluate performance of model adaptation by the previously proposed HMM decomposition method on telephone speech recognition. The HMM decomposition method separates a composed HMM into a known phoneme HMM and an unknown noise and channel HMM by maximum likelihood (ML) estimation of the HMM parameters. A transfer function (telephone channel) HMM is estimated using adaptation speech data by applying the HMM decomposition twice in the linear spectral domain for noise and in the cepstral domain for channel. The telephone speech data for evaluation are recorded through 10 kinds of ordinary analog telephone handsets and cordless telephone handsets. The test results show that the average phrase accuracy with the clean speech HMMs is 60.9% for the ordinary analog telephone handsets, and 19.6% for the cordless telephone handsets. By the HMM decomposition method, the average phrase accuracy is improved to 78.1% for the ordinary analog telephone handsets, and 50.5% for the cordless telephone handsets.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-340"
  },
  "tolba98_icslp": {
   "authors": [
    [
     "Hesham",
     "Tolba"
    ],
    [
     "Douglas",
     "O'Shaughnessy"
    ]
   ],
   "title": "Comparative experiments to evaluate a voiced-unvoiced-based pre-processing approach to robust automatic speech recognition in low-SNR environments",
   "original": "i98_0341",
   "page_count": 4,
   "order": 341,
   "p1": "paper 0341",
   "pn": "",
   "abstract": [
    "This paper presents an evaluation of a robust Voiced-Unvoiced-based large-vocabulary Continuous-Speech Recognition (CSR) system in the presence of highly interfering noise. Comparative experiments have indicated that the inclusion of an accurate Voiced-Unvoiced (V-U) classifier in our design of a CSR system improves the performance of such a recognizer, for speech contaminated by both additive Gaussian and uniform noises. Our results show that the V-U-based CSR system outperforms the CMS-based and the RASTA-PLP-based CSR systems in such environments for a wide range of SNRs.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-341"
  },
  "unoki98_icslp": {
   "authors": [
    [
     "Masashi",
     "Unoki"
    ],
    [
     "Masato",
     "Akagi"
    ]
   ],
   "title": "Signal extraction from noisy signal based on auditory scene analysis",
   "original": "i98_1041",
   "page_count": 4,
   "order": 342,
   "p1": "paper 1041",
   "pn": "",
   "abstract": [
    "This paper proposes a method of extracting the desired signal from a noisy signal. This method solves the problem of segregating two acoustic sources by using constraints related to the four regularities proposed by Bregman and by making two improvements to our previously proposed method. One is to incorporate a method of estimating the fundamental frequency using the Comb filtering on the filterbank. The other is to reconsider the constraints on the separation block, which constrain the instantaneous amplitude, input phase, and fundamental frequency of the desired signal. Simulations performed to segregate a vowel from a noisy vowel and to compare the results of using all or only some constraints showed that our improved method can segregate real speech precisely using all the constraints related to the four regularities and that the absence some constraints reduces the accuracy.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-342"
  },
  "usagawa98_icslp": {
   "authors": [
    [
     "Tsuyoshi",
     "Usagawa"
    ],
    [
     "Kenji",
     "Sakai"
    ],
    [
     "Masanao",
     "Ebata"
    ]
   ],
   "title": "Frequency domain binaural model as the front end of speech recognition system",
   "original": "i98_0190",
   "page_count": 7,
   "order": 343,
   "p1": "paper 0190",
   "pn": "",
   "abstract": [
    "In this paper, the frequency domain binaural model is introduced. The proposed model is the revised one of the former time domain model which calculates the interaural crosscorrelation. The new model requires the less computational load and has the comparable performance. It is based on the FFT analysis and uses the cross-power spectrum to obtained interaural phase difference. The performance of models is examined not only in the isolated word speech recognition task and but also in the speech enhancement task. As the results of experiment, the improvement of robustness in speech recognition task corresponds to about 15-20dB when the surrounding noise is white noise. That is a few decibell better than one obtained by the time domain model. However, when the surrounding noise is speech, the improvement decreases to 10-15dB. In addition, the proposed model can reproduce the signal component from the specified direction as the binaural signal.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-343"
  },
  "yu98_icslp": {
   "authors": [
    [
     "An-Tzyh",
     "Yu"
    ],
    [
     "Hsiao-Chuan",
     "Wang"
    ]
   ],
   "title": "A study on the recognition of low bit-rate encoded speech",
   "original": "i98_0038",
   "page_count": 4,
   "order": 344,
   "p1": "paper 0038",
   "pn": "",
   "abstract": [
    "The recognition of encoded speech provides the possible applications of speech recognition in low bit-rate communication systems. This kind of applications will become necessary in the Internet and digital mobile phones. In this paper, the encoded parameters of speech signal in various speech coding systems are evaluated according to a designated speech recognition task. The HMM with mixture of continuous Gaussian densities is the framework of the speech recognition system.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-344"
  },
  "hwang98_icslp": {
   "authors": [
    [
     "Tai-Hwei",
     "Hwang"
    ],
    [
     "Hsiao-Chuan",
     "Wang"
    ]
   ],
   "title": "Weighted parallel model combination for noisy speech recognition",
   "original": "i98_0039",
   "page_count": 4,
   "order": 345,
   "p1": "paper 0039",
   "pn": "",
   "abstract": [
    "ABSTRACT This paper proposes a modified parameter mapping scheme for parallel model combination (PMC) method. The modification aims to improve the discriminative capabilities of the compensated models. It is achieved by the rearrangement of the distributions of state models in order to emphasize the contribution of the mean in the following process. Both distributions of speech model and noise model are shaped in cepstral domain through a covariance contracting procedure. After the compensation steps, an expanding procedure of the adapted covariance is necessary to release the emphasis. Using this process, the discriminative capability is increased so that the recognition accuracy is improved. In this paper, the recognition of Chinese names demonstrates the improvement to the original PMC method, especially when SNR is low.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-345"
  },
  "woo98_icslp": {
   "authors": [
    [
     "Daniel",
     "Woo"
    ]
   ],
   "title": "Favourable and unfavourable short duration segments of speech in noise",
   "original": "i98_0700",
   "page_count": 4,
   "order": 346,
   "p1": "paper 0700",
   "pn": "",
   "abstract": [
    "Human perceptual experiments are described that present listeners with segmented stop consonant speech stimuli in noise. The selection of short duration speech segments is based on a local measure of the signal-to-noise ratio calculated over 1ms windows. The aim is to create stimuli with known fluctuations occurring between a speech and noise sample to assess whether the presence of short duration \"gaps\" in the noise produce favourable and unfavourable signal regions that influence identification. Perceptual results are reported that suggest human listeners make better use of signals that comprise only of positive, local signal-to-noise ratio segments. Such regions are assumed to be more favourable for stimuli identification. Presentation of stimuli containing only negative signal-to-noise ratio regions does not appear to contribute as much. A model that is based on the accumulation of short duration spectral segments is presented that produces a similar set of identification functions for the same test stimuli.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-346"
  },
  "cosi98_icslp": {
   "authors": [
    [
     "Piero",
     "Cosi"
    ],
    [
     "Stefano",
     "Pasquin"
    ],
    [
     "Enrico",
     "Zovato"
    ]
   ],
   "title": "Auditory modeling techniques for robust pitch extraction and noise reduction",
   "original": "i98_1053",
   "page_count": 4,
   "order": 347,
   "p1": "paper 1053",
   "pn": "",
   "abstract": [
    "A novel method for robust pitch extraction, based on the correlogram output of the Lyon's cochlear model is described. The value of the autocorrelation lag for which the signals of the cochlear channels have the same periodicity can be computed thus tracking how the pitch of the input signal varies in the time domain. In the case of a stationary noise, a sort of 'spectral-subtraction' technique, built in the correlogram domain named 'correlogram subtraction', is applied to enhance the signal before computing its fundamental frequency. Finally, a correction algorithm based on an 'island driven' strategy, working on particular zones of the signal with stable pitch values, is used to refine the pitch estimate. This method of pitch extraction is extremely reliable, even in the case of a signal to noise ratio of 0dB. The same subtraction technique, with some new specific filter-bank energy-based modifications, is considered to re-synthesize, by an inversion strategy, a clean version of an input noisy signal. The quality of the re-synthesized signal is quite promising, leading us to try, in the future, to use this technique as a new signal enhancement scheme.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-347"
  },
  "ambikairajah98_icslp": {
   "authors": [
    [
     "Eliathamby",
     "Ambikairajah"
    ],
    [
     "Graham",
     "Tattersall"
    ],
    [
     "Andrew",
     "Davis"
    ]
   ],
   "title": "Wavelet transform-based speech enhancement",
   "original": "i98_0140",
   "page_count": 4,
   "order": 348,
   "p1": "paper 0140",
   "pn": "",
   "abstract": [
    "This paper describes a speech enhancement system using a novel combination of a Fast Wavelet Transform structure, together with \"Wiener filtering\" in the wavelet domain. The specific application of interest is the enhancement of speech when a cellular phone is used within a moving vehicle. Subjective tests carried out using speech with additive vehicle noise at a signal-to-noise ratio of 10 dB indicate that the Wavelet transform-based Wiener filtering approach works well. In particular, the technique was compared to several other common enhancement methods such as thresholding applied in the wavelet domain, FFT-based Wiener filtering, and spectral subtraction, and was found to outperform these other techniques.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-348"
  },
  "logan98_icslp": {
   "authors": [
    [
     "Beth",
     "Logan"
    ],
    [
     "Tony",
     "Robinson"
    ]
   ],
   "title": "A practical perceptual frequency autoregressive HMM enhancement system",
   "original": "i98_1083",
   "page_count": 4,
   "order": 349,
   "p1": "paper 1083",
   "pn": "",
   "abstract": [
    "We have previously developed an adaptive speech enhancement scheme. This models speech and noise using perceptual frequency or `warped' autoregressive HMMs (AR-HMMs) and estimates the clean speech and noise parameters within this framework. In this paper, we investigate the use of our system as a front-end to a clean MFCC recognition system. We make two main modifications to our scheme. First, we use MMSE spectral rather than time domain estimators for enhancement. Second, for computational reasons, we form estimators using non-warped AR-HMMs. To avoid mismatch when converting between warped and non-warped models, we use parallel models. Results are presented for small and medium vocabulary tasks. On the simple task, we approach the performance of a matched system when language model information is included. On the second task, we are unable to incorporate a language model due to modelling deficiencies in AR-HMMs. However, we still demonstrate substantial improvements over baseline results.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-349"
  },
  "hansen98_icslp": {
   "authors": [
    [
     "John H. L.",
     "Hansen"
    ],
    [
     "Bryan L.",
     "Pellom"
    ]
   ],
   "title": "An effective quality evaluation protocol for speech enhancement algorithms",
   "original": "i98_0917",
   "page_count": 4,
   "order": 350,
   "p1": "paper 0917",
   "pn": "",
   "abstract": [
    "Much progress has been made in speech enhancement algorithm formulation in recent years. However, while researchers in the speech coding and recognition communities have standard criteria for algorithm performance comparison, similar standards do not exist for researchers in speech enhancement. This paper discusses the necessary ingredients for an effective speech enhancement evaluation. We propose that researchers use the evaluation core test set of TIMIT (192 sentences), with a set of noise files, and a combination of objective measures and subjective testing for broad and fine phone-level quality assessment. Evaluation results include overall objective speech quality measure scores, measure histograms, and phoneme class and individual phone scores. The reported results are meant to illustrate specific ways of detailing quality assessment for an enhancement algorithm.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-350"
  },
  "park98_icslp": {
   "authors": [
    [
     "Jin-Nam",
     "Park"
    ],
    [
     "Tsuyoshi",
     "Usagawa"
    ],
    [
     "Masanao",
     "Ebata"
    ]
   ],
   "title": "An adaptive beamforming microphone array system using a blind deconvolution",
   "original": "i98_0418",
   "page_count": 4,
   "order": 351,
   "p1": "paper 0418",
   "pn": "",
   "abstract": [
    "This paper proposes an adaptive microphone array using blind deconvolution. The method realizes an signal enhancement based on the combination with beamforming using blind deconvolution, synchronized summation and DSA method. The proposed method improves a performance of estimation by the iterative operation of blind deconvolution using a cost-function base on the coherency function.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-351"
  },
  "singh98_icslp": {
   "authors": [
    [
     "Latchman",
     "Singh"
    ],
    [
     "Sridha",
     "Sridharan"
    ]
   ],
   "title": "Speech enhancement using critical band spectral subtraction",
   "original": "i98_1134",
   "page_count": 4,
   "order": 352,
   "p1": "paper 1134",
   "pn": "",
   "abstract": [
    "This paper proposes a new enhancement technique for the enhancement of broadband noise corrupted speech. The technique exploits the human auditory systems inability to distinguish between individual frequency components within critical frequency bands. Spectral subtraction is used and the spectrum is considered as critical frequency bands rather than individual frequency components. The proposed technique is compared with the existing spectral subtraction technique, using both subjective and objective speech assessment measures. Results are quoted and indicate that there is a significant increase in intelligibility and quality.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-352"
  },
  "badin98_icslp": {
   "authors": [
    [
     "Pierre",
     "Badin"
    ],
    [
     "G√©rard",
     "Bailly"
    ],
    [
     "Monica",
     "Raybaudi"
    ],
    [
     "Christoph",
     "Segebarth"
    ]
   ],
   "title": "A three-dimensional linear articulatory model based on MRI data",
   "original": "i98_0014",
   "page_count": 4,
   "order": 353,
   "p1": "paper 0014",
   "pn": "",
   "abstract": [
    "Based on a set of 3D vocal tract images obtained by MRI, a 3D linear articulatory model has been built using guided Principal Component Analysis. It constitutes an extension to the lateral dimension of the mid-sagittal model previously developed from a radiofilm recorded on the same subject. The parameters of the 2D model have been found to be good predictors of the 3D shapes, for most configurations. A first evaluation of the model in terms of area functions and formants is presented.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-353"
  },
  "perrier98_icslp": {
   "authors": [
    [
     "Pascal",
     "Perrier"
    ],
    [
     "Yohan",
     "Payan"
    ],
    [
     "Joseph",
     "Perkell"
    ],
    [
     "Fr√©d√©ric",
     "Jolly"
    ],
    [
     "Majid",
     "Zandipour"
    ],
    [
     "Melanie",
     "Matthies"
    ]
   ],
   "title": "On loops and articulatory biomechanics",
   "original": "i98_0112",
   "page_count": 4,
   "order": 354,
   "p1": "paper 0112",
   "pn": "",
   "abstract": [
    "This study explores the following hypothesis: forward looping movements of the tongue that are observed in VCV sequences are due partly to the anatomical arrangement of the tongue muscles and how they are used to produce a velar closure. The study uses an anatomically based two-dimensional biomechanical tongue model. Tissue elastic properties are accounted for in finite-element modeling, and movement is controlled by constant-rate control parameter shifts. Tongue raising and lowering movements are produced by the model with the combined actions of the genioglossus, styloglossus and hyoglossus. Simulations of V1CV2 movements were made, where C is a velar consonant and V is [a], [i] or [u]. The resulting trajectories describe movements that begin to loop forward before consonant closure. Examination of subject data show similar looping movements. These observations support the idea that the biomechanical properties of the tongue could be the main factor responsible for the loops.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-354"
  },
  "demolin98_icslp": {
   "authors": [
    [
     "Didier",
     "Demolin"
    ],
    [
     "V√©ronique",
     "Lecuit"
    ],
    [
     "Thierry",
     "Metens"
    ],
    [
     "Bruno",
     "Nazarian"
    ],
    [
     "Alain",
     "Soquet"
    ]
   ],
   "title": "Magnetic resonance measurements of the velum port opening",
   "original": "i98_0532",
   "page_count": 7,
   "order": 355,
   "p1": "paper 0532",
   "pn": "",
   "abstract": [
    "M.R.I. techniques have been used to describe velum opening of French vowels. Data based on 18 joined axial slices of 4 mm thickness were recorded with two subjects. Differences in velum opening are calculated from areas measured in the tract between the lowered velum and the back pharynx wall. A 3 D modelling of this tract is also proposed.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-355"
  },
  "matsumura98_icslp": {
   "authors": [
    [
     "Masafumi",
     "Matsumura"
    ],
    [
     "Takuya",
     "Niikawa"
    ],
    [
     "Takao",
     "Tanabe"
    ],
    [
     "Takashi",
     "Tachimura"
    ],
    [
     "Takeshi",
     "Wada"
    ]
   ],
   "title": "Cantilever-type force-sensor-mounted palatal plate for measuring palatolingual contact stress and pattern during speech phonation",
   "original": "i98_0507",
   "page_count": 4,
   "order": 356,
   "p1": "paper 0507",
   "pn": "",
   "abstract": [
    "A 15-cantilever-type force-sensor unit is presented for the measurement of palatolingual contact stress and pattern during palatal consonant phonation. The force sensor unit is composed of a strain gauge and a cantilever, and is embedded in a thin palatal plate attached to the human hard palate. It is 3 mm wide, by 5 mm long, and 1.3 mm thick. The output of the force sensor unit at the low stress range of 0-64 kPa (0-5 gw) is proportional to the stress applied to the force sensing unit, with nearly no hysteresis. Measurement error of the force sensor is less than 1.7%. Error by mechanical interference among cantilever-type force sensors is less than 0.2%. The presented 15-cantilever-type force-sensor-mounted palatal plate allows for ready observation of the dynamic aspect of the palatolingual contact stress and patterns during the phonation of consonants.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-356"
  },
  "kaburagi98_icslp": {
   "authors": [
    [
     "Tokihiko",
     "Kaburagi"
    ],
    [
     "Masaaki",
     "Honda"
    ]
   ],
   "title": "Determination of the vocal tract spectrum from the articulatory movements based on the search of an articulatory-acoustic database",
   "original": "i98_0425",
   "page_count": 4,
   "order": 357,
   "p1": "paper 0425",
   "pn": "",
   "abstract": [
    "This paper presents a method for determining the vocal-tract spectrum from the positions of fixed points on the articulatory organs. The method is based on the search of a database comprised of pairs of articulatory and acoustic data representing the direct relationship between the articulator position and vocal-tract spectrum. To compile the database, the electro-magnetic articulograph (EMA) system is used to measure the movements of the jaw, lips, tongue, velum, and larynx simultaneously with speech waveforms. The spectrum estimation is accomplished by selecting database samples neighboring the input articulator position and interpolating the selected samples. In addition, phoneme categorization of the input position is performed to restrict the search area of the database to portions of the same phoneme category. Experiments show that the mean estimation error is 2.24 dB and the quality of speech synthesized from the estimated spectrum can be improved by using the phoneme categorization.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-357"
  },
  "honda98_icslp": {
   "authors": [
    [
     "Kiyoshi",
     "Honda"
    ],
    [
     "Mark K.",
     "Tiede"
    ]
   ],
   "title": "An MRI study on the relationship between oral cavity shape and larynx position",
   "original": "i98_0686",
   "page_count": 4,
   "order": 358,
   "p1": "paper 0686",
   "pn": "",
   "abstract": [
    "Individual variation of larynx position reflects human morphological differences and thus contributes to generating biological information in speech sounds. This study examines the factors of orofacial morphology that co-vary with larynx position based on MRI data collected for 12 Japanese and 12 English speakers. The materials are midsagittal craniofacial images, and the method is based on the measurement of angles and indices. Among all the measures examined, the aspect ratio of the oral cavity in the lateral view showed the highest correlation (r=0.87) with larynx height index (ratio of arytenoid - palatal plane distance and anterior nasal spine - nasopharyngeal wall distance), and a facial angle (angle of maxillary incisor - nasion - nasopharyngeal wall) showed the second highest correlation (r=0.66) with larynx height index. The result indicates that larynx position co-varies with oral cavity shape, being higher when oral cavity shape is flatter and more prognathic.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-358"
  },
  "clermont98_icslp": {
   "authors": [
    [
     "Frantz",
     "Clermont"
    ],
    [
     "Parham",
     "Mokhtari"
    ]
   ],
   "title": "Acoustic-articulatory evaluation of the upper vowel-formant region and its presumed speaker-specific potency",
   "original": "i98_0087",
   "page_count": 4,
   "order": 359,
   "p1": "paper 0087",
   "pn": "",
   "abstract": [
    "We present some evidence indicating that phonetic distinctiveness and speaker individuality, are indeed manifested in vowels' vocal-tract (VT) sha pes estimated from the lower and upper formants, respectively. The methodology developed to demonstrate this dichotomy, implicates Schroeder's (1967) acoustic-articulatory model which can be coerced to yield area-function approximations to VT-shapes of differing formant components. Using ten steady-state vowels recorded in /hVd/-context, five times at random, by four adult-male speakers of Australian English, VT-shape variability was then measured on an intra- and an inter-speaker basis. Gross shapes estimated from the lower formants, caused the largest spread amongst the vowels of individual speakers. By contrast, more detailed sha pes estimated from certain higher formants of front and back vowels, caused the largest spread amongst speakers. These results contribute a quasi-articulatory substantiation of a long-standing view on the speaker-specific potency of the upper vowel-formant region, together with some useful implications for speech and speaker recognition.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-359"
  },
  "hoole98_icslp": {
   "authors": [
    [
     "Philip",
     "Hoole"
    ],
    [
     "Christian",
     "Kroos"
    ]
   ],
   "title": "Control of larynx height in vowel production",
   "original": "i98_1097",
   "page_count": 4,
   "order": 360,
   "p1": "paper 1097",
   "pn": "",
   "abstract": [
    "Digital video filming of the thyroid prominence was used to measure larynx height in German vowels, with focus on contrasts involving front unrounded, front rounded and back rounded vowels. The study aimed to provide a foundation for interpreting the acoustic consequences of articulatory maneuvres not only at the larynx but also elsewhere in the vocal tract. Results showed the expected pattern of lower larynx position for the rounded vowels. However no clear preference emerged for the same, more, or less larynx lowering on front rounded versus back rounded vowels. Coarticulatory effects of the flanking consonants were weak. The most striking result was that the magnitude of the differences between vowels varied substantially over speakers. This reinforces the contention that interpretation of vertical laryngeal gestures must be embedded in speaker-specific analysis of downstream articulatory maneuvres. Work in this direction is currently in progress.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-360"
  },
  "alku98_icslp": {
   "authors": [
    [
     "Paavo",
     "Alku"
    ],
    [
     "Juha",
     "Vintturi"
    ],
    [
     "Erkki",
     "Vilkman"
    ]
   ],
   "title": "Analyzing the effect of secondary excitations of the vocal tract on vocal intensity in different loudness conditions",
   "original": "i98_0067",
   "page_count": 4,
   "order": 361,
   "p1": "paper 0067",
   "pn": "",
   "abstract": [
    "For voiced speech the main excitation of the vocal tract occurs at the end of the glottal closing phase when the rate of change of the flow reaches its absolute maximum. This study presents a straightforward method that yields a numerical value to characterize the effect of the main excitation on vocal intensity. The method, Energy Ratio by Modified Excitation (ERME), takes advantage of the glottal flow and the model of the vocal tract transfer function given by inverse filtering and it synthesizes two signals based on the source-filter theory. The first synthesized sound is produced using the glottal flow waveform given by inverse filtering per se. The second signal is synthesized by removing the main excitation from the differentiated glottal flow. ERME is defined as the ratio between the energy of the first synthesized signal and the energy of the second one. It is shown that when the loudness of speech increases, the value of ERME first rises but in the case of loud voices it starts to decrease. This behavior of ERME shows that effects of secondary excitations of the vocal tract that occur during glottal opening become important in the production of loud voices.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-361"
  },
  "ramsay98b_icslp": {
   "authors": [
    [
     "Gordon",
     "Ramsay"
    ]
   ],
   "title": "An analysis of modal coupling effects during the glottal cycle: formant synthesizers from time-domain finite-difference simulations",
   "original": "i98_0670",
   "page_count": 4,
   "order": 362,
   "p1": "paper 0670",
   "pn": "",
   "abstract": [
    "Speech is typically modelled using time-domain or frequency-domain simulations of the acoustic field in the vocal tract. Using a biorthogonal modal decomposition, it is shown that time-domain finite-difference simulations can be transformed algebraically into equivalent formant synthesizers, the parameters of which vary in time and are calculated directly from the laws of physics. Examining the structure of the equivalent formant synthesizer, it is observed that formant excitation is largely due to internal modal coupling effects, induced by rapid perturbation of the acoustic eigenmodes caused by vibration of the glottis, and does not rely precisely on external sources provided by boundary conditions. This leads to a novel interpretation and justification of traditional models of the glottal source.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-362"
  },
  "esling98_icslp": {
   "authors": [
    [
     "John H.",
     "Esling"
    ]
   ],
   "title": "Laryngoscopic analysis of pharyngeal articulations and larynx-height voice quality settings",
   "original": "i98_0617",
   "page_count": 4,
   "order": 363,
   "p1": "paper 0617",
   "pn": "",
   "abstract": [
    "Using fibreoptic laryngoscopy to observe pharyngeal articulations, the aryepiglottic sphincter mechanism is shown to be responsible for the production of speech sounds in the phonetic category \"pharyngeal.\" Major differences in auditory/acoustic quality are also produced when the larynx as a whole is raised or lowered during the production of pharyngeals. The voiceless pharyngeal fricative and voiced pharyngeal approximant are the result of increased sphincteric constriction of the laryngeal \"tube\" in a continuum that begins with normal glottal stop and ventricular fold closure. A pharyngeal stop is produced when the aryepiglottic sphincter mechanism achieves complete closure, and trilling accompanying friction is evident at the pharyngeal place of articulation in both voiceless and voiced modes. It is suggested that all five sounds share a common, pharyngeal place of articulation, but differ in manner of articulation. Raised larynx is the default setting for these articulations, but they may be produced with lowered larynx.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-363"
  },
  "matsuzaki98_icslp": {
   "authors": [
    [
     "Hiroki",
     "Matsuzaki"
    ],
    [
     "Kunitoshi",
     "Motoki"
    ],
    [
     "Nobuhiro",
     "Miki"
    ]
   ],
   "title": "Effects of shapes of radiational aperture on radiation characteristics",
   "original": "i98_0656",
   "page_count": 4,
   "order": 364,
   "p1": "paper 0656",
   "pn": "",
   "abstract": [
    "The acoustic characteristics of acoustic tubes with protrusions at the radiation end are computed by FEM simulation. In the first experiment, two different shapes of the protrusions, a symmetrical and an asymmetrical shape with respect to the vertical, are investigated. Frequency characteristics of the radiation impedance are computed from simulation results. The simulation results show that the results of FEM simulation are in good agreement with our measurement results. The proposed 3-D radiational model is useful for analysis of the acoustic characteristics of human speech. In the 2nd experiment, the protrusion is attached to our 3-D vocal tract model. The vocal tract shape corresponds to the Japanese vowel /a/. The cross sections of the tubes are eliptic in shape. The simulation results show that the vocal tract transfer function of the FEM results is different from our previous FEM results and 1-D analytical solution.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-364"
  },
  "harrington98_icslp": {
   "authors": [
    [
     "Jonathan",
     "Harrington"
    ],
    [
     "Mary E.",
     "Beckman"
    ],
    [
     "Janet",
     "Fletcher"
    ],
    [
     "Sallyanne",
     "Palethorpe"
    ]
   ],
   "title": "An electropalatographic, kinematic, and acoustic analysis of supralaryngeal correlates of word-level prominence contrasts in English",
   "original": "i98_0646",
   "page_count": 4,
   "order": 365,
   "p1": "paper 0646",
   "pn": "",
   "abstract": [
    "This study examines the phonetic characteristics of primary versus secondary stress on the first syllables of the surname 'Wheateron' and related adjective 'Wheateresque' in post-nuclear, deaccented position in a dialogue produced 40 times by 3 Australian English talkers. Synchronised acoustic, electromagnetometer, and electropalatographic recordings were analysed. One subject had a higher F0 in the primary stressed syllable. The other two had a longer acoustic duration for the syllable's voiced portion, corresponding to a longer lip closing movement. One of these two also had a larger and faster lip opening movement into the vowel. Taken together, the results show that primary versus secondary lexical stress may be differentiated even when accent contrasts are neutralised, although the differences are inconsistent across talkers and small by comparison to those that have been shown to characterise the accented-unaccented contrast.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-365"
  },
  "tabain98_icslp": {
   "authors": [
    [
     "Marija",
     "Tabain"
    ]
   ],
   "title": "Consistencies and inconsistencies between EPG and locus equation data on coarticulation",
   "original": "i98_0668",
   "page_count": 4,
   "order": 366,
   "p1": "paper 0668",
   "pn": "",
   "abstract": [
    "Following a previous study using locus equation (LE) and electropalatographic (EPG) data to examine coarticulation of voiced consonants and vowels in CV syllables [1], the present study examines voiceless stops and fricatives using the same analysis techniques. It is found that when LE data for stops is sampled at the stop burst, rather than at vowel onset, the correlation between LE data on coarticulation and EPG data on coarticulation is quite high. By contrast, results for the fricatives are quite poor. It is suggested that the LE is capable of capturing rather gross differences in coarticulatory resistance, such as that involving a tongue tip rather than a tongue body, but that it is not capable of capturing more subtle differences in coarticulation, such as those involving different coronal articulations. This explanation is supported by work in progress on Australian Aboriginal languages which have up to four coronal places of articulation [2].\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-366"
  },
  "bailly98_icslp": {
   "authors": [
    [
     "G√©rard",
     "Bailly"
    ],
    [
     "Pierre",
     "Badin"
    ],
    [
     "Anne",
     "Vilain"
    ]
   ],
   "title": "Synergy between jaw and lips/tongue movements : consequences in articulatory modelling",
   "original": "i98_0066",
   "page_count": 4,
   "order": 367,
   "p1": "paper 0066",
   "pn": "",
   "abstract": [
    "Linear component articulatory models are built using an iterative subtraction of linear predictors of the vocal tract geometry. In this paper we consider the contribution of jaw displacement to tongue and lips movements using sets of cineradiographic data from three different speakers. We show that linear prediction overestimates this contribution by capturing not only the intrinsic mechanical jaw-tongue coupling but also the synergetic control observed in the corpus. We then propose a subtraction of the jaw contribution which do not affect the performance of the model in terms of data prediction.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-367"
  },
  "hoole98b_icslp": {
   "authors": [
    [
     "Philip",
     "Hoole"
    ]
   ],
   "title": "Modelling tongue configuration in German vowel production",
   "original": "i98_1096",
   "page_count": 4,
   "order": 368,
   "p1": "paper 1096",
   "pn": "",
   "abstract": [
    "The PARAFAC method of factor analysis was used to investigate patterns of tongue shaping in a corpus of 15 German vowels spoken in 3 consonant contexts by 7 speakers at 2 speech rates, using data from electromagnetic articulography. A two-factor model was extracted, giving a succinct, speaker-independent characterization of the German vowel space and of some important coarticulatory effects on vowel articulation. Moreover, the factors appeared to have a plausible physiological substrate. The PARAFAC model places strong constraints on the form that speaker-specific effects can take, since speaker differences must be captured in a single multiplicative weight per speaker and factor. While these constraints appeared acceptable for modelling vocalic aspects of articulation, more consonantally-related aspects, such as coarticulatory behaviour of the tongue-tip, appeared much more difficult to capture in the PARAFAC framework.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-368"
  },
  "wrench98_icslp": {
   "authors": [
    [
     "Alan A.",
     "Wrench"
    ],
    [
     "Alan D.",
     "McIntosh"
    ],
    [
     "Colin",
     "Watson"
    ],
    [
     "William J.",
     "Hardcastle"
    ]
   ],
   "title": "Optopalatograph: real-time feedback of tongue movement in 3D",
   "original": "i98_1117",
   "page_count": 4,
   "order": 369,
   "p1": "paper 1117",
   "pn": "",
   "abstract": [
    "In this paper the latest prototype Optopalatograph (OPG) is described and its operation is demonstrated graphically and in comparison to theoretical predictions. The system is divided into three parts - the optopalate itself; a separate self contained unit composed of 16 switched infra-red light sources, associated control logic and 16 receivers; and a computer with A/D card running software to analyse and interpret graphically the sensor outputs. The current prototype measures distances of up to 20mm between all of the 16 pre-selected points on the hard palate and the surface of the tongue at a frame rate of 100Hz. We conclude that the new prototype provides a practical measurement system with a subjectively informative real-time display but further development is required in order to obtain objective accuracy.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-369"
  },
  "meynadier98_icslp": {
   "authors": [
    [
     "Yohann",
     "Meynadier"
    ],
    [
     "Michel",
     "Pitermann"
    ],
    [
     "Alain",
     "Marchal"
    ]
   ],
   "title": "Effects of contrastive focal accent on linguopalatal articulation and coarticulation in the French [kskl] cluster",
   "original": "i98_0542",
   "page_count": 4,
   "order": 370,
   "p1": "paper 0542",
   "pn": "",
   "abstract": [
    "This paper investigates the effects of contrastive focal accent placement on lingual articulation and coarticulation of French [kskl] clusters in word-medial position. The EPG results show that (i) this type of accent does not systematically increase the amplitude, but the duration of linguopalatal constrictions (particularly their release); (ii) it directly lengthens the temporal interval between the articulatory hold phases of two contiguous consonants; (iii) no matter what the accent position is, it can affect the whole cluster; (iv) the gestural co-ordination of biconsonant sequences seemed to vary with the focal accent more according to articulatory than syllable boundary rhythmic constraints.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-370"
  },
  "kitamura98_icslp": {
   "authors": [
    [
     "Christine",
     "Kitamura"
    ],
    [
     "Denis",
     "Burnham"
    ]
   ],
   "title": "Acoustic and affective qualities of IDS in English",
   "original": "i98_0909",
   "page_count": 4,
   "order": 371,
   "p1": "paper 0909",
   "pn": "",
   "abstract": [
    "This study examines modifications to the acoustic and affective qualities of infant-direct speech (IDS) from birth to 12 months. The acoustic analysis of fundamental frequency shows that IDS has the highest level of mean fundamental frequency (F 0 ) in speech to infants at 6 months, and the highest level of pitch range in speech to infants at 12 months. Sex-based differences were evident with mothers using higher mean-F 0 and pitch range in speech to female than male infants. The mother's speech samples were also rated on five scales of communicative intent. Two factors, labelled 'Affective' and 'Attentional', were extracted from these scales. Analysis of variance of the derived factor scores shows that mothers have the highest scores on the Affective factor at 6 and 12 months, while the maximum on the Attentional factor occurs at 9 months. Mothers also increase their use of both these IDS components more in speech to girls than boys.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-371"
  },
  "thanavisuth98_icslp": {
   "authors": [
    [
     "Chayada",
     "Thanavisuth"
    ],
    [
     "Sudaporn",
     "Luksaneeyanawin"
    ]
   ],
   "title": "Acoustic qualities of IDS and ADS in Thai",
   "original": "i98_0912",
   "page_count": 4,
   "order": 372,
   "p1": "paper 0912",
   "pn": "",
   "abstract": [
    "Infant Directed Speech (IDS) samples were collected longitudinally from six mothers speaking to their infants (three with female infants and three with male infants) when their infants were newborns, 3 months, 6 months, 9 months and 12 months of age. Adult Directed Speech (ADS) samples of these mothers were also collected to be compared with these IDS samples. This maternal speech was then examined in terms of the phonetic characteristics, specially the prosodic aspects-pitch, tempo, and loudness. Twenty utterances from each of the six mothers speaking to their infants at each of the five ages and to an adult, totaling 720 utterances (20 utterances x 6 mothers x 6 age groups) were acoustically analyzed. They are fundamental frequencies, frequency ranges, utterance duration, syllable duration, numbers of syllable per utterance, and intensity values. Results of the comparison between IDS and ADS showed that IDS used higher fundamental frequencies (263.3 Hz vs 247.99 Hz), greater frequency range in semitones (15.16 st vs 13.5 st), shorter utterance duration (1106 ms vs 1506 ms), longer syllable duration (343 ms vs 220 ms), and less numbers of syllable per utterance (3.85 vs 7.63). Concerning the age-related changes of prosodic features, there seems to be three age intervals which show unique modification to the pattern of prosodic characteristics. The first age group is the newborns as opposed to other age groups, mothers use longer utterance duration, longer syllable duration, and more numbers of syllable per utterance. In the 3- and the 6-month-old periods, the mothers' speech style changes to an increase of the mean Ff and pitch modulation. By the time the infants reach 9 and 12 months of age, there is a successive decrease of Ff of the speech directed to the 6MO to the 9MO and the 12MO. Most prosodic features of the IDS directed to the 12MO are almost similar to those of the ADS. The study of intensity values in IDS fails to reveal any significant variation due to a general phonetic problem in the instrumental study of the degree of loudness in speech. The variations of the phonetic characteristics among the 6 groups in all aspects were statistically analyzed using the Analysis of Variance (ANOVA). It was found that every variation of the prosodic features investigated across age groups was statistically significant. It seems that these phonetic characteristics in the mothers' speech are adjusted to accommodate the communicative functions between the mothers and their infants and these functions have developed according to other developments of the infants.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-372"
  },
  "luksaneeyanawin98_icslp": {
   "authors": [
    [
     "Sudaporn",
     "Luksaneeyanawin"
    ],
    [
     "Chayada",
     "Thanavisuth"
    ],
    [
     "Suthasinee",
     "Sittigasorn"
    ],
    [
     "Onwadee",
     "Rukkarangsarit"
    ]
   ],
   "title": "Pragmatic characteristics of infant directed speech",
   "original": "i98_0913",
   "page_count": 4,
   "order": 373,
   "p1": "paper 0913",
   "pn": "",
   "abstract": [
    "A longitudinal study of the pragmatic characteristics of the Infant Directed Speech (IDS) of middle class Thai mothers when talking to their babies at birth (Newborn), and when the infants were 3-, 6-, 9-, and 12-months-old was conducted. Three aspects were investigated: (1) the verbal acts which the mothers use with their infants using the framework of Thai linguistic action verbs (LAVs), and speech act verbs (SAVs) found in IDS; (2) the use of interrogative sentences in IDS both in quantitative and qualitative terms, and (3) the use of final particles incorporating both the status and mood particles which the mothers use in IDS. Regarding SAVs, overall IDS was made up of 38.6% expressives, 23.2% assertives, 20% directives, 15.3% questions, and 2.4% interaction-management. Comparison of SAVs in IDS directed to the five age groups shows a similarity in the pragmatic characteristics of the 3-and 6-month-olds, as opposed to the 9- and 12-month-olds. IDS to Newborns is unique compared with these two groups. It was found that 27.8 % of the IDS utterances were interrogatives, with a peak incidence of interrogatives at 3 months. The incidence of Yes-No interrogatives decreased as the age of the infants increased, whereas the incidence of WH- interrogatives increased as the age of the infants increased. Regarding the type of information mothers were seeking in WH- interrogatives, VP information-seeking is prominent for the Newborn and 3-month speech, but decreased as the infants got older. NP information-seeking was low for speech to the Newborns and 3-month-olds, and increased in speech to the 6-, 9-, and 12-month-olds. The information-seeking in adjuncts (location, time, manner etc.) was high in the 9- and 12-month-olds, when the infants were able to move about and started to explore their physical world. Interrogatives are used mostly for didactic functions; mothers asked and then answered the questions themselves. A large number of repetitive question forms are found, and these are used to draw the attention of infants in the mother-child interaction. Analysis of the use of the final particles reflects very clear didactic and emotive functions in IDS. The pragmatic characteristics of IDS during the first twelve months of the infants' lives varied significantly across ages. Mothers adapted their communication strategies in talking to their babies. These variations were found to help accommodate the mother-child interaction from the time when the infants were not able to communicate at all (Newborns), to the time when they started to communicate with nonverbal acts such as gazing, smiling and crying with eye contact (3 to 6 months), and in turn to the time when they started to communicate with hand touching and rudimentary verbal communication (9 to 12 months). IDS input is not just input data for infants to learn linguistic forms, but plays an important role in the development of the linguistic behaviour with the ultimate goal of communicating with other human beings successfully.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-373"
  },
  "burnham98_icslp": {
   "authors": [
    [
     "Denis",
     "Burnham"
    ],
    [
     "Elizabeth",
     "Francis"
    ],
    [
     "Ute",
     "Vollmer-Conna"
    ],
    [
     "Christine",
     "Kitamura"
    ],
    [
     "Vicky",
     "Averkiou"
    ],
    [
     "Amanda",
     "Olley"
    ],
    [
     "Mary",
     "Nguyen"
    ],
    [
     "Cal",
     "Paterson"
    ]
   ],
   "title": "Are you my little pussy-cat? acoustic, phonetic and affective qualities of infant- and pet-directed speech",
   "original": "i98_0916",
   "page_count": 4,
   "order": 374,
   "p1": "paper 0916",
   "pn": "",
   "abstract": [
    "It has been suggested that infant-directed speech (IDS) is a special speech register conducive to gaining infants' attention, eliciting and expressing affect, and teaching infants about the phonology of the ambient language. Certain similarities have been observed between IDS and pet-directed speech (PDS), but until now these have not been systematically studied. In this study IDS, PDS, and adult directed speech (ADS) are compared on acoustic, phonetic, and affective measures. It has been suggested that IDS should be more tuned to a linguistic didactic function than PDS, but that the two should be similar acoustically and affectively. The results show that IDS and PDS are generally different from ADS, but that IDS and PDS do not differ substantially, either acoustically, or phonetically. As the expected difference between IDS and PDS was not obtained, it could be suggested that dependent variables thought to measure the functions of special speech registers may not necessarily do so unequivocally.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-374"
  },
  "burnham98b_icslp": {
   "authors": [
    [
     "Denis",
     "Burnham"
    ]
   ],
   "title": "Special speech registers: talking to australian and Thai infants, and to pets",
   "original": "i98_0915",
   "page_count": 2,
   "order": 375,
   "p1": "paper 0915",
   "pn": "",
   "abstract": [
    "Infant directed speech (IDS) is the special speech register used to talk to human infants. This paper provides an historical and theoretical background to the study of IDS and special speech registers, as an introduction to four papers which follow. These are concerned with Australian IDS [Kitamura & Burnham], Thai IDS [Thanavisuth & Luksaneeyanawin; Luksaneeyanawin, Thanavisuth, Sittigasorn, & Rukkarangsarit], and Australian pet directed speech [Burnham, Francis, Vollmer-Conna, Kitamura, Averkiou, Olley, Nguyen, & Paterson].\n",
    ""
   ]
  },
  "masuko98_icslp": {
   "authors": [
    [
     "Takashi",
     "Masuko"
    ],
    [
     "Keiichi",
     "Tokuda"
    ],
    [
     "Takao",
     "Kobayashi"
    ]
   ],
   "title": "A very low bit rate speech coder using HMM with speaker adaptation",
   "original": "i98_0777",
   "page_count": 4,
   "order": 376,
   "p1": "paper 0777",
   "pn": "",
   "abstract": [
    "This paper describes a speaker adaptation technique for a phonetic vocoder based on HMM. In the vocoder, the encoder performs phoneme recognition and transmits phoneme indexes and state durations to the decoder, and the decoder synthesizes speech using HMM-based speech synthesis technique. One of the main problems of this vocoder is that the voice characteristics of synthetic speech depend on HMMs used in the decoder, and are therefore fixed regardless of a variety of input speakers. To overcome this problem, we adapt HMMs to input speech by transmitting transfer vectors, information on mismatch between the input speech and HMMs. The results of the subjective tests show that the performance of the proposed vocoder without quantization of transfer vectors is comparable to that of a speaker dependent vocoder.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-375"
  },
  "ekudden98_icslp": {
   "authors": [
    [
     "E.",
     "Ekudden"
    ],
    [
     "R.",
     "Hagen"
    ],
    [
     "B.",
     "Johansson"
    ],
    [
     "S.",
     "Hayashi"
    ],
    [
     "A.",
     "Kataoka"
    ],
    [
     "S.",
     "Kurihara"
    ]
   ],
   "title": "ITU-t g.729 extension at 6.4 kbps",
   "original": "i98_0808",
   "page_count": 4,
   "order": 377,
   "p1": "paper 0808",
   "pn": "",
   "abstract": [
    "This paper describes the 6.4 kbit/s CS-ACELP coder being standardized as annex D to ITU-T G.729. The coder is based on the same building blocks as the 8 kbit/s G.729 to facilitate low complexity extensions to G.729 in terms of additional memory usage. It is fully switchable with the 8 kbit/s coder and provides additional flexibility to existing and emerging G.729 applications. The fixed codebook is a 2-pulse algebraic codebook. The adaptive codebook quantization has been changed and a new conjugate structure gain codebook is used. In order to compensate for the sparser algebraic codebook, an adaptive post-processing technique is used to enhance the quality for unvoiced speech and background noise sounds. Subjective tests have indicated that the coder has a performance close to that of G.729, and equivalent to that of G.723.1 at 6.3 kbit/s for speech.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-376"
  },
  "mudugamuwa98_icslp": {
   "authors": [
    [
     "Damith J.",
     "Mudugamuwa"
    ],
    [
     "Alan B.",
     "Bradley"
    ]
   ],
   "title": "Adaptive transformation for segmented parametric speech coding",
   "original": "i98_0107",
   "page_count": 4,
   "order": 378,
   "p1": "paper 0107",
   "pn": "",
   "abstract": [
    "In voice coding applications where there is no constraint on the encoding delay, segment coding techniques can be used to achieve a reduction in data rate. For low data rate linear predictive coding schemes, increasing the encoding delay allows one to exploit any long term temporal stationarities on an interframe basis, thus reducing the transmission bandwidth or storage needs of the speech signal. Transform coding has previously been applied in low data rate speech coding to exploit both the interframe and the intraframe correlation [1][6][8]. This paper investigates the potential of an adaptive transformation scheme for a segmented parametric speech representation. The problem of transform quantization is formulated and a solution methodology was proposed. The potential benefit of the use of the proposed adaptive transformation scheme is discussed in the context of segmented LSPs.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-377"
  },
  "epps98_icslp": {
   "authors": [
    [
     "Julien",
     "Epps"
    ],
    [
     "W. Harvey",
     "Holmes"
    ]
   ],
   "title": "Speech enhancement using STC-based bandwidth extension",
   "original": "i98_0711",
   "page_count": 4,
   "order": 379,
   "p1": "paper 0711",
   "pn": "",
   "abstract": [
    "Telephone speech is typically bandlimited to 4 kHz, resulting in a 'muffled' quality. Coding speech with bandwidth greater than 4 kHz reduces this distortion, but requires a higher bit rate to avoid other types of distortion. An alternative to coding wider bandwidth speech is to exploit correlation between the 0-4 kHz and 4-8 kHz speech bands to re-synthesize wideband speech from narrowband speech. This paper presents a method for re-synthesizing narrowband coded speech using sinusoidal transform coding (STC), modified codebook mapping and a novel method for the synthesis of highband unvoiced components. Informal listening test results indicate that this method produces a significant quality improvement in speech which has been coded using narrowband standards.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-378"
  },
  "zhang98d_icslp": {
   "authors": [
    [
     "Weihua",
     "Zhang"
    ],
    [
     "W. Harvey",
     "Holmes"
    ]
   ],
   "title": "Performance and optimization of the SEEVOC algorithm",
   "original": "i98_1128",
   "page_count": 4,
   "order": 380,
   "p1": "paper 1128",
   "pn": "",
   "abstract": [
    "The Spectral Envelope Estimation Vocoder (SEEVOC) is a successful spectral envelope estimation method that plays an important role in low bit rate speech coding based on the sinusoidal model. This paper investigates the properties and limitations of the SEEVOC algorithm, and shows that the required accuracy for the initial pitch estimate is greater than commonly supposed. It also generalizes and optimizes the SEEVOC algorithm by choice of the search range parameters a and b. Rules for the optimum choice of a and b are derived, based on both theoretical analysis and experimental results. The effects of noise on the SEEVOC algorithm are also investigated. Experimental results show that the SEEVOC algorithm performs better for voiced speech in the presence of noise than linear prediction (LP) analysis.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-379"
  },
  "holmes98_icslp": {
   "authors": [
    [
     "Wendy J.",
     "Holmes"
    ]
   ],
   "title": "Towards a unified model for low bit-rate speech coding using a recognition-synthesis approach",
   "original": "i98_0553",
   "page_count": 4,
   "order": 381,
   "p1": "paper 0553",
   "pn": "",
   "abstract": [
    "This paper proposes a recognition-synthesis approach to speech coding which uses a formant trajectory model for both recognition and synthesis. It is argued that this unified approach to coding has the potential to achieve low data rates whilst preserving speech quality and paralinguistic information. A simple coding scheme is described which establishes the principles of this approach. Formant analysis is applied to the input speech, and the formant features are input to a linear-trajectory segmental hidden Markov model recognizer to locate segment boundaries. The formant parameters for each segment are coded using a linear trajectory description, and used to drive a parallel-formant synthesizer to reproduce the utterance at the receiver. The coding method has been tested on utterances from a variety of speakers. In the current system, which has not yet been optimised for coding efficiency, speech is typically coded at 600-1000 bits/s with good intelligibility, whilst preserving speaker characteristics.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-380"
  },
  "skoglund98_icslp": {
   "authors": [
    [
     "Jan",
     "Skoglund"
    ],
    [
     "W. Bastiaan",
     "Kleijn"
    ]
   ],
   "title": "On the significance of temporal masking in speech coding",
   "original": "i98_0747",
   "page_count": 4,
   "order": 382,
   "p1": "paper 0747",
   "pn": "",
   "abstract": [
    "This paper addresses the issue of masking of noise in voiced speech. First, we examine the audibility of cyclostationary narrow-band noise added to voiced speech generated by synthetic excitation. Varying the temporal location of noise within a pitch cycle corresponds to varying its phase spectrum. Using this fact, we find that a phase change of the noise in the high frequency region is more perceptible for a low-pitched sound than for a high-pitched sound. We propose a pitch-dependent temporal weighting function and we show experimentally that it is beneficial to the quantization of pitch-cycle waveforms.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-381"
  },
  "kleijn98_icslp": {
   "authors": [
    [
     "W. Bastiaan",
     "Kleijn"
    ],
    [
     "Huimin",
     "Yang"
    ],
    [
     "Ed F.",
     "Deprettere"
    ]
   ],
   "title": "Waveform interpolation coding with pitch-spaced subbands",
   "original": "i98_1069",
   "page_count": 4,
   "order": 383,
   "p1": "paper 1069",
   "pn": "",
   "abstract": [
    "We present new waveform-interpolation coding procedures which allow perfect reconstruction of the speech signal from the unquantized parameter set. Instead of using adaptive parameter extraction methods, we combine a time warping of the original signal with nonadaptive parameter extraction methods. The new coding structure has good performance at low bit rates and provides convergence to the original waveform with increasing rate.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-382"
  },
  "chong98_icslp": {
   "authors": [
    [
     "Nicola R.",
     "Chong"
    ],
    [
     "Ian S.",
     "Burnett"
    ],
    [
     "Joe F.",
     "Chicharo"
    ]
   ],
   "title": "An improved decomposition method for WI using IIR wavelet filter banks",
   "original": "i98_0142",
   "page_count": 4,
   "order": 384,
   "p1": "paper 0142",
   "pn": "",
   "abstract": [
    "In this paper, we present an alternative characteristic waveform (CW) decomposition mechanism for the Waveform Interpolation (WI) paradigm based on the Pitch Synchronous Wavelet Transform (PSWT). In this technique, IIR filters replace the conventional FIR filters of the PSWT, offering computational and spectral magnitude performance advantages, in addition to significant delay reductions. Previously, the PSWT has only incorporated filter banks with slowly reacting FIR wavelet filters. While these filters possess the desirable properties of linear phase, and design simplicity, a large delay is incurred which increases exponentially with increasing resolution. The progression to IIR filter banks gives rise to a multi-resolution decomposition mechanism, beneficial for real-time applications, such as speech coding, where delay is an important issue.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-383"
  },
  "alku98b_icslp": {
   "authors": [
    [
     "Paavo",
     "Alku"
    ],
    [
     "Susanna",
     "Varho"
    ]
   ],
   "title": "A new linear predictive method for compression of speech signals",
   "original": "i98_0003",
   "page_count": 4,
   "order": 385,
   "p1": "paper 0003",
   "pn": "",
   "abstract": [
    "A new linear predictive method is presented in this study. The method, Linear Prediction with Linear Extrapolation (LPLE), reformulates the computation of linear prediction by combining the preceding values of sample x(n) into consecutive sample pairs (i.e., x(n-2i), x(n-2i+1)). Each of these pairs determines a regression line the value of which at time instant n is used as a data sample in the prediction. The optimal LPLE-predictor is obtained by minimizing the square of the prediction error using the autocorrelation method. The rationale for the new method is the fact that LPLE yields an all-pole filter of order 2p when the number of unknowns in the normal equations equals p. Therefore the new all-pole modeling method can be used in speech coding applications. Preliminary experiments of the present study show that LPLE is able to model speech spectra more accurately in comparison to conventional linear prediction in the case when a very small number of prediction parameters is required to be used in order to greatly compress the spectral information of speech signals.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-384"
  },
  "ghaemmaghami98_icslp": {
   "authors": [
    [
     "Shahrokh",
     "Ghaemmaghami"
    ],
    [
     "Mohamed",
     "Deriche"
    ],
    [
     "Sridha",
     "Sridharan"
    ]
   ],
   "title": "Hierarchical temporal decomposition: a novel approach to efficient compression of spectral characteristics of speech",
   "original": "i98_0673",
   "page_count": 4,
   "order": 386,
   "p1": "paper 0673",
   "pn": "",
   "abstract": [
    "The authors propose a new approach to Temporal Decomposition (TD) of characteristic parameters of speech for very low rate coding applications. The method models the articulatory dynamics employing a hierarchical error minimization algorithm which does not use Singular Value Decomposition. It is also much faster than conventional TD and could be implemented in real-time. High flexibility is achieved with the proposed method to comply with the desired coding requirements, such as compression ratio, accuracy, delay, and computational complexity. This method can be used for coding spectral parameters at rates 1000-1200 b/s with high fidelity and an algorithmic delay of less than 150 msec.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-385"
  },
  "hura98_icslp": {
   "authors": [
    [
     "Susan L.",
     "Hura"
    ]
   ],
   "title": "Speech intelligibility testing for new technologies",
   "original": "i98_0042",
   "page_count": 4,
   "order": 387,
   "p1": "paper 0042",
   "pn": "",
   "abstract": [
    "There are several tests of speech intelligibility currently available which employ a variety of methods. The most appropriate method for testing intelligibility of speech transmitted via telephony is a forced choice task in which listeners hear speech samples and identify what they hear from among a set of alternatives displayed onscreen. This methodology allows tests to be run quickly and scored automatically. A major flaw in existing forced-choice intelligibility tests is the use of unfamiliar words, nonwords, and proper names along with common words. A stimulus set that is mixed in this way may introduce response biases into the test and therefore produce results that are less predictive of actual intelligibility performance. The Intelligibility of Familiar Items Test (IFIT) ameliorates several methodological flaws found in earlier tests. The IFIT uses a stimulus set composed of high familiarity real English words and tests consonants in initial and final word position and vowels in word medial position.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-386"
  },
  "kim98c_icslp": {
   "authors": [
    [
     "Sung Joo",
     "Kim"
    ],
    [
     "Sangho",
     "Lee"
    ],
    [
     "Woo Jin",
     "Han"
    ],
    [
     "Yung Hwan",
     "Oh"
    ]
   ],
   "title": "Efficient quantization of LSF parameters based on temporal decomposition",
   "original": "i98_0469",
   "page_count": 4,
   "order": 388,
   "p1": "paper 0469",
   "pn": "",
   "abstract": [
    "In this paper, we present a restricted temporal decomposition method for LSF parameters. The event vectors estimated by this method preserve the ordering property of LSF parameters so that they can be quantized efficiently. Experimental results show that interpolated LSF parameters can be quantized transparently at the rate of 753bps. Also we design a LPC vocoder at 996bps as an application of the proposed method. According to a listening test, the reconstructed speech of our vocoder has reasonable quality comparing with 2400bps LPC10e.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-387"
  },
  "kohata98_icslp": {
   "authors": [
    [
     "Minoru",
     "Kohata"
    ]
   ],
   "title": "A sinusoidal harmonic vocoder at 1.2 kbps using auditory perceptual characteristics",
   "original": "i98_0037",
   "page_count": 4,
   "order": 389,
   "p1": "paper 0037",
   "pn": "",
   "abstract": [
    "In this paper, a very low bit speech coder at 1.2 Ops is newly proposed. Like the LPC vocoder, it requires few types of information (power, pitch, and spectral information), but its quality is far superior. In the proposed vocoder, the synthesized speech quality is improved based on auditory perceptual characteristics. The synthesis method is one of harmonic coding, using sinusoids whose frequencies are multiples of the fundamental frequency, where the amplitudes of the sinusoids, are adaptively modulated using Gammatone filters as a perceptual weighting filter. The sinusoids' phases are also adjusted so as to maximize the perceptual quality. In order to reduce the total bit rate to 1.2 Ops, a new segment coder for spectral information (LSP coefficients) using DP matching is also proposed. The quality of the synthesized speech is considerably improved compared with that of the simple I-PC vocoder, according to MOS and preference tests.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-388"
  },
  "koishida98_icslp": {
   "authors": [
    [
     "Kazuhito",
     "Koishida"
    ],
    [
     "Gou",
     "Hirabayashi"
    ],
    [
     "Keiichi",
     "Tokuda"
    ],
    [
     "Takao",
     "Kobayashi"
    ]
   ],
   "title": "A 16 kbit/s wideband CELP coder using MEL-generalized cepstral analysis and its subjective evaluation",
   "original": "i98_0904",
   "page_count": 4,
   "order": 390,
   "p1": "paper 0904",
   "pn": "",
   "abstract": [
    "We have proposed a wideband CELP coder, called MGC-CELP, which provides high quality speech by utilizing mel-generalized cepstral (MGC) analysis instead of linear prediction (LP). In this paper, we investigate the performance of the wideband MGC-CELP coder at 16 kbit/s in terms of short-term predictor order, i.e., order of MGC analysis. Subjective tests show that the MGC-CELP coder with a predictor of order 20 gives better performance than ITU-T G.722 at 64 kbit/s. It is also found that the MGC-CELP coder with 12th order achieves comparable quality to the 64 kbit/s G.722, and outperforms the 16 kbit/s conventional CELP coder using 20th-order LP analysis under the same conditions.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-389"
  },
  "molyneux98_icslp": {
   "authors": [
    [
     "D. J.",
     "Molyneux"
    ],
    [
     "C. I.",
     "Parris"
    ],
    [
     "X. Q.",
     "Sun"
    ],
    [
     "B. M. G.",
     "Cheetham"
    ]
   ],
   "title": "Comparison of spectral estimation techniques for low bit-rate speech coding",
   "original": "i98_0946",
   "page_count": 4,
   "order": 391,
   "p1": "paper 0946",
   "pn": "",
   "abstract": [
    "Many low bit-rate speech coders represent the spectral envelope by an all-pole digital filter whose coefficients are calculated by a form of linear prediction (LP) analysis. The lower the bit-rate, the more critical will be the accuracy of the spectral analysis for achieving good quality speech. This paper compares four known techniques: a technique based on cubic spline interpolation, DAP, MVDR, and iterative all-pole modelling. First, the accuracy obtained for artificial and real speech spectra is assessed for each technique by calculating the degree of spectral distortion with reference to the spectral envelope sampled at the pitch-harmonics. Then, each technique is used to characterise the spectral amplitudes generated by a 2.4 kb/s multi-band excitation (MBE) coder. Results show that significantly better spectral accuracy is obtained using DAP. However listening tests on MBE encoded speech indicate that the advantage of DAP over the other techniques is not strongly perceptible.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-390"
  },
  "nakatoh98_icslp": {
   "authors": [
    [
     "Yoshihisa",
     "Nakatoh"
    ],
    [
     "Takeshi",
     "Norimatsu"
    ],
    [
     "Ah Heng",
     "Low"
    ],
    [
     "Hiroshi",
     "Matsumoto"
    ]
   ],
   "title": "Low bit rate coding for speech and audio using mel linear predictive coding (MLPC) analysis",
   "original": "i98_1100",
   "page_count": 4,
   "order": 392,
   "p1": "paper 1100",
   "pn": "",
   "abstract": [
    "This paper proposes a low bit rate coding method for speech and audio using a new analysis method named MLPC (Mel-LPC analysis). In MLPC analysis a spectrum envelope is estimated on a mel- or bark-frequency scale, so as to improve the spectral resolution in the low frequency band. This analysis is accomplished with about two-fold increase in computation over the standard LPC analysis. Our coding algorithm using MLPC analysis consists of five key parts: time frequency transformation, inverse filtering by MLPC spectrum envelope, power normalization, perceptual weighting estimation, and multi-stage VQ. In subjective experiments, we have investigated the performance of MLPC analysis, through paired comparison tests between the MLPC analysis and the standard LPC one in inverse filtering. In all bit rates, almost all the listeners feel decoding sounds by the MLPC analysis is superior to the LPC one. Especially in low bit rate, there is a great difference between them.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-391"
  },
  "pan98b_icslp": {
   "authors": [
    [
     "Jeng-Shyang",
     "Pan"
    ],
    [
     "Chin-Shiuh",
     "Shieh"
    ],
    [
     "Shu-Chuan",
     "Chu"
    ]
   ],
   "title": "Comparison study on VQ codevector index assignment",
   "original": "i98_0031",
   "page_count": 4,
   "order": 393,
   "p1": "paper 0031",
   "pn": "",
   "abstract": [
    "Vector quantization is a popular technique in low bit rate coding of speech signal. The transmission index of the codevector is highly sensitive to channel noise. The channel distortion can be reduced by organizing the codevector indices suitably. Several index assignment algorithms are studied comparatively. Among them, the index allocation algorithm proposed by Wu and Barba is the fastest method but the channel distortion is the worst one. The proposed parallel tabu search algorithm reach the best performance of channel distortion.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-392"
  },
  "parry98_icslp": {
   "authors": [
    [
     "John J.",
     "Parry"
    ],
    [
     "Ian S.",
     "Burnett"
    ],
    [
     "Joe F.",
     "Chicharo"
    ]
   ],
   "title": "Using linguistic knowledge to improve the design of low-bit rate LSF quantisation",
   "original": "i98_0137",
   "page_count": 4,
   "order": 394,
   "p1": "paper 0137",
   "pn": "",
   "abstract": [
    "In this paper we investigate an alternative approach to the design of low-bit rate (LBR) quantisation. This approach incorporates phonetic information into the structure of Line Spectral Frequency (LSF) codebooks. In prior work vector quantisation (VQ) has been used to quantise stochastic processes. Speech signals can, however, be described in terms of phonetic segments and linguistic rules. A trained LSF codebook, like the phonetic inventory of a language, is a static description of spectral behaviour of speech. As clear relationships exist between phonetic segments and LSFs the structure of an LSF codebook can be analysed in terms of the phonetic segments. The investigation leads to the conclusion that phonetic information can be usefully employed in codebook training in terms of perceptual performance and bit-rate reductions.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-393"
  },
  "petrinovic98_icslp": {
   "authors": [
    [
     "Davor",
     "Petrinovic"
    ]
   ],
   "title": "Transform coding of LSF parameters using wavelets",
   "original": "i98_1114",
   "page_count": 4,
   "order": 395,
   "p1": "paper 1114",
   "pn": "",
   "abstract": [
    "A method of inter-frame transform coding of Line Spectrum Frequencies (LSF) using the Discrete Wavelet Transform is presented in this paper. Each component of the LSFs (or of their linear transform) is treated separately and is decomposed into a set of subband signals using the nonuniform filter bank. Subband signals are quantized and coded independently. By the appropriate choice of the mother Wavelet, subband signal with the lowest rate comprises most of the LSF waveform energy. Filter bank effectively decorrelates the input signal, enabling more efficient quantization of the subband signals. A suitable weighted Euclidean distance measure in the Wavelet domain is proposed, defining optimal static or dynamic bit allocation of the subband signals. It is shown that the average bit rate for coding of the DCT transformed LSFs can be reduced by 0.9 bits per vector component by using a very simple Wavelet. The total delay due to the inter-frame coding is only 90ms that is acceptable even for a medium bit rate speech coders.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-394"
  },
  "plante98_icslp": {
   "authors": [
    [
     "F.",
     "Plante"
    ],
    [
     "B. M. G.",
     "Cheetham"
    ],
    [
     "D.",
     "Marston"
    ],
    [
     "P. A.",
     "Barrett"
    ]
   ],
   "title": "Source controlled variable bit-rate speech coder based on waveform interpolation",
   "original": "i98_0848",
   "page_count": 4,
   "order": 396,
   "p1": "paper 0848",
   "pn": "",
   "abstract": [
    "This paper describes a source controlled variable bit-rate (SC-VBR) speech coder based on the concept of prototype waveform interpolation. The coder uses a four mode classification : silence, voiced, unvoiced and transition. These modes are detected after the speech has been decomposed into slowly evolving (SEW) and rapidly evolving (REW) waveforms. A voicing activity detection (VAD), the relative level of SEW and REW and the cross-correlation coefficient between characteristic waveform segments are used to make the classification. The encoding of the SEW components is improved using a gender adaptation. In tests using conversational speech, the SC-VBR allows a compression factor of around 3. The VBR coder was evaluated against a fixed rate 4.6kbit/s PWI coder for clean speech and noisy speech and was found to perform better for male speech and for noisy speech.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-395"
  },
  "ribeiro98_icslp": {
   "authors": [
    [
     "Carlos M.",
     "Ribeiro"
    ],
    [
     "Isabel M.",
     "Trancoso"
    ]
   ],
   "title": "Improving speaker recognisability in phonetic vocoders",
   "original": "i98_0448",
   "page_count": 4,
   "order": 397,
   "p1": "paper 0448",
   "pn": "",
   "abstract": [
    "Phonetic vocoding is one of the methods for coding speech below 1000 bit/s. The transmitter stage includes a phone recogniser whose index is transmitted together with prosodic information such as duration, energy and pitch variation. This type of coder does not transmit spectral speaker characteristics and speaker recognisability thus becomes a major problem. In our previous work, we adapted a speaker modification strategy to minimise this problem, modifying a codebook to match the spectral characteristics of the input speaker. This is done at the cost of transmitting the LSP averages computed for vowel and glide phones. This paper presents new codebook generation strategies, with gender dependence and interpolation frames, that lead to better speaker recognisability and speech quality. Relatively to our previous work, some effort was also devoted to deriving more efficient quantization methods for the speaker-specific information , that considerably reduced the average bit rate, without quality degradation.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-396"
  },
  "ahkuputra98_icslp": {
   "authors": [
    [
     "Visarut",
     "Ahkuputra"
    ],
    [
     "Somchai",
     "Jitapunkul"
    ],
    [
     "Nutthacha",
     "Jittiwarangkul"
    ],
    [
     "Ekkarit",
     "Maneenoi"
    ],
    [
     "Sawit",
     "Kasuriya"
    ]
   ],
   "title": "A comparison of Thai speech recognition systems using hidden Markov model, neural network, and fuzzy-neural network",
   "original": "i98_0283",
   "page_count": 4,
   "order": 398,
   "p1": "paper 0283",
   "pn": "",
   "abstract": [
    "The recognition of ten Thai isolated numerals from zero to nine and 60 Thai polysyllabic words are compared between different recognition techniques, namely, Neural Network, Modified Backpropagation Neural Network, Fuzzy-Neural Network, and Hidden Markov Model. The 15-state left-to-right discrete hidden markov model in cooperation with the vector quantization technique has been studied and compared with the multilayer perceptron neural network using the error backpropagation, the modified backpropagation, and also with the fuzzy-neural network with the same configuration. The recognition error on Thai isolated numerals using the conventional neural network, the modified neural network, the fuzzy-neural network, and the hidden markov model techniques are 26.97 percent, 22.00 percent, 8.50 percent, and 15.75 percent respectively.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-397"
  },
  "freitag98_icslp": {
   "authors": [
    [
     "Felix",
     "Freitag"
    ],
    [
     "Enric",
     "Monte"
    ]
   ],
   "title": "Phoneme recognition with statistical modeling of the prediction error of neural networks",
   "original": "i98_0455",
   "page_count": 4,
   "order": 399,
   "p1": "paper 0455",
   "pn": "",
   "abstract": [
    "This paper presents a speech recognition system which incorporates predictive neural networks. The neural networks are used to predict observation vectors of speech. The prediction error vectors are modeled on the state level by Gaussian densities, which provide the local similarity measure for the Viterbi algorithm during recognition. The system is evaluated on a continuous speech phoneme recognition task. Compared with a HMM reference system, the proposed system obtained better results in the speech recognition experiments.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-398"
  },
  "fukada98_icslp": {
   "authors": [
    [
     "Toshiaki",
     "Fukada"
    ],
    [
     "Takayoshi",
     "Yoshimura"
    ],
    [
     "Yoshinori",
     "Sagisaka"
    ]
   ],
   "title": "Neural network based pronunciation modeling with applications to speech recognition",
   "original": "i98_0658",
   "page_count": 4,
   "order": 400,
   "p1": "paper 0658",
   "pn": "",
   "abstract": [
    "We propose a method for automatically generating a pronunciation dictionary based on a pronunciation neural network that can predict plausible pronunciations (realized pronunciations) from canonical pronunciations. This method can generate multiple forms of realized pronunciations using the pronunciation network. Experimental results on spontaneous speech show that the automatically-derived pronunciation dictionary gives consistently higher recognition rates than a conventional dictionary.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-399"
  },
  "haskey98_icslp": {
   "authors": [
    [
     "Stephen J.",
     "Haskey"
    ],
    [
     "Sekharajit",
     "Datta"
    ]
   ],
   "title": "A comparative study of OCON and MLP architectures for phoneme recognition",
   "original": "i98_0568",
   "page_count": 4,
   "order": 401,
   "p1": "paper 0568",
   "pn": "",
   "abstract": [
    "In this paper a comparative study between One-Class-One-Network (OCON) and Multi-Layered Perceptron (MLP) neural networks for vowel phoneme recognition is presented. The OCON architecture, first proposed by I.C.Jou et al, is similar in design to a conventional feed-forward MLP, only each class had its own dedicated sub-network containing a single output node. Conventional MLPs usually consist of fully-connected nodes which not only result in a large number of weighted connections but also create the problem of cross-class interference. Using vowel phoneme data from the DARPA TIMIT corpus of read speech, MLP and OCON architectures were trained and the relative effects of recognition and convergence rates during both intra and inter-class adaptation tested. The OCON showed an increase in the convergence rate of 273% and an improvement of adapted recognition rates against the MLP of over 12%.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-400"
  },
  "hosom98_icslp": {
   "authors": [
    [
     "John-Paul",
     "Hosom"
    ],
    [
     "Ronald A.",
     "Cole"
    ],
    [
     "Piero",
     "Cosi"
    ]
   ],
   "title": "Evaluation and integration of neural-network training techniques for continuous digit recognition",
   "original": "i98_0613",
   "page_count": 4,
   "order": 402,
   "p1": "paper 0613",
   "pn": "",
   "abstract": [
    "This paper describes a set of experiments on neural-network training and search techniques that, when combined, have resulted in a 54% reduction in error on the continuous digits recognition task. The best system had word-level accuracy of 97.52% on a test set of the OGI 30K Numbers corpus, which contains naturally-produced continuous digit strings recorded over telephone channels. Experiments investigated effects of the feature set, the amount of data used for training, the type of context-dependent categories to be recognized, the values for duration limits, and the type of grammar. The experiments indicate that the grammar and duration limits had a greater effect on recognition accuracy than the output categories, cepstral features, or a 50% increase in the amount of training data.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-401"
  },
  "jia98_icslp": {
   "authors": [
    [
     "Ying",
     "Jia"
    ],
    [
     "Limin",
     "Du"
    ],
    [
     "Ziqiang",
     "Hou"
    ]
   ],
   "title": "Hierarchical neural networks (HNN) for Chinese continuous speech recognition",
   "original": "i98_0415",
   "page_count": 4,
   "order": 403,
   "p1": "paper 0415",
   "pn": "",
   "abstract": [
    "To integrate the hierarchy structure of discrimination between all HMM states for Chinese Initials and Finals, we constructed in this paper Hierarchical Neural Networks (HNN), which differ from Jordan's HME in such extensions as more complex parameterization for gate and/or expert and dimension-reduced expert network. With these extensions, we can reuse those pre-trained simple node networks in a hierarchy structure (HNN), and fine-tune them jointly by Generalized Expectation Maximization (GEM) algorithm. The proposed HNNs were used within hybrid HMM-ANN models to perform the estimation of posterior probabilities for HMM states. Instead of using a large monolithic neural network, the HNN system can be trained in a short time compared with MLP estimator and result in a speed-up in decoding time over the conventional systems. We have applied the proposed hybrid HMM-HNN method to the recognition task of Chinese Continuous Speech., achieve a promising word error rate of 26.4%.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-402"
  },
  "keller98_icslp": {
   "authors": [
    [
     "Eric",
     "Keller"
    ]
   ],
   "title": "Neural network motivation for segmental distribution",
   "original": "i98_0937",
   "page_count": 4,
   "order": 404,
   "p1": "paper 0937",
   "pn": "",
   "abstract": [
    "Feature representations mediating between acoustic input and symbolic representation promise to reduce learning time needed for automatic speech signal segmentation. Experiments are reported that circumscribe simple acoustic inputs and appropriate feature sets for neural network training. Stable and compatible solutions for English and French were identified.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-403"
  },
  "mirghafori98_icslp": {
   "authors": [
    [
     "Nikki",
     "Mirghafori"
    ],
    [
     "Nelson",
     "Morgan"
    ]
   ],
   "title": "Combining connectionist multi-band and full-band probability streams for speech recognition of natural numbers",
   "original": "i98_1150",
   "page_count": 4,
   "order": 405,
   "p1": "paper 1150",
   "pn": "",
   "abstract": [
    "Multi-band automatic speech recognition is a new and exploratory area of speech recognition which has been getting much attention in the research community. It has been shown that multi-band ASR reduces word error in noisy conditions, particularly in the case of narrow band noise. In this work we show that multi-band ASR could be used to improve the speech recognition accuracy of natural numbers for clean speech when the multi-band (MB) information stream is used in addition to the full-band (FB) one. We also observe that a similar combination method significantly reduces the error rate on reverberant speech. Finally, we analyze the error patterns of the full-band and multi-band paradigms to understand why the combination of the two streams is effective.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-404"
  },
  "pizzolato98_icslp": {
   "authors": [
    [
     "Ednaldo B.",
     "Pizzolato"
    ],
    [
     "T. Jeff",
     "Reynolds"
    ]
   ],
   "title": "Initial speech recognition results using the multinet architecture",
   "original": "i98_0821",
   "page_count": 4,
   "order": 406,
   "p1": "paper 0821",
   "pn": "",
   "abstract": [
    "Multinet is a connectionist architecture designed for certain difficult multi-class pattern classification tasks. These are characterised by very large input feature spaces, rendering a monolithic classifier impractical. The architecture consists of a layer with at least one primary 'detector' for each class, followed by a combining net which estimates the posterior probabilities for all classes. Typically primary detectors only input a subset of the input features. Thus the architecture decomposes classification in two ways: by class and by factoring of the input space dimensions. Multinet incorporates the ideas of Modular Neural Networks and Ensembles. In this paper, we investigate the use of Multinet on standard HMM and hybrid HMM-NN systems that we run on the same tasks. The value and potential of the Multinet approach is shown by detailing successive improvements to the Multinet system which are easily obtained because of the modularity of the architecture.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-405"
  },
  "takara98_icslp": {
   "authors": [
    [
     "Tomio",
     "Takara"
    ],
    [
     "Yasushi",
     "Iha"
    ],
    [
     "Itaru",
     "Nagayama"
    ]
   ],
   "title": "Selection of the optimal structure of the continuous HMM using the genetic algorithm",
   "original": "i98_1066",
   "page_count": 4,
   "order": 407,
   "p1": "paper 1066",
   "pn": "",
   "abstract": [
    "The hidden Markov models (HMMs) are widely used for automatic speech recognition because they have a powerful algorithm used in estimating the model's parameters, and also achieve a high performance. Once a structure of the model is given, the model's parameters are obtained auto- matically by feeding training data. However, there is still an unresolved problem with the HMM, i.e. how to design an optimal HMM structure. In answer to this problem, we proposed the application of a genetic algorithm (GA) to search out such an optimal structure, and we showed this method to be effective for isolated word recognition. However, the test of this method was restricted to discrete HMMs. In this paper, we propose a new application of the GA to the continuous HMM (CHMM) which is thought to be more effective than the discrete HMM. We report the results of our experiment showing the effectiveness of the genetic algorithm in automatic speech recognition.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-406"
  },
  "tran98_icslp": {
   "authors": [
    [
     "Dat",
     "Tran"
    ],
    [
     "Michael",
     "Wagner"
    ],
    [
     "Tu Van",
     "Le"
    ]
   ],
   "title": "A proposed decision rule for speaker recognition based on fuzzy c-means clustering",
   "original": "i98_0797",
   "page_count": 4,
   "order": 408,
   "p1": "paper 0797",
   "pn": "",
   "abstract": [
    "In vector quantisation (VQ) based speaker recognition, the minimum overall average distortion rule is used as a criterion to assign a given sequence of acoustic vectors to a speaker model known as a codebook. An alternative decision rule based on fuzzy c-means clustering is proposed in this paper. A set of membership functions associated with vectors for codebooks are defined as discriminant functions and the maximum overall average membership function rule is stated. The theoretical analysis and the experimental results show that this rule can be used in both speaker identification and speaker verification. It is more effective than the minimum overall average distortion rule.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-407"
  },
  "tran98b_icslp": {
   "authors": [
    [
     "Dat",
     "Tran"
    ],
    [
     "Tu Van",
     "Le"
    ],
    [
     "Michael",
     "Wagner"
    ]
   ],
   "title": "Fuzzy Gaussian mixture models for speaker recognition",
   "original": "i98_0798",
   "page_count": 4,
   "order": 409,
   "p1": "paper 0798",
   "pn": "",
   "abstract": [
    "A fuzzy clustering based modification of Gaussian mixture models (GMMs) for speaker recognition is proposed. In this modification, fuzzy mixture weights are introduced by redefining the distances used in the fuzzy c-means (FCM) functionals. Their reestimation formulas are proved by minimising the FCM functionals. The experimental results show that the fuzzy GMMs can be used in speaker recognition and it is more effective than the GMMs in tests on the TI46 database.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-408"
  },
  "wutiwiwatchai98_icslp": {
   "authors": [
    [
     "Chai",
     "Wutiwiwatchai"
    ],
    [
     "Somchai",
     "Jitapunkul"
    ],
    [
     "Visarut",
     "Ahkuputra"
    ],
    [
     "Ekkarit",
     "Maneenoi"
    ],
    [
     "Sudaporn",
     "Luksaneeyanawin"
    ]
   ],
   "title": "A new strategy of fuzzy-neural network for Thai numeral speech recognition",
   "original": "i98_0349",
   "page_count": 4,
   "order": 410,
   "p1": "paper 0349",
   "pn": "",
   "abstract": [
    "In this research, a new strategy of Fuzzy-Neural Network system was proposed for Thai numeral speech recognition. Instead of using the fuzzy membership input with class membership desired-output during training procedure as proposed by several researches, we used the fuzzy membership input with fundamental binary desired-output. This can reduce the misunderstood training, decrease the training time and also improve the recognition ability. The system was tested on the Thai ten-numeral speech (0-9) recognition. The error rate for speaker-independent test achieved 9.2% compared to 14% error rate of conventional neural network system while the error rate of the system using class membership desired-output is quite high because of misunderstood training.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-409"
  },
  "wutiwiwatchai98b_icslp": {
   "authors": [
    [
     "Chai",
     "Wutiwiwatchai"
    ],
    [
     "Somchai",
     "Jitapunkul"
    ],
    [
     "Visarut",
     "Ahkuputra"
    ],
    [
     "Ekkarit",
     "Maneenoi"
    ],
    [
     "Sudaporn",
     "Luksaneeyanawin"
    ]
   ],
   "title": "Thai polysyllabic word recognition using fuzzy-neural network",
   "original": "i98_0350",
   "page_count": 4,
   "order": 411,
   "p1": "paper 0350",
   "pn": "",
   "abstract": [
    "In this research, the Fuzzy-Neural Network (fuzzy-NN) model was proposed for Speaker-Independent Thai polysyllabic word recognition. Various fuzzy membership functions on linguistic properties were used to convert exact features extracted from input speech to the fuzzy membership values. The fuzzy membership values were arranged to be new input vector of Multilayer Perceptron (MLP) neural network. The binary desired outputs were used during training. 70 Thai words consist of ten numerals, the others were single-syllable, double-syllable and triple-syllable, 20 words in each group, were used for system evaluation. In order to improve recognition accuracy, number of syllable and tonal level detected were conducted for speech preclassification. The Pi fuzzy membership function provided the best recognition accuracy among other functions; Trapezoidal, and Triangular function. Under an optimal condition, the achieved recognition error rates were 5.6% on dependent test and 6.7% on independent test, which were respectively 3.3% and 3.4% decreasing from the conventional Neural Network system.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-410"
  },
  "glaeser98_icslp": {
   "authors": [
    [
     "Axel",
     "Glaeser"
    ]
   ],
   "title": "Modular neural networks for low-complex phoneme recognition",
   "original": "i98_0133",
   "page_count": 4,
   "order": 412,
   "p1": "paper 0133",
   "pn": "",
   "abstract": [
    "We present a Modular Neural Network (MNN) for phoneme recognition within the framework of a hybrid system (neural networks and HMMs) for speakerindependent single word recognition. With this approach, we are taking the computational effort into account which is used as an additional criterion for assessing the system performance. The main idea of the proposed MNN is the distribution of the complexity for the phoneme classification task on a set of modules. Each of these modules is a single neural network which is characterized by its high degree of specialization. The number of interfaces, and therewith the possibilities for infiltering external acoustic-phonetic knowledge, increases for a modular architecture. Moreover, after the development of a suitable topology for the MNN, each of the modules can be optimized for its specific phoneme recognition task. This is done by detecting and pruning irrelevant input parameters and leads to a more efficient system in terms of memory and computational requirements.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-411"
  },
  "freitas98_icslp": {
   "authors": [
    [
     "Joao F. G. de",
     "Freitas"
    ],
    [
     "Sue E.",
     "Johnson"
    ],
    [
     "Mahesan",
     "Niranjan"
    ],
    [
     "Andrew H.",
     "Gee"
    ]
   ],
   "title": "Global optimisation of neural network models via sequential sampling-importance resampling",
   "original": "i98_0213",
   "page_count": 4,
   "order": 413,
   "p1": "paper 0213",
   "pn": "",
   "abstract": [
    "We propose a novel strategy for training neural networks using sequential Monte Carlo algorithms. This global optimisation strategy allows us to learn the probability distribution of the network weights in a sequential framework. It is well suited to applications involving on-line, nonlinear or non-stationary signal processing. We show how the new algorithms can outperform extended Kalman filter (EKF) training.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-412"
  },
  "rottland98_icslp": {
   "authors": [
    [
     "J√∂rg",
     "Rottland"
    ],
    [
     "Andre",
     "Ludecke"
    ],
    [
     "Gerhard",
     "Rigoll"
    ]
   ],
   "title": "Efficient computation of MMI neural networks for large vocabulary speech recognition systems",
   "original": "i98_0331",
   "page_count": 4,
   "order": 414,
   "p1": "paper 0331",
   "pn": "",
   "abstract": [
    "This paper describes, how to train Maximum Mutual Information Neural Networks (MMINN) in an efficient way, with a new topology. Large vocabulary speech recognition systems, based on a Hybrid MMI/connectionist HMM combination, have shown good performance on several tasks (RM and WSJ). MMINNs are trained to maximize the mutual information between the index of the winning output neuron (Winner-Takes-All network) and the phonetical class of the corresponding acoustic frame. One major problem of MMI-neural networks is the high computational effort, which is needed for the training of the neural networks. The computational effort is proportional to the input and output size of the neural network and to the number of training samples. This paper shows two approaches, that demonstrate, how these long training times can be reduced with very low or even no loss in recognition accuracy. This is achieved by the use of phonetical knowledge, to build a network topology based on phonetical classes.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-413"
  },
  "selouani98_icslp": {
   "authors": [
    [
     "Sid-Ahmed",
     "Selouani"
    ],
    [
     "Jean",
     "Caelen"
    ]
   ],
   "title": "Modular connectionist systems for identifying complex arabic phonetic features",
   "original": "i98_0358",
   "page_count": 4,
   "order": 415,
   "p1": "paper 0358",
   "pn": "",
   "abstract": [
    "This paper concerns the identification of Arabic macro-classes and phonetic features by systems using a hierarchy of neural networks. These systems are composed of sub-neural-networks (SNNs) carrying out binary discrimination sub-tasks. Two types of architecture are presented: serial structure of experts and parallel disposition of them. This mixture of experts is composed of typically time delay neural networks using a version of autoregressive backpropagation algorithm (AR-TDNN). These hierarchical configurations are confronted to a monolithic system using standard backpropagation learning procedure. The test database consists of 60 VCV utterances and 50 phrases pronounced by 6 Algerian native speakers. The parallel configuration achieved much fewer error rate (13% vs. 16% and 28%) than other architectures. The parallel mixture of experts is incorporated in a hybrid structure (HMM-SNN) in the order to enhance performances of standard HMMs. Identification results show that 10% reduction of error rate is obtained by the hybrid system.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-414"
  },
  "pham98b_icslp": {
   "authors": [
    [
     "Tuan",
     "Pham"
    ],
    [
     "Michael",
     "Wagner"
    ]
   ],
   "title": "Fuzzy-integration based normalization for speaker verification",
   "original": "i98_0953",
   "page_count": 4,
   "order": 416,
   "p1": "paper 0953",
   "pn": "",
   "abstract": [
    "Similarity or likelihood normalization techniques are important for speaker verification systems as they help to alleviate the variations in the speech signals. In the conventional normalization, the a priori probabilities of the cohort speakers are assumed to be equal. From this standpoint, we apply the theory of fuzzy measure and fuzzy integral to combine the likelihood values of the cohort speakers in which the assumption of equal a priori probabilities is relaxed. This approach replaces the conventional normalization term by the fuzzy integral which acts as a non-linear fusion of the similarity measures of an utterance assigned to the cohort speakers. We illustrate the performance of the proposed approach by testing the speaker verification system with both the conventional and the fuzzy algorithms using the commercial speech corpus TI46. The results in terms of the equal error rates show that the speaker verification system using the fuzzy integral is more flexible and more favorable than the conventional normalization method.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-415"
  },
  "shimodaira98_icslp": {
   "authors": [
    [
     "Hiroshi",
     "Shimodaira"
    ],
    [
     "Jun",
     "Rokui"
    ],
    [
     "Mitsuru",
     "Nakai"
    ]
   ],
   "title": "Improving the generalization performance of the MCE/GPD learning",
   "original": "i98_0795",
   "page_count": 4,
   "order": 417,
   "p1": "paper 0795",
   "pn": "",
   "abstract": [
    "A novel method to prevent the over-fitting effect and improve the generalization performance of the Minimum Classification Error (MCE) / Generalized Probabilistic Descent (GPD) learning is proposed. The MCE/GPD method, which is one of the newest discriminative-learning approaches proposed by Katagiri and Juang in 1992, results in better recognition performance in various areas of pattern recognition than the maximum-likelihood (ML) based approach where a posteriori probabilities are estimated. Despite its superiority in recognition performance, it still suffers from the problem of over-fitting to the training samples as it is with other learning algorithms. In the present study, a regularization technique is employed to the MCE method to overcome this problem. Feed-forward neural networks are employed as a recognition platform to evaluate the recognition performance of the proposed method. Recognition experiments are conducted on several sorts of datasets. The proposed method shows better generalization performance than the original one\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-416"
  },
  "kitazoe98_icslp": {
   "authors": [
    [
     "Tetsuro",
     "Kitazoe"
    ],
    [
     "Tomoyuki",
     "Ichiki"
    ],
    [
     "Sung-Ill",
     "Kim"
    ]
   ],
   "title": "Acoustic speech recognition model by neural net equation with competition and cooperation",
   "original": "i98_0965",
   "page_count": 4,
   "order": 418,
   "p1": "paper 0965",
   "pn": "",
   "abstract": [
    "The equation of neural nets for stereo vision is applied to speech recognition. We use Coupled Pattern Recognition (CPR) equation which has been shown to organize depth perception very well through competition and cooperation. We construct Gaussian probability density function for each phoneme from a number of training data. The input data to be recognized are compared to the pdf's and the similarity measures are obtained for each phoneme. The CPR equation develops neuron activities by receiving the similarity measures as input. A recognition is achieved when the activities arrive at a stable states. The recognition rates for 25 Japanese phoneme are 74.75% in average which is compared to 71.53% Hidden Markov Model. A certain technical improvement is applied to our neuron model, by dividing data of a phoneme into two part, one for the former frames, the other for the latter frames.A remarkable improvement is obtained with average recognition rate of 79.79%.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-417"
  },
  "ngan98_icslp": {
   "authors": [
    [
     "Julie",
     "Ngan"
    ],
    [
     "Aravind",
     "Ganapathiraju"
    ],
    [
     "Joseph",
     "Picone"
    ]
   ],
   "title": "Improved surname pronunciations using decision trees",
   "original": "i98_0384",
   "page_count": 4,
   "order": 419,
   "p1": "paper 0384",
   "pn": "",
   "abstract": [
    "Proper noun pronunciation generation is a particularly challenging problem in speech recognition since a large percentage of proper nouns often defy typical letter-to-sound conversion rules. In this paper, we present decision tree methods which outperform neural network techniques. Using the decision tree method, we have achieved an overall error rate of 45.5%, which is a 35% reduction over the previous techniques. Our best system is a binary decision tree that uses a context length of 3 and employs information gain ratio as the splitting rule.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-418"
  },
  "benitez98_icslp": {
   "authors": [
    [
     "M. Carmen",
     "Benitez"
    ],
    [
     "Antonio",
     "Rubio"
    ],
    [
     "Pedro",
     "Garc√≠a"
    ],
    [
     "Jesus",
     "Diaz-Verdejo"
    ]
   ],
   "title": "Word verification using confidence measures in speech recognition",
   "original": "i98_1082",
   "page_count": 4,
   "order": 420,
   "p1": "paper 1082",
   "pn": "",
   "abstract": [
    "In this work we propose a novel way of discriminating the words that are recognized by a speech recognition system as correctly or incorrectly detected words. The procedure consists of the extraction of a set of characteristics for each word. Utilizing these characteristics, we have built two classifiers: the first one is a vector quantizer, while the second one, though also a vector quantizer, was trained using adaptative technique learning (LVQ). The results obtained show an improvement in the performance of the recognizer achieved by reducing the number of insertions with no significant reduction in the correctly detected words.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-419"
  },
  "bernardis98_icslp": {
   "authors": [
    [
     "Giulia",
     "Bernardis"
    ],
    [
     "Herv√©",
     "Bourlard"
    ]
   ],
   "title": "Improving posterior based confidence measures in hybrid HMM/ANN speech recognition systems",
   "original": "i98_0318",
   "page_count": 4,
   "order": 421,
   "p1": "paper 0318",
   "pn": "",
   "abstract": [
    "In this paper we define and investigate a set of confidence measures based on hybrid Hidden Markov Model/Artificial Neural Network acoustic models. These measures are using the neural network to estimate the local phone posterior probabilities, which are then combined and normalized in different ways. Experimental results will show that the use of an appropriate duration normalization is very important to obtain good estimates of the phone and word confidences. The different measures are evaluated at the phone and word levels on both isolated word (PHONEBOOK) and continuous speech (BREF) recognition tasks. It will be shown that one of those confidence measures is well suited for utterance verification, and that (as one could expect) confidence measures at the word level perform better than those at the phone level. Finally, using the resulting approach on PHONEBOOK to rescore the N-best list is shown yielding a 34% decrease in word error rate.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-420"
  },
  "caminero98_icslp": {
   "authors": [
    [
     "Javier",
     "Caminero"
    ],
    [
     "Eduardo",
     "L√≥pez"
    ],
    [
     "Luis A.",
     "Hern√°ndez"
    ]
   ],
   "title": "Two-pass utterance verification algorithm for long natural numbers recognition",
   "original": "i98_0440",
   "page_count": 4,
   "order": 422,
   "p1": "paper 0440",
   "pn": "",
   "abstract": [
    "There are many Spontaneous Dialogue Recognition based applications like home-banking ones where long numbers recognition facilities are crucial to complete a request from the user. Rejection and Utterance Verification (UV) are difficult problems in these applications. In this contribution we improve our previously proposed UV procedure in order to increase the correction of recognition errors, to solve grammatical ambiguities from the user, and to make more efficient the rejection of misrecognized or out-of-vocabulary (OOV) utterances. In spite of the verification performance, the proposed algorithm complies with the real-time constrains which are mandatory in real applications. We evaluate our method and present recognition results from the long natural number recognition task of a real Data Driven application through the telephone line on a multilingual environment. Experimental results show that the proposed method obtains a significant reduction in terms of recognition errors and achieves an extraordinary low false acceptance rate in all cases for different languages.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-421"
  },
  "chen98c_icslp": {
   "authors": [
    [
     "Berlin",
     "Chen"
    ],
    [
     "Hsin-Min",
     "Wang"
    ],
    [
     "Lee-Feng",
     "Chien"
    ],
    [
     "Lin-Shan",
     "Lee"
    ]
   ],
   "title": "A*-admissible key-phrase spotting with sub-syllable level utterance verification",
   "original": "i98_0305",
   "page_count": 4,
   "order": 423,
   "p1": "paper 0305",
   "pn": "",
   "abstract": [
    "In this paper, we propose an A*-admissible key-phrase spotting framework, which needs little domain knowledge and is capable of extracting salient key-phrase fragments from an input utterance in real-time. There are two key features in our approach. Firstly, the acoustic models and the search framework are specially designed such that very high degree vocabulary flexibility can be achieved for any desired application tasks. Secondly, the search framework uses an efficient two-pass A* search to generate N-best key-phrase candidates and then several sub-syllable level verification functions are properly weighted and used to further improve the recognition accuracy. Experimental results show that the A*-admissible key-phrase spotting with sub-word level utterance method outperforms the baseline methods used in common approaches.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-422"
  },
  "fischer98_icslp": {
   "authors": [
    [
     "Volker",
     "Fischer"
    ],
    [
     "Yuqing",
     "Gao"
    ],
    [
     "Eric",
     "Janke"
    ]
   ],
   "title": "Speaker-independent upfront dialect adaptation in a large vocabulary continuous speech recognizer",
   "original": "i98_0233",
   "page_count": 4,
   "order": 424,
   "p1": "paper 0233",
   "pn": "",
   "abstract": [
    "Large vocabulary continuous speech recognition systems show a significant decrease in performance if a users pronunciation differs largely from those observed during system training. This can be considered as the main reason why most commercially available systems recommend - if not enforce - the individual end user to read an enrollment script for the speaker dependent reestimation of acoustic model parameters. Thus, the improvement of recognition rates for dialect speakers is an important issue both with respect to a broader acceptance and a more convenient or natural use of such systems. This paper compares different techniques that aim on a better speaker independent recognition of dialect speech in a large vocabulary continuous speech recognizer. The methods discussed comprise Bayesian adaptation and speaker clustering techniques and deal with both the availability and absence of dialect training material. Results are given for a case study that aims on the improvement of a German speech recognizer for Austrian speakers.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-423"
  },
  "gunawardana98_icslp": {
   "authors": [
    [
     "Asela",
     "Gunawardana"
    ],
    [
     "Hsiao-Wuen",
     "Hon"
    ],
    [
     "Li",
     "Jiang"
    ]
   ],
   "title": "Word-based acoustic confidence measures for large-vocabulary speech recognition",
   "original": "i98_0401",
   "page_count": 4,
   "order": 425,
   "p1": "paper 0401",
   "pn": "",
   "abstract": [
    "Word level confidence measures are of use in many areas of speech recognition. Comparing the hypothesized word score to the score of a 'filler' model has been the most popular confidence measure because it is highly efficient, and does not require a large amount of training data. This paper explores an extension of this technique which also compares the hypothesized word score to the scores of words that are commonly confused for it, while maintaining efficiency and the low demand for training data. The proposed method gives a 39% relative false accept rate reduction over the 'filler'- model baseline, at a false reject rate of 5%.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-424"
  },
  "gupta98_icslp": {
   "authors": [
    [
     "Sunil K.",
     "Gupta"
    ],
    [
     "Frank K.",
     "Soong"
    ]
   ],
   "title": "Improved utterance rejection using length dependent thresholds",
   "original": "i98_1040",
   "page_count": 4,
   "order": 426,
   "p1": "paper 1040",
   "pn": "",
   "abstract": [
    "In this paper, we propose to use an utterance length (duration) dependent threshold for rejecting an unknown input utterance with a general speech (garbage) model. A general speech model, comparing with more sophisticated anti-subword models, is a more viable solution to the utterance rejection problem for low-cost applications with stringent storage and computational constraints. However, the rejection performance using such a general model with a fixed, universal rejection threshold is in general worse than the anti-models with higher discriminations. Without adding complexities to the rejection algorithm, we propose to vary the rejection threshold according to the utterance length. The experimental results show that significant improvement in rejection performance can be obtained by using the proposed, length dependent rejection threshold over a fixed threshold. We investigate utterance rejection in a command phrase recognition task. The equal error rate, a good figure of merit for calibrating the performance of utterance verification algorithms, is reduced by almost 23% when the proposed length dependent threshold is used.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-425"
  },
  "ho98_icslp": {
   "authors": [
    [
     "Ching Hsiang",
     "Ho"
    ],
    [
     "Saeed",
     "Vaseghi"
    ],
    [
     "Aimin",
     "Chen"
    ]
   ],
   "title": "Bayesian constrained frequency warping HMMS for speaker normalisation",
   "original": "i98_0370",
   "page_count": 4,
   "order": 427,
   "p1": "paper 0370",
   "pn": "",
   "abstract": [
    "This paper presents a Bayesian constrained frequency warping technique. The Bayesian approach provides for inclusion of the prior information of the frequency warping parameter and for adjusting the search range in order to obtain the best warping factor dependent on HMMs. We introduce novel frequency warping (FWP) HMMs which are different warped versions of HMMs. Instead of frequency warping of the input speech we warp the spectrum of the HMMs. This is equivalent to HMMs which have both time and frequency warping capabilities. Experimentally FWP HMMs outperform the conventional constrained frequency warping approach. Furthermore, the best warping factor is estimated in two stages, a coarse stage followed by a fine stage. This method efficiently gauges the optimal warping factor and normalises the FWP HMMs.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-426"
  },
  "ida98_icslp": {
   "authors": [
    [
     "Masaki",
     "Ida"
    ],
    [
     "Ryuji",
     "Yamasaki"
    ]
   ],
   "title": "An evaluation of keyword spotting performance utilizing false alarm rejection based on prosodic information",
   "original": "i98_0159",
   "page_count": 4,
   "order": 428,
   "p1": "paper 0159",
   "pn": "",
   "abstract": [
    "In this paper, we describe our effort in developing new method of false alarm rejection for keyword spotting type of speech recognition system that we have developed about a year ago. This false alarm rejection uses prosodic similarities, and works as posterior rescore basis. In keyword spotting, there is always false alarm problem. Here, we propose a technique to reject those false alarms using prosodic features. In Japanese, prosodic information is expressed in intonation form, while may of other languages is using stress accents. Therefore, it is easy to calculate prosodic information using fundamental frequency, so called F0, in our language. In our new keyword spotting engine, we get result by combining two scores. One is phonetic score calculated by front engine, and the other is pitch score calculated by post engine described in this paper. We have accomplished 13points improvement on keyword recognition accuracy using this method.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-427"
  },
  "tran98c_icslp": {
   "authors": [
    [
     "Dieu",
     "Tran"
    ],
    [
     "Ken-ichi",
     "Iso"
    ]
   ],
   "title": "Predictive speaker adaptation and its prior training",
   "original": "i98_1089",
   "page_count": 4,
   "order": 429,
   "p1": "paper 1089",
   "pn": "",
   "abstract": [
    "In this paper, we propose a Predictive Speaker Adaptation technique (PSA) in which speaker dependent HMM(SD-HMM) for a new speaker is predicted using adaptation utterances and a speaker independent HMM(SI-HMM). The method requires a prior training in order to estimate the parameters of the prediction function. For this purpose, we first prepare many speaker's fully trained SD-HMMs and their adaptation utterances(same for all speakers). In addition many speaker-specific BW-HMMs are built from the SI-HMM by means of Baum-Welch re-estimation on the adaptation utterances. The model-pair SD-HMM and BW-HMM for each speaker is used as training example for the input and output of the prediction function to find the speaker-independent prediction parameters. During adaptation, estimation of the new speaker's SD-HMM is carried out from his BW-HMM with the predetermined parameters. 60,000-word recognition experiments reported a word error-rate reduction of 16% when only 10 adaptation words were used.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-428"
  },
  "meliani98_icslp": {
   "authors": [
    [
     "Rachida El",
     "M√©liani"
    ],
    [
     "Douglas",
     "O'Shaughnessy"
    ]
   ],
   "title": "Powerful syllabic fillers for general-task keyword-spotting and unlimited-vocabulary continuous-speech recognition",
   "original": "i98_0837",
   "page_count": 4,
   "order": 430,
   "p1": "paper 0837",
   "pn": "",
   "abstract": [
    "We choose to represent, unlike other teams, vocabulary words and out-vocabulary words with the same set of subword HMMs. Secondly we replace the classical one-phoneme transcription of fillers in the lexicon by a new, more powerful one-syllable transcription. As for the language model, the problem produced, in the case of unlimited-vocabulary continuous-speech recognition, by the lack of information on new words in the training corpus is solved through the use of the limited information we gathered on new words. The results obtained in general-task keyword spotting as well as unlimited-vocabulary continuous-speech recognition demonstrate the efficiency of the choice of a one-syllable transcription rather than a one-phoneme one. As for the results in unlimited-vocabulary continuous-speech recognition, the language model using information from words of frequency one is demonstrated to be a new promising method of determination of a language model for new words.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-429"
  },
  "pao98_icslp": {
   "authors": [
    [
     "Christine",
     "Pao"
    ],
    [
     "Philipp",
     "Schmid"
    ],
    [
     "James R.",
     "Glass"
    ]
   ],
   "title": "Confidence scoring for speech understanding systems",
   "original": "i98_0392",
   "page_count": 4,
   "order": 431,
   "p1": "paper 0392",
   "pn": "",
   "abstract": [
    "This research investigates the use of utterance-level features for confidence scoring. Confidence scores are used to accept or reject user utterances in our conversational weather information system. We have developed an automatic labeling algorithm based on a semantic frame comparison between recognized and transcribed orthographies. We explore recognition-based features along with semantic, linguistic, and application-specific features for utterance rejection. Discriminant analysis is used in an iterative process to select the best set of classification features for our utterance rejection sub-system. Experiments show that we can correctly reject over 60% of incorrectly understood utterances while accepting 98% of all correctly understood utterances.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-430"
  },
  "ramabhadran98_icslp": {
   "authors": [
    [
     "Bhuvana",
     "Ramabhadran"
    ],
    [
     "Abraham",
     "Ittycheriah"
    ]
   ],
   "title": "Phonological rules for enhancing acoustic enrollment of unknown words",
   "original": "i98_0534",
   "page_count": 4,
   "order": 432,
   "p1": "paper 0534",
   "pn": "",
   "abstract": [
    "Phonetic baseforms are the basic recognition units in most speech recognition systems. These baseforms are usually determined by linguists once a vocabulary is chosen and not modified thereafter. However, several applications, such as name dialing, require the user be able to add new words to the vocabulary. These new words are often names, or task-specific jargon, that have user-specific pronunciations. This paper describes a novel method for generating phonetic transcriptions (baseforms) of words based on acoustic evidence alone. It does not require any prior acoustic representation of the new word, is vocabulary independent, and uses phonological rules in a post processing stage to enhance the quality of the baseforms thus produced. Our experiments demonstrate the high decoding accuracies obtained when baseforms deduced using this approach are incorporated into our speech recognizer. Our experiments also compare the use of acoustic models that are trained on task-specific data with models trained for a general purpose ( to do digit, names, large vocabulary recognition, etc.), for generating phonetic transcriptions.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-431"
  },
  "setlur98_icslp": {
   "authors": [
    [
     "Anand R.",
     "Setlur"
    ],
    [
     "Rafid A.",
     "Sukkar"
    ]
   ],
   "title": "Recognition-based word counting for reliable barge-in and early endpoint detection in continuous speech recognition",
   "original": "i98_0168",
   "page_count": 4,
   "order": 433,
   "p1": "paper 0168",
   "pn": "",
   "abstract": [
    "In this paper, we present a word counting method that enables speech recognition systems to perform reliable barge-in detection and also make a fast and accurate determination of end of speech. This is achieved by examining partial recognition hypotheses and imposing certain \"word stability\" criteria. Typically, a voice activity detector is used for both barge-in detection and end of speech determination. We propose augmenting the voice activity detector with this more reliable recognition-based method. Experimental results for a connected digit task show that this approach is more robust for supporting barge-in since it is less prone to interrupting the announcement when extraneous speech input is encountered. Also, by using the early endpoint decision criterion, average response times are sped up 75% for this connected digit task.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-432"
  },
  "westphal98_icslp": {
   "authors": [
    [
     "Martin",
     "Westphal"
    ],
    [
     "Tanja",
     "Schultz"
    ],
    [
     "Alex",
     "Waibel"
    ]
   ],
   "title": "Linear discriminant - a new criterion for speaker normalization",
   "original": "i98_0755",
   "page_count": 4,
   "order": 434,
   "p1": "paper 0755",
   "pn": "",
   "abstract": [
    "In Vocal Tract Length Normalization (VTLN) a linear or nonlinear frequency transformation compensates for different vocal tract lengths. Finding good estimates for the speaker specific warp parameters is a critical issue. Despite good results using the Maximum Likelihood criterion to find parameters for a linear warping, there are concerns using this method. We searched for a new criterion that enhances the interclass separability in addition to optimizing the distribution of each phonetic class. Using such a criterion Linear Discriminant Analysis determines a linear transformation in a lower dimensional space. For VTLN, we keep the dimension constant and warp the training samples of each speaker such that the Linear Discriminant is optimized. Although that criterion depends on all training samples of all speakers it can iteratively provide speaker specific warp factors. We discuss how this approach can be applied in speech recognition and present first results on two different recognition tasks.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-433"
  },
  "williams98_icslp": {
   "authors": [
    [
     "Gethin",
     "Williams"
    ],
    [
     "Steve",
     "Renals"
    ]
   ],
   "title": "Confidence measures derived from an acceptor HMM",
   "original": "i98_0644",
   "page_count": 4,
   "order": 435,
   "p1": "paper 0644",
   "pn": "",
   "abstract": [
    "In this paper we define a number of confidence measures derived from an acceptor HMM and evaluate their performance for the task of utterance verification using the North American Business News (NAB) and Broadcast News (BN) corpora. Results are presented for decodings made at both the word and phone level which show the relative profitability of rejection provided by the diverse set of confidence measures. The results indicate that language model dependent confidence measures have reduced performance on BN data relative to that for the more grammatically constrained NAB data. An explanation linking the observations that rejection is more profitable for noisy acoustics, for a reduced vocabulary and at the phone level is also given.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-434"
  },
  "wu98c_icslp": {
   "authors": [
    [
     "Chung-Hsien",
     "Wu"
    ],
    [
     "Yeou-Jiunn",
     "Chen"
    ],
    [
     "Yu-Chun",
     "Hung"
    ]
   ],
   "title": "Telephone speech multi-keyword spotting using fuzzy search algorithm and prosodic verification",
   "original": "i98_0218",
   "page_count": 4,
   "order": 436,
   "p1": "paper 0218",
   "pn": "",
   "abstract": [
    "In this paper a fuzzy search algorithm is proposed to deal with the recognition error for telephone speech. Since the prosodic information is a very special and important feature for Mandarin speech, we integrate the prosodic information into keyword verification. For multi-keyword detection, we define a keyword relation and a weighting function for reasonable keyword combinations. In the keyword recognizer, 94 INITIAL and 38 FINAL context-dependent Hidden Markov Models (HMM's) are used to construct the phonetic recognizer. For prosodic verification, a total of 175 context-dependent HMM's and five anti-prosodic HMM's are used. In this system, 1275 faculty names and department names are selected as the keywords. Using a test set of 3595 conversional speech utterance from 37 speakers (21 male, 16 female), the proposed fuzzy search algorithm and prosodic verification can reduce the error rate from 17.64% to 11.29% for multiple keywords embedded in non-keyword speech.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-435"
  },
  "yamashita98_icslp": {
   "authors": [
    [
     "Yoichi",
     "Yamashita"
    ],
    [
     "Toshikatsu",
     "Tsunekawa"
    ],
    [
     "Riichiro",
     "Mizoguchi"
    ]
   ],
   "title": "Topic recognition for news speech based on keyword spotting",
   "original": "i98_0023",
   "page_count": 4,
   "order": 437,
   "p1": "paper 0023",
   "pn": "",
   "abstract": [
    "This paper describes topic identification for Japanese TV news speech based on the keyword spotting technique. Three thousands of nouns are selected as keywords which contribute to topic identification, based on criterion of mutual information and a length of the word. This set of the keywords identified the correct topic for 76.3% of articles from newspaper text data. Further, we performed keyword spotting for TV news speech and identified the topics of the spoken message by calculating possibilities of the topics in terms of an acoustic score of the spotted word and a topic probability of the word. In order to neutralize effect of false alarms, bias of the topics in the keyword set is removed. Topic identification rate is 66.5% assuming that identification is correct if the correct topic is included in the top three topics. The removal of the bias improved the identification rate by 6.1%.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-436"
  },
  "nooteboom98_icslp": {
   "authors": [
    [
     "Sieb G.",
     "Nooteboom"
    ],
    [
     "Meinou van",
     "Dijk"
    ]
   ],
   "title": "Heads and tails in word perception: evidence for `early-to-late' processing in listening and reading",
   "original": "i98_0117",
   "page_count": 4,
   "order": 438,
   "p1": "paper 0117",
   "pn": "",
   "abstract": [
    "Sequential models of word perception assigned a very special role to word onsets. This accounted in a natural way for evidence that lexical access is easier from word beginnings than from word endings, a property speech perception shares with reading. Sequential models badly failed on other scores, however. More recent competition models seem to give equal weight to stimulus information, independent of position within the word. The present word recognition experiment aimed at testing the hypothesis that, other things being equal, mismatches are more damaging to word perception at onsets than at offsets of embedded words, both in speech perception and in reading. Results show that word recognition is quite good in all conditions, even when word onsets are mutilated, and mis-timed, thus lending support to competition models. Yet, the results also show that lexical access is modulated by some early-to-late or left-to-right component, as if human word perception displays a mixture of sequential and competition processing.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-437"
  },
  "riele98_icslp": {
   "authors": [
    [
     "Saskia te",
     "Riele"
    ],
    [
     "Hugo",
     "Quen√©"
    ]
   ],
   "title": "Evidence for early effects of sentence context on word segmentation",
   "original": "i98_0326",
   "page_count": 4,
   "order": 439,
   "p1": "paper 0326",
   "pn": "",
   "abstract": [
    "The present paper focuses on the segmentation of two-word phrases containing two closely competing lexical hypotheses. It is hypothesized that the bottom-up information, which also includes a mechanism called the Possible-Word Constraint, is explored first in segmenting these phrases. Non-sensory sentential information influences this process at a later stage and only shows an effect if the bottom-up information does not lead to one dominating interpretation. The results of the present experiment show that beside the acoustic information listeners can and do use contextual information at a relatively early moment, at which the two possible segmentations are both still active and the bottom-up information has not yet suppressed the acoustically inconsistent interpretation. This effect became apparent, since disambiguating bottom-up information arrived relatively late in the stimulus phrases. Hence, it was concluded that both sensory and non-sensory information are employed to affect activation levels of competing lexical hypotheses at an early moment.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-438"
  },
  "quene98_icslp": {
   "authors": [
    [
     "Hugo",
     "Quen√©"
    ],
    [
     "Maya van",
     "Rossum"
    ],
    [
     "Mieke van",
     "Wijck"
    ]
   ],
   "title": "Assimilation and anticipation in word perception",
   "original": "i98_0113",
   "page_count": 4,
   "order": 440,
   "p1": "paper 0113",
   "pn": "",
   "abstract": [
    "Words in connected speech are often assimilated to subsequent words. Some property of that upcoming word may then be determined in advance; these advance assimilatory cues may facilitate perception of that word. A gating experiment was conducted in Dutch, studying anticipatory voice assimilation between plosives, in 24 two-word combinations. In Dutch, voicing in a word-final plosive can only be caused by anticipatory assimilation to the next, voiced initial plosive, e.g. \"rie[db]lint\". Voiced and unvoiced variants of final and initial plosives were cross-spliced. Responses for assimilated, voiced-final stimuli show a strong bias to voiced-initial responses, as predicted. Even at longer gates in the hybrid condition \"rie[dp]lint\", after hearing the unvoiced initial plosive, listeners often came up with a voiced-initial response, with high confidence. Hence, advance phonological 'voiced-initial' cues were often stronger than acoustic 'unvoiced-initial' cues. These gating results suggest that listeners use advance assimilatory cues in word perception.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-439"
  },
  "kelly98_icslp": {
   "authors": [
    [
     "M. Louise",
     "Kelly"
    ],
    [
     "Ellen Gurman",
     "Bard"
    ],
    [
     "Catherine",
     "Sotillo"
    ]
   ],
   "title": "Lexical activation by assimilated and reduced tokens",
   "original": "i98_0565",
   "page_count": 4,
   "order": 441,
   "p1": "paper 0565",
   "pn": "",
   "abstract": [
    "Running speech contains abundant assimilated and phonologically reduced tokens, but there is considerable debate about how such varied pronunciations disrupt access to the corresponding words in the listeners' mental lexicons. While previous studies have examined the effects of carefully produced or electronically edited reductions, we present two experiments which compare cross-modal repetition priming for lexical decisions by more reduced spontaneous forms and less reduced read forms of the same words uttered by the same speakers in the same phrases. Though less priming is found for the more reduced spontaneous tokens, both versions of words produce significant priming effects, whether the majority of stimuli are taken from spontaneous speech (Experiment 1) or from read speech (Experiment 2). Priming is more robust if tokens themselves contain the context licensing reduction.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-440"
  },
  "akagi98b_icslp": {
   "authors": [
    [
     "Masato",
     "Akagi"
    ],
    [
     "Mamoru",
     "Iwaki"
    ],
    [
     "Tomoya",
     "Minakawa"
    ]
   ],
   "title": "Fundamental frequency fluctuation in continuous vowel utterance and its perception",
   "original": "i98_0027",
   "page_count": 4,
   "order": 442,
   "p1": "paper 0027",
   "pn": "",
   "abstract": [
    "This paper reports how rapid fluctuations of fundamental frequencies in continuously uttered vowels influence vowel quality and shows that vowel qualities with various fundamental frequency fluctuations can be discriminated perceptually. For this purpose, electroglottographs (EGGs) of vowels uttered by nine males were obtained using Laryngograph, and fundamental frequencies with rapid fluctuations were estimated from them. Analyzing forty-five estimated fundamental frequencies, they can be classified into four groups. Moreover, psychoacoustic experiments, with five subjects, evaluating voice quality by multidimensional scaling (MDS) showed that voice quality of the synthesized speech using the fundamental frequencies of the groups was completely discriminable and there was a distinctive frequency band of fundamental frequency fluctuation for specifying each group perceptually.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-441"
  },
  "amano98_icslp": {
   "authors": [
    [
     "Shigeaki",
     "Amano"
    ],
    [
     "Tadahisa",
     "Kondo"
    ]
   ],
   "title": "Estimation of mental lexicon size with word familiarity database",
   "original": "i98_0015",
   "page_count": 4,
   "order": 443,
   "p1": "paper 0015",
   "pn": "",
   "abstract": [
    "A familiarity database was developed for about 80,000 Japanese words of which familiarity scores were rated by 32 Japanese adults using a 7-point scale in auditory, visual, and audio-visual modalities. Auditory, visual, and audio-visual stimulus words were selected from the database according to their word familiarity for size estimation of the mental lexicon. Sixty Japanese adults participated in a two-alternative forced-choice task (Know-Don't know) for the stimulus words. The size of the mental lexicon was estimated as the number of words of which familiarity is above a particular word corresponding to 50% point on the fitted logistic curve to \"know\"-response probability of the stimulus words. The estimated size was about 68,000 for auditory words, and about 66,000 both for visual and audio-visual words when homophones and homographs were included. The results suggest that very small difference in the mental lexicon size among modalities.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-442"
  },
  "aylett98b_icslp": {
   "authors": [
    [
     "Matthew",
     "Aylett"
    ],
    [
     "Alice",
     "Turk"
    ]
   ],
   "title": "Vowel quality in spontaneous speech: what makes a good vowel?",
   "original": "i98_0824",
   "page_count": 4,
   "order": 444,
   "p1": "paper 0824",
   "pn": "",
   "abstract": [
    "Clear speech is characterised by longer segmental durations and less target undershoot which results in more extreme spectral features. This paper deals with the clarity of vowels produced in spontaneous speech in a large corpus of task-oriented dialogues. We present an automatic technique for measuring vowel clarity on the basis of a vowel's spectral characteristics. This technique was evaluated using a perceptual test. Subjects rated the 'goodness' of vowels with different spectral characteristics with controlled duration and amplitude and these results were compared with an automatic rating. Results indicated that although agreement between subjects and the automatic measurement was poor it was as poor as the agreement between subjects. On the basis of these results we address the following questions: 1. Can subjects reliably judge the clarity of vowels excerpted from spontaneous speech without duration cues? 2. Can a statistical model reliably predict the subjects' response to such vowels?\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-443"
  },
  "neagu98_icslp": {
   "authors": [
    [
     "Adrian",
     "Neagu"
    ],
    [
     "G√©rard",
     "Bailly"
    ]
   ],
   "title": "Cooperation and competition of burst and formant transitions for the perception and identification of French stops",
   "original": "i98_1009",
   "page_count": 4,
   "order": 445,
   "p1": "paper 1009",
   "pn": "",
   "abstract": [
    "In this paper, we study the influence of the vocalic context on the perception and automatic recognition of stops. In a previous perception experiment [1] using conflicting cues stimuli, we have shown that place of articulation cued by formant transitions may be overwritten by the place cued by the burst. This effect is inversely proportional to the vowel aperture. Here we give special attention to /i/ context where nor burst, nor formant transitions seem to carry rich information on place of articulation. We present here automatic recognition experiments that confirm perception results. Taking into account both segments increase identification rates, early fusion of segmental cues performs best and most errors come from the front unrounded vocalic context. We introduce the \"burst characteristic frequency\" (BF) that palliates for the poor discriminative power of the traditional cues in the front context. Moreover we present perception results showing the perceptual relevance of BF.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-444"
  },
  "bonneau98_icslp": {
   "authors": [
    [
     "Anne",
     "Bonneau"
    ],
    [
     "Yves",
     "Laprie"
    ]
   ],
   "title": "The effect of modifying formant amplitudes on the perception of French vowels generated by copy synthesis",
   "original": "i98_0260",
   "page_count": 4,
   "order": 446,
   "p1": "paper 0260",
   "pn": "",
   "abstract": [
    "With synthetic stimuli copied from natural vowels and including up to five formants, we investigated some transformations of the perceived identity of vowels by means of modifications of formant amplitude levels. Synthetic stimuli were generated by means of a new method of copy synthesis. With stimuli copied from /u/ we found that, despite the presence of F2, it is possible to transform the timbre of /u/ into that of a front vowel only by raising the amplitude level of F3 and higher formants. We also analysed how the timbre of the vowels /i/ and /y/ changed as a function of the level of F2, F3 and F4, and showed how sensitive was the timbre of the vowel /i/ to the decrease of the level of F3 and F4. Such transformations, realized with stimuli very close to natural vowels, reinforce the importance of formant amplitude levels in some vocalic distinctions.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-445"
  },
  "chen98d_icslp": {
   "authors": [
    [
     "Hsuan-Chih",
     "Chen"
    ],
    [
     "Michael C. W.",
     "Yip"
    ],
    [
     "Sum-Yin",
     "Wong"
    ]
   ],
   "title": "Segmental and tonal processing in Cantonese",
   "original": "i98_0660",
   "page_count": 4,
   "order": 447,
   "p1": "paper 0660",
   "pn": "",
   "abstract": [
    "In a tone language, such as Cantonese, both segmental and tonal distinctions between words are pervasive. However, previous work in Cantonese has demonstrated that in speeded-response tasks, tone is more likely to be misprocessed than is segmental structure. The present study examined whether this tone disadvantage would also hold after the initial auditory processing of a syllable had been done. Cantonese listeners were asked to perform same-different judgments on two sequentially presented open syllables along a specific dimension (i.e., onset, rime, tone, or the whole syllable) according to an instruction which was visually presented at the acoustic offset of the second syllable. Manipulating whether the difference between two syllables was in onset, rime, or tone resulted in equally robust effects across the various decision tasks on performance, indicating that tone functions as effectively as segmental structure in spoken-word processing once the related information of a syllable is encoded.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-446"
  },
  "yip98_icslp": {
   "authors": [
    [
     "Michael C. W.",
     "Yip"
    ],
    [
     "Po-Yee",
     "Leung"
    ],
    [
     "Hsuan-Chih",
     "Chen"
    ]
   ],
   "title": "Phonological similarity effects in Cantonese spoken-word processing",
   "original": "i98_0661",
   "page_count": 3,
   "order": 448,
   "p1": "paper 0661",
   "pn": "",
   "abstract": [
    "A Cantonese experiment is described in which the shadowing of spoken targets as a function of phonological similarity to either a succeeding prime (backward priming) or a preceding prime (forward priming) is investigated. In the backward priming conditions, alternations of onset, rime, or tone between prime and target produced inhibition, whereas in the forward priming conditions, alternations of tone led to facilitation. The results are discussed in terms of the processing and memory of Cantonese syllables.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-447"
  },
  "damper98_icslp": {
   "authors": [
    [
     "Bob I.",
     "Damper"
    ],
    [
     "Steve R.",
     "Gunn"
    ]
   ],
   "title": "On the learnability of the voicing contrast for initial stops",
   "original": "i98_0843",
   "page_count": 4,
   "order": 449,
   "p1": "paper 0843",
   "pn": "",
   "abstract": [
    "The categorical perception (CP) of syllable-initial stop consonants has been intensively studied using psychophysical procedures over many decades. However, computational models consisting of an auditory `front end' and a learning system as a `back end' convincingly mimic the essentials of CP. Unlike real listeners, such models can be systematically manipulated to uncover the basis of their categorisations. In this paper, we explore the use of modern inductive learning techniques in simulating CP.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-448"
  },
  "cerrato98_icslp": {
   "authors": [
    [
     "Loredana",
     "Cerrato"
    ],
    [
     "Mauro",
     "Falcone"
    ]
   ],
   "title": "Acoustic and perceptual characteristic of Italian stop consonants",
   "original": "i98_0463",
   "page_count": 4,
   "order": 450,
   "p1": "paper 0463",
   "pn": "",
   "abstract": [
    "We report the results of a study carried out to analyse the acoustic and perceptual characteristics of Italian stop consonants. The aim of this study is twofold: give an acoustical description of Italian stops and investigate which are the perceptual cues relative to their place of articulation. From the acoustic point of view we report: the measurements relative to the length of the whole consonant and of its release burst; the F1 and F2 of the following vowel measured at the beginning of it. Moreover we counted the presence of the release burst and we tried to describe its acoustical characteristics in terms of the spectral structure. From the perceptual point of view we report the results of three perceptual tests that we run with the aim of evaluating whether the release burst or the formant transitions are more relevant for the perception of Italian stop consonants' place of articulation.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-449"
  },
  "fernandez98_icslp": {
   "authors": [
    [
     "Santiago",
     "Fern√°ndez"
    ],
    [
     "Sergio",
     "Feij√≥o"
    ],
    [
     "Ramon",
     "Balsa"
    ],
    [
     "Nieves",
     "Barros"
    ]
   ],
   "title": "Acoustic cues for the auditory identification of the Spanish fricative /f/",
   "original": "i98_0451",
   "page_count": 4,
   "order": 451,
   "p1": "paper 0451",
   "pn": "",
   "abstract": [
    "This study deals with the distinction of the fricative noises of the spanish fricatives /th/ and /f/. Previous studies revealed that fricative noises of both phonemes are perceptually similar, auditory identification being significantly dependent on contextual effects: /f/ in the /u/ context is well identified (about 85% correct identification rate), while in the /e/ context identification is much lower (about 60%). Identification of /th/ is low for every vocalic context (about 60%). These effects were identical for both Hypo and Hyper forms of speech. The objective of this paper is to determine which acoustic properties of /f/ in the /u/ context make it a well defined phoneme for the two different forms of speech. We conclude that the cues for the identification of the isolated fricative noise of /f/ seem to be in the low frequency region of the spectrum.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-450"
  },
  "fernandez98b_icslp": {
   "authors": [
    [
     "Santiago",
     "Fern√°ndez"
    ],
    [
     "Sergio",
     "Feij√≥o"
    ],
    [
     "Ramon",
     "Balsa"
    ],
    [
     "Nieves",
     "Barros"
    ]
   ],
   "title": "Recognition of vowels in fricative context.",
   "original": "i98_0452",
   "page_count": 4,
   "order": 452,
   "p1": "paper 0452",
   "pn": "",
   "abstract": [
    "The role of fricative context on vowel recognition in a series of FV syllables being part of natural Spanish words is investigated. Perceptual tests were carried out to assess the recognition of vowels in fricative context, in two conditions: 1) Isolated vowel; 2) Fricative noise + vowel. Analysis of results show that adding the fricative noise improves the recognition of the vowel, while the acoustic analysis reveal that the distribution of the vowels is affected by fricative context. A possible explanation for this improvement, i.e. the coarticulatory influence of the vowel on the fricative, was investigated. The results indicate that coarticulation cannot explain that improvement, since only 7.7% of the cases which improve when the fricative is added, show a clear influence of the vowel on the fricative.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-451"
  },
  "fernandez98c_icslp": {
   "authors": [
    [
     "Santiago",
     "Fern√°ndez"
    ],
    [
     "Sergio",
     "Feij√≥o"
    ],
    [
     "Plinio",
     "Almeida"
    ]
   ],
   "title": "Voicing affects perceived manner of articulation.",
   "original": "i98_0453",
   "page_count": 4,
   "order": 453,
   "p1": "paper 0453",
   "pn": "",
   "abstract": [
    "The perception of voiced fricatives by native speakers of a language which lacks those phonemes is studied in this paper. Brasilian portuguese and Galician languages were chosen because they are historically related. A forced choice test reveals that listeners correctly perceive the place of articulation of the voiced fricatives. In order to examine whether the perception of fricative manner can be overridden by the voicing characteristics an open test was carried out. Listeners perceive voiced fricatives as a voiced phoneme with different manner of articulation and similar place of articulation or as its voiceless counterpart, depending on whether vocal-fold vibration extends over the whole obstruent interval or not. Results are discussed in terms of both historical phonetic changes and second language acquisition.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-452"
  },
  "hazan98_icslp": {
   "authors": [
    [
     "Valerie",
     "Hazan"
    ],
    [
     "Andrew",
     "Simpson"
    ],
    [
     "Mark",
     "Huckvale"
    ]
   ],
   "title": "Enhancement techniques to improve the intelligibility of consonants in noise : speaker and listener effects",
   "original": "i98_0487",
   "page_count": 4,
   "order": 454,
   "p1": "paper 0487",
   "pn": "",
   "abstract": [
    "The aim of our work is to increase the intelligibility of speech in noise by modifying regions of the signal that contain acoustic cues to consonant identity in order to make it more resistant to subsequent degradation. 36 vowel-consonant-vowel stimuli were recorded by four untrained speakers. The vowel onset/offset and consonant constriction/occlusion regions were selectively amplified and stimuli were presented to listeners in a background of noise (0 dB SNR). Enhanced tokens from all speakers were significantly more intelligible than natural tokens and the improvement was greater for the initially least intelligible speakers. Speech material for two speakers was then presented to Japanese and Spanish learners of English and controls. For all groups, the enhanced consonants were more intelligible. Error patterns were related to the 'distance' between the consonantal systems of the listeners' L1 and L2. These results demonstrate the robustness of our enhancement techniques across speaker and listener types.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-453"
  },
  "jian98b_icslp": {
   "authors": [
    [
     "Fran H. L.",
     "Jian"
    ]
   ],
   "title": "Boundaries of perception of long tones in taiwanese speech",
   "original": "i98_0145",
   "page_count": 4,
   "order": 455,
   "p1": "paper 0145",
   "pn": "",
   "abstract": [
    "In this work we set out to investigate the fundamental frequency boundaries of perception of the Taiwanese long tones. We are interested in how the variations in fundamental frequency affect the perception of linguistic tones in Taiwanese speech. Our investigation is adopted from similar studies of tones in Mandarin speech. As opposed to Mandarin tones that can be perceived with little difficulty the seven Taiwanese tones have a more subtle structure and are consequently harder to perceive successfully. The experimental results in this paper allow us to quantify these perceptual boundaries. The experiments consisted of a perception test involving over 150 Taiwanese subjects where the task involved identifying the tone of the words played back in a random sequence. The stimuli consisted of a set of tone pairs and a selection of intermediate tone words obtained by linearly interpolating between the words of the tone pairs.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-454"
  },
  "kato98_icslp": {
   "authors": [
    [
     "Hiroaki",
     "Kato"
    ],
    [
     "Minoru",
     "Tsuzaki"
    ],
    [
     "Yoshinori",
     "Sagisaka"
    ]
   ],
   "title": "Effects of phonetic quality and duration on perceptual acceptability of temporal changes in speech",
   "original": "i98_0411",
   "page_count": 4,
   "order": 456,
   "p1": "paper 0411",
   "pn": "",
   "abstract": [
    "To establish a perceptually valid rule for the durational control of synthetic speech, it is necessary to know the degree to which a given temporal error or distortion is acceptable to human listeners. Two perceptual experiments were conducted to estimate the acceptability of modifications in either vocalic or consonantal durations as a function of two attributes of the modified portions, i.e., the phonetic quality and the original (unmodified) duration. The results showed that the listeners' acceptable modification ranges were narrowest for vowels, and widest for voiceless fricatives and silent closures, with nasals in between. They were also narrower for those portions with shorter base durations. The effect of the original duration was larger for the vowel stimuli than for the voiceless fricative stimuli. The perceptual mechanism mediating these results is discussed with regard to the dependency of the listeners' temporal sensitivity on the stimulus loudness and base duration. [Re: http://www.hip.atr.co.jp/~kato/single_duration/]\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-455"
  },
  "kiefte98_icslp": {
   "authors": [
    [
     "Michael",
     "Kiefte"
    ],
    [
     "Terrance M.",
     "Nearey"
    ]
   ],
   "title": "Dynamic vs. static spectral detail in the perception of gated stops",
   "original": "i98_0898",
   "page_count": 4,
   "order": 457,
   "p1": "paper 0898",
   "pn": "",
   "abstract": [
    "In order to assess the importance of dynamic spectral information within the first few milliseconds following oral release for the identification of prevocalic stop consonants, 23.75 ms gated CV syllables were presented to listeners for identification. In addition to these, subjects were presented with the same tokens reconstructed from their minimum phase decomposition such that they have the same long-term power spectrum as their original counterparts, but with differing internal dynamic spectral detail. Subjects' results from this experiment were then modelled with logistic regression analysis using mel cepstral coefficients with and without dynamic spectral information encoded in order to demonstrate the effect that reduced temporal information has in the context of automatic classification. Preliminary results from this experiment show that some dynamic spectral detail is used by listeners even for very short stimuli. We conclude that models of speech perception must take spectral variation over very short time frames into account.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-456"
  },
  "otake98_icslp": {
   "authors": [
    [
     "Takashi",
     "Otake"
    ],
    [
     "Kiyoko",
     "Yoneyama"
    ]
   ],
   "title": "Phonological units in speech segmentation and phonological awareness",
   "original": "i98_0035",
   "page_count": 4,
   "order": 458,
   "p1": "paper 0035",
   "pn": "",
   "abstract": [
    "This paper explores the relationship between phonological units in speech segmentation and phonological awareness by investigating Japanese Brazilians living in Japan. The first experiment investigated the size of the phonological unit in speech segmentation using the Japanese materials and methodology in Otake et al. (1993). As for French subjects in the earlier study, the miss rates showed an effect of syllabic segmentation, suggesting that the Japanese Brazilians segmented Japanese into syllables. The second experiment investigated phonological units in phonological awareness, using a mid-chunk-unit search task in which subjects were asked to identify the middle unit within a word. 96% of the mid-chunk unit choices were syllable-based. The results of the two experiments suggest that Japanese Brazilians exploit syllables both as a speech segmentation unit and as a unit to represent within-word structure.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-457"
  },
  "shriberg98_icslp": {
   "authors": [
    [
     "Elizabeth",
     "Shriberg"
    ],
    [
     "Andreas",
     "Stolcke"
    ]
   ],
   "title": "How far do speakers back up in repairs? a quantitatve model",
   "original": "i98_0058",
   "page_count": 4,
   "order": 459,
   "p1": "paper 0058",
   "pn": "",
   "abstract": [
    "Speakers frequently retrace one or more words when continuing after a break in fluency. Syntactic principles constrain the points from which speakers retrace; however syntactic principles do not provide predictions about the relative usage of different allowable retrace points. Such predictions are useful for automatic processing of repairs in speech technology, particularly if they use information readily available to a speech recognizer. We propose a quantitative model that predicts the overall distribution of retrace lengths in a large corpus of spontaneous speech, based only on word position. The model has two components: (1) a constant, position-independent probability for extending a retrace by one more word; and (2) a position-dependent probability to \"skip\" to the beginning of the sentence. Results have implications for modeling repairs in speech applications and constrain explanatory models in psycholinguistics.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-458"
  },
  "steinhauer98_icslp": {
   "authors": [
    [
     "Karsten",
     "Steinhauer"
    ],
    [
     "Kai",
     "Alter"
    ],
    [
     "Angela D.",
     "Friederici"
    ]
   ],
   "title": "Don't blame it (all) on the pause: further ERP evidence for a prosody-induced garden-path in running speech",
   "original": "i98_0147",
   "page_count": 4,
   "order": 460,
   "p1": "paper 0147",
   "pn": "",
   "abstract": [
    "This paper investigates the prosodic relevance of a pause which, along with other prosodic parameters, served to indicate an Intonational Phrase (IPh) boundary. Event-related brain potentials (ERPs) were recorded while subjects listened to both intact and altered German Early and Late Closure (EC/LC) sentences. The EC sentences were prosodically highly accepted and well comprehended even when the original pause at the boundary position was removed. Furthermore, a reversed garden-path (initial EC preference in LC sentences) was successfully induced by a false IPh boundary irrespective of whether the pause was present or not. The ERP patterns disclosed the on-line processing of simple and garden-path sentences in more detail. The data clearly demonstrate that in the presence of other prosodic parameters pause insertion is a completely dispensable cue for boundary marking. The ERP technique proved to be superior to behavioral on-line measures as data collection does not interrupt speech presentation.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-459"
  },
  "vroomen98_icslp": {
   "authors": [
    [
     "Jean",
     "Vroomen"
    ],
    [
     "Beatrice de",
     "Gelder"
    ]
   ],
   "title": "The role of stress for lexical selection in dutch",
   "original": "i98_0348",
   "page_count": 3,
   "order": 461,
   "p1": "paper 0348",
   "pn": "",
   "abstract": [
    "In the present study, we examined whether stress constrains the number of activated lexical candidates. In a phoneme monitoring task, we used Dutch carrier words that start in their citation form with a reduced vowel (denoted as @), but which can also be produced with an unreduced vowel. For example, a word such as frequent (meaning frequent) can be pronounced as fr@QUENT or freQUENT. We examined whether mis-stressing these words had an effect on the activation of their lexical representation. Twenty subjects detected a target phoneme (e.g., the 't') in fr@QUENT, freQUENT, FR@quent, or FREquent; stress denoted in capitals. Results showed that target phonemes in words were reacted faster than in pseudowords, but neither stress, nor the nature of the vowel had an effect on the size of lexical effect. This confirms that stress is not part of the lexical input representation.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-460"
  },
  "tuomainen98_icslp": {
   "authors": [
    [
     "Jyrki",
     "Tuomainen"
    ],
    [
     "Jean",
     "Vroomen"
    ],
    [
     "Beatrice de",
     "Gelder"
    ]
   ],
   "title": "The perception of stressed syllables in finnish",
   "original": "i98_0760",
   "page_count": 4,
   "order": 462,
   "p1": "paper 0760",
   "pn": "",
   "abstract": [
    "The effect of word level prominence on detection speed of word boundaries in Finnish was investigated in two word spotting experiments. The results showed that the perceived stress was not a function of the fundamental frequency (F0) difference between the preceding syllable and the first syllable of the target word. Given the fast response times, the results suggest that subjects perceived in both experiments the first syllable of the target as stressed. This seems to indicate that when words are recognized in continuous speech the acoustic cues in the F0 contour signaling prominence may not be computed relative to the prominence of neighboring syllables. Instead, we hypothesize that subjects may be sensitive to a local pitch movement indicating change in the F0 slope.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-461"
  },
  "yamakawa98_icslp": {
   "authors": [
    [
     "Kimiko",
     "Yamakawa"
    ],
    [
     "Ryoji",
     "Baba"
    ]
   ],
   "title": "The perception of the morae with devocalized vowels in Japanese language.",
   "original": "i98_0312",
   "page_count": 4,
   "order": 463,
   "p1": "paper 0312",
   "pn": "",
   "abstract": [
    "Usually the first vowel of the Japanese word 'susugi' (rinse) disappears, and so the pronunciation of 'susugi' is not [susugi] but [ssugi]. We made two psychophysical experiments. In the first one we shortened the part of [ss] of [ssugides] and [korewassugides] (This is rinse.) in 6 stages. [korewassugides] with shortened [ss] is easy to perceive as [korewasugides] (This is cedar) , but [ssugides] with shortened [ss] is not easy to perceive of [sugides] (Japanese cedar). In the second experiment we changed the pitches of [sugides] and researched how Japanese perceive these sounds.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-462"
  },
  "massaro98_icslp": {
   "authors": [
    [
     "Dominic W.",
     "Massaro"
    ]
   ],
   "title": "Categorical perception: important phenomenon or lasting myth?",
   "original": "i98_0970",
   "page_count": 3,
   "order": 464,
   "p1": "paper 0970",
   "pn": "",
   "abstract": [
    "Categorical perception, or the perceived equality of instances within a phoneme category, has been a central concept in the experimental and theoretical investigation of speech perception. It can be found as fact in most introductory textbooks in perception, cognition, linguistics and cognitive science. This paper analyzes the reasons for the persistent endurance of this concept. A variety of empirical and theoretical research findings are described in order to inform and hopefully to provide a more critical look at this pervasive concept. Given the demise of categorical perception, it is necessary to shift our theoretical focus to how multiple sources of continuous information are processed to support the perception of spoken language.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-463"
  },
  "gerrits98_icslp": {
   "authors": [
    [
     "Ellen",
     "Gerrits"
    ],
    [
     "Bert",
     "Schouten"
    ]
   ],
   "title": "Categorical perception of vowels",
   "original": "i98_0265",
   "page_count": 4,
   "order": 465,
   "p1": "paper 0265",
   "pn": "",
   "abstract": [
    "One of the often recurring results in categorical perception research is the difference between the perception of consonants, mainly stop consonants, and vowels. Stop consonants are said to be categorically perceived, whereas the perception of vowels is often called continuous. The difference in perception between stop consonants and vowels could be due to the difference in coding between these speech sounds.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-464"
  },
  "kakehi98_icslp": {
   "authors": [
    [
     "Kazuhiko",
     "Kakehi"
    ],
    [
     "Yuki",
     "Hirose"
    ]
   ],
   "title": "Suprasegmental cues for the segmentation of identical vowel sequences in Japanese",
   "original": "i98_1011",
   "page_count": 4,
   "order": 466,
   "p1": "paper 1011",
   "pn": "",
   "abstract": [
    "This paper investigates how hearers cope with a sequence of more than two identical vowels --a common occurrence in Japanese speech. In the segmentation of identical vowels there are no spectral cues and very small power envelope changes in usual utterances containing identical vowels. We consider the effects of suprasegmental information such as duration, pitch pattern and rhythm of speech as important cues, and examine how, and to what extent a hearer can successfully make use of such information to segment each mora in a consecutive vowel series with and without the preceding sentential context.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-465"
  },
  "ainsworth98_icslp": {
   "authors": [
    [
     "William A.",
     "Ainsworth"
    ]
   ],
   "title": "Perception of concurrent approximant-vowel syllables",
   "original": "i98_0510",
   "page_count": 4,
   "order": 467,
   "p1": "paper 0510",
   "pn": "",
   "abstract": [
    "Some experiments are described which explore the perception of the glides /w/ and /j/ spoken simultaneously. These cannot be spoken in isolation, like vowel sounds, but must be combined with vowels to form syllables. In previous experiments /w/ and /j/ were combined with the vowels /i/ and /a/ to form the four syllables /wi/, /wa, /ji/ and /ja/. It was found that if both the vowels and their pitches differed the consonants could be identified by some of the listeners part of the time. The effect of fundamental frequency on perception has now been explored. Each pair of syllables had different consonants and different vowels but one syllable had a pitch of 100 Hz whilst the other had a pitch of between 100 and 200 Hz. It was found that some syllables were perceived like vowels. Effects one syllable of the pair leading the other have also been systematically explored.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-466"
  },
  "behne98_icslp": {
   "authors": [
    [
     "Dawn M.",
     "Behne"
    ],
    [
     "Peter E.",
     "Czigler"
    ],
    [
     "Kirk P. H.",
     "Sullivan"
    ]
   ],
   "title": "Perceived Swedish vowel quantity: effects of postvocalic consonant duration",
   "original": "i98_1031",
   "page_count": 4,
   "order": 468,
   "p1": "paper 1031",
   "pn": "",
   "abstract": [
    "In the production of Swedish, vowel quantity is known to be realized in the vowel, but also affects duration of a postvocalic consonant. The goal of this study is to examine the use of postvocalic consonant duration as a perceptual cue to vowel quantity. Listeners' responses and reaction times were recorded for synthesized materials in which the vowel spectra and duration were kept constant and the postvocalic consonant duration was adjusted. Results show no indication that listeners actively used the duration of a postvocalic consonant to identify vowel quantity. These findings suggest that adjustments in postvocalic consonant duration in Swedish productions may be temporal artifacts of the preceding vowel quantity rather than reflecting linguistically relevant information.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-467"
  },
  "cutler98_icslp": {
   "authors": [
    [
     "Anne",
     "Cutler"
    ],
    [
     "Rebecca",
     "Treiman"
    ],
    [
     "Brit van",
     "Ooijen"
    ]
   ],
   "title": "Orthografik inkoncistensy ephekts in foneme detektion?",
   "original": "i98_0094",
   "page_count": 4,
   "order": 469,
   "p1": "paper 0094",
   "pn": "",
   "abstract": [
    "The phoneme detection task is widely used in spoken word recognition research. Alphabetically literate participants, however, are more used to explicit representations of letters than of phonemes. The present study explored whether phoneme detection is sensitive to how target phonemes are, or may be, orthographically realised. Listeners detected the target sounds [b,m,t,f,s,k] in word-initial position in sequences of isolated English words. Response times were faster to the targets [b,m,t], which have consistent word-initial spelling, than to the targets [f,s,k], which are inconsistently spelled, but only when listeners' attention was drawn to spelling by the presence in the experiment of many irregularly spelled fillers. Within the inconsistent targets [f,s,k], there was no significant difference between responses to targets in words with majority and minority spellings. We conclude that performance in the phoneme detection task is not necessarily sensitive to orthographic effects, but that salient orthographic manipulation can induce such sensitivity.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-468"
  },
  "derwing98_icslp": {
   "authors": [
    [
     "Bruce L.",
     "Derwing"
    ],
    [
     "Terrance M.",
     "Nearey"
    ],
    [
     "Yeo Bom",
     "Yoon"
    ]
   ],
   "title": "The effect of orthographic knowledge on the segmentation of speech",
   "original": "i98_0978",
   "page_count": 3,
   "order": 470,
   "p1": "paper 0978",
   "pn": "",
   "abstract": [
    "This study is part of a cross-linguistic investigation of phonological units, with emphasis on such ostensive universals as the segment (C or V), the syllable (e.g., CVC), and the rime (VC). Prior work has indicated that speakers of some languages (e.g., Chinese) may not segment words into units smaller than the whole syllable, while in other languages (e.g., Korean and Japanese) units called the body (CV) or the mora may supplant the rime. However, the native speakers tested so far were all relatively well educated, literate, and often bilingual. Thus they were all exposed to writing systems that might have influenced their performance. Since knowledge of orthography has not been controlled in previous studies, in the present research we will test speakers of English and Korean who have not been subjected to the influence of spelling. These include preliterate children, who are the focus of the study.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-469"
  },
  "mcqueen98_icslp": {
   "authors": [
    [
     "James M.",
     "McQueen"
    ],
    [
     "Anne",
     "Cutler"
    ]
   ],
   "title": "Spotting (different types of) words in (different types of) context",
   "original": "i98_0033",
   "page_count": 4,
   "order": 471,
   "p1": "paper 0033",
   "pn": "",
   "abstract": [
    "Results of a word-spotting experiment are presented in which Dutch listeners tried to spot different types of bisyllabic Dutch words embedded in different types of nonsense contexts. Embedded verbs were not reliably harder to spot than embedded nouns; this suggests that nouns and verbs are recognised via the same basic processes. Iambic words were no harder to spot than trochaic words, suggesting that trochaic words are not in principle easier to recognise than iambic words. Words were harder to spot in consonantal contexts (i.e., contexts which themselves could not be words) than in longer contexts which contained at least one vowel (i.e., contexts which, though not words, were possible words of Dutch). A control experiment showed that this difference was not due to acoustic differences between the words in each context. The results support the claim that spoken-word recognition is sensitive to the viability of sound sequences as possible words.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-470"
  },
  "ohala98_icslp": {
   "authors": [
    [
     "Manjari",
     "Ohala"
    ],
    [
     "John J.",
     "Ohala"
    ]
   ],
   "title": "Correlation between consonantal VC transitions and degree of perceptual confusion of place contrast in hindi",
   "original": "i98_0238",
   "page_count": 4,
   "order": 472,
   "p1": "paper 0238",
   "pn": "",
   "abstract": [
    "A previous study of Hindi VC transitions revealed that its five places of stop articulation exhibit considerable contextual variability. We studied whether these VC formant patterns may nevertheless contain sufficient cues to differentiate the place or whether they also require the cues at the stop release. Twenty-one listeners were asked to identify the final stop in syllables with varying place and preceding vowel, with and without the final stop release. There were 86% correct judgments of place with the stop releases but 63% without. In the gated condition the responses showed a marked asymmetry: stops normally having weak bursts (labial and dental) were most often correct but were also the favored erroneous response for the other stops. The results further suggest a qualification to Steriade's claim that retroflex stop's VC place cues are more robust than release: this may be true after low vowels but is not true after /i/.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-471"
  },
  "house98_icslp": {
   "authors": [
    [
     "David",
     "House"
    ],
    [
     "Dik",
     "Hermes"
    ],
    [
     "Fr√©d√©ric",
     "Beaugendre"
    ]
   ],
   "title": "Perception of tonal rises and falls for accentuation and phrasing in Swedish",
   "original": "i98_0044",
   "page_count": 4,
   "order": 473,
   "p1": "paper 0044",
   "pn": "",
   "abstract": [
    "In previous experiments with Dutch, French and Swedish listeners, it was shown that the location in the syllable of the onset of a rising or falling pitch movement is critical for the perception of accentuation. As the onset of the pitch movement is shifted through the syllable, there is a point at which the percept of accentuation shifts from one syllable to the next. This point is termed the accentuation boundary. It has also been proposed that in certain positions, the percept of accentuation conflicts with the percept of phrasing. An experiment with Swedish listeners was carried out using the same stimuli as used for the accentuation study, but now the task was to determine the phrasing of the syllables. The results indicate that perceptual phrase boundaries can be determined in the same way as accentuation boundaries. Differences in the locations of the boundaries can be interpreted in terms of strengths of tonal cues for accentuation and phrasing.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-472"
  },
  "greenberg98_icslp": {
   "authors": [
    [
     "Steven",
     "Greenberg"
    ],
    [
     "Takayuki",
     "Arai"
    ],
    [
     "Rosaria",
     "Silipo"
    ]
   ],
   "title": "Speech intelligibility derived from exceedingly sparse spectral information",
   "original": "i98_0074",
   "page_count": 4,
   "order": 474,
   "p1": "paper 0074",
   "pn": "",
   "abstract": [
    "Traditional models of speech assume that a detailed analysis of the acoustic spectrum is essential for understanding spoken language. The validity of this assumption was tested by partitioning the spectrum of spoken sentences into 1/3-octave channels (\"slits\") and measuring the intelligibility associated with each channel presented alone and in concert with the others. Four spectral channels, distributed over the speech-audio range (0.3 - 6 kHz) are sufficient for human listeners to decode sentential material with nearly 90% accuracy, although more than 70% of the spectrum is missing. Word recognition often remains relatively high (60-83%) when just two or three channels are presented concurrently, even though the intelligibility of these same slits, presented in isolation, is less than 9%. Such data suggest that intelligibility is derived from a compound \"image\" of the modulation spectrum distributed across the frequency spectrum. Because intelligibility seriously degrades when slits are desynchronized by more than 25 ms, this image is probably derived from both the amplitude and phase components of the modulation spectrum.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-473"
  },
  "flynn98_icslp": {
   "authors": [
    [
     "Mark C.",
     "Flynn"
    ],
    [
     "Richard C.",
     "Dowell"
    ],
    [
     "Graeme M.",
     "Clark"
    ]
   ],
   "title": "Adults with a severe-to-profound hearing impairment. investigating the effects of linguistic context on speech perception",
   "original": "i98_0121",
   "page_count": 4,
   "order": 475,
   "p1": "paper 0121",
   "pn": "",
   "abstract": [
    "Linguistic context is known to influence speech perception abilities in adults with normal hearing. Recent reports question the importance of context for adults with a severe-to-profound hearing impairment. The severe reduction and distortion in acoustic input may result in the listener perceiving insufficient acoustic-phonetic cues to allow access to higher level linguistic processing. To investigate this further, a detailed study of the speech recognition of adults with a severe-to-profound hearing impairment (N=34) was undertaken. A series of aided speech recognition tasks, sequentially examined the different levels of processing in the speech perception chain. The investigation concluded that the effects of severe-to-profound hearing impairment did not reduce the listener's ability to take advantage of contextual cues. There was, however, wide variability between participants in the utilisation of contextual processing. This indicates that to estimate \"real-life\" speech perception skills, an evaluation of contextual processing ability is required.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-474"
  },
  "koopmansvanbeinum98_icslp": {
   "authors": [
    [
     "Florien J.",
     "Koopmans-van Beinum"
    ],
    [
     "Caroline E.",
     "Schwippert"
    ],
    [
     "Cecile T. L.",
     "Kuijpers"
    ]
   ],
   "title": "Speech perception in dyslexia: measurements from birth onwards",
   "original": "i98_0288",
   "page_count": 4,
   "order": 476,
   "p1": "paper 0288",
   "pn": "",
   "abstract": [
    "ABSTRACT This paper concentrates on a small, but essential part within a large national Dutch research program on developmental dyslexia, namely the development of auditory test material for experiments with children from birth onwards. Since it is likely that the basis of any phoneme awareness in children is laid already in their first year of life, it is of great importance to follow the perceptual development of children at risk of dyslexia from birth onwards. In order to investigate the nature and origin of the perceptual deficit dyslexic people are afflicted with, a number of auditory tests are designed. The various steps in the development of the definite set of auditory tests, to be used in the actual research program, the rationale underlying these various steps, and first results of the pilot tests are described in the present paper.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-475"
  },
  "croot98_icslp": {
   "authors": [
    [
     "Karen",
     "Croot"
    ]
   ],
   "title": "An acoustic analysis of vowel production across tasks in a case of non-fluent progressive aphasia",
   "original": "i98_0013",
   "page_count": 4,
   "order": 477,
   "p1": "paper 0013",
   "pn": "",
   "abstract": [
    "This paper presents an acoustic investigation of vowel production in reading, naming and repetition tasks by LM, a man with non-fluent progressive aphasia. Plots in the F1/F2 plane showing the centroids of the acoustic targets of [i: E A V O u:] and the formant trajectories of [ai ei ou] demonstrate that LM achieved greater differentiation of targets in reading than in naming or repetition, and that the vowel space for repetition was distorted relative to that of the other two tasks. An earlier study of LM's speech argued that phonological information available from the stimuli in reading and repetition tasks facilitated the activation of stored phonological representations for speech production (Croot, Patterson & Hodges, 1988); the present study suggests that articulatory processing is also facilitated directly or indirectly by the availability of phonological information.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-476"
  },
  "doorn98_icslp": {
   "authors": [
    [
     "Jan van",
     "Doorn"
    ],
    [
     "Sharynne",
     "McLeod"
    ],
    [
     "Elise",
     "Baker"
    ],
    [
     "Alison",
     "Purcell"
    ],
    [
     "William",
     "Thorpe"
    ]
   ],
   "title": "Speech technology in clinical environments",
   "original": "i98_0709",
   "page_count": 4,
   "order": 478,
   "p1": "paper 0709",
   "pn": "",
   "abstract": [
    "Traditionally, perceptual judgement of speech disorders by clinicians has been a cornerstone of speech language pathology. Increasingly, it is being argued that acoustic speech analysis should supplement aural perception in the clinic. For successful clinical application of speech technology, experts in acoustic analysis generally agree that a working knowledge of acoustic phonetics, digital signal processing and the literature on the acoustic characteristics of speech disorders are required. However, it is not necessarily compellingly obvious to clinicians. This paper examines the issues by examining how the various components fit into the clinical picture. It examines when and how speech technology can be used by clinicians, arguing that clinicians need to be able to do much more than just operate the system. The paper concludes that successful integration of speech technology into clinical environments provides an opportunity to for technologists and clinicians to work together to produce effective speech technology for clinical applications.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-477"
  },
  "seneff98b_icslp": {
   "authors": [
    [
     "Stephanie",
     "Seneff"
    ],
    [
     "Ed",
     "Hurley"
    ],
    [
     "Raymond",
     "Lau"
    ],
    [
     "Christine",
     "Pao"
    ],
    [
     "Philipp",
     "Schmid"
    ],
    [
     "Victor",
     "Zue"
    ]
   ],
   "title": "GALAXY-II: a reference architecture for conversational system development",
   "original": "i98_1153",
   "page_count": 4,
   "order": 479,
   "p1": "paper 1153",
   "pn": "",
   "abstract": [
    "GALAXY is a client-server architecture for accessing on-line information using spoken dialogue which was first introduced at ICSLP-94. It has served as the testbed for developing human language technologies for our group for several years. Recently, we have initiated a significant redesign of its architecture to enable many researchers to develop their own applications, using either exclusively their own servers or intermixing them with servers developed by others. This redesign was done in part due to the fact that GALAXY has been designated as the prototype reference architecture for the new DARPA Communicator Program. The new architecture, GALAXY-II, makes use of a scripting language for flow control to provide flexible interaction among the servers, and a set of libraries to support rapid prototyping of new servers. In this paper, we describe the new architecture in some detail, and report on the current status of its development.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-478"
  },
  "chung98_icslp": {
   "authors": [
    [
     "Grace",
     "Chung"
    ],
    [
     "Stephanie",
     "Seneff"
    ]
   ],
   "title": "Improvements in speech understanding accuracy through the integration of hierarchical linguistic, prosodic, and phonological constraints in the jupiter domain",
   "original": "i98_0603",
   "page_count": 4,
   "order": 480,
   "p1": "paper 0603",
   "pn": "",
   "abstract": [
    "This paper explores some issues in designing conversational systems with integrated higher level constraints. We experiment with a configuration that combines a context-dependent acoustic front-end, using MIT's SUMMIT recognizer, with ANGIE, a hierarchical framework that models word substructure and phonological processes, and with TINA, a trainable probabilistic natural language (NL) model. Working in the Jupiter weather domain, we develop a computationally tractable system which incorporates higher level linguistic, prosodic and phonological constraints together in the second of a two-pass strategy. Experiments are evaluated using a new understanding performance metric, and the new integrated system achieves up to 17.1% relative reduction in understanding error and 15.4% reduction in word error. In addition, we investigate the possibilities of a two-pass system which relies on the first stage for pruning based on syllable-level constraint, and applies linguistic and prosodic knowledge largely at the second stage.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-479"
  },
  "ng98_icslp": {
   "authors": [
    [
     "Kenney",
     "Ng"
    ]
   ],
   "title": "Towards robust methods for spoken document retrieval",
   "original": "i98_1088",
   "page_count": 4,
   "order": 481,
   "p1": "paper 1088",
   "pn": "",
   "abstract": [
    "In this paper, we investigate a number of robust indexing and retrieval methods in an effort to improve spoken document retrieval performance in the presence of speech recognition errors. In particular, we examine expanding the original query representation to include confusible terms; developing a new document-query retrieval measure based on approximate matching that is less sensitive to recognition errors; expanding the document representation to include multiple recognition hypotheses; modifying the original query using automatic relevance feedback to include new terms found in the top ranked documents; and combining information from multiple subword unit representations. We study the different methods individually and then explore the effects of combining them. Experiments on radio broadcast news data show that using a combination of these methods can improve retrieval performance by over 20%.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-480"
  },
  "sproat98b_icslp": {
   "authors": [
    [
     "Richard",
     "Sproat"
    ],
    [
     "Jan P. H. van",
     "Santen"
    ]
   ],
   "title": "Automatic ambiguity detection",
   "original": "i98_0041",
   "page_count": 4,
   "order": 482,
   "p1": "paper 0041",
   "pn": "",
   "abstract": [
    "Most work on sense disambiguation presumes that one knows beforehand --- e.g. from a thesaurus --- a set of polysemous terms. But published lists invariably give only partial coverage. For example, the English word tan has several obvious senses, but one may overlook the abbreviation for tangent. In this paper, we present an algorithm for identifying interesting polysemous terms and measuring their degree of polysemy, given an unlabeled corpus. The algorithm involves: (i) collecting all terms within a k-term window of the target term; (ii) computing the inter-term distances of the contextual terms, and reducing the multi-dimensional distance space to two dimensions using standard methods; (iii) converting the two-dimensional representation into radial coordinates and using isotonic/antitonic regression to compute the degree to which the distribution deviates from a single-peak model. The amount of deviation is the proposed polysemy index.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-481"
  },
  "fischer98b_icslp": {
   "authors": [
    [
     "Julia",
     "Fischer"
    ],
    [
     "Juergen",
     "Haas"
    ],
    [
     "Elmar",
     "N√∂th"
    ],
    [
     "Heinrich",
     "Niemann"
    ],
    [
     "Frank",
     "Deinzer"
    ]
   ],
   "title": "Empowering knowledge based speech understanding through statistics",
   "original": "i98_0369",
   "page_count": 4,
   "order": 483,
   "p1": "paper 0369",
   "pn": "",
   "abstract": [
    "In this paper we present an innovative approach to speech understanding which is based on a fine-grained knowledge representation automatically compiled from a semantic network and on iterative optimization. Besides allowing an efficient exploitation of parallelism, any-time capability is provided since after each iteration step a (sub-)optimal solution is always available. We apply this approach to a real-world task, which is a dialog system able to answer queries about the German train timetable. In order to speed up the search for the best interpretation of an utterance we make use of statistical methods, e.g. neural networks, n-grams, and classification trees, which are trained on application relevant utterances collected over the public telephone network. At the moment the real-time factor for interpreting the initial user's utterance is 0.7.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-482"
  },
  "nagai98_icslp": {
   "authors": [
    [
     "Akito",
     "Nagai"
    ],
    [
     "Yasushi",
     "Ishikawa"
    ]
   ],
   "title": "Concept-driven speech understanding incorporated with a statistic language model",
   "original": "i98_1023",
   "page_count": 4,
   "order": 484,
   "p1": "paper 1023",
   "pn": "",
   "abstract": [
    "We have proposed a method of concept-driven semantic interpretation based on general semantic knowledge of conceptual dependency. In our approach, a concept is a unit of semantic interpretation and an utterance is regarded as a sequence of concepts that convey an intention. However, a considerable number of accepted results were not syntactically meaningful. This is because the order in which linguistic features occurred in the sequence of concepts was not taken into account in constructing the whole meaning from the concepts: only semantic constraint was used to attain linguistic robustness. Therefore, we introduce a statistical language model which calculates the plausibility of a sequence of concepts from the points of view of the order in which shallow linguistic features occur. Experimental results of speech understanding for 1000-word-vocabulary spontaneous speech show that the proposed method significantly improves the system performance.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-483"
  },
  "colas98_icslp": {
   "authors": [
    [
     "Jos√©",
     "Col√°s"
    ],
    [
     "Javier",
     "Ferreiros"
    ],
    [
     "Juan Manuel",
     "Montero"
    ],
    [
     "Julio",
     "Pastor"
    ],
    [
     "Ascensi√≥n",
     "Gallardo"
    ],
    [
     "Jos√© Manuel",
     "Pardo"
    ]
   ],
   "title": "On the limitations of stochastic conceptual finite-state language models for speech understanding",
   "original": "i98_1095",
   "page_count": 4,
   "order": 485,
   "p1": "paper 1095",
   "pn": "",
   "abstract": [
    "In a limited domain task (e.g. airline reservation, database retrieval, etc) many robust understanding systems, designed for both speech and text input, have been implemented [1][3][5][6][10] based on the Stochastic Conceptual Finite-State paradigm (Semantic Network) or CHRONUS paradigm [11] (Conceptual Hidden Representation of Natural Unconstrained Speech), which establishes relations between conceptual entities through a probabilistic graph-like structure. The use of this kind of grammar to model semantic information presents limitations, which have been analysed during the implementation of a flexible architecture for a robust information retrieval system, based on the same paradigm [1]. We have tried to solve some of them by integrating a set of conceptual probabilistic and non-probabilistic grammars, which allow certain complexity in the functionality of the application, such as applying non-SQL functions to the results of SQL queries in order to retrieve information not explicitly included in the database, translating certain natural spoken sentences (that would produce difficult embedded queries and therefore more natural queries) without so many restrictions in the relative position of inter-concept relationship.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-484"
  },
  "ward98b_icslp": {
   "authors": [
    [
     "Todd",
     "Ward"
    ],
    [
     "Salim",
     "Roukos"
    ],
    [
     "Chalapathy",
     "Neti"
    ],
    [
     "Jerome",
     "Gros"
    ],
    [
     "Mark",
     "Epstein"
    ],
    [
     "Satya",
     "Dharanipragada"
    ]
   ],
   "title": "Towards speech understanding across multiple languages",
   "original": "i98_0400",
   "page_count": 4,
   "order": 486,
   "p1": "paper 0400",
   "pn": "",
   "abstract": [
    "In this paper we describe our initial efforts in building a natural language understanding (NLU) system across multiple languages. The system allows users to switch languages seamlessly in a single session without requiring any switch in the speech recognition system. Context-dependence is maintained across utterances, even when the user changes languages. Towards this end we have begun building a universal speech recognizer for English and French languages. We experiment with a common phonology for both French and English with a novel mechanism to handle language dependent variations. Our best results so far show about 5% relative performance degradation for English relative to a unilingual English system and a 9% relative degradation in French relative to a unilingual French system. The NLU system uses the same statistical understanding algorithms for each language, making system development, maintenance and portability vastly superior to systems built customly for each language.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-485"
  },
  "stolcke98_icslp": {
   "authors": [
    [
     "Andreas",
     "Stolcke"
    ],
    [
     "Elizabeth",
     "Shriberg"
    ],
    [
     "Rebecca",
     "Bates"
    ],
    [
     "Mari",
     "Ostendorf"
    ],
    [
     "Dilek",
     "Hakkani"
    ],
    [
     "Madelaine",
     "Plauche"
    ],
    [
     "Gokhan",
     "Tur"
    ],
    [
     "Yu",
     "Lu"
    ]
   ],
   "title": "Automatic detection of sentence boundaries and disfluencies based on recognized words",
   "original": "i98_0059",
   "page_count": 4,
   "order": 487,
   "p1": "paper 0059",
   "pn": "",
   "abstract": [
    "We study the problem of detecting linguistic events at interword boundaries, such as sentence boundaries and disfluency locations, in speech transcribed by an automatic recognizer. Recovering such events is crucial to facilitate speech understanding and other natural language processing tasks. Our approach is based on a combination of prosodic cues modeled by decision trees, and word-based event N-gram language models. Several model combination approaches are investigated. The techniques are evaluated on conversational speech from the Switchboard corpus. Model combination is shown to give a significant win over individual knowledge sources.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-486"
  },
  "reichl98_icslp": {
   "authors": [
    [
     "Wolfgang",
     "Reichl"
    ],
    [
     "Bob",
     "Carpenter"
    ],
    [
     "Jennifer",
     "Chu-Carroll"
    ],
    [
     "Wu",
     "Chou"
    ]
   ],
   "title": "Language modeling for content extraction in human-computer dialogues",
   "original": "i98_0588",
   "page_count": 4,
   "order": 488,
   "p1": "paper 0588",
   "pn": "",
   "abstract": [
    "In this paper we discuss the role of language modeling in a novel natural language dialogue system designed to automatically route incoming customer calls. We arrive at two significant conclusions: First, standard word error rate measures do not reflect application specific requirements; highly reliable content extraction is possible with relatively high word error rates. Secondly blending human-human data with human-machine data did not improve the performance in language modeling.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-487"
  },
  "gillett98_icslp": {
   "authors": [
    [
     "John",
     "Gillett"
    ],
    [
     "Wayne",
     "Ward"
    ]
   ],
   "title": "A language model combining trigrams and stochastic context-free grammars",
   "original": "i98_0872",
   "page_count": 4,
   "order": 489,
   "p1": "paper 0872",
   "pn": "",
   "abstract": [
    "We propose a class trigram language model in which each class is specified by a probabilistic context-free grammar. We show how to estimate the parameters of the model, and how to smooth these estimates. We present experimental perplexity and speech recognition results.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-488"
  },
  "souvignier98_icslp": {
   "authors": [
    [
     "Bernd",
     "Souvignier"
    ],
    [
     "Andreas",
     "Kellner"
    ]
   ],
   "title": "Online adaptation of language models in spoken dialogue systems",
   "original": "i98_0961",
   "page_count": 4,
   "order": 490,
   "p1": "paper 0961",
   "pn": "",
   "abstract": [
    "The robust estimation of language models for new applications of spoken dialogue systems often suffers from a lack of available training material. An alternative to training is to adapt initial language models to a new task by exploiting material from recognition. We investigate different methods for online-adaptation of language models. Apart from supervised and unsupervised adaptation, we look at two refined approaches: the first allows multiple hypotheses from N-best lists for adaptation and the second uses confidence measures to reject unreliably recognized sentences. We apply adaptation both to the language model used in the recognizer to focus the beam search and to the stochastic language understanding grammar. It turns out that the understanding grammar can be improved quite significantly using N-best lists or confidence measures, whereas unsupervised adaptation may even result in a deterioration of the system. The language model used in the recognizer is also improved very satisfactory.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-489"
  },
  "riccardi98_icslp": {
   "authors": [
    [
     "Giuseppe",
     "Riccardi"
    ],
    [
     "Alexandros",
     "Potamianos"
    ],
    [
     "Shrikanth",
     "Narayanan"
    ]
   ],
   "title": "Language model adaptation for spoken language systems",
   "original": "i98_1052",
   "page_count": 4,
   "order": 491,
   "p1": "paper 1052",
   "pn": "",
   "abstract": [
    "In a human-machine interaction (dialog) the statistical language variations are large among different stages of the dialog and across different speakers. Moreover, spoken dialog systems require extensive training data for training stochastic language models. In this paper we address the problem of open-vocabulary language models allowing the user for any possible response at each stage of the dialog. We propose a novel off-line adaptation of stochastic language models effective for their generalization (open-vocabulary) and selective (dialog context) properties. We outline the integration of the finite state dialog and the language model adaptation algorithm. The performance of the speech recognition and understanding language models are evaluated with the Carmen Sandiego multimodal computer game. The new language models give an overall understanding error rate reduction of 44% over the baseline system.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-490"
  },
  "bigi98_icslp": {
   "authors": [
    [
     "Brigitte",
     "Bigi"
    ],
    [
     "Renato De",
     "Mori"
    ],
    [
     "Marc",
     "El-Beze"
    ],
    [
     "Thierry",
     "Spriet"
    ]
   ],
   "title": "Detecting topic shifts using a cache memory",
   "original": "i98_0077",
   "page_count": 4,
   "order": 492,
   "p1": "paper 0077",
   "pn": "",
   "abstract": [
    "The use of cache memories and symmetric Kullback-Leibler distances is proposed for topic classification and topic-shift detection. Experiments with a large corpus of articles from the French newspaper \"Le Monde show tangible advantages when different models are combined with a suitable strategy. Experimental results show that different strategies for topic shift detection have to be used depending on whether high recall or high precision are sought. Furthermore, methods based on topic independent distributions provide complementary candidates with respect to the use of topic-dependent distributions leading to an increase in recall with a minor loss in precision.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-491"
  },
  "levin98_icslp": {
   "authors": [
    [
     "Lori",
     "Levin"
    ],
    [
     "Ann",
     "Thyme-Gobbel"
    ],
    [
     "Alon",
     "Lavie"
    ],
    [
     "Klaus",
     "Ries"
    ],
    [
     "Klaus",
     "Zechner"
    ]
   ],
   "title": "A discourse coding scheme for conversational Spanish",
   "original": "i98_1000",
   "page_count": 4,
   "order": 493,
   "p1": "paper 1000",
   "pn": "",
   "abstract": [
    "This paper describes a 3-level manual discourse coding scheme that we have devised for manual tagging of the CallHome Spanish (CHS) and CallFriend Spanish (CFS) databases used in the CLARITY project. The goal of CLARITY is to explore the use of discourse structure in understanding conversational speech. The project combines empirical methods for dialogue processing with state-of-the art LVCSR (using the JANUS recognizer). The three levels of the coding scheme are (1) a speech act level consisting of a tag set extended from DAMSL and Switchboard; (2) dialogue game level defined by initiative and intention; and (3) an activity level defined within topic units. The manually tagged dialogues are used to train automatic classifiers. We present preliminary results for automatic speech act classification and topic boundary identification and inter-coder speech act confusion matrices.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-492"
  },
  "arai98_icslp": {
   "authors": [
    [
     "Kazuhiro",
     "Arai"
    ],
    [
     "Jeremy H.",
     "Wright"
    ],
    [
     "Giuseppe",
     "Riccardi"
    ],
    [
     "Allen L.",
     "Gorin"
    ]
   ],
   "title": "Grammar fragment acquisition using syntactic and semantic clustering",
   "original": "i98_0063",
   "page_count": 4,
   "order": 494,
   "p1": "paper 0063",
   "pn": "",
   "abstract": [
    "A new method is proposed for automatically acquiring Fragments to understand fluent speech. The goal of this method is to generate a collection of Fragments, each representing a set of syntactically and semantically similar phrases. First, phrases frequently observed in the training set are selected as candidates. Each candidate phrase has three associated probability distributions of : following contexts, preceding contexts, and associated semantic actions. The similarity between candidate phrases is measured by applying the Kullback-Leibler distance to these three probability distributions. Candidate phrases that are close in all three distances are clustered into a Fragment. Salient sequences of these Fragments are then automatically acquired, and exploited by a spoken language understanding to classify calls in AT&T's ``How May I Help You?'' task. The experimental results show that the average and maximum improvements in call-type classification performance of 2.2% and 2.8% are respectively achieved by introducing the Fragments.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-493"
  },
  "brndsted98c_icslp": {
   "authors": [
    [
     "Tom",
     "Br√∏ndsted"
    ]
   ],
   "title": "Non-expert access to unification based speech understanding",
   "original": "i98_0810",
   "page_count": 4,
   "order": 495,
   "p1": "paper 0810",
   "pn": "",
   "abstract": [
    "The paper describes the concepts behind a sub grammar design tool being developed within the EU-funded language-engineering project REWARD. The tool is a sub-component of a general platform for designing spoken language systems and addresses dialogue designers who are non-experts in natural language processing and speech technology. Yet, the tool interfaces to a powerful and \"professional\" unification grammar formalism that is interpreted by a corresponding natural language parser and in a derived finite state approximation form is used for constraining speech recognition. The tool performs some basic intersection and unification operations on feature sets to generate template-like rules complying with the unification grammar formalism.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-494"
  },
  "carpenter98_icslp": {
   "authors": [
    [
     "Bob",
     "Carpenter"
    ],
    [
     "Jennifer",
     "Chu-Carroll"
    ]
   ],
   "title": "Natural language call routing: a robust, self-organizing approach",
   "original": "i98_0076",
   "page_count": 4,
   "order": 496,
   "p1": "paper 0076",
   "pn": "",
   "abstract": [
    "We have developed a domain independent, automatically trained, call router which directs customer calls based on their response to an open-ended ``How may I direct your call?'' query. Routing behavior is trained from a corpus of transcribed and hand-routed calls and then carried out using vector-based information retrieval techniques. Terms consist of sequences of morphologically reduced content words. Documents representing routing destinations consist of weighted term frequencies derived from calls to that destination in the training corpus. In this paper, we evaluate our approach in the context of a large financial services call center with thousands of possible customer activities and dozens of routing destinations. We evaluate the system's performance on ambiguous and unambiguous calls when given either accurate transcriptions or fairly noisy real-time speech recognizer output. We conclude that in a highly complex call center, our system performs at roughly the same level of accuracy as human operators.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-495"
  },
  "ghosh98_icslp": {
   "authors": [
    [
     "Debajit",
     "Ghosh"
    ],
    [
     "David",
     "Goddeau"
    ]
   ],
   "title": "Automatic grammar induction from semantic parsing",
   "original": "i98_0167",
   "page_count": 4,
   "order": 497,
   "p1": "paper 0167",
   "pn": "",
   "abstract": [
    "One approach to spoken language understanding converts a transcribed utterance into a semantic representation, which is then interpreted to produce a response. This can be accomplished with conventional parsing technology given a syntactic grammar and semantic composition rules. However, constructing such a grammar can be difficult and time-consuming. An alternative approach is to learn the rules from translated examples. This eliminates the need for knowledge engineering but requires the collection and annotation of the examples, which can be as difficult. This research investigates using semantic information to learn syntax automatically. After describing a semantic parsing mechanism for parsing utterances based on meaning, we illustrate a grammar induction technique which uses semantic parsing's results to create syntactic rules. We also present experiments which use these rules in syntactic parsing experiments in two domains. The learned grammar covers 98% of semantically-valid utterances in its original domain and 85% in a different domain.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-496"
  },
  "kono98_icslp": {
   "authors": [
    [
     "Yasuyuki",
     "Kono"
    ],
    [
     "Takehide",
     "Yano"
    ],
    [
     "Munehiko",
     "Sasajima"
    ]
   ],
   "title": "BTH: an efficient parsing algorithm for word-spotting",
   "original": "i98_0169",
   "page_count": 4,
   "order": 498,
   "p1": "paper 0169",
   "pn": "",
   "abstract": [
    "This paper presents a new parsing algorithm, BTH, which is capable of efficiently parsing a keyword lattice that contains a large number of false alarms. The BTH parser runs without unfolding the given keyword lattice, so that it can efficiently obtain a set of word sequences acceptable to the given grammar as the parser result. The algorithm has been implemented on Windows-based PCs and is tested by applying it to the car navigation task that has a scale of practical applications. The result shows promise in implementing the function of spontaneous speech understanding from sentence utterance in next-generation car navigation systems.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-497"
  },
  "kronenberg98_icslp": {
   "authors": [
    [
     "Susanne",
     "Kronenberg"
    ],
    [
     "Franz",
     "Kummert"
    ]
   ],
   "title": "Syntax coordination: interaction of discourse and extrapositions",
   "original": "i98_0515",
   "page_count": 4,
   "order": 499,
   "p1": "paper 0515",
   "pn": "",
   "abstract": [
    "The model presented here for parsing spoken German offers a way to proceed incrementally discontinuous constructions. Typical constructions in task oriented dialogs are Extrapositions to the right. These are defined as syntactical constructions where a constituent is extraposed in the Nachfeld. Basing on the assumption that the extraposed constituent is not part of the source sentence the model works by coordinating the syntactical information given by the source sentence and the extraposed constituent to complete the extaposed constituent to a whole sentence. Therefore, the standard LR(1)-parser is extended by two additional actions. The parsing strategy works by deriving the first part of the construction until the extraposed constituent is reached. The both new actions enable the parser to proceed further by taking the syntactical information of the source sentence to complete the extraposed constituent to a sentence of its own. The repeated use of these actions guarantees that every intended reading will be performed.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-498"
  },
  "lin98b_icslp": {
   "authors": [
    [
     "Bor-Shen",
     "Lin"
    ],
    [
     "Berlin",
     "Chen"
    ],
    [
     "Hsin-Min",
     "Wang"
    ],
    [
     "Lin-Shan",
     "Lee"
    ]
   ],
   "title": "Hierarchical tag-graph search for spontaneous speech understanding in spoken dialog systems",
   "original": "i98_0449",
   "page_count": 4,
   "order": 500,
   "p1": "paper 0449",
   "pn": "",
   "abstract": [
    "It has been relatively difficult to develop natural language parsers for spoken dialog systems, not only because of the possible recognition errors, pauses, hesitations, out-of-vocabulary words, and the grammatically incorrect sentence structures, but because of the great efforts required to develop a general enough grammar with satisfactory coverage and flexibility to handle different applications. In this paper, a new hierarchical graph-based search scheme with layered structure is presented, which is shown to provide more robust and flexible spontaneous speech understanding for spoken dialog systems.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-499"
  },
  "niimi98_icslp": {
   "authors": [
    [
     "Yasuhisa",
     "Niimi"
    ],
    [
     "Noboru",
     "Takinaga"
    ],
    [
     "Takuya",
     "Nishimoto"
    ]
   ],
   "title": "Extraction of the dialog act and the topic from utterances in a spoken dialog system",
   "original": "i98_1138",
   "page_count": 4,
   "order": 501,
   "p1": "paper 1138",
   "pn": "",
   "abstract": [
    "ABSTRACT This paper presents an approach to extraction of dialog acts and topics from utterances in a spoken dialog system. Two knowledge sources are used to describe the dialog history. One is a transition network of dialog acts and the other is a tree of topics which might appear in domain communications. Dialog acts and topics are extracted through bottom-up and top-down analyses. Bottom-up candidates are decided by applying a set of specially designed rules to the semantic representation of an utterance, and top-down candidates by using the current state of the dialog history. The logical ANDs between bottom-up and top-down candidates are taken to decide the dialog act and topic of an utterance. This method was examined with a corpus of fourteen dialogs including 335 utterances. Correct extraction rates were 85% for the topic and 82% for the dialog act.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-500"
  },
  "printz98_icslp": {
   "authors": [
    [
     "Harry",
     "Printz"
    ]
   ],
   "title": "Fast computation of maximum entropy / minimum divergence feature gain",
   "original": "i98_1129",
   "page_count": 4,
   "order": 502,
   "p1": "paper 1129",
   "pn": "",
   "abstract": [
    "Maximum entropy / minimum divergence modeling is a powerful technique for constructing probability models, which has been applied to a variety of problems in natural language processing. A maximum entropy / minimum divergence (MEMD) model is built from a base model, and a set of feature functions, also called simply features, whose empirical expectations on some training corpus are known. A fundamental difficulty with this technique is that while there are typically millions of features that could be incorporated into a given model, in general it is not computationally feasible, or even desirable, to use them all. Thus some means must be devised for determining each feature's predictive power, also known as its gain. Once the gains are known, the features can be ranked according to their utility, and only the most gainful ones retained. This paper presents a new algorithm for computing feature gain that is fast, accurate and memory-efficient.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-501"
  },
  "riccardi98b_icslp": {
   "authors": [
    [
     "Giuseppe",
     "Riccardi"
    ],
    [
     "Allen L.",
     "Gorin"
    ]
   ],
   "title": "Stochastic language models for speech recognition and understanding",
   "original": "i98_0111",
   "page_count": 4,
   "order": 503,
   "p1": "paper 0111",
   "pn": "",
   "abstract": [
    "Stochastic language models for speech recognition have traditionally been designed and evaluated in order to optimize word accuracy. In this work, we present a novel framework for training stochastic language models by optimizing two different criteria appropriate for speech recognition and language understanding. First, the language entropy and \"salience\" measure are used for learning the \"relevant\" spoken language features (phrases). Secondly, a novel algorithm for training stochastic finite state machines is presented which incorporates the acquired phrase structure into a single stochastic language model. Thirdly, we show the benefit of our novel framework with an end-to-end evaluation of a large vocabulary spoken language system for call routing.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-502"
  },
  "essdykema98_icslp": {
   "authors": [
    [
     "Carol Van",
     "Ess-Dykema"
    ],
    [
     "Klaus",
     "Ries"
    ]
   ],
   "title": "Linguistically engineered tools for speech recognition error analysis",
   "original": "i98_0787",
   "page_count": 4,
   "order": 504,
   "p1": "paper 0787",
   "pn": "",
   "abstract": [
    "In order to improve Large Vocabulary Continuous Speech Recognition (LVCSR) systems, it is essential to discover exactly how our current systems are underperforming. The major intellectual tool for solving this problem is error analysis: careful investigation of just which factors are contributing to errors in the recognizers. This paper presents our observations of the effects that discourse (i.e., dialog) modeling has on LVCSR system performance. As our title indicates, we emphasize the recognition error analysis methodology we developed and what it showed us as opposed to emphasizing development of the discourse model itself. In the first analysis of our output data, we focussed on errors that could be eliminated by Dialog Act discourse tagging using Dialog Act-specific language models. In a second analysis, we manipulated the parameterization of the Dialog Act-specific language models, enabling us to acquire evidence of the constraints these models introduced. The word error rate did not significantly decrease since the error rate in the largest category of Dialog Acts, namely Statements, did not significantly decrease. We did, however, observe significant error reduction in the less frequently occurring Dialog Acts and we report on the characteristic of the error corrections. We discovered that discourse models can introduce simple syntactic constraints and that they are most sensitive to parts of speech.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-503"
  },
  "takeda98_icslp": {
   "authors": [
    [
     "Kazuya",
     "Takeda"
    ],
    [
     "Atsunori",
     "Ogawa"
    ],
    [
     "Fumitada",
     "Itakura"
    ]
   ],
   "title": "Estimating entropy of a language from optimal word insertion penalty",
   "original": "i98_0456",
   "page_count": 4,
   "order": 505,
   "p1": "paper 0456",
   "pn": "",
   "abstract": [
    "The relationship between the optimal value of word insertion penalty and entropy of the language is discussed, based on the hypothesis that the optimal word insertion penalty compensates the probability given by a language model to the true probability. It is shown that the optimal word insertion penalty can be calculated as the difference between test set entropy of the given language model and true entropy of the given test set sentences. The correctness of the idea is confirmed through recognition experiment, where the entropy of the given set of sentences are estimated from two different language models and word insertion penalty optimized for each language model.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-504"
  },
  "tseng98_icslp": {
   "authors": [
    [
     "Shu-Chuan",
     "Tseng"
    ]
   ],
   "title": "A linguistic analysis of repair signals in co-operative spoken dialogues",
   "original": "i98_0993",
   "page_count": 4,
   "order": 506,
   "p1": "paper 0993",
   "pn": "",
   "abstract": [
    "This paper presents results of a corpus-based analysis of speech repairs, investigating repair signals which mark the existence of possible repairs. Dividing speech repairs into three parts: erroneous part, editing term and correction, this paper provides empirical evidence which supports the notion that speech repairs are produced in a rather regular syntactic pattern. Phrases seem to play a particular role in the production of speech repairs, as phrasal boundaries frequently correspond to boundaries within or around repairs. Related acoustic-prosodic features highlighting the internal structure of repairs including F0 , duration and tonal patterns are also examined and discussed with respect to specific syntactic patterns.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-505"
  },
  "valverdealbacete98_icslp": {
   "authors": [
    [
     "Francisco J.",
     "Valverde-Albacete"
    ],
    [
     "Jos√© Manuel",
     "Pardo"
    ]
   ],
   "title": "A hierarchical language model for CSR",
   "original": "i98_0587",
   "page_count": 4,
   "order": 507,
   "p1": "paper 0587",
   "pn": "",
   "abstract": [
    "In this paper, we present a new language model that includes some of the most promising techniques for overcoming linguistic inadequacy, - including POS tagging and refining, hierarchical, locally conditioned grammars, parallel modelling of acoustic and linguistic domains - and some of our own: language modelling as language parsing, and a better integration of the whole process with the acoustic model resulting in a richer educt from the language modelling process. We are building this model for a translation into Spanish of the DARPA RM task, maintaining the same 1k words vocabulary and some 1000 sentences.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-506"
  },
  "wright98b_icslp": {
   "authors": [
    [
     "Jeremy H.",
     "Wright"
    ],
    [
     "Allen L.",
     "Gorin"
    ],
    [
     "Alicia",
     "Abella"
    ]
   ],
   "title": "Spoken language understanding within dialogs using a graphical model of task structure",
   "original": "i98_0385",
   "page_count": 4,
   "order": 508,
   "p1": "paper 0385",
   "pn": "",
   "abstract": [
    "We describe a procedure for contextual interpretation of spoken sentences within dialogs. Task structure is represented in a graphical form, enabling the interpreter algorithm to be efficient and task-independent. Recognized spoken input may consist either of a single sentence with utterance-verification scores, or of a word lattice with arc weights. A confidence model is used throughout and all inferences are probability-weighted. The interpretation consists of a probability for each class and for each auxiliary information label needed for task completion. Anaphoric references are permitted.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-507"
  },
  "suzuki98b_icslp": {
   "authors": [
    [
     "Yoshimi",
     "Suzuki"
    ],
    [
     "Fumiyo",
     "Fukumoto"
    ],
    [
     "Yoshihiro",
     "Sekiguchi"
    ]
   ],
   "title": "Keyword extraction of radio news using domain identification based on categories of an encyclopedia",
   "original": "i98_0739",
   "page_count": 4,
   "order": 509,
   "p1": "paper 0739",
   "pn": "",
   "abstract": [
    "In this paper, we propose a keyword extraction method for dictation of radio news which consists of several domains. In our method, newspaper articles which are automatically classified into suitable domains are used in order to calculate feature vectors. The feature vectors show term-domain interdependence and are used for selecting a suitable domain of each part of radio news. Keywords are extracted by using the selected domain. The results of keyword extraction experiments showed that our methods are effective for keyword extraction of radio news.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-508"
  },
  "droppo98_icslp": {
   "authors": [
    [
     "James",
     "Droppo"
    ],
    [
     "Alex",
     "Acero"
    ]
   ],
   "title": "Maximum a posteriori pitch tracking",
   "original": "i98_0071",
   "page_count": 4,
   "order": 510,
   "p1": "paper 0071",
   "pn": "",
   "abstract": [
    "A Maximum a posteriori framework for computing pitch tracks as well as voicing decisions is presented. The proposed algorithm consists of creating a time-pitch energy distribution based on predictable energy that improves on the normalized cross-correlation. A large database is used to evaluate the algorithm's performance against two standard solutions, using glottal closure instants (GCI) obtained from electroglottogram (EGG) signals as a reference. The new MAP algorithm exhibits higher pitch accuracy and better voiced/unvoiced discrimination.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-509"
  },
  "yang98d_icslp": {
   "authors": [
    [
     "Dekun",
     "Yang"
    ],
    [
     "Georg F.",
     "Meyer"
    ],
    [
     "William A.",
     "Ainsworth"
    ]
   ],
   "title": "Vowel separation using the reassigned amplitude-modulation spectrum",
   "original": "i98_0511",
   "page_count": 4,
   "order": 511,
   "p1": "paper 0511",
   "pn": "",
   "abstract": [
    "This paper presents a method for segregating and recognizing concurrent vowels based on the amplitude modulation spectrum. Vowel segregation is accomplished by F0-guided grouping of harmonic components encoded in the amplitude modulation spectrum while vowel recognition is achieved by classifying the segregated vowel spectrum. Main features of the method are (1) the reassigned technique is employed to obtain a high resolution amplitude modulation spectrum and (2) Fisher's linear discriminant analysis is used to improve the performance of vowel classification. The method is tested on a double-vowel identification task and some preliminary results are provided.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-510"
  },
  "batlle98_icslp": {
   "authors": [
    [
     "Eloi",
     "Batlle"
    ],
    [
     "Climent",
     "Nadeu"
    ],
    [
     "Jos√© A.R.",
     "Fonollosa"
    ]
   ],
   "title": "Feature decorrelation methods in speech recognition. a comparative study",
   "original": "i98_0473",
   "page_count": 4,
   "order": 512,
   "p1": "paper 0473",
   "pn": "",
   "abstract": [
    "In this paper we study various decorrelation methods for the features used in speech recognition and we compare the performance of each one by running several tests with a speech database. First of all we study the Principal Components Analysis (PCA). PCA extracts the dimensions along which the data vary the most, and thus it allows us to reduce the dimension of the data points without significant loss of performance. The second transform we study is the Discrete Cosine Transform (DCT). As it will be shown, it is an approximation of the PCA analysis. By applying this transform to FBE parameters we obtain the MFCC coefficients. A further step is taken with the Linear Discriminant Analysis (LDA), which, not only reduces the dimensionality of the problem, but also discriminates among classes to reduce the confusion error. The last method we study is Frequency Filtering (FF). This method consists of a linear filtering of the frequency sequence of the log FBE that both decorrelates and equalizes the variance of the coefficients.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-511"
  },
  "caraty98_icslp": {
   "authors": [
    [
     "Marie-Jos√©",
     "Caraty"
    ],
    [
     "Claude",
     "Montaci√©"
    ]
   ],
   "title": "Multi-resolution for speech analysis",
   "original": "i98_1142",
   "page_count": 4,
   "order": 513,
   "p1": "paper 1142",
   "pn": "",
   "abstract": [
    "In the purpose to deal with artifact on observations measurements resulting from usual speech processing, we propose to extend the representation of the speech signal by taking a sequence of sets of observations instead of a simple sequence of observations. A set of observations is computed from temporal Multi-Resolution (MR) analysis. This method is designed to be adapted to any usual mode and technique of analysis. Its originality is to take into account two main variations in the analysis, -the center of the frame and -the duration of the frame. In speech processing, multi-resolution analysis has many applications. MR analysis is a basic representation -to locate the stationary and non-stationary parts of speech from the inertia computation, -to select the best representative observation from centroid or generalized centroid. Preliminary experiments are presented. The first one consists in the MR analysis of pieces of the French and the English-American speech databases (i.e., TIMIT, BREF80) and on the inertia as a criterion of location of stationary and non-stationary parts of the speech signal. The second one is on the computation of the phoneme prototypes of the two speech databases. At last, some perspectives are discussed.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-512"
  },
  "cassidy98_icslp": {
   "authors": [
    [
     "Steve",
     "Cassidy"
    ],
    [
     "Catherine",
     "Watson"
    ]
   ],
   "title": "Dynamic features in children's vowels",
   "original": "i98_0664",
   "page_count": 4,
   "order": 514,
   "p1": "paper 0664",
   "pn": "",
   "abstract": [
    "As part of a long term project to develop speech recognitions systems for young computer users, specifically children aged between 6 and 11 years, this paper presents a preliminary investigation into the classification of children's vowels. In earlier studies of adult speech we found that dynamic or time-varying cues were useful in classifying diphthongal vowels but provided no advantage for monophthongs if duration is included as an additional cue. In this study we investigate whether dynamic cues (modelled by Discrete Cosine Transform coefficients) are present to a greater or lesser extent in children's vowels. Our hypothesis is that some of the observed variability in children's vowels may be due to systematic time-varying features. We found that the children's monophthong data was better separated by a combination of DCT coefficients and vowel duration than by the formant data sampled at the vowel midpoint plus duration. This result contrasts with our finding on Australian adult data in which we found it was necessary to model the formant trajectory only to separate the diphthongs.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-513"
  },
  "veth98b_icslp": {
   "authors": [
    [
     "Johan de",
     "Veth"
    ],
    [
     "Louis",
     "Boves"
    ]
   ],
   "title": "Effectiveness of phase-corrected rasta for continuous speech recognition",
   "original": "i98_0359",
   "page_count": 4,
   "order": 515,
   "p1": "paper 0359",
   "pn": "",
   "abstract": [
    "Phase-corrected RASTA is a new technique for channel normalization that consists of classical RASTA filtering followed by a phase correction operation. In this manner, the channel bias is as effectively removed as with classical RASTA, without introducing a left context dependency. The performance of the phase-corrected RASTA channel normalization technique was evaluated for a continuous speech recognition task. Using context-independent hidden Markov models we found that phase-corrected RASTA reduces the best-sentence word error rate (WER) by 23 % compared to classical RASTA. For context-dependent models phase-corrected RASTA reduces WER by 15 % compared to classical RASTA.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-514"
  },
  "dharanipragada98_icslp": {
   "authors": [
    [
     "Satya",
     "Dharanipragada"
    ],
    [
     "Ramesh A.",
     "Gopinath"
    ],
    [
     "Bhaskar D.",
     "Rao"
    ]
   ],
   "title": "Techniques for capturing temporal variations in speech signals with fixed-rate processing",
   "original": "i98_0590",
   "page_count": 4,
   "order": 516,
   "p1": "paper 0590",
   "pn": "",
   "abstract": [
    "Fixed-rate feature extraction which is used in most current speech recognizers is equivalent to sampling the feature trajectories at a uniform rate. Often this sampling rate is well below the Nyquist rate and thus leads to distortions in the sampled feature stream due to aliasing. In this paper we explore various techniques, ranging from simple cepstral and spectral smoothing to filtering and data-driven dimensionality expansion using Linear Discriminant Analysis (LDA), to counter aliasing and the variable rate nature of information in speech signals. Smoothing in the spectral domain results in a reduction in the variance of the short term spectral estimates which directly translates to reduction in the variances of the Gaussians in the acoustic models. With these techniques we obtain modest improvements, both in word error rate and robustness to noise, on large vocabulary speech recognition tasks.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-515"
  },
  "du98_icslp": {
   "authors": [
    [
     "Limin",
     "Du"
    ],
    [
     "Kenneth N.",
     "Stevens"
    ]
   ],
   "title": "Automatic detection of landmark for nasal consonants from speech waveform",
   "original": "i98_0302",
   "page_count": 4,
   "order": 517,
   "p1": "paper 0302",
   "pn": "",
   "abstract": [
    "A knowledge-based approach towards automatically detecting nasal landmarks (/m/, /n/, and /ng/) from speech waveform is developed. The acoustic characteristics Fn1 locus calculated on each frame of speech waveform as the mass center of spectrum amplitude in the vicinity of the lowest spectral prominence between 150-1000Hz, and A23 locus calculated on the same speech frame as a band energy between 1000-3000Hz were incorporated together to construct the nasal landmark detector, which alarms at the instants of closure and release of nasal murmur. Experiment observations on the acoustic characteristics of Fn1 and A23 and the nasal consonant landmark detection results on the VCV database are also presented.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-516"
  },
  "dutoit98_icslp": {
   "authors": [
    [
     "Thierry",
     "Dutoit"
    ],
    [
     "Juergen",
     "Schroeter"
    ]
   ],
   "title": "Plug and play software for designing high-level speech processing systems",
   "original": "i98_0520",
   "page_count": 4,
   "order": 518,
   "p1": "paper 0520",
   "pn": "",
   "abstract": [
    "Software engineering for research and development in the area of signal processing is by no means unimportant. For speech processing, in particular, it should be a priority: given the intrinsic complexity of text-to-speech or recognition systems, there is little hope to do state-of-the-art research without solid and extensible code. This paper describes a simple and efficient methodology for the design of maximally reusable and extensible software components for speech and signal processing. The resulting programming paradigm allows software components to be advantageously combined with each other in a way that recalls the concept of hardware plug-and-play, without the need for incorporating complex schedulers to control data flows. It has been successfully used for the design of a software library for high-level speech processing systems at AT&T Labs, as well as for several other large-scale software projects.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-517"
  },
  "girardi98_icslp": {
   "authors": [
    [
     "Alexandre",
     "Girardi"
    ],
    [
     "Kiyohiro",
     "Shikano"
    ],
    [
     "Satoshi",
     "Nakamura"
    ]
   ],
   "title": "Creating speaker independent HMM models for restricted database using STRAIGHT-TEMPO morphing",
   "original": "i98_0687",
   "page_count": 4,
   "order": 519,
   "p1": "paper 0687",
   "pn": "",
   "abstract": [
    "In speaker independent speech recognition, one problem we often face is the insufficient database for training. This problem is even more serious for children database. Besides, adult data when used as children data is affected by differences in pitch and spectral frequency stretch that affects recognition. In this paper, as an approach to solve the above problem, we applied STRAIGHT-TEMPO algorithm to morph adult data towards children data, in order to construct more robust HMM acoustic models, as well as to study the effect of a combined change in the pitch and spectral frequency stretch of the original utterances in the database. Using the morphed database, we analyzed the level of improvement that can be obtained, in terms of recognition rate, compared with non morphed data.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-518"
  },
  "charonnat98_icslp": {
   "authors": [
    [
     "Laure",
     "Charonnat"
    ],
    [
     "Michel",
     "Guitton"
    ],
    [
     "Joel",
     "Crestel"
    ],
    [
     "Gerome",
     "All√©e"
    ]
   ],
   "title": "Restoration of hyperbaric speech by correction of the formants and the pitch",
   "original": "i98_1119",
   "page_count": 4,
   "order": 520,
   "p1": "paper 1119",
   "pn": "",
   "abstract": [
    "This paper describes an hyperbaric speech processing algorithm combining a restoration of the formants position and a correction of the pitch. The pitch is corrected using an algorithm of time-scale modification associated to an oversampling module. This operation does not only perform a shift of the fundamental frequency, but induces a shift of the other frequencies of the signal. This shift, as well as the formants shift due to the hyperbaric environment, is corrected by the formants restoration module, based on the linear speech production model.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-519"
  },
  "gutierrezarriola98_icslp": {
   "authors": [
    [
     "Juana M.",
     "Gutierrez-Arriola"
    ],
    [
     "Yung-Sheng",
     "Hsiao"
    ],
    [
     "Juan Manuel",
     "Montero"
    ],
    [
     "Jos√© Manuel",
     "Pardo"
    ],
    [
     "Donald G.",
     "Childers"
    ]
   ],
   "title": "Voice conversion based on parameter transformation",
   "original": "i98_0468",
   "page_count": 4,
   "order": 521,
   "p1": "paper 0468",
   "pn": "",
   "abstract": [
    "This paper describes a voice conversion system based on parameter transformation. Voice conversion is a process of making one person's voice \"source\" sound like another person's voice \"target\". We will present a voice conversion scheme consisting of three stages. First an analysis is performed on the natural speech to obtain the acoustical parameters. These parameters will be voiced and unvoiced regions, the glottal source model, pitch, energy, formants and bandwidths. Once these parameters have been obtained for two different speakers they are transformed using linear functions. Finally the transformed parameters are synthesized by means of a formant synthesizer. Experiments will show that this scheme is effective in transforming the speaker individuality. It will also be shown that the transformation can not be unique from one speaker to another but it has to be divided in several functions each to transform a certain part of the speech signal. Segmentation based on spectral stability will divide the sentence into parts, for each segment a transformation function will be applied.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-520"
  },
  "tian98_icslp": {
   "authors": [
    [
     "Jilei",
     "Tian"
    ],
    [
     "Ramalingam",
     "Hariharan"
    ],
    [
     "Kari",
     "Laurila"
    ]
   ],
   "title": "Noise robust two-stream auditory feature extraction method for speech recognition",
   "original": "i98_0325",
   "page_count": 4,
   "order": 522,
   "p1": "paper 0325",
   "pn": "",
   "abstract": [
    "Part of the problems in noise robust speech recognition can be attributed to poor acoustic modeling and use of inappropriate features. It is known that the human auditory system is superior to the best speech recognizer currently available. Hence, in this paper, we propose a new two-stream feature extractor that incorporates some of the key functions of the peripheral auditory subsystem. To enhance noise robustness, the input is divided into low-pass and high-pass channels to form so-called static and dynamic streams. These two streams are independently processed and recombined to produce a single stream, containing 13 feature vector components, with improved linguistic information. Speaker-dependent isolated-word recognition tests, using the proposed front-end, produced an average 39% and 17% error rate reductions, over all noisy environments, as compared to the standard Mel Frequency Cepstral Coefficient (MFCC) front-ends with 13 (statics only) and 26 (statics and deltas) feature vector components, respectively.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-521"
  },
  "halberstadt98_icslp": {
   "authors": [
    [
     "Andrew K.",
     "Halberstadt"
    ],
    [
     "James R.",
     "Glass"
    ]
   ],
   "title": "Heterogeneous measurements and multiple classifiers for speech recognition",
   "original": "i98_0396",
   "page_count": 4,
   "order": 523,
   "p1": "paper 0396",
   "pn": "",
   "abstract": [
    "This paper addresses the problem of acoustic phonetic modeling. First, heterogeneous acoustic measurements are chosen in order to maximize the acoustic-phonetic information extracted from the speech signal in preprocessing. Second, classifier systems are presented for successfully utilizing high-dimensional acoustic measurement spaces. The techniques used for achieving these two goals can be broadly categorized as hierarchical, committee-based, or a hybrid of these two. This paper presents committee-based and hybrid approaches. In context-independent classification and context-dependent recognition on the TIMIT core test set using 39 classes, the system achieved error rates of 18.3% and 24.4%, respectively. These error rates are the lowest we have seen reported on these tasks. In addition, experiments with a telephone-based weather information word recognition task led to word error rate reductions of 10-16%.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-522"
  },
  "harte98_icslp": {
   "authors": [
    [
     "Naomi",
     "Harte"
    ],
    [
     "Saeed",
     "Vaseghi"
    ],
    [
     "Ben",
     "Milner"
    ]
   ],
   "title": "Joint recognition and segmentation using phonetically derived features and a hybrid phoneme model",
   "original": "i98_0259",
   "page_count": 4,
   "order": 524,
   "p1": "paper 0259",
   "pn": "",
   "abstract": [
    "This paper encompasses the approaches of segmental modelling and the use of dynamic features in addressing the constraints of the IID assumption in standard HMM. Phonetic features are introduced which capture the transitional dynamics across a phoneme unit via a DCT transformation of a variable length segment. Alongside this, the use of a hybrid phoneme model is proposed. Classification experiments demonstrate the potential of these features and this model to match the performance of standard HMM. The extension of these features to full recognition is explored and details of a novel recognition framework presented alongside preliminary results. Lattice rescoring based on these models and features is also explored. This reduces the set of segmentations considered allowing a more detailed exploration of the nature of the model and features and the challenges in using the proposed recognition strategy.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-523"
  },
  "hermansky98_icslp": {
   "authors": [
    [
     "Hynek",
     "Hermansky"
    ],
    [
     "Sangita",
     "Sharma"
    ]
   ],
   "title": "TRAPS - classifiers of temporal patterns",
   "original": "i98_0615",
   "page_count": 4,
   "order": 525,
   "p1": "paper 0615",
   "pn": "",
   "abstract": [
    "The work proposes a radically different set of features for ASR where TempoRAl Patterns of spectral energies are used in place of the conventional spectral patterns. The approach has several inherent advantages, among them robustness to stationary or slowly varying disturbances.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-524"
  },
  "holmes98b_icslp": {
   "authors": [
    [
     "John N.",
     "Holmes"
    ]
   ],
   "title": "Robust measurement of fundamental frequency and degree of voicing",
   "original": "i98_0351",
   "page_count": 4,
   "order": 526,
   "p1": "paper 0351",
   "pn": "",
   "abstract": [
    "Both for robust fundamental frequency (F0) measurement and to provide a degree of voicing indication, a new algorithm has been developed based on multi-channel autocorrelation analysis. The speech is filtered into eight separate frequency bands, representing the lowest 500 Hz and seven overlapping band-pass channels each about 1000 Hz wide. The outputs of all the band-pass channels are full-wave rectified and band-pass filtered between 50 Hz and 500 Hz. Autocorrelation functions are calculated for the signals from all eight channels, and these functions are used both for the F0 measurement and for the voicing indication. Optional dynamic programming is provided to maximize the continuity of position of the correlation peaks selected for fundamental period measurement. The algorithm has been designed for coding onto a 16-bit integer DSP, using less than 4 MIPS processing power and 1500 words of data memory.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-525"
  },
  "holzrichter98_icslp": {
   "authors": [
    [
     "John F.",
     "Holzrichter"
    ],
    [
     "Gregory C.",
     "Burnett"
    ],
    [
     "Todd J.",
     "Gable"
    ],
    [
     "Lawrence C.",
     "Ng"
    ]
   ],
   "title": "Micropower electro-magnetic sensors for speech characterization, recognition, verification, and other applications",
   "original": "i98_1064",
   "page_count": 4,
   "order": 527,
   "p1": "paper 1064",
   "pn": "",
   "abstract": [
    "Experiments have been conducted using a variety of very low power EM sensors that measure articulator motions occurring in two frequency bands, 1 Hz to 20Hz, and 70 Hz to 7 kHz. They enable noise free estimates of a voiced excitation function, accurate pitch measurements, generalized transfer function descriptions, and detection of vocal articulator motions.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-526"
  },
  "shen98b_icslp": {
   "authors": [
    [
     "Jia-Lin",
     "Shen"
    ],
    [
     "Jeih-Weih",
     "Hung"
    ],
    [
     "Lin-Shan",
     "Lee"
    ]
   ],
   "title": "Robust entropy-based endpoint detection for speech recognition in noisy environments",
   "original": "i98_0232",
   "page_count": 4,
   "order": 528,
   "p1": "paper 0232",
   "pn": "",
   "abstract": [
    "This paper presents an entropy-based algorithm for accurate and robust endpoint detection for speech recognition under noisy environments. Instead of using the conventional energy-based features, the spectral entropy is developed to identify the speech segments accurately. Experimental results show that this algorithm outperforms the energy-based algorithms in both detection accuracy and recognition performance under noisy environments, with an average error rate reduction of more than 16%.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-527"
  },
  "shen98c_icslp": {
   "authors": [
    [
     "Jia-Lin",
     "Shen"
    ],
    [
     "Wen-Liang",
     "Hwang"
    ]
   ],
   "title": "Statistical integration of temporal filter banks for robust speech recognition using linear discriminant analysis (LDA)",
   "original": "i98_0447",
   "page_count": 4,
   "order": 529,
   "p1": "paper 0447",
   "pn": "",
   "abstract": [
    "This paper presents a study on statistical integration of temporal filter banks for robust speech recognition using linear discriminant analysis (LDA). The temporal properties of stationary features were first captured and represented using a bank of well-defined temporal filters. Then these derived temporal features can be integrated and compressed using the LDA technique. Experimental results show that the recognition performance can be significantly improved both in clean and in noisy environments.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-528"
  },
  "iskra98_icslp": {
   "authors": [
    [
     "Dorota J.",
     "Iskra"
    ],
    [
     "William H.",
     "Edmondson"
    ]
   ],
   "title": "Feature-based approach to speech recognition",
   "original": "i98_0778",
   "page_count": 3,
   "order": 530,
   "p1": "paper 0778",
   "pn": "",
   "abstract": [
    "The alternative approach to speech recognition proposed here is based on pseudo-articulatory representations (PARs), which can be described as approximations of distinctive features, and aims to establish a mapping between them and their acoustic specifications (in this case cepstral coefficients). This mapping which is used as the basis for recognition is first done for vowels. It is obtained using multiple regression analysis after all the vowels have been described in terms of phonetic features and an average cepstral vector has been calculated for each of them. Based on this vowel model, the PAR values are calculated for consonants. At this point recognition is performed using a brute search mechanism to derive PAR trajectories and subsequently dynamic programming to obtain a phone sequence. The results are not as good as when hidden Markov modelling is used, but very promising taking into account the early stage of the experiments and the novelty of the approach.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-529"
  },
  "kamata98_icslp": {
   "authors": [
    [
     "Hiroyuki",
     "Kamata"
    ],
    [
     "Akira",
     "Kaneko"
    ],
    [
     "Yoshihisa",
     "Ishida"
    ]
   ],
   "title": "Periodicity emphasis of voice wave using nonlinear IIR digital filters and its applications",
   "original": "i98_1016",
   "page_count": 4,
   "order": 531,
   "p1": "paper 1016",
   "pn": "",
   "abstract": [
    "We propose a new method for emphasizing the periodicity of voice wave using chaotic neurons, and propose a practical method to detect the fundamental frequency of human voice. The chaotic neuron is a kind of nonlinear recursive mapping proposed in the field of nonlinear theory and is usually used to generate the chaotic signal. Besides, when the chaotic neuron is considered in the theory of linear signal processing, we can interpret that the chaotic neuron is a positive feedback IIR digital filter of first order, therefore, it gives a spectrum slope to the target spectrum of input speech signal. In this study, we try to tune up the chaotic neuron to amplify the low frequency components to emphasize the component of fundamental frequency. As the result, spectrum peaks based on the formants are canceled, the spectrum peak corresponded to the fundamental frequency of voiced speech can be detected easily. In addition, a nonlinear function that has a dead band is included in the feedback loop of the chaotic neuron. As its effect, noise components of unvoiced speech are not amplified.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-530"
  },
  "king98_icslp": {
   "authors": [
    [
     "Simon",
     "King"
    ],
    [
     "Todd",
     "Stephenson"
    ],
    [
     "Stephen",
     "Isard"
    ],
    [
     "Paul",
     "Taylor"
    ],
    [
     "Alex",
     "Strachan"
    ]
   ],
   "title": "Speech recognition via phonetically featured syllables",
   "original": "i98_0557",
   "page_count": 4,
   "order": 532,
   "p1": "paper 0557",
   "pn": "",
   "abstract": [
    "We describe a speech recogniser which uses a speech production-motivated phonetic-feature description of speech. We argue that this is a natural way to describe the speech signal and offers an efficient intermediate parameterisation for use in speech recognition. We also propose to model this description at the syllable rather than phone level. The ultimate goal of this work is to generate syllable models whose parameters explicitly describe the trajectories of the phonetic features of the syllable. We hope to move away from Hidden Markov Models (HMMs) of context-dependent phone units. As a step towards this, we present a preliminary system which consists of two parts: recognition of the phonetic features from the speech signal using a neural network; and decoding of the feature-based description into phonemes using HMMs.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-531"
  },
  "koreman98_icslp": {
   "authors": [
    [
     "Jacques",
     "Koreman"
    ],
    [
     "Bistra",
     "Andreeva"
    ],
    [
     "William J.",
     "Barry"
    ]
   ],
   "title": "Do phonetic features help to improve consonant identification in ASR?",
   "original": "i98_0549",
   "page_count": 6,
   "order": 533,
   "p1": "paper 0549",
   "pn": "",
   "abstract": [
    "The hidden Markov modelling experiments presented in this paper show that consonant identification results can be improved substantially if a neural network is used to extract linguistically relevant information from the acoustic signal before applying hidden Markov modelling. The neural network - or in this case a combination of two Kohonen networks - takes 12 mel-frequency cepstral coefficients, overall energy and the corresponding delta parameters as input and outputs distinctive phonetic features, like [(plus-minus)uvular] and [(plus-minus)plosive]. Not only does this preprocessing of the data lead to better consonant identification rates, the confusions that occur between the consonants are less severe from a phonetic viewpoint, as is demonstrated. One reason for the improved consonant identification is that the acoustically variable consonant realisations can be mapped onto identical phonetic features by the neural network. This makes the input to hidden Markov modelling more homogenous and improves consonant identification. Furthermore, by using phonetic features the neural network helps the system to focus on linguistically relevant information in the acoustic signal.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-532"
  },
  "kuwabara98_icslp": {
   "authors": [
    [
     "Hisao",
     "Kuwabara"
    ]
   ],
   "title": "Perceptual and acoustic properties of phonemes in continuous speech for different speaking rate",
   "original": "i98_0034",
   "page_count": 4,
   "order": 534,
   "p1": "paper 0034",
   "pn": "",
   "abstract": [
    "Investigations have been made on the perceptual properties of CV-syllables taken out from continuous speech spoken with three different speaking rate. Fifteen short Japanese sentences were spoke by a male speaker with 1) fast speaking rate, 2) normal rate and 3) slow rate. The results reveal that individual syllables do not have enough phonetic information to be correctly identified especially for the fast speech. The average identification of syllables for the fast speech is 35%, 59% for the normal and 86% for the slow. It has been found that syllable perception almost entirely depends on the consonant identification.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-533"
  },
  "lee98g_icslp": {
   "authors": [
    [
     "Joohun",
     "Lee"
    ],
    [
     "Ki Yong",
     "Lee"
    ]
   ],
   "title": "On robust sequential estimator based on t-distribution with forgetting factor for speech analysis",
   "original": "i98_0296",
   "page_count": 4,
   "order": 535,
   "p1": "paper 0296",
   "pn": "",
   "abstract": [
    "In this paper, to estimate the time-varying speech parameters having non-Gaussian excitation source, we use the robust sequential estimator(RSE) based on t-distribution and introduce the forgetting factor. By using the RSE based on t-distribution with small degree of freedom, we can alleviate efficiently the effects of outliers to obtain the better performance of parameter estimation. Moreover, by the forgetting factor, the proposed algorithm can estimate the accurate parameters under the rapid variation of speech signal.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-534"
  },
  "long98_icslp": {
   "authors": [
    [
     "Christopher John",
     "Long"
    ],
    [
     "Sekharajit",
     "Datta"
    ]
   ],
   "title": "Discriminant wavelet basis construction for speech recognition",
   "original": "i98_0802",
   "page_count": 3,
   "order": 536,
   "p1": "paper 0802",
   "pn": "",
   "abstract": [
    "In this paper, a new feature extraction methodology based on Wavelet Transforms is examined, which unlike some conventional parameterisation techniques, is flexible enough to cope with the broadly differing characteristics of typical speech signals. A training phase is involved during which the final classifier is invoked to associate a cost function (a proxy for misclassification) with a given resolution. The sub spaces are then searched and pruned to provide a Wavelet Basis best suited to the classification problem. Comparative results are given illustrating some improvement over the Short-Time Fourier Transform using two differing subclasses of speech.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-535"
  },
  "matsumoto98_icslp": {
   "authors": [
    [
     "Hiroshi",
     "Matsumoto"
    ],
    [
     "Yoshihisa",
     "Nakatoh"
    ],
    [
     "Yoshinori",
     "Furuhata"
    ]
   ],
   "title": "An efficient mel-LPC analysis method for speech recognition",
   "original": "i98_0047",
   "page_count": 4,
   "order": 537,
   "p1": "paper 0047",
   "pn": "",
   "abstract": [
    "This paper proposes a simple and efficient time domain technique to estimate an all-pole model on a mel-frequency axis (Mel-LPC), i.e., a bilinear transformed all-pole model by Strube. Autocorrelation coefficients on the mel-frequency axis are exactly derived by computing cross-correlation coefficients between speech signal and all-pass filtered one without any approximation. This method requires only two-fold computational cost as compared to conventional linear prediction analysis. The recognition performance of mel-cepstral parameters obtained by the Mel LPC analysis is compared with those of conventional LP mel-cepstra and the mel-frequency cepstrum coefficients (MFCC) through gender-dependent phoneme and word recognition tests. The results show that the Mel-LPC cepstrum attains a significant improvement in recognition accuracy over conventional LP mel-cepstrum, and gives slightly higher accuracy for male speakers and slightly lower accuracy for female speakers than MFCC.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-536"
  },
  "mcmahon98_icslp": {
   "authors": [
    [
     "Philip",
     "McMahon"
    ],
    [
     "Paul",
     "McCourt"
    ],
    [
     "Saeed",
     "Vaseghi"
    ]
   ],
   "title": "Discriminative weighting of multi-resolution sub-band cepstral features for speech recognition",
   "original": "i98_0315",
   "page_count": 4,
   "order": 538,
   "p1": "paper 0315",
   "pn": "",
   "abstract": [
    "This paper explores possible strategies for the recombination of independent multi-resolution sub-band based recognisers. The multi-resolution approach is based on the premise that additional cues for phonetic discrimination may exist in the spectral correlates of a particular sub-band, but not in another. Weights are derived via discriminative training using the 'Minimum Classification Error' (MCE) criterion on log-likelihood scores. Using this criterion the weights for correct and competing classes are adjusted in opposite directions, thus conveying the sense of enforcing separation of confusable classes. Discriminative re-combination is shown to provide significant increases for both phone classification and continuous recognition tasks on the TIMIT database. Weighted recombination of independent multi-resolution sub-band models is also shown to provide robustness improvements in broadband noise.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-537"
  },
  "meron98_icslp": {
   "authors": [
    [
     "Yoram",
     "Meron"
    ],
    [
     "Keikichi",
     "Hirose"
    ]
   ],
   "title": "Separation of singing and piano sounds",
   "original": "i98_0416",
   "page_count": 4,
   "order": 539,
   "p1": "paper 0416",
   "pn": "",
   "abstract": [
    "Our goal is to develop a singing synthesis system, in which \"singing units\" are automatically extracted from existing musical recordings of a singer accompanied by a musical instrument (piano). This paper concentrates on the problem of separating singer from accompaniment. Existing separation methods require the knowledge of the exact frequencies of the signals to be separated, and are prone to degrading the quality of the separated signals. In this paper, we use the framework of the sinusoidal modeling approach. We suggest the use of further sources of information, available to the specific task: advance knowledge of the music score, knowledge about the piano sound, and a relatively large database of the piano and singer signals, which is used to build a model of the piano sound. Results show that using musical score information and piano note modeling can improve separation quality.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-538"
  },
  "minematsu98_icslp": {
   "authors": [
    [
     "Nobuaki",
     "Minematsu"
    ],
    [
     "Seiichi",
     "Nakagawa"
    ]
   ],
   "title": "Modeling of variations in cepstral coefficients caused by F0 changes and its application to speech processing",
   "original": "i98_0052",
   "page_count": 4,
   "order": 540,
   "p1": "paper 0052",
   "pn": "",
   "abstract": [
    "Correlation of spectral variations and F0 changes in a vowel is firstly analyzed, where the variations are also compared to VQ distortions calculated in a five-vowel space. It is shown that the F0 change approximately by a half octave produces the spectral variation comparable to the VQ distortion when the codebook size is the number of the vowels. Next, a model to predict the cepstral coefficients' variations caused by the F0 changes is built using the multivariate regression analysis. Experiments show that the generated frame by the model has a remarkably small distance to the target frame. Furthermore, the model is evaluated separately in terms of a spectral envelope predictor with a given F0 and a mapping function of feature sub-spaces. While the models should be built dependently on phonemes and speakers as the former, adequate selection of parameters can enable the speaker/phoneme-independent models to work effectively as the latter.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-539"
  },
  "niyogi98_icslp": {
   "authors": [
    [
     "Partha",
     "Niyogi"
    ],
    [
     "Partha",
     "Mitra"
    ],
    [
     "Man Mohan",
     "Sondhi"
    ]
   ],
   "title": "A detection framework for locating phonetic events",
   "original": "i98_0665",
   "page_count": 4,
   "order": 541,
   "p1": "paper 0665",
   "pn": "",
   "abstract": [
    "We consider the problem of detecting stop consonants in continuously spoken speech. We pose the problem as one of finding the optimal filter (linear or non-linear) that operates on a particular appropriately chosen representation. We discuss the performance of several variants of a canonical stop detector and consider its implications for human and machine speech recognition.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-540"
  },
  "nadeu98_icslp": {
   "authors": [
    [
     "Climent",
     "Nadeu"
    ],
    [
     "Felix",
     "Galindo"
    ],
    [
     "Jaume",
     "Padrell"
    ]
   ],
   "title": "On frequency averaging for spectral analysis in speech recognition",
   "original": "i98_1135",
   "page_count": 4,
   "order": 542,
   "p1": "paper 1135",
   "pn": "",
   "abstract": [
    "Many speech recognition systems use logarithmic filter-bank energies or a linear transformation of them to represent the speech signal. Usually, each of those energies is routinely computed as a weighted average of the periodogram samples that lie in the corresponding frequency band. In this work, we attempt to gain an insight into the statistical properties of the frequency-averaged periodogram (FAP) from which those energies are samples. Thus, we have shown that the FAP is statistically and asymptotically equivalent to a multiwindow estimator that arises from the Thomson[HEX 146]s optimization approach and uses orthogonal sinusoids as windows. The FAP and other multiwindow estimators are tested in a speech recognition application, observing the influence of several design factors. Particularly, a technique that is computationally simple like the FAP[HEX 146]s one, and which is equivalent to use multiple cosine windows, appears as an alternative to be taken into consideration.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-541"
  },
  "namba98_icslp": {
   "authors": [
    [
     "Munehiro",
     "Namba"
    ],
    [
     "Yoshihisa",
     "Ishida"
    ]
   ],
   "title": "Wavelet transform domain blind equalization and its application to speech analysis",
   "original": "i98_0055",
   "page_count": 4,
   "order": 543,
   "p1": "paper 0055",
   "pn": "",
   "abstract": [
    "In this paper, a wavelet transform domain realization of the blind equalization technique termed as EVA is applied to speech analysis. The conventional linear prediction problem can be viewed as a constrained blind equalization problem. Because the EVA does not impose any restriction to the probability distribution in the input (the glottal excitation), the principal features of speech can be effectively separated from a speech in a short duration. The computational complexity will be a problem, but the proposed implementation in a wavelet transform domain promotes the faster convergence in the analysis of speech signal.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-542"
  },
  "pearson98b_icslp": {
   "authors": [
    [
     "Steve",
     "Pearson"
    ]
   ],
   "title": "A novel method of formant analysis and glottal inverse filtering",
   "original": "i98_0647",
   "page_count": 4,
   "order": 544,
   "p1": "paper 0647",
   "pn": "",
   "abstract": [
    "This paper presents a class of methods for automatically extracting formant parameters from speech. The methods rely on an iterative optimization algorithm. It was found that formant parameter data derived with these methods was less prone to discontinuity errors than conventional methods. Also, experiments were conducted that demonstrated that these methods are capable of better accuracy in formant estimation than LPC, especially for the first formant. In some cases, the analytic (non-iterative) solution has been derived, making real time applications feasible. The main target that we have been pursuing is text-to-speech (TTS) conversion. These methods are being used to automatically analyze a concatenation database, without the need for a tuning phase to fix errors. In addition, they are instrumental in realizing high quality pitch tracking, and pitch epoch marking.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-543"
  },
  "araujo98_icslp": {
   "authors": [
    [
     "Antonio J.",
     "Araujo"
    ],
    [
     "Vitor C.",
     "Pera"
    ],
    [
     "Marcio N.",
     "Souza"
    ]
   ],
   "title": "Vector quantizer acceleration for an automatic speech recognition application",
   "original": "i98_0319",
   "page_count": 4,
   "order": 545,
   "p1": "paper 0319",
   "pn": "",
   "abstract": [
    "For a real-time application of an automatic speech recognition system, hardware acceleration can be the key to reduce the execution time. Vector quantization is an important task that a recognizer based on discrete hidden Markov models must perform. Due to the amount of floating point operations executed, the vector quantizer is an excellent candidate to be accelerated by customized hardware. The design, implementation and obtained results of a hardware solution based on field programmable gate array devices are presented.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-544"
  },
  "pfitzinger98_icslp": {
   "authors": [
    [
     "Hartmut R.",
     "Pfitzinger"
    ]
   ],
   "title": "Local speech rate as a combination of syllable and phone rate",
   "original": "i98_0523",
   "page_count": 4,
   "order": 546,
   "p1": "paper 0523",
   "pn": "",
   "abstract": [
    "This investigation focuses on deriving local speech rate directly out of the speech signal, which differs from syllable rate and from phone rate. Since local speech rate modifies acoustic cues (e.g. transitions), phones, and even words, it is one of the most important prosodic cues. Our local speech rate estimation method is based on a linear combination of the syllable rate and of the phone rate, since this investigation strongly suggests that neither the syllable rate nor the phone rate on its own represent the speech rate sufficiently. Our results show (a) that perceptual local speech rate correlates better with local syllable rate than with local phone rate (r=0.81>r=0.73), (b) that the linear combination of both is well-correlated with perceptual local speech rate (r=0.88), and (c) that it is now possible to calculate the perceptual local speech rate with the aid of automatic phone boundary detectors and syllable nuclei detectors directly from the speech signal.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-545"
  },
  "rossato98_icslp": {
   "authors": [
    [
     "Solange",
     "Rossato"
    ],
    [
     "Gang",
     "Feng"
    ],
    [
     "Rafael",
     "Laboissiere"
    ]
   ],
   "title": "Recovering gestures from speech signals: a preliminary study for nasal vowels",
   "original": "i98_0540",
   "page_count": 4,
   "order": 547,
   "p1": "paper 0540",
   "pn": "",
   "abstract": [
    "For nasal vowels, a gesture as simple as the lowering of the velum produces complex acoustic spectra. However, we still find a relative simplicity in the perceptual space; nasality is perceived easily. In this preliminary study, we use statistic method to recover the gesture of the velum. In order to reduce the extreme variability of nasal vowels, we introduced a simulation based on Maeda's model instead of using a natural speech signal. In previous studies, nasality is supposed to increase either with size of the nasal area or with the area ratio between nasal and oral tracts at the extremity of the velum. In this work, both types of data are considered and analyzed with linear and non-linear tools. Finally, statistic inference is described and results are given for various areas of the nasal tract entrance and for various area ratios. The results show that velar port area is correctly estimated for small values while area ratio is a better parameter when velar port area increases.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-546"
  },
  "ruske98_icslp": {
   "authors": [
    [
     "Guenther",
     "Ruske"
    ],
    [
     "Robert",
     "Faltlhauser"
    ],
    [
     "Thilo",
     "Pfau"
    ]
   ],
   "title": "Extended linear discriminant analysis (ELDA) for speech recognition",
   "original": "i98_0100",
   "page_count": 4,
   "order": 548,
   "p1": "paper 0100",
   "pn": "",
   "abstract": [
    "Speech recognition systems based on hidden Markov models (HMM) favourably apply a linear discriminant analysis transform (LDA) which yields low-dimensional and uncorrelated feature components. However, since the distributions in the HMM states usually are modeled by mixture gaussian densities, the description by second-order moments no longer is correct. For this purpose we introduced a new \"extended linear discriminant analysis\" transform (ELDA) which starts from conventional LDA. The ELDA transform is derived by use of a gradient descent optimization procedure based on a \"minimum classification error\" (MCE) principle, which is applied to the original high-dimensional pattern space. The transform matrix, the best fitting prototype of the correct class (i.e. HMM state) and the nearest rival are adapted. We developed a method which additionally updates all prototypes by a separate maximum likelihood (ML) estimation step. This avoids that such means and covariances, which mostly remain unaffected by the MCE procedure, may diverge step by step.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-547"
  },
  "samouelian98_icslp": {
   "authors": [
    [
     "Ara",
     "Samouelian"
    ],
    [
     "Jordi",
     "Robert-Ribes"
    ],
    [
     "Mike",
     "Plumpe"
    ]
   ],
   "title": "Speech, silence, music and noise classification of TV broadcast material",
   "original": "i98_0620",
   "page_count": 4,
   "order": 549,
   "p1": "paper 0620",
   "pn": "",
   "abstract": [
    "Speech processing can be of great help for indexing and archiving TV broadcast material. Broadcasting station standards will be soon digital. There will be a huge increase in the use of speech processing techniques for maintaining the archives as well as accessing them. We present an application of information theory to the classification and automatic labelling of TV broadcast material into speech, music and noise. We use information theory to construct a decision tree from several different TV programs and then apply it to a different set of TV programs. We present classification results on training and test data sets. Frame level correct classification rate, for training data was 95.5%, while for test data it ranged from 60.4% to 84.5%, depending on TV program type. At the segment level, correct recognition rate and accuracy on train data were 100% and 95.1%, respectively while for test data the % correct ranged from 80% to 100% and %accuracy ranged from 64.7% to 100%.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-548"
  },
  "schoentgen98_icslp": {
   "authors": [
    [
     "Jean",
     "Schoentgen"
    ],
    [
     "Alain",
     "Soquet"
    ],
    [
     "V√©ronique",
     "Lecuit"
    ],
    [
     "Sorin",
     "Ciocea"
    ]
   ],
   "title": "The relation between vocal tract shape and formant frequencies can be described by means of a system of coupled differential equations",
   "original": "i98_1104",
   "page_count": 4,
   "order": 550,
   "p1": "paper 1104",
   "pn": "",
   "abstract": [
    "The objective is to present a formalism which offers a framework for several articulatory models and notions such as targets, gestures and the quantal principle of speech production. The formalism is based on coupled differential equations that relate the vocal tract shape to its eigenfrequencies. The shape of the vocal tract is described either directly by means of an area function model or indirectly via an articulatory model. Possible synergetic relations between phonetic gestures or targets and the quantal principle of speech production are discussed.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-549"
  },
  "suh98_icslp": {
   "authors": [
    [
     "Youngjoo",
     "Suh"
    ],
    [
     "Kyuwoong",
     "Hwang"
    ],
    [
     "Oh-Wook",
     "Kwon"
    ],
    [
     "Jun",
     "Park"
    ]
   ],
   "title": "Improving speech recognizer by broader acoustic-phonetic group classification",
   "original": "i98_0638",
   "page_count": 0,
   "order": 551,
   "p1": "paper 0638",
   "pn": "",
   "abstract": [
    "We propose a new approach to improve the performance of speech recognizers by utilizing acoustic-phonetic knowledge sources. We use the unvoiced, voiced. and silence (UVS) group information of the input speech signal in the conventional speech recognizer. We extract the UVS information by, using a recurrent neural network (RNN). generate a rule-based score, and then add the score representing the INS information to the conventional spectral feature-driven score in the search module. Experimental results showed that the approach reduces 9% of errors in a 5000- word Korean spontaneous speech recognition domain.\n",
    ""
   ]
  },
  "thorpe98_icslp": {
   "authors": [
    [
     "C. William",
     "Thorpe"
    ]
   ],
   "title": "Separation of speech source and filter by time-domain deconvolution",
   "original": "i98_0244",
   "page_count": 4,
   "order": 552,
   "p1": "paper 0244",
   "pn": "",
   "abstract": [
    "A subtractive deconvolution algorithm is described which allows one to separate a voiced speech signal into two components, representing the time-invariant and dynamic parts of the signal respectively. The resulting dynamic component can be encoded at a lower data rate than can the original speech signal. Results are presented which validate the utility of decomposing the speech waveform into these two components, and demonstrate the ability of the algorithm to represent speech signals at a reduced data rate.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-550"
  },
  "tolba98b_icslp": {
   "authors": [
    [
     "Hesham",
     "Tolba"
    ],
    [
     "Douglas",
     "O'Shaughnessy"
    ]
   ],
   "title": "On the application of the AM-FM model for the recovery of missing frequency bands of telephone speech",
   "original": "i98_0343",
   "page_count": 4,
   "order": 553,
   "p1": "paper 0343",
   "pn": "",
   "abstract": [
    "This study presents a novel technique to reconstruct the missing frequency bands of bandlimited telephone speech signals. This technique is based on the Amplitude and Frequency Modulation (AM-FM) model, which models the speech signal as the sum of N successive AM-FM signals. Based on a least-mean-square error criterion, each AM-FM signal is modified using an iterative algorithm in order to regenerate the high-frequency AM-FM signals. These modified signals are then combined in order to reconstruct the broadband speech signal. Experiments were conducted using speech signals extracted from the NTIMIT database. Such experiments demonstrate the ability of the algorithm for speech recovery, in terms of a comparison between the original and synthesized speech and informal listening tests.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-551"
  },
  "yang98e_icslp": {
   "authors": [
    [
     "Chang-Sheng",
     "Yang"
    ],
    [
     "Hideki",
     "Kasuya"
    ]
   ],
   "title": "Estimation of voice source and vocal tract parameters using combined subspace-based and amplitude spectrum-based algorithm",
   "original": "i98_1143",
   "page_count": 4,
   "order": 554,
   "p1": "paper 1143",
   "pn": "",
   "abstract": [
    "In this paper, a high quality pole-zero speech analysis technique is proposed. The speech production process is represented by a source-filter model. A Rosenberg-Klatt model is used to approximate a voicing source waveform for voiced speech, whereas a white noise is assumed for unvoiced. The vocal tract transfer function is represented by a pole-zero filter. For voiced speech, parameters of the source model are jointly estimated with those of the vocal tract filter. A combined algorithm is developed to estimate the vocal tract parameters, i.e., formants and anti-formants which are calculated from the poles and zeros of the filter. By the algorithm, poles are estimated based on a subspace algorithm, while zeros are estimated from the amplitude spectrum. For unvoiced speech, an AR model is assumed, which can be solved by LPC analysis. An experiment using synthesized nasal sounds shows that the poles and zeros are estimated quite accurately.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-552"
  },
  "zheng98_icslp": {
   "authors": [
    [
     "Fang",
     "Zheng"
    ],
    [
     "Zhanjiang",
     "Song"
    ],
    [
     "Ling",
     "Li"
    ],
    [
     "Wenjian",
     "Yu"
    ],
    [
     "Fengzhou",
     "Zheng"
    ],
    [
     "Wenhu",
     "Wu"
    ]
   ],
   "title": "The distance measure for line spectrum pairs applied to speech recognition",
   "original": "i98_0171",
   "page_count": 4,
   "order": 555,
   "p1": "paper 0171",
   "pn": "",
   "abstract": [
    "The Line Spectrum Pair (LSP) based on the principle of linear predictive coding (LPC) plays a very important role in the speech synthesis; it has many interesting properties. Several famous speech compression / decompression algorithms, including the famous code excited linear predictive coding (CELP), are based on the LSP analysis, where the information loss or predicting errors are often very small due to the LSP's characteristics. Unfortunately till now there is not a satisfying kind of distance measure available for LSP so that this kind of features can be used for speech recognition applications. In this paper, the principle of LSP analysis is studied at first, and then several distance measures for LSP are proposed which can describe very well the difference between two groups of different LSP parameters. Experimental results are also given to show the efficiency of the proposed distance measures.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-553"
  },
  "ainsworth98b_icslp": {
   "authors": [
    [
     "William A.",
     "Ainsworth"
    ],
    [
     "Charles R.",
     "Day"
    ],
    [
     "Georg F.",
     "Meyer"
    ]
   ],
   "title": "Improving pitch estimation with short duration speech samples",
   "original": "i98_0512",
   "page_count": 4,
   "order": 556,
   "p1": "paper 0512",
   "pn": "",
   "abstract": [
    "Hermes' Sub Harmonic Summation (SHS) pitch determination algorithm is an effective technique for extracting the percept of pitch from human speech. Effective determination of the pitch in a passage of speech is believed to be fundamental for higher level speech processing applications such as speech or speaker recognition. Of particular interest is the need to extract pitch from speech in less than ideal conditions eg. in the presence of noise or using very short analysis windows. In an attempt to deliver accurate pitch estimates from relatively short analysis windows this paper describes an evaluation of two forms of the SHS procedure: in one case, FFT-SHS, the procedure uses the conventional Fast Fourier Transform (FFT) in its spectral analysis step; in the second case, RAFT-SHS, the ReAssigned Fourier Transform (RAFT) technique is used instead of the FFT.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-554"
  },
  "kawahara98_icslp": {
   "authors": [
    [
     "Hideki",
     "Kawahara"
    ],
    [
     "Alain de",
     "Cheveigne"
    ],
    [
     "Roy D.",
     "Patterson"
    ]
   ],
   "title": "An instantaneous-frequency-based pitch extraction method for high-quality speech transformation: revised TEMPO in the STRAIGHT-suite",
   "original": "i98_0659",
   "page_count": 4,
   "order": 557,
   "p1": "paper 0659",
   "pn": "",
   "abstract": [
    "A new source information extraction algorithm is proposed to provide a reliable source signal for an extremely high-quality speech analysis, modification, and transformation system called STRAIGHT-suite (Speech Transformation and Representation based on Adaptive Interpolation of weiGHTed spectrogram). The proposed method makes use of instantaneous frequencies in harmonic components based on their reliability. A performance evaluation is conducted using a simultaneous EGG (Electroglottograph) recording as the reference signal. The error variance for F0 extraction using the proposed algorithm is shown to be about 1/3 that of the previous F0 extraction method used in STRAIGHT-suite, although the previous algorithm is still competitive with conventional F0 extraction methods.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-555"
  },
  "aikawa98_icslp": {
   "authors": [
    [
     "Kiyoaki",
     "Aikawa"
    ]
   ],
   "title": "Speaker-independent speech recognition using micro segment spectrum integration",
   "original": "i98_0262",
   "page_count": 4,
   "order": 558,
   "p1": "paper 0262",
   "pn": "",
   "abstract": [
    "This paper proposes a new spectral estimation method for automatic speech recognition. The spectrum estimated with the conventional data window of around 30 ms shows harmonic structure in the voiced portions of speech data. The harmonic frequency interval is often comparable to the formant frequency interval for female voices with high F0, which results in spectral estimation error. The new idea is to estimate spectrum by taking the Lp norm of the time series of the spectrum obtained from a very short speech segment. The new method, called the micro-segment spectrum integration, provides (1) precise spectral estimation not affected by harmonic structure, and (2) noise-robustness by suppressing noisy speech segments. Phoneme recognition experiments demonstrate that the micro-segment spectrum integration method outperforms conventional spectral estimation methods.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-556"
  },
  "funaki98_icslp": {
   "authors": [
    [
     "Keiichi",
     "Funaki"
    ],
    [
     "Yoshikazu",
     "Miyanaga"
    ],
    [
     "Koji",
     "Tochinai"
    ]
   ],
   "title": "On robust speech analysis based on time-varying complex AR model",
   "original": "i98_1001",
   "page_count": 4,
   "order": 559,
   "p1": "paper 1001",
   "pn": "",
   "abstract": [
    "We have already developed time-varying complex AR (TV-CAR) parameter estimation based on minimizing mean square error (MMSE) for analytic speech signal. Although the MMSE approach is commonly and successfully applied in various parameter estimation such as conventional LPC, it is well-known that an MMSE method easily suffers from biased and inaccurate spectrum estimation due to non-Gaussian nature of glottal excitation for voiced speech in the context of speech analysis. This paper offers robust parameter estimation algorithm for the TV-CAR model by applying Huber's robust M-estimation approach and two kinds of robust algorithms are derived: Newton-type algorithm and weighted least squares (WLS) algorithm. The preliminary experiments with synthetic signal generated by glottal source model excitation and natural speech uttered by female speaker demonstrate that the time-varying complex AR method is sufficiently robust against non-Gaussian nature of glottal source excitation owing to the improved resolution in the frequency domain.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-557"
  },
  "hermansky98b_icslp": {
   "authors": [
    [
     "Hynek",
     "Hermansky"
    ],
    [
     "Narendranath",
     "Malayath"
    ]
   ],
   "title": "Spectral basis functions from discriminant analysis",
   "original": "i98_0616",
   "page_count": 4,
   "order": 560,
   "p1": "paper 0616",
   "pn": "",
   "abstract": [
    "The work examines Karhunen-Loeve Transform and Linear Discriminant Analysis as means for designing optimized spectral bases for the projection of the critical-band auditory-like spectrum.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-558"
  },
  "suzuki98c_icslp": {
   "authors": [
    [
     "Shin",
     "Suzuki"
    ],
    [
     "Takesi",
     "Okadome"
    ],
    [
     "Masaaki",
     "Honda"
    ]
   ],
   "title": "Determination of articulatory positions from speech acoustics by applying dynamic articulatory constraints",
   "original": "i98_0130",
   "page_count": 4,
   "order": 561,
   "p1": "paper 0130",
   "pn": "",
   "abstract": [
    "A method for determining articulatory parameters from speech acoustics is presented. The method is based on a search of an articulatory-acoustic codebook which is designed from simultaneous observation data of articulatory motions and speech acoustics. The codebook search employs dynamic constraints on acoustic behavior as well as articulatory behavior. There are two constrains. One of the constraints is use of spectral segments in the codebook search and the other is use of the smoothness of articulatory trajectories in the articulatory parameter path search. The articulatory parameters are determined by selecting the articulatory code vector in the codebook which minimizes the weighted distance measure of segmental spectral distance and squared distance between succeeding articulatory parameters. The results of an experiment show that an rms error between the estimated and observed articulatory parameter was about 2.0 mm on average, and the articulatory features for vowels and consonants are recovered well.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-559"
  },
  "li98b_icslp": {
   "authors": [
    [
     "Yang",
     "Li"
    ],
    [
     "Yunxin",
     "Zhao"
    ]
   ],
   "title": "Recognizing emotions in speech using short-term and long-term features",
   "original": "i98_0379",
   "page_count": 4,
   "order": 562,
   "p1": "paper 0379",
   "pn": "",
   "abstract": [
    "The acoustic characteristics of speech are influenced by speakers' emotional status. In this study, we attempted to recognize the emotional status of individual speakers by using speech features that were extracted from short-time analysis frames as well as speech features that represented entire utterances. Principal component analysis was used to analyze the importance of individual features in representing emotional categories. Three classification methods including vector quantization, artificial neural networks and Gaussian mixture density model were used. Classifications using short-term features only, long-term features only and both short-term and long-term features were conducted. The best recognition performance of 62% accuracy was achieved by using the Gaussian mixture density method with both short-term and long-term features.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-560"
  },
  "robert98_icslp": {
   "authors": [
    [
     "Arnaud",
     "Robert"
    ],
    [
     "Jan",
     "Eriksson"
    ]
   ],
   "title": "Periphear : a nonlinear active model of the auditory periphery",
   "original": "i98_0748",
   "page_count": 4,
   "order": 563,
   "p1": "paper 0748",
   "pn": "",
   "abstract": [
    "This paper describes a phenomenological model of the auditory periphery which consists of a bank of nonlinear time-varying parallel filters. Realistic filter shapes are obtained with the all-pole gammatone filter (APGF) which provides both a good approximation of the far more complex wave-propagation or cochlear mechanics models and a very simple implementation. The model also includes an active, distributed feedback that controls the damping parameter of the APGF. As a result, the model reproduces several observed phenomena including compression and two-tone suppression. It is now used to study responses to complex stimuli in models of the auditory nerve and cochlear nucleus neurons and to provide physiologically plausible front-end for speech analysis.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-561"
  },
  "ramesh98_icslp": {
   "authors": [
    [
     "Padma",
     "Ramesh"
    ],
    [
     "Partha",
     "Niyogi"
    ]
   ],
   "title": "The voicing feature for stop consonants: acoustic phonetic analyses and automatic speech recognition experiments",
   "original": "i98_0881",
   "page_count": 4,
   "order": 564,
   "p1": "paper 0881",
   "pn": "",
   "abstract": [
    "We examine the distinctive feature [voice] that separates the voiced from the unvoiced sounds for the case of stop consonants. We conduct acoustic phonetic analyses on a large database and demonstrate the superior separability using a temporal measure (voice onset time; VOT) rather than spectral measures. We describe several algorithms to automatically estimate the VOT from continuous speech and compare them on a speech recognition problem to reduce error rates by as much as 53 percent over a baseline HMM based system.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-562"
  },
  "basu98_icslp": {
   "authors": [
    [
     "Sankar",
     "Basu"
    ],
    [
     "St√©phane",
     "Maes"
    ]
   ],
   "title": "Wavelet-based energy binning cepstral features for automatic speech recognition",
   "original": "i98_0982",
   "page_count": 4,
   "order": 565,
   "p1": "paper 0982",
   "pn": "",
   "abstract": [
    "Speech production models, coding methods as well as text to speech technology often lead to the introduction of modulation models to represent speech signals with primary components which are amplitude-and-phase-modulated sine functions. Parallelisms between properties of the wavelet transform of primary components and algorithmic representations of speech signals derived from auditory nerve models like the EIH lead to the introduction of synchrosqueezing measures. On the other hand, in automatic speech (and speaker) recognition, cepstral feature have imposed themselves quasi-universally as acoustic characteristic of speech utterances. This paper analyses cepstral representation in the context of the synchrosqueezed representation - wastrum. It discusses energy accumulation derived wastra as opposed to classical MEL and LPC derived cepstra. In the former method the primary components and formants play a primary role. Recognition results are presented on the Wall Street Journal database using IBM continuous decoder.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-563"
  },
  "silva98_icslp": {
   "authors": [
    [
     "Carlos",
     "Silva"
    ],
    [
     "Samir",
     "Chennoukh"
    ]
   ],
   "title": "Articulatory analysis using a codebook for articulatory based low bit-rate speech coding",
   "original": "i98_0899",
   "page_count": 5,
   "order": 566,
   "p1": "paper 0899",
   "pn": "",
   "abstract": [
    "Fundamental to the success of the articulatory based speech coding is the mapping from acoustics to articulatory description. As the mapping is not unique and based on articulatory continuity criteria, the non-uniqueness of the articulatory trajectories is solved using a forward dynamic network. In this paper, we present new results on forward dynamic network used to estimate articulatory trajectories when using an improved articulatory codebook for acoustic-to-articulatory mapping. The improvement on the codebook design is based on a new model that provides more details on the vocal tract area function and on more appropriate articulatory parameter samplings according to the articulatory-acoustics relation.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-564"
  },
  "fang98b_icslp": {
   "authors": [
    [
     "Chen",
     "Fang"
    ],
    [
     "Yuan",
     "Baozong"
    ]
   ],
   "title": "The modeling and realization of natural speech generation system",
   "original": "i98_1034",
   "page_count": 4,
   "order": 567,
   "p1": "paper 1034",
   "pn": "",
   "abstract": [
    "The paper gives an overall discussion on problems in Chinese natural speech generation. A Chinese Bi-directional Grammar is developed to suit for Chinese Language understanding and generation. A comprehensive discription about the structure of characteristic network of all ranks in language have been built up. In Natural language generation, text planning is proceeded at first to extract concrete content related to the semantic. Through text organization the internal generation structure is formed. Grammar realization transforms internal structure to natural language. After the text of natural language generated, the next step is to convert the text into speech. We build up a speech characteristic database with speech of 50 thousand phrases and hundreds of pronunciation rules. After recognizing the structure of the input text and abstracting the rhythm characteristics in text, the database gives completely a description from Chinese characters to speech. The whole Chinese character in GB2312-80 can be described to speech. Based on the research all above, a natural speech generation system is established. It can automatically plan and organize the output sentences in natural speech. The synthetic speech has good quality in naturalness and intelligibility.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-565"
  },
  "eklund98b_icslp": {
   "authors": [
    [
     "Robert",
     "Eklund"
    ]
   ],
   "title": "ko tok ples ensin bilong tok pisin or the TP-CLE: a first report from a pilot speech-to-speech translation project from Swedish to tok pisin",
   "original": "i98_0804",
   "page_count": 4,
   "order": 568,
   "p1": "paper 0804",
   "pn": "",
   "abstract": [
    "This paper describes an operational speech-to-speech translation system from Swedish to Tok Pisin within the framework of the Spoken Language Translator project. The domain of translation is ATIS. The grammar formalism used in the SLT project is the Core Language Engine. A general presentation of Tok Pisin is provided, as well as a description of some grammatical characteristics of Tok Pisin of potential interest for the testing of grammar machines. The first step of a CLE implementation of Tok Pisin is described. A corpus of Tok Pisin ATIS data has been created from data collected on location in New Ireland, Papua New Guinea, and observations are made as to the relative importance of some of the grammatical phenomena discussed in the paper. A Tok Pisin synthesizer based on an already existing Swedish concatenative synthesis is described. Despite a marked Swedish accent, preliminary evaluation indicates that intelligible speech output is produced.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-566"
  },
  "garciavarea98_icslp": {
   "authors": [
    [
     "Ismael",
     "Garc√≠a-Varea"
    ],
    [
     "Francisco",
     "Casacuberta"
    ],
    [
     "Hermann",
     "Ney"
    ]
   ],
   "title": "An iterative, DP-based search algorithm for statistical machine translation",
   "original": "i98_0209",
   "page_count": 4,
   "order": 569,
   "p1": "paper 0209",
   "pn": "",
   "abstract": [
    "The increasing interest in the statistical approach to Machine Translation is due to the development of effective algorithms for training the probabilistic models proposed so far. However, one of the problems with Statistical Machine Translation is the design of efficient algorithms for translating a given input string. For some interesting models, only (good) approximate solutions can be found. Recently a Dynamic- Programming-like algorithm has been introduced which computes approximate solutions for some models. These solutions can be improved by using an iterative algorithm that refines the successive solutions and uses a smoothing technique for some probabilistic distribution of the models based on an interpolation of different distributions. The technique resulting from this combination has been tested on the \"Tourist Task\" corpus, which was generated in a semi-automated way. The best results achieved were a translation word-error rate of 9.3% and a sentence-error rate of 44.4%.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-567"
  },
  "gawronska98_icslp": {
   "authors": [
    [
     "Barbara",
     "Gawronska"
    ],
    [
     "David",
     "House"
    ]
   ],
   "title": "Information extraction and text generation of news reports for a Swedish-English bilingual spoken dialogue system",
   "original": "i98_1047",
   "page_count": 4,
   "order": 570,
   "p1": "paper 1047",
   "pn": "",
   "abstract": [
    "This paper describes an experimental dialog system designed to retrieve information and generate summaries of internet news reports related to user queries in Swedish and English. The extraction component is based on parsing and on matching the parsing output against stereotypic event templates. Bilingual text generation is accomplished by filling the templates after which grammar components generate the final text. The interfaces between the templates and the language-specific text generators are marked for prosodic information resulting in a text output where deaccentuation, accentuation, levels of focal accentuation, and phrasing are specified. These prosodic markers, which are primarily dependent on the giveness/newness structure of the text, modify the default prosody rules of the text-to-speech system which then reads the text with subsequent improvement in intonation.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-568"
  },
  "hulstijn98_icslp": {
   "authors": [
    [
     "Joris",
     "Hulstijn"
    ],
    [
     "Arjan van",
     "Hessen"
    ]
   ],
   "title": "Utterance generation for transaction dialogues",
   "original": "i98_0776",
   "page_count": 4,
   "order": 571,
   "p1": "paper 0776",
   "pn": "",
   "abstract": [
    "This paper discusses the utterance generation module of a spoken dialogue system for transactions. Transactions are interesting because they involve obligations of both parties: the system should provide all relevant information; the user should feel committed to the transaction once it has been concluded. Utterance generation plays a major role in this. The utterance generation module works with prosodically annotated utterance templates. An appropriate template for a given dialogue act is selected by the following parameters: utterance type, body of the template, given information, wanted and new information. Templates respect rules of accenting and deaccenting.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-569"
  },
  "ishikawa98_icslp": {
   "authors": [
    [
     "Kai",
     "Ishikawa"
    ],
    [
     "Eiichiro",
     "Sumita"
    ],
    [
     "Hitoshi",
     "Iida"
    ]
   ],
   "title": "Example-based error recovery method for speech translation: repairing sub-trees according to the semantic distance",
   "original": "i98_0725",
   "page_count": 4,
   "order": 572,
   "p1": "paper 0725",
   "pn": "",
   "abstract": [
    "In speech translation, recognition errors produced by the speech recognition process can cause parsing and translation errors. Because of this, the development of a robust error handling framework is quite essential to improve the performance of the speech translation system. Previously, a robust translation method was proposed by Wakita, which translates only reliable parts in utterances. In this method, however, the recall of translated parts for a whole utterance is low, and sometimes no translation is output. In this paper, we propose an example-based error recovery method to solve the low recall problem of Wakita's method. The proposed method recovers an unreliable utterance, by repairing the parse-tree of the utterance based on similar example parse-trees in the tree- bank. A recovered translation is generated from the recovered tree.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-570"
  },
  "krahmer98b_icslp": {
   "authors": [
    [
     "Emiel",
     "Krahmer"
    ],
    [
     "Mariet",
     "Theune"
    ]
   ],
   "title": "Context sensitive generation of descriptions",
   "original": "i98_0277",
   "page_count": 4,
   "order": 573,
   "p1": "paper 0277",
   "pn": "",
   "abstract": [
    "Probably the best current algorithm for generating definite descriptions is the Incremental Algorithm due to Dale and Reiter. If we want to use this algorithm in a Concept-to-Speech system, however, we encounter two limitations: (1) the algorithm is insensitive to the linguistic context and thus always produces the same description for an object, (2) the output is a list of properties which uniquely determine one object from a set of objects: how this list is to be expressed in spoken natural language is not addressed. We propose a modification of the Incremental Algorithm based on the idea that a definite description refers to the most salient element in the current context satisfying the descriptive content. We show that the modified algorithm allows for the context-sensitive generation of both distinguishing and anaphoric descriptions, while retaining the attractive properties of Dale and Reiter's original algorithm.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-571"
  },
  "levin98b_icslp": {
   "authors": [
    [
     "Lori",
     "Levin"
    ],
    [
     "Donna",
     "Gates"
    ],
    [
     "Alon",
     "Lavie"
    ],
    [
     "Alex",
     "Waibel"
    ]
   ],
   "title": "An interlingua based on domain actions for machine translation of task-oriented dialogues",
   "original": "i98_0999",
   "page_count": 4,
   "order": 574,
   "p1": "paper 0999",
   "pn": "",
   "abstract": [
    "This paper describes an interlingua for spoken language translation that is based on domain actions in the travel planning domain. Domain actions are composed of speech acts (e.g., request-information), attributes (e.g., size, price), and objects (e.g., hotel, flight) and can take arguments. Development of the interlingua is guided by a database containing travel dialogues in English, Korean, Japanese, and Italian. There are currently 423 domain actions that cover hotel reservation and transportation. The interlingua will soon be extended to cover tours, tourist attractions, and events. The interlingua is used by the C-STAR speech translation consortium for translating travel planning dialogues in six languages: English, Japanese, German, Korean, Italian, and French. The paper also addresses the role of the interlingua in Carnegie Mellon's JANUS translation system.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-572"
  },
  "williams98b_icslp": {
   "authors": [
    [
     "Sandra",
     "Williams"
    ]
   ],
   "title": "Generating pitch accents in a concept-to-speech system using a knowledge base",
   "original": "i98_0799",
   "page_count": 4,
   "order": 575,
   "p1": "paper 0799",
   "pn": "",
   "abstract": [
    "This paper describes a concept-to-speech system for generating spoken descriptions of routes between places within Macquarie University Computing Department. The Natural Language Generation (NLG) component of the system generates a textual route description marked with intonational information. The discourse structure of the route description is related closely to the knowledge representation of the route. The NLG component includes a pitch accenting algorithm which places appropriate pitch accents on elements of the utterance requiring particular emphasis or stress. Our pitch accenting algorithm uses a domain knowledge base and a discourse history. From these it determines whether information selected to form the content of the utterance is shared mutual domain knowledge, given information, or new information. It can then assign an appropriate pitch accent to one word in each prosodic phrase. The text-to-speech component then determines the appropriate syllable to be accented in the word.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-573"
  },
  "ruland98_icslp": {
   "authors": [
    [
     "Tobias",
     "Ruland"
    ],
    [
     "C. J.",
     "Rupp"
    ],
    [
     "J√∂rg",
     "Spilker"
    ],
    [
     "Hans",
     "Weber"
    ],
    [
     "Karsten L.",
     "Worm"
    ]
   ],
   "title": "Making the most of multiplicity: a multi-parser multi-strategy architecture for the robust processing of spoken language",
   "original": "i98_0570",
   "page_count": 4,
   "order": 576,
   "p1": "paper 0570",
   "pn": "",
   "abstract": [
    "This paper describes ongoing research on robust spoken language understanding in the context of the Verbmobil speech-to-speech machine translation project. We focus on recent developments in the processing steps which map a word lattice to a semantic representations. The approach described firstly applies speech repair correction to word lattices. Four analysis methods of varying depth are then applied in parallel to the normalized word lattices, producing output for sub-portions of the lattice in the same semantic description language, the VIT format. These fragmentary analyses are stored and combined by a further processing component, which finally selects a sequence of semantic representations as a result.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-574"
  },
  "yi98_icslp": {
   "authors": [
    [
     "Jon R. W.",
     "Yi"
    ],
    [
     "James R.",
     "Glass"
    ]
   ],
   "title": "Natural-sounding speech synthesis using variable-length units",
   "original": "i98_1151",
   "page_count": 4,
   "order": 577,
   "p1": "paper 1151",
   "pn": "",
   "abstract": [
    "The goal of this work was to develop a speech synthesis system which concatenates variable-length units to create natural-sounding speech. Our initial work showed that by careful design of system responses to ensure consistent intonation contours, natural-sounding speech synthesis was achievable with word- and phrase-level concatenation. In order to extend the flexibility of this framework, we focused on generating novel words from a corpus of sub-word units. The design of the corpus was motivated by perceptual experiments that investigated where speech could be spliced with minimal audible distortion and what contextual constraints were necessary to maintain in order to produce natural-sounding speech. From this sub-word corpus, a Viterbi search selects a sequence of units based on how well they match the input specification and concatenation constraints. This concatenative speech synthesis system, ENVOICE, has been used in a conversational system in two application domains to convert meaning representations into speech waveforms.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-575"
  },
  "klabbers98b_icslp": {
   "authors": [
    [
     "Esther",
     "Klabbers"
    ],
    [
     "Emiel",
     "Krahmer"
    ],
    [
     "Mariet",
     "Theune"
    ]
   ],
   "title": "A generic algorithm for generating spoken monologues",
   "original": "i98_0278",
   "page_count": 4,
   "order": 578,
   "p1": "paper 0278",
   "pn": "",
   "abstract": [
    "The defining property of a Concept-to-Speech system is that it combines language and speech generation. Language generation converts the input-concepts into natural language, which speech generation subsequently transforms into speech. Potentially, this leads to a more `natural sounding' output than can be achieved in a plain Text-to-Speech system, since the correct placement of pitch accents and intonational boundaries ---an important factor contributing to the `naturalness' of the generated speech--- is co-determined by syntactic and discourse information, which is typically available in the language generation module. In this paper, a generic algorithm for the generation of coherent spoken monologues is discussed, called D2S. Language generation is done by a module called LGM which is based on TAG-like syntactic structures with open slots, combined with conditions which determine when the syntactic structure can be used properly. A speech generation module converts the output of the LGM into speech using either phrase-concatenation or diphone-synthesis.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-576"
  },
  "hitzeman98_icslp": {
   "authors": [
    [
     "Janet",
     "Hitzeman"
    ],
    [
     "Alan W.",
     "Black"
    ],
    [
     "Paul",
     "Taylor"
    ],
    [
     "Chris",
     "Mellish"
    ],
    [
     "Jon",
     "Oberlander"
    ]
   ],
   "title": "On the use of automatically generated discourse-level information in a concept-to-speech synthesis system",
   "original": "i98_0591",
   "page_count": 6,
   "order": 579,
   "p1": "paper 0591",
   "pn": "",
   "abstract": [
    "This paper describes the latest version of the SOLE concept-to-speech system, which uses linguistic information provided by a natural language generation system to improve the prosody of synthetic speech. We discuss the types of linguistic information that prove most useful and the implications for text-to-speech systems.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-577"
  },
  "alshawi98_icslp": {
   "authors": [
    [
     "Hiyan",
     "Alshawi"
    ],
    [
     "Srinivas",
     "Bangalore"
    ],
    [
     "Shona",
     "Douglas"
    ]
   ],
   "title": "Learning phrase-based head transduction models for translation of spoken utterances",
   "original": "i98_0293",
   "page_count": 4,
   "order": 580,
   "p1": "paper 0293",
   "pn": "",
   "abstract": [
    "We describe a method for learning head-transducer models of translation automatically from examples consisting of transcribed spoken utterances and reference translations of the utterances. The method proceeds by first searching for a hierarchical alignment (specifically a synchronized dependency tree) of each training example. The alignments produced are optimal with respect to a cost function that takes into account co-occurrence statistics and the recursive decomposition of the example into aligned substrings. A probabilistic head-transducer model is then constructed from the alignments. We report results of applying the method to English-to-Spanish translation in the domain of air travel information and English-to-Japanese translation in the domain of telephone operator assistance. We also report on a variation on this model-construction method in which multi-word pairings are used in the computation of the hierarchical alignments and head transducer models.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-578"
  },
  "fukada98b_icslp": {
   "authors": [
    [
     "Toshiaki",
     "Fukada"
    ],
    [
     "Detlef",
     "Koll"
    ],
    [
     "Alex",
     "Waibel"
    ],
    [
     "Kouichi",
     "Tanigaki"
    ]
   ],
   "title": "Probabilistic dialogue act extraction for concept based multilingual translation systems",
   "original": "i98_0657",
   "page_count": 4,
   "order": 581,
   "p1": "paper 0657",
   "pn": "",
   "abstract": [
    "This paper describes a probabilistic method for dialogue act (DA) extraction for concept-based multilingual translation systems. A DA is a unit of a semantic interlingua and it consists of speaker information, speech act, concept and argument. Probabilistic models for the extraction of speech acts or concepts are trained as speech act or concept dependent word n-gram models. The proposed method is evaluated on DA-annotated English and Japanese databases. The experimental results show that the proposed method gives a better performance compared to the conventional grammar-based approach. In addition, the proposed method is much more robust for erroneous inputs obtained as speech recognition outputs.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-579"
  },
  "wang98d_icslp": {
   "authors": [
    [
     "Ye-Yi",
     "Wang"
    ],
    [
     "Alex",
     "Waibel"
    ]
   ],
   "title": "Fast decoding for statistical machine translation",
   "original": "i98_0826",
   "page_count": 4,
   "order": 582,
   "p1": "paper 0826",
   "pn": "",
   "abstract": [
    "We investigated an efficient decoding algorithm for statistical machine translation. Compared to the other algorithms, this new algorithm is applicable to different translation models, and it is much faster. Experiments showed that the algorithm achieved an overall performance comparable to the state of the art decoding algorithms.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-580"
  },
  "takezawa98b_icslp": {
   "authors": [
    [
     "Toshiyuki",
     "Takezawa"
    ],
    [
     "Tsuyoshi",
     "Morimoto"
    ],
    [
     "Yoshinori",
     "Sagisaka"
    ],
    [
     "Nick",
     "Campbell"
    ],
    [
     "Hitoshi",
     "Iida"
    ],
    [
     "Fumiaki",
     "Sugaya"
    ],
    [
     "Akio",
     "Yokoo"
    ],
    [
     "Seiichi",
     "Yamamoto"
    ]
   ],
   "title": "A Japanese-to-English speech translation system: ATR-MATRIX",
   "original": "i98_0957",
   "page_count": 4,
   "order": 583,
   "p1": "paper 0957",
   "pn": "",
   "abstract": [
    "We have built a new speech translation system called ATR-MATRIX (ATR's Multilingual Automatic Translation System for Information Exchange). This system can recognize natural Japanese utterances such as those used in daily life, translate them into English and output synthesized speech. This system is running on a workstation or a high-end PC and achieves nearly real-time processing. The current implementation of our system deals with a hotel room reservation task/domain. We plan to develop a bidirectional speech translation system, i.e., Japanese-to-English and English-to-Japanese. We also plan to develop multi-language output functions from ATR-MATRIX (Japanese-to-English, German and Korean) for the international joint experiment of C-STAR II (Consortium for Speech Translation Advanced Research).\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-581"
  },
  "hirschberg98_icslp": {
   "authors": [
    [
     "Julia",
     "Hirschberg"
    ],
    [
     "Christine H.",
     "Nakatani"
    ]
   ],
   "title": "Acoustic indicators of topic segmentation",
   "original": "i98_0976",
   "page_count": 4,
   "order": 584,
   "p1": "paper 0976",
   "pn": "",
   "abstract": [
    "The segmentation of text and speech into topics and subtopics is an important step in document interpretation. For text, formatting information, such as headings and paragraphing, is available to aid in this endeavor, although this information is by no means sufficient. For speech, the task is even more difficult. We present results of the application of machine learning techniques to the automatic identification of intonational phrases beginning and ending 'topics' determined independently by annotators for two corpora | the Boston Directions Corpus and the Broadcast News (HUB-4) DARPA/NIST database.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-582"
  },
  "grabe98_icslp": {
   "authors": [
    [
     "Esther",
     "Grabe"
    ],
    [
     "Francis",
     "Nolan"
    ],
    [
     "Kimberley J.",
     "Farrar"
    ]
   ],
   "title": "IVie - a comparative transcription system for intonational variation in English",
   "original": "i98_0099",
   "page_count": 4,
   "order": 585,
   "p1": "paper 0099",
   "pn": "",
   "abstract": [
    "In this paper, we offer an alternative to ToBI, the current de facto standard for machine-readable labelling of English prosody. We have three reasons for arguing that an alternative is needed. Firstly, the ToBI tone inventory is not maximally constrained; it appears to be difficult for transcribers to reach high inter-transcriber agreement scores for tone labels. Secondly, the growing demand for prosodically labelled data from non-standard varieties of English suggests a need for a transparent comparative transcription system. ToBI was not designed for this purpose. Thirdly, the low inter-transcriber agreement scores for ToBI suggest that the system is not as easy to apply as it may at first appear. In the present paper, we describe an alternative: the IViE system (Intonational Variation in English). We describe the structure of IViE and discuss its application with examples.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-583"
  },
  "chou98_icslp": {
   "authors": [
    [
     "Fu-Chiang",
     "Chou"
    ],
    [
     "Chiu-Yu",
     "Tseng"
    ],
    [
     "Lin-Shan",
     "Lee"
    ]
   ],
   "title": "Automatic segmental and prosodic labeling of Mandarin speech database",
   "original": "i98_0266",
   "page_count": 4,
   "order": 586,
   "p1": "paper 0266",
   "pn": "",
   "abstract": [
    "In this paper we describe the techniques and methodology developed for automatic labeling of segmental and prosodic information for the Mandarin speech database. There are two major procedures. First, the text is converted into the phonetic network of possible pronunciations, and this network is aligned with the speech data by recognition processes. Secondly, many acoustic prosodic features are derived and the break indices are labeled with these features by decision trees. For the segmental labeling, 96.5% of automatically determined segment boundaries are accurate within a range of 20 ms. For the prosodic labeling, 84.9% of the automatic labeled break indices are the same with the manual labeled one.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-584"
  },
  "rapp98_icslp": {
   "authors": [
    [
     "Stefan",
     "Rapp"
    ]
   ],
   "title": "Automatic labelling of German prosody",
   "original": "i98_0907",
   "page_count": 4,
   "order": 587,
   "p1": "paper 0907",
   "pn": "",
   "abstract": [
    "We present research on an automatic labelling system that is able to produce a phonological tonal labelling according to the ToBI like intonation model for German developed by Fery. The current system was trained on about 1 hour of expert prosodically labelled speech from a single male radio news announcer. We present experiments for finding a suitable feature set drawn from features that describe the prosodic correlates fundamental frequency, duration and intensity as well as some lexical and syntactic features. With the best feature set, we achieve a recognition rate of 78.7% for speaker dependent recognition of ToBI labels (simultaneously predicting prominence and phrasing) and 86.9% for the simpler accented/not accented decision. Although the system's accuracy is well below that of human transcribers, it is a useful tool actively used in our laboratory due to it's ability to process large amounts of speech data at low costs.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-585"
  },
  "karjalainen98_icslp": {
   "authors": [
    [
     "Matti",
     "Karjalainen"
    ],
    [
     "Toomas",
     "Altosaar"
    ],
    [
     "Miikka",
     "Huttunen"
    ]
   ],
   "title": "An efficient labeling tool for the Quicksig speech database",
   "original": "i98_0885",
   "page_count": 4,
   "order": 588,
   "p1": "paper 0885",
   "pn": "",
   "abstract": [
    "An automated speech signal labeling tool, developed for the QuickSig speech database environment, is described. It is based primarily on the use of neural networks as diphone event detectors. For robustness, only coarse categories of diphones, such as stop-vowel and vowel-nasal, are used. 64 such detectors are implemented to cover all of the Finnish diphones. The preprocessing of speech signals is carried out using warped linear prediction and the diphone events from neural network outputs are matched to the given text transcription using a simple rule-based parser. In the case of isolated word labeling of single speaker signals a well trained system makes about 1-2 % of coarse labeling errors and the deviation of boundary positions, compared to careful manual labeling, is on average about 10 ms. Generalization ability to label other speakers shows promising.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-586"
  },
  "bratt98_icslp": {
   "authors": [
    [
     "Harry",
     "Bratt"
    ],
    [
     "Leonardo",
     "Neumeyer"
    ],
    [
     "Elizabeth",
     "Shriberg"
    ],
    [
     "Horacio",
     "Franco"
    ]
   ],
   "title": "Collection and detailed transcription of a speech database for development of language learning technologies",
   "original": "i98_0926",
   "page_count": 4,
   "order": 589,
   "p1": "paper 0926",
   "pn": "",
   "abstract": [
    "We describe the methodologies for collecting and annotating a Latin-American Spanish speech database. The database includes recordings by native and nonnative speakers. The nonnative recordings are annotated with ratings of pronunciation quality and detailed phonetic transcriptions. We use the annotated database to investigate rater reliability, the effect of each phone on overall perceived nonnativeness, and the frequency of specific pronunciation errors.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-587"
  },
  "deshmukh98_icslp": {
   "authors": [
    [
     "Neeraj",
     "Deshmukh"
    ],
    [
     "Aravind",
     "Ganapathiraju"
    ],
    [
     "Andi",
     "Gleeson"
    ],
    [
     "Jonathan",
     "Hamaker"
    ],
    [
     "Joseph",
     "Picone"
    ]
   ],
   "title": "Resegmentation of SWITCHBOARD",
   "original": "i98_0685",
   "page_count": 4,
   "order": 590,
   "p1": "paper 0685",
   "pn": "",
   "abstract": [
    "The SWITCHBOARD (SWB) corpus is one of the most important benchmarks for recognition tasks involving large vocabulary conversational speech (LVCSR). The high error rates on SWB are largely attributable to an acoustic model mismatch, the high frequency of poorly articulated monosyllabic words, and large variations in pronunciations. It is imperative to improve the quality of segmentations and transcriptions of the training data to achieve better acoustic modeling. By adapting existing acoustic models to only a small subset of such improved transcriptions, we have achieved a 2% absolute improvement in performance.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-588"
  },
  "aiello98_icslp": {
   "authors": [
    [
     "Demetrio",
     "Aiello"
    ],
    [
     "Cristina",
     "Delogu"
    ],
    [
     "Renato De",
     "Mori"
    ],
    [
     "Andrea Di",
     "Carlo"
    ],
    [
     "Marina",
     "Nisi"
    ],
    [
     "Silvia",
     "Tummeacciu"
    ]
   ],
   "title": "Automatic generation of visual scenarios for spoken corpora acquisition",
   "original": "i98_0499",
   "page_count": 4,
   "order": 591,
   "p1": "paper 0499",
   "pn": "",
   "abstract": [
    "The paper describes a system, in JAVA, for written and visual scenario generation to collect speech corpora in the framework of a Tourism Information System. Methods and experimental results are also presented for evaluating the degree of understanding of the proposed scenarios. The corpus generated from visual scenarios appears to be much richer than the one generated from textual descriptions.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-589"
  },
  "cettolo98b_icslp": {
   "authors": [
    [
     "Mauro",
     "Cettolo"
    ],
    [
     "Daniele",
     "Falavigna"
    ]
   ],
   "title": "Automatic detection of semantic boundaries based on acoustic and lexical knowledge",
   "original": "i98_0333",
   "page_count": 4,
   "order": 592,
   "p1": "paper 0333",
   "pn": "",
   "abstract": [
    "In spoken dialogue systems, the minimal unit of analysis does not necessarily correspond to a full sentence. A possible approach for language processing is that of splitting the sentence in a sequence of units that can be successively processed by linguistic modules. The goal of the Semantic Boundary (SB) detector is to locate boundaries inside a sentence in order to obtain such minimal units. Useful information for SB detection can be extracted both from the acoustic signal of the utterance and from its corresponding word sequence. In the paper techniques for semantic boundary prediction, based on both acoustic and lexical knowledge, will be presented. Furthermore, a method for combining the two knowledge sources will be proposed. Finally, performance obtained on a corpus of hundreds of person-to-person dialogues will be provided. Best result gives 62.8% recall and 71.8% precision.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-590"
  },
  "gholampour98_icslp": {
   "authors": [
    [
     "Iman",
     "Gholampour"
    ],
    [
     "Kambiz",
     "Nayebi"
    ]
   ],
   "title": "A new fast algorithm for automatic segmentation of continuous speech",
   "original": "i98_0182",
   "page_count": 4,
   "order": 593,
   "p1": "paper 0182",
   "pn": "",
   "abstract": [
    "In this paper a new method for automatic segmentation of continuous speech into phone-like units is addressed. Our method is based on a very fast presegmentation algorithm which uses a new statistical modeling of speech and searching in a multilevel structure, called Dendrogram, for decreasing insertion rate. Performance of algorithms have been tested over a large set of TIMIT sentences. According to these tests, our final segmentation algorithm is capable of detecting nearly 97% of segments with an average boundary position error of less than 7 msec and average insertion rate of less than 12.7%. In addition to acceptable precision, our overall segmentation scheme has very low computation cost and it can be implemented in real time on an average Pentium PC. The major advantage of presented algorithms is that no training or threshold estimation is needed in realizing them. Details of proposed algorithms and their performance results are included in the paper.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-591"
  },
  "iida98_icslp": {
   "authors": [
    [
     "Akemi",
     "Iida"
    ],
    [
     "Nick",
     "Campbell"
    ],
    [
     "Soichiro",
     "Iga"
    ],
    [
     "Fumito",
     "Higuchi"
    ],
    [
     "Michiaki",
     "Yasumura"
    ]
   ],
   "title": "Acoustic nature and perceptual testing of corpora of emotional speech",
   "original": "i98_0818",
   "page_count": 4,
   "order": 594,
   "p1": "paper 0818",
   "pn": "",
   "abstract": [
    "This paper proposes three corpora of emotional speech in Japanese that maximize the expression of each emotion (expressing joy, anger, and sadness) for use with CHATR, the concatenative speech synthesis system being developed at ATR. A perceptual experiment was conducted using the synthesized speech generated from each emotion corpus and the results proved to be significantly identifiable. Authors' current work is to identify the local acoustic features relevant for specifying a particular emotion type. F0 and duration showed significant differences among emotion types. AV (amplitude of voicing source) and GN (glottal noise) also showed differences. This paper reports on the corpus design, the perceptual experiment, and the results of the acoustic analysis.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-592"
  },
  "kang98_icslp": {
   "authors": [
    [
     "Pyungsu",
     "Kang"
    ],
    [
     "Jiyoung",
     "Kang"
    ],
    [
     "Jinyoung",
     "Kim"
    ]
   ],
   "title": "Korean prosodic break index labelling by a new mixed method of LDA and VQ",
   "original": "i98_0264",
   "page_count": 4,
   "order": 595,
   "p1": "paper 0264",
   "pn": "",
   "abstract": [
    "We present a new mixed method of LDA-VQ to predict Korean prosodic break index(PBI) for a given utterance. PBI can be used as an important cue of syntactic discontinuity in continuous speech recognition(CSR). Our proposed method, LDA-VQ model, consists of three steps. At the first step, PBI was predicted with the information of syllable and pause duration through the linear discriminant analysis(LDA) method. At the second step, syllable tone information was used to estimate PBI. In this step we used vector quantization(VQ) for coding the syllable tones and PBI is estimated by tri-tone model. In the last step, two PBI predictors were integrated by a weight factor. The LDA-VQ method was tested on 200 literal style spoken sentences. The experimental results showed 72% accuracy.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-593"
  },
  "laws98_icslp": {
   "authors": [
    [
     "Mark",
     "Laws"
    ],
    [
     "Richard",
     "Kilgour"
    ]
   ],
   "title": "MOOSE: management of otago speech environment",
   "original": "i98_1116",
   "page_count": 4,
   "order": 596,
   "p1": "paper 1116",
   "pn": "",
   "abstract": [
    "With the advent of spoken language computer interface systems, the storage and management of speech corpora is becoming more of an issue in the development of such systems. Until recently, even large corpora were stored as individual text and speech files, or as a single, monolithic file. The issues involved in management and retrieval of the data have been, to a large extent, overlooked. Relational database management systems (RDBMS) are proposed as an ideal tool for the management of speech corpora. Relationships between words and phonemes, and the realisations of these, can be stored and retrieved efficiently. RDBMS may be constructed with various levels, to store speaker, language, label transcription, and phonetic information, plus speech as isolated words, and derived segmented units. An implementation of such a system is presented to manage the Otago Speech Corpus, currently called Management Of Otago Speech Environment (MOOSE). The ability of the MOOSE to be applied to other corpora is currently under investigation.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-594"
  },
  "malfrere98b_icslp": {
   "authors": [
    [
     "Fabrice",
     "Malfr√®re"
    ],
    [
     "Olivier",
     "Deroo"
    ],
    [
     "Thierry",
     "Dutoit"
    ]
   ],
   "title": "Phonetic alignment: speech synthesis based vs. hybrid HMM/ANN",
   "original": "i98_0354",
   "page_count": 4,
   "order": 597,
   "p1": "paper 0354",
   "pn": "",
   "abstract": [
    "In this paper we compare two different methods for phonetically labeling a speech database. The first approach is based on the alignment of the speech signal on a high quality synthetic speech pattern, and the second one uses a hybrid HMM/ANN system. Both systems have been evaluated on French read utterances from a speaker never seen in the training stage of the HMM/ANN system and manually segmented. This study outlines the advantages and drawbacks of both methods. The high quality speech synthetic system has the great advantage that no training stage is needed, while the classical HMM/ANN system easily allows multiple phonetic transcriptions. We deduce a method for the automatic constitution of phonetically labeled speech databases based on using the synthetic speech segmentation tool to bootstrap the training process of our hybrid HMM/ANN system. The importance of such segmentation tools will be a key point for the development of improved speech synthesis and recognition systems.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-595"
  },
  "millar98_icslp": {
   "authors": [
    [
     "J. Bruce",
     "Millar"
    ]
   ],
   "title": "Customisation and quality assessment of spoken language description",
   "original": "i98_0705",
   "page_count": 4,
   "order": 598,
   "p1": "paper 0705",
   "pn": "",
   "abstract": [
    "This paper describes a world-wide web based system which allows a speech data corpus developer to interact with a system for the comprehensive description of a spoken language data corpus. The interface at the browser allows the user to define the speaking environment that is to be described and then progressively to describe it and all the data files arising from it in a modular fashion. The quality of the description can be tested incrementally and the feedback generated used to further update the description. The user is effectively guided through the necessary modules in a way that reminds but does not demand adherence to a strict pattern. The complete data description can be displayed on screen and also downloaded in a simple text form to the user. Users may register as clients of the system in which case they can store descriptions on the system and upgrade these descriptions at any time.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-596"
  },
  "montacie98_icslp": {
   "authors": [
    [
     "Claude",
     "Montaci√©"
    ],
    [
     "Marie-Jos√©",
     "Caraty"
    ]
   ],
   "title": "A silence/noise/music/speech splitting algorithm",
   "original": "i98_1141",
   "page_count": 4,
   "order": 599,
   "p1": "paper 1141",
   "pn": "",
   "abstract": [
    "In this paper, we present techniques to warp audio data of a video movie on its movie script. In order to improve this script warping, a new algorithm has been developed to split audio data into silence, noise, music and speech segments without training step. This segments splitting uses multiple techniques such as voiced/unvoiced segmentation, pitch detection, pitch tracking, speaker and speech recognition techniques. The 102.47 minutes of the film movie \"Contes de Printemps\" produced by E. Rohmer have been indexed with these techniques with an average shifting lower than one second between the time-code script and audio data.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-597"
  },
  "pye98_icslp": {
   "authors": [
    [
     "David",
     "Pye"
    ],
    [
     "Nicholas J.",
     "Hollinghurst"
    ],
    [
     "Timothy J.",
     "Mills"
    ],
    [
     "Kenneth R.",
     "Wood"
    ]
   ],
   "title": "Audio-visual segmentation for content-based retrieval",
   "original": "i98_0517",
   "page_count": 4,
   "order": 600,
   "p1": "paper 0517",
   "pn": "",
   "abstract": [
    "This paper reports recent work at ORL on segmentation of digital audio/video recordings. Firstly, we describe an audio segmentation algorithm that partitions a soundtrack into manageably sized segments for speech recognition. Secondly, we present an algorithm for detecting camera shot-break locations in the video. The output of these two algorithms is combined to produce a semantically meaningful segmentation of audio/video content, appropriate for information retrieval. We report the success of the algorithms in the context of television news retrieval.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-598"
  },
  "rapp98b_icslp": {
   "authors": [
    [
     "Stefan",
     "Rapp"
    ],
    [
     "Grzegorz",
     "Dogil"
    ]
   ],
   "title": "Same news is good news: automatically collecting reoccurring radio news stories",
   "original": "i98_0906",
   "page_count": 4,
   "order": 601,
   "p1": "paper 0906",
   "pn": "",
   "abstract": [
    "We present methods for finding same or almost same news stories in the hourly radio news broadcasts spoken by the same or different announcers. They allow to establish a large database of repeated and professionally read speech at low costs that is especially interesting for prosody research, but also, e.g., for concept-to-speech and socio-linguistic studies. An automatically recorded complete radio news broadcast is first segmented into individual news stories using HMM recognition. Then, the word sequence estimates of the stories are either compared directly (naive method) or realigned with the signal of other stories (realignment method) to find out which stories were read before and which not. Both methods can be further improved by computing ``meta distances'' that also take into account distances to other stories. We find that the realignment method combined with meta distances is the most reliable of the methods on real life data.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-599"
  },
  "brindopke98b_icslp": {
   "authors": [
    [
     "Christel",
     "Brind√∂pke"
    ],
    [
     "Brigitte",
     "Schaffranietz"
    ]
   ],
   "title": "An annotation system for melodic aspects of German spontaneous speech",
   "original": "i98_0352",
   "page_count": 4,
   "order": 602,
   "p1": "paper 0352",
   "pn": "",
   "abstract": [
    "This article presents a phonetically defined annotation system for German speech melody, whose descriptive units cover the perceptive relevant pitch movements. The units for the melodic annotation are based on a model for read German utterances. The implementation of our descriptive melodic units as part of a recently developed labelling and testing environment for melodic aspects of speech allows a comfortable and intersubjective application of the melodic units for the annotation of speech. An experimental evaluation by a rating-experiment secures that the melodic units describe spontaneous speech as adequately as read speech\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-600"
  },
  "stober98_icslp": {
   "authors": [
    [
     "Karlheinz",
     "St√∂ber"
    ],
    [
     "Wolfgang",
     "Hess"
    ]
   ],
   "title": "Additional use of phoneme duration hypotheses in automatic speech segmentation",
   "original": "i98_0239",
   "page_count": 4,
   "order": 603,
   "p1": "paper 0239",
   "pn": "",
   "abstract": [
    "We describe a new approach for speaker independent automatic phoneme alignment. Typical algorithms for this task use only phoneme-to-frame similarity measures which are somehow maximised or minimised. In addition to such similarity measures, we use phoneme duration hypotheses generated by the speech synthesis system HADIFIX. For algorithms based on dynamic programming, it is difficult to use these duration hypotheses, so we create a cost-function consisting of phoneme-to-frame and segment-to-duration hypotheses similarity measures and minimise this cost-function by a Genetic Algorithm. The results show that the accuracy of automatically determined phoneme boundaries increases. This accounts especially for speakers not used in the training phase.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-601"
  },
  "isard98_icslp": {
   "authors": [
    [
     "Amy",
     "Isard"
    ],
    [
     "David",
     "McKelvie"
    ],
    [
     "Henry S.",
     "Thompson"
    ]
   ],
   "title": "Towards a minimal standard for dialogue transcripts: a new SGML architecture for the HCRC map task corpus",
   "original": "i98_0322",
   "page_count": 4,
   "order": 604,
   "p1": "paper 0322",
   "pn": "",
   "abstract": [
    "The rapid growth in availability of high-quality recordings of natural spoken dialogue (and natural spoken material more generally) has encouraged us to to improve the interchange of transcripts of such material, in order that these resources be easy to exploit by the scientific community as a whole. In this paper, we describe a new SGML architecture which we have recently adopted for the HCRC Map Task corpus (a corpus of spontaneous task-oriented dialogues) with precisely these issues in view. This architecture is oriented towards ease of processing and update.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-602"
  },
  "moreno98b_icslp": {
   "authors": [
    [
     "Pedro J.",
     "Moreno"
    ],
    [
     "Chris",
     "Joerg"
    ],
    [
     "Jean-Manuel Van",
     "Thong"
    ],
    [
     "Oren",
     "Glickman"
    ]
   ],
   "title": "A recursive algorithm for the forced alignment of very long audio segments",
   "original": "i98_0068",
   "page_count": 4,
   "order": 605,
   "p1": "paper 0068",
   "pn": "",
   "abstract": [
    "In this paper we address the problem of aligning very long (often more than one hour) audio files to their corresponding textual transcripts in an effective manner. We present an efficient recursive technique to solve this problem that works well even on noisy speech signals. The key idea of this algorithm is to turn the forced alignment problem into a recursive speech recognition problem with a gradually restricting dictionary and language model. The algorithm is tolerant to acoustic noise and errors or gaps in the text transcript or audio tracks. We report experimental results on a 3 hour audio file containing TV and radio broadcasts. We will show accurate alignments on speech under a variety of real acoustic conditions such as speech over music and speech over telephone lines. We also report results when the same audio stream has been corrupted with white additive noise or compressed using a popular web encoding format such as RealAudio. This algorithm has been used in our internal multimedia indexing project. It has processed more than 200 hours of audio from varied sources, such as WGBH NOVA documentaries and NPR web audio files. The system aligns speech media content in about one to five times realtime, depending on the acoustic conditions of the audio signal.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-603"
  },
  "kessens98_icslp": {
   "authors": [
    [
     "Judith M.",
     "Kessens"
    ],
    [
     "Mirjam",
     "Wester"
    ],
    [
     "Catia",
     "Cucchiarini"
    ],
    [
     "Helmer",
     "Strik"
    ]
   ],
   "title": "The selection of pronunciation variants: comparing the performance of man and machine",
   "original": "i98_0372",
   "page_count": 4,
   "order": 606,
   "p1": "paper 0372",
   "pn": "",
   "abstract": [
    "In this paper the performance of an automatic transcription tool is evaluated. The transcription tool is a Continuous Speech Recognizer (CSR) running in forced recognition mode. For evaluation the performance of the CSR was compared to that of nine expert listeners. Both man and the machine carried out exactly the same task: deciding whether a segment was present or not in 467 cases. It turned out that the performance of the CSR is comparable to that of the experts.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-604"
  },
  "barker98_icslp": {
   "authors": [
    [
     "Jon",
     "Barker"
    ],
    [
     "Gethin",
     "Williams"
    ],
    [
     "Steve",
     "Renals"
    ]
   ],
   "title": "Acoustic confidence measures for segmenting broadcast news",
   "original": "i98_0643",
   "page_count": 4,
   "order": 607,
   "p1": "paper 0643",
   "pn": "",
   "abstract": [
    "In this paper we define an acoustic confidence measure based on the estimates of local posterior probabilities produced by a HMM/ANN large vocabulary continuous speech recognition system. We use this measure to segment continuous audio into regions where it is and is not appropriate to expend recognition effort. The segmentation is computationally inexpensive and provides reductions in both overall word error rate and decoding time. The technique is evaluated using material from the Broadcast News corpus.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-605"
  },
  "pellom98_icslp": {
   "authors": [
    [
     "Bryan L.",
     "Pellom"
    ],
    [
     "John H. L.",
     "Hansen"
    ]
   ],
   "title": "A duration-based confidence measure for automatic segmentation of noise corrupted speech",
   "original": "i98_0853",
   "page_count": 4,
   "order": 608,
   "p1": "paper 0853",
   "pn": "",
   "abstract": [
    "In this study, a duration-based measure is formulated for assigning confidence scores to phonetic time-alignments produced by an automatic speech segmentation system. For speech corrupted by additive noise or telephone channel environments, the proposed confidence measure is shown to provide a reliable means by which gross segmentation errors can be automatically detected and marked for human hand correction. The measure is evaluated by computing Receiver Operating Characteristic (ROC) curves to illustrate the expected trade-off in probability of detecting gross segmentation errors versus false alarm rates.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-606"
  },
  "hain98_icslp": {
   "authors": [
    [
     "Thomas",
     "Hain"
    ],
    [
     "Philip C.",
     "Woodland"
    ]
   ],
   "title": "Segmentation and classification of broadcast news audio",
   "original": "i98_0851",
   "page_count": 4,
   "order": 609,
   "p1": "paper 0851",
   "pn": "",
   "abstract": [
    "Broadcast news audio data contains a wide variety of different speakers and audio conditions (channel and background noise). This paper describes a segmentation, gender detection and audio classification scheme for such data which aims to provide a speech recogniser with a stream of reasonably-sized segments, each from a single speaker and audio type while discarding non-speech data. Each segment is labelled as either narrow or wide band and from either a female or male speaker. The segmentation system has been evaluated on the DARPA 1997 broadcast news data set and detailed segmentation accuracy results are presented. It is shown that the speech recognition accuracy for these automatically derived segments is very nearly the same as that for manually segmented data.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-607"
  },
  "lindberg98_icslp": {
   "authors": [
    [
     "B√∏rge",
     "Lindberg"
    ],
    [
     "Robrecht",
     "Comeyne"
    ],
    [
     "Christoph",
     "Draxler"
    ],
    [
     "Francesco",
     "Senia"
    ]
   ],
   "title": "Speaker recruitment methods and speaker coverage - experiences from a large multilingual speech database collection",
   "original": "i98_1126",
   "page_count": 4,
   "order": 610,
   "p1": "paper 1126",
   "pn": "",
   "abstract": [
    "With the globalisation and evolving technology of voice-driven man-machine interfaces there is a growing demand for acquisition of spoken language resources in a number of speaker populations being representative for a number of languages and countries. In this paper experience from work within a large consortium in creating large multilingual speech databases for tele-services are reported. In particular the methods and experiences in recruiting speakers for such recordings are reported across a number of participating partners. The reporting is from the SpeechDat project (http://speechdat.phonetik.uni-muenchen.de).\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-608"
  },
  "campione98b_icslp": {
   "authors": [
    [
     "Estelle",
     "Campione"
    ],
    [
     "Jean",
     "V√©ronis"
    ]
   ],
   "title": "A multilingual prosodic database",
   "original": "i98_0844",
   "page_count": 4,
   "order": 611,
   "p1": "paper 0844",
   "pn": "",
   "abstract": [
    "We present a prosodic corpus in five languages (French, English, Italian, German and Spanish) comprising 4 hours and 20 minutes of speech and involving 50 different speakers (5 male and 5 female per language). The recordings on which the corpus is based are extracted from the EUROM 1 speech database and consists of passages of about five sentences. The corpus was stylized automatically by an algorithm which factors out microprosodic effects and represents the intonation contour of utterances by a series of target points. Once interpolated by a smooth curve (spline), these points produce a contour undistinguishable from the original when re-synthesized, apart from a few detection errors. A symbolic coding of the 50000 pitch movements of the corpus is also provided, along with the time-alignment of orthographic transcription to signal at word-level. The entire corpus was verified and manually corrected by experts for each language. It will be made available at production cost for research through the European Language Resource Association (ELRA).\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-609"
  },
  "cole98_icslp": {
   "authors": [
    [
     "Ronald A.",
     "Cole"
    ],
    [
     "Mike",
     "Noel"
    ],
    [
     "Victoria",
     "Noel"
    ]
   ],
   "title": "The CSLU speaker recognition corpus",
   "original": "i98_0856",
   "page_count": 4,
   "order": 612,
   "p1": "paper 0856",
   "pn": "",
   "abstract": [
    "This paper describes the CSLU Speaker Recognition Corpus data collection. The corpus was motivated by a need for speech data from many speakers, under different environmental conditions, with each speaker providing data over a significant period of time. The corpus was designed to provide sufficient data to study phonetic variability within and across sessions, and to design and evaluate systems for both vocabulary independent and vocabulary specific recognition and verification tasks. The protocol includes fixed vocabulary phrases, digit strings, personal utterances (e.g., eye color), and fluent speech. The resulting Speaker Recognition Corpus is a collection of telephone speech recordings from over 500 participants collected over a two-year period. We describe the data collection procedure, the protocol, the transcription methods and the current status of the Speaker Recognition Corpus.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-610"
  },
  "aist98b_icslp": {
   "authors": [
    [
     "Gregory",
     "Aist"
    ],
    [
     "Peggy",
     "Chan"
    ],
    [
     "Xuedong",
     "Huang"
    ],
    [
     "Li",
     "Jiang"
    ],
    [
     "Rebecca",
     "Kennedy"
    ],
    [
     "DeWitt",
     "Latimer"
    ],
    [
     "Jack",
     "Mostow"
    ],
    [
     "Calvin",
     "Yeung"
    ]
   ],
   "title": "How effective is unsupervised data collection for children's speech recognition?",
   "original": "i98_0929",
   "page_count": 4,
   "order": 613,
   "p1": "paper 0929",
   "pn": "",
   "abstract": [
    "Children present a unique challenge to automatic speech recognition. Today's state-of-the-art speech recognition systems still have problems handling children's speech because acoustic models are trained on data collected from adult speech. In this paper we describe an inexpensive way to mend this problem. We collected children's speech when they interact with an automated reading tutor. These data are subsequently transcribed by a speech recognition system and automatically filtered. We studied how to use these automatically collected data to improve children's speech recognition system's performance. Experiments indicate that automatically collected data can reduce the error rate significantly on children's speech.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-611"
  },
  "shyuu98_icslp": {
   "authors": [
    [
     "Jyh-Shing",
     "Shyuu"
    ],
    [
     "Wang",
     "Jhing-Fa"
    ]
   ],
   "title": "An algorithm for automatic generation of Mandarin phonetic balanced corpus",
   "original": "i98_0960",
   "page_count": 4,
   "order": 614,
   "p1": "paper 0960",
   "pn": "",
   "abstract": [
    "This paper proposed an algorithm for automatic generation of Mandarin phonetic balanced corpus. The design of phonetic balanced corpus is particularly important for the collection of continuous speech database to reduce the co-articulate effects in continuous speech recognition(CSR). Traditionally, balanced corpus is generated manually or semi- automatically. Our proposed algorithm tries to find a minimum number of sentences from a large text corpus set and ensures that 408 Mandarin base syllables and 38*22 co-articulations between vowels and consonants are distributed in the extracted sentences.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-612"
  },
  "bird98_icslp": {
   "authors": [
    [
     "Steven",
     "Bird"
    ],
    [
     "Mark",
     "Liberman"
    ]
   ],
   "title": "Towards a formal framework for linguistic annotations",
   "original": "i98_0774",
   "page_count": 2,
   "order": 615,
   "p1": "paper 0774",
   "pn": "",
   "abstract": [
    "`Linguistic annotation' is a term covering any transcription, translation or annotation of textual data or recorded linguistic signals. While there are several ongoing efforts to provide formats and tools for such annotations and to publish annotated linguistic databases, the lack of widely accepted standards is becoming a critical problem. Proposed standards, to the extent they exist, have focussed on file formats. This paper focuses instead on the logical structure of linguistic annotations. We survey a wide variety of annotation formats and demonstrate a common conceptual core. This provides the foundation for an algebraic framework which encompasses the representation, archiving and query of linguistic annotations, while remaining consistent with many alternative file formats.\n",
    ""
   ]
  },
  "altosaar98_icslp": {
   "authors": [
    [
     "Toomas",
     "Altosaar"
    ],
    [
     "Martti",
     "Vainio"
    ]
   ],
   "title": "Forming generic models of speech for uniform database access",
   "original": "i98_0887",
   "page_count": 4,
   "order": 616,
   "p1": "paper 0887",
   "pn": "",
   "abstract": [
    "This paper presents a formalism that models speech from different databases generically. For each utterance in a speech database a communication framework is first constructed which is composed of a set of communication planes, such as acoustic, orthographic, linguistic, and phonetic. Each plane in turn is made up of a set of levels to represent the plane's structural hierarchy, e.g., for the linguistic plane, levels such as sentence, word, syllable, and phoneme may exist. Information from speech databases is parsed and compiled into such objects and exhibit both individual and class inherited behaviour. Once placed into the framework these objects can have their relationships to other objects explicitly defined by links on the same level, across different levels, and across different planes. Speech from databases covering different languages and annotation styles can therefore be modelled generically allowing for uniform database access. Searches can be performed on the framework and the results used for further analyses.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-613"
  },
  "cook98_icslp": {
   "authors": [
    [
     "Gary",
     "Cook"
    ],
    [
     "Tony",
     "Robinson"
    ],
    [
     "James",
     "Christie"
    ]
   ],
   "title": "Real-time recognition of broadcast news",
   "original": "i98_0065",
   "page_count": 4,
   "order": 617,
   "p1": "paper 0065",
   "pn": "",
   "abstract": [
    "Although the performance of state-of-the-art automatic speech recognition systems on the challenging task of broadcast news transcription has improved considerably in recent years, many of the systems operate in 130-300 times real-time. Many applications of automatic transcription of broadcast news, eg. closed-caption subtitles for television broadcasts, require real-time operation. This paper describes a connectionist-HMM system for broadcast news transcription, and the modifications to this system necessary for real-time operation. We show that real-time operation is possible with a relative increase in word error rate of about 12%.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-614"
  },
  "yu98b_icslp": {
   "authors": [
    [
     "Ha-Jin",
     "Yu"
    ],
    [
     "Hoon",
     "Kim"
    ],
    [
     "Jae-Seung",
     "Choi"
    ],
    [
     "Joon-Mo",
     "Hong"
    ],
    [
     "Kew-Suh",
     "Park"
    ],
    [
     "Jong-Seok",
     "Lee"
    ],
    [
     "Hee-Youn",
     "Lee"
    ]
   ],
   "title": "Automatic recognition of Korean broadcast news speech",
   "original": "i98_0412",
   "page_count": 4,
   "order": 618,
   "p1": "paper 0412",
   "pn": "",
   "abstract": [
    "This paper describes preliminary results of automatic recognition of Korean broadcast-news speech. We have been working on flexible vocabulary isolated-word speech recognition, and the same HMM models are used for broadcast-news continuous speech recognition. The recognizer is trained by using phonetically balanced isolated words speech, rather than the broadcast news speech itself. In this research, we use several different lexica to investigate the recognition performance according to the length of the words. We also propose a long-distance bigram language model, which can be used at the first stage of the search, so that it can reduce the recognition errors caused by earlier pruning of correct hypothesis.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-615"
  },
  "glass98_icslp": {
   "authors": [
    [
     "James R.",
     "Glass"
    ],
    [
     "Timothy J.",
     "Hazen"
    ]
   ],
   "title": "Telephone-based conversational speech recognition in the JUPITER domain",
   "original": "i98_0593",
   "page_count": 4,
   "order": 619,
   "p1": "paper 0593",
   "pn": "",
   "abstract": [
    "This paper describes our experiences with developing a telephone-based speech recognizer as part of a conversational system in the weather information domain. This system has been used to collect spontaneous speech data which has proven to be extremely valuable for research in a number of different areas. After describing the corpus we have collected, we describe the development of the recognizer vocabulary, pronunciations, language and acoustic models for this system, and report on the current performance of the recognizer under several different conditions.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-616"
  },
  "hon98_icslp": {
   "authors": [
    [
     "Hsiao-Wuen",
     "Hon"
    ],
    [
     "Yun-Cheng",
     "Ju"
    ],
    [
     "Keiko",
     "Otani"
    ]
   ],
   "title": "Japanese large-vocabulary continuous speech recognition system based on microsoft whisper",
   "original": "i98_0597",
   "page_count": 4,
   "order": 620,
   "p1": "paper 0597",
   "pn": "",
   "abstract": [
    "Input of Asian ideographic characters has traditionally been one of the biggest impediments for information processing in Asia. Speech is arguably the most effective and efficient input method for Asian non-spelling characters. This paper presents a Japanese large-vocabulary continuous speech recognition system based on Microsoft Whisper technology. We focus on the aspects of the system that are language specific and demonstrate the adaptability of the Whisper system to new languages. In this paper, we demonstrate that our pronunciation/part-of-speech distinguished morpheme based language models and Whisper based Japanese senonic acoustic models are able to yield state-of-the-art Japanese LVCSR recognition performance. The speaker-independent character and Kana error rates on the JNAS database are 10% and 5% respectively.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-617"
  },
  "gauvain98_icslp": {
   "authors": [
    [
     "Jean-Luc",
     "Gauvain"
    ],
    [
     "Lori F.",
     "Lamel"
    ],
    [
     "Gilles",
     "Adda"
    ]
   ],
   "title": "Partitioning and transcription of broadcast news data",
   "original": "i98_0084",
   "page_count": 4,
   "order": 621,
   "p1": "paper 0084",
   "pn": "",
   "abstract": [
    "Radio and television broadcasts consist of a continuous stream of data comprised of segments of different linguistic and acoustic natures, which poses challenges for transcription. In this paper we report on our recent work in transcribing broadcast news data, including the problem of partitioning the data into homogeneous segments prior to word recognition. Gaussian mixture models are used to identify speech and non-speech segments. A maximum-likelihood segmentation/clustering process is then applied to the speech segments using GMMs and an agglomerative clustering algorithm. The clustered segments are then labeled according to bandwidth and gender. The recognizer is a continuous mixture density, tied-state cross-word context-dependent HMM system with a 65k trigram language model. Decoding is carried out in three passes, with a final pass incorporating cluster-based test-set MLLR adaptation. The overall word transcription error on the Nov'97 unpartitioned evaluation test data was 18.5%.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-618"
  },
  "tsukada98_icslp": {
   "authors": [
    [
     "Hajime",
     "Tsukada"
    ],
    [
     "Hirofumi",
     "Yamamoto"
    ],
    [
     "Toshiyuki",
     "Takezawa"
    ],
    [
     "Yoshinori",
     "Sagisaka"
    ]
   ],
   "title": "Grammatical word graph re-generation for spontaneous speech recognition",
   "original": "i98_0485",
   "page_count": 4,
   "order": 622,
   "p1": "paper 0485",
   "pn": "",
   "abstract": [
    "We propose a novel recognition method for generating an accurate grammatical word-graph allowing grammatical deviations. Our method uses both an n-gram and a grammar-based statistical language model and aligns utterances with the grammar by adding deviation information during the search process. Our experiments confirm that the word-graph obtained by our proposed method is superior to the one obtained by only using the n-gram with the same word-graph density. In addition, our recognition method can search enormous hypotheses more efficiently than the conventional word-graph based search method.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-619"
  },
  "yodo98_icslp": {
   "authors": [
    [
     "Norimichi",
     "Yodo"
    ],
    [
     "Kiyohiro",
     "Shikano"
    ],
    [
     "Satoshi",
     "Nakamura"
    ]
   ],
   "title": "Compression algorithm of trigram language models based on maximum likelihood estimation",
   "original": "i98_0716",
   "page_count": 4,
   "order": 623,
   "p1": "paper 0716",
   "pn": "",
   "abstract": [
    "In this paper we propose an algorithm for reducing the size of back-off N-gram models, with less affecting its performance than the traditional cutoff method. The algorithm is based on the Maximum Likelihood (ML) estimation and realizes an N-gram language model with a given number of N-gram probability parameters that minimize the training set perplexity. To confirm the effectiveness of our algorithm, we apply it to trigram and bigram models, and the experiments in terms of perplexity and word error rate in a dictation system are carried out.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-620"
  },
  "uebler98_icslp": {
   "authors": [
    [
     "Ulla",
     "Uebler"
    ],
    [
     "Heinrich",
     "Niemann"
    ]
   ],
   "title": "Morphological modeling of word classes for language models",
   "original": "i98_0338",
   "page_count": 4,
   "order": 624,
   "p1": "paper 0338",
   "pn": "",
   "abstract": [
    "It is well known that good language models improve performance of speech recognition. One requirement for the estimation of language models is a sufficient amount of texts of the application domain. If not all words of the domain occur in the training texts for language models, a way must be found to model these words adequately. In this paper we report on a new approach of building word classes for language modeling in the bilingual (German, Italian) SpeeData project. The main idea is to classify words according to their morphological properties. Therefore we decompose words into their morphological units and put the words with the same prefix or suffix into the same class. Since morphological decomposition is error prone for unknown word stems, we also decomposed words by counting beginnings and endings of different length and used these subunits like prefixes and suffixes. The advantage of this approach is that it can be carried out automatically. We achieved a reduction in error rate from 9.83 % to 5.77 % for morphological decomposition and 5.99 % for automatical decomposition which can be performed without any morphological knowledge.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-621"
  },
  "zitouni98_icslp": {
   "authors": [
    [
     "Imed",
     "Zitouni"
    ],
    [
     "Kamel",
     "Smaili"
    ],
    [
     "Jean-Paul",
     "Haton"
    ],
    [
     "Sabine",
     "Deligne"
    ],
    [
     "Fr√©d√©ric",
     "Bimbot"
    ]
   ],
   "title": "A comparative study between polyclass and multiclass language models",
   "original": "i98_0498",
   "page_count": 4,
   "order": 625,
   "p1": "paper 0498",
   "pn": "",
   "abstract": [
    "In this paper, we introduce the concept of Multiclass for language modeling and we compare it to the Polyclass model. The originality of the Multiclass is its capability to parse a string of class/tags into variable length independent sequences. A few experimental tests were carried out on a class corpus extracted from the French \"Le Monde\" word corpus labeled automatically. This corpus contains a set of 43 million of words. In our experiments, Multiclass outperform first-order Polyclass but are slightly outperformed by second-order Polyclass.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-622"
  },
  "klakow98_icslp": {
   "authors": [
    [
     "Dietrich",
     "Klakow"
    ]
   ],
   "title": "Log-linear interpolation of language models",
   "original": "i98_0522",
   "page_count": 4,
   "order": 626,
   "p1": "paper 0522",
   "pn": "",
   "abstract": [
    "Combining different language models is an important task. Linear interpolation is the established method to do this. We will present a new method called log-linear interpolation (LLI) which combines the simplicity of linear interpolation with essential parts from maximum-entropy models by linearly interpolating the scores of different models. The first series of experiments focuses on adaptation. Unigram, bigram and trigram models trained on NAB are combined with unigram and bigram models trained on a small domain specific corpus. LLI compares favorably with linear interpolation. The second series combines bigram and distance-bigram models. Here, relative improvements are larger (~20% in perplexity). This task seems to be the ideal application of LLI. To further scrutinize the method, first frequent pairs of words are joined to phrases and next, bigram and distance-bigram are combined by LLI. This experiment yields perplexities just 2.5% above the original trigram perplexity.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-623"
  },
  "clarkson98_icslp": {
   "authors": [
    [
     "Philip",
     "Clarkson"
    ],
    [
     "Tony",
     "Robinson"
    ]
   ],
   "title": "The applicability of adaptive language modelling for the broadcast news task",
   "original": "i98_0962",
   "page_count": 4,
   "order": 627,
   "p1": "paper 0962",
   "pn": "",
   "abstract": [
    "Adaptive language models have consistently been shown to lead to a significant reduction in language model perplexity compared to the equivalent static trigram model on many data sets. When these language models have been applied to speech recognition, however, they have seldom resulted in a corresponding reduction in word error rate. This paper will investigate some of the possible reasons for this apparent discrepancy, and will explore the circumstances under which adaptive language models can be useful. We will concentrate on cache-based and mixture-based models and their use on the Broadcast News task.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-624"
  },
  "nguyen98_icslp": {
   "authors": [
    [
     "Long",
     "Nguyen"
    ],
    [
     "Richard",
     "Schwartz"
    ]
   ],
   "title": "The BBN single-phonetic-tree fast-match algorithm",
   "original": "i98_1028",
   "page_count": 4,
   "order": 628,
   "p1": "paper 1028",
   "pn": "",
   "abstract": [
    "In January 1993, BBN demonstrated for the first time a real-time, 20K-word, speaker-independent, continuous speech recognition system, implemented in software on an off-the-shelf workstation. The key to the real-time system was a novel, proprietary fast-match algorithm which had two important properties: high-accuracy recognition and run-time proportional to only the cube root of the vocabulary size. This paper describes that fast-match algorithm in detail. While a number of fast-match algorithms have been published, the BBN algorithm continues to have novel features that have not appeared in the literature. In this fast-match, the vocabulary is organized as a phonetic tree. The last phonemes of the words always locate at the leaves. Each node of the phonetic tree is assigned a set id representing a group of words which share this node. The acoustic models associated with the nodes are the composite triphones where there could be more than one right context. The language models used in this fast-match are the Pr(set_of_words|some_word) which are evaluated at every node of the words during the search. The search itself is similar to the usual beam search with one addition: To activate a node, we temporarily use some set bigrams; to leave that node, we take out that temporary bigram.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-625"
  },
  "lee98h_icslp": {
   "authors": [
    [
     "Akinobu",
     "Lee"
    ],
    [
     "Tatsuya",
     "Kawahara"
    ],
    [
     "Shuji",
     "Doshita"
    ]
   ],
   "title": "An efficient two-pass search algorithm using word trellis index",
   "original": "i98_0655",
   "page_count": 4,
   "order": 629,
   "p1": "paper 0655",
   "pn": "",
   "abstract": [
    "We propose an efficient two-pass search algorithm for LVCSR. Instead of conventional word graph, the first preliminary pass generates \"word trellis index\", keeping track of all survived word hypotheses within the beam every time-frame. As it represents all found word boundaries non-deterministically, we can (1) obtain accurate sentence-dependent hypotheses on the second search, and (2) avoid expensive word-pair approximation on the first pass. The second pass performs an efficient stack decoding search, where the index is referred to as predicted word list and heuristics. Experimental results on 5,000-word Japanese dictation task show that, compared with the word-graph method, this trellis-based method runs with less than 1/10 memory cost while keeping high accuracy. Finally, by handling inter-word context dependency, we achieved the word error rate of 5.6%.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-626"
  },
  "schuster98_icslp": {
   "authors": [
    [
     "Mike",
     "Schuster"
    ]
   ],
   "title": "Nozomi -- a fast, memory-efficient stack decoder for LVCSR",
   "original": "i98_0464",
   "page_count": 4,
   "order": 630,
   "p1": "paper 0464",
   "pn": "",
   "abstract": [
    "This paper describes some of the implementation details of the ``Nozomi'' stack decoder for LVCSR. The decoder was tested on a Japanese Newspaper Dictation Task using a 5000 word vocabulary. Using continuous density acoustic models with 2000 and 3000 states trained on the JNAS/ASJ corpora and a 3-gram LM trained on the RWC text corpus, both models provided by the IPA group, it was possible to reach more than 95% word accuracy on the standard test set. With computationally cheap acoustic models we could achieve around 89% accuracy in nearly realtime on a 300 Mhz Pentium II. Using a disk-based LM the memory usage could be optimized to 4 MB in total.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-627"
  },
  "kemp98_icslp": {
   "authors": [
    [
     "Thomas",
     "Kemp"
    ],
    [
     "Alex",
     "Waibel"
    ]
   ],
   "title": "Reducing the OOV rate in broadcast news speech recognition",
   "original": "i98_0757",
   "page_count": 4,
   "order": 631,
   "p1": "paper 0757",
   "pn": "",
   "abstract": [
    "To achieve the long-term goal of robust, real-time broadcast news transcription, several problems have to be overcome, e.g. the variety of acoustic conditions and the unlimited vocabulary. In this paper we address the problem of unlimited vocabulary. We show, that this problem is more serious for German than it is for English. Using a speech recognition system with a large vocabulary, we dynamically adapt the active vocabulary to the topic of the current news segment. This is done by using information retrieval (IR) techniques on a large collection of texts automatically gathered from the internet. The same technique is also used to adapt the language model of the recognition system. The process of vocabulary adaptation and language model retraining is completely unsupervised. We show, that dynamic vocabulary adaptation can significantly reduce the out-of-vocabulary (OOV) rate and the word error rate of our broadcast news transcription system View4You.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-628"
  },
  "bacchiani98_icslp": {
   "authors": [
    [
     "Michiel",
     "Bacchiani"
    ],
    [
     "Mari",
     "Ostendorf"
    ]
   ],
   "title": "Using automatically-derived acoustic sub-word units in large vocabulary speech recognition",
   "original": "i98_0586",
   "page_count": 4,
   "order": 632,
   "p1": "paper 0586",
   "pn": "",
   "abstract": [
    "Although most parameters in a speech recognition system are estimated from data, the unit inventory and lexicon are generally hand crafted and therefore unlikely to be optimal. This paper describes a joint solution to the problems of learning a unit inventory and corresponding lexicon from data. The methodology, which requires multiple training tokens per word, is then extended to handle infrequently observed words using a hybrid system that combines automatically-derived units with phone-based units. The hybrid system outperforms a phone-based system in first-pass decoding experiments on a large vocabulary conversational speech recognition task.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-629"
  },
  "mcallaster98_icslp": {
   "authors": [
    [
     "Don",
     "McAllaster"
    ],
    [
     "Lawrence",
     "Gillick"
    ],
    [
     "Francesco",
     "Scattone"
    ],
    [
     "Michael",
     "Newman"
    ]
   ],
   "title": "Fabricating conversational speech data with acoustic models: a program to examine model-data mismatch",
   "original": "i98_0986",
   "page_count": 4,
   "order": 633,
   "p1": "paper 0986",
   "pn": "",
   "abstract": [
    "We present a study of data simulated using acoustic models trained on Switchboard data, and then recognized using various Switchboard-trained models. Simple development models give a word error rate (WER) of about 47%, when recognizing real Switchboard conversations. If we simulate speech from word transcriptions, obtaining the word pronunciations from our recognition dictionary, the WER drops by a factor of five to ten. If we use more realistic hand-labeled phonetic transcripts to fabricate data, we obtain WERs in the low 40's, close to those found in actual speech data. These and other experiments we describe in the paper suggest that there is a substantial mismatch between real speech and the combination of our acoustic models and the pronunciations in our recognition dictionary. The use of simulation in speech recognition research appears to be a promising tool in our efforts to understand and reduce the size of this mismatch.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-630"
  },
  "chou98b_icslp": {
   "authors": [
    [
     "Wu",
     "Chou"
    ],
    [
     "Wolfgang",
     "Reichl"
    ]
   ],
   "title": "High resolution decision tree based acoustic modeling beyond CART",
   "original": "i98_0607",
   "page_count": 4,
   "order": 634,
   "p1": "paper 0607",
   "pn": "",
   "abstract": [
    "In this paper, an m-level optimal subtree based phonetic decision tree clustering algorithm is described. Unlike prior approaches, the m-level optimal subtree in the proposed approach is to generate log likelihood estimates using multiple mixture Gaussians for phonetic decision tree based state tying. It provides a more accurate model of the log likelihood variations in node splitting and it is consistent with the acoustic space partition introduced by the set of phonetic questions applied during the decision tree state tying process. In order to reduce the algorithmic complexity, a caching scheme based on previous search results is also described. It leads to a significant speed up of the m-level optimal subtree construction without degradation of the recognition performance, making the proposed approach suitable for large vocabulary speech recognition tasks. Experimental results on a standard (Wall Street Journal) speech recognition task indicate that the proposed m-level optimal subtree approach outperforms the conventional approach of using single mixture Gaussians in phonetic decision tree based state tying.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-631"
  },
  "kemp98b_icslp": {
   "authors": [
    [
     "Thomas",
     "Kemp"
    ],
    [
     "Alex",
     "Waibel"
    ]
   ],
   "title": "Unsupervised training of a speech recognizer using TV broadcasts",
   "original": "i98_0758",
   "page_count": 4,
   "order": 635,
   "p1": "paper 0758",
   "pn": "",
   "abstract": [
    "Current speech recognition systems require large amounts of expensive transcribed data for parameter estimation. In this work we describe our experiments which are aimed at training a speech recognizer without transcriptions. The experiments were carried out with untranscribed TV newscast recordings. The newscasts were automatically segmented into segments of similar acoustic background condition. We develop a training scheme, where a recognizer is bootstrapped using very little transcribed data and is improved using new, untranscribed speech. We show that it is necessary to use a confidence measure to judge the initial transcriptions of the recognizer before using them. Higher improvements can be achieved if the number of parameters in the system is increased when more data becomes available. We show, that the beneficial effect of unsupervised training is not compensated by MLLR adaptation on the hypothesis. Using the described methods, we found that the untranscribed data gives roughly one third of the improvement of the transcribed material.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-632"
  },
  "lee98i_icslp": {
   "authors": [
    [
     "Clark Z.",
     "Lee"
    ],
    [
     "Douglas",
     "O'Shaughnessy"
    ]
   ],
   "title": "A new method to achieve fast acoustic matching for speech recognition",
   "original": "i98_0208",
   "page_count": 4,
   "order": 636,
   "p1": "paper 0208",
   "pn": "",
   "abstract": [
    "For large vocabulary continuous speech recognition based on hidden Markov models, we often face the issue of trade-off between the accuracy and the speed. A new method is proposed in this article such that complex models are used to retain a high accuracy whereas the speed is achieved by using the similarities in acoustic matches. These similarities are based on the assumption that we refer as a look-phone-context property. By using the look-phone-context property, the number of acoustic matches can be substantially reduced in the course of scoring all possible phonetic transcriptions of recognition hypotheses. Experiments on the speaker-independent Wall Street Journal task show that a fast-response system can be reached without compromising the accuracy.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-633"
  },
  "duchateau98_icslp": {
   "authors": [
    [
     "Jacques",
     "Duchateau"
    ],
    [
     "Kris",
     "Demuynck"
    ],
    [
     "Dirk Van",
     "Compernolle"
    ],
    [
     "Patrick",
     "Wambacq"
    ]
   ],
   "title": "Improved parameter tying for efficient acoustic model evaluation in large vocabulary continuous speech recognition",
   "original": "i98_0161",
   "page_count": 4,
   "order": 637,
   "p1": "paper 0161",
   "pn": "",
   "abstract": [
    "In an HMM based large vocabulary continuous speech recognition system, the evaluation of - context dependent - acoustic models is very time consuming. In Semi-Continuous HMMs, a state is modelled as a mixture of elementary - generally gaussian - probability density functions. Observation probability calculations of these states can be made faster by reducing the size of the mixture of gaussians used to model them. In this paper, we propose different criteria to decide which gaussians should remain in the mixture for a state, and which ones can be removed. The performance of the criteria is compared on context dependent tied state models using the WSJ recognition task. Our novel criterion, which decides to remove a gaussian in a state if it is based on too few acoustic data, outperforms the other described criteria.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-634"
  },
  "sankar98_icslp": {
   "authors": [
    [
     "Ananth",
     "Sankar"
    ]
   ],
   "title": "A new look at HMM parameter tying for large vocabulary speech recognition",
   "original": "i98_0193",
   "page_count": 4,
   "order": 638,
   "p1": "paper 0193",
   "pn": "",
   "abstract": [
    "Most current state-of-the-art large-vocabulary continuous speech recognition (LVCSR) systems are based on state-clustered hidden Markov models (HMMs). Typical systems use thousands of state clusters, each represented by a Gaussian mixture model with a few tens of Gaussians. In this paper, we show that models with far more parameter tying, like phonetically tied mixture (PTM) models, give better performance in terms of both recognition accuracy and speed. In particular, we achieved between a 5 and 10% improvement in word error rate, while cutting the number of Gaussian distance computations in half, for three different Wall Street Journal (WSJ) test sets, by using a PTM system with 38 phone-class state clusters, as compared to a state-clustered system with 937 state clusters. For both systems, the total number of Gaussians was fixed at about 30,000. This result is of real practical significance as we show that a conceptually simpler PTM system can achieve faster and more accurate performance than current state-of-the-art state-clustered HMM systems.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-635"
  },
  "gopinath98_icslp": {
   "authors": [
    [
     "Ramesh A.",
     "Gopinath"
    ],
    [
     "Bhuvana",
     "Ramabhadran"
    ],
    [
     "Satya",
     "Dharanipragada"
    ]
   ],
   "title": "Factor analysis invariant to linear transformations of data",
   "original": "i98_0397",
   "page_count": 4,
   "order": 639,
   "p1": "paper 0397",
   "pn": "",
   "abstract": [
    "Modeling data with Gaussian distributions is an important statistical problem. To obtain robust models one imposes constraints the means and covariances of these distributions. Constrained ML modeling implies the existence of optimal feature spaces where the constraints are more valid. This paper introduces one such constrained ML modeling technique called factor analysis invariant to linear transformations(FACILT) which is essentially factor analysis in optimal feature spaces. FACILT is a generalization of several existing methods for modeling covariances. This paper presents an EM algorithm for FACILT modeling.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-636"
  },
  "ando98_icslp": {
   "authors": [
    [
     "Akio",
     "Ando"
    ],
    [
     "Akio",
     "Kobayashi"
    ],
    [
     "Toru",
     "Imai"
    ]
   ],
   "title": "A thesaurus-based statistical language model for broadcast news transcription",
   "original": "i98_0016",
   "page_count": 4,
   "order": 640,
   "p1": "paper 0016",
   "pn": "",
   "abstract": [
    "This paper describes a thesaurus-based class n-gram model for broadcast news transcription. The most important issue concerned with class n-gram models is how to develop a word classification. We construct a word classification mapping based on a thesaurus so as to maximize the average mutual information function on a training corpus. To examine the effectiveness of the new method, we compare it with two our previous methods, in which the same thesaurus is used but word-class mappings are determined in the different manners. The new method achieved substantially lower perplexity for 83 news transcription sentences broadcast on June 4, 1996.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-637"
  },
  "balakrishnan98_icslp": {
   "authors": [
    [
     "Sreeram V.",
     "Balakrishnan"
    ]
   ],
   "title": "Effect of task complexity on search strategies for the motorola lexicus continuous speech recognition system",
   "original": "i98_0295",
   "page_count": 4,
   "order": 641,
   "p1": "paper 0295",
   "pn": "",
   "abstract": [
    "As speech recognition systems are increasingly applied to real world problems, it is often desirable to use the same recognition engine for a variety of tasks of differing complexity. This paper explores the relationship between the complexity of the recognition task and the best strategies for pruning the recognition search space. We examine two types of task: 20000 word WSJ dictation, and phone book access using a 60 word grammar. For both tasks we compare two strategies for pruning the search space: absolute pruning, where the number of hypotheses is controlled by eliminating ones that have a score less than a fixed beamwidth below the best scoring hypothesis, and rank based pruning, where hypotheses are ranked by score and all hypotheses beneath a certain rank are eliminated. We present statistics characterizing the behaviour of the recognizer under different pruning strategies and show how the strategies affect error-rates.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-638"
  },
  "bansal98_icslp": {
   "authors": [
    [
     "Dhananjay",
     "Bansal"
    ],
    [
     "Mosur K.",
     "Ravishankar"
    ]
   ],
   "title": "New features for confidence annotation",
   "original": "i98_0829",
   "page_count": 4,
   "order": 642,
   "p1": "paper 0829",
   "pn": "",
   "abstract": [
    "In this paper we describe two new confidence measures for estimating the reliability of speech-to-text output: \"Likelihood Dependence\" and \"Neighborhood Dependence\". Each word in the speech-to-text output for a given utterance is annotated with these two measures. Likelihood dependence for a given word occurrence indicates how critical that word is to the overall utterance likelihood; i.e., how much worse is the likelihood of the next best utterance if that word is eliminated from the recognition. Neighborhood dependence measures how stable a given word is when neighboring words are changed in the recognition. We show that correct and incorrect words in the recognition behave significantly differently with respect to these measures. We also show that on the broadcast news task they perform better than some of the existing, commonly used measures.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-639"
  },
  "bellegarda98b_icslp": {
   "authors": [
    [
     "Jerome R.",
     "Bellegarda"
    ]
   ],
   "title": "Multi-Span statistical language modeling for large vocabulary speech recognition",
   "original": "i98_0134",
   "page_count": 4,
   "order": 643,
   "p1": "paper 0134",
   "pn": "",
   "abstract": [
    "The goal of multi-span language modeling is to integrate the various constraints, both local and global, that are present in the language. In this paper, local constraints are captured via the usual n-gram approach, while global constraints are taken into account through the use of latent semantic analysis. An integrative formulation is derived for the combination of these two paradigms, resulting in an entirely data-driven, multi-span framework for large vocabulary speech recognition. Because of the inherent complementarity in the two types of constraints, the performance of the integrated language model compares favorably with the corresponding n-gram performance. On a subset of the Wall Street Journal speaker-independent, 20,000-word vocabulary, continuous speech task, we observed a reduction in perplexity of about 25%, and a reduction in average error rate of about 15%.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-640"
  },
  "chengalvarayan98b_icslp": {
   "authors": [
    [
     "Rathinavelu",
     "Chengalvarayan"
    ]
   ],
   "title": "Maximum-likelihood updates of HMM duration parameters for discriminative continuous speech recognition",
   "original": "i98_0021",
   "page_count": 4,
   "order": 644,
   "p1": "paper 0021",
   "pn": "",
   "abstract": [
    "Previous studies showed that a significantly enhanced recognition performance can be achieved by incorporating information about HMM duration along with the cepstral parameters. The reestimation formula for the duration parameters have been derived in the past using fixed segmentation during K-means training and the duration statistics are always fixed throughout the additional minimum string error (MSE) training process. In this study, we update the duration parameters along with other model parameters during discriminative training iterations. The convergence property of the training property based on the MSE approach is investigated, and experimental results on wireline connected digit recognition task demonstrated a 6% word error rate reduction by using the newly trained duration model parameters as compared to fixed duartion parameters during MSE training.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-641"
  },
  "coccaro98_icslp": {
   "authors": [
    [
     "Noah",
     "Coccaro"
    ],
    [
     "Daniel",
     "Jurafsky"
    ]
   ],
   "title": "Towards better integration of semantic predictors in statistical language modeling",
   "original": "i98_0852",
   "page_count": 4,
   "order": 645,
   "p1": "paper 0852",
   "pn": "",
   "abstract": [
    "We introduce a number of techniques designed to help integrate semantic knowledge with N-gram language models for automatic speech recognition. Our techniques allow us to integrate Latent Semantic Analysis (LSA), a word-similarity algorithm based on word co-occurrence information, with N-gram models. While LSA is good at predicting content words which are coherent with the rest of a text, it is a bad predictor of frequent words, has a low dynamic range, and is inaccurate when combined linearly with N-grams. We show that modifying the dynamic range, applying a per-word confidence metric, and using geometric rather than linear combinations with N-grams produces a more robust language model which has a lower perplexity on a Wall Street Journal test-set than a baseline N-gram model.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-642"
  },
  "pastor98_icslp": {
   "authors": [
    [
     "Julio",
     "Pastor"
    ],
    [
     "Jos√©",
     "Col√°s"
    ],
    [
     "Ruben",
     "San-Segundo"
    ],
    [
     "Jos√© Manuel",
     "Pardo"
    ]
   ],
   "title": "An asymmetric stochastic language model based on multi-tagged words",
   "original": "i98_1108",
   "page_count": 4,
   "order": 646,
   "p1": "paper 1108",
   "pn": "",
   "abstract": [
    "Tag definition in stochastic language models (n-grams and n-pos) is based on grouping together words with similar right and left context behavior. A modification of the n-gram model using multi-tagged words and unsupervised clustering was already introduced for French with a corpus of millions of non-tagged words. We present a variation of bi-pos language model where two tag sets are defined and assigned to each word (multi-tagged model) using grammatical information. Each tag set is based on different context behavior. We use linguistic expert knowledge and a simple automatic clustering procedure to obtain groups of words with similar left context behavior (first set of tags) and with similar right context (second set of tags). We propose a grammatical based model useful when no big text corpus is available and a performance increase has been observed when multi-tagged words are used because of its better adaptation to the language.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-643"
  },
  "digalakis98_icslp": {
   "authors": [
    [
     "Vassilis",
     "Digalakis"
    ],
    [
     "Leonardo",
     "Neumeyer"
    ],
    [
     "Manolis",
     "Perakakis"
    ]
   ],
   "title": "Product-code vector quantization of cepstral parameters for speech recognition over the WWW",
   "original": "i98_0940",
   "page_count": 4,
   "order": 647,
   "p1": "paper 0940",
   "pn": "",
   "abstract": [
    "We follow the paradigm that we have previously introduced for the encoding of the recognizer parameters in a client-server model used for recognition over wireless networks and the WWW, trying to maximize recognition performance instead of perceptual reproduction. We present a new encoding scheme for the mel frequency-warped cepstral parameters (MFCCs) that uses product-code vector quantization, and we find that the required bit rate to achieve the recognition performance of high-quality unquantized speech is just 2000 bits per second. We also investigate the effect of additive noise on the recognition performance when quantized features are used, and we find that a small increase in the bit rate can provide the necessary robustness.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-644"
  },
  "doherty98_icslp": {
   "authors": [
    [
     "Bernard",
     "Doherty"
    ],
    [
     "Saeed",
     "Vaseghi"
    ],
    [
     "Paul",
     "McCourt"
    ]
   ],
   "title": "Context dependent tree based transforms for phonetic speech recognition",
   "original": "i98_0323",
   "page_count": 4,
   "order": 648,
   "p1": "paper 0323",
   "pn": "",
   "abstract": [
    "This paper presents a novel method for modeling phonetic context using linear context transforms. Initial investigations have shown the feasibility of synthesising context dependent models from context independent models through weighted interpolation of the peripheral states of a given hidden markov model with its adjacent model. This idea can be further extended, to maximum likelihood estimation of not only single weights, but a matrix of weights or a transform. This paper outlines the application of Maximum Likelihood Linear Regression (MLLR) as a means of modeling context dependency in continuous density Hidden Markov Models (HMM).\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-645"
  },
  "johnson98_icslp": {
   "authors": [
    [
     "Michael T.",
     "Johnson"
    ],
    [
     "Mary P.",
     "Harper"
    ],
    [
     "Leah H.",
     "Jamieson"
    ]
   ],
   "title": "Interfacing acoustic models with natural language processing systems",
   "original": "i98_0871",
   "page_count": 4,
   "order": 649,
   "p1": "paper 0871",
   "pn": "",
   "abstract": [
    "The research presented here focuses on implementation and efficiency issues associated with the use of word graphs for interfacing acoustic speech recognition systems with natural language processing systems. The effectiveness of various pruning methods for graph construction is examined, as well as techniques for word graph compression. In addition, the word graph representation is compared to another predominant interface method, the N-best sentence list.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-646"
  },
  "jang98b_icslp": {
   "authors": [
    [
     "Photina Jaeyoun",
     "Jang"
    ],
    [
     "Alexander G.",
     "Hauptmann"
    ]
   ],
   "title": "Hierarchical cluster language modeling with statistical rule extraction for rescoring n-best hypotheses during speech decoding",
   "original": "i98_0934",
   "page_count": 4,
   "order": 650,
   "p1": "paper 0934",
   "pn": "",
   "abstract": [
    "We propose an unsupervised learning algorithm that learns hierarchical patterns of word sequences in spoken language utterances. It extracts cluster rules from training data based on high n-gram language model probabilities to cluster words or segment a sentence. Cluster trees, similar to parse trees, are constructed from the learned cluster rules. This hierarchical clustering adds grammatical structure onto a traditional trigram language model. The learned cluster rules are used to rescore and improve the n-best utterance hypothesis list which is output by a speech recognizer based on acoustic and trigram language model scores. Our hierarchical cluster language model was trained on TREC broadcast news data from 1995 and 1996, and reduced word error rate on the HUB-4 1997 broadcast news development set by 0.3% absolute. Prior symbolic knowledge in the form of rules can also be incorporated by simply applying the rules to the training data before the applicable learning iteration.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-647"
  },
  "kai98_icslp": {
   "authors": [
    [
     "Atsuhiko",
     "Kai"
    ],
    [
     "Yoshifumi",
     "Hirose"
    ],
    [
     "Seiichi",
     "Nakagawa"
    ]
   ],
   "title": "Dealing with out-of-vocabulary words and speech disfluencies in an n-gram based speech understanding system",
   "original": "i98_0785",
   "page_count": 4,
   "order": 651,
   "p1": "paper 0785",
   "pn": "",
   "abstract": [
    "In this study, we investigate the effectiveness of an unknown word processing(UWP) algorithm, which is incorporated into an N-gram language model based speech recognition system for dealing with filled pauses and out-of-vocabulary(OOV) words. We have already been investigated the effect of the UWP algorithm, which utilizes a simple subword sequence decoder, in a spoken dialog system using a context free grammar(CFG) as a language model. The effect of the UWP algorithm was investigated using an N-based continuous speech recognition system on both a small dialog task and a large-vocabulary read speech dictation task. The experiment results showed that the UWP improves the recognition accuracy and an N-gram based system with the UWP can improve the understanding performance in compared with a CFG-based system.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-648"
  },
  "kobayashi98_icslp": {
   "authors": [
    [
     "Tetsunori",
     "Kobayashi"
    ],
    [
     "Yosuke",
     "Wada"
    ],
    [
     "Norihiko",
     "Kobayashi"
    ]
   ],
   "title": "Source-extended language model for large vocabulary continuous speech recognition",
   "original": "i98_0708",
   "page_count": 4,
   "order": 652,
   "p1": "paper 0708",
   "pn": "",
   "abstract": [
    "Information source extension is utilized to improve the language model for large vocabulary continuous speech recognition (LVCSR). McMillan's theory, source extension make the model entropy close to the real source entropy, implies that the better language model can be obtained by source extension (making new unit through word concatenations and using the new unit for the language modeling). In this paper, we examined the effectiveness of this source extension. Here, we tested two methods of source extension: frequency-based extension and entropy-based extension. We tested the effect in terms of perplexity and recognition accuracy using Mainichi newspaper articles and JNAS speech corpus. As the results, the bi-gram perplexity is improved from 98.6 to 70.8 and tri-gram perplexity is improved from 41.9 to 26.4. The bigram-based recognition accuracy is improved from 79.8% to 85.3%.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-649"
  },
  "kobayashi98b_icslp": {
   "authors": [
    [
     "Akio",
     "Kobayashi"
    ],
    [
     "Kazuo",
     "Onoe"
    ],
    [
     "Toru",
     "Imai"
    ],
    [
     "Akio",
     "Ando"
    ]
   ],
   "title": "Time dependent language model for broadcast news transcription and its post-correction",
   "original": "i98_0973",
   "page_count": 4,
   "order": 653,
   "p1": "paper 0973",
   "pn": "",
   "abstract": [
    "This paper presents two linguistic techniques to improve broadcast news transcription. The first one is an adaptation of a language model which reflects current news content. It is based on a weighted mixture of long-term news scripts and latest scripts as training data. The mixture weights are given by the EM algorithm for linear interpolation and then normalized by their text sizes. Not only n-grams but also the vocabulary are updated by the latest news. We call it the Time Dependent Language Model (TDLM). It achieved a 4.4% reduction in perplexity and 0.7% improvement in word accuracy over the baseline language model. The second technique is correction of the decoded transcriptions by their corresponding electronic draft scripts. The corresponding drafts are found by using a sentence similarity measure between them. Parts to be considered as recognition errors are replaced with the original drafts. This post-correction led to a 6.7% improvement in word accuracy.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-650"
  },
  "koreman98b_icslp": {
   "authors": [
    [
     "Jacques",
     "Koreman"
    ],
    [
     "William J.",
     "Barry"
    ],
    [
     "Bistra",
     "Andreeva"
    ]
   ],
   "title": "Exploiting transitions and focussing on linguistic properties for ASR",
   "original": "i98_0548",
   "page_count": 4,
   "order": 654,
   "p1": "paper 0548",
   "pn": "",
   "abstract": [
    "Three cross-language ASR experiments which use hidden Markov modelling are described. Their goal is to process the signal so that it better exploits its linguistically relevant properties for consonant identification. Experiment 1 shows that consonant identification improves when vowel transitions are used. Particularly the consonants' place of articulation is identified better, because the vowel transitions contain formant trajectories which depend on the consonant's place of articulation. Experiment 2 shows that mapping acoustic parameters onto phonetic features before applying hidden Markov modelling greatly improves consonant identification rates. In experiment 3, the acoustic parameters from the vowel transitions are also mapped onto consonantal (not vocalic!) features (\"relational processing\"), as are the acoustic parameters belonging to the consonants. The additional use of vowel transitions does not further improve consonant identification, however. This is probably due to undertraining of the vowel transitions in the Kohonen network.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-651"
  },
  "lau98_icslp": {
   "authors": [
    [
     "Raymond",
     "Lau"
    ],
    [
     "Stephanie",
     "Seneff"
    ]
   ],
   "title": "A unified framework for sublexical and linguistic modelling supporting flexible vocabulary speech understanding",
   "original": "i98_0053",
   "page_count": 4,
   "order": 655,
   "p1": "paper 0053",
   "pn": "",
   "abstract": [
    "Previously, we introduced the ANGIE framework for modelling speech where morphological and phonological substructures of words are jointly characterized by a context-free grammar and represented in a multi-layered hierarchical structure. We also demonstrated a phonetic recognizer and a word-spotter based on ANGIE. In this work, we extend ANGIE to a competitive continuous speech recognition system. Furthermore, given that ANGIE is based on a context-free framework, we have decided to combine ANGIE with TINA, a context-free based framework for natural language understanding, into an integrated system. The integration led to a 21.7% reduction in word error rate compared to a baseline word bigram recognizer on ATIS. We also examined the addition of new words to the vocabulary, an area we believe will benefit from both ANGIE and the ANGIE-plus-TINA integration. The combination reduced error rate by 20.8% over the baseline and outperformed several other configurations tested not involving an integrated ANGIE-plus-TINA.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-652"
  },
  "bahl98_icslp": {
   "authors": [
    [
     "Lalit R.",
     "Bahl"
    ],
    [
     "S. De",
     "Gennaro"
    ],
    [
     "P. De",
     "Souza"
    ],
    [
     "E.",
     "Epstein"
    ],
    [
     "J.M. Le",
     "Roux"
    ],
    [
     "B.",
     "Lewis"
    ],
    [
     "C.",
     "Waast"
    ]
   ],
   "title": "A method for modeling liaison in a speech recognition system for French",
   "original": "i98_0114",
   "page_count": 4,
   "order": 656,
   "p1": "paper 0114",
   "pn": "",
   "abstract": [
    "In French the pronunciations of many words change dramatically depending on the word immediately preceding it. The result of this phenomenon, known as \"liaison\", in an ASR system that does not model \"liaison\" is the requirement of unnatural pronunciation and much user dissatisfaction. We present, in this paper, the development of an acoustic model which takes into account the wide variability of word pronunciations caused by the liaison, the integration of this model into a French continuous speech recognition system and decoding results.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-653"
  },
  "liu98b_icslp": {
   "authors": [
    [
     "Fu-Hua",
     "Liu"
    ],
    [
     "Michael",
     "Picheny"
    ]
   ],
   "title": "On variable sampling frequencies in speech recognition",
   "original": "i98_0838",
   "page_count": 4,
   "order": 657,
   "p1": "paper 0838",
   "pn": "",
   "abstract": [
    "In this paper we describe a novel approach to address the issue of different sampling frequencies in speech recognition. When a recognition task needs a different sampling frequency from that of the reference system, it is customary to re-train the system for the new sampling rate. To circumvent the tedious training process, we propose a new approach termed Sampling Rate Transformation (SRT) to perform the transformation directly on speech recognition system. By re-scaling the mel-filter design and filtering the system in spectrum domain, SRT converts the existing system to the target spectral range. New systems are obtained without using any data from the test environment. SRT reduces the word error rate from 29.89% to 18.17% given 11KHz test data and a 16KHz SI system. The matched system for 11KHz has an error rate of 16.17%. We also examine MLLR and MAP. The best result from MLLR is 17.92% with 4.5 hours of speech. Similar improvements are also observed in the speaker adaptation mode.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-654"
  },
  "ma98_icslp": {
   "authors": [
    [
     "Kristine",
     "Ma"
    ],
    [
     "George",
     "Zavaliagkos"
    ],
    [
     "Rukmini",
     "Iyer"
    ]
   ],
   "title": "Pronunciation modeling for large vocabulary conversational speech recognition",
   "original": "i98_0866",
   "page_count": 4,
   "order": 658,
   "p1": "paper 0866",
   "pn": "",
   "abstract": [
    "In this paper, we address the issue of deriving and using more realistic pronunciations to represent words spoken in natural conversational speech. Previous approaches include using automatic phoneme-based rule-learning techniques, linguistic transformation rules, and phonetically hand-labelled corpus to expand the number of pronunciation variants per word. While rule-based approaches have the advantage of being easily extensible to infrequent or unobserved words, they suffer from the problem of over generalization. Using hand-transcribed data, one can obtain a more concise set of new pronunciations but it cannot be extended to unobserved or infrequently occuring words. In this paper, we adopt the hand-labelled corpus scheme to improve pronunciations for frequent multi and single words occurring in the training data, while using the rule-based techniques to learn pronunciation variants and their weights for the infrequent words. Furthermore, we experiment with a new approach for speaker-dependent pronunciation modeling. The newly expanded dictionaries are evaluated on the Switchboard and Callhome corpora, giving a slight reduction in word recognition error rate.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-655"
  },
  "basu98b_icslp": {
   "authors": [
    [
     "Sankar",
     "Basu"
    ],
    [
     "Abraham",
     "Ittycheriah"
    ],
    [
     "St√©phane",
     "Maes"
    ]
   ],
   "title": "Time shift invariant speech recognition",
   "original": "i98_0983",
   "page_count": 4,
   "order": 659,
   "p1": "paper 0983",
   "pn": "",
   "abstract": [
    "When shifting by a few samples a speech signal, we have observed significant variations of the feature vectors produced by the acoustic front-end. Furthermore, these utterances when decoded with a continuous speech recognition system leads to dramatically different word error rates. This paper analyzes the phenomena and illustrates the well known result that classical acoustic front end processors including spectrum and cepstra based techniques suffer from time-shift. After describing the effect of sample sized shifts on the spectral estimates of the signal, we propose several techniques which take advantage of shift variations to multiply the amount of training that speech utterances can provide. Eventually, we illustrate how it is possible to slightly modify the acoustic front-end to render the recognizer invariant to small shifts.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-656"
  },
  "marino98_icslp": {
   "authors": [
    [
     "Jos√© B.",
     "Mari√±o"
    ],
    [
     "Pau",
     "Paches-Leal"
    ],
    [
     "Albino",
     "Nogueiras"
    ]
   ],
   "title": "The demiphone versus the triphone in a decision-tree state-tying framework",
   "original": "i98_0250",
   "page_count": 4,
   "order": 660,
   "p1": "paper 0250",
   "pn": "",
   "abstract": [
    "The performances of the demiphone (a context dependent subword unit that models independently the left and the right parts of a phoneme) and the triphone are compared. Continuous density hidden Markov modeling for both types of units is tested with the HTK software using decision-tree state clustering. The speech material is taken from the SpeechDat Spanish database, composed by continuous speech utterances recorded through the public telephone network. The training corpus is speaker and task independent. Two testing sets are tried: isolated words corresponding to speaker names, city names and phonetically rich words; and numbers of Spanish identification cards and dates. The main conclusion is that the demiphone simplifies the recognition system and yields a better performance than the triphone. This result may be explained by the ability of the demiphone to provide an excellent tradeoff between a detailed coarticulation modeling and a proper parameter estimation.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-657"
  },
  "mori98_icslp": {
   "authors": [
    [
     "Shinsuke",
     "Mori"
    ],
    [
     "Masafumi",
     "Nishimura"
    ],
    [
     "Nobuyasu",
     "Itoh"
    ]
   ],
   "title": "Word clustering for a word bi-gram model",
   "original": "i98_0989",
   "page_count": 4,
   "order": 661,
   "p1": "paper 0989",
   "pn": "",
   "abstract": [
    "In this paper we describe a word clustering method for class-based n-gram model. The measurement for clustering is the entropy on a corpus different from the corpus for n-gram model estimation. The search method is based on the greedy algorithm. We applied this method to a Japanese EDR corpus and English Penn Treebank corpus. The perplexities of word-based n-gram model on EDR corpus and Penn Treebank are 153.1 and 203.5 respectively. And Those of class-based n-gram model, estimated through our method, are 146.4 and 136.0 respectively. The result tells us that our clustering methods is better than the Brown's method and the Ney's method called leaving-one-out.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-658"
  },
  "neto98_icslp": {
   "authors": [
    [
     "Joao P.",
     "Neto"
    ],
    [
     "Ciro",
     "Martins"
    ],
    [
     "Luis B.",
     "Almeida"
    ]
   ],
   "title": "A large vocabulary continuous speech recognition hybrid system for the portuguese language",
   "original": "i98_0562",
   "page_count": 4,
   "order": 662,
   "p1": "paper 0562",
   "pn": "",
   "abstract": [
    "Due to the enormous development of large vocabulary, speaker-independent continuous speech recognition systems, which occur essentially for the US English language, there is a large demand of this kind of systems for other languages. In this paper we present the work done in the development of a large vocabulary, speaker-independent continuous speech recognition hybrid system for the European Portuguese language. This is a difficult task due to the basic development stage of this technology in the European Portuguese language. The development of a system of this kind for a new language depends on the availability of the appropriate source components, mainly a speech corpus and large amounts of texts. This work became possible due to the development of a new database (BD-PUBLICO), a large vocabulary speech corpus for the European Portuguese language developed by us over the last two years.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-659"
  },
  "padmanabhan98_icslp": {
   "authors": [
    [
     "Mukund",
     "Padmanabhan"
    ],
    [
     "Bhuvana",
     "Ramabhadran"
    ],
    [
     "Sankar",
     "Basu"
    ]
   ],
   "title": "Speech recognition performance on a new voicemail transcription task",
   "original": "i98_0210",
   "page_count": 4,
   "order": 663,
   "p1": "paper 0210",
   "pn": "",
   "abstract": [
    "In this paper we describe a new testbed for developing speech recognition algorithms - a VoiceMail transcription task, analogous to other tasks such as the Switchboard, CallHome, and the Hub 4 tasks, which are currently used by speech recognition researchers. We describe (i) the use of compound words to model co-articulation effects in commonly occurring words (ii) the use of linguistically derived phonological (that model phenomena such as degemination, palatization etc) for other words (iii) a new model-complexity adaptation technique that uses a discriminant measure to allocate gaussians to the mixtures modelling the acoustic units (allophones) (iv) experiments in using different feature extraction methods (v) we also investigated the efficacy of some well known acoustic adaptation techniques on this task. We then reported experimental results that showed that most of the modelling techniques we investigated were useful in reducing the word error rate - from 87% (when decoded with Switchboard acoustic and language models) to 38%.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-660"
  },
  "palazuelos98_icslp": {
   "authors": [
    [
     "Sira",
     "Palazuelos"
    ],
    [
     "Santiago",
     "Aguilera"
    ],
    [
     "Jos√©",
     "Rodrigo"
    ],
    [
     "Juan",
     "Godino"
    ]
   ],
   "title": "Grammatical and statistical word prediction system for Spanish integrated in an aid for people with disabilities",
   "original": "i98_0381",
   "page_count": 4,
   "order": 664,
   "p1": "paper 0381",
   "pn": "",
   "abstract": [
    "In this paper, we describe and evaluate recent work and results achieved in a word prediction system for Spanish, applying both statistical and grammatical methods: word pairs and trios, bipos and tripos models, and a stochastic context free grammar. The predictor is included in several software applications, to enhance the typing rate for users with motorical problems, who can only use a switch for writing. These users have difficulties in writing, and usually in communicating with other people, and the inclusion of word prediction in the system allow them to increase their typing rate from 2-3 words per minute up to 8-10 words per minute.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-661"
  },
  "papineni98_icslp": {
   "authors": [
    [
     "Kishore",
     "Papineni"
    ],
    [
     "Satya",
     "Dharanipragada"
    ]
   ],
   "title": "Segmentation using a maximum entropy approach",
   "original": "i98_0559",
   "page_count": 3,
   "order": 665,
   "p1": "paper 0559",
   "pn": "",
   "abstract": [
    "Consider generating phonetic baseforms from orthographic spellings. Availability of a segmentation (grouping) of the characters can be exploited to achieve better phonetic translation. We are interested in building segmentation models without using explicit segmentation or alignment information during training. The heart of our segmentation algorithm is a conditional probabilistic model that predicts whether there are less, equal, or more phones than characters in the word. We use just this contraction-expansion information on whole words for training the model. The model has three components: a prior model, a set of features, and weights of the features. The features are selected and weights assigned in maximum entropy framework. Even though the model is trained on whole words, we effectively localize it on substrings to induce segmentation of the word to be segmented. Segmentation is also aided by considering substrings in both forward and backward directions.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-662"
  },
  "berger98_icslp": {
   "authors": [
    [
     "Adam",
     "Berger"
    ],
    [
     "Harry",
     "Printz"
    ]
   ],
   "title": "Recognition performance of a large-scale dependency grammar language model",
   "original": "i98_0679",
   "page_count": 4,
   "order": 666,
   "p1": "paper 0679",
   "pn": "",
   "abstract": [
    "We describe a large-scale investigation of dependency grammar language models. Our work includes several significant departures from earlier studies, notably a larger training corpus, improved model structure, different feature types, new feature selection methods, andmore coherent training and test data. We report word error rate (WER) results of a speech recognition experiment, in which we used these models to rescore the output of the IBM speech recognition system.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-663"
  },
  "ramaswamy98b_icslp": {
   "authors": [
    [
     "Ganesh N.",
     "Ramaswamy"
    ],
    [
     "Harry",
     "Printz"
    ],
    [
     "Ponani S.",
     "Gopalakrishnan"
    ]
   ],
   "title": "A bootstrap technique for building domain-dependent language models",
   "original": "i98_0611",
   "page_count": 4,
   "order": 667,
   "p1": "paper 0611",
   "pn": "",
   "abstract": [
    "In this paper, we propose a new bootstrap technique to build domain-dependent language models. We assume that a seed corpus consisting of a small amount of data relevant to the new domain is available, which is used to build a reference language model. We also assume the availability of an external corpus, consisting of a large amount of data from various sources, which need not be directly relevant to the domain of interest. We use the reference language model and a suitable metric, such as the perplexity measure, to select sentences from the external corpus that are relevant to the domain. Once we have a sufficient number of new sentences, we can rebuild the reference language model. We then continue to select additional sentences from the external corpus, and this process continues to iterate until some satisfactory termination point is achieved. We also describe several methods to further enhance the bootstrap technique, such as combining it with mixture modeling and class-based modeling. The performance of the proposed approach was evaluated through a set of experiments, and the results are discussed. Analysis of the convergence properties of the approach and the conditions that need to be satisfied by the external corpus and the seed corpus are highlighted, but detailed work on these issues is deferred for the future.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-664"
  },
  "sanchez98_icslp": {
   "authors": [
    [
     "Joan-Andreu",
     "Sanchez"
    ],
    [
     "Jos√©-Miguel",
     "Benedi"
    ]
   ],
   "title": "Estimation of the probability distributions of stochastic context-free grammars from the k-best derivations",
   "original": "i98_1032",
   "page_count": 4,
   "order": 668,
   "p1": "paper 1032",
   "pn": "",
   "abstract": [
    "The use of the Inside-Outside algorithm for the estimation of the probability distributions of Stochastic Context-Free Grammars in Natural-Language processing is restricted due to the time complexity per iteration and the large number of iterations that it needs to converge. Alternatively, an algorithm based on the Viterbi score (VS) is used. This VS algorithm converges more rapidly, but obtains less competitive models. We propose a new algorithm that only considers the k-best derivations in the estimation process. The time complexity per iteration of the algorithm is practically the same as that of the VS algorithm. The proposed algorithm has been proven with a part of the Wall Street Journal task processed in the Penn Treebank project. The perplexity of a test set for the VS algorithm was 24.22 and this value was 22.73 for the proposed algorithm with k= 3, which means an improvement of 6.15%.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-665"
  },
  "sankar98b_icslp": {
   "authors": [
    [
     "Ananth",
     "Sankar"
    ]
   ],
   "title": "Robust HMM estimation with Gaussian merging-splitting and tied-transform HMMs",
   "original": "i98_0194",
   "page_count": 4,
   "order": 669,
   "p1": "paper 0194",
   "pn": "",
   "abstract": [
    "We present two different approaches for robust estimation of the parameters of context-dependent hidden Markov models (HMMs) for speech recognition. The first approach, the Gaussian Merging-Splitting (GMS) algorithm, uses Gaussian splitting to uniformly distribute the Gaussians in acoustic space, and merging so as to compute only those Gaussians that have enough data for robust estimation. We show that this method is more robust than our previous training technique. The second approach, called tied-transform HMMs, uses maximum-likelihood transformation-based acoustic adaptation algorithms to transform a small HMM to a much larger HMM. Since the transforms are shared or tied among Gaussians in the larger HMM, robust estimation is achieved. We show that this approach gives a significant improvement in recognition accuracy and a dramatic reduction in memory needed to store the models.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-666"
  },
  "seymore98_icslp": {
   "authors": [
    [
     "Kristie",
     "Seymore"
    ],
    [
     "Stanley",
     "Chen"
    ],
    [
     "Ronald",
     "Rosenfeld"
    ]
   ],
   "title": "Nonlinear interpolation of topic models for language model adaptation",
   "original": "i98_0897",
   "page_count": 4,
   "order": 670,
   "p1": "paper 0897",
   "pn": "",
   "abstract": [
    "Topic adaptation for language modeling is concerned with adjusting the probabilities in a language model to better reflect the expected frequencies of topical words for a new document. We present a novel technique for adapting a language model to the topic of a document, using a nonlinear interpolation of n-gram language models. A three-way, mutually exclusive division of the vocabulary into general, on-topic and off-topic word classes is used to combine word predictions from a topic-specific and a general language model. We achieve a slight decrease in perplexity and speech recognition word error rate on a Broadcast News test set using these techniques. Our results are compared to results obtained through linear interpolation of topic models.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-667"
  },
  "takagi98_icslp": {
   "authors": [
    [
     "Kazuyuki",
     "Takagi"
    ],
    [
     "Rei",
     "Oguro"
    ],
    [
     "Kenji",
     "Hashimoto"
    ],
    [
     "Kazuhiko",
     "Ozeki"
    ]
   ],
   "title": "Performance evaluation of word phrase and noun category language models for broadcast news speech recognition",
   "original": "i98_0026",
   "page_count": 4,
   "order": 671,
   "p1": "paper 0026",
   "pn": "",
   "abstract": [
    "This paper reports our work to improve a bigram language model for Japanese TV broadcast news speech recognition. First, frequent word strings were grouped into phrases in order that the phrases were added to the lexicon as new units of recognition. The test set perplexity was improved when frequent function word strings were used as additional recognition units. The speech recognition performance was improved both by grouping function word strings and by grouping compound nouns that were selected by word association ratio. Secondly, in order to alleviate the OOV problem related with nouns, we built and tested a language model that allows switching its noun lexicon according to the domain of the article to be recognized next.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-668"
  },
  "tolba98c_icslp": {
   "authors": [
    [
     "Hesham",
     "Tolba"
    ],
    [
     "Douglas",
     "O'Shaughnessy"
    ]
   ],
   "title": "Robust automatic continuous-speech recognition based on a voiced-unvoiced decision",
   "original": "i98_0342",
   "page_count": 4,
   "order": 672,
   "p1": "paper 0342",
   "pn": "",
   "abstract": [
    "In this paper, the implementation of a robust front-end to be used for a large-vocabulary Continuous Speech Recognition (CSR) system based on a Voiced-Unvoiced (V-U) decision has been addressed. Our approach is based on the separation of the speech signal into voiced and unvoiced components. Consequently, speech enhancement can be achieved through processing of the voiced and the unvoiced components separately. Enhancement of the voiced component is performed using an adaptive comb filtering, whereas the unvoiced component is enhanced using the modified spectral subtraction approach. We proved via experiments that the proposed CSR system is robust in additive noisy environments (SNR down to 0 dB).\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-669"
  },
  "torrecilla98_icslp": {
   "authors": [
    [
     "Juan Carlos",
     "Torrecilla"
    ],
    [
     "Ismael",
     "Cortazar"
    ],
    [
     "Luis A.",
     "Hern√°ndez"
    ]
   ],
   "title": "Double tree beam search using hierarchical subword units",
   "original": "i98_0321",
   "page_count": 4,
   "order": 673,
   "p1": "paper 0321",
   "pn": "",
   "abstract": [
    "In this paper we proposed an efficient beam search procedure that combines well-known search techniques as a lexicon organization using tree-structured grammars with a novel approach of using different types of subword units depending on the local scores of the active words. An efficient double-tree structure using phonemes and triphones is presented. Experimental results on an isolated word recognition systems reveals that the proposed strategy is suitable for important reductions in computational cost with only negligible increases in recognition errors. Tests over a vocabulary of 955 Spanish words presents a 0.5% of increase in error rate for a 32% reduction in the number of senones to be evaluated.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-670"
  },
  "mulbregt98_icslp": {
   "authors": [
    [
     "Paul van",
     "Mulbregt"
    ],
    [
     "Ira",
     "Carp"
    ],
    [
     "Lawrence",
     "Gillick"
    ],
    [
     "Steve",
     "Lowe"
    ],
    [
     "Jon",
     "Yamron"
    ]
   ],
   "title": "Text segmentation and topic tracking on broadcast news via a hidden Markov model approach",
   "original": "i98_0116",
   "page_count": 4,
   "order": 674,
   "p1": "paper 0116",
   "pn": "",
   "abstract": [
    "Expertise in the automatic transcription of broadcast speech has progressed to the point of being able to use the resulting transcripts for information retrieval purposes. In this paper, we first describe a corpus of automatically recognized broadcast news, a method for segmenting the broadcast into stories, and finally apply this method to retrieve stories relating to a specific topic. The method is based on Hidden Markov Models and is in analogy with the usual implementation of HMMs in speech recognition.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-671"
  },
  "oneill98_icslp": {
   "authors": [
    [
     "Philip",
     "O'Neill"
    ],
    [
     "Saeed",
     "Vaseghi"
    ],
    [
     "Bernard",
     "Doherty"
    ],
    [
     "Wooi Haw",
     "Tan"
    ],
    [
     "Paul",
     "McCourt"
    ]
   ],
   "title": "Multi-phone strings as subword units for speech recognition",
   "original": "i98_0178",
   "page_count": 4,
   "order": 675,
   "p1": "paper 0178",
   "pn": "",
   "abstract": [
    "The choice of speech unit affects the accuracy, complexity, expandability and ease of adaptation of ASRs to speaker and environmental variations. This paper explores a method of subword modelling based on the concept of multi-phone strings. The motivation in using the longer duration multi-phone strings is to reduce the loss of contextual information, cross-phone correlation, and transitions. Multi-phone strings are an alternative to context-dependent phones and they include many of the syllables. An advantage of multi-phone units is the existence of more than one valid multi-phone transcription for each monophone sequence, this can be used to improve ASR accuracy. A particular case of multi-phone strings namely phone-pairs is investigated in detail. Experimental Evaluation on TIMIT and WSJCAM0 are presented.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-672"
  },
  "veilleux98_icslp": {
   "authors": [
    [
     "Nanette M.",
     "Veilleux"
    ],
    [
     "Stefanie",
     "Shattuck-Hufnagel"
    ]
   ],
   "title": "Phonetic modification of the syllable /tu/ in two spontaneous american English dialogues",
   "original": "i98_0382",
   "page_count": 4,
   "order": 676,
   "p1": "paper 0382",
   "pn": "",
   "abstract": [
    "In a pilot study of phonetic modification of function words in 2 spontaneous speech dialogues, 99 utterances of the syllable /tu/ corresponding to the morphemes to, two, too, -to and to- included ten pronunciation variants. Factors influencing phonetic modification included phonetic context, prosody, part of speech, adjacent disfluency and individual speaker. 11% of the acoustic landmarks defining /t/ closure, /t/ release and vowel jaw opening maximum were not detectable in hand labelling. In a separate corpus, 59% of recognition errors involved grammatical or function words like conjunctions, articles, prepositions, pronouns and auxilliary verbs, and for 17 tokens of /tu/, half were misrecognized. Implications of these preliminary results for linguistic theory, cognitive modelling of speech processing and automatic speech recognition are discussed.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-673"
  },
  "weng98_icslp": {
   "authors": [
    [
     "Fuliang",
     "Weng"
    ],
    [
     "Andreas",
     "Stolcke"
    ],
    [
     "Ananth",
     "Sankar"
    ]
   ],
   "title": "Efficient lattice representation and generation",
   "original": "i98_0136",
   "page_count": 4,
   "order": 677,
   "p1": "paper 0136",
   "pn": "",
   "abstract": [
    "We describe two new techniques for reducing word lattice sizes without eliminating hypotheses. The first technique is an algorithm to reduce the size of non-deterministic bigram word lattices by merging redundant nodes. On bigram word lattices generated from Hub4 Broadcast News speech, this reduces lattice sizes by half on average. The second technique is an improved algorithm for expanding lattices with trigram language models. Backed-off trigram probabilities are encoded without node duplication by factoring the probabilities into bigram probabilities and backoff weights, and duplicating nodes only for explicit trigrams. Experiments on Broadcast News show that this method reduces trigram lattice sizes by a factor of 6, and reduces expansion time by more than a factor of 10. Compared to conventionally expanded lattices, recognition with the compactly expanded lattices was also found to be 40% faster, without affecting recognition accuracy.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-674"
  },
  "wester98_icslp": {
   "authors": [
    [
     "Mirjam",
     "Wester"
    ],
    [
     "Judith M.",
     "Kessens"
    ],
    [
     "Helmer",
     "Strik"
    ]
   ],
   "title": "Modeling pronunciation variation for a dutch CSR: testing three methods",
   "original": "i98_0371",
   "page_count": 4,
   "order": 678,
   "p1": "paper 0371",
   "pn": "",
   "abstract": [
    "This paper describes how the performance of a continuous speech recognizer for Dutch has been improved by modeling pronunciation variation. We used three methods to model pronunciation variation. First, within-word variation was dealt with. Phonological rules were applied to the words in the lexicon, thus automatically generating pronunciation variants. Secondly, cross-word pronunciation variation was modeled using two different approaches. The first approach was to model cross-word processes by adding the variants as separate words to the lexicon and in the second approach this was done by using multi-words. For each of the methods, recognition experiments were carried out. A significant improvement was found for modeling within-word variation. Furthermore, modeling cross-word processes using multi-words leads to significantly better results than modeling them using separate words in the lexicon.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-675"
  },
  "whittaker98b_icslp": {
   "authors": [
    [
     "Edward W. D.",
     "Whittaker"
    ],
    [
     "Philip C.",
     "Woodland"
    ]
   ],
   "title": "Comparison of language modelling techniques for Russian and English",
   "original": "i98_0967",
   "page_count": 4,
   "order": 679,
   "p1": "paper 0967",
   "pn": "",
   "abstract": [
    "In this paper the main differences between language modelling of Russian and English are examined. A Russian corpus and a comparable English corpus are described. The effects of high inflectionality in Russian and the relationship between the out-of-vocabulary rate and vocabulary size are investigated. Standard word and class N-gram language modelling techniques are applied to the two corpora and perplexity results are reported. A novel approach to the modelling of inflected languages is proposed and its efficacy compared with the other techniques.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-676"
  },
  "witschel98_icslp": {
   "authors": [
    [
     "Petra",
     "Witschel"
    ]
   ],
   "title": "Optimized POS-based language models for large vocabulary speech recognition",
   "original": "i98_0471",
   "page_count": 4,
   "order": 680,
   "p1": "paper 0471",
   "pn": "",
   "abstract": [
    "Stochastic language models based on word n-grams require huge amount of training material especially for large vocabulary systems. Using n-grams based on classes much less training material is necessary and higher coverage can be achieved. Building classes on basis of linguistic characteristics (POS) has the advantage that new words can be assigned easily. Until now for POS-based language models class sets have usually been defined by linguistic experts. In this paper we present an approach where for a given number of classes a class set is generated automatically such that entropy of language model is minimized. We perform experiments on German medical reports of about 1.2 million words of text and 24000 words of vocabulary. Using our approach we generate an exemplary class set of 196 optimized POS-classes. Comparing the optimized POS-based language model to the language model based on 196 normally defined classes we get an improvement up to 10% in test set perplexity.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-677"
  },
  "wright98c_icslp": {
   "authors": [
    [
     "Mark",
     "Wright"
    ],
    [
     "Simon",
     "Hovell"
    ],
    [
     "Simon",
     "Ringland"
    ]
   ],
   "title": "Reducing peak search effort using two-tier pruning",
   "original": "i98_0450",
   "page_count": 4,
   "order": 681,
   "p1": "paper 0450",
   "pn": "",
   "abstract": [
    "Many of the pruning strategies used to remove less likely hypotheses from the search space in large vocabulary speech recognition (LVR) systems, have a peak search space many times greater than the average search space. This paper discusses two such strategies used within BT's speech recognition architecture, Step pruning and Histogram pruning. Two-tier pruning is proposed as a simple but powerful extension applicable to either of the above strategies. This seeks to limit the expansion of the search space between the prune and acoustic match processes without affecting accuracy. It is shown that the application of two-tier pruning to either strategy reduces peak search effort, and results in an average reduction in run time of 33% and 53% for step pruning and histogram pruning respectively, with no loss in top-N accuracy.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-678"
  },
  "zavaliagkos98_icslp": {
   "authors": [
    [
     "George",
     "Zavaliagkos"
    ],
    [
     "Man-Hung",
     "Siu"
    ],
    [
     "Thomas",
     "Colthurst"
    ],
    [
     "Jayadev",
     "Billa"
    ]
   ],
   "title": "Using untranscribed training data to improve performance",
   "original": "i98_1007",
   "page_count": 4,
   "order": 682,
   "p1": "paper 1007",
   "pn": "",
   "abstract": [
    "This paper explores techniques for utilizing untranscribed training data pools to increase the available training data for automatic speech recognition systems. It has been well established that current speech recognition technology, especially in Large Vocabulary Conversational Speech Recognition (LVCSR), is largely language independent, and that the dominant factor with regards to performance on a certain language is the amount of available training data. The paper addresses this need for increased training data by presenting ways to use untranscribed acoustic data to increase the training data size and thus improve speech recognition.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-679"
  },
  "jan98_icslp": {
   "authors": [
    [
     "Ea-Ee",
     "Jan"
    ],
    [
     "Raimo",
     "Bakis"
    ],
    [
     "Fu-Hua",
     "Liu"
    ],
    [
     "Michael",
     "Picheny"
    ]
   ],
   "title": "Telephone band LVCSR for hearing-impaired users",
   "original": "i98_0862",
   "page_count": 4,
   "order": 683,
   "p1": "paper 0862",
   "pn": "",
   "abstract": [
    "Large vocabulary automatic speech recognition might assist hearing impaired telephone users by displaying a transcription of the incoming side of the conversation, but the system would have to achieve sufficient accuracy on conversational-style, telephone-bandwidth speech. We describe our development work toward such a system. This work comprised three phases: Experiments with clean data filtered to 200-3500Hz, experiments with real telephone data, and language model development. In the first phase, the speaker independent error rate was reduced from 25% to 12% by using MLLT, increasing the number of cepstral components from 9 to 13, and increasing the number of Gaussians from 30,000 to 120,000. The resulting system, however, performed less well on actual telephony, producing an error rate of 28.4%. By additional adaptation and the use of an LDA and CDCN combination, the error rate was reduced to 19.1%. Speaker adaptation reduces the error rate to 10.96%. These results were obtained with read speech. To explore the language-model requirements in a more realistic situation, we collected some conversational speech with an arrangement in which one participant could not hear the conversation but only saw recognizer output on a screen. We found that a mixture of language models, one derived from the Switchboard corpus and the other from prepared texts, resulted in approximately 10% fewer errors than either model alone.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-680"
  },
  "bonafonte98b_icslp": {
   "authors": [
    [
     "Antonio",
     "Bonafonte"
    ],
    [
     "Jos√© B.",
     "Mari√±o"
    ]
   ],
   "title": "Using x-gram for efficient speech recognition",
   "original": "i98_1125",
   "page_count": 4,
   "order": 684,
   "p1": "paper 1125",
   "pn": "",
   "abstract": [
    "X-grams are a generalization of the n-grams, where the number of previous conditioning words is different for each case and decided from the training data. X-grams reduce perplexity with respect to trigrams and need less number of parameters. In this paper, the representation of the x-grams using finite state automata is considered. This representation leads to a new model, the non-deterministic x-grams, an approximation that is much more efficient, suffering small degradation on the modeling capability. Empirical experiments for a continuous speech recognition task show how, for each ending word, the number of transitions is reduced from 1222 (the size of the lexicon) to around 66.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-681"
  },
  "kawahara98b_icslp": {
   "authors": [
    [
     "Tatsuya",
     "Kawahara"
    ],
    [
     "Tetsunori",
     "Kobayashi"
    ],
    [
     "Kazuya",
     "Takeda"
    ],
    [
     "Nobuaki",
     "Minematsu"
    ],
    [
     "Katsunobu",
     "Itou"
    ],
    [
     "Mikio",
     "Yamamoto"
    ],
    [
     "Atsushi",
     "Yamada"
    ],
    [
     "Takehito",
     "Utsuro"
    ],
    [
     "Kiyohiro",
     "Shikano"
    ]
   ],
   "title": "Sharable software repository for Japanese large vocabulary continuous speech recognition",
   "original": "i98_0763",
   "page_count": 4,
   "order": 685,
   "p1": "paper 0763",
   "pn": "",
   "abstract": [
    "The project of Japanese LVCSR (Large Vocabulary Continuous Speech Recognition) platform is introduced. It is a collaboration of researchers of different academic institutes and intended to develop a sharable software repository of not only databases but also models and programs. The platform consists of a standard recognition engine, Japanese phone models and Japanese statistical language models. A set of Japanese phone HMMs are trained with ASJ (Acoustic Society of Japan) databases of 20K sentence utterances per each gender. Japanese word N-gram (2-gram and 3-gram) models are constructed with a corpus of Mainichi newspaper of four years. The recognition engine JULIUS is developed for assessment of both acoustic and language models. The modules are integrated as a Japanese LVCSR system and evaluated on 5000-word dictation task. The software repository is available to the public.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-682"
  },
  "itou98_icslp": {
   "authors": [
    [
     "Katunobu",
     "Itou"
    ],
    [
     "Mikio",
     "Yamamoto"
    ],
    [
     "Kazuya",
     "Takeda"
    ],
    [
     "Toshiyuki",
     "Takezawa"
    ],
    [
     "Tatsuo",
     "Matsuoka"
    ],
    [
     "Tetsunori",
     "Kobayashi"
    ],
    [
     "Kiyohiro",
     "Shikano"
    ],
    [
     "Shuichi",
     "Itahashi"
    ]
   ],
   "title": "The design of the newspaper-based Japanese large vocabulary continuous speech recognition corpus",
   "original": "i98_0722",
   "page_count": 4,
   "order": 686,
   "p1": "paper 0722",
   "pn": "",
   "abstract": [
    "In this paper we present the first public Japanese speech corpus for large vocabulary continuous speech recognition (LVCSR) technology, which we have titled JNAS (Japanese Newspaper Article Sentences). We designed it to be comparable to the corpora used in the American and European LVCSR projects. The corpus contains speech recordings (60 hrs.) and their orthographic transcriptions for 306 speakers (153 males and 153 females) reading excerpts from the newspaper's articles and phonetically balanced (PB) sentences. This corpus contains utterances of about 45,000 sentences as a whole with each speaker reading about 150 sentences. JNAS is being distributed on 16 CD-ROMs.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-683"
  },
  "ogata98_icslp": {
   "authors": [
    [
     "Jun",
     "Ogata"
    ],
    [
     "Yasuo",
     "Ariki"
    ]
   ],
   "title": "Indexing and classification of TV news articles based on speech dictation using word bigram",
   "original": "i98_0126",
   "page_count": 4,
   "order": 687,
   "p1": "paper 0126",
   "pn": "",
   "abstract": [
    "In order to construct a news database with a function of video on demand (VOD), it is required to classify news articles into topics. In this paper, we propose a method to automatically index and classify TV news articles into 10 topics based on a speech dictation techniques using speaker independent triphone HMMs and word bigram.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-684"
  },
  "siu98_icslp": {
   "authors": [
    [
     "Man-Hung",
     "Siu"
    ],
    [
     "Rukmini",
     "Iyer"
    ],
    [
     "Herbert",
     "Gish"
    ],
    [
     "Carl",
     "Quillen"
    ]
   ],
   "title": "Parametric trajectory mixtures for LVCSR",
   "original": "i98_0890",
   "page_count": 4,
   "order": 688,
   "p1": "paper 0890",
   "pn": "",
   "abstract": [
    "Parametric trajectory models explicitly represent the temporal evolution of the speech features as a Gaussian process with time-varying parameters. HMMs are a special case of such models, one in which the trajectory constraints in the speech segment are ignored by the assumption of conditional independence across frames within the segment. In this paper, we investigate in detail some extensions to our trajectory modeling approach aimed at improving LVCSR performance: (i) improved modeling of mixtures of trajectories via better initialization, (ii) modeling of context dependence, and (iii) improved segment boundaries by means of search. We will present results in terms of both phone classification and recognition accuracy on the Switchboard corpus.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-685"
  },
  "glaeser98b_icslp": {
   "authors": [
    [
     "Axel",
     "Glaeser"
    ],
    [
     "Fr√©d√©ric",
     "Bimbot"
    ]
   ],
   "title": "Steps toward the integration of speaker recognition in real-world telecom applications",
   "original": "i98_0878",
   "page_count": 4,
   "order": 689,
   "p1": "paper 0878",
   "pn": "",
   "abstract": [
    "The current market situation is characterized by a significant interest in speaker recognition functionalities in telecommunication systems (e.g. phone banking). This paper presents a field-test assessment of a speaker recognition algorithm, in a realistic context. Such field tests are particularly useful because the requirements for those real-world systems can be significantly different from those focused on by the research laboratories. Therefore, the results presented in this paper are divided into two groups. The quantitative ones describe the performance achieved in terms of Equal Error Rates (EER) as a function of the field-test conditions and different limitations on the enrollment and test duration. On the other hand, we discuss some innovative qualitative outcomes which are mainly based on non-technical but subjective impressions reported by the participants.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-686"
  },
  "chung98b_icslp": {
   "authors": [
    [
     "Hyun-Yeol",
     "Chung"
    ],
    [
     "Cheol-Jun",
     "Hwang"
    ],
    [
     "Shi-Wook",
     "Lee"
    ]
   ],
   "title": "A bimodal Korean address entry/retrieval system",
   "original": "i98_0460",
   "page_count": 4,
   "order": 690,
   "p1": "paper 0460",
   "pn": "",
   "abstract": [
    "This paper describes the development of a Korean address entry/retrieval system using bimodal input; speech recognition and touch sensitive display. The system works on a personal computer and employs automatic speech recognition and touch sensitive display techniques as user interface for input Korean address, which consisted with about 40,000 words. To meet the needs that practical speech recognition system should be worked in real time without any degradation of performance of recognition accuracy, 1)speaker and environmental adaptation by Maximum a posteriori(MAP) estimation were adopted for higher recognition and 2)fast search by tree-structured lexicon and frame synchronous beam search technique were employed for real time response. To offer more convenient user interface, touch sensitive display is also implemented. As the results, the system worked in 3 seconds after completion of address utterance with sentence recognition accuracy of above 96%.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-687"
  },
  "delogu98_icslp": {
   "authors": [
    [
     "Cristina",
     "Delogu"
    ],
    [
     "Andrea Di",
     "Carlo"
    ],
    [
     "Paolo",
     "Rotundi"
    ],
    [
     "Danilo",
     "Sartori"
    ]
   ],
   "title": "Usability evaluation of IVR systems with DTMF and ASR",
   "original": "i98_0320",
   "page_count": 4,
   "order": 691,
   "p1": "paper 0320",
   "pn": "",
   "abstract": [
    "The paper presents an usability evaluation of 4 different prototypes of the same IVR application: three of them are automatic speech recognition (ASR) based and the other is Dual Tone Multi Frequency (DTMF) based. Our work consists of the automation of a service providing information about new facilities offered by Telecom Italia. The usability of the different prototypes has been evaluated through objective and subjective measures. Objective measures such as task completion and correctness, number of calls for task, transaction time, number of turns, and recognition accuracy have been obtained through the system's logfiles and the recorded speech utterances. To gather subjective measures the users were asked to fill in a questionnaire about their perception of the quality of the overall interaction, their effort in interacting with the system, and their satisfaction with different features of the system. In general a good correspondence between objective and subjective measures was found.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-688"
  },
  "fung98_icslp": {
   "authors": [
    [
     "Pascale",
     "Fung"
    ],
    [
     "Chi Shun",
     "Cheung"
    ],
    [
     "Kwok Leung",
     "Lam"
    ],
    [
     "Wai Kat",
     "Liu"
    ],
    [
     "Yuen Yee",
     "Lo"
    ]
   ],
   "title": "SALSA version 1.0: a speech-based web browser for hong kong English",
   "original": "i98_0942",
   "page_count": 4,
   "order": 692,
   "p1": "paper 0942",
   "pn": "",
   "abstract": [
    "In this paper, we present a prototype speech-based Web browser, SALSA1.0, and describe some of the research issues we need to address while building this system for Hong Kong users. SALSA1.0 allows the user to speak English command words as well as partial or complete link names on any page. The research issues involved in building SALSA1.0 are mainly (1) how to handle large accent variations and mixed-language and (2) how to handle unknown words, especially proper names, in Web links. The recognition engine for SALSA1.0 is trained on WSJ data, and then retrained on a small amount of Hong Kong accent WSJ data to handle accent variations. An edit-distance algorithm is used to replace all unknown words by the closest known word in the word network for recognition. With these methods, link name recognition rate is at 91.20% for links without unknown words, and 82.40% for links with unknown words. SALSA is currently being developed into a multilingual, natural language-based Intranet service provider for HKUST campus information access.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-689"
  },
  "pargellis98_icslp": {
   "authors": [
    [
     "Andrew",
     "Pargellis"
    ],
    [
     "Qiru",
     "Zhou"
    ],
    [
     "Antoine",
     "Saad"
    ],
    [
     "Chin-Hui",
     "Lee"
    ]
   ],
   "title": "A language for creating speech applications",
   "original": "i98_0388",
   "page_count": 4,
   "order": 693,
   "p1": "paper 0388",
   "pn": "",
   "abstract": [
    "This paper describes an embedded Voice Interface Language (VIL) that enables the rapid prototyping and creation of applications requiring a voice interface. It can be integrated into popular script languages such as Perl or Tcl/Tk. Three levels of single-word commands enable the application designers to access basic speech processing technologies, such as automatic speech recognition and text-to-speech functions, without knowing details of the underlying technologies. VIL is a platform and domain independent speech application programming interface (API) that enables users to add a speech interface to their applications. The domain dependent components are defined by including a set of application specific arguments with each VIL command. Since the platform is an open architecture system, third party speech processing components may also be integrated into the platform and accessed by VIL.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-690"
  },
  "graham98_icslp": {
   "authors": [
    [
     "Robert",
     "Graham"
    ],
    [
     "Chris",
     "Carter"
    ],
    [
     "Brian",
     "Mellor"
    ]
   ],
   "title": "The use of automatic speech recognition to reduce the interference between concurrent tasks of driving and phoning",
   "original": "i98_0516",
   "page_count": 4,
   "order": 694,
   "p1": "paper 0516",
   "pn": "",
   "abstract": [
    "Previous research has found that using manually-operated mobile phones while driving significantly increases the risk of a collision. It has been suggested that automatic speech recognition (ASR) interfaces may reduce the interference between the tasks of phoning and driving. A laboratory experiment was designed to examine this hypothesis, and also to investigate the optimal design for in-car ASR systems. Forty-eight participants dialled phone numbers from memory while carrying out a concurrent tracking task. Tracking performance was found to be adversely affected while using a manual phone. This effect was significantly reduced, although not eliminated, with a speech phone. Participants also perceived the mental workload of manual dialling while driving to be greater than speech dialling. A system of audio feedback was found to be marginally preferable to combined audio plus visual feedback. The recognition accuracy of the ASR device did not appear to have any significant bearing on driving performance nor acceptance. The results are encouraging for the use of speech interfaces in the car for phone and other functions.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-691"
  },
  "hirayama98_icslp": {
   "authors": [
    [
     "Makoto J.",
     "Hirayama"
    ],
    [
     "Taro",
     "Sugahara"
    ],
    [
     "Zhiyong",
     "Peng"
    ],
    [
     "Junichi",
     "Yamazaki"
    ]
   ],
   "title": "Interactive listening to structured speech content on the internet",
   "original": "i98_1058",
   "page_count": 4,
   "order": 695,
   "p1": "paper 1058",
   "pn": "",
   "abstract": [
    "Interactive information browsing of World Wide Web is a key application of the Internet and visual web browsers are widely used to access information. However, visual web browsing is not suitable in some circumstances such as in mobile environment. Therefore, we propose interactive listening to structured speech content for accessing information. Our proposed model of the interactive information listening services is that structured audio contents (HyperAudio) in a HyperAudio server are listened using a HyperAudio player whose appearance is similar to a portable radio. Unlike radio broadcasting programs, the HyperAudio contents have logical structures and hyperlinks so that listeners can listen desired information interactively. To put such logical structures into audio, a simple markup language was used. A prototype system of the HyperAudio server and players was implemented to test and evaluate feasibility and usability of the HyperAudio architecture.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-692"
  },
  "jo98_icslp": {
   "authors": [
    [
     "Cheol-Woo",
     "Jo"
    ]
   ],
   "title": "MSF format for the representation of speech synchronized moving image",
   "original": "i98_0692",
   "page_count": 4,
   "order": 696,
   "p1": "paper 0692",
   "pn": "",
   "abstract": [
    "This paper describes the structure of a new multimedia file format. Also the procedures for implementing its encoder and the player are described. Multimedia Sound File(MSF) format reduced the size of the file. The display software is improved in the points that it requires only small sized image database compared to that of current similar programs require hugh amount of image database. This software tool can effectively display animated facial images and speech sounds together in synchronized form even at PC level. Implemented tool can be used as a plugin or an independent form. Encoder software is implemented to facilitate the production of the msf file. Files from the segmentation of speech signal into phonemic units are used as an input to the encoder.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-693"
  },
  "qvarfordt98_icslp": {
   "authors": [
    [
     "Pernilla",
     "Qvarfordt"
    ],
    [
     "Arne",
     "Jonsson"
    ]
   ],
   "title": "Effects of using speech in timetable information systems for WWW",
   "original": "i98_0477",
   "page_count": 4,
   "order": 697,
   "p1": "paper 0477",
   "pn": "",
   "abstract": [
    "Design of information systems where spatial and temporal information is merged and can be accessed using various modalities requires careful examination on how to combine the communication modalities to achieve efficient interaction. In this paper we present ongoing work on designing a multimodal interface with timetable information for local buses where the same database information can be accessed by different user categories with various information needs. The prototype interface was evaluated to investigate how speech contributes to the interaction. The results showed that the subjects used a more optimal sequence of actions when using speech, and did fewer errors. We also present suggestions for future design of multimodal interfaces.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-694"
  },
  "kemp98c_icslp": {
   "authors": [
    [
     "Thomas",
     "Kemp"
    ],
    [
     "Petra",
     "Geutner"
    ],
    [
     "Michael",
     "Schmidt"
    ],
    [
     "Borislav",
     "Tomaz"
    ],
    [
     "Manfred",
     "Weber"
    ],
    [
     "Martin",
     "Westphal"
    ],
    [
     "Alex",
     "Waibel"
    ]
   ],
   "title": "The interactive systems labs view4you video indexing system",
   "original": "i98_0759",
   "page_count": 4,
   "order": 698,
   "p1": "paper 0759",
   "pn": "",
   "abstract": [
    "The recognition of broadcast news is a challenging problem in speech recognition. To achieve the long-term goal of robust, real-time news transcription, several problems have to be overcome, e.g. the variety of acoustic conditions and the unlimited vocabulary. Recently, a number of sites have been working on content-addressable multi-media information sources. In the presented paper, we focus on extending this work towards a multi-lingual environment, where queries and multimedia documents may appear in multiple languages. In cooperation with the Informedia project at CMU, we attempt to provide cross-lingual access to German and Serbo-Croatian newscasts.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-695"
  },
  "kim98d_icslp": {
   "authors": [
    [
     "Hyung-Jin",
     "Kim"
    ],
    [
     "Lee",
     "Hetherington"
    ]
   ],
   "title": "SEMOLE: a robust framework for gathering information from the world wide web",
   "original": "i98_1076",
   "page_count": 4,
   "order": 699,
   "p1": "paper 1076",
   "pn": "",
   "abstract": [
    "This paper describes seMole (se-mantic Mole), a robust framework for harvesting information from the World Wide Web. Unlike commercially available harvesting programs that use absolute addressing, seMole uses a semantic addressing scheme to gather information from HTML pages. Instead of relying on the HTML structure to locate data, semantic addressing relies on the relative position of key/value pairs to locate data. This scheme abstracts away from the underlying HTML structure of Web pages, allowing information gathering to only depend on the content of pages, which in large part does not change over time. We use this framework to gather information from various data sources including Boston Sidewalk and the CNN Weather Site. Through these experiments we find that seMole is more robust to changes in the Web sites and it is simpler to use and maintain than systems that use absolute addressing.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-696"
  },
  "bakman98_icslp": {
   "authors": [
    [
     "Lau",
     "Bakman"
    ],
    [
     "Mads",
     "Blidegn"
    ],
    [
     "Martin",
     "Wittrup"
    ],
    [
     "Lars Bo",
     "Larsen"
    ],
    [
     "Thomas B.",
     "Moeslund"
    ]
   ],
   "title": "Enhancing a WIMP based interface with speech, gaze tracking and agents",
   "original": "i98_0766",
   "page_count": 4,
   "order": 700,
   "p1": "paper 0766",
   "pn": "",
   "abstract": [
    "This paper describes an attempt to enhance a windows based (WIMP - Windows Icon Menu Pointer) environment. The goal is to establish whether user interaction on the common desktop PC can be augmented by adding new modalities to the WIMP interface, thus bridging the gap between todays interaction patterns and future interfaces comprising e.g. advanced conversational capabilities, VR technology, etc. A user survey was carried out to establish the trouble spots of the WIMP interface on the most common desktop work station, the Windows 95 PC. On the basis of this, a number of new modalities were considered. Spoken in- and output and gaze tracking were selected together with the concept of an interface agent for further investigation. A system was developed to control the interaction of the in- and output modalities, and set of five scenarios were constructed to test the proposed ideas. In these, a number of test subjects used the existing and added modalities in various configurations.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-697"
  },
  "nakatani98_icslp": {
   "authors": [
    [
     "Christine H.",
     "Nakatani"
    ],
    [
     "Steve",
     "Whittaker"
    ],
    [
     "Julia",
     "Hirschberg"
    ]
   ],
   "title": "Now you hear it, now you don't: empirical studies of audio browsing behavior behavior",
   "original": "i98_1003",
   "page_count": 4,
   "order": 701,
   "p1": "paper 1003",
   "pn": "",
   "abstract": [
    "We present several studies that investigate how people use audio documents and uncover new principles for designing audio navigation technology. In particular, we report on an ethnographic study of voicemail users, exploring the behaviors and needs of users of current voicemail technology. To constrain design choices for better technology, we then study how people navigate through audio and how they perform basic information processing tasks on a voicemail corpus. Observations and analyses from the user experiments lead to new principles of design for audio document interfaces, which are embodied in a prototype structural audio browser. Specifically, we conclude that the reinforcement of audio memory and appropriate definition of content-based playback units are important properties of interfaces suited to human audio processing behaviors.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-698"
  },
  "qiao98_icslp": {
   "authors": [
    [
     "Rongyu",
     "Qiao"
    ],
    [
     "Youngkyu",
     "Choi"
    ],
    [
     "Johnson I.",
     "Agbinya"
    ]
   ],
   "title": "A voice verifier for face/voice based person verification system",
   "original": "i98_0307",
   "page_count": 4,
   "order": 702,
   "p1": "paper 0307",
   "pn": "",
   "abstract": [
    "A person verification system based on voice and facial images has been developed within CSIRO Telecommunications and Industrial Physics, Australia, for use in low-to-medium security systems. It provides a unique ID, which is non-intrusive, fast, and has no need for memorising passwords. A stand-alone version of the voice verifier has an error rate of less than 8%, while the face verifier has an error rate of less than 5%. By combining the two modules, an error rate of less than 1% is achieved. This paper describes in detail the method and some of the important practical issues in the implementation of the voice verifier. It also addresses the issue of decision making if the two sub-systems produce contradictory results.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-699"
  },
  "robertribes98_icslp": {
   "authors": [
    [
     "Jordi",
     "Robert-Ribes"
    ]
   ],
   "title": "On the use of automatic speech recognition for TV captioning",
   "original": "i98_0621",
   "page_count": 4,
   "order": 703,
   "p1": "paper 0621",
   "pn": "",
   "abstract": [
    "This study analyses the possible use of automatic speech recognition (ASR) for the automatic captioning of TV programs. Captioning requires: (1) transcribing the spoken words and (2) determining the times at which the caption has to appear and disappear on the screen. These times have to match as closely as possible the corresponding times on the audio signal. Automatic speech recognition can be used to determine both aspects: the spoken words and their times. This paper focuses on the question: would perfect automatic speech recognition systems be able to automate the captioning process? We present quantitative data on the discrepancy between the audio signal and the manually generated captions. We show how ASR alone can even lower the efficiency of captioning. The techniques needed to automate the captioning process are presented.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-700"
  },
  "serridge98_icslp": {
   "authors": [
    [
     "Ben",
     "Serridge"
    ]
   ],
   "title": "An undergraduate course on speech recognition based on the CSLU toolkit",
   "original": "i98_0925",
   "page_count": 3,
   "order": 704,
   "p1": "paper 0925",
   "pn": "",
   "abstract": [
    "This paper describes an undergraduate course in speech recognition, based on the CSLU Toolkit, which was taught at the Universidad de las Americas in Puebla, Mexico. Throughout the course, laboratory assignments based on the toolkit guided students through the process of creating a recognizer, while in-class lectures consistently referred to the architecture of the toolkit as a concrete example of an existing system. The class was organized so that lectures and laboratory assignments followed the steps taken in the creation of a new recognizer. The students first recorded and labeled their own corpus, then proceeded to design and train neural network based recognizers, before finally testing for performance and creating sample applications. As a final project, students performed simple, well-defined experiments using the recognizers they had constructed. The CSLU Toolkit is freely available for non-commercial use from http://cslu.cse.ogi.edu/. In the future, similar courses based on the toolkit could be created and shared by many researchers in the speech community via the world-wide web.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-701"
  },
  "yang98f_icslp": {
   "authors": [
    [
     "Ping-Fai",
     "Yang"
    ],
    [
     "Yannis",
     "Stylianou"
    ]
   ],
   "title": "Real time voice alteration based on linear prediction",
   "original": "i98_0814",
   "page_count": 4,
   "order": 705,
   "p1": "paper 0814",
   "pn": "",
   "abstract": [
    "In this paper we present the application of a set of voice alteration algorithms based on Linear Prediction (LP). We present a study of some potential application areas of voice alteration technology and argue that near real time performance is a critical requirement for many. One benefit of our algorithms is their simplicity and therefore feasibility of implementation in real time system. To this end, we built an experimental platform on a personal computer. We also present our implementation and user experience from this effort.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-702"
  },
  "tan98_icslp": {
   "authors": [
    [
     "Beng Tiong",
     "Tan"
    ],
    [
     "Yong",
     "Gu"
    ],
    [
     "Trevor",
     "Thomas"
    ]
   ],
   "title": "Evaluation and implementation of a voice-activated dialing system with utterance verification",
   "original": "i98_0480",
   "page_count": 4,
   "order": 706,
   "p1": "paper 0480",
   "pn": "",
   "abstract": [
    "This study investigates the utterance verification (UV) algorithm for a voice-activated dialing (VAD) system. The UV techniques help to improve the system accuracy of a VAD system and to improve the efficiency of user interface by reducing the need of confirmation. In this paper, we examine various UV methods, namely, all-phone garbage model (GM), N-best likelihood ratio (NBLR), and the combined methods. The performances of a VAD system with UV at various vocabulary sizes are studied. By rejecting 9.5% of correctly recognized names, the system error rate become less than 0.3% which represent a reduction of 91% in error rate over the baseline system. The UV technique can reduce the number of confirmation by at least 88% with a system error rate of 0.28%.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-703"
  },
  "wang98e_icslp": {
   "authors": [
    [
     "Hsin-Min",
     "Wang"
    ],
    [
     "Bor-Shen",
     "Lin"
    ],
    [
     "Berlin",
     "Chen"
    ],
    [
     "Bo-Ren",
     "Bai"
    ]
   ],
   "title": "Towards a Mandarin voice memo system",
   "original": "i98_0192",
   "page_count": 4,
   "order": 707,
   "p1": "paper 0192",
   "pn": "",
   "abstract": [
    "Using voice memos in stead of text memos is believed to be more natural, convenient, and attractive. This paper presents a working Mandarin voice memo system that provides functions of automatic notification and voice retrieval. The main techniques include the content-based spoken document retrieval approach and the date-time expression detection and understanding approach. Extensive preliminary experiments were performed and encouraging results were demonstrated.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-704"
  },
  "shinozaki98_icslp": {
   "authors": [
    [
     "Tsubasa",
     "Shinozaki"
    ],
    [
     "Masanobu",
     "Abe"
    ]
   ],
   "title": "Development of CAI system employing synthesized speech responses",
   "original": "i98_0409",
   "page_count": 4,
   "order": 708,
   "p1": "paper 0409",
   "pn": "",
   "abstract": [
    "This paper proposes a Computer Assisted Instruction (CAI) system that teaches students how to write Japanese characters. The most important feature of the system is the usage of synthesized speech to interact with users. The CAI system has a video display tablet interface. A user traces a pattern of a character using the tablet pen, and simultaneously his tracing is shown on the display. When the trace line is outside the pattern, the system simultaneously outputs synthesized speech to correct the errors. To design strategies for generating instructions, behavior and instruction messages of a human teacher were recorded and analyzed. One of the most interesting challenges of the system is a function that changes the \"personality\" of the teacher, such as a strict teacher, a friendly teacher, and a short-tempered teacher. According to the experimental results, it was confirmed that the proposed system makes it possible to convey a particular impression using synthesized speech.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-705"
  },
  "kellner98_icslp": {
   "authors": [
    [
     "Andreas",
     "Kellner"
    ],
    [
     "Bernhard",
     "Rueber"
    ],
    [
     "Hauke",
     "Schramm"
    ]
   ],
   "title": "Using combined decisions and confidence measures for name recognition in automatic directory assistance systems",
   "original": "i98_0454",
   "page_count": 4,
   "order": 709,
   "p1": "paper 0454",
   "pn": "",
   "abstract": [
    "Directory assistance systems are amongst the most challenging applications of speech recognition. Today, complete automation of the service fails because of the lacking accuracy of current speech recognizers, which are simply not able to differentiate between hundreds of thousands or even millions of different names occurring in large cities. In this paper, we show that this situation can be remedied by systematically combining all available knowledge sources (last names, first names, street names, partly including their spelled versions) in a statistically optimal way. Especially designed confidence measures for N-best lists are proposed to detect misrecognized turns. Applying these techniques in a hierarchical setup is judged as the enabling step for automating large scale directory assistance. In first experiments, we e.g. are able to service 72% of the inquiries for a database of 1.3 million entries with a remaining error rate of only 6% (or 62% with an error rate of 2%).\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-706"
  },
  "buntschuh98_icslp": {
   "authors": [
    [
     "Bruce",
     "Buntschuh"
    ],
    [
     "Candace A.",
     "Kamm"
    ],
    [
     "Giuseppe Di",
     "Fabbrizio"
    ],
    [
     "Alicia",
     "Abella"
    ],
    [
     "Mehryar",
     "Mohri"
    ],
    [
     "Shrikanth",
     "Narayanan"
    ],
    [
     "I.",
     "Zeljkovic"
    ],
    [
     "R.D.",
     "Sharp"
    ],
    [
     "Jeremy H.",
     "Wright"
    ],
    [
     "S.",
     "Marcus"
    ],
    [
     "J.",
     "Shaffer"
    ],
    [
     "R.",
     "Duncan"
    ],
    [
     "J.G.",
     "Wilpon"
    ]
   ],
   "title": "VPQ: a spoken language interface to large scale directory information",
   "original": "i98_0877",
   "page_count": 4,
   "order": 710,
   "p1": "paper 0877",
   "pn": "",
   "abstract": [
    "VPQ (Voice Post Query) is a dialog system for spoken access to information in the AT&T personnel database (>120,000 entries). An explicit design goal is for the initial interaction with the system to be rather unconstrained and to rely on tighter, prompt-constrained, dialog only when absolutely necessary. The purpose of VPQ is to explore and exploit the capabilities of ``state-of-the-art'' speech recognition systems for this high-perplexity task, and to develop the natural language understanding and dialog control components necessary for effective and efficient user interactions. The VPQ task includes simple interactions, where the initial request is unambiguous and the system's response provides the desired information or completes a call to the requested person, as well as more complex interactions where ambiguities or errors require dialog-driven resolution. This paper highlights the inherent challenges in this task, the major components of the system, the rationale for their design, and how they perform.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-707"
  },
  "choi98b_icslp": {
   "authors": [
    [
     "John",
     "Choi"
    ],
    [
     "Don",
     "Hindle"
    ],
    [
     "Julia",
     "Hirschberg"
    ],
    [
     "Ivan",
     "Magrin-Chagnolleau"
    ],
    [
     "Christine H.",
     "Nakatani"
    ],
    [
     "Fernando",
     "Pereira"
    ],
    [
     "Amit",
     "Singhal"
    ],
    [
     "Steve",
     "Whittaker"
    ]
   ],
   "title": "SCAN - speech content based audio navigator: a system overview",
   "original": "i98_0604",
   "page_count": 4,
   "order": 711,
   "p1": "paper 0604",
   "pn": "",
   "abstract": [
    "SCAN (Speech Content based Audio Navigator) is a spoken document retrieval system integrating speaker-independent, large-vocabulary speech recognition with information-retrieval to support query-based retrieval of information from speech archives. Initial development focused on the application of SCAN to the broadcast news domain. This paper provides an overview of this system, including a description of its graphical user interface which incorporates machine-generated speech transcripts to provide local contextual navigation and random access for browsing large speech databases.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-708"
  },
  "ferreiros98b_icslp": {
   "authors": [
    [
     "Javier",
     "Ferreiros"
    ],
    [
     "Jos√©",
     "Col√°s"
    ],
    [
     "Javier",
     "Macias-Guarasa"
    ],
    [
     "Alejandro",
     "Ruiz"
    ],
    [
     "Jos√© Manuel",
     "Pardo"
    ]
   ],
   "title": "Controlling a HIFI with a continuous speech understanding system",
   "original": "i98_0988",
   "page_count": 4,
   "order": 712,
   "p1": "paper 0988",
   "pn": "",
   "abstract": [
    "In this paper we present a speech understanding system that accepts continuous speech sentences as input to command a HIFI set. The string of words obtained from the recogniser is sent to the understanding system that tries to fill in a set of frames specifying the triplet (SUBSYSTEM, PARAMETER, VALUE). All circumstances (understanding incompleteness, HIFI set status, result of the command execution) are confirmed back to the user via a text to speech system with substitutable-concept pattern-based generated messages. The understanding engine is based on semantic-like tagging, including \"garbage\" tag, and context-dependent rules for meaning extraction. The system allows the application developer to follow the reasoning process (as every understanding rule has an associated concept pattern), spoken by the speech generation module. The concepts for speech generation are randomly substituted with alternative expressions having the same meaning to achieve a certain degree of naturalness in the response speech.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-709"
  },
  "lamel98_icslp": {
   "authors": [
    [
     "Lori F.",
     "Lamel"
    ],
    [
     "Samir",
     "Bennacef"
    ],
    [
     "Jean-Luc",
     "Gauvain"
    ],
    [
     "Herv√©",
     "Dartigues"
    ],
    [
     "Jean-Noel",
     "Temem"
    ]
   ],
   "title": "User evaluation of the mask kiosk",
   "original": "i98_0085",
   "page_count": 4,
   "order": 713,
   "p1": "paper 0085",
   "pn": "",
   "abstract": [
    "In this paper we report on a series of user trials carried out to assess the performance and usability of the MASK prototype kiosk. The aim of the ESPRIT Multimodal Multimedia Service Kiosk (MASK) project was to pave the way for more advanced public service applications with user interfaces employing multimodal, multi-media input and output. The prototype kiosk, was developed after analysis of the technological requirements in the context of users and the tasks they perform in carrying out travel enquiries, in close collaboration with the French Railways (SNCF) and the Ergonomics group at UCL. The time to complete the transaction with the MASK kiosk is reduced by about 30% compared to that required for the standard kiosk, and the success rate is 85% for novices and 94% once familiar with the system. In addition to meeting or exceeding the performance goals set at the project onset in terms of success rate, transaction time, and user satisfaction, the MASK kiosk was judged to be user-friendly and simple to use.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-710"
  },
  "bernsen98_icslp": {
   "authors": [
    [
     "Niels Ole",
     "Bernsen"
    ],
    [
     "Laila",
     "Dybkjaer"
    ]
   ],
   "title": "Is speech the right thing for your application?",
   "original": "i98_0062",
   "page_count": 4,
   "order": 714,
   "p1": "paper 0062",
   "pn": "",
   "abstract": [
    "Use of speech input to, and speech output from, computer systems is spreading at a growing pace. This means that, increasingly, developers of systems and interfaces are faced with the question of whether or not to use speech input and/or speech output for the applications they are about to build. This paper presents results from a pilot test of a theory-based approach to speech functionality. The test uses a corpus of claims about speech functionality derived from recent literature on speech and multimodality.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-711"
  },
  "godinollorente98_icslp": {
   "authors": [
    [
     "Juan Ignacio",
     "Godino Llorente"
    ],
    [
     "Santiago",
     "Aguilera Navarro"
    ],
    [
     "Sira",
     "Palazuelos Cagigas"
    ],
    [
     "Alberto",
     "Nieto Altuzarra"
    ],
    [
     "Pedro",
     "Gomez Vilda"
    ]
   ],
   "title": "A PC-based tool for helping in diagnosis of pathologic voice",
   "original": "i98_0558",
   "page_count": 4,
   "order": 715,
   "p1": "paper 0558",
   "pn": "",
   "abstract": [
    "We present a diagnosis tool for the voice clinic which runs on a personal computer. The application records different registers in real time. The signals to be captured and stored are the following: * Videoendoscopic images recorded with fibroscope or telelaringoscope. * Electroglottographic signal during fonation * Voice signal * Air flow signal All these different signals are recorded with specific transducers and standard digitalisation signal boards, using microphone input and input line simultaneously. Several systems to help in the diagnostic have been previously developed, but none of them captures the four mentioned signals simultaneously. All of them are highly interesting from the clinical point of view, and assist the expert in making the decision. The main advantage, related to other systems, is that looking at the videoendoscopic record, clinicians are able to label voice registers with the associated pathologies.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-712"
  },
  "sjolander98_icslp": {
   "authors": [
    [
     "Kaare",
     "Sj√∂lander"
    ],
    [
     "Jonas",
     "Beskow"
    ],
    [
     "Joakim",
     "Gustafson"
    ],
    [
     "Erland",
     "Lewin"
    ],
    [
     "Rolf",
     "Carlson"
    ],
    [
     "Bj√∂rn",
     "Granstr√∂m"
    ]
   ],
   "title": "Web-based educational tools for speech technology",
   "original": "i98_0361",
   "page_count": 4,
   "order": 716,
   "p1": "paper 0361",
   "pn": "",
   "abstract": [
    "This paper describes the efforts at KTH in creating educational tools for speech technology. The demand for such tools is increasing with the advent of speech as a medium for man-machine communication. The world wide web was chosen as our platform in order to increase the usability and accessibility of our computer exercises. The aim was to provide dedicated educational software instead of exercises based on complex research tools. Currently, the set of exercises comprises basic speech analysis, multi-modal speech synthesis and spoken dialogue systems. Students access web pages in which the exercises have been embedded as applets. This makes it possible to use them in a classroom setting, as well as from the students? home computers.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-713"
  },
  "sutton98_icslp": {
   "authors": [
    [
     "Stephen",
     "Sutton"
    ],
    [
     "Ronald A.",
     "Cole"
    ],
    [
     "Jacques de",
     "Villiers"
    ],
    [
     "Johan",
     "Schalkwyk"
    ],
    [
     "Pieter",
     "Vermeulen"
    ],
    [
     "Michael W.",
     "Macon"
    ],
    [
     "Yonghong",
     "Yan"
    ],
    [
     "Edward",
     "Kaiser"
    ],
    [
     "Brian",
     "Rundle"
    ],
    [
     "Khaldoun",
     "Shobaki"
    ],
    [
     "John-Paul",
     "Hosom"
    ],
    [
     "Alex",
     "Kain"
    ],
    [
     "Johan",
     "Wouters"
    ],
    [
     "Dominic W.",
     "Massaro"
    ],
    [
     "Michael",
     "Cohen"
    ]
   ],
   "title": "Universal speech tools: the CSLU toolkit",
   "original": "i98_0649",
   "page_count": 4,
   "order": 717,
   "p1": "paper 0649",
   "pn": "",
   "abstract": [
    "A set of freely available, universal speech tools is needed to accelerate progress in the speech technology. The CSLU Toolkit represents an effort to make core technology and fundamental infrastructure accessible, affordable and easy to use. The CSLU Toolkit has been under development for five years. This paper describes recent improvements, additions and uses of the CSLU Toolkit.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-714"
  },
  "serridge98b_icslp": {
   "authors": [
    [
     "Ben",
     "Serridge"
    ],
    [
     "Alejandro",
     "Barbosa"
    ],
    [
     "Ronald A.",
     "Cole"
    ],
    [
     "Nora",
     "Munive"
    ],
    [
     "Alcira",
     "Vargas"
    ]
   ],
   "title": "Creating a mexican Spanish version of the CSLU toolkit",
   "original": "i98_0923",
   "page_count": 4,
   "order": 718,
   "p1": "paper 0923",
   "pn": "",
   "abstract": [
    "The CSLU Toolkit is designed to facilitate the rapid development of spoken dialogue systems for a wide variety of applications, as well as to provide a framework for conducting research in the underlying speech technologies. This paper describes the creation of a Mexican Spanish version of the CSLU Toolkit (both synthesis and recognition) undertaken at the Universidad de las Americas in Puebla, Mexico. Based on the Festival Speech Synthesis System of the University of Edinburgh, we have developed a complete concatenative text-to-speech system for Mexican Spanish, which is currently incorporated into the toolkit and includes both a male and female voice. In the area of recognition, we have created a set of task-specific Spanish recognizers for continuous digits, spelled words, and yes/no phrases, as well as a \"general-purpose\" phonetic recognizer suitable for arbitrary sub-domains. Using the Rapid Application Developer (RAD) component of the CSLU Toolkit, it is now possible to quickly prototype spoken dialogue systems in Spanish. The Spanish components of the CSLU Toolkit are freely available for non-commercial use from the following web page: http://info.pue.udlap.mx/~sistemas/tlatoa.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-715"
  },
  "garciamateo98_icslp": {
   "authors": [
    [
     "Carmen",
     "Garc√≠a-Mateo"
    ],
    [
     "Qiru",
     "Zhou"
    ],
    [
     "Chin-Hui",
     "Lee"
    ],
    [
     "Andrew",
     "Pargellis"
    ]
   ],
   "title": "A voice user interface demonstration system for mexican Spanish",
   "original": "i98_0884",
   "page_count": 4,
   "order": 719,
   "p1": "paper 0884",
   "pn": "",
   "abstract": [
    "We present a Mexican Spanish voice user interface demonstration system. It was built on a speech research platform developed at Bell Labs, which provides major speech technology and interface components, including automatic speech recognition, text-to-speech synthesis, audio input/output functions and telephone interface. The application is written in the PERL script language with an embedded Voice Interface Language (VIL) that connects the speech and interface modules to PERL. Given the set of multilingual speech processing capabilities on the platform and the VIL, we were able to quickly develop a Mexican Spanish system using PERL with speech-enabled messaging and information access functionality similar to our English voice user interface demonstration system.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-716"
  },
  "minagawakawai98_icslp": {
   "authors": [
    [
     "Yasuyo",
     "Minagawa-Kawai"
    ],
    [
     "Shigeru",
     "Kiritani"
    ]
   ],
   "title": "Non-native productions of Japanese single stops that are too long for one mora unit",
   "original": "i98_1025",
   "page_count": 4,
   "order": 720,
   "p1": "paper 1025",
   "pn": "",
   "abstract": [
    "The difficulty for non-native speakers in producing Japanese geminate stops with a long enough closure has been pointed out and extensively studied. However, the reverse problem exists for particular language speakers such as Chinese. For these speakers, production of Japanese intervocalic single stops sound like geminate stops. This study aimed to show acoustic evidence of this problem in the production of Japanese voiceless stops by learners of Japanese, and to compare speakers from different language groups, English, Chinese and French speakers. The results of the experiment indicated that since some of the stop productions by Americans and Chinese were too long for one mora unit, the Japanese heard a geminate (2 mora) instead of a single. Because French subjects had a small difference in the stop durations from the Japanese value, only a few of their productions of single stops were perceived as geminates. The language differences in timing pattern for Japanese stops were interpreted to reflect differences in timing implementation of native languages that is stress-timed and syllable-timed, at least concerning the difference between English and French speakers.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-717"
  },
  "yamada98b_icslp": {
   "authors": [
    [
     "Nobuko",
     "Yamada"
    ]
   ],
   "title": "The process of generation and development of second language Japanese accentuation",
   "original": "i98_1056",
   "page_count": 4,
   "order": 721,
   "p1": "paper 1056",
   "pn": "",
   "abstract": [
    "This study will investigate how non-native speakers of Japanese acquire Japanese accentuation from the viewpoint of the location of the accent nucleus. Hypothetical models for the process of generation and for developmental sequence of interlanguage Japanese accentuation, which is interim accentual system created by learners, will be proposed. The subjects appear to generate their interlanguage as the results of application of strategies or examples of accentuation. Those seem to be discovered from L2 input, or chosen and fetch from their memory. The subjects' competence of accentuation appear to be developed by L2 input, starting with L1 and universal property. They seem to discover and apply 5 types of strategies toward acquisition of target accentual rules of Japanese.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-718"
  },
  "funatsu98_icslp": {
   "authors": [
    [
     "Seiya",
     "Funatsu"
    ],
    [
     "Shigeru",
     "Kiritani"
    ]
   ],
   "title": "Perceptual properties of Russians with Japanese fricatives",
   "original": "i98_0809",
   "page_count": 4,
   "order": 722,
   "p1": "paper 0809",
   "pn": "",
   "abstract": [
    "This study investigated the perceptual properties of second language learners in acquiring second language phonemes. The case where the relation between two phonemes of a second language and those of a native language changes according to following vowels was studied. The perceptual properties of Russians with regards to Japanese fricatives were examined. In the perception test, the confusion of [(omitted)o] with [so] was very large. This phenomenon could be caused by the difference between the transition onset time from [s'] to vowels and that from the other consonants to vowels. It is considered that, in the case of following vowel [a] and [o], Russians equated Japanese [s] and [(omitted)] with Russian [s] and [s'] respectively. However, in the case of [u], they did not equate them in such a manner. This is probably because the acoustic properties of Japanese [(omitted)] are very different from those of Russian [u].\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-719"
  },
  "cucchiarini98_icslp": {
   "authors": [
    [
     "Catia",
     "Cucchiarini"
    ],
    [
     "Febe De",
     "Wet"
    ],
    [
     "Helmer",
     "Strik"
    ],
    [
     "Louis",
     "Boves"
    ]
   ],
   "title": "Assessment of dutch pronunciation by means of automatic speech recognition technology",
   "original": "i98_0751",
   "page_count": 4,
   "order": 723,
   "p1": "paper 0751",
   "pn": "",
   "abstract": [
    "Experiments were carried out to determine whether log-likelihood ratios (LRs) can be employed to improve automatic assessment of Dutch pronunciation. Read speech of natives and non-natives was judged by three groups of expert raters and was then analyzed by means of a continuous speech recognizer. Three automatic measures were calculated, two LRs and rate of speech (ros), and then compared with the expert ratings. It appears that expert ratings of pronunciation quality can accurately be predicted on the basis of ros alone and that LRs do not contribute to better prediction. However, LRs can be useful to automatic pronunciation assessment because they can help detect fast speakers who produce totally wrong sentences.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-720"
  },
  "langlais98_icslp": {
   "authors": [
    [
     "Philippe",
     "Langlais"
    ],
    [
     "Anne-Marie",
     "√ñster"
    ],
    [
     "Bj√∂rn",
     "Granstr√∂m"
    ]
   ],
   "title": "Phonetic-level mispronunciation detection in non-native Swedish speech",
   "original": "i98_0311",
   "page_count": 4,
   "order": 724,
   "p1": "paper 0311",
   "pn": "",
   "abstract": [
    "This contribution presents part of the work initiated at the CTT for the development of speech technology to assist non-native speakers learn Swedish. This study focuses mainly on the automatic location of mispronunciations at a phonetic level. We first describe the database we created for this work and then report on the reliability of several phonetic scores to automatically locate segmental problems in student utterances.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-721"
  },
  "akahaneyamada98_icslp": {
   "authors": [
    [
     "Reiko",
     "Akahane-Yamada"
    ],
    [
     "Erik",
     "McDermott"
    ],
    [
     "Takahiro",
     "Adachi"
    ],
    [
     "Hideki",
     "Kawahara"
    ],
    [
     "John S.",
     "Pruitt"
    ]
   ],
   "title": "Computer-based second language production training by using spectrographic representation and HMM-based speech recognition scores",
   "original": "i98_0429",
   "page_count": 4,
   "order": 725,
   "p1": "paper 0429",
   "pn": "",
   "abstract": [
    "How can we provide feedback to second language (L2) learners about the goodness of their productions in an automatic way? In this paper, we introduce our attempts to provide effective feedback when we train native speakers of Japanese to produce English /r/ and /l/. First, we adopted spectrographic representation overlayed with formant frequencies as feedback. Second, we investigated the correlation between human judgments of L2 production quality and acoustic scores produced by an HMM-based speech recognition system. We also adopted the HMM-based scores as feedback in the production training. Evaluation of the pre- and post-training productions by human judges showed that production abilities of the trainees improved in both training groups, suggesting that both spectrographic representation and HMM-based scores were useful and meaningful as feedback. These results are discussed in the context of optimizing L2 speech training.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-722"
  },
  "hardison98_icslp": {
   "authors": [
    [
     "Debra M.",
     "Hardison"
    ]
   ],
   "title": "Spoken word identification by native and nonnative speakers of English: effects of training, modality, context and phonetic environment",
   "original": "i98_0120",
   "page_count": 4,
   "order": 726,
   "p1": "paper 0120",
   "pn": "",
   "abstract": [
    "Several experiments explored the contribution of visual information (lip movements) to spoken word identification by Japanese and Korean learners of English, and native speakers (NSs), and its interaction with sentence context, phonetic environment and, for learners, perceptual training (involving /r,l,p,f,theta,s/ using minimal pairs). The gating technique was applied to videotaped stimuli presented audiovisually (AV) or audio(A)-only to both groups. Stimuli were familiar, bisyllabic words beginning with the following visual categories: bilabial (/p/), labiodental (/f/), /r/, /l/, and nonlabials (/s,t,k/) combined with high, low and rounded vowels. Test (pretest--posttest), initial consonant-vowel (CV) sequence, modality of presentation (AV vs. A), and condition (context or excised word) were independent variables. Groups of NSs were presented with the same stimuli. NNS results revealed word identification was significantly earlier after perceptual training, in AV vs. A-only presentation, in context vs. excised word condition, and varied significantly with initial CV sequence. NS results also revealed significant effects of modality (earlier identification in AV vs. A-only), context, and initial CV sequence. Findings indicate the transfer of perceptual training from segment identification to the process of word identification in connected speech, and are consistent with a multiple-trace model of spoken language processing incorporating visual input.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-723"
  },
  "tyler98_icslp": {
   "authors": [
    [
     "Michael D.",
     "Tyler"
    ]
   ],
   "title": "The effect of background knowledge on first and second language comprehension difficulty",
   "original": "i98_0833",
   "page_count": 4,
   "order": 727,
   "p1": "paper 0833",
   "pn": "",
   "abstract": [
    "First and proficient second language users listened to a passage while concurrently performing a calculation verification task. The number of correct calculations achieved in the dual-task was compared to a single-task condition to index difficulty of language comprehension. Access to background knowledge was manipulated between participants by the presentation of a topic sentence. The difference between first and second language comprehension difficulty was greater when background knowledge was unavailable than when it was available. As each participant relied solely upon information from the speech signal for comprehension when the topic of the passage was not provided, it was concluded that the processes involved in decoding the speech signal generally consume more resources in second language than first language users.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-724"
  },
  "tsukada98b_icslp": {
   "authors": [
    [
     "Kimiko",
     "Tsukada"
    ]
   ],
   "title": "Comparison of cross-language coarticulation: English, Japanese and Japanese-accented English",
   "original": "i98_0096",
   "page_count": 4,
   "order": 728,
   "p1": "paper 0096",
   "pn": "",
   "abstract": [
    "This study investigates the cross-language coarticulation patterns in Australian English and Japanese. F2 trajectories between the vowel target and vowel onset/offset in the context of /d/ were plotted and locus equations were fitted to the datapoints to capture the degree of coarticulation. Three talker groups were considered: native talkers of Australian English (AE), L2 English talkers (L1 Japanese, hence, JE) and native Japanese talkers (J). Two native groups AE and J clearly showed different coarticulation patterns between the alveolar stop and the adjacent vowels. There was some suggestion that /d/ is most resistant to coarticulation in AE, least in J, while JE talkers produced intermediate coarticulation values. The deviation of JE talkers' F2 trajectories from the AE group appeared more pronounced at the vowel offset than at the onset. Differences in coarticulatory patterns between native and L2 talkers may contribute to the perception of 'foreign (Japanese) accent' in the JE production.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-725"
  },
  "imaizumi98_icslp": {
   "authors": [
    [
     "Satoshi",
     "Imaizumi"
    ],
    [
     "Hidemi",
     "Itoh"
    ],
    [
     "Yuji",
     "Tamekawa"
    ],
    [
     "Toshisada",
     "Deguchi"
    ],
    [
     "Koichi",
     "Mori"
    ]
   ],
   "title": "Plasticity of non-native phonetic perception and production: a training study",
   "original": "i98_0432",
   "page_count": 4,
   "order": 729,
   "p1": "paper 0432",
   "pn": "",
   "abstract": [
    "Audiovisual perceptual training of non-native phonetic contrasts was conducted for 10 naive Japanese adults using audiovisual recordings of 13 native English speakers articulating 90 rl minimal word pairs, and analyzed changes in perceptual and articulatory representations of non-native phonetic contrasts. The speech identification score drastically improved during the training. The improvement in non-native rl perceptual distinction was clearly associated with the changes in the perceptual and articulatory representations, which represents perceptual/ articulatory dissimilarities between the non-native and native phonemes as maps created using a multi-dimensional scaling analysis (MDS). Results suggested that the new non-native phonetic categories can be acquired through proper training even in adulthood so that distances among exemplars within each of the acquired categories shrunk and distances between the categories stretch considerably compared to those of pretraining stage in the perceptual and articulatory representations.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-726"
  },
  "watson98_icslp": {
   "authors": [
    [
     "Ian",
     "Watson"
    ]
   ],
   "title": "The relation between perceptual and production categories in acquisition",
   "original": "i98_1085",
   "page_count": 4,
   "order": 730,
   "p1": "paper 1085",
   "pn": "",
   "abstract": [
    "Results from two experiments on acquisition of the word initial voicing contrast are compared, one on perception, one on production. Both involved monolingual and bilingual (French/English) subjects. Monolinguals are shown to have frequent disparities between their use of parameters in production and perception. Furthermore, bilingualism is found to affect subjects' production and their perception differentially, in a way which suggests that there cannot be isomorphism between their production and perceptual representations. However, the ongoing interference between the phonetic categories in bilinguals' two languages also argues that at the phonological level there is identity between cognates in their two languages. This pattern of phonological identity but phonetic difference is taken as support for models of speech processing in which distinct representations are attributed to phonology and phonetics, and to production and perception.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-727"
  },
  "hazan98b_icslp": {
   "authors": [
    [
     "Valerie",
     "Hazan"
    ],
    [
     "Sarah",
     "Barrett"
    ]
   ],
   "title": "The development of perceptual cue-weighting in children aged 6 to 12",
   "original": "i98_0488",
   "page_count": 4,
   "order": 731,
   "p1": "paper 0488",
   "pn": "",
   "abstract": [
    "Development in the ability to categorise a range of phonemic contrasts was examined in 85 children aged 6 to 12 and thirteen adults using synthetic continua presented in identification tests. This study aimed to investigate: (1) whether there would be age effects in the sharpness of categorisation; (2) whether sharpness of categorisation would differ across phonemic contrasts and (3) whether children aged 6 or more would give greater perceptual weighting than adults to acoustic cues characterised by rapid spectral change. Significant differences in identification function gradients were found between the 6;0-7;6 year olds and children aged 9;6 to 12;6, and between all children and adults. Sharper identification function gradients were obtained for stop place and voicing contrasts than for fricative place and voicing contrasts. There was no strong support for the view that children aged 6 or older gave greater weight to dynamic cues than adults.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-728"
  },
  "cutler98b_icslp": {
   "authors": [
    [
     "Anne",
     "Cutler"
    ],
    [
     "Takashi",
     "Otake"
    ]
   ],
   "title": "Assimilation of place in Japanese and dutch",
   "original": "i98_0093",
   "page_count": 4,
   "order": 732,
   "p1": "paper 0093",
   "pn": "",
   "abstract": [
    "Assimilation of place of articulation across a nasal and a following stop consonant is obligatory in Japanese, but not in Dutch. In four experiments the processing of assimilated forms by speakers of Japanese and Dutch was compared, using a task in which listeners blended pseudo-word pairs such as ranga-serupa. An assimilated blend of this pair would be rampa, an unassimilated blend rangpa. Japanese listeners produced significantly more assimilated than unassimilated forms, both with pseudo-Japanese and pseudo-Dutch materials, while Dutch listeners produced significantly more unassimilated than assimilated forms in each materials set. This suggests that Japanese listeners, whose native-language phonology involves obligatory assimilation constraints, represent the assimilated nasals in nasal-stop sequences as unmarked for place of articulation, while Dutch listeners, who are accustomed to hearing unassimilated forms, represent the same nasal segments as marked for place of articulation.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-729"
  },
  "kondo98_icslp": {
   "authors": [
    [
     "Yuko",
     "Kondo"
    ],
    [
     "Yumiko",
     "Arai"
    ]
   ],
   "title": "Prosodic constraint on v-to-v coarticulation in Japanese",
   "original": "i98_0186",
   "page_count": 4,
   "order": 733,
   "p1": "paper 0186",
   "pn": "",
   "abstract": [
    "Coarticulation is considered to be constrained by language specific grammar. In particular, prosodic units, which have an important function in organizing speech, are likely to affect the extent of coarticulation. The present study addresses the question of the relationship between V-to-V coarticulation and bimoraic foot in Japanese. It was shown that the foot boundary constrains the extent of coarticulation. It was also shown that both within- and across-foot, anticipatory effects are stronger than carryover effects in Japanese.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-730"
  },
  "cucchiarini98b_icslp": {
   "authors": [
    [
     "Catia",
     "Cucchiarini"
    ],
    [
     "Henk van den",
     "Heuvel"
    ]
   ],
   "title": "Postvocalic /r/-deletion in standard dutch: how experimental phonology can profit from ASR technology",
   "original": "i98_0753",
   "page_count": 4,
   "order": 734,
   "p1": "paper 0753",
   "pn": "",
   "abstract": [
    "In this study automatic speech recognition (ASR) techniques were used to substantiate the findings of phonological research on postvocalic /r/-deletion in Standard Dutch. A database containing spontaneous speech utterances stemming from man-machine interactions in an automatic train-table inquiry system was used for this purpose. Pronunciation variants with and without /r/ were automatically generated on the basis of our specification of the phonological rule of /r/-deletion and were then included in the lexicon of a continuous speech recognizer (CSR) which was used in forced recognition mode. The results show that in a corpus containing 214,102 words, in which /r/-deletion could be applied 16,865 times, it was actually applied in 47.6% of the cases. This is a high percentage of occurrence for a phenomenon that has so sporadically been described. Furthermore, the results substantiate our rule specification: /r/-deletion occurs more often after schwa than after any other vowel.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-731"
  },
  "hajek98_icslp": {
   "authors": [
    [
     "John",
     "Hajek"
    ],
    [
     "Ian",
     "Watson"
    ]
   ],
   "title": "More evidence for the perceptual basis of sound change? suprasegmental effects in the development of distinctive nasalization",
   "original": "i98_0254",
   "page_count": 4,
   "order": 735,
   "p1": "paper 0254",
   "pn": "",
   "abstract": [
    "Cross-linguistic studies of the development of distinctive nasalization show evidence of significant suprasegmental conditioning. Amongst conditioning factors uncovered so far are vowel length and stress. Across languages it is reported that in the related contexts /V:N/ and /VN/, identical except for vowel length, phonologization of nasalization and N-deletion always occur preferentially in the context of long vowels. There is also cross-linguistic evidence of stress-conditioning of distinctive nasalization: nasalization and N-deletion appear to occur preferentially in stressed syllables. In this study, we discuss the results of an experiment designed to measure the possible effects of vowel duration and prominence on the perception of vowel nasalization. Both are seen to have an effect, although in different ways. Results presented here also lend support to the hypothesis that some sound changes, such as those involved in distinctive nasalization, may have a primarily perceptual basis.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-732"
  },
  "dang98_icslp": {
   "authors": [
    [
     "Jianwu",
     "Dang"
    ],
    [
     "Kiyoshi",
     "Honda"
    ]
   ],
   "title": "Speech production of vowel sequences using a physiological articulatory model",
   "original": "i98_0639",
   "page_count": 4,
   "order": 736,
   "p1": "paper 0639",
   "pn": "",
   "abstract": [
    "This report describes the development of a physiologically-based articulatory model, which consists of the tongue, mandible, hyoid bone and vocal tract wall. These organs are represented as a midsagittal quasi-3D layer with a thickness of 2 cm for tongue tissue and 3 cm for tract wall. The geometry of these organs and muscles are extracted from volumetric MR images of a male speaker. Both the soft and rigid structures are represented by mass-points and viscoelastic springs for connective tissue, where the springs for bony organs are set to extremely large stiffness. This design is suitable to compute soft tissue deformations and rigid organ displacements simultaneously using a single algorithm, and thus reduces computational complexities of the simulation. A novel control method is developed to produce dynamic actions of the vocal tract, as well as to handle the collision of the tongue to surrounding walls. Area functions are obtained for vowel sequences based on model's vocal tract widths in the midsagittal and parasagittal planes. The proposed model demonstrated plausible dynamic behaviors for human speech articulation.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-733"
  },
  "cox98b_icslp": {
   "authors": [
    [
     "Felicity",
     "Cox"
    ],
    [
     "Sallyanne",
     "Palethorpe"
    ]
   ],
   "title": "Regional variation in the vowels of female adolescents from sydney",
   "original": "i98_0417",
   "page_count": 4,
   "order": 737,
   "p1": "paper 0417",
   "pn": "",
   "abstract": [
    "This paper examines the assumption that Australian English vowel variation within urban centres is restricted to Broadness variation and is a consequence of socioeconomic rather than regional factors. The acoustic structure of vowel realisations for subjects across three different regions in Sydney is compared to shed some light on the theory of regional uniformity. 95 adolescent female speakers of General Australian English produced multiple repetitions of 18 different vowels in the h-d context. The vowel formants for each group were compared using ANOVA with post-hoc Bonferroni. Results revealed several significant area effects not necessarily associated with broadness variation. As area often subsumes a range of socioeconomic factors, it was necessary to also investigate whether some other socioeconomic indicator would account for the results. An examination of parental occupation, education and area confirm area as the most important of these factors in influencing vowel realisation for this group of speakers.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-734"
  },
  "watson98b_icslp": {
   "authors": [
    [
     "Catherine",
     "Watson"
    ],
    [
     "Jonathan",
     "Harrington"
    ],
    [
     "Sallyanne",
     "Palethorpe"
    ]
   ],
   "title": "A kinematic analysis of new zealand and australian English vowel spaces",
   "original": "i98_0633",
   "page_count": 4,
   "order": 738,
   "p1": "paper 0633",
   "pn": "",
   "abstract": [
    "This study presents a kinematic and acoustic comparison of the vowels spaces of New Zealand English (NZE) and Australian English (AE). Five talkers of NZE (3F and 2M) and five female talkers of AE each produced between 9 - 15 tokens of the monophthongs in HEAD, HID,HAD, and HERD. For each token, measurements of lip aperture, lip protrusion, jaw height, and tongue height and backness were made, in addition to formants and vowel duration being calculated. There were three main findgs from this study. Firstly the widely held view that the NZE HID vowel may have lowered as it centralised may be incorrect. In this study results suggest that although the NZE HID vowel is retracted it remains a high front vowel. The second results is HERD is lip rounded in NZE and AE. The final finding is that there is no significant difference in vowel duration between NZE and AE HEAD and HAD, despite NZE HEAD and HAD being phonetically more raised. The implications of these results are discussed.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-735"
  },
  "nguyen98b_icslp": {
   "authors": [
    [
     "Noel",
     "Nguyen"
    ],
    [
     "Sarah",
     "Hawkins"
    ]
   ],
   "title": "Syllable-onset acoustic properties associated with syllable-coda voicing",
   "original": "i98_0539",
   "page_count": 4,
   "order": 739,
   "p1": "paper 0539",
   "pn": "",
   "abstract": [
    "This study investigates durational and spectral variation in syllable-onset /l/s dependent on voicing in the coda. 1560 pairs of (C)lVC monosyllables differing in the voicing of the final stop were read by 4 British English speakers. Onset /l/ was longer before voiced than voiceless codas, and darker (for 3 speakers) as measured by F2 frequency and spectral centre of gravity. Differences due to other variables (lexical status, isolation/carrier context, syllable onset, vowel quality and regional accent) are outlined. It is proposed that coda voicing is a feature associated with the whole syllable, phonetically implemented as a variety of properties spread throughout the syllabic domain. Implications for word recognition are outlined.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-736"
  },
  "nguyen98c_icslp": {
   "authors": [
    [
     "Noel",
     "Nguyen"
    ],
    [
     "Alan A.",
     "Wrench"
    ],
    [
     "Fiona",
     "Gibbon"
    ],
    [
     "William J.",
     "Hardcastle"
    ]
   ],
   "title": "Articulatory, acoustic and perceptual aspects of fricative-stop coarticulation",
   "original": "i98_0533",
   "page_count": 4,
   "order": 740,
   "p1": "paper 0533",
   "pn": "",
   "abstract": [
    "Stops are not identified in the same way depending on preceding fricatives. According to Mann and Repp (1981), such context-dependent variations in the perception of stops originate from the influence of fricatives on how stops are produced. This study aimed further to explore this hypothesis. A first experiment showed that the effect of fricatives on the identification of stops tends to be confined to the most ambiguous stimuli, when a large range of acoustic cues to place of stop articulation is provided to the listener. In a second experiment, articulatory and acoustic data were gathered in the production of fricative-stop sequences. Although on the whole consistent with previous findings, our results indicate that many articulatory dimensions are brought into play in fricative-stop articulatory patterns, thus making it difficult to establish a direct link between the articulatory and perceptual levels.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-737"
  },
  "son98_icslp": {
   "authors": [
    [
     "Rob J. J. H. van",
     "Son"
    ],
    [
     "Florien J.",
     "Koopmans-van Beinum"
    ],
    [
     "Louis C. W.",
     "Pols"
    ]
   ],
   "title": "Efficiency as an organizing principle of natural speech",
   "original": "i98_0203",
   "page_count": 4,
   "order": 741,
   "p1": "paper 0203",
   "pn": "",
   "abstract": [
    "A large part of the variation in natural speech appears along the dimensions of articulatory precision / perceptual distinctiveness. We propose that this variation is the result of an effort to communicate efficiently. Speaking is considered efficient if the speech sound contains only the information needed to understand it. This efficiency is tested by means of a corpus of spontaneous and matched read speech, and syllable and word frequencies as measures of information content (12007 syllables, 8046 word forms, 1582 intervocalic consonants, and 2540 vowels). It is indeed found that the duration and spectral reduction of consonants and vowels correlate with the frequency of syllables and words in this corpus. Consonant intelligibility correlates with both the acoustic factors and the syllable and word frequencies. It is concluded that the principle of efficient communication organizes at least some aspects of speech production.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-738"
  },
  "karlsson98_icslp": {
   "authors": [
    [
     "Inger",
     "Karlsson"
    ],
    [
     "Tanja",
     "B√§nziger"
    ],
    [
     "Jana",
     "Dankovicova"
    ],
    [
     "Tom",
     "Johnstone"
    ],
    [
     "Johan",
     "Lindberg"
    ],
    [
     "Haakan",
     "Melin"
    ],
    [
     "Francis",
     "Nolan"
    ],
    [
     "Klaus R.",
     "Scherer"
    ]
   ],
   "title": "Within-speaker variability due to speaking manners",
   "original": "i98_0737",
   "page_count": 4,
   "order": 742,
   "p1": "paper 0737",
   "pn": "",
   "abstract": [
    "Some preliminary investigations of within-speaker variations due to voluntary and induced speaking manners have been performed. The ultimate aim of the investigations was to suggest methods to take care of within-speaker variations in automatic speaker verification. Special software was developed to systematically elicit different types of voluntary and involuntary speech variations that might realistically occur in every-day situations. A database containing speech from 50 Swedish male speakers was collected using this software. Acoustic analyses have been performed on and the results compared between voluntary and involuntary speech variations. The acoustic parameters that have been studied included segment durations, formant frequencies at vowel midpoints, fundamental frequency and overall amplitude and amplitude in frequency bands.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-739"
  },
  "kuhn98_icslp": {
   "authors": [
    [
     "Roland",
     "Kuhn"
    ],
    [
     "Patrick",
     "Nguyen"
    ],
    [
     "Jean-Claude",
     "Junqua"
    ],
    [
     "Lloyd",
     "Goldwasser"
    ],
    [
     "Nancy",
     "Niedzielski"
    ],
    [
     "Steven",
     "Fincke"
    ],
    [
     "Ken",
     "Field"
    ],
    [
     "Matteo",
     "Contolini"
    ]
   ],
   "title": "Eigenvoices for speaker adaptation",
   "original": "i98_0303",
   "page_count": 4,
   "order": 743,
   "p1": "paper 0303",
   "pn": "",
   "abstract": [
    "We have devised a new class of fast adaptation techniques for speech recognition. These techniques are based on prior knowledge of speaker variation, obtained by applying Principal Component Analysis (PCA) or a similar technique to T vectors of dimension D derived from T speaker-dependent models. This offline step yields T basis vectors called ``eigenvoices''. We constrain the model for new speaker S to be located in the space spanned by the first K eigenvoices. Speaker adaptation involves estimating the K eigenvoice coefficients for the new speaker; typically, K is very small compared to D. We conducted mean adaptation experiments on the Isolet database. With a large amount of supervised adaptation data, most eigenvoice techniques performed slightly better than MAP or MLLR; with small amounts of supervised adaptation data or for unsupervised adaptation, some eigenvoice techniques performed much better. We believe that the eigenvoice approach would yield rapid adaptation for most speech recognition systems.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-740"
  },
  "johnson98b_icslp": {
   "authors": [
    [
     "Sue E.",
     "Johnson"
    ],
    [
     "Philip C.",
     "Woodland"
    ]
   ],
   "title": "Speaker clustering using direct maximisation of the MLLR-adapted likelihood",
   "original": "i98_0726",
   "page_count": 4,
   "order": 744,
   "p1": "paper 0726",
   "pn": "",
   "abstract": [
    "In this paper speaker clustering schemes are investigated in the context of improving unsupervised adaptation for broadcast news transcription. The various techniques are presented within a framework of top-down split-and-merge clustering. Since these schemes are to be used for MLLR-based adaptation, a natural evaluation metric for clustering is the increase in data likelihood from adaptation. Two types of cluster splitting criteria have been used. The first minimises a covariance-based distance measure and for the second we introduce a two-step E-M type procedure to form clusters which directly maximise the likelihood of the adapted data. It is shown that the direct maximisation technique produces a higher data likelihood and also gives a reduction in word error rate.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-741"
  },
  "viikki98_icslp": {
   "authors": [
    [
     "Olli",
     "Viikki"
    ],
    [
     "Kari",
     "Laurila"
    ]
   ],
   "title": "Incremental on-line speaker adaptation in adverse conditions",
   "original": "i98_0313",
   "page_count": 4,
   "order": 745,
   "p1": "paper 0313",
   "pn": "",
   "abstract": [
    "In this paper, we examine the use of speaker adaptation in adverse noise conditions. In particular, we focus on incremental on-line speaker adaptation since it, in addition to its other advantages, enables joint speaker and environment adaptation. First, we show that on-line adaptation is superior to off-line adaptation when realistic changing noise conditions are considered. Next, we show that a conventional left-to-right HMM structure is not well suited for on-line adaptation in variable noise conditions due to unreliable state-frame alignments of noisy utterances. To overcome this problem, we suggest the use of state duration constrained HMMs. Our experimental results indicate that the performance gain due to adaptation is much greater with duration constrained HMMs than obtained with conventional left-to-right HMMs. In addition to the appropriate model structure, we point out that in long-term adaptation, such as incremental on-line adaptation, the supervised approach is a necessity.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-742"
  },
  "gales98_icslp": {
   "authors": [
    [
     "Mark J. F.",
     "Gales"
    ]
   ],
   "title": "Cluster adaptive training for speech recognition",
   "original": "i98_0375",
   "page_count": 4,
   "order": 746,
   "p1": "paper 0375",
   "pn": "",
   "abstract": [
    "When performing speaker adaptation there are two conflicting requirements. The transform must be powerful enough to model the speaker. Second, the transform should be rapidly estimated for any particular speaker. Recently the most popular adaptation schemes have used many parameters to adapt the models. This paper examines an adaptation scheme requiring few parameters to adapt the models, cluster adaptive training. It may be viewed as a simple extension to speaker clustering. A linear interpolation of the cluster means is used as the mean of the particular speaker. This scheme naturally falls into an adaptive training framework. Maximum likelihood estimates of the interpolation weights are given. Furthermore, re-estimation formulae for cluster means, represented both explicitly and by sets of transforms of some canonical mean, are given. On a speaker-independent task CAT reduced the word error rate using very little adaptation data compared to a standard system. a speaker independent model set.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-743"
  },
  "chien98_icslp": {
   "authors": [
    [
     "Jen-Tzung",
     "Chien"
    ]
   ],
   "title": "On-line hierarchical transformation of hidden Markov models for speaker adaptation",
   "original": "i98_0102",
   "page_count": 4,
   "order": 747,
   "p1": "paper 0102",
   "pn": "",
   "abstract": [
    "This paper presents a novel framework of on-line hierarchical transformation of hidden Markov models (HMM's) for speaker adaptation. Our aim is to incrementally transform (or adapt) all the HMM parameters to a new speaker even though part of HMM units are unseen in adaptation data. The transformation paradigm is formulated according to the approximate Bayesian estimate, which the prior statistics and the transformation parameters are incrementally updated for each consecutive adaptation data. Using this formulation, the updated prior statistics and the current block of data are sufficient for on-line transformation. Further, we establish a hierarchical tree of HMM's and use it to dynamically control the transformation sharing for each HMM unit. In the speaker adaptation experiments, we demonstrate the superiority of proposed on-line transformation to other method.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-744"
  },
  "suzuki98d_icslp": {
   "authors": [
    [
     "Motoyuki",
     "Suzuki"
    ],
    [
     "Toshiaki",
     "Abe"
    ],
    [
     "Hiroki",
     "Mori"
    ],
    [
     "Shozo",
     "Makino"
    ],
    [
     "Hirotomo",
     "Aso"
    ]
   ],
   "title": "High-speed speaker adaptation using phoneme dependent tree-structured speaker clustering",
   "original": "i98_0992",
   "page_count": 4,
   "order": 748,
   "p1": "paper 0992",
   "pn": "",
   "abstract": [
    "The tree-structured speaker clustering was proposed as a high-speed speaker adaptation method. It can select the model which is most similar to a target speaker. However, this method does not consider speaker difference dependent on phoneme class. In this paper, we propose a speaker adaptation method based on speaker clustering by taking speaker difference dependent on phoneme class into account. The experimental results showed that the new method gave a better performance than the original method. Furthermore, we propose the improved method which use a tree-structure of a similar phoneme as the substitute for the phoneme which does not appear in the adaptation data. From the experimental results, the improved method gave a better performance than the method previously proposed.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-745"
  },
  "anastasakos98_icslp": {
   "authors": [
    [
     "Tasos",
     "Anastasakos"
    ],
    [
     "Sreeram V.",
     "Balakrishnan"
    ]
   ],
   "title": "The use of confidence measures in unsupervised adaptation of speech recognizers",
   "original": "i98_0599",
   "page_count": 4,
   "order": 749,
   "p1": "paper 0599",
   "pn": "",
   "abstract": [
    "Confidence estimation of the output hypothesis of a speech recognizer offers a way to assess the probability that the recognized words are correct. This work investigates the application of confidence scores for selection of speech segments in unsupervised speaker adaptation. Our approach is motivated by initial experiments that show that the use of mis-labeled data has a significant cost in the performance of particular adaptation schemes. We focus on a rapid self-adaptation scenario that uses only a few seconds of adaptation data. The adaptation algorithm is based on an extension to the MLLR transformation method that can be applied to the observation vectors. We present experimental results of this work on the ARPA WSJ large vocabulary dictation task.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-746"
  },
  "mcdonough98_icslp": {
   "authors": [
    [
     "John",
     "McDonough"
    ],
    [
     "William",
     "Byrne"
    ],
    [
     "Xiaoqiang",
     "Luo"
    ]
   ],
   "title": "Speaker normalization with all-pass transforms",
   "original": "i98_0869",
   "page_count": 4,
   "order": 750,
   "p1": "paper 0869",
   "pn": "",
   "abstract": [
    "Speaker normalization is a process in which the short-time features of speech from a given speaker are transformed so as to better match some speaker independent model. Vocal tract length normalization (VTLN) is a popular speaker normalization scheme wherein the frequency axis of the short-time spectrum associated with a speaker's speech is rescaled or warped prior to the extraction of cepstral features. In this work, we develop a novel speaker normalization scheme by exploiting the fact that frequency domain transformations similar to that inherent in VTLN can be accomplished entirely in the cepstral domain through the use of conformal maps. We propose a class of such maps, designated all-pass transforms for reasons given hereafter, and in a set of speech recognition experiments conducted on the Switchboard Corpus demonstrate their capacity to achieve word error rate reductions of 3.7% absolute.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-747"
  },
  "zheng98b_icslp": {
   "authors": [
    [
     "Rong",
     "Zheng"
    ],
    [
     "Zuoying",
     "Wang"
    ]
   ],
   "title": "Toward on-line learning of Chinese continuous speech recognition system",
   "original": "i98_0276",
   "page_count": 4,
   "order": 751,
   "p1": "paper 0276",
   "pn": "",
   "abstract": [
    "In this paper, we presented an integrated on-line learning scheme, which combined the state-of-art speaker normalization and adaptation techniques to improve the performance of our large vocabulary Chinese continuous speech recognition (CSR) system. We used VTLN to remove inter-speaker variation in both training and testing stage. To facilitate dynamic transformation scale determination, we devised a tree-based transformation method as the key component of our incremental adaptation. Experiments shows that the combined scheme of on-line learning (incremental & unsupervised) system, which gives approximately 22~26% error reduction rate, was proved to be better than either method when used separately at 18.34% and 2.7%.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-748"
  },
  "oviatt98b_icslp": {
   "authors": [
    [
     "Sharon L.",
     "Oviatt"
    ]
   ],
   "title": "The CHAM model of hyperarticulate adaptation during human-computer error resolution",
   "original": "i98_0049",
   "page_count": 4,
   "order": 752,
   "p1": "paper 0049",
   "pn": "",
   "abstract": [
    "When using interactive systems, people adapt their speech during attempts to resolve system recognition errors. This paper summarizes the two-stage Computer-elicited Hyperarticulate Adaptation Model (CHAM), which accounts for systematic changes in human speech during interactive error handling. According to CHAM, Stage I adaptation is manifest as a singular change involving the increased duration of speech and pauses. This change is associated with a moderate degree of hyperarticulation, which occurs during a low rate of system errors. In contrast, State II adaptations are associated with more extreme hyperarticulation during a high system error rate. It entails change in multiple features of speech - including duration, articulation, intonation pattern, fundamental frequency and amplitude. This paper summarizes the empirical findings and linguistic theory upon which CHAM is based, as well as the model's main predictions. Finally, the implications of CHAM are discussed for designing future interactive systems with improved error handling.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-749"
  },
  "uebler98b_icslp": {
   "authors": [
    [
     "Ulla",
     "Uebler"
    ],
    [
     "Michael",
     "Sch√ºssler"
    ],
    [
     "Heinrich",
     "Niemann"
    ]
   ],
   "title": "Bilingual and dialectal adaptation and retraining",
   "original": "i98_0337",
   "page_count": 4,
   "order": 753,
   "p1": "paper 0337",
   "pn": "",
   "abstract": [
    "In this paper, we report our investigations on the use of adaptation and retraining in our bilingual (Italian, German) and multidialectal recognition system. Our approach for bilingual speech recognition is to assume the two languages as being one, which is best suited for a task where Italian and German natives speak both languages, resulting in a variety of accents and dialects. We performed adaptation on single speakers and speaker groups built from combinations of spoken and native language. Furthermore, we performed retraining on partitions of the adaptation or training data. Our experiments led to an error rate reduction in all cases: compared to the baseline system, we achieved an overall improvement of 14, 12--14 and 7 % for speaker adaptation, speaker group adaptation and retraining, respectively. Furthermore, we found among others that performance is rather stable for Italian between adaptation and retraining, while adaptation for German outperforms retraining by far.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-750"
  },
  "schultz98_icslp": {
   "authors": [
    [
     "Tanja",
     "Schultz"
    ],
    [
     "Alex",
     "Waibel"
    ]
   ],
   "title": "Language independent and language adaptive large vocabulary speech recognition",
   "original": "i98_0577",
   "page_count": 4,
   "order": 754,
   "p1": "paper 0577",
   "pn": "",
   "abstract": [
    "This paper describes the design of a multilingual speech recognizer using an LVCSR dictation database which has been collected under the project GlobalPhone. This project at the University of Karlsruhe investigates LVCSR systems in 15 languages of the world, namely Arabic, Chinese, Croatian, English, French, German, Italian, Japanese, Korean, Portuguese, Russian, Spanish, Swedish, Tamil, and Turkish. Based on a global phoneme set we built different multilingual speech recognition systems for five of the 15 languages. Context dependent phoneme models are created data-driven by introducing questions about languages and language groups to our polyphone clustering procedure. We apply the resulting multilingual models to unseen languages and present several recognition results in language independent and language adaptive setups. The results indicate that the method of parameter sharing should be decided depending on whether multilingual or crosslingual speech recognition is projected.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-751"
  },
  "kawai98b_icslp": {
   "authors": [
    [
     "Goh",
     "Kawai"
    ],
    [
     "Keikichi",
     "Hirose"
    ]
   ],
   "title": "A method for measuring the intelligibility and nonnativeness of phone quality in foreign language pronunciation training",
   "original": "i98_0782",
   "page_count": 4,
   "order": 755,
   "p1": "paper 0782",
   "pn": "",
   "abstract": [
    "The problem addressed is automatically detecting, measuring and correcting nonnative pronunciation characteristics (so-called \"foreign accents\") in foreign language speech. Systemic, structural and realizational differences between L1 (native language) and L2 (target language) appear as phone insertions, deletions and substitutions. A bilingual phone recognizer using native-trained acoustic models of the learner's L1 and L2 was developed to identify insertions, deletions and substitutions of L2 phones. Recognition results are combined with knowledge of phonetics, phonology and pedagogy to show learners which phones were mispronounced and to instruct how to modify their articulatory gestures for more native-sounding speech. The degree of the learner's foreign accent is measured based on the number of alternate pronunciations the learner uses; the number decreases as learning progresses. Evaluation experiments using Japanese and American English indicate that the system is an effective component technology for computer-aided pronunciation learning.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-752"
  },
  "blamey98_icslp": {
   "authors": [
    [
     "Peter",
     "Blamey"
    ],
    [
     "Julia",
     "Sarant"
    ],
    [
     "Tanya",
     "Serry"
    ],
    [
     "Roger",
     "Wales"
    ],
    [
     "Christopher",
     "James"
    ],
    [
     "Johanna",
     "Barry"
    ],
    [
     "Graeme M.",
     "Clark"
    ],
    [
     "M.",
     "Wright"
    ],
    [
     "R.",
     "Tooher"
    ],
    [
     "C.",
     "Psarros"
    ],
    [
     "G.",
     "Godwin"
    ],
    [
     "M.",
     "Rennie"
    ],
    [
     "T.",
     "Meskin"
    ]
   ],
   "title": "Speech perception and spoken language in children with impaired hearing",
   "original": "i98_0248",
   "page_count": 4,
   "order": 756,
   "p1": "paper 0248",
   "pn": "",
   "abstract": [
    "Fifty seven children with impaired hearing aged 4-12 years were evaluated with speech perception and language measures as the first stage of a longitudinal study. The Clinical Evaluation of Language Fundamentals (CELF) and Peabody Picture Vocabulary Test (PPVT) were used to evaluate the children's spoken language. Regression analyses indicated that scores on both tests were significantly correlated with chronological age, but delayed relative to children with normal hearing. Performance increased at 45% of the rate expected for children with normal hearing for the CELF, and 62% for the PPVT. Perception scores were not significantly correlated with chronological age, but were highly correlated with results on the PPVT and CELF. The data suggest a complex relationship whereby hearing impairment reduces speech perception, which slows language development, which has a further adverse effect on speech perception.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-753"
  },
  "cucchiarini98c_icslp": {
   "authors": [
    [
     "Catia",
     "Cucchiarini"
    ],
    [
     "Helmer",
     "Strik"
    ],
    [
     "Louis",
     "Boves"
    ]
   ],
   "title": "Quantitative assessment of second language learners' fluency: an automatic approach",
   "original": "i98_0752",
   "page_count": 4,
   "order": 757,
   "p1": "paper 0752",
   "pn": "",
   "abstract": [
    "This paper describes an experiment aimed at determining whether native and non-native speakers of Dutch significantly differ on a number of quantitative measures related to fluency and whether these measures can be successfully employed to predict fluency scores. Read speech of 20 native and 60 non-native speakers of Dutch was scored for fluency by nine experts and was then analyzed by means of an automatic speech recognizer in order to calculate nine quantitative measures of speech quality that are known to be related to perceived fluency. The results show that the natives' scores on the fluency ratings and on the quantitative measures significantly differ from those of the non-natives, with the native speakers being considered more fluent. Furthermore, it appears that quantitative variables such as rate of speech, phonation-time ratio, number of pauses, and mean length of runs are able to predict fluency scores with a high degree of accuracy.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-754"
  },
  "dalsgaard98_icslp": {
   "authors": [
    [
     "Paul",
     "Dalsgaard"
    ],
    [
     "Ove",
     "Andersen"
    ],
    [
     "William J.",
     "Barry"
    ]
   ],
   "title": "Cross-language merged speech units and their descriptive phonetic correlates",
   "original": "i98_0482",
   "page_count": 4,
   "order": 758,
   "p1": "paper 0482",
   "pn": "",
   "abstract": [
    "The focus of this paper is to formulate an approach to merging phonemes across languages and to evaluate the resulting cross-language merged speech units on the basis of the traditional acoustic-phonetic descriptions of the phonemes. The methodology is based on the belief that some phonemes across a set of languages may be similar enough to be equated, contrasting traditional phonology which treats phonemes from one language independent from phonemes from another language. The identification of cross-language speech units is performed by an iterative data-driven procedure, which merges acoustically similar phonemes from within one language as well as across languages. The paper interprets a number of merged speech units on the basis of articulatory descriptions.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-755"
  },
  "eklund98c_icslp": {
   "authors": [
    [
     "Robert",
     "Eklund"
    ],
    [
     "Elizabeth",
     "Shriberg"
    ]
   ],
   "title": "Crosslinguistic disfluency modelling: a comparative analysis of Swedish and american English human--human and human--machine dialogues",
   "original": "i98_0805",
   "page_count": 4,
   "order": 759,
   "p1": "paper 0805",
   "pn": "",
   "abstract": [
    "We report results from a cross-language study of disfluencies (DFs) in Swedish and American English human-machine and human-human dialogs. The focus is on comparisons not directly affected by differences in overall rates since these could be associated with task details. Rather, we focus on differences suggestive of how speakers utilize DFs in the different languages, including: relative rates of the use of hesitation forms, the location of hesitations, and surface characteristics of DFs. Results suggest that although the languages differ in some respects (such as the ability to insert filled pauses within `words'), in many analyses the languages show similar behavior. Such results provide suggestions for cross-linguistic DF modeling in both theoretical and applied fields.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-756"
  },
  "franco98_icslp": {
   "authors": [
    [
     "Horacio",
     "Franco"
    ],
    [
     "Leonardo",
     "Neumeyer"
    ]
   ],
   "title": "Calibration of machine scores for pronunciation grading",
   "original": "i98_0764",
   "page_count": 4,
   "order": 760,
   "p1": "paper 0764",
   "pn": "",
   "abstract": [
    "Our proposed paradigm for automatic assessment of pronunciation quality uses hidden Markov models (HMMs) to generate phonetic segmentations of the student's speech. From these segmentations, we use the HMMs to obtain spectral match and duration scores. In this work we focus on the problem of mapping different machine scores to obtain an accurate prediction of the grades that a human expert would assign to the pronunciation. We discuss the application of different approaches based on minimum mean square error (MMSE) estimation and Bayesian classification. We investigate the characteristics of the different mappings as well as the effects of the prior distribution of grades in the calibration database. We finally suggest a simple method to extrapolate mappings from one language to another.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-757"
  },
  "geutner98b_icslp": {
   "authors": [
    [
     "Petra",
     "Geutner"
    ],
    [
     "Michael",
     "Finke"
    ],
    [
     "Alex",
     "Waibel"
    ]
   ],
   "title": "Phonetic-distance-based hypothesis driven lexical adaptation for transcribing multlingual broadcast news",
   "original": "i98_0771",
   "page_count": 4,
   "order": 761,
   "p1": "paper 0771",
   "pn": "",
   "abstract": [
    "High OOV-rates are one of the most prevailing problems for languages with a rapid vocabulary growth, e.g. when transcribing Serbo-Croatian and German broadcast news. Hypothesis-Driven-Lexical-Adaptation (HDLA) has been shown to decrease high OOV-rates significantly by using morphology-based linguistic knowledge. This paper introduces another approach to dynamically adapt a recognition lexicon to the utterance to be recognized. Instead of morphological knowledge about word stems and inflection endings, distance measures based on Levenstein distance are used. Results based on phoneme and grapheme distances will be presented. Our distance-based approach requires no expert knowledge about a specific language and no definition of complex grammar rules. Instead, grapheme sequences or the phoneme representation of words are sufficient to apply our HDLA-algorithm easily to any new language. With our proposed technique OOV-rates were decreased by more than half from 8.7% to 4%, thereby also improving recognition performance by an absolute 4.1% from 29.5% to 25.4% word error rate.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-758"
  },
  "jo98b_icslp": {
   "authors": [
    [
     "Chul-Ho",
     "Jo"
    ],
    [
     "Tatsuya",
     "Kawahara"
    ],
    [
     "Shuji",
     "Doshita"
    ],
    [
     "Masatake",
     "Dantsuji"
    ]
   ],
   "title": "Automatic pronunciation error detection and guidance for foreign language learning",
   "original": "i98_0741",
   "page_count": 4,
   "order": 762,
   "p1": "paper 0741",
   "pn": "",
   "abstract": [
    "We propose an effective application of speech recognition to foreign language pronunciation learning. The objective of our system is to detect pronunciation errors and provide diagnostic feedback through speech processing and recognition methods. Automatic pronunciation error detection is used for two kinds of mispronunciation, that is mistake and linguistical inheritance. The correlation between automatic detection and human judgement shows its reliability. For the feedback guidance to an erroneous phone, we set up classifiers for the well-recognized articulatory features, the place of articulation and the manner of articulation, in order to identify the cause of incorrect articulation. It provides feedback guidance on how to correct mispronunciation.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-759"
  },
  "leung98_icslp": {
   "authors": [
    [
     "Roger Ho-Yin",
     "Leung"
    ],
    [
     "Hong C.",
     "Leung"
    ]
   ],
   "title": "Lexical access for large-vocabulary speech recognition",
   "original": "i98_0229",
   "page_count": 4,
   "order": 763,
   "p1": "paper 0229",
   "pn": "",
   "abstract": [
    "In this paper, the lexical characteristics of two Chinese dialects and American English are explored. Different lexical representations are investigated, including the tonal syllables, base syllables, phonemes, and the broad phonetic classes. Multiple measurements are made, such as coverage, uniqueness, and cohort sizes. Our results are based on lexicons of 44K and 52K words in Chinese and English obtained from the CallHome Corpus and the COMLEX Corpus, respectively. We have found that the set of the most frequent 4,000 words has coverage of 92% and 77% for Chinese and English, respectively. The phonetic representation unique specifies 85%, 87% and 93% of the lexicon for Mandarin, Cantonese, and English, respectively. While the three languages appear quite different when they are described by their full phoneme sets, their characteristics are more similar when they are represented in terms of broad phonetic classes.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-760"
  },
  "liu98c_icslp": {
   "authors": [
    [
     "Sharlene",
     "Liu"
    ],
    [
     "Sean",
     "Doyle"
    ],
    [
     "Allen",
     "Morris"
    ],
    [
     "Farzad",
     "Ehsani"
    ]
   ],
   "title": "The effect of fundamental frequency on Mandarin speech recognition",
   "original": "i98_0847",
   "page_count": 4,
   "order": 764,
   "p1": "paper 0847",
   "pn": "",
   "abstract": [
    "ABSTRACT We study the effects of modeling tone in Mandarin speech recognition. Including the neutral tone, there are 5 tones in Mandarin and these tones are syllable-level phenomena. A direct acoustic manifestation of tone is the fundamental frequency (f0). We will report on the effect of f0 on the acoustic recognition accuracy of a Mandarin recognizer. In particular, we put f0, its first derivative (f0'), and its second derivative (f0'') in separate streams of the feature vector. Stream weights are adjusted to investigate the individual effects of f0, f0', and f0'' to recognition accuracy. Our results show that incorporating the f0 feature negatively impacted accuracy, whereas f0' increased accuracy and f0'' seemed to have no effect.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-761"
  },
  "markham98_icslp": {
   "authors": [
    [
     "Duncan",
     "Markham"
    ]
   ],
   "title": "The perception of nativeness: variable speakers and flexible listeners",
   "original": "i98_0424",
   "page_count": 4,
   "order": 765,
   "p1": "paper 0424",
   "pn": "",
   "abstract": [
    "Tests of foreign accent usually treat native listeners as reliable providers of accentedness ratings, and pay too little heed to task-specific effects on non-native speakers' performance. This paper details a number of factors which in fact influence native listeners' perceptions, and the native-like behaviour of non-native speakers' productions, based on the results of a large study of phonetic performance in second language learners. Listeners were observed to vary, at times considerably, in their perception of accent, depending on context, and type of stimulus, and at times showed distinctly idiosyncratic scoring patterns. Listeners' reactions to speaker voice pathology, mixed dialect pronunciation, and artefacts of read speech are discussed, and the effects of using different types of scoring system are examined.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-762"
  },
  "mctear98b_icslp": {
   "authors": [
    [
     "Michael F.",
     "McTear"
    ],
    [
     "Eamonn A.",
     "O'Hare"
    ]
   ],
   "title": "Voice dictation in the secondary school classroom",
   "original": "i98_0546",
   "page_count": 4,
   "order": 766,
   "p1": "paper 0546",
   "pn": "",
   "abstract": [
    "This paper reports on an exploratory study in which a group of second year secondary school pupils with reading ages ranging from 8.3 to 12.9 performed a set of tasks using the IBM VoiceType dictation package in order to determine the benefits of voice dictation for classroom use. The results showed that pupils with varying reading ages could dictate at comparable speeds and often with similar degrees of accuracy. Homophones were almost never a source of error in the texts produced with voice dictation, as compared with the children's handwritten texts. The implications of these findings for the use of dictation software in the classroom and for further studies of the potential of voice dictation for improving children's spelling and composition skills are discussed.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-763"
  },
  "nakayama98_icslp": {
   "authors": [
    [
     "Kazuo",
     "Nakayama"
    ],
    [
     "Kaoru",
     "Tomita-Nakayama"
    ]
   ],
   "title": "The importance of the first syllable in English spoken word recognition by adult Japanese speakers",
   "original": "i98_0446",
   "page_count": 4,
   "order": 767,
   "p1": "paper 0446",
   "pn": "",
   "abstract": [
    "We investigated English spoken word recognition of adult Japanese speakers. We found that the accurate recognition of the first syllable played an important role in recognizing a word correctly. It was implied that their recognition performance would be enhanced by time-scale expansion and/or dynamic range compression. The duration of a beginning word is so short that the listener can't recognize it correctly. In the first experiment, we found that they had difficulty in recognizing both isolated words and the extracted words, especially when the word did not begin with a strong syllable. In the second experiment, the extracted words and the corresponding time-scale expanded words were given. The result indicated that the expanded words were better recognized. It is found that the time-scale modification of the extracted words didn't lose intelligibility even around the ratio of 2.00, as was clear from the fact that the recognition improved.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-764"
  },
  "oster98_icslp": {
   "authors": [
    [
     "Anne-Marie",
     "√ñster"
    ]
   ],
   "title": "Spoken L2 teaching with contrastive visual and auditory feedback",
   "original": "i98_0256",
   "page_count": 4,
   "order": 768,
   "p1": "paper 0256",
   "pn": "",
   "abstract": [
    "Teaching strategies and positive results from training of both perception and production of spoken Swedish with 13 immigrants are reported. The learners participated in six training sessions lasting thirty minutes, twice a week. The training had a positive effect on the L2-speakers' perception and production of individual Swedish sounds, stress, intonation and rhythm. The positive results were obtained through auditory and visually contrastive feedback provided through a PC running the IBM SpeechViewer software. Skill building modules together with the Speech Pattering Module \"Pitch and Loudness\" was used, that displays the speech signal as graphical curves and diagrams. A split screen offered a comparison of the student's production with a correct model by the teacher. Pitch and loudness were displayed either separately or combined.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-765"
  },
  "sandra98_icslp": {
   "authors": [
    [
     "Dominiek",
     "Sandra"
    ],
    [
     "Steven",
     "Gillis"
    ]
   ],
   "title": "The role of phonological, morphological, and orthographic knowledge in the intuitive syllabification of dutch words: a longitudinal approach",
   "original": "i98_1106",
   "page_count": 4,
   "order": 769,
   "p1": "paper 1106",
   "pn": "",
   "abstract": [
    "Children of three different ages (five, eight, and ten years old) were asked to syllabify a list of auditorily presented words. The list composition was such that the effect of different knowledge sources on the children's intuitive syllabification could be assessed: the relative importance of language-universal versus language-specific phonological constraints, the effect of morphological complexity, and the effect of orthographic knowledge. The results indicate that five-year old children are already aware of language-specific constraints and are sensitive to the phonological distinction between continuant and non-continuant consonants. Literate children (eight and ten years old) are influenced in their syllabification behavior by their orthographic knowledge, i.e. once children have reached the literate stage it is difficult for them to separate phonological and orthographic knowledge in this phonological task. Finally, children in all three age groups did not syllabify singulars differently than phonologically closely matched plurals.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-766"
  },
  "shirose98_icslp": {
   "authors": [
    [
     "Ayako",
     "Shirose"
    ],
    [
     "Haruo",
     "Kubozono"
    ],
    [
     "Shigeru",
     "Kiritani"
    ]
   ],
   "title": "The acquisition of Japanese compound accent rule",
   "original": "i98_1107",
   "page_count": 4,
   "order": 770,
   "p1": "paper 1107",
   "pn": "",
   "abstract": [
    "This paper reports the results of research on the process of acquisition of Japanese compound accent rules by children aged 4-5. The results reveal: 1) Children acquire general rules before they acquire lexical idiosyncratic rules. 2) Children failed to retain the accent of the second element, and instead place an incorrect accent on the penultimate foot. This result suggests that children acquire placing accent on the penultimate foot prior to retaining the lexical accent of the second element. We discussed a similarity between above result and a constraint-reranking phenomenon in adults' phonology. 3) The syllable, which plays a important role in adults' CA rules, does not contribute to the CA rules in children's phonology. We assumed that children have not acquired sufficient understanding of the syllable to contribute to the CA rules.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-767"
  },
  "so98_icslp": {
   "authors": [
    [
     "Lydia K. H.",
     "So"
    ],
    [
     "Zhou",
     "Jing"
    ]
   ],
   "title": "The acquisition of putonghua phonology",
   "original": "i98_0956",
   "page_count": 3,
   "order": 771,
   "p1": "paper 0956",
   "pn": "",
   "abstract": [
    "Ibis paper reports the phoneme repertoires and phonological error patterns of 600 Chinese-speaking children aged 2;0 to 7;0. The findings support the hypotheses that phonological acquisition is influenced by the ambient language and the mother tongue.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-768"
  },
  "tomitanakayama98_icslp": {
   "authors": [
    [
     "Kaoru",
     "Tomita-Nakayama"
    ],
    [
     "Kazuo",
     "Nakayama"
    ],
    [
     "Masayuki",
     "Misaki"
    ]
   ],
   "title": "Enhancing speech processing of Japanese learners of English utilizing time-scale expansion with constant pitch",
   "original": "i98_0180",
   "page_count": 4,
   "order": 772,
   "p1": "paper 0180",
   "pn": "",
   "abstract": [
    "This study demonstrated that the time-scale expansion of speech with the constant pitch (henceforth, expanded speech) enhanced the speech recognition of Japanese learners of English in contrast to the previous studies in which the time-scale expanded speeches did not contribute to the speech recognition chiefly because of the severe distortion of original speech and pitch change. Experiments were administered with the stimuli of original normal speech and the corresponding expanded speech. These results showed that the expanded speech stimuli were intelligible to many of the subjects. The hypotheses are that the expanded speech enhances listeners' speech processing and also enables the listeners to call into play virtual memory capacity for an on-line speech processing, which are more apparent in a longer stimulus. The expanded speech worked well with most subjects. Another prescriptions should be prepared for the rest of the subjects for whom the expanded speech was not solely much effective.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-769"
  },
  "warnke98_icslp": {
   "authors": [
    [
     "Volker",
     "Warnke"
    ],
    [
     "Elmar",
     "N√∂th"
    ],
    [
     "Jan",
     "Buckow"
    ],
    [
     "Stefan",
     "Harbeck"
    ],
    [
     "Heinrich",
     "Niemann"
    ]
   ],
   "title": "A bootstrap training approach for language model classifiers",
   "original": "i98_0316",
   "page_count": 4,
   "order": 773,
   "p1": "paper 0316",
   "pn": "",
   "abstract": [
    "In this paper, we present a bootstrap training approach for language model (LM) classifiers. Training class dependent LM and running them in parallel, LM can serve as classifiers with any kind of symbol sequence, e.g., word or phoneme sequences for tasks like topic spotting or language identification (LID). Irrespective of the special symbol sequence used for a LM classifier, the training of a LM is done with a manually labeled training set for each class obtained from not necessarily cooperative speakers. Therefore, we have to face some erroneous labels and deviations from the originally intended class specification. Both facts can worsen classification. It might therefore be better not to use all utterances for training but to automatically select those utterances that improve recognition accuracy; this can be done by a bootstrap procedure. We present the results achieved with our best approach on the VERBMOBIL-corpus for the tasks dialog act classification and LID.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-770"
  },
  "whiteside98b_icslp": {
   "authors": [
    [
     "Sandra P.",
     "Whiteside"
    ],
    [
     "Jeni",
     "Marshall"
    ]
   ],
   "title": "Voice onset time patterns in 7-, 9- and 11-year old children",
   "original": "i98_0154",
   "page_count": 4,
   "order": 774,
   "p1": "paper 0154",
   "pn": "",
   "abstract": [
    "Voice onset time (VOT) is a key temporal feature in spoken language. There is some evidence to suggest that there are sex differences in VOT patterns. The cause of these sex differences could be attributed to sexual dimorphism of the vocal apparatus. There is also some evidence to suggest that phonetic sex differences could also be attributed to learned stylistic and linguistic factors. This study reports on an investigation into the VOT patterns for /p b t d/ in a group of thirty children aged 7 (n=10), 9 (n=10) and 11 (n=10) years, with equal numbers of girls (n=5) and boys (n=5) in each age group. Age and sex differences were examined for in the VOT data. Age, sex and age-by-sex interactions were found. The results are presented and discussed.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-771"
  },
  "whiteside98c_icslp": {
   "authors": [
    [
     "Sandra P.",
     "Whiteside"
    ],
    [
     "Carolyn",
     "Hodgson"
    ]
   ],
   "title": "Some developmental patterns in the speech of 6-, 8- and 10-year old children: an acoustic phonetic study",
   "original": "i98_0155",
   "page_count": 4,
   "order": 775,
   "p1": "paper 0155",
   "pn": "",
   "abstract": [
    "The process of the development of fine motor speech skills co-occurs with the maturation of the vocal apparatus. This brief study presents some acoustic phonetic characteristics of the speech of twenty pre-adolescent (6-, 8- and 10-year-olds) boys and girls. The speech data were elicited via a picture-naming task. Both age and sex differences in the acoustic phonetic characteristics of selected vowels and consonants are examined. The acoustic phonetic characteristics that were investigated included formant frequency values and coarticulation (or gestural overlap) patterns. Age, sex and age-by-sex differences for the acoustic phonetic characteristics are presented and discussed for the data with reference to speech development and the sexual dimorphism of the vocal apparatus.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-772"
  },
  "brown98_icslp": {
   "authors": [
    [
     "Lisa-Jane",
     "Brown"
    ],
    [
     "John",
     "Locke"
    ],
    [
     "Peter",
     "Jones"
    ],
    [
     "Sandra P.",
     "Whiteside"
    ]
   ],
   "title": "Language development after extreme childhood deprivation: a case study",
   "original": "i98_0791",
   "page_count": 4,
   "order": 776,
   "p1": "paper 0791",
   "pn": "",
   "abstract": [
    "The atypical linguistic processing and cognitive development of previously institutionalised, adopted Romanian children are being researched using a neurolinguistic theory of development. Of particular concern is the Critical Period Hypothesis which holds that language capacity can only develop in response to relevant stimulation during a pre-determined period in childhood. The research impetus derives from the need to understand the course of first language acquisition in children who have suffered extreme deprivation at an early age. The purpose of this paper is to attempt to analyse what these children can tell us about the potential for language development in the face of such deprived circumstances. In order to examine this, a theory of neurolinguistic development will be applied to the case study of a formerly institutionalised Romanian child, Maria. A key question will be addressed: Has Maria's early deprivation set for her an irreversible path in terms of attaining normal language development?\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-773"
  },
  "williams98c_icslp": {
   "authors": [
    [
     "Geoff",
     "Williams"
    ],
    [
     "Mark",
     "Terry"
    ],
    [
     "Jonathan",
     "Kaye"
    ]
   ],
   "title": "Phonological elements as a basis for language-independent ASR",
   "original": "i98_0622",
   "page_count": 4,
   "order": 777,
   "p1": "paper 0622",
   "pn": "",
   "abstract": [
    "This paper proposes a novel architecture for language-independent ASR based on government phonology (GP). We use experimental data to show that phoneme-based recognisers perform poorly on languages other than the original target, rendering such systems inadequate for multi-lingual speech recognition, a result we attribute to the inadequacy of the phoneme as a linguistic unit. In the proposed GP model, recognition targets are a small set of sub-segmental primes, or \"elements\", found in all languages, which have been previously shown to be robustly detected in a language-independent manner. Well-formedness constraints are captured by simple parameter settings which can be easily encoded as rules and applied as top-down constraints in a speech recogniser. Hence, given a set of trained element detectors, a recogniser for any given language can in principle be rapidly built by selection of the appropriate lexicon and constraints. We describe the design of experimental architectures for our GP-based system.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-774"
  },
  "zmarich98_icslp": {
   "authors": [
    [
     "Claudio",
     "Zmarich"
    ],
    [
     "Roberta",
     "Lanni"
    ]
   ],
   "title": "A phonetic and acoustic study of babbling in an Italian child",
   "original": "i98_1004",
   "page_count": 4,
   "order": 778,
   "p1": "paper 1004",
   "pn": "",
   "abstract": [
    "This single case study aims to combine the auditory assessment method with the precision offered by the instrumental measurement of acoustic characteristics, in order to investigate the phonetic aspect of early speech development, namely babbling and early words. While generic progress may be determined in the increasing prevalence of the number of CV syllables within the global repertory of utterances, the aspects that better reveal the influence of a target language include the frequency of occurrence of vowel types, especially if their classification refers to front-back dimension, in combination with an expansion and refining of phonotactic possibilities. Further, acoustic and articulatory evidence reveals an initial tendency for more control of height dimension than front/back. The patterns of C-V associations suggest that the child develops from a babbling phase characterized by the overwhelming prevalence of front articulations, to a phase characterized by the presence of the first words, where the patterns predicted by the MacNeilage and Davis theory occur, perhaps owing to the presence of the same patterns in the target lexicon.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-775"
  },
  "kuhn98b_icslp": {
   "authors": [
    [
     "Roland",
     "Kuhn"
    ],
    [
     "Jean-Claude",
     "Junqua"
    ],
    [
     "Philip D.",
     "Martzen"
    ]
   ],
   "title": "Rescoring multiple pronunciations generated from spelled words",
   "original": "i98_0304",
   "page_count": 4,
   "order": 779,
   "p1": "paper 0304",
   "pn": "",
   "abstract": [
    "Building on earlier work, we show how a set of binary decision trees can be trained to generate an ordered list of possible pronunciations from a spelled word. Training is carried out on a database consisting of spelled words paired with their pronunciations (in a particular language). We show how phonotactic information can be learned by a second set of decision trees, which reorder the multiple pronunciations generated by the first set. The paper defines the ``inclusion'' metric for scoring phoneticizers that generate multiple pronunciations. Experimental results employing this metric indicate that phonotactic reordering yields a slight improvement when only the top pronunciation is retained, and a large improvement when more than one hypothesis is retained. Isolated-word recognition results which show good performance for automatically-generated pronunciations are given.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-776"
  },
  "blanco98_icslp": {
   "authors": [
    [
     "Yolanda",
     "Blanco"
    ],
    [
     "Maria",
     "Cuellar"
    ],
    [
     "Arantxa",
     "Villanueva"
    ],
    [
     "Fernando",
     "Lacunza"
    ],
    [
     "Rafael",
     "Cabeza"
    ],
    [
     "Beatriz",
     "Marcotegui"
    ]
   ],
   "title": "SIVHA, visual speech synthesis system",
   "original": "i98_1127",
   "page_count": 4,
   "order": 780,
   "p1": "paper 1127",
   "pn": "",
   "abstract": [
    "This paper presents SIVHA, a high quality Spanish speech synthesis system for severe disabled persons controlled by their eye movements. The system follows the eye-gaze of the patients along the screen and constructs the text with the selected words. When the user considers that the construction of the message has been finished, the synthesis of the message can be ordered. The system is divided in three modules. The first one determines the point of the screen the user is looking at, the second one is an interface to construct the sentences and the third one is the synthesis itself.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-777"
  },
  "bruijn98_icslp": {
   "authors": [
    [
     "C. G. de",
     "Bruijn"
    ],
    [
     "Sandra P.",
     "Whiteside"
    ],
    [
     "P. A.",
     "Cudd"
    ],
    [
     "D.",
     "Syder"
    ],
    [
     "K. M.",
     "Rosen"
    ],
    [
     "L.",
     "Nord"
    ]
   ],
   "title": "Using automatic speech recognition and its possible effects on the voice",
   "original": "i98_0501",
   "page_count": 4,
   "order": 781,
   "p1": "paper 0501",
   "pn": "",
   "abstract": [
    "Literature and individual reports contain indications that the use of speech recognition based human computer interfaces could potentially lead to vocal fatigue, or even to symptoms associated with dysphonia. While more and more people opt for a speech driven computer interface as an alternative input method to the keyboard, and these speech recognition systems become more and widely used, both in the home and office environment, it has become necessary to qualify any potential risks of voice damage. This study reports about ongoing research that investigates acoustic changes in the voice, after use of a discrete speech recognition system. Acoustic analyses were carried out on two Swedish users of such a system. So far, for one of the users, two of the acoustic parameters under investigation that could be an indicator of vocal fatigue, show a significant difference directly before and after use of a speech recognition system.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-778"
  },
  "fearn98_icslp": {
   "authors": [
    [
     "Robert Alexander",
     "Fearn"
    ]
   ],
   "title": "The importance of F0 or voice pitch for perception of tonal language: simulations with cochlear implant speech processing strategies",
   "original": "i98_0666",
   "page_count": 4,
   "order": 782,
   "p1": "paper 0666",
   "pn": "",
   "abstract": [
    "Cochlear implants were initially distributed in Western countries. The speech processing strategies that have been developed to drive the implants provide detailed information about the spectral envelope and transients. This information is necessary to identify the phonemes of English and other Western languages. Cochlear implants have more recently been distributed in Eastern countries where tonal languages such as Mandarin and Cantonese are spoken. In addition to spectral envelope and transient information, tonal languages require finer resolution of pitch to distinguish different words. Speech processing strategies only provide the cochlear implant user with relatively low resolution of pitch. This study investigates the importance of voice pitch (F0) in detection of phonetically identical Cantonese words, which vary in pitch. Simulations of speech processing strategies were performed with a normal hearing subject and the results suggest that F0 is very important for correct identification of tonal words.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-779"
  },
  "brunnegaard98_icslp": {
   "authors": [
    [
     "Karin",
     "Brunnegaard"
    ],
    [
     "Katja",
     "Laakso"
    ],
    [
     "Lena",
     "Hartelius"
    ],
    [
     "Elisabeth",
     "Ahlsen"
    ]
   ],
   "title": "Assessing high-level language in individuals with multiple sclerosis: a pilot study",
   "original": "i98_0496",
   "page_count": 4,
   "order": 783,
   "p1": "paper 0496",
   "pn": "",
   "abstract": [
    "This study describes the development of a test battery to assess high-level language function in Swedish and a description of the test performances of a group of 9 individuals with multiple sclerosis (MS). The test battery included tasks such as repetition of long sentences, understanding of complicated logico-grammatical sentences, naming famous people, resolving ambiguities, recreating sentences, understanding metaphors, making inferences, defining words. The MS group included individuals with self-reported language problems as well as individuals without any such problems. Their performances were compared to a group of 7 control subjects with a Kruskal-Wallis one-way ANOVA which indicated significantly different total mean scores. Post hoc analysis with Mann-Whitney U-tests revealed that the group with self-reported language problems had significantly lower mean scores when compared to control subjects and to MS subjects without self-reported language problems. None of the language difficulties were detected by a standard aphasia test.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-780"
  },
  "hiki98_icslp": {
   "authors": [
    [
     "Shizuo",
     "Hiki"
    ],
    [
     "Kazuya",
     "Imaizumi"
    ],
    [
     "Yumiko",
     "Fukuda"
    ]
   ],
   "title": "Design of cochlear implant device for transmitting voice pitch information in speech sound of asian languages",
   "original": "i98_1091",
   "page_count": 4,
   "order": 784,
   "p1": "paper 1091",
   "pn": "",
   "abstract": [
    "Resolution of the fundamental frequency of speech sound required for the design of a speech processor of a cochlear implant device is investigated, with special regard to transmitting voice pitch information in Asian languages. Clinical application of the cochlear implant has spread rapidly in recent years to Asian countries where a variety of languages having different voice pitch information from English and other European languages are spoken. The perceptually acceptable area and required resolution of duration and fundamental frequency is estimated on a two-dimensional chart consisting of logarithmic time and frequency scales, based on the typical voice pitch contours of Japanese word accent and Chinese syllabic tone. As a result, it is shown that much finer quantizing and time sampling for the change in fundamental frequency is required compared with sentence intonation and emphasis common to other languages. It is also shown that the amount of information conveyed by combined use of lipreading with a cochlear implant is not sufficient for supplementing the voice pitch information. A possible way of transmitting such voice pitch information by transmission of the waveform of speech sound directly to the auditory area of cortex, where the waveform is reconstructed and voice pitch is extracted, is discussed.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-781"
  },
  "ho98b_icslp": {
   "authors": [
    [
     "Aileen K.",
     "Ho"
    ],
    [
     "John L.",
     "Bradshaw"
    ],
    [
     "Robert",
     "Iansek"
    ],
    [
     "Robin J.",
     "Alfredson"
    ]
   ],
   "title": "Abnormal volume-duration relationship in parkinsonian speech",
   "original": "i98_0011",
   "page_count": 3,
   "order": 785,
   "p1": "paper 0011",
   "pn": "",
   "abstract": [
    "Past studies on Parkinsonian speech have generally examined the parameters of speech separately. Thus volume and suprasegmental duration have largely been described independently of each other on the assumption that two measures are not related. This assumption was tested by manipulating intensity and examining the corresponding effect on duration. Twelve Parkinson's disease (PD) patients and twelve normal healthy controls read according to three conditions; as softly as possible, as loudly as possible, and with no volume instruction (at normal volume). Total Duration of reading (with pauses), and Net Duration (without pauses) were examined. For Net Duration, both groups were similar, and did not vary across volume conditions. PD patients, however, demonstrated decreased Total Duration as speech volume was increased. The abnormal Parkinsonian relationship is suggestive of a trade-off between the two parameters in order to achieve adequately loud reading, and may be explained by increased attention associated with increased effort when speaking louder.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-782"
  },
  "jo98c_icslp": {
   "authors": [
    [
     "Cheol-Woo",
     "Jo"
    ],
    [
     "Dae-Hyun",
     "Kim"
    ]
   ],
   "title": "Analysis of disordered speech signal using wavelet transform",
   "original": "i98_0691",
   "page_count": 4,
   "order": 786,
   "p1": "paper 0691",
   "pn": "",
   "abstract": [
    "In this paper a method to analyze pathological speech signal using wavelet transform is suggested. Pathological speech signal is commercially available pathological voice database and analyzed by the suggested method. Normal speech signal is also from the same database and analyzed as well. Then the results are compared to find the differences between normal and pathological speech. Three level wavelet transform is used. Normalized energy ratios between the levels and normalized peak-to-peak values are used as parameters. As a result, it was possible to distinguish between normal and pathological speech signal.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-783"
  },
  "kitazawa98_icslp": {
   "authors": [
    [
     "Shigeyoshi",
     "Kitazawa"
    ],
    [
     "Hiroyuki",
     "Kirihata"
    ],
    [
     "Tatsuya",
     "Kitamura"
    ]
   ],
   "title": "Multi-channel pulsation strategy for electric stimulation of cochlea",
   "original": "i98_1036",
   "page_count": 4,
   "order": 787,
   "p1": "paper 1036",
   "pn": "",
   "abstract": [
    "This paper describes a speech coding strategy for a cochlear implant system assuming a Nucleus Cochlear Implant receiver stimulator. Speech processor converts input speech into a series of stimulation electrode position and stimulation current intensities. This process can be optimized with a decomposing process of an acoustic signal into a given set of impulse responses corresponding to a set of electrode channels. An error minimization algorithm can find a optimal stimulation sequence that minimizes distortion of transferred speech and maximize transferred phonological information as well as sound qualities. Re-synthesized sound quality was qualitatively evaluated. Environmental sound can also be recognizable with this method.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-784"
  },
  "agelfors98_icslp": {
   "authors": [
    [
     "Eva",
     "Agelfors"
    ],
    [
     "Jonas",
     "Beskow"
    ],
    [
     "Martin",
     "Dahlquist"
    ],
    [
     "Bj√∂rn",
     "Granstr√∂m"
    ],
    [
     "Magnus",
     "Lundeberg"
    ],
    [
     "Karl-Erik",
     "Spens"
    ],
    [
     "Tobias",
     "√ñhman"
    ]
   ],
   "title": "Synthetic faces as a lipreading support",
   "original": "i98_0362",
   "page_count": 4,
   "order": 788,
   "p1": "paper 0362",
   "pn": "",
   "abstract": [
    "In the Teleface project the possibility to use a synthetic face as a visual telephone communication aid for hearing impaired persons is evaluated. In an earlier study, NH, a group of normal hearing persons participated. This paper describes the results of two multimodal intelligibility tests with hearing impaired persons, where the additional information provided by a synthetic as well as a natural face is evaluated. In a first round with hearing impaired persons, HI:1, twelve subjects were presented with VCV-syllables and \"everyday sentences\" together with a questionnaire. The intelligibility score for the VCV-syllables presented as audio alone, was 30%. When adding a synthetic face the score improved to 55% and when instead adding the natural face it was 58%. In a second round, HI:2, fifteen hearing impaired persons were presented with the sentence material and a questionnaire. The audio track was filtered to simulate telephone bandwidth. The intelligibility score for the audio only condition was 57% correctly identified keywords. Together with a synthetic face it was 66% and with a natural face 83%. Answers in the questionnaires were collected and analysed. The general subjective rating of the synthetic face was positive and the subjects would like to use such a type of aid if available.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-785"
  },
  "martin98_icslp": {
   "authors": [
    [
     "Lois",
     "Martin"
    ],
    [
     "John",
     "Bench"
    ]
   ],
   "title": "Predicting language scores from the speech perception scores of hearing-impaired children",
   "original": "i98_1006",
   "page_count": 4,
   "order": 789,
   "p1": "paper 1006",
   "pn": "",
   "abstract": [
    "The ability to understand speech in 27 hearing impaired children was assessed using the BKB/A Picture Related Sentence test for children. The mean sentence score for the group was 72% (range 100-18%). Language scores (CELF-R) and Verbal Scale IQ (WISC-R) scores were significantly below the norm (72.8 and 89.2 respectively). Performance Scale IQ scores were slightly above the norm (106.3). Sentences scores were correlated significantly with language score (r = 0.49). Further investigation showed that the predictability of language scores could be improved when sensation level was taken into account. Sensation level was negatively correlated with language scores (r = - 0.51), demonstrating that children with better language abilities perceived speech at relatively lower intensity levels. The observed sensation levels from the group were compared with the expected levels for normally hearing children. This difference measure yielded a correlation coefficient of - 0.73 with language scores.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-786"
  },
  "skljarov98_icslp": {
   "authors": [
    [
     "Oleg P.",
     "Skljarov"
    ]
   ],
   "title": "Content-independent duration model on categories of voice and unvoice segments",
   "original": "i98_1149",
   "page_count": 4,
   "order": 790,
   "p1": "paper 1149",
   "pn": "",
   "abstract": [
    "Trying to understand the experimental data on segmentation of a speech signal by a principle \"Voice/Unvoice\" has led us to the hypothesis about a pair of logistical dependence between durations of these segments. The segmentation was carried out with the help of the computer program working in quasi real time. The hypothesis about logistic recurrent dependence for sequence of segments durations has allowed to make a conclusion about quasi rhythmical organization of this sequence. With the help of offered recurrent dependencies it is possible to explain statistical peculiarities of speech behaviour of stutterers in comparison with normal speech behaviour. These logistic dependencies were confirmed by direct experimental data. The assumption of origins of specified rhythm is made. These origins are hidden at the level of control of speech production and perception. Is shown, that the chaotic nature of offered dynamics of formation of large-scale temporary structure allows to enter concept of the information into consideration by a natural way.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-787"
  },
  "soltanifarani98_icslp": {
   "authors": [
    [
     "Ali-Asghar",
     "Soltani-Farani"
    ],
    [
     "Edward H.S.",
     "Chilton"
    ],
    [
     "Robin",
     "Shirley"
    ]
   ],
   "title": "Dynamical spectrogram, an aid for the deaf",
   "original": "i98_0438",
   "page_count": 4,
   "order": 791,
   "p1": "paper 0438",
   "pn": "",
   "abstract": [
    "Visual perception of speech through spectrogram reading has long been a subject of research, as an aid for the deaf or hearing impaired. Attributing the lack of success in this type of visual aid mainly to the static form of information presented by the spectrograms, this paper proposes a system of dynamic visualisation for speech sounds. This system samples a high resolved, auditory-based spectrogram, with a window of 20 milliseconds duration, so that exploiting the periodicity of the input sound, it produces a phase-locked sequence of images. This sequence is then animated at a rate of 50 images per second to produce a movie-like image displaying both the time-varying and time-independent information of the underlying sound. Results of several preliminary experiments for evaluation of the potential usefulness of the system for the deaf, undertaken by normal-hearing subjects, support the quick learning and persistence of the gestures for small sets of single words and motivate further investigations.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-788"
  },
  "varley98_icslp": {
   "authors": [
    [
     "Rosemary A.",
     "Varley"
    ],
    [
     "Sandra P.",
     "Whiteside"
    ]
   ],
   "title": "Evidence of dual-route phonetic encoding from apraxia of speech: implications for phonetic encoding models",
   "original": "i98_0151",
   "page_count": 4,
   "order": 792,
   "p1": "paper 0151",
   "pn": "",
   "abstract": [
    "Contemporary psycholinguistic models suggest that there may be dual routes operating in phonetic encoding: a direct route which uses stored syllabic units, and an indirect route which relies on the on-line assembly of sub-syllabic units. The more computationally efficient direct route is more likely to be used for high frequency words, while the indirect route is most likely to be used for novel or low frequency words. We suggest that the acquired neurological disorder of apraxia of speech (AOS), provides a window to speech encoding mechanisms and that the disorder represents an impairment of direct route encoding mechanisms and, therefore, a reliance on indirect mechanisms. We report an investigation of the production of high and low frequency words across three subject groups: non-brain damaged control (NBDC, N=3); brain damaged control (BDC, N=3) and speakers with AOS (N=4). The results are presented and discussed within the dual-route phonetic encoding hypothesis.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-789"
  },
  "cheesman98_icslp": {
   "authors": [
    [
     "M. F.",
     "Cheesman"
    ],
    [
     "K. L.",
     "Smilsky"
    ],
    [
     "T. M.",
     "Major"
    ],
    [
     "F.",
     "Lewis"
    ],
    [
     "L. M.",
     "Boorman"
    ]
   ],
   "title": "Speech communication profiles across the adult lifespan: persons without self-identified hearing impairment",
   "original": "i98_0841",
   "page_count": 4,
   "order": 793,
   "p1": "paper 0841",
   "pn": "",
   "abstract": [
    "A sample of 209 adults ranging from 20 to 79 years of age were studied to measure speech communication profiles as a function of age in persons who did not identify themselves as hearing impaired. The study was conducted in order to evaluate age-related speech perception abilities and communication profiles in a population who do not esent for hearing assessment and who are not included in census statistics as having hearing problems. Audiometric assessment, demographic and hearing history self-reports, speech reception thresholds, consonant discrimination perception in quiet and noi , and the Communication Profile for the Hearing Impaired (CPHI) were the instruments used to develop speech communication profiles. Hearing performance decreased with increased age. However, despite self-reports of no hearing impairment, many subjects ov age 50 had audiometric thresholds that indicated hearing impairment. The responses to the CPHI were correlated to audiometric thresholds, but also to the age of the respondent, when hearing thresholds had been controlled statistically. A comparison of C I responses from this study and that of two other samples in clinical populations revealed only slightly different patterns of behaviour in the present sample when confronted with communication difficulties.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-790"
  },
  "barry98_icslp": {
   "authors": [
    [
     "William J.",
     "Barry"
    ]
   ],
   "title": "Time as a factor in the acoustic variation of schwa",
   "original": "i98_0554",
   "page_count": 4,
   "order": 794,
   "p1": "paper 0554",
   "pn": "",
   "abstract": [
    "Schwa is commonly regarded phonologically as an unspecified vowel, and phonetically as \"targetless\", i.e., as the product of its context. Methods used in studies investigating the issue are discussed, and it is argued that the evidence for targetlessness is unconvincing. An experiment is presented in which schwa is produced in symmetrical vowel and consonant contexts under varying speech rate conditions. It is shown that the contextual influence on schwa depends on its duration, a result that is not compatible with the concept of a targetless vowel. This conclusion is discussed in relation to the possible phonological status of the target specification.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-791"
  },
  "boshoff98_icslp": {
   "authors": [
    [
     "Hendrik F. V.",
     "Boshoff"
    ],
    [
     "Elizabeth C.",
     "Botha"
    ]
   ],
   "title": "On the structure of vowel space: a genealogy of general phonetic concepts",
   "original": "i98_0969",
   "page_count": 4,
   "order": 795,
   "p1": "paper 0969",
   "pn": "",
   "abstract": [
    "This paper gives an overview and an analysis of representative vowel space depictions found in the phonetic literature of the past four centuries. A visual representation of 24 selected structural proposals with time is given in poster form, after translation to a common notation. A first-order approach is developed by which the distance between any pair of vowel schemes may be measured. Using these distances and other considerations, trends are detected and major innovations identified. An attempt is then made to trace the genealogy of five general phonetic concepts concerning the structure of vowel space: discretizing the continuum, the assumption of symmetry, triangular vs. square arrangements, the development of 'roundedness' as an independent dimension and the treatment of 'central' vowels. Such a historical perspective is deemed necessary for a full appreciation and utilization of the current vowel schemes. The poster and other relevant files are available at http://www.ee.up.ac.za/hendrik/icslp5.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-792"
  },
  "lecuit98_icslp": {
   "authors": [
    [
     "V√©ronique",
     "Lecuit"
    ],
    [
     "Didier",
     "Demolin"
    ]
   ],
   "title": "The relationship between intensity and subglottal pressure with controlled pitch",
   "original": "i98_0530",
   "page_count": 4,
   "order": 796,
   "p1": "paper 0530",
   "pn": "",
   "abstract": [
    "This paper present a study of the relationship between subglottal pressure (SGP) and intensity of the speech signal in the case of sustained French oral vowels with controlled pitch. The corpus is based on a male and a female subjects, the SGP is measured directly by tracheal puncture. The results show that the relationship between intensity and subglottal pressure varies when one considers the vowel or the pitch parameter. This experiment is useful in the framework of speech production model design. These preliminary results emphasize the need to investigate thoroughly the relationship between all the parameters involved and to do s o on abundant and accurate experimental data.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-793"
  },
  "soquet98_icslp": {
   "authors": [
    [
     "Alain",
     "Soquet"
    ],
    [
     "V√©ronique",
     "Lecuit"
    ],
    [
     "Thierry",
     "Metens"
    ],
    [
     "Bruno",
     "Nazarian"
    ],
    [
     "Didier",
     "Demolin"
    ]
   ],
   "title": "Segmentation of the airway from the surrounding tissues on magnetic resonance images: a comparative study",
   "original": "i98_0531",
   "page_count": 4,
   "order": 797,
   "p1": "paper 0531",
   "pn": "",
   "abstract": [
    "Magnetic Resonance Imaging techniques are uniquely attractive in their ability to provide an extensive body of information on the vocal tract geometry. Once the images are acquired, they must be further processed in order to segment the airway from the surrounding tissues, so as to locate the air passage. This problem has been addressed in several ways i n the literature. In this paper, we carry out a comparative study of different approaches to the same body of data in order to assess the accuracy of the different methods. It is shown that the different methods present small average error and large error distribution.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-794"
  },
  "dusan98_icslp": {
   "authors": [
    [
     "Sorin",
     "Dusan"
    ],
    [
     "Li",
     "Deng"
    ]
   ],
   "title": "Recovering vocal tract shapes from MFCC parameters",
   "original": "i98_0367",
   "page_count": 4,
   "order": 798,
   "p1": "paper 0367",
   "pn": "",
   "abstract": [
    "Recovering vocal tract shapes from the speech signal is a well known inversion problem of transformation from the articulatory system to speech acoustics. Most of the studies on this problem in the past have been focused on vowels. There have not been general methods effective for recovering the vocal tract shapes from the speech signal for all classes of speech sounds. In this paper we describe our attempt towards speech inverse mapping by using the mel-frequency cepstrum coefficients to represent the acoustic parameters of the speech signal. An inversion method is developed based on Kalman filtering and a dynamic-system model describing the articulatory motion. This method uses an articulatory-acoustic codebook derived from Maeda's articulatory model.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-795"
  },
  "esling98b_icslp": {
   "authors": [
    [
     "John H.",
     "Esling"
    ],
    [
     "Jocelyn",
     "Clayards"
    ],
    [
     "Jerold A.",
     "Edmondson"
    ],
    [
     "Qiu",
     "Fuyuan"
    ],
    [
     "Jimmy G.",
     "Harris"
    ]
   ],
   "title": "Quantification of pharyngeal articulations using measurements from laryngoscopic images",
   "original": "i98_0618",
   "page_count": 4,
   "order": 799,
   "p1": "paper 0618",
   "pn": "",
   "abstract": [
    "One difficulty in the physical observation of articulatory structures in the pharynx is the reliability of measurements for comparative purposes. The objective of the present approach is to review and explore techniques of identifying and measuring degrees of adjustment of the pharyngeal articulatory mechanism in the production of auditorily controlled settings. Anatomical landmarks are identified on computer images transferred from laryngoscopic videotapes, and measurements are taken of dimensions defined by the configuration of the pharyngeal articulators. Comparisons are taken across linguistically contrastive consonants and vowels, where the activity of the laryngeal sphincter accompanies tongue retraction and larynx raising, and implications are drawn for the measurement of voice quality settings. Measurements of the \"tense,\" raised-larynx series of Yi (Nosu) are illustrated.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-796"
  },
  "fon98_icslp": {
   "authors": [
    [
     "Janice",
     "Fon"
    ]
   ],
   "title": "Variance and invariance in speech rate as a reflection of conceptual planning",
   "original": "i98_0842",
   "page_count": 5,
   "order": 800,
   "p1": "paper 0842",
   "pn": "",
   "abstract": [
    "This study investigates the variance and invariance in speech rate as a reflection of conceptual planning and cognitive rhythm. A four-frame comic strip was used to elicit speech and low-pass smoothing was done afterwards to filter out high frequency noise. Results showed that subjects invariably planned narration in terms of story plots. Variance lies in the way subjects synchronize planning and execution stages. Some tended to start the execution stage before the planning stage ends while others were inclined to speak only after macroplanning was done. Lexical retrieval failure is one of the main causes for disruptive story-plot-based temporal cycle.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-797"
  },
  "fujimoto98_icslp": {
   "authors": [
    [
     "Masako",
     "Fujimoto"
    ],
    [
     "Emi",
     "Murano"
    ],
    [
     "Seiji",
     "Niimi"
    ],
    [
     "Shigeru",
     "Kiritani"
    ]
   ],
   "title": "Correspondence between the glottal gesture overlap pattern and vowel devoicing in Japanese",
   "original": "i98_1105",
   "page_count": 3,
   "order": 801,
   "p1": "paper 1105",
   "pn": "",
   "abstract": [
    "Correspondence between the glottal opening gesture pattern and vowel devoicing in Japanese was examined using PGG with special reference to the pattern of glottal gesture overlap and blending into the neighboring vowel. The results showed that most of the tokens demonstrated either a single glottal opening pattern with a devoiced vowel, or a double glottal opening with a voiced vowel during /CiC/ sequences as generally expected. Some tokens, however, showed a double glottal opening with a devoiced vowel, or a single glottal opening with a partially voiced vowel. From the viewpoint of gestural overlap analysis of vowel devoicing, an intermediate process of gestural overlap may explain the occurrence of the case in which the vowel was devoiced and showed a double phase opening. Nevertheless, the presence of a partially voiced vowel with a single opening phase clearly shows the complexity of vowel devoicing in Japanese, since there are possibly two different patterns of glottal opening (single phase and double phase), which could be observed in PGG analysis, in utterances with partially voiced vowels.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-798"
  },
  "fujisawa98_icslp": {
   "authors": [
    [
     "Yukiko",
     "Fujisawa"
    ],
    [
     "Nobuaki",
     "Minematsu"
    ],
    [
     "Seiichi",
     "Nakagawa"
    ]
   ],
   "title": "Evaluation of Japanese manners of generating word accent of English based on a stressed syllable detection technique",
   "original": "i98_1147",
   "page_count": 4,
   "order": 802,
   "p1": "paper 1147",
   "pn": "",
   "abstract": [
    "While English word accent is linguistically almost the same as Japanese one, the word accent acoustically differs between the two languages. This fact lets us easily suppose that Japanese learners tend to generate the word accent in a manner of not English but Japanese. We propose two methods for automating the detection of the generated word accent and the evaluation of how it is generated. By using context-sensitive HMMs, stressed and unstressed syllables were modeled separately for their structure and for their position in a word. In the matching process, weighting factors were multiplied with several likelihood scores derived from different acoustic parameters. The optimal combination of the factors for the detection can be thought to reflect each speaker's own manner of the accent generation. The analysis of the optimal combination showed different tendencies between Japanese and native speakers, which mainly accorded with findings in previous studies on English teaching.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-799"
  },
  "ishihara98b_icslp": {
   "authors": [
    [
     "Shunichi",
     "Ishihara"
    ]
   ],
   "title": "Independence of consonantal voicing and vocoid F0 perturbation in English and Japanese",
   "original": "i98_0630",
   "page_count": 4,
   "order": 803,
   "p1": "paper 0630",
   "pn": "",
   "abstract": [
    "Data collected from Japanese and English showed that both phonetically fully voiced and (partially) devoiced allophones of /d/ have very similar perturbatory effect on the F0 of the following vowel. It is considered, therefore, that the phonetic voicing of /d/ (periodicity during the closure) is not clearly correlated with lower levels of F0 on the following vowel. Although the F0 perturbation may be caused by some aspects in the production of the preceding stop which is not necessarily manifested in actual vocal cord vibration, this result indicates that there is still a possibility that people may deliberately control the F0 of the following vowel as an additional cue to the phonological difference between voiceless and voiced stop consonants.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-800"
  },
  "jurafsky98_icslp": {
   "authors": [
    [
     "Daniel",
     "Jurafsky"
    ],
    [
     "Alan",
     "Bell"
    ],
    [
     "Eric",
     "Fosler-Lussier"
    ],
    [
     "Cynthia",
     "Girand"
    ],
    [
     "William",
     "Raymond"
    ]
   ],
   "title": "Reduction of English function words in switchboard",
   "original": "i98_0669",
   "page_count": 4,
   "order": 804,
   "p1": "paper 0669",
   "pn": "",
   "abstract": [
    "The causes of pronunciation reduction in 8458 occurrences of ten frequent English function words in a four-hour sample from conversations from the Switchboard corpus were examined. Using ordinary linear and logistic regression models, we examined the length of the words, the form of their vowel (basic, full, or reduced), and final obstruent deletion. For %words with a final obstruent, whether it was present or not. For all of these we found strong, independent effects of speaking rate, predictability, the form of the following word, and %following disfluencies symptomatic of planning problem disfluencies. The results bear on issues in speech recognition, models of speech production, and conversational analysis.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-801"
  },
  "kim98e_icslp": {
   "authors": [
    [
     "Hee-Sun",
     "Kim"
    ]
   ],
   "title": "Duration compensation in non-adjacent consonant and temporal regularity",
   "original": "i98_0720",
   "page_count": 4,
   "order": 805,
   "p1": "paper 0720",
   "pn": "",
   "abstract": [
    "This study reports a case of consonant-induced duration compensation within a higher phonological unit in Korean. The main finding of this study is that longer duration in a consonant is partly compensated by shorter duration even in non-adjacent consonants. Results of experiments revealed that the shortening in non-adjacent consonants is a subsidiary compensation process to maintain constant duration at the larger domain than the syllable. Based on these observations, an attempt at modeling speech timing is presented with the perspective that speech production system is a process having multiple simultaneous tasks and the optimal output is obtained by a compromise between their objectives.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-802"
  },
  "mori98b_icslp": {
   "authors": [
    [
     "Keisuke",
     "Mori"
    ],
    [
     "Yorinobu",
     "Sonoda"
    ]
   ],
   "title": "Relationship between lip shapes and acoustical characteristics during speech",
   "original": "i98_1029",
   "page_count": 4,
   "order": 806,
   "p1": "paper 1029",
   "pn": "",
   "abstract": [
    "A quantitative knowledge of the articulatory characteristics is necessary for understanding the dynamics of speech production. Accordingly, it is expected that observations of the shape of mouth will provide useful data for the study on articulatory behaviors in speech production. This paper describes characteristic changes in the shape of the mouth on the basis of processed image data taken by high-speed video recorder, and studies recognition tests jointly using articulatory behavior of lips and sound pattern during speech. The speech material used in this paper were nonsense words of form /eCVCe/ (V: a, i, u, e, o, C: p, b, m). Subject were four adult males, all of them were native speaker of Japanese. In recognition of the consonant, consonant was more closely related with shape pattern than formant pattern. These results show effect of consonant (/p/, /b/, and /m/) on the middle vowel of utterance.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-803"
  },
  "motoki98_icslp": {
   "authors": [
    [
     "Kunitoshi",
     "Motoki"
    ],
    [
     "Hiroki",
     "Matsuzaki"
    ]
   ],
   "title": "A model to represent propagation and radiation of higher-order modes for 3-d vocal-tract configuration",
   "original": "i98_0677",
   "page_count": 4,
   "order": 807,
   "p1": "paper 0677",
   "pn": "",
   "abstract": [
    "For the representation of acoustic characteristics of three-dimensional vocal-tract shapes, it is necessary to consider the effects of higher-order modes. This paper proposes an acoustic model of the vocal-tract which incorporates the coupling of the higher-order modes, including both propagative and evanescent modes. A cascaded structure of acoustic tubes connected asymmetrically is introduced as a physical approximation of the vocal-tract. The acoustic characteristics, which are dependent not only on the vocal-tract area function but also on the vocal-tract configuration, can be investigated by the proposed model. Preliminary results of numerical computations for relatively simple configurations suggest that additional resonances at frequencies above 4.3kHz are formed by the propagative higher-order modes, while those at frequencies below 3 kHz are influenced by the evanescent higher-order modes. These results are also confirmed by the FEM simulations.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-804"
  },
  "niikawa98_icslp": {
   "authors": [
    [
     "Takuya",
     "Niikawa"
    ],
    [
     "Masafumi",
     "Matsumura"
    ],
    [
     "Takashi",
     "Tachimura"
    ],
    [
     "Takeshi",
     "Wada"
    ]
   ],
   "title": "FEM analysis of aspirated air flow in three-dimensional vocal tract during fricative consonant phonation",
   "original": "i98_0509",
   "page_count": 4,
   "order": 808,
   "p1": "paper 0509",
   "pn": "",
   "abstract": [
    "This paper deals with estimations of aspirated air flow in a three-dimensional vocal tract during fricative consonant phonation using the Finite Element Method (FEM). The shape of the 3-D vocal tract during phonation of fricative consonant /s/ is reconstructed from 32 coronal Magnetic Resonance (MR) images. MR images of the dental crown that contains a small amount of water were obtained using a dental crown plate. A 3-D FEM vocal tract model is formed so that the number of elements is 28686, the number of nodes is 7010, and a rigid wall constitutes the vocal tract wall. Results showed that the flow rate was high at the narrow space made between the upper central incisors and the tongue surface. An electric equivalent circuit for fricative consonant phonation was designed in consideration of the location of the noise source.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-805"
  },
  "okadome98_icslp": {
   "authors": [
    [
     "Takesi",
     "Okadome"
    ],
    [
     "Tokihiko",
     "Kaburagi"
    ],
    [
     "Masaaki",
     "Honda"
    ]
   ],
   "title": "Trajectory formation of articulatory movements for a given sequence of phonemes",
   "original": "i98_0118",
   "page_count": 4,
   "order": 809,
   "p1": "paper 0118",
   "pn": "",
   "abstract": [
    "The method proposed here produces trajectories of articulatory movements based on a kinematic triphone model and the minimum-jerk model. The kinematic triphone model, which is constructed from articulatory data obtained in the experiments through the use of a magnetic sensor system, is characterized by three kinematic features for a triphone and intervals between two successive phonemes in the triphone. After extracting a kinematic feature for a phoneme in a given sentence, for each point on the articulator, the minimum-jerk trajectory which coincides with the extremum of the time integral of the square of the magnitude of jerk of the point is formulated, which requires only linear computation. The method predicts both the qualitative features and the quantitative details experimentally observed.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-806"
  },
  "shih98b_icslp": {
   "authors": [
    [
     "Chilin",
     "Shih"
    ],
    [
     "Bernd",
     "M√∂bius"
    ]
   ],
   "title": "Contextual effects on voicing profiles of German and Mandarin consonants",
   "original": "i98_0176",
   "page_count": 4,
   "order": 810,
   "p1": "paper 0176",
   "pn": "",
   "abstract": [
    "We present a study of the voicing profiles of consonants in Mandarin Chinese and German. The voicing profile is defined as the frame-by-frame voicing status of a speech sound in continuous speech. We are particularly interested in discrepancies between the phonological voicing status of a speech sound and its actual phonetic realization in connected speech. We further examine the contextual factors that cause voicing variations and test the cross-language validity of these factors. The result can be used to improve speech synthesis, and to refine phone models to enhance the performance of automatic speech segmentation and recognition.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-807"
  },
  "lundberg98_icslp": {
   "authors": [
    [
     "Andrew J.",
     "Lundberg"
    ],
    [
     "Maureen",
     "Stone"
    ]
   ],
   "title": "Reconstructing the tongue surface from six cross-sectional contours: ultrasound data",
   "original": "i98_0865",
   "page_count": 4,
   "order": 811,
   "p1": "paper 0865",
   "pn": "",
   "abstract": [
    "This paper presents a method for reconstructing 3D tongue surfaces during speech from 6 cross-sectional contours. The method reduces the dimensionality of the tongue surface and maintains highly accurate reproduction of local deformation features. This modification is an essential step if multi-plane tongue movements are to be reconstructed practically into tongue surface movements. Six cross-sectional contours were used to reconstruct 3D tongue surfaces and these were compared to reconstructions from 60 contours. The best set of 6 cross-sectional contours was determined from an optimized set of 6 midsagittal points. These points had been optimized to predict the midsagittal contour. Errors and reconstruction coverage for the midsagittal optimization were comparable to those resulting from an optimization over the entire surface, indicating this was an adequate method for calculating a sparse data set for use in reconstructing 3D tongue surface behavior.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-808"
  },
  "terao98_icslp": {
   "authors": [
    [
     "Yasushi",
     "Terao"
    ],
    [
     "Tadao",
     "Murata"
    ]
   ],
   "title": "Articulability of two consecutive morae in Japanese speech production: evidence from sound exchange errors in spontaneous speech",
   "original": "i98_0435",
   "page_count": 4,
   "order": 812,
   "p1": "paper 0435",
   "pn": "",
   "abstract": [
    "In the present study, we would like to discuss how the articulability of two consecutive morae plays an important role in the stage at which sound exchange errors occur. Our assumption is based on the analysis of Japanese sound exchange error data which have been collected from the spontaneous speech of adults and infants. We hypothesized that /dara/ in /kadara/(error form) was more articulable than /rada/ in /karada/(correct form). Three experiments were carried out to confirm Phonological/phonetic characteristics of the unit were shown through the results of experiments and some related observations.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-809"
  },
  "vilain98_icslp": {
   "authors": [
    [
     "Anne",
     "Vilain"
    ],
    [
     "Christian",
     "Abry"
    ],
    [
     "Pierre",
     "Badin"
    ]
   ],
   "title": "Coarticulation and degrees of freedom in the elaboration of a new articulatory plant: GENTIANE",
   "original": "i98_0489",
   "page_count": 4,
   "order": 813,
   "p1": "paper 0489",
   "pn": "",
   "abstract": [
    "A new articulatory model GENTIANE, elaborated from an X-ray film built on a corpus of VCV sequences performed by a skilled French speaker, enabled us to analyse coarticulation of main consonant types in vowel contexts from a degrees of freedom approach. The data displayed an overall coarticulatory versatility, except for an absolute invariance in the labio-dental constriction point. For consonant types recruiting the tongue, the variance explained by the degrees of freedom of the model evidenced specific compensation strategies: tongue tip compensation betokened the common coronal status of the dental plosive and the post-alveolar fricative; whereas tongue dorsum compensation signed the dorsal nature of the velar plosive.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-810"
  },
  "wakumoto98_icslp": {
   "authors": [
    [
     "Masahiko",
     "Wakumoto"
    ],
    [
     "Shinobu",
     "Masaki"
    ],
    [
     "Kiyoshi",
     "Honda"
    ],
    [
     "Toshikazu",
     "Ohue"
    ]
   ],
   "title": "A pressure sensitive palatography: application of new pressure sensitive sheet for measuring tongue-palatal contact pressure",
   "original": "i98_0457",
   "page_count": 4,
   "order": 814,
   "p1": "paper 0457",
   "pn": "",
   "abstract": [
    "This paper describes a new method for measuring the tongue-palatal contact pressure using a thin pressure sensor and its application for speech research. The new pressure sensor is composed of thin pressure sensitive ink whose electrical resistance is proportional to the physical forces applied to the sensor. Several sensors were arranged on the surface of the palatal plate. This setup was used to measure the tongue pressure toward the hard palate during closure for Japanese stop consonants [t] and [d]. Results obtained from 10 Japanese subjects showed the tongue-palatal contact pressure for [t] to be stronger than that for [d]. In addition, the sensors placed on the non-contact area showed no pressure change, indicating negligible effects of intra-oral air pressure during consonantal closure.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-811"
  },
  "whiteside98d_icslp": {
   "authors": [
    [
     "Sandra P.",
     "Whiteside"
    ],
    [
     "Rosemary A.",
     "Varley"
    ]
   ],
   "title": "Dual-route phonetic encoding: some acoustic evidence",
   "original": "i98_0150",
   "page_count": 4,
   "order": 815,
   "p1": "paper 0150",
   "pn": "",
   "abstract": [
    "Contemporary psycholinguistic models suggest that there may be two possible routes in phonetic encoding: a 'direct' route which uses stored syllabic units, and an 'indirect' route which relies on the on-line assembly of sub-syllabic units. The computationally more efficient direct route is likely to be used for high frequency words, whereas the indirect route is most likely to be used for novel or low frequency words. This paper presents some acoustic evidence that suggests that there may be dual routes operating in phonetic encoding. The data reported suggest that a group of normal speakers may be employing different routes in the phonetic encoding of high and low frequency words elicited via a repetition task. The evidence is presented and discussed within the framework of a dual-route hypothesis, and in light of other acoustic evidence reported in the literature.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-812"
  },
  "zellner98_icslp": {
   "authors": [
    [
     "Brigitte",
     "Zellner"
    ]
   ],
   "title": "Fast and slow speech rate: a characterisation for French",
   "original": "i98_0822",
   "page_count": 4,
   "order": 816,
   "p1": "paper 0822",
   "pn": "",
   "abstract": [
    "Many phonetic studies have shown that changes in speech rate have numerous effects at various levels of the temporal structure. This observation is reinforced by a verification with speech synthesis. Changing the number of syllables per second is not a satisfactory manner of creating natural-sounding fast or slow synthetic speech. A systematic comparison of sentences read at two speech rates by a highly fluent French speaker allows a ranking of various mechanisms used to slow down speech. Pausing and producing additional syllables transform the phonological structure of utterances since they impede interlexical binding. It is claimed that knowing the degree of this interlexical binding allows a better characterisation of speech rate changes, and then a better generation of synthetic rhythms. Finally, the expected relationship between lengthening of speech units and pausing was not confirmed in our results. This suggests that the theory on slowing down speech needs revision.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-813"
  },
  "ramesh98b_icslp": {
   "authors": [
    [
     "Padma",
     "Ramesh"
    ],
    [
     "Chin-Hui",
     "Lee"
    ],
    [
     "Biing-Hwang",
     "Juang"
    ]
   ],
   "title": "Context dependent anti subword modeling for utterance verification",
   "original": "i98_0880",
   "page_count": 4,
   "order": 817,
   "p1": "paper 0880",
   "pn": "",
   "abstract": [
    "Utterance verification is used in spoken language dialog systems to reject the speech that does not belong to the task and to correctly recognize the sentences that do. Current verification systems use context dependent (CD) or context independent (CI) subword models and CI anti-subword models. We propose many methods of modeling the CD anti-subword models. We have compared these anti-models and show that the anti-models with the same context have the most separation between the speech that contains the subword and the speech that does not contain the subword. We have also conducted recognition/verification experiments with a two pass verifier and two one pass verification systems to compare the different types of anti-subword models. Our results show that the same context anti-subword models have the best recognition/verification performance.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-814"
  },
  "dolfing98_icslp": {
   "authors": [
    [
     "J. G. A.",
     "Dolfing"
    ],
    [
     "Andreas",
     "Wendemuth"
    ]
   ],
   "title": "Combination of confidence measures in isolated word recognition",
   "original": "i98_0481",
   "page_count": 4,
   "order": 818,
   "p1": "paper 0481",
   "pn": "",
   "abstract": [
    "In the context of command-and-control applications, we exploit confidence measures in order to classify utterances into two categories: utterances within the vocabulary which are recognized correctly, and other (out-of-vocabulary= OOV and misrecognized) utterances. We investigate the classification error rate (CER) of several classes of confidence measures and transformations based on a database containing 3345 utterances by 50 male and female individuals, employing data-independent and data-dependent measures. The transformations we investigated include mapping to single confidence measures, LDA-transformed measures, and other linear combinations of these measures. These combinations are computed by means of neural networks trained with Bayes-optimal, and with Gardner-Derrida-optimal criteria. Compared to a recognition system without confidence measures, the selection of (various combinations of) confidence measures, and the selection of suitable neural network architectures and training methods, continuously improves the CER from 16.7% to 6.6% (-60% relative). Furthermore, a linear perceptron generalizes better than a non-linear backpropagation network.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-815"
  },
  "willett98b_icslp": {
   "authors": [
    [
     "Daniel",
     "Willett"
    ],
    [
     "Andreas",
     "Worm"
    ],
    [
     "Christoph",
     "Neukirchen"
    ],
    [
     "Gerhard",
     "Rigoll"
    ]
   ],
   "title": "Confidence measures for HMM-based speech recognition",
   "original": "i98_0525",
   "page_count": 4,
   "order": 819,
   "p1": "paper 0525",
   "pn": "",
   "abstract": [
    "In this paper, we describe our work on the field of confidence measures for HMM-based speech recognition. Confidence measures are a means of estimating the recognition reliability for single words of the recognizer output. The possible applications of such measures are manifold. We present our experiments with well known approaches and propose some new ones. Particularly, we propose to combine the mere acoustical measures with language model-based ones for continuous speech recognition that involves a stochastic language model. This slightly improves the acoustical measures and preserves their advantage of being computationally very cheap. Experiments are carried out on a German isolated word recognition system and on continuous speech recognition systems for the Resource Management database and the Wall Street Journal WSJ0 task.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-816"
  },
  "jiang98b_icslp": {
   "authors": [
    [
     "Li",
     "Jiang"
    ],
    [
     "Xuedong",
     "Huang"
    ]
   ],
   "title": "Vocabulary-independent word confidence measure using subword features",
   "original": "i98_0625",
   "page_count": 4,
   "order": 820,
   "p1": "paper 0625",
   "pn": "",
   "abstract": [
    "This paper discusses how to compute word-level confidence measures based on sub-word features for large-vocabulary speaker-independent speech recognition. The performance of confidence measure using features at word, phone and senone level is experimentally studied. A framework of transformation function based system using sub-word features is proposed for high performance confidence estimation. In this system, discriminative training is used to optimize the parameters of the transformation function. In comparison to the baseline, experiments show that the proposed system reduces the equal error rate by 15%, with up to 40% false acceptance error reduction at various fixed false rejection rate. The combination of multiple features under the proposed framework is also discussed.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-817"
  },
  "lin98c_icslp": {
   "authors": [
    [
     "Qiguang",
     "Lin"
    ],
    [
     "Subrata",
     "Das"
    ],
    [
     "David",
     "Lubensky"
    ],
    [
     "Michael",
     "Picheny"
    ]
   ],
   "title": "A new confidence measure based on rank-ordering subphone scores",
   "original": "i98_0806",
   "page_count": 4,
   "order": 821,
   "p1": "paper 0806",
   "pn": "",
   "abstract": [
    "This paper presents a new approach to measuring how confidently a word has been correctly recognized, or confidence measure. The approach consists of three major steps: (1) standard decoding; (2) forced Viterbi alignment (needed for stack decoders); and (3) rank-ordering of the subphone scores. More specifically, from the aligned sentence the third step computes the likelihood scores of the hypothesized subphone and all other competing subphones. A list of the subphones is generated in the descending order of the scores and a rank is assigned to the hypothesized subphone according to its positioning. Additional processing of selective weighting and upper-bound limiting is applied to minimize contamination of rank computations by bad segments or by highly-variable phones. The obtained rank is then used as the confidence measure. Results of word rejection experiments show that the new approach outperforms other measures such as whole-word scores by reducing the equal error rate from 32% to 20%.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-818"
  },
  "kawahara98c_icslp": {
   "authors": [
    [
     "Tatsuya",
     "Kawahara"
    ],
    [
     "Kentaro",
     "Ishizuka"
    ],
    [
     "Shuji",
     "Doshita"
    ],
    [
     "Chin-Hui",
     "Lee"
    ]
   ],
   "title": "Speaking-style dependent lexicalized filler model for key-phrase detection and verification",
   "original": "i98_0761",
   "page_count": 4,
   "order": 822,
   "p1": "paper 0761",
   "pn": "",
   "abstract": [
    "A task-independent filler modeling for robust key-phrase detection and verification is proposed. Instead of assuming task-specific lexical knowledge, our model is designed to characterize phrases depending on the speaking-style, thus can be trained with large corpora of different but similar tasks. We present two implementations of the portable and general model. The dialogue-style dependent model trained with the ATIS corpus is used as a filler and shown to be effective in detection-based speech understanding on different dialogue applications. The lecture-style dependent filler model trained with transcriptions of various oral presentations also improves the verification of key-phrases uttered during lectures.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-819"
  },
  "duchnowski98_icslp": {
   "authors": [
    [
     "Paul",
     "Duchnowski"
    ],
    [
     "Louis",
     "Braida"
    ],
    [
     "Maroula",
     "Bratakos"
    ],
    [
     "David",
     "Lum"
    ],
    [
     "Matthew",
     "Sexton"
    ],
    [
     "Jean",
     "Krause"
    ]
   ],
   "title": "A speechreading aid based on phonetic ASR",
   "original": "i98_0589",
   "page_count": 4,
   "order": 823,
   "p1": "paper 0589",
   "pn": "",
   "abstract": [
    "Manual Cued Speech (MCS) is an effective method of communication by the deaf and hearing-impaired. We describe our work on assessing the feasibility of automatic determination and presentation of cues without intervention by the speaker. The conclusions of this study are applied to the design and implementation of a prototype automatic cueing system using HMM-based automatic speech recognition software to identify the cues in real time. We also describe the features of our cue display that enhance its effectiveness such as style of cue images and the timing of their transitions. Our experiments show keyword reception by experienced MCS users improves significantly with the use of our system (66%) relative to speechreading alone (35%) on low-context sentences.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-820"
  },
  "nouza98_icslp": {
   "authors": [
    [
     "Jan",
     "Nouza"
    ]
   ],
   "title": "Training speech through visual feedback patterns",
   "original": "i98_1139",
   "page_count": 4,
   "order": 824,
   "p1": "paper 1139",
   "pn": "",
   "abstract": [
    "The paper describes a new version of a visual feedback aid for speech training. The aid is a PC based speech processing system that visualizes incoming signal and its most relevant parameters (such as volume, pitch, timing, spectrum) and compares them to utterances recorded by reference speakers. The goal is to help a trained person in identifying the most severe deviations in his or her pronunciation. The learning through visual comparison is supported by displaying multiple reference utterances, including phonetic labels both to the reference speakers' and trainee's speech, indicating the areas with larger deviations in any of the displayed features and offering a simple tutoring assessment of the trainee's attempts. Primarily, the system was aimed at hearing-impaired users, but its features make it well applicable also for foreign language pronunciation learning and practicing. The latter possibility was verified in an experiment in which a group of subjects tried to learn pronunciation of a couple of words in an exotic for them foreign language.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-821"
  },
  "maruyama98_icslp": {
   "authors": [
    [
     "Ichiro",
     "Maruyama"
    ],
    [
     "Yoshiharu",
     "Abe"
    ],
    [
     "Takahiro",
     "Wakao"
    ],
    [
     "Eiji",
     "Sawamura"
    ],
    [
     "Terumasa",
     "Ehara"
    ],
    [
     "Katsuhiko",
     "Shirai"
    ]
   ],
   "title": "Word sequence pair spotting for synchronization of speech and text in production of closed-caption TV programs for the hearing impaired",
   "original": "i98_1113",
   "page_count": 4,
   "order": 825,
   "p1": "paper 1113",
   "pn": "",
   "abstract": [
    "This paper describes a method of automatically synchronizing TV news speech and its captions. A news item consists of sentences and often has a corresponding computerized text, which can be used as a caption. We have developed a new phonetically HMM-based word spotter. In this word spotter, word sequences before and after a synchronization point are concatenated and scoring is based on the state of the synchronization point. The detection accuracy of the proposed method is shown to be superior to a conventional method using no word sequence pair. Model configurations are shown for detection failure, an announcer's misstatements and restatements, and erroneous transcriptions. A 100% detection rate with no false alarms is achieved by combining multiple word sequence pairs in series. A 100% detection rate with few false alarms is obtained by using model configurations for misstatements or erroneous transcriptions.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-822"
  },
  "ho98c_icslp": {
   "authors": [
    [
     "Aileen K.",
     "Ho"
    ],
    [
     "John L.",
     "Bradshaw"
    ],
    [
     "Robert",
     "Iansek"
    ],
    [
     "Robin J.",
     "Alfredson"
    ]
   ],
   "title": "Volume regulation in parkinsonian speech",
   "original": "i98_0010",
   "page_count": 4,
   "order": 826,
   "p1": "paper 0010",
   "pn": "",
   "abstract": [
    "This study investigated the ability to regulate speech volume in a group of six-volume impaired idiopathic Parkinson's disease (PD) patients and their age and sex-matched controls. Participants were asked to read under three conditions; as softly as possible, as loudly as possible, and at normal volume (no volume instruction). The stimuli consisted of a target sentence, easily read in one breath, embedded in a short paragraph of text. Mean volume and volume over time (intensity slope) for the target sentence were obtained. It was found that for all three conditions, patients' speech volume was less than controls' by a constant. Patients also showed a significantly greater reduction of volume (negative intensity slope) towards the end of the sentence, especially for the loud condition. The findings indicate that patients with Parkinsonian hypophonic dysarthria have significant difficulty maintaining speech volume in addition to the inadequate generation of overall speech volume.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-823"
  },
  "strangert98_icslp": {
   "authors": [
    [
     "Eva",
     "Strangert"
    ],
    [
     "Mattias",
     "Heldner"
    ]
   ],
   "title": "On the amount and domain of focal lengthening in Swedish",
   "original": "i98_0462",
   "page_count": 4,
   "order": 827,
   "p1": "paper 0462",
   "pn": "",
   "abstract": [
    "Temporal effects of focus in Swedish were studied in short sentences with systematic variation of the length and prosodic pattern of target words in different syntactic positions. Generally, focus caused an average increase in word duration of about 25%. Variations of word length, stress, or word accent pattern did not produce any systematic effects on the amount of lengthening of the target word, but the lengthening varied considerably between speakers and different positions in the sentence. The most extensive lengthening occurred in combination with the insertion of a boundary after the word in focus in those cases when the word preceded a strong syntactic boundary. Within words, stressed syllables were lengthened most, and lengthening of a primary and following secondary stressed syllable was equal to that of a single primary stressed syllable. As unstressed syllables were also affected, the domain of focal lengthening is assumed to be the word.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-824"
  },
  "hirst98_icslp": {
   "authors": [
    [
     "Daniel",
     "Hirst"
    ],
    [
     "Corine",
     "Astesano"
    ],
    [
     "Albert Di",
     "Cristo"
    ]
   ],
   "title": "Differential lengthening of syllabic constituents in French: the effect of accent type and speaking style",
   "original": "i98_0733",
   "page_count": 4,
   "order": 828,
   "p1": "paper 0733",
   "pn": "",
   "abstract": [
    "This paper presents results from the analysis of segmental duration in French in three different speaking styles, (a) 'reading'; (b) 'news broadcast' ; (c) 'spontaneous interview'. A ten minute corpus was hand segmented and labelled using six accent categories (i) unstressed (ii) word-initial (iii) emphatic word-initial (iv) word-final (v) intonation unit final before non-terminal boundary (vi) intonation unit final before terminal boundary. Results show that when different accent types and different speaking styles are taken into account there is no uniform lengthening of prosodic constituents in French, whether the syllable or some other suprasegmental unit is taken as the domain for lengthening. Instead differential lengthening is observed consistently with essentially greater lengthening towards the beginning of the syllable for initial prominence and greater lengthening towards the end of the syllable for final prominence. The degree of the different types of lengthening was moreover significantly different across the speaking styles.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-825"
  },
  "quimbo98_icslp": {
   "authors": [
    [
     "Felix C. M.",
     "Quimbo"
    ],
    [
     "Tatsuya",
     "Kawahara"
    ],
    [
     "Shuji",
     "Doshita"
    ]
   ],
   "title": "Prosodic analysis of fillers and self-repair in Japanese speech",
   "original": "i98_0762",
   "page_count": 4,
   "order": 829,
   "p1": "paper 0762",
   "pn": "",
   "abstract": [
    "The prosodic features of filled pauses (fillers) and self-repair are investigated with a view towards the detection of disfluencies. First, we compare the prosodic features of typical fillers and their fluent homonyms using read sentences of identical phoneme sequences. It is confirmed that the fillers (1) have at least 2 times longer duration than their non-disfluent counterparts, (2) tend to be followed by definitely longer pauses, and (3) have much smaller movement in their pitch contours. Then, the spontaneous fillers segmented out from a dialogue corpus are also analyzed. The same tendency is confirmed, but some samples lie halfway between the read fillers and their fluent homonyms. The abruptly cut-off endings in the self-repair are also analyzed by comparing with the ordinary endings of words. It is found that a short phoneme ending coupled with a relatively short succeeding pause indicates the abrupt cutoff.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-826"
  },
  "ni98_icslp": {
   "authors": [
    [
     "Jinfu",
     "Ni"
    ],
    [
     "Goh",
     "Kawai"
    ],
    [
     "Keikichi",
     "Hirose"
    ]
   ],
   "title": "A synthesis-oriented model of phrasal pitch movements in standard Chinese",
   "original": "i98_0750",
   "page_count": 4,
   "order": 830,
   "p1": "paper 0750",
   "pn": "",
   "abstract": [
    "This paper proposes a computable, tone-driven method to model phrasal pitch movements in standard Chinese by (1) formulating physical constraints for phonetic control mechanisms, (2) defining four phrasal tones quantified by model parameters for generating phrasal tunes, and (3) forming phrasal pitch movements by mapping lexical tones onto phrasal tunes. Several experiments confirm the method's validity. The proposed method is an effective component technology for analyzing, synthesizing and understanding spoken Chinese intonation.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-827"
  },
  "guo98_icslp": {
   "authors": [
    [
     "Qing",
     "Guo"
    ],
    [
     "Fang",
     "Zheng"
    ],
    [
     "Jian",
     "Wu"
    ],
    [
     "Wenhu",
     "Wu"
    ]
   ],
   "title": "Non-linear probability estimation method used in HMM for modeling frame correlation",
   "original": "i98_0124",
   "page_count": 6,
   "order": 831,
   "p1": "paper 0124",
   "pn": "",
   "abstract": [
    "In this paper we present a novel method to incorporate temporal correlation into a speech recognition system based on HMM. An obvious way to incorporate temporal correlation is to condition the probability of the current observation on the current state as well as on the previous observation and the previous state. But use this method directly must lead to unreliable parameter estimates for the number of parameters to be estimated may increase too excessively to limited train data. In this paper, we approximate the joint conditional PD by non-linear estimation method. The HMM incorporated temporal correlation by non-linear estimation method, which we called it FC HMM does not need any additional parameters and it only brings a little additional computing quantity. The results in the experiment show that the top 1 recognition rate of FC HMM has been raised by 6 percent compared to the traditional HMM method.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-828"
  },
  "kumagai98_icslp": {
   "authors": [
    [
     "Shuri",
     "Kumagai"
    ]
   ],
   "title": "Patterns of linguopalatal contact during Japanese vowel devoicing",
   "original": "i98_0222",
   "page_count": 4,
   "order": 832,
   "p1": "paper 0222",
   "pn": "",
   "abstract": [
    "It is widely claimed that close vowels in Japanese are devoiced when they occur between voiceless consonants. In this paper, voiceless vowels are represented symbolically as [V-] and voiced vowels as [V+]. The patterns of linguopalatal contact during C[V-]C units and the corresponding C[V+]C units are examined using the method of electropalatography (EPG). Our results show that C[V-]C units and the corresponding C[V+]C units often differ with respect to: (1) the amount (patterns) of tongue-palate contact from C1 (the preceding consonant) to C2 (the following consonant) and (2) the articulatory time interval from C1 to C2. Generally, the amount of linguopalatal contact is significantly greater at the front part of the palate in C[V-]C units compared to the corresponding C[V+]C units. The articulatory time interval from C1 to C2 is generally shorter in C[V-]C units compared to the corresponding C[V+]C units, though this is not always the case for all consonantal types. However, the articulatory gesture of the vowel appears to exist between voiceless consonants regardless of whether they are voiced or devoiced. Devoiced vowels have often been examined from the aspect of the opening gesture of the glottis since a turbulent noise during devoiced vowels is expected to be made at the glottis. However, our study seems to suggest that a turbulent noise can also be produced in the oral cavity - as well as at the glottis - by increasing the degree of tongue-palate contact. In principle, it is expected that the larger the tongue-palate contact is, the greater the turbulent noise will become due to the increased rate of airflow. This kind of linguopalatal contact appears to be a positive effort of a speaker rather than simply a matter of a shorter articulatory time interval in C[V-]C units: both factors seem to be related to the production of vowel devoicing, which seems to suggest that aerodynamic effects are involved.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-829"
  },
  "yu98c_icslp": {
   "authors": [
    [
     "Xiao",
     "Yu"
    ],
    [
     "Guangrui",
     "Hu"
    ]
   ],
   "title": "Speech separation based on the GMM PDF estimation",
   "original": "i98_0286",
   "page_count": 6,
   "order": 833,
   "p1": "paper 0286",
   "pn": "",
   "abstract": [
    "In this paper, the speech separation task will be regarded as a convolutive mixture Blind Source Separation (BSS) problem. The Maximum Entropy (ME) algorithm, the Minimum Mutual Information (MMI) algorithm and the Maximum Likelihood (ML) algorithm are main approaches of the algorithms solving the BSS problem. The relationship of these three algorithms has been analyzed in this paper. Based on the feedback network architecture, a new speech separation algorithm is proposed by using the Gaussian Mixture Model (GMM) pdf estimation in this paper. From the computer simulation results, it can be concluded that the proposed algorithm can get faster convergence rate and lower output Mean Square Error than the conventional ME algorithm.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-830"
  },
  "luo98b_icslp": {
   "authors": [
    [
     "Xiaoqiang",
     "Luo"
    ]
   ],
   "title": "Growth transform of a sum of rational functions and its application in estimating HMM parameters",
   "original": "i98_0364",
   "page_count": 6,
   "order": 834,
   "p1": "paper 0364",
   "pn": "",
   "abstract": [
    "Gopalakrishnan et al described a method called \"growth transform\" to optimize rational functions over a domain, which has been found useful to train discriminatively Hidden Markov Models(HMM) in speech recognition. A sum of rational functions is encountered when the contributions from other HMM states are weighted in estimating Gaussian parameters of a state, and the weights are optimized using cross- validation. We will show that the growth transform of a sum of rational function can be obtained by computing term-wise gradients and term-wise function values, as opposed to forming first a single rational function and then applying the result in [Gopal91]. This is computationally advantageous when the objective function consists of many rational terms and the dimensionality of the domain is high. We also propose a gradient directed search algorithm to find the appropriate transform constant C.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-831"
  },
  "wester98b_icslp": {
   "authors": [
    [
     "Mirjam",
     "Wester"
    ],
    [
     "Judith M.",
     "Kessens"
    ],
    [
     "Helmer",
     "Strik"
    ]
   ],
   "title": "Two automatic approaches for analyzing connected speech processes in dutch",
   "original": "i98_0373",
   "page_count": 6,
   "order": 835,
   "p1": "paper 0373",
   "pn": "",
   "abstract": [
    "This paper describes two automatic approaches used to study connected speech processes (CSPs) in Dutch. The first approach was from a linguistic point of view - the top-down method. This method can be used for verification of hypotheses about CSPs. The second approach - the bottom-up method -uses a constrained phone recognizer to generate phone transcriptions. An alignment was carried out between the two transcriptions and a reference transcription. A comparison between the two methods showed that 68% agreement was achieved on the CSPs. Although phone accuracy is only 63%, the bottom-up approach is useful for studying CSPs. From the data generated using the bottom-up method, indications of which CSPs are present in the material can be found. These indications can be used to generate hypotheses which can then be tested using the top-down method.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-832"
  },
  "koolwaaij98_icslp": {
   "authors": [
    [
     "Johan W.",
     "Koolwaaij"
    ],
    [
     "Johan de",
     "Veth"
    ]
   ],
   "title": "The use of broad phonetic class models in speaker recognition",
   "original": "i98_0380",
   "page_count": 6,
   "order": 836,
   "p1": "paper 0380",
   "pn": "",
   "abstract": [
    "We investigate the use of broad phonetic class (BPC) models in a text independent speaker recognition task. These models can be used to bring down the variability due to the intrinsic differences between mutual phonetic classes in the speech material used for training of the speaker models. Combining BPC recognition with text independent speaker recognition moves a bit in the direction of text dependent speaker recognition: a task which is known to reach better performance. The performance of BPC modelling is compared to our baseline system using ergodic 5-state HMMs. The question which BPC contains most speaker specific information is addressed. Also, it is investigated if and how the BPC alignment is correlated with the state alignment from the baseline system to check the assumption that states of an ergodic HMM can model broad phonetic classes.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-833"
  },
  "miquelez98_icslp": {
   "authors": [
    [
     "Jorge",
     "Miqu√©lez"
    ],
    [
     "Rocio",
     "Sesma"
    ],
    [
     "Yolanda",
     "Blanco"
    ]
   ],
   "title": "Analysis and treatment of esophageal speech for the enhancement of its comprehension",
   "original": "i98_0592",
   "page_count": 3,
   "order": 837,
   "p1": "paper 0592",
   "pn": "",
   "abstract": [
    "This paper resumes an analysis of esophageal speech, and the developing of a method for improving its intelligibility through speech synthesis. Esophageal speech is characterized by low average frequency, while the formant patterns are found to be similar of those of normal speakers. The treatment is different for voiced and unvoiced frames of the signal. While the unvoiced frames are hold like in the original speech, the voiced frames are re-synthesized using linear prediction. Various models of vocal sources have been tested, and the results were better with a polynomial model. The fundamental frequency is raised up to normal values, keeping the intonation.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-834"
  },
  "lacunza98_icslp": {
   "authors": [
    [
     "Fernando",
     "Lacunza"
    ],
    [
     "Yolanda",
     "Blanco"
    ]
   ],
   "title": "High quality text-to-speech system in Spanish for handicapped people",
   "original": "i98_0596",
   "page_count": 5,
   "order": 838,
   "p1": "paper 0596",
   "pn": "",
   "abstract": [
    "This paper describes a high-quality Text-to-Speech system for Spanish, based on the concatenation of diphonemes with the MBR-PSOLA algorithm. Since it was designed as a substitute of natural voice for handicapped people, it must offer a easy to hear speech, with emotional and emphatic information embedded in it. This is obtained with the prosody generator, which uses a series of phonological patterns for phonic groups and a grammatical database to vary three speech parameters: pitch, amplitude and duration. This system accepts plain text, which can be complemented with data about emotions and emphasis.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-835"
  },
  "ng98b_icslp": {
   "authors": [
    [
     "Corinna",
     "Ng"
    ],
    [
     "Ross",
     "Wilkinson"
    ],
    [
     "Justin",
     "Zobel"
    ]
   ],
   "title": "Factors affecting speech retrieval",
   "original": "i98_0740",
   "page_count": 6,
   "order": 839,
   "p1": "paper 0740",
   "pn": "",
   "abstract": [
    "Collections of speech documents can be searched using speech retrieval, where the documents are processed by a speech recogniser to give text that can be searched by text retrieval techniques. We investigated the use of a phoneme-based recogniser to obtain phoneme sequences. We found that phoneme recognition is worse than word recognition, because of lack of context and difficulty in phoneme boundary detection. Comparing the transcriptions of two different phoneme-based recogniser, we found that the effects of training using well-defined phoneme data, the lack of a language model, and lack of a context-dependent model affected recognition performance. Retrieval using trigrams performed better than quadgrams because the longer n-gram features contained too many transcription errors. Comparing the phonetic transcriptions from a word recogniser to that from a phoneme recogniser, we found that using 61 phones modelled with an algorithmic approach were better than using 40 phones modelled with a dictionary approach.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-836"
  },
  "frid98_icslp": {
   "authors": [
    [
     "Johan",
     "Frid"
    ]
   ],
   "title": "Perception of words with vowel reduction",
   "original": "i98_0743",
   "page_count": 4,
   "order": 840,
   "p1": "paper 0743",
   "pn": "",
   "abstract": [
    "This study deals with listeners' ability to identify linguistic units from linguistically incomplete stimuli and relates this to the potentiality of vowel reduction in a word. Synthetic speech was used to produce stimuli that were similar to real words, but where the vowel in the pre-stress syllable was excluded. Listeners then performed a lexical decision test, where they had to decide whether a stimulus sounded like a word or not. The effects of the identity of the removed vowel and of features of the consonants adjacent to the removed vowel were then examined, as well as syllabic features. For type of vowel, lower word rates were found for words with the vowels /a/ and /o/, whereas words with nasals after the reduced vowel tended to result in higher word rates. Furthermore, words that still conformed to the phonotactic structure of Swedish after reduction got lower word rates than words that violated this, possibly because the conforming words are more eligible to resyllabification, which renders them as phonotactically legal nonsense words rather than real words.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-837"
  },
  "ahmer98_icslp": {
   "authors": [
    [
     "Ingrid",
     "Ahmer"
    ],
    [
     "Robin W.",
     "King"
    ]
   ],
   "title": "Automated captioning of television programs: development and analysis of a soundtrack corpus",
   "original": "i98_0419",
   "page_count": 6,
   "order": 841,
   "p1": "paper 0419",
   "pn": "",
   "abstract": [
    "The purpose of this research is to investigate methods for applying speech recognition techniques to improve the productivity of off-line captioning for television. We posit that existing corpora for training continuous speech recognisers are unrepresentative of the acoustic conditions of television soundtracks. To evaluate the use of application specific models to this task we have developed a soundtrack corpus (representing a single genre of television programming) for acoustic analysis and a text corpus (from the same genre) for language modelling. These corpora are built from components of the manual captioning process. Captions were used to automatically segment and label the acoustic soundtrack data at sentence level, with manual post-processing to classify and verify the data. The text corpus was derived using automatic processing from approximately 1 million words of caption text. The results confirm the acoustic profile of the task to be characteristically different to that of most other speech recognition tasks (with the soundtrack corpus being almost devoid of clean speech). The text corpus indicates that application specific language modelling will be effective for the chosen genre, although a lexicon providing complete lexical coverage is unattainable. There is a high correspondence between captions and soundtrack speech for the chosen genre, confirming that closed-captions can be a useful data source for generating labelled acoustic data. The corpora provide a high quality resource to support further research into automated speech recognition.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-838"
  },
  "lefevre98_icslp": {
   "authors": [
    [
     "Fabrice",
     "Lef√®vre"
    ],
    [
     "Claude",
     "Montaci√©"
    ],
    [
     "Marie-Jos√©",
     "Caraty"
    ]
   ],
   "title": "On the influence of the delta coefficients in a HMM-based speech recognition system",
   "original": "i98_0573",
   "page_count": 6,
   "order": 842,
   "p1": "paper 0573",
   "pn": "",
   "abstract": [
    "The delta coefficients are a conventional method to include temporal information in the speech recognition systems. In particular, they are widely used in the gaussian HMM-based systems. Some attempts were made to introduce the delta coefficients in the K-Nearest Neighbours (K-NN) HMM-based system that we recently developed. An introduction of the delta coefficients directly in the representation space is shown not to be suitable with the K-NN probability density function (pdf) estimator. So, we investigate whether the delta coefficient could be used to improve the K-NN HMM-based system in other ways. In this purpose, an analysis of the delta coefficients in the gaussian HMM-based systems is proposed. It leads to the conclusion that the delta coefficients influence also the recognition process.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-839"
  },
  "low98b_icslp": {
   "authors": [
    [
     "Raymond",
     "Low"
    ],
    [
     "Roberto",
     "Togneri"
    ]
   ],
   "title": "Speech recognition using the probabilistic neural network",
   "original": "i98_0645",
   "page_count": 5,
   "order": 843,
   "p1": "paper 0645",
   "pn": "",
   "abstract": [
    "A novel technique for speaker independent automated speech recognition is proposed. We take a segment model approach to Automated Speech Recognition (ASR), considering the trajectory of an utterance in vector space, then classify using a modified Probabilistic Neural Network (PNN) and maximum likelihood rule. The system performs favourably with established techniques. Our system achieves in excess of 94% with isolated digit recognition, 88% with isolated alphabetic letters, and 83% with the confusable /e/ set. A favourable compromise between recognition accuracy and computer memory and speech can also be reached by performing clustering on the training data for the PNN.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-840"
  },
  "zitouni98b_icslp": {
   "authors": [
    [
     "Imed",
     "Zitouni"
    ]
   ],
   "title": "A language modeling based on a hierarchical approach: m_n^v",
   "original": "i98_0727",
   "page_count": 6,
   "order": 844,
   "p1": "paper 0727",
   "pn": "",
   "abstract": [
    "In contrast to conventional n-gram approches, which are the most used language model in continuous speech recognition system, the multigram approach models a stream of variable-length sequences. To overcome the independence assumption in classical multigram, we propose in this paper a hierarchical model which successively relaxes this assumption. We called this model: Mnv. The estimation of the model parameters can be formulated as a Maximum Likelihood estimation problem from incomplete data used at different levels (j in 1...v). We show that estimates of the model parameters can be computed through an iterative Expectation-Maximization algorithm. A few experimental tests were carried out on a corpus extracted from the French ``Le Monde''. Results show that Mnv outperforms based multigram and interpolated bigram but are comparable to the interpolated trigram model.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-841"
  },
  "watanabe98_icslp": {
   "authors": [
    [
     "Michiko",
     "Watanabe"
    ]
   ],
   "title": "Temporal variables in lectures in the Japanese language",
   "original": "i98_0812",
   "page_count": 5,
   "order": 845,
   "p1": "paper 0812",
   "pn": "",
   "abstract": [
    "In second language input studies, speaking speed is regarded as one of the most influential factors in comprehension. However, research in this area has mainly been conducted on written texts read aloud. The present study investigated temporal variables, such as articulation rate and ratio and frequency of fillers and silent pauses, in three university lectures given in Japanese. It was found that the total duration ratio of fillers was as great as that of silent pauses. It also became clear that, for individual speakers, articulation rate and frequency of fillers are relatively constant, while frequency of silent pauses varies depending on discourse section. Of total pause ratio, pause frequency and articulation rate, the latter correlated best with listener ratings of speech speed. The findings suggest that spontaneous speech requires methods of speech speed measurement different from those for read speech.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-842"
  },
  "aylett98c_icslp": {
   "authors": [
    [
     "Matthew",
     "Aylett"
    ]
   ],
   "title": "Building a statistical model of the vowel space for phoneticians",
   "original": "i98_0823",
   "page_count": 6,
   "order": 846,
   "p1": "paper 0823",
   "pn": "",
   "abstract": [
    "Vowel space data (A two dimensional F1/F2 plot) is of interest to phoneticians for the purpose of comparing different accents, languages, speaker styles and individual speakers. Current automatic methods used by speech technologists do not generally produce traditional vowel space models; instead they tend to produce hyper dimensional code books covering the entire speakers speech stream. This makes it difficult to relate results generated by these methods to observations in laboratory phonetics. In order to address these problems a model was developed based on a mixture Gaussian density function fitted using expectation maximisation on F1/F2 data producing a probability distribution in F1/F2 space. Speech was pre-processed using voicing to automatically excerpt vowel data without any need for segmentation and a parametric fit algorithm was applied to calculate likely vowel targets. The result was a clear visualisation of a speaker's vowel space requiring no segmented or labelled speech.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-843"
  },
  "fox98_icslp": {
   "authors": [
    [
     "Michelle Minnick",
     "Fox"
    ]
   ],
   "title": "Computer-mediated input and the acquisition of L2 vowels",
   "original": "i98_0911",
   "page_count": 6,
   "order": 847,
   "p1": "paper 0911",
   "pn": "",
   "abstract": [
    "Programs for testing and training of difficult vowel distinctions in American English were created for subjects to access via the Internet using a web browser. The testing and training data include many likely vowel confusions for speakers of different L1s. The training program focuses on one distinction at a time, and adjusts to concentrate on particular contexts or exemplars that are difficult for the individual subject. In the current study, 52 subjects participated in testing and 2 subjects participated in training. In the testing portion, results indicate that the L1 and the fluency level in English, as well as individual variability, have an effect on perceptual ability. In the training portion, subjects showed improvement on the contrasts on which they trained. Because these programs make extensive data collection over large populations and large distances easy, this method of research will facilitate further investigation of questions regarding second language acquisition.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-844"
  },
  "malik98_icslp": {
   "authors": [
    [
     "Najam",
     "Malik"
    ],
    [
     "W. Harvey",
     "Holmes"
    ]
   ],
   "title": "Speech analysis by subspace methods of spectral line estimation",
   "original": "i98_1026",
   "page_count": 6,
   "order": 848,
   "p1": "paper 1026",
   "pn": "",
   "abstract": [
    "Over frames of short time duration, filtered speech may be described as a finite linear combination of sinusoidal components. In the case of a frame of voiced speech the frequencies are considered to be harmonics of a fundamental frequency. It can be assumed further that the speech samples are observed in additive white noise of zero mean, resulting in a standard signal-plus-noise model. This model has a nonlinear dependence on the frequencies of the sinusoids but is linear in their coefficients. We use subspace line spectral estimation methods of Pisarenko and Prony type to estimate the frequencies and use the results in voiced-unvoiced classification and pitch estimation, followed by analysis of the speech waveform into its sinusoidal components.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-845"
  },
  "hansson98_icslp": {
   "authors": [
    [
     "Petra",
     "Hansson"
    ]
   ],
   "title": "Pausing in Swedish spontaneous speech",
   "original": "i98_1042",
   "page_count": 5,
   "order": 849,
   "p1": "paper 1042",
   "pn": "",
   "abstract": [
    "Pauses in spontaneous speech have a less restricted distribution than pauses in read discourse; however, they are not distributed in a haphazard way. The majority of the perceived pauses in the examined Swedish spontaneous speech material, 73%, occurred in one of the following positions: between sentences, after discourse markers and conjunctions, and before accented content words. There is a range of acoustic correlates of perceived pauses in spontaneous speech, such as silent intervals, hesitation sounds, prepausal lengthening, glottalization and specific F0 patterns. The acoustic manifestation of a pause, e.g. the duration of the pause and the F0 pattern associated with the pause, is to some extent dependent on the pause's position and function.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-846"
  },
  "zetterholm98_icslp": {
   "authors": [
    [
     "Elisabeth",
     "Zetterholm"
    ]
   ],
   "title": "Prosody and voice quality in the expression of emotions",
   "original": "i98_1043",
   "page_count": 5,
   "order": 850,
   "p1": "paper 1043",
   "pn": "",
   "abstract": [
    "Terms for voice quality or phonation types for use in normal speech often come from studies of pathological speech (laryngeal settings) and it is hard to describe voice quality, especially the variations of a normal voice. In normal speech we use different voice qualities both for linguistic distinctions in some languages, prosodically as a boundary signal, socially depending on social and regional variants and paralinguistically in attitudes and emotions. This paper shows some reference types of voice qualities, recorded by a trained phonetician, and their acoustic correlates. In a pilot study a male actor recorded four attitudinally neutral sentences using five different emotions which are being compared to his neutral voice. It is evident that voice quality, as well as rhythm and intonation, plays an important role in giving the impression of different emotions.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-847"
  },
  "lunn98_icslp": {
   "authors": [
    [
     "Julie",
     "Lunn"
    ],
    [
     "Alan A.",
     "Wrench"
    ],
    [
     "Janet Mackenzie",
     "Beck"
    ]
   ],
   "title": "Acoustic analysis of /l/ in glossectomees",
   "original": "i98_1118",
   "page_count": 6,
   "order": 851,
   "p1": "paper 1118",
   "pn": "",
   "abstract": [
    "The production of /l/ is examined for pre- and post-operative patients who have undergone surgery in three distinct areas (anterior, posterior or lateral tongue) followed by radiotherapy and reconstruction. Results show F1 and F2 to be raised after surgery in all cases. Normalised measures of tongue height (F1-F0) and extension (F2-F1) revealed no significant change after surgery to the side of the tongue but in the other two categories, results indicated a change normally associated with both raising and fronting of the tongue. The paper compares these results with findings from other studies and considers possible mechanisms for the observed changes.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1998-848"
  }
 },
 "sessions": [
  {
   "title": "Keynote Speeches",
   "papers": [
    "clark98_icslp",
    "seneff98_icslp"
   ]
  },
  {
   "title": "Text-To-Speech Synthesis 1-6",
   "papers": [
    "bagshaw98_icslp",
    "bellegarda98_icslp",
    "shih98_icslp",
    "yoshimura98_icslp",
    "fordyce98_icslp",
    "fitt98_icslp",
    "faulkner98_icslp",
    "donovan98_icslp",
    "hawkins98_icslp",
    "zhang98_icslp",
    "sonntag98_icslp",
    "sproat98_icslp",
    "bunnell98_icslp",
    "acero98_icslp",
    "akamine98_icslp",
    "vainio98_icslp",
    "chappell98_icslp",
    "chen98_icslp",
    "jeon98_icslp",
    "cox98_icslp",
    "cronk98_icslp",
    "tournemire98_icslp",
    "esquerra98_icslp",
    "fang98_icslp",
    "bonafonte98_icslp",
    "ferencz98_icslp",
    "kagoshima98_icslp",
    "hirose98_icslp",
    "klabbers98_icslp",
    "kuo98_icslp",
    "lee98_icslp",
    "lee98b_icslp",
    "lenzo98_icslp",
    "mannell98_icslp",
    "mizuno98_icslp",
    "mochizuki98_icslp",
    "pagel98_icslp",
    "roth98_icslp",
    "sannier98_icslp",
    "sef98_icslp",
    "seto98_icslp",
    "shiga98_icslp",
    "tzoukermann98_icslp",
    "venditti98_icslp",
    "wang98_icslp",
    "breen98_icslp",
    "pearson98_icslp",
    "syrdal98_icslp",
    "wouters98_icslp",
    "plumpe98_icslp",
    "holzapfel98_icslp",
    "eklund98_icslp",
    "campbell98_icslp",
    "saito98_icslp",
    "sakurai98_icslp",
    "kain98_icslp",
    "mohler98_icslp"
   ]
  },
  {
   "title": "Spoken Language Models and Dialog 1-5",
   "papers": [
    "gustafson98_icslp",
    "failenschmid98_icslp",
    "lin98_icslp",
    "georgila98_icslp",
    "wang98b_icslp",
    "popovici98_icslp",
    "ramaswamy98_icslp",
    "poesio98_icslp",
    "constantinides98_icslp",
    "aist98_icslp",
    "swerts98_icslp",
    "noguchi98_icslp",
    "stromback98_icslp",
    "okato98_icslp",
    "suzuki98_icslp",
    "hanrieder98_icslp",
    "aretoulaki98_icslp",
    "kaspar98_icslp",
    "heeman98_icslp",
    "araki98_icslp",
    "brndsted98_icslp",
    "bull98_icslp",
    "davies98_icslp",
    "devillers98_icslp",
    "furui98_icslp",
    "guan98_icslp",
    "hone98_icslp",
    "iwase98_icslp",
    "flychteriksson98_icslp",
    "kamm98_icslp",
    "kawabata98_icslp",
    "kumamoto98_icslp",
    "mctear98_icslp",
    "okada98_icslp",
    "pouteau98_icslp",
    "willett98_icslp",
    "wu98_icslp",
    "wyard98_icslp",
    "yang98_icslp",
    "yano98_icslp"
   ]
  },
  {
   "title": "Prosody and Emotion 1-6",
   "papers": [
    "rose98_icslp",
    "thompson98_icslp",
    "coupe98_icslp",
    "rose98b_icslp",
    "alter98_icslp",
    "amir98_icslp",
    "schroder98_icslp",
    "aylett98_icslp",
    "kim98_icslp",
    "buckow98_icslp",
    "cahn98_icslp",
    "collins98_icslp",
    "fujisaki98_icslp",
    "holm98_icslp",
    "horiuchi98_icslp",
    "ishihara98_icslp",
    "iwano98_icslp",
    "jang98_icslp",
    "jokisch98_icslp",
    "kong98_icslp",
    "krishnan98_icslp",
    "kubozono98_icslp",
    "lee98c_icslp",
    "lopez98_icslp",
    "maeda98_icslp",
    "maekawa98_icslp",
    "maghbouleh98_icslp",
    "mixdorff98_icslp",
    "mizuno98b_icslp",
    "nakai98_icslp",
    "ohno98_icslp",
    "ohno98b_icslp",
    "portele98_icslp",
    "portele98b_icslp",
    "rank98_icslp",
    "rilliard98_icslp",
    "koike98_icslp",
    "streefkerk98_icslp",
    "tamoto98_icslp",
    "tsukahara98_icslp",
    "wang98c_icslp",
    "whiteside98_icslp",
    "zhang98b_icslp",
    "zhu98_icslp",
    "bolfanstosic98_icslp",
    "zhu98b_icslp",
    "zhu98c_icslp",
    "montero98_icslp",
    "pereira98_icslp",
    "swerts98b_icslp",
    "caspers98_icslp",
    "jun98_icslp",
    "krahmer98_icslp",
    "taylor98_icslp",
    "fujisaki98b_icslp",
    "campione98_icslp",
    "malfrere98_icslp",
    "vereecken98_icslp",
    "wright98_icslp",
    "low98_icslp",
    "gallwitz98_icslp",
    "arvaniti98_icslp",
    "dogil98_icslp",
    "brindopke98_icslp",
    "veronis98_icslp"
   ]
  },
  {
   "title": "Hidden Markov Model Techniques 1-3",
   "papers": [
    "luo98_icslp",
    "bilmes98_icslp",
    "sun98_icslp",
    "lee98d_icslp",
    "gravier98_icslp",
    "iyer98_icslp",
    "aizawa98_icslp",
    "demuynck98_icslp",
    "preez98_icslp",
    "eide98_icslp",
    "fritsch98_icslp",
    "ganapathiraju98_icslp",
    "gandhi98_icslp",
    "hamaker98_icslp",
    "hanai98_icslp",
    "simonin98_icslp",
    "kojima98_icslp",
    "koshiba98_icslp",
    "chesta98_icslp",
    "lee98e_icslp",
    "mak98_icslp",
    "martindelalamo98_icslp",
    "szarvas98_icslp",
    "ming98_icslp",
    "molloy98_icslp",
    "nogueirasrodriguez98_icslp",
    "nogueirasrodriguez98b_icslp",
    "ramsay98_icslp",
    "wellekens98_icslp",
    "wellekens98b_icslp",
    "neukirchen98_icslp",
    "witt98_icslp",
    "yang98b_icslp",
    "zweig98_icslp"
   ]
  },
  {
   "title": "Speaker and Language Recognition 1-4",
   "papers": [
    "sivakumaran98_icslp",
    "barlow98_icslp",
    "beringer98_icslp",
    "berkling98_icslp",
    "bond98_icslp",
    "stockmal98_icslp",
    "burger98_icslp",
    "genoud98_icslp",
    "lincoln98_icslp",
    "dersch98_icslp",
    "preez98b_icslp",
    "faundezzanuy98_icslp",
    "gu98_icslp",
    "hernando98_icslp",
    "jin98_icslp",
    "kido98_icslp",
    "kim98b_icslp",
    "kinoshita98_icslp",
    "korkmazskiy98_icslp",
    "foldvik98_icslp",
    "kyung98_icslp",
    "cheng98_icslp",
    "liu98_icslp",
    "lloydthomas98_icslp",
    "markov98_icslp",
    "markov98b_icslp",
    "matrouf98_icslp",
    "monte98_icslp",
    "moreno98_icslp",
    "muramatsu98_icslp",
    "noda98_icslp",
    "ortegagarcia98_icslp",
    "pfau98_icslp",
    "pham98_icslp",
    "rodriguezlinares98_icslp",
    "rose98c_icslp",
    "schmidtnielsen98_icslp",
    "slomka98_icslp",
    "soltau98_icslp",
    "ward98_icslp",
    "yanguas98_icslp",
    "zhang98c_icslp",
    "rosenberg98_icslp",
    "parris98_icslp",
    "nishida98_icslp",
    "doddington98_icslp",
    "corradaemmanuel98_icslp",
    "nordstrom98_icslp",
    "matsui98_icslp",
    "melin98_icslp",
    "petrovskadelacretaz98_icslp",
    "li98_icslp",
    "olsen98_icslp",
    "olsen98b_icslp",
    "moody98_icslp",
    "sonmez98_icslp",
    "reynolds98_icslp",
    "caseiro98_icslp",
    "braun98_icslp",
    "vuuren98_icslp"
   ]
  },
  {
   "title": "Multimodal Spoken Language Processing 1-3",
   "papers": [
    "breen98b_icslp",
    "cohen98_icslp",
    "czap98_icslp",
    "downey98_icslp",
    "cettolo98_icslp",
    "fries98_icslp",
    "kanzaki98_icslp",
    "brndsted98b_icslp",
    "clow98_icslp",
    "shigeno98_icslp",
    "takezawa98_icslp",
    "vanegas98_icslp",
    "xu98_icslp",
    "ostermann98_icslp",
    "arslan98_icslp",
    "yamamoto98_icslp",
    "roy98_icslp",
    "dupont98_icslp",
    "oviatt98_icslp",
    "johnston98_icslp",
    "hirasawa98_icslp",
    "yokoyama98_icslp",
    "whittaker98_icslp"
   ]
  },
  {
   "title": "Isolated Word Recognition",
   "papers": [
    "azzopardi98_icslp",
    "bayya98_icslp",
    "chen98b_icslp",
    "chengalvarayan98_icslp",
    "choi98_icslp",
    "ferreiros98_icslp",
    "hoshimi98_icslp",
    "jian98_icslp",
    "johansen98_icslp",
    "karnjanadecha98_icslp",
    "kawai98_icslp",
    "koizumi98_icslp",
    "konuma98_icslp",
    "lee98f_icslp",
    "lyu98_icslp",
    "tanaka98_icslp",
    "yang98c_icslp"
   ]
  },
  {
   "title": "Robust Speech Processing in Adverse Environments 1-5",
   "papers": [
    "roberts98_icslp",
    "thambiratnam98_icslp",
    "morin98_icslp",
    "yamada98_icslp",
    "gonzalezrodriguez98_icslp",
    "jiang98_icslp",
    "wu98b_icslp",
    "surendran98_icslp",
    "junqua98_icslp",
    "crafa98_icslp",
    "hunke98_icslp",
    "kenny98_icslp",
    "zhou98_icslp",
    "boughazale98_icslp",
    "kirchhoff98_icslp",
    "wark98_icslp",
    "akagi98_icslp",
    "bayya98b_icslp",
    "berthommier98_icslp",
    "chang98_icslp",
    "chu98_icslp",
    "veth98_icslp",
    "dociofernandez98_icslp",
    "doclo98_icslp",
    "dupont98b_icslp",
    "gallardoantolin98_icslp",
    "geutner98_icslp",
    "girin98_icslp",
    "sarikaya98_icslp",
    "heon98_icslp",
    "huerta98_icslp",
    "hung98_icslp",
    "karray98_icslp",
    "song98_icslp",
    "linhard98_icslp",
    "pan98_icslp",
    "macho98_icslp",
    "raj98_icslp",
    "schless98_icslp",
    "shen98_icslp",
    "shin98_icslp",
    "takiguchi98_icslp",
    "tolba98_icslp",
    "unoki98_icslp",
    "usagawa98_icslp",
    "yu98_icslp",
    "hwang98_icslp",
    "woo98_icslp",
    "cosi98_icslp",
    "ambikairajah98_icslp",
    "logan98_icslp",
    "hansen98_icslp",
    "park98_icslp",
    "singh98_icslp"
   ]
  },
  {
   "title": "Articulatory Modelling 1-2",
   "papers": [
    "badin98_icslp",
    "perrier98_icslp",
    "demolin98_icslp",
    "matsumura98_icslp",
    "kaburagi98_icslp",
    "honda98_icslp",
    "clermont98_icslp",
    "hoole98_icslp",
    "alku98_icslp",
    "ramsay98b_icslp",
    "esling98_icslp",
    "matsuzaki98_icslp",
    "harrington98_icslp",
    "tabain98_icslp",
    "bailly98_icslp",
    "hoole98b_icslp",
    "wrench98_icslp",
    "meynadier98_icslp"
   ]
  },
  {
   "title": "Talking to Infants, Pets and Lovers",
   "papers": [
    "kitamura98_icslp",
    "thanavisuth98_icslp",
    "luksaneeyanawin98_icslp",
    "burnham98_icslp",
    "burnham98b_icslp"
   ]
  },
  {
   "title": "Speech Coding 1-3",
   "papers": [
    "masuko98_icslp",
    "ekudden98_icslp",
    "mudugamuwa98_icslp",
    "epps98_icslp",
    "zhang98d_icslp",
    "holmes98_icslp",
    "skoglund98_icslp",
    "kleijn98_icslp",
    "chong98_icslp",
    "alku98b_icslp",
    "ghaemmaghami98_icslp",
    "hura98_icslp",
    "kim98c_icslp",
    "kohata98_icslp",
    "koishida98_icslp",
    "molyneux98_icslp",
    "nakatoh98_icslp",
    "pan98b_icslp",
    "parry98_icslp",
    "petrinovic98_icslp",
    "plante98_icslp",
    "ribeiro98_icslp"
   ]
  },
  {
   "title": "Neural Networks, Fuzzy and Evolutionary Methods 1",
   "papers": [
    "ahkuputra98_icslp",
    "freitag98_icslp",
    "fukada98_icslp",
    "haskey98_icslp",
    "hosom98_icslp",
    "jia98_icslp",
    "keller98_icslp",
    "mirghafori98_icslp",
    "pizzolato98_icslp",
    "takara98_icslp",
    "tran98_icslp",
    "tran98b_icslp",
    "wutiwiwatchai98_icslp",
    "wutiwiwatchai98b_icslp",
    "glaeser98_icslp",
    "freitas98_icslp",
    "rottland98_icslp",
    "selouani98_icslp",
    "pham98b_icslp",
    "shimodaira98_icslp",
    "kitazoe98_icslp",
    "ngan98_icslp"
   ]
  },
  {
   "title": "Utterance Verification and Word Spotting 1 / Speaker Adaptation 1",
   "papers": [
    "benitez98_icslp",
    "bernardis98_icslp",
    "caminero98_icslp",
    "chen98c_icslp",
    "fischer98_icslp",
    "gunawardana98_icslp",
    "gupta98_icslp",
    "ho98_icslp",
    "ida98_icslp",
    "tran98c_icslp",
    "meliani98_icslp",
    "pao98_icslp",
    "ramabhadran98_icslp",
    "setlur98_icslp",
    "westphal98_icslp",
    "williams98_icslp",
    "wu98c_icslp",
    "yamashita98_icslp"
   ]
  },
  {
   "title": "Human Speech Perception 1-4",
   "papers": [
    "nooteboom98_icslp",
    "riele98_icslp",
    "quene98_icslp",
    "kelly98_icslp",
    "akagi98b_icslp",
    "amano98_icslp",
    "aylett98b_icslp",
    "neagu98_icslp",
    "bonneau98_icslp",
    "chen98d_icslp",
    "yip98_icslp",
    "damper98_icslp",
    "cerrato98_icslp",
    "fernandez98_icslp",
    "fernandez98b_icslp",
    "fernandez98c_icslp",
    "hazan98_icslp",
    "jian98b_icslp",
    "kato98_icslp",
    "kiefte98_icslp",
    "otake98_icslp",
    "shriberg98_icslp",
    "steinhauer98_icslp",
    "vroomen98_icslp",
    "tuomainen98_icslp",
    "yamakawa98_icslp",
    "massaro98_icslp",
    "gerrits98_icslp",
    "kakehi98_icslp",
    "ainsworth98_icslp",
    "behne98_icslp",
    "cutler98_icslp",
    "derwing98_icslp",
    "mcqueen98_icslp",
    "ohala98_icslp",
    "house98_icslp",
    "greenberg98_icslp"
   ]
  },
  {
   "title": "Speech and Hearing Disorders 1",
   "papers": [
    "flynn98_icslp",
    "koopmansvanbeinum98_icslp",
    "croot98_icslp",
    "doorn98_icslp"
   ]
  },
  {
   "title": "Spoken Language Understanding Systems 1-4",
   "papers": [
    "seneff98b_icslp",
    "chung98_icslp",
    "ng98_icslp",
    "sproat98b_icslp",
    "fischer98b_icslp",
    "nagai98_icslp",
    "colas98_icslp",
    "ward98b_icslp",
    "stolcke98_icslp",
    "reichl98_icslp",
    "gillett98_icslp",
    "souvignier98_icslp",
    "riccardi98_icslp",
    "bigi98_icslp",
    "levin98_icslp",
    "arai98_icslp",
    "brndsted98c_icslp",
    "carpenter98_icslp",
    "ghosh98_icslp",
    "kono98_icslp",
    "kronenberg98_icslp",
    "lin98b_icslp",
    "niimi98_icslp",
    "printz98_icslp",
    "riccardi98b_icslp",
    "essdykema98_icslp",
    "takeda98_icslp",
    "tseng98_icslp",
    "valverdealbacete98_icslp",
    "wright98b_icslp",
    "suzuki98b_icslp"
   ]
  },
  {
   "title": "Signal Processing and Speech Analysis 1-3",
   "papers": [
    "droppo98_icslp",
    "yang98d_icslp",
    "batlle98_icslp",
    "caraty98_icslp",
    "cassidy98_icslp",
    "veth98b_icslp",
    "dharanipragada98_icslp",
    "du98_icslp",
    "dutoit98_icslp",
    "girardi98_icslp",
    "charonnat98_icslp",
    "gutierrezarriola98_icslp",
    "tian98_icslp",
    "halberstadt98_icslp",
    "harte98_icslp",
    "hermansky98_icslp",
    "holmes98b_icslp",
    "holzrichter98_icslp",
    "shen98b_icslp",
    "shen98c_icslp",
    "iskra98_icslp",
    "kamata98_icslp",
    "king98_icslp",
    "koreman98_icslp",
    "kuwabara98_icslp",
    "lee98g_icslp",
    "long98_icslp",
    "matsumoto98_icslp",
    "mcmahon98_icslp",
    "meron98_icslp",
    "minematsu98_icslp",
    "niyogi98_icslp",
    "nadeu98_icslp",
    "namba98_icslp",
    "pearson98b_icslp",
    "araujo98_icslp",
    "pfitzinger98_icslp",
    "rossato98_icslp",
    "ruske98_icslp",
    "samouelian98_icslp",
    "schoentgen98_icslp",
    "suh98_icslp",
    "thorpe98_icslp",
    "tolba98b_icslp",
    "yang98e_icslp",
    "zheng98_icslp",
    "ainsworth98b_icslp",
    "kawahara98_icslp",
    "aikawa98_icslp",
    "funaki98_icslp",
    "hermansky98b_icslp",
    "suzuki98c_icslp",
    "li98b_icslp",
    "robert98_icslp",
    "ramesh98_icslp",
    "basu98_icslp",
    "silva98_icslp"
   ]
  },
  {
   "title": "Spoken Language Generation and Translation 1-2",
   "papers": [
    "fang98b_icslp",
    "eklund98b_icslp",
    "garciavarea98_icslp",
    "gawronska98_icslp",
    "hulstijn98_icslp",
    "ishikawa98_icslp",
    "krahmer98b_icslp",
    "levin98b_icslp",
    "williams98b_icslp",
    "ruland98_icslp",
    "yi98_icslp",
    "klabbers98b_icslp",
    "hitzeman98_icslp",
    "alshawi98_icslp",
    "fukada98b_icslp",
    "wang98d_icslp",
    "takezawa98b_icslp"
   ]
  },
  {
   "title": "Segmentation, Labelling and Speech Corpora 1-4",
   "papers": [
    "hirschberg98_icslp",
    "grabe98_icslp",
    "chou98_icslp",
    "rapp98_icslp",
    "karjalainen98_icslp",
    "bratt98_icslp",
    "deshmukh98_icslp",
    "aiello98_icslp",
    "cettolo98b_icslp",
    "gholampour98_icslp",
    "iida98_icslp",
    "kang98_icslp",
    "laws98_icslp",
    "malfrere98b_icslp",
    "millar98_icslp",
    "montacie98_icslp",
    "pye98_icslp",
    "rapp98b_icslp",
    "brindopke98b_icslp",
    "stober98_icslp",
    "isard98_icslp",
    "moreno98b_icslp",
    "kessens98_icslp",
    "barker98_icslp",
    "pellom98_icslp",
    "hain98_icslp",
    "lindberg98_icslp",
    "campione98b_icslp",
    "cole98_icslp",
    "aist98b_icslp",
    "shyuu98_icslp",
    "bird98_icslp",
    "altosaar98_icslp"
   ]
  },
  {
   "title": "Large Vocabulary Continuous Speech Recognition 1-6",
   "papers": [
    "cook98_icslp",
    "yu98b_icslp",
    "glass98_icslp",
    "hon98_icslp",
    "gauvain98_icslp",
    "tsukada98_icslp",
    "yodo98_icslp",
    "uebler98_icslp",
    "zitouni98_icslp",
    "klakow98_icslp",
    "clarkson98_icslp",
    "nguyen98_icslp",
    "lee98h_icslp",
    "schuster98_icslp",
    "kemp98_icslp",
    "bacchiani98_icslp",
    "mcallaster98_icslp",
    "chou98b_icslp",
    "kemp98b_icslp",
    "lee98i_icslp",
    "duchateau98_icslp",
    "sankar98_icslp",
    "gopinath98_icslp",
    "ando98_icslp",
    "balakrishnan98_icslp",
    "bansal98_icslp",
    "bellegarda98b_icslp",
    "chengalvarayan98b_icslp",
    "coccaro98_icslp",
    "pastor98_icslp",
    "digalakis98_icslp",
    "doherty98_icslp",
    "johnson98_icslp",
    "jang98b_icslp",
    "kai98_icslp",
    "kobayashi98_icslp",
    "kobayashi98b_icslp",
    "koreman98b_icslp",
    "lau98_icslp",
    "bahl98_icslp",
    "liu98b_icslp",
    "ma98_icslp",
    "basu98b_icslp",
    "marino98_icslp",
    "mori98_icslp",
    "neto98_icslp",
    "padmanabhan98_icslp",
    "palazuelos98_icslp",
    "papineni98_icslp",
    "berger98_icslp",
    "ramaswamy98b_icslp",
    "sanchez98_icslp",
    "sankar98b_icslp",
    "seymore98_icslp",
    "takagi98_icslp",
    "tolba98c_icslp",
    "torrecilla98_icslp",
    "mulbregt98_icslp",
    "oneill98_icslp",
    "veilleux98_icslp",
    "weng98_icslp",
    "wester98_icslp",
    "whittaker98b_icslp",
    "witschel98_icslp",
    "wright98c_icslp",
    "zavaliagkos98_icslp",
    "jan98_icslp",
    "bonafonte98b_icslp",
    "kawahara98b_icslp",
    "itou98_icslp",
    "ogata98_icslp",
    "siu98_icslp"
   ]
  },
  {
   "title": "Speech Technology Applications and Human-Machine Interface 1-3",
   "papers": [
    "glaeser98b_icslp",
    "chung98b_icslp",
    "delogu98_icslp",
    "fung98_icslp",
    "pargellis98_icslp",
    "graham98_icslp",
    "hirayama98_icslp",
    "jo98_icslp",
    "qvarfordt98_icslp",
    "kemp98c_icslp",
    "kim98d_icslp",
    "bakman98_icslp",
    "nakatani98_icslp",
    "qiao98_icslp",
    "robertribes98_icslp",
    "serridge98_icslp",
    "yang98f_icslp",
    "tan98_icslp",
    "wang98e_icslp",
    "shinozaki98_icslp",
    "kellner98_icslp",
    "buntschuh98_icslp",
    "choi98b_icslp",
    "ferreiros98b_icslp",
    "lamel98_icslp",
    "bernsen98_icslp",
    "godinollorente98_icslp",
    "sjolander98_icslp",
    "sutton98_icslp",
    "serridge98b_icslp",
    "garciamateo98_icslp"
   ]
  },
  {
   "title": "Language Acquisition 1-2",
   "papers": [
    "minagawakawai98_icslp",
    "yamada98b_icslp",
    "funatsu98_icslp",
    "cucchiarini98_icslp",
    "langlais98_icslp",
    "akahaneyamada98_icslp",
    "hardison98_icslp",
    "tyler98_icslp",
    "tsukada98b_icslp",
    "imaizumi98_icslp",
    "watson98_icslp",
    "hazan98b_icslp"
   ]
  },
  {
   "title": "Acoustic Phonetics 1-2",
   "papers": [
    "cutler98b_icslp",
    "kondo98_icslp",
    "cucchiarini98b_icslp",
    "hajek98_icslp",
    "dang98_icslp",
    "cox98b_icslp",
    "watson98b_icslp",
    "nguyen98b_icslp",
    "nguyen98c_icslp",
    "son98_icslp",
    "karlsson98_icslp"
   ]
  },
  {
   "title": "Speaker Adaptation 2-3",
   "papers": [
    "kuhn98_icslp",
    "johnson98b_icslp",
    "viikki98_icslp",
    "gales98_icslp",
    "chien98_icslp",
    "suzuki98d_icslp",
    "anastasakos98_icslp",
    "mcdonough98_icslp",
    "zheng98b_icslp",
    "oviatt98b_icslp"
   ]
  },
  {
   "title": "Multilingual Perception and Recognition 1",
   "papers": [
    "uebler98b_icslp",
    "schultz98_icslp",
    "kawai98b_icslp"
   ]
  },
  {
   "title": "Language Acquisition 3 / Multilingual Perception and Recognition 2",
   "papers": [
    "blamey98_icslp",
    "cucchiarini98c_icslp",
    "dalsgaard98_icslp",
    "eklund98c_icslp",
    "franco98_icslp",
    "geutner98b_icslp",
    "jo98b_icslp",
    "leung98_icslp",
    "liu98c_icslp",
    "markham98_icslp",
    "mctear98b_icslp",
    "nakayama98_icslp",
    "oster98_icslp",
    "sandra98_icslp",
    "shirose98_icslp",
    "so98_icslp",
    "tomitanakayama98_icslp",
    "warnke98_icslp",
    "whiteside98b_icslp",
    "whiteside98c_icslp",
    "brown98_icslp",
    "williams98c_icslp",
    "zmarich98_icslp",
    "kuhn98b_icslp"
   ]
  },
  {
   "title": "Speech and Hearing Disorders 2 / Speech Processing for the Speech and Hearing Impaired 1",
   "papers": [
    "blanco98_icslp",
    "bruijn98_icslp",
    "fearn98_icslp",
    "brunnegaard98_icslp",
    "hiki98_icslp",
    "ho98b_icslp",
    "jo98c_icslp",
    "kitazawa98_icslp",
    "agelfors98_icslp",
    "martin98_icslp",
    "skljarov98_icslp",
    "soltanifarani98_icslp",
    "varley98_icslp",
    "cheesman98_icslp"
   ]
  },
  {
   "title": "Human Speech Production",
   "papers": [
    "barry98_icslp",
    "boshoff98_icslp",
    "lecuit98_icslp",
    "soquet98_icslp",
    "dusan98_icslp",
    "esling98b_icslp",
    "fon98_icslp",
    "fujimoto98_icslp",
    "fujisawa98_icslp",
    "ishihara98b_icslp",
    "jurafsky98_icslp",
    "kim98e_icslp",
    "mori98b_icslp",
    "motoki98_icslp",
    "niikawa98_icslp",
    "okadome98_icslp",
    "shih98b_icslp",
    "lundberg98_icslp",
    "terao98_icslp",
    "vilain98_icslp",
    "wakumoto98_icslp",
    "whiteside98d_icslp",
    "zellner98_icslp"
   ]
  },
  {
   "title": "Utterance Verification and Word Spotting 2",
   "papers": [
    "ramesh98b_icslp",
    "dolfing98_icslp",
    "willett98b_icslp",
    "jiang98b_icslp",
    "lin98c_icslp",
    "kawahara98c_icslp"
   ]
  },
  {
   "title": "Speech Processing for the Speech-Impaired and Hearing-Impaired 2",
   "papers": [
    "duchnowski98_icslp",
    "nouza98_icslp",
    "maruyama98_icslp",
    "ho98c_icslp",
    "strangert98_icslp",
    "hirst98_icslp",
    "quimbo98_icslp",
    "ni98_icslp"
   ]
  },
  {
   "title": "SST Student Day - Poster Sessions",
   "papers": [
    "guo98_icslp",
    "kumagai98_icslp",
    "yu98c_icslp",
    "luo98b_icslp",
    "wester98b_icslp",
    "koolwaaij98_icslp",
    "miquelez98_icslp",
    "lacunza98_icslp",
    "ng98b_icslp",
    "frid98_icslp",
    "ahmer98_icslp",
    "lefevre98_icslp",
    "low98b_icslp",
    "zitouni98b_icslp",
    "watanabe98_icslp",
    "aylett98c_icslp",
    "fox98_icslp",
    "malik98_icslp",
    "hansson98_icslp",
    "zetterholm98_icslp",
    "lunn98_icslp"
   ]
  }
 ],
 "doi": "10.21437/ICSLP.1998"
}