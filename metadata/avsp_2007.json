{
 "location": "Kasteel Groenendaal, Hilvarenbeek, The Netherlands",
 "startDate": "31/8/2007",
 "endDate": "3/9/2007",
 "conf": "AVSP",
 "year": "2007",
 "name": "avsp_2007",
 "series": "AVSP",
 "SIG": "AVISA",
 "title": "Auditory-Visual Speech Processing",
 "title1": "Auditory-Visual Speech Processing",
 "date": "31 August - 3 September 2007",
 "papers": {
  "stekelenburg07_avsp": {
   "authors": [
    [
     "Jeroen J.",
     "Stekelenburg"
    ],
    [
     "Jean",
     "Vroomen"
    ]
   ],
   "title": "Neural correlates of multisensory integration of ecologically valid audiovisual events",
   "original": "av07_L1-1",
   "page_count": 4,
   "order": 1,
   "p1": "paper L1-1",
   "pn": "",
   "abstract": [
    "A question that has emerged over recent years is whether audiovisual (AV) speech perception is a special case of multisensory perception. Electrophysiological (ERP) studies have found that auditory neural activity (N1 component of the ERP) induced by speech is suppressed and speeded up when a speech sound is accompanied by concordant lip movements. In Experiment 1, we show that this AV interaction is not speech-specific. Ecologically valid non-speech AV events (actions performed by an actor such as handclapping) were associated with a similar speeding up and suppression of auditory N1 amplitude as AV speech (syllables). Experiment 2 demonstrated that these AV interactions were not influenced by whether A and V were congruent or incongruent. In Experiment 3 we show that the AV interaction on N1 was absent when there was no anticipatory visual motion, indicating that the AV interaction only occurred when visual anticipatory motion preceded the sound. These results demonstrate that the visually induced speedingup and suppression of auditory N1 amplitude reflect multisensory integrative mechanisms of AV events that crucially depend on whether vision predicts when the sound occurs.\n",
    ""
   ]
  },
  "calabresi07_avsp": {
   "authors": [
    [
     "Marco",
     "Calabresi"
    ],
    [
     "Sharon M.",
     "Thomas"
    ],
    [
     "Tim J.",
     "Folkard"
    ],
    [
     "Deborah A.",
     "Hall"
    ]
   ],
   "title": "Speechreading in context: an ERP study",
   "original": "av07_L1-2",
   "page_count": 6,
   "order": 2,
   "p1": "paper L1-2",
   "pn": "",
   "abstract": [
    "Aim - To determine whether single-word speechreading is sensitive to prior phrase context. Background - We used a well-established Event Related Potential (ERP) paradigm to elicit the N400, a negative wave around 400 ms. The N400 reflects activation by lexical semantic features and is modulated by contextual integration. Methods - Seventeen adult native English speakers participated. They were shown written three-word contexts followed by a final word (cloze) presented as a silent video clip. Context and cloze formed typical phrases (60 trials, e.g. \"No smoke without fire\") and anomalous phrases (120 trials, e.g. \"Two of a trade\"). Subjects were instructed to identify the cloze at the end of each trial. Typicality (typical/anomalous) and accuracy were used to classify trials. ERPs were computed off-line by averaging epochs aligned to the onset of articulatory movements. Results and discussion - For all four conditions, reliable evoked activity emerged only after 500 ms post-onset. Topographical analysis revealed a symmetrical sustained negative field at the frontal sites and positive field at the posterior sites. Statistical comparisons showed that only the waveform for the typical-correct condition differed from the other three. The difference, however, was not compatible with an N400. This finding suggests that the speechreading process integrates lexical semantic features differently from spoken and written language perception. We speculate on those cognitive mechanisms which may be responsible for the observed effects.\n",
    ""
   ]
  },
  "mol07_avsp": {
   "authors": [
    [
     "Lisette",
     "Mol"
    ],
    [
     "Emiel",
     "Krahmer"
    ],
    [
     "Alfons",
     "Maes"
    ],
    [
     "Marc",
     "Swerts"
    ]
   ],
   "title": "The communicative import of gestures: evidence from a comparative analysis of human-human and human-machine interactions",
   "original": "av07_L1-3",
   "page_count": 6,
   "order": 3,
   "p1": "paper L1-3",
   "pn": "",
   "abstract": [
    "Is communication the primary functional role of gesticulation? We conducted a study in which participants narrated to a presumed computer system, a presumed addressee in another room (via web cam), or an addressee in the same room, who could either see them or not. Participants produced significantly fewer gestures when they thought to be talking to a computer system. Our results show that people narrate differently to a computer system than to a human addressee. Considering the difference in gesticulation, it seems plausible that, in a narrative task, most gestures are produced with a communicative intent.\n",
    ""
   ]
  },
  "fagel07_avsp": {
   "authors": [
    [
     "Sascha",
     "Fagel"
    ],
    [
     "Gérard",
     "Bailly"
    ],
    [
     "Frédéric",
     "Elisei"
    ]
   ],
   "title": "Intelligibility of natural and 3d-cloned German speech",
   "original": "av07_L2-1",
   "page_count": 6,
   "order": 4,
   "p1": "paper L2-1",
   "pn": "",
   "abstract": [
    "We investigate the intelligibility of natural visual and audiovisual speech compared to re-synthesized speech movements rendered by a talking head. This talking head is created using the speaker cloning methodology of the Institut de la Communication Parlée in Grenoble (now department for speech and cognition in GIPSA-Lab). A German speaker with colored markers on the face was recorded audiovisually using multiple cameras. The three-dimensional coordinates of the markers were extracted and parameterized. Spoken VCV sequences were then visually re-synthesized. A perception experiment was carried out to measure the visual and audiovisual intelligibility of natural and synthesized video, using the original audio with and without added noise. Identification scores show that the clone is capable of recovering almost 70% of the intelligibility gain provided by the original face. Part of this loss is due to missing visual cues in the present synthesis, due notably to the lack of a tongue.\n",
    ""
   ]
  },
  "lucey07_avsp": {
   "authors": [
    [
     "Patrick",
     "Lucey"
    ],
    [
     "Gerasimos",
     "Potamianos"
    ],
    [
     "Sridha",
     "Sridharan"
    ]
   ],
   "title": "An extended pose-invariant lipreading system",
   "original": "av07_L2-2",
   "page_count": 5,
   "order": 5,
   "p1": "paper L2-2",
   "pn": "",
   "abstract": [
    "In recent work, we have concentrated on the problem of lipreading from non-frontal views (poses). In particular, we have focused on the use of profile views, and proposed two approaches for lipreading on basis of visual features extracted from such views: (a) Direct statistical modeling of the features, namely use of view-dependent statistical models; and (b) Normalization of such features by their projection onto the \"space\" of frontal-view visual features, which allows employing one set of statistical models for all available views. The latter approach has been considered for two only poses (frontal and profile views), and for visual features of a specific dimensionality. In this paper, we further extend this work, by investigating its applicability to the case where data from three views are available (frontal, left- and right-profile). In addition, we examine the effect of visual feature dimensionality on the pose-normalization approach. Our experiments demonstrate that results generalize well to three views, but also that feature dimensionality is crucial to the effectiveness of the approach. In particular, feature dimensionality larger than 30 is detrimental to multi-pose visual speech recognition performance.\n",
    ""
   ]
  },
  "elisei07_avsp": {
   "authors": [
    [
     "Frédéric",
     "Elisei"
    ],
    [
     "Gérard",
     "Bailly"
    ],
    [
     "Alix",
     "Casari"
    ],
    [
     "Stephan",
     "Raidt"
    ]
   ],
   "title": "Towards eye gaze aware analysis and synthesis of audiovisual speech",
   "original": "av07_L2-3",
   "page_count": 6,
   "order": 6,
   "p1": "paper L2-3",
   "pn": "",
   "abstract": [
    "Eye gaze plays many important roles in audiovisual speech, especially in face-to-face interactions. Eyelid shapes are known to correlate with gaze direction. This correlation is perceived and should be restored when animating 3D talking heads. This paper presents a data-based construction method that models the users eyelid geometric deformations caused by gazing and blinking during conversation. This 3D eyelid and gaze model has been used to analyze and automatically reconstruct our German speakers gaze. This can potentially complement or replace infra-red based eye tracking when it is important to collect not only where the user looks but also how (ocular expressions). This method may be used as a tool to study expressive speech and gaze patterns related to cognitive activities (speaking, listening, thinking...).\n",
    ""
   ]
  },
  "auerjr07_avsp": {
   "authors": [
    [
     "Edward T.",
     "Auer Jr"
    ]
   ],
   "title": "Further modeling of the effects of lexical uniqueness in speechreading: examining individual differences in segmental perception and testing predictions for sentence level performance",
   "original": "av07_L3-1",
   "page_count": 4,
   "order": 7,
   "p1": "paper L3-1",
   "pn": "",
   "abstract": [
    "Two experiments are reported that extend previous investigations of the influence of the lexicon on visual spoken word recognition. Experiment 1 investigated individual differences in segment level perception. Behavioral identification of consonants and vowels presented in either consonant-vowel or consonant-vowel-consonant nonsense syllables were collected from participant groups chosen to differ in their sentential speechreading ability (6 - more accurate versus 6 - less accurate). The results demonstrate that the two groups differed in vowel identification accuracy, but not in consonant identification accuracy. The groups also differed in the patterning of confusions between both consonant and vowel segments. This resulted in different degrees of modeled uniqueness and expected lexical equivalence class sizes between the two groups. Experiment 2 extended the lexical modeling approach [1] to sentence length stimuli. A set of sentences was developed that comprised key words predicted to be easy, moderate, or difficult to recognize. 123 (41- easy, 41-moderate, 41-difficult) spoken sentences were presented visual-alone to 40 participants for open set identification. 301 keywords spoken in isolation were presented visual-alone to 23 participants for open set identification. Identification accuracy varied as a function of predicted difficulty for both words spoken in isolation and within sentence context. The effect of predicted difficulty on identification accuracy interacted with presentation context (sentence versus isolation). These results suggest that the advantage enjoyed by words predicted to be easy to recognize in isolation is reduced when those words are spoken in sentential contexts.\n",
    ""
   ]
  },
  "jesse07_avsp": {
   "authors": [
    [
     "Alexandra",
     "Jesse"
    ],
    [
     "James M.",
     "McQueen"
    ]
   ],
   "title": "Visual lexical stress information in audiovisual spoken-word recognition",
   "original": "av07_L3-2",
   "page_count": 5,
   "order": 8,
   "p1": "paper L3-2",
   "pn": "",
   "abstract": [
    "Listeners use suprasegmental auditory lexical stress information to resolve the competition words engage in during spoken-word recognition. The present study investigated whether (a) visual speech provides lexical stress information, and, more importantly, (b) whether this visual lexical stress information is used to resolve lexical competition. Dutch word pairs that differ in the lexical stress realization of their first two syllables, but not segmentally (e.g., 'OCtopus' and 'okTOber'; capitals marking primary stress) served as auditory-only, visual-only, and audiovisual speech primes. These primes either matched (e.g., 'OCto-'), mismatched (e.g., 'okTO-'), or were unrelated to (e.g., 'maCHI-') a subsequent printed target (octopus), which participants had to make a lexical decision to. To the degree that visual speech contains lexical stress information, lexical decisions to printed targets should be modulated through the addition of visual speech. Results show, however, no evidence for a role of visual lexical stress information in audiovisual spoken-word recognition.\n",
    ""
   ]
  },
  "knowland07_avsp": {
   "authors": [
    [
     "Vicky",
     "Knowland"
    ],
    [
     "Jyrki",
     "Tuomainen"
    ],
    [
     "Stuart",
     "Rosen"
    ]
   ],
   "title": "The effects of perceptual load and set on audio-visual speech integration",
   "original": "av07_L3-3",
   "page_count": 1,
   "order": 9,
   "p1": "paper L3-3",
   "pn": "",
   "abstract": [
    "This study examines the hypothesis that audio-visual integration of speech requires both expectation to perceive speech and sufficient attentional resources to allow multimodal integration. Audio-visual integration was measured by recording susceptibility to the McGurk effect whilst participants simultaneously performed a primary visual task under conditions of high or low perceptual load. According to the perceptual load hypothesis (Lavie & Tsal 1994) distracter stimuli (in this case the moving lips) are only processed if the detection of target stimuli in the primary visual task does not exceed attentional capacity limits. If this hypothesis is accurate then, under conditions of high perceptual load, multimodal integration during speech perception will only occur if this integration is a pre-attentive process. In addition, instead of natural syllables, half the participants were played sine wave speech (SWS) tokens, which may be heard as non-speech sounds, such as beeps and whistles, or as speech sounds, depending on the listeners expectations. Tuomainen et al (2005) demonstrated that audio-visual integration using SWS only occurs when participants are expecting auditory speech stimuli. We anticipated that auditory and visual stimuli will be integrated when participants are in speech mode and only under these conditions will perceptual load influence the degree of integration. When these same stimuli are not expected to be speech no integration will occur and perceptual load will not have any influence over the level of integration. The results will be discussed.\n",
    ""
   ]
  },
  "traunmuller07_avsp": {
   "authors": [
    [
     "Hartmut",
     "Traunmüller"
    ],
    [
     "Niklas",
     "Öhrström"
    ]
   ],
   "title": "The auditory and the visual percept evoked by the same audiovisual vowels",
   "original": "av07_L4-1",
   "page_count": 6,
   "order": 10,
   "p1": "paper L4-1",
   "pn": "",
   "abstract": [
    "In analyses and models of audiovisual speech perception, it has been common to consider three percepts: (1) the auditory percept evoked by acoustic stimuli, (2) the visual percept evoked by optic stimuli and (3) a common percept evoked by synchronous optic and acoustic stimuli. Here, it is shown that a vocal percept that is heard and influenced by vision has to be distinguished from a gestural percept that is seen and influenced by audition. In the two experiments reported, syllables distinguished solely by their vowels [i], [y] or [e] were presented to phonetically sophisticated subjects auditorily, visually and in incongruently cross-dubbed audiovisual form. In the first, the subjects rated roundedness, lip spreading, openness and backness of the vowels they heard - in the second of those they saw. The results confirmed that roundedness is mainly heard by eye while openness is heard by ear. Heard backness (retraction) varied with the acoustic and optic presence of roundedness. Seen openness was substantially influenced by acoustic cues, while there was no such influence on seen roundedness. The results are discussed in the context of theories and models.\n",
    ""
   ]
  },
  "swerts07_avsp": {
   "authors": [
    [
     "Marc",
     "Swerts"
    ],
    [
     "Emiel",
     "Krahmer"
    ]
   ],
   "title": "Acoustic effects of visual beats",
   "original": "av07_L4-2",
   "page_count": 6,
   "order": 11,
   "p1": "paper L4-2",
   "pn": "",
   "abstract": [
    "Speakers employ acoustic cues (pitch accents) to indicate that a word is important, but may also use visual cues (such as manual beat gestures, head nods, and eyebrow movements) for this purpose. Even though these acoustic and visual cues are related, the exact nature of this relationship is far from well understood. We investigate whether producing a visual beat leads to changes in how acoustic prominence is realized in speech. For this, we use an original experimental paradigm in which speakers are instructed to realize a target sentence with different distributions of acoustic and visual cues for prominence. Acoustic analyses reveal that the production of a visual beat indeed has an effect on the acoustic realization of the co-occuring speech, in particular on duration and the higher formants (F2 and F3), independent of the kind of visual beat and of the presence and position of pitch accents.\n",
    ""
   ]
  },
  "barbosa07_avsp": {
   "authors": [
    [
     "Adriano V.",
     "Barbosa"
    ],
    [
     "Hani C.",
     "Yehia"
    ],
    [
     "Eric",
     "Vatikiotis-Bateson"
    ]
   ],
   "title": "MATLAB toolbox for audiovisual speech processing",
   "original": "av07_L5-1",
   "page_count": 6,
   "order": 12,
   "p1": "paper L5-1",
   "pn": "",
   "abstract": [
    "Audiovisual speech processing has reached a stage of maturity where there are now numerous computational procedures needed to measure and assess multimodal signals. However, as is often the case, the results of these procedures are better known than the procedures themselves. This paper presents a MATLAB toolbox consisting of an extensive collection of tools we have developed over the past 10 years. These tools are not intended to be the final answer for multimodal speech analysis; rather they are presented as an easy-to-use and welldocumented library whose scope is sufficiently broad to be useful to both experts and novices.\n",
    "The toolbox includes procedures for measuring, organizing, modeling, and validating multiple streams of time-varying data, including acoustics, two- and threedimensional motions of the speaker. In addition to physical and derived (from video) marker data, new functions have been implemented that incorporate optical flow techniques based on the OpenCV library. When complete the toolbox will allow us to track human body gestures during speech from video noninvasively and to quantify the correspondences between different performance modalities within and across speakers.\n",
    ""
   ]
  },
  "barker07_avsp": {
   "authors": [
    [
     "Jon",
     "Barker"
    ],
    [
     "Xu",
     "Shao"
    ]
   ],
   "title": "Audio-visual speech fragment decoding",
   "original": "av07_L5-2",
   "page_count": 6,
   "order": 13,
   "p1": "paper L5-2",
   "pn": "",
   "abstract": [
    "This paper presents a robust speech recognition technique called audio-visual speech fragment decoding (AV-SFD), in which the visual signal is exploited both as a cue for source separation and as a carrier of phonetic information. The model builds on the existing audio-only SFD technique which, based on the auditory scene analysis account of perceptual organisation, works by combining a bottom-up layer which identifies sound fragments, and a model-driven layer which searches for fragment groupings that can be interpreted as recognisable speech utterances. In AV-SFD, the visual signal is used in the model-driven stage improving the ability of the decoder to distinguish between foreground and background fragments. The system has been evaluated using an audio-visual version of Pascal Speech Separation Challenge. At low SNRs, recognition error rates are reduced by around 20% relative to the performance of a conventional multistream AV-ASR system.\n",
    ""
   ]
  },
  "hu07_avsp": {
   "authors": [
    [
     "Roland",
     "Hu"
    ],
    [
     "Robert I.",
     "Damper"
    ]
   ],
   "title": "Audio-visual person identification on the XM2VTS database",
   "original": "av07_L5-3",
   "page_count": 6,
   "order": 14,
   "p1": "paper L5-3",
   "pn": "",
   "abstract": [
    "This paper presents a multimodal person identification system based on combination of audio and visual classifiers. The audio classifier was built by using mel-frequency cepstrum coefficient features and Gaussian mixture models. The visual classifier was implemented by Haar-like features and AdaBoost algorithm for face detection, and principal component analysis for identification. A new method is proposed to estimate the optimal weighting parameter based on probability density function estimation under Gaussian assumptions. Simulations indicate that the proposed method obtains slightly better results than the frequently-used empirical method of optimising on held-out training data.\n",
    ""
   ]
  },
  "hazan07_avsp": {
   "authors": [
    [
     "Valerie",
     "Hazan"
    ],
    [
     "Anke",
     "Sennema"
    ]
   ],
   "title": "The impact of visual training on the perception and production of a non-native phonetic contrast",
   "original": "av07_L6-1",
   "page_count": 6,
   "order": 15,
   "p1": "paper L6-1",
   "pn": "",
   "abstract": [
    "Many studies have shown that the perception of difficult non-native phonetic contrasts can be improved through auditory training. More recently, studies comparing the effectiveness of auditory and audiovisual training have shown an advantage for audiovisual training at least for contrasts that are sufficiently visually-salient. Audiovisual training also led to improvements in the pronunciation of the trained consonants. The current study, which trained the /l/-/r/ contrast with Japanese learners of English, investigated training effectiveness using visual stimuli alone, i.e. with trainees seeing but not hearing the speakers. Fifteen Japanese students participated in the seven-session training programme and there were eleven controls. Pre/post tests were carried out in auditory (A), visual (V) and audiovisual (AV) test conditions and participants were also recorded before and after the training reading a list of words which included the sounds /l/ and /r/. Visual training was successful in significantly increasing the discriminability of the /l/-/r/ contrast in trainees in V and AV test conditions but there was no carry-over to the A condition. There was generalisation to nonsense words by unknown speakers. Visual influence was also tested by comparing performance in A and AV test conditions, and in a simple McGurk task. AV benefit increased to a greater extent for trainees. In the McGurk test, visual influence in the identification of discrepant (A /ba/- V /ga/) stimuli increased significantly in the post-test but this also occurred for controls, so might be due to a foreignlanguage effect as most participants were attending a phonetics summer school in a foreign country. Results of an identification and rating test evaluating the participants pronunciation of /l/ and /r/ pre- and post-training showed no evidence of any improvements in pronunciation following visual training.\n",
    ""
   ]
  },
  "baier07_avsp": {
   "authors": [
    [
     "Rebecca",
     "Baier"
    ],
    [
     "William J.",
     "Idsardi"
    ],
    [
     "Jeffrey",
     "Lidz"
    ]
   ],
   "title": "Two-month-olds are sensitive to lip rounding in dynamic and static speech events",
   "original": "av07_L6-2",
   "page_count": 5,
   "order": 16,
   "p1": "paper L6-2",
   "pn": "",
   "abstract": [
    "Our research replicates and extends previous work on infant audio-visual speech perception, demonstrating that two-month-old infants have knowledge of the audio-visual connection between the visual aspects of lip-posture and the pronunciation of vowels and glides. Using the intermodal preferential looking paradigm [6] the infants display a clear ability to distinguish /a/-/u/, /i/-/u/ and /i/-/wi/. The infants ability to discriminate /i/-/wi/ shows that even dynamic aspects of speech production within single syllables are salient to the infants.\n",
    ""
   ]
  },
  "kim07_avsp": {
   "authors": [
    [
     "Jeesun",
     "Kim"
    ],
    [
     "Chris",
     "Davis"
    ]
   ],
   "title": "Restoration effects in auditory and visual speech",
   "original": "av07_L6-3",
   "page_count": 4,
   "order": 17,
   "p1": "paper L6-3",
   "pn": "",
   "abstract": [
    "In this paper we examined the use of context to \"restore\" a degraded auditory signal using the standard Auditory-Only (AO) phoneme restoration paradigm and an Auditory-Visual (AV) version. The experiment was a modified replication of Trout and Poser who used the Samuel discrimination paradigm in which white noise was either superimposed (added) on a phoneme in a word or replaced it entirely. One modification was the use of a 2IFC task in which both the added and replaced versions were presented. Another modification was that we used single words rather than sentences. The key issue was whether visual speech would augment or decrement the phoneme restoration effect. Trout and Poser used a signal detection analysis and found that visual speech reduced the bias to report a stimulus as intact (added). We found that visual speech increased the phoneme restoration effect.\n",
    ""
   ]
  },
  "gan07_avsp": {
   "authors": [
    [
     "Tian",
     "Gan"
    ],
    [
     "Wolfgang",
     "Menzel"
    ],
    [
     "Shiqiang",
     "Yang"
    ]
   ],
   "title": "An audio-visual speech recognition framework based on articulatory features",
   "original": "av07_P01",
   "page_count": 5,
   "order": 18,
   "p1": "paper P01",
   "pn": "",
   "abstract": [
    "This paper presents an audio-visual speech recognition framework based on articulatory features, which tries to combine the advantages of both areas, and shows a better recognition accuracy compared to a phone-based recognizer. In our approach, we use HMMs to model abstract articulatory classes, which are extracted in parallel from both the speech signal and the video frames. The N-best outputs of these independent classifiers are combined to decide on the best articulatory feature tuples. By mapping these tuples to phones, a phone stream can be generated. A lexical search finally maps this phone stream to meaningful word transcriptions. We demonstrate the potential of our approach by a preliminary experiment on the GRID database, which contains continuous English voice commands for a small vocabulary task.\n",
    ""
   ]
  },
  "cave07_avsp": {
   "authors": [
    [
     "Christian",
     "Cavé"
    ],
    [
     "Aurélie",
     "Stroumza"
    ],
    [
     "Mireille",
     "Bastien-Toniazzo"
    ]
   ],
   "title": "The Mcgurk effect in dyslexic and normal-reading children: an experimental study",
   "original": "av07_P02",
   "page_count": 2,
   "order": 19,
   "p1": "paper P02",
   "pn": "",
   "abstract": [
    "The McGurk effect was investigated in a group of ten-yearold dyslexic children and in two control groups of normal readers. Audio and audiovisual stimuli were presented in silence or with a masking noise. The results indicated no significant differences between the three groups for the auditory stimuli. For the audiovisual stimuli, the dyslexic group showed fewer illusory percepts than the group of sameage normal readers but performed similarly to the group with the same reading level.\n",
    ""
   ]
  },
  "ali07_avsp": {
   "authors": [
    [
     "Azra Nahid",
     "Ali"
    ]
   ],
   "title": "Exploring semantic cueing effects using Mcgurk fusion",
   "original": "av07_P03",
   "page_count": 6,
   "order": 20,
   "p1": "paper P03",
   "pn": "",
   "abstract": [
    "In this paper highlights that audiovisual speech perception is not all autonomous and that lexical semantic context and word meaning can influence McGurk fusion and the rate of fusion responses too. Sentence context can actually diminish McGurk fusion rate when semantic cueing from the context biases against the fusion expected. However, it did not block the fusion entirely even when the cue strongly favoured the audio or visual channel. Probabilistic grammars were used to measure the strength and directionality of speech cueing effects, using conditional probabilities estimated from relative frequencies in the British National Corpus. Results from conditional probabilities showed that the greater the strength of the semantic bias the greater the reduction in fusion rate. Whereas, positive semantic cueing examples of Sharma (1989) showing an increase in fusion rates when the phrase favoured the expected fusion. Audiovisual speech perception can be influenced by the semantic context which adds to evidence from other types of experiment that speech perception is not all bottom-up processing.\n",
    ""
   ]
  },
  "sugano07_avsp": {
   "authors": [
    [
     "Yoshimori",
     "Sugano"
    ]
   ],
   "title": "Modeling the auditory capture effect in a bimodal synchronous tapping",
   "original": "av07_P04",
   "page_count": 4,
   "order": 21,
   "p1": "paper P04",
   "pn": "",
   "abstract": [
    "Synchronous tapping to simultaneously presented asynchronous visual sequence and auditory sequence is more attracted to the concurrently presented auditory sequences than the reverse. This we will call the auditory capture effect (ACE). This study examined ACE over a wide range of sequence IOI (500 ms, 1000 ms, 1500 ms). Moreover, a model fitting the response time (RT) data was considered. The model best fit was the Gabor function, which is the product of a sine and a Gaussian. The model was shown to have a good fit with the variety of the RT data transition. The psychological meaning of the model and the model's prediction of the fluctuation of taps will be discussed.\n",
    ""
   ]
  },
  "vroomen07_avsp": {
   "authors": [
    [
     "Jean",
     "Vroomen"
    ],
    [
     "Sabine van",
     "Linden"
    ],
    [
     "Martijn",
     "Baart"
    ]
   ],
   "title": "Lipread aftereffects in auditory speech perception: measuring aftereffects after a twenty-four hours delay",
   "original": "av07_P05",
   "page_count": 3,
   "order": 22,
   "p1": "paper P05",
   "pn": "",
   "abstract": [
    "Lipreading can evoke an immediate bias on auditory phoneme perception and it can produce an aftereffect reflecting a shift in the phoneme boundary caused by exposure to an auditory ambiguous stimulus that is combined with nonambiguous lipread speech (recalibration). Here, we tested the stability of lipread-induced recalibration over time. Aftereffects were measured directly after exposure and after 24 hours. Aftereffects dissipated quickly during testing and were not observable anymore after a 24 hours delay.\n",
    ""
   ]
  },
  "berger07_avsp": {
   "authors": [
    [
     "Marie-Odile",
     "Berger"
    ]
   ],
   "title": "Realistic face animation from sparse stereo meshes",
   "original": "av07_P06",
   "page_count": 6,
   "order": 23,
   "p1": "paper P06",
   "pn": "",
   "abstract": [
    "Being able to produce realistic facial animation is crucial for many speech applications in language learning technologies. For reaching realism, it is necessary to acquire and to animate dense 3D models of the face. Recovering dense models is often achieved using stereovision techniques. Unfortunately, reconstruction artifacts are common and are mainly due to the difficulty to match points on untextured areas of the face between images. In this paper, we propose a robust and fully automatic method to produce realistic dense animation. Our input data are a dense 3D mesh of the talker obtained for one viseme as well as a corpus of stereo sequences of a talker painted with markers that allows the face kinematics to be learned. The main contribution of the paper is to transfer the kinematics learned on a sparse mesh onto the 3D dense mesh, thus allowing dense facial animation. Examples of face animations are provided which prove the reliability of the proposed method.\n",
    ""
   ]
  },
  "rivet07_avsp": {
   "authors": [
    [
     "Bertrand",
     "Rivet"
    ],
    [
     "Laurent",
     "Girin"
    ],
    [
     "Christine",
     "Servière"
    ],
    [
     "Dinh-Tuan",
     "Pham"
    ],
    [
     "Christian",
     "Jutten"
    ]
   ],
   "title": "Audiovisual speech source separation: a regularization method based on visual voice activity detection",
   "original": "av07_P07",
   "page_count": 5,
   "order": 24,
   "p1": "paper P07",
   "pn": "",
   "abstract": [
    "Audio-visual speech source separation consists in mixing visual speech processing techniques (e.g. lip parameters tracking) with source separation methods to improve and/or simplify the extraction of a speech signal from a mixture of acoustic signals. In this paper, we present a new approach to this problem: visual information is used here as a voice activity detector (VAD). Results show that, in the difficult case of realistic convolutive mixtures, the classic problem of the permutation of the output frequency channels can be solved using the visual information with a simpler processing than when using only audio information.\n",
    ""
   ]
  },
  "davis07_avsp": {
   "authors": [
    [
     "Chris",
     "Davis"
    ],
    [
     "Jeesun",
     "Kim"
    ],
    [
     "Takaaki",
     "Kuratate"
    ],
    [
     "Johnson",
     "Chen"
    ],
    [
     "S.",
     "Stelarc"
    ],
    [
     "Denis",
     "Burnham"
    ]
   ],
   "title": "Making a thinking-talking head",
   "original": "av07_P08",
   "page_count": 4,
   "order": 25,
   "p1": "paper P08",
   "pn": "",
   "abstract": [
    "This paper describes the Thinking-Talking Head; an interdisciplinary project that sits between and draws upon engineering/computer science and behavioural/cognitive science; research and performance; implementation and evaluation. The project involves collaboration between computer scientists, engineers, language technologists and cognitive scientists, and its aim is twofold (a) to create a 3-D computer animation of a human head that will interact in real time with human agents, and (b) to serve as a research platform to drive research in the contributing disciplines, and in talking head research in general. The thinkingtalking head will emulate elements of face-to-face conversation through speech (including intonation), gaze and gesture. So it must have an active sensorium that accurately reflects the properties of its immediate environment, and must be able to generate appropriate communicative signals to feedback to the interlocutor. Here we describe the current implementation and outline how we are tackling issues concerning both the outputs (synthetic voice, visual speech, facial expressiveness and naturalness) from and inputs (auditory-visual speech recognition, emotion recognition, auditory-visual speaker localization) to the head. We describe how these head functions will be tuned and evaluated using various paradigms, including an imitation paradigm.\n",
    ""
   ]
  },
  "bertrand07_avsp": {
   "authors": [
    [
     "Roxane",
     "Bertrand"
    ],
    [
     "Gaëlle",
     "Ferré"
    ],
    [
     "Philippe",
     "Blache"
    ],
    [
     "Robert",
     "Espesser"
    ],
    [
     "Stéphane",
     "Rauzy"
    ]
   ],
   "title": "Backchannels revisited from a multimodal perspective",
   "original": "av07_P09",
   "page_count": 5,
   "order": 26,
   "p1": "paper P09",
   "pn": "",
   "abstract": [
    "Abstract In this study, we analyze the role of several linguistic cues (prosodic units, pitch contours, discourse markers, morphological categories, and gaze direction) in French turntaking face-to-face interactions. Specifically, we investigate vocal and gestural backchannel signals (BCs) produced by a recipient to show his active listening. We show that some particular pitch contours and discursive markers play a systematic role in inducing both gestural and vocal BCs. Conversely, morphological categories and gestural cues rather play a role for gestural BCs.\n",
    ""
   ]
  },
  "pilling07_avsp": {
   "authors": [
    [
     "Michael",
     "Pilling"
    ],
    [
     "Sharon",
     "Thomas"
    ]
   ],
   "title": "Temporal factors in the electrophysiological markers of audiovisual speech integration",
   "original": "av07_P10",
   "page_count": 5,
   "order": 27,
   "p1": "paper P10",
   "pn": "",
   "abstract": [
    "Recent research had shown that concurrent visual speech modulates the cortical event-related potential N1/P2 to auditory speech. Audiovisually presented speech results in an N1-P2 that is reduced in peak amplitude and with shorter peak latencies than unimodal auditory speech [11]. This effect on the N1/P2 is consistent with a model in which visual speech integrates with auditory speech at an early processing stage in the auditory cortex by suppressing auditory cortical activity. We examined the effects of audiovisual temporal synchrony in producing modulations in the N1/P2. With the visual stream presented in synchrony with the auditory stream our results replicated the basic findings of reduced peak amplitudes in the N1/P2 compared to a unimodal auditory condition. With the visual stream temporally mismatched with the auditory stream (so that the auditory speech signal was presented 200 ms before its recorded position) the recorded N1/P2 was similar to unimodal auditory speech. The results are discussed in terms of Wassenhoves analysis-by-synthesis model of audiovisual integration.\n",
    ""
   ]
  },
  "irwin07_avsp": {
   "authors": [
    [
     "Amy",
     "Irwin"
    ],
    [
     "Sharon",
     "Thomas"
    ],
    [
     "Michael",
     "Pilling"
    ]
   ],
   "title": "Regional accent familiarity and speechreading performance",
   "original": "av07_P11",
   "page_count": 4,
   "order": 28,
   "p1": "paper P11",
   "pn": "",
   "abstract": [
    "The effect of accent [pronunciation of speech sounds determined by a speakers regional or national location] on auditory speech comprehension has been well documented, but research is lacking as to its effects on visual speech understanding. In order to address this, the present study examined the effect of regional accent variation on speechreading performance. The aim was to determine if familiarity with an accent would prove to be an advantage when speechreading, and if certain regional accents would be associated with a greater clarity of visual signal than others. The study examined both the identification of accent based on visual or auditory sentences and the effect of accent variation on auditory and visual speech comprehension. Of particular interest was the effect of accent on speechreading accuracy. The two British accents chosen for comparison were Nottingham and Glaswegian.\n",
    "Results showed that accent discrimination is possible using only visual speech. Greater accuracy was achieved when participants differentiated between the accents based on auditory sentences, but with visual speech, performance was also greater than chance [p. > .05]. This suggests that the two accents have sufficiently distinct patterns of visual articulation and auditory pronunciation to allow participants to discriminate between them. Further, the participants visual and auditory speech recognition scores were adversely affected by the use of an unfamiliar accent, with keyword recognition accuracy being reduced. These findings both replicate previous findings on the effects of auditory accent, and indicate that accent can also impact the understanding of visual speech.\n",
    ""
   ]
  },
  "thomas07_avsp": {
   "authors": [
    [
     "Sharon M.",
     "Thomas"
    ],
    [
     "Michael",
     "Pilling"
    ]
   ],
   "title": "Benefits of facial and textual information in understanding of vocoded speech",
   "original": "av07_P12",
   "page_count": 5,
   "order": 29,
   "p1": "paper P12",
   "pn": "",
   "abstract": [
    "Exposure to audiovisually presented vocoded speech is more effective than exposure to auditory-only vocoded speech in improving the subsequent ability to understand vocoded speech [1]. In addition, improvements in the audiovisual training condition were more rapid and greater in magnitude than in the auditory-only condition. The current study was conducted to establish whether exposure to concurrent textual information also results in improvements in the ability to recognize vocoded speech. Baseline measures of identification performance with auditory-only vocoded speech sentences were assessed for 45 participants. Participants then performed a speech identification task, where they were exposed to vocoded speech in either audiovisual (Group 1), auditory-only (Group 2), or auditory-only with concurrent text conditions (Group 3). Following exposure, participants were tested again on identification performance with auditory-only vocoded speech. Exposure to concurrent text improved subsequent understanding of vocoded speech, to a level similar to that seen with audiovisual speech exposure. In a second experiment, groups of normal hearing adults were exposed to vocoded non-lexical nonsense words in auditory-only with text and audiovisual speech presentation conditions. Exposure to both nonsense audiovisual and concurrent text conditions improved subsequent understanding of lexical auditoryonly vocoded speech, and there was no difference between the levels of improvement. In summary, to computerbased audiovisual and concurrent text exposure improves the ability to recognize vocoded speech over exposure to auditory stimuli alone. This effect does not appear to be dependent on exposure to lexical items.\n",
    ""
   ]
  },
  "melenchon07_avsp": {
   "authors": [
    [
     "Javier",
     "Melenchón"
    ],
    [
     "Jordi",
     "Simó"
    ],
    [
     "Germán",
     "Cobo"
    ],
    [
     "Elisa",
     "Martínez"
    ]
   ],
   "title": "Objective viseme extraction and audiovisual uncertainty: estimation limits between auditory and visual modes",
   "original": "av07_P13",
   "page_count": 4,
   "order": 30,
   "p1": "paper P13",
   "pn": "",
   "abstract": [
    "An objective way to obtain consonant visemes for any given Spanish speaking person is proposed. Its face is recorded while speaking a balanced set of sentences and stored as an audiovisual sequence. Visual and auditory modes are segmented by allophones and a distance matrix is built to find visually similar perceived allophones. Results show high correlation with tedious subjective earlier evaluations regardless of being in English. In addition, estimation between modes is also studied, revealing a tradeoff between performances in both modes: given a set of auditory groups and another of visual ones for each grouping criteria, increasing the estimation performance of one mode is translated to decreasing that of the other one. Moreover, the tradeoff is very similar (<7% between maximum and minimum values) in all observed examples.\n",
    ""
   ]
  },
  "rivet07b_avsp": {
   "authors": [
    [
     "Bertrand",
     "Rivet"
    ],
    [
     "Andrew",
     "Aubrey"
    ],
    [
     "Laurent",
     "Girin"
    ],
    [
     "Yulia",
     "Hicks"
    ],
    [
     "Christian",
     "Jutten"
    ],
    [
     "Jonathon",
     "Chambers"
    ]
   ],
   "title": "Development and comparison of two approaches for visual speech analysis with application to voice activity detection",
   "original": "av07_P14",
   "page_count": 5,
   "order": 31,
   "p1": "paper P14",
   "pn": "",
   "abstract": [
    "In this paper we present two novel methods for visual voice activity detection (V-VAD) which exploit the bimodality of speech (i.e. the coherence between speakers lips and the resulting speech). The first method uses appearance parameters of a speakers lips, obtained from an active appearance model (AAM). An HMM then dynamically models the change in appearance over time. The second method uses a retinal filter on the region of the lips to extract the required parameter. A corpus of a single speaker is applied to each method in turn, where each method is used to classify voice activity as speech or non speech. The efficiency of each method is evaluated individually using receiver operating characteristics and their respective performances are then compared and discussed. Both methods achieve a high correct silence detection rate for a small false detection rate.\n",
    ""
   ]
  },
  "batliner07_avsp": {
   "authors": [
    [
     "Anton",
     "Batliner"
    ],
    [
     "Christian",
     "Hacker"
    ],
    [
     "Moritz",
     "Kaiser"
    ],
    [
     "Hannes",
     "Mögele"
    ],
    [
     "Elmar",
     "Nöth"
    ]
   ],
   "title": "Taking into account the user²s focus of attention with the help of audio-visual information: towards less artificial human-machine-communication",
   "original": "av07_P15",
   "page_count": 6,
   "order": 32,
   "p1": "paper P15",
   "pn": "",
   "abstract": [
    "In the German SmartWeb project, the user is interacting with the web via a PDA in order to get information on, for example, points of interest. To overcome the tedious use of devices such as push-to-talk, but still to be able to tell whether the user is addressing the system or talking to herself or to a third person, we developed a module that monitors speech and video in parallel. Our database (3.2 hours of speech, 2086 turns) has been recorded in a real-life setting, indoors as well as outdoors, with unfavourable acoustic and light conditions. With acoustic features, we classify up to 4 different types of addressing (talking to the system: On-Talk, reading from the display: Read Off- Talk, paraphrasing information presented on the screen: Paraphrasing Off-Talk, talking to a third person or to oneself: Spontaneous Off-Talk). With a camera integrated in the PDA, we record the users face and decide whether she is looking onto the PDA or somewhere else. We use three different types of turn features based on classification scores of frame-based face detection and word-based analysis: 13 acoustic-prosodic features, 18 linguistic features, and 9 video features. The classification rate for acoustics only is up to 62 % for the four-class problem, and up to 77 % for the most important two-class problem \"user is focussing on interaction with the system or not\". For video only, it is 45 % and 71 %, respectively. By combining the two modalities, and using linguistic information in addition, classification performance for the two-class problem so far rises up to 85 %.\n",
    ""
   ]
  },
  "milner07_avsp": {
   "authors": [
    [
     "Ben",
     "Milner"
    ],
    [
     "Ibrahim",
     "Almajai"
    ]
   ],
   "title": "Noisy audio speech enhancement using Wiener filters derived from visual speech",
   "original": "av07_P16",
   "page_count": 6,
   "order": 33,
   "p1": "paper P16",
   "pn": "",
   "abstract": [
    "The aim of this paper is to use visual speech information to create Wiener filters for audio speech enhancement. Wiener filters require estimates of both clean speech statistics and noisy speech statistics. Noisy speech statistics are obtained from the noisy input audio while obtaining clean speech statistics is more difficult and is a major problem in the creation of Wiener filters for speech enhancement. In this work the clean speech statistics are estimated from frames of visual speech that are extracted in synchrony with the audio. The estimation procedure begins by modelling the joint density of clean audio and visual speech features using a Gaussian mixture model (GMM). Using the GMM and an input visual speech vector a maximum a posterior (MAP) estimate of the audio feature is made. The effectiveness of speech enhancement using the visually-derived Wiener filter has been compared to a conventional audio-based Wiener filter implementation using a perceptual evaluation of speech quality (PESQ) analysis. PESQ scores in train noise at different signal-to-noise ratios (SNRs) show that the visuallyderived Wiener filter significantly outperforms the audio- Wiener filter at lower SNRs.\n",
    ""
   ]
  },
  "almajai07_avsp": {
   "authors": [
    [
     "Ibrahim",
     "Almajai"
    ],
    [
     "Ben",
     "Milner"
    ]
   ],
   "title": "Maximising audio-visual speech correlation",
   "original": "av07_P17",
   "page_count": 5,
   "order": 34,
   "p1": "paper P17",
   "pn": "",
   "abstract": [
    "The aim of this work is to investigate a selection of audio and visual speech features with the aim of finding pairs that maximise audio-visual correlation. Two audio speech features have been used in the analysis - filterbank vectors and the first four formant frequencies. Similarly, three visual features have also been considered - active appearance model (AAM), 2-D DCT and cross-DCT. From a database of 200 sentences, audio and visual speech features have been extracted and multiple linear regression used to measure the audio-visual correlation. Results reveal filterbank features to exhibit multiple correlation of around R=0.8 to visual features, while formant frequencies show substantially less correlation to visual features - R=0.6 for formants 1 and 2 and less than R=0.4 for formants 3 and 4. The three visual features show almost identical correlation to the audio features, varying in multiple correlation by less than 0.1, even though the methods of visual feature extraction are very different. Measuring the audio-visual correlation within each phoneme and then averaging the correlation across all phonemes showed an increase in correlation to R=0.9.\n",
    ""
   ]
  },
  "sakamoto07_avsp": {
   "authors": [
    [
     "Shuichi",
     "Sakamoto"
    ],
    [
     "Akihiro",
     "Tanaka"
    ],
    [
     "Komi",
     "Tsumura"
    ],
    [
     "Yôiti",
     "Suzuki"
    ]
   ],
   "title": "Effect of speed difference between time-expanded speech and talker²s moving image on word or sentence intelligibility",
   "original": "av07_P18",
   "page_count": 5,
   "order": 35,
   "p1": "paper P18",
   "pn": "",
   "abstract": [
    "This study investigated effects, on a speech intelligibility, of asynchronicity between a speech signal and a talkers moving image induced by time-expansion of the speech signal. First, a word intelligibility test (Exp. 1) was administered to younger listeners. Words were processed using STRAIGHT software to expand the speech signal by 0 to 400 ms. The word intelligibility test was administered under three conditions: visualonly, auditory-only, and auditory-visual (AV) conditions. Results showed that intelligibility scores under the AV condition were statistically higher than those under the auditory-only condition, even when the speech signal was expanded by 400 ms. Second, a sentence intelligibility test (Exp. 2) was administered to older adults. For all sentences, each phrase was expanded by 0 to 400 ms. This test was administered under the same conditions as those used for Exp. 1. Results showed that sentence intelligibility scores under the AV condition were statistically higher than those under the audio-only condition when the length of expansion was less than or equal to 200 ms. The results of Exp. 1 and Exp. 2 suggest that the talkers moving image is effective to enhance speech intelligibility if the lag between the speech signal and the talkers moving image is less than or equal to 200 ms.\n",
    ""
   ]
  },
  "basirat07_avsp": {
   "authors": [
    [
     "Anahita",
     "Basirat"
    ],
    [
     "Marc",
     "Sato"
    ],
    [
     "Jean-Luc",
     "Schwartz"
    ]
   ],
   "title": "Audiovisual verbal transformations, as a way to study audiovisual interactions in speech perception",
   "original": "av07_P19",
   "page_count": 1,
   "order": 36,
   "p1": "paper P19",
   "pn": "",
   "abstract": [
    "Verbal transformations refer to perceptual changes experienced while listening to a speech form cycled in rapid and continuous repetition. In order to test whether visual information from the speakers articulatory gestures may modify the emergence and stability of verbal auditory percepts, participants were instructed to report any perceptual changes during unimodal, congruent audiovisual and incongruent audiovisual presentations of distinct repeated syllables. In a first experiment, the perceptual stability of reported auditory percepts was significantly modulated by the modality presentation. In a second experiment, when presenting audiovisual stimuli consisting in a stable audio track dubbed with a video track alternating congruent and incongruent stimuli, a strict correlation between the timing of perceptual transitions and the timing of video switches was found. Finally, a third experiment showed that the vocal tract opening onset event provided by the visual input could play the role of a bootstrap mechanism in the search for transformations. Altogether, these results demonstrate the capacity of visual information to control the multistable perception of speech in its phonetic content and temporal course. The Verbal Transformation Effect thus provides a useful experimental paradigm to explore audiovisual interactions in speech perception. The ability of vision to influence verbal transformations in their temporal course raise general questions about how and when the human brain integrates input from different senses. The recent neurophysiological evidence in favour of a cortical \"dorsal route\" (Hickok & Poeppel, 2000, 2004) for speech perception, linking temporal, parietal and frontal regions, and involved both in audiovisual speech perception (Callan et al., 2003; Skipper et al., 2005, 2007) and in verbal transformations (Sato et al., 2004) provides a natural circuit for audiovisual verbal transformations. The fact that this circuit connects perceptual and motor regions, and that motor processes have been advocated to play a role both in the verbal transformation circuit (Sato et al., 2006) and for audiovisual integration in speech perception (Schwartz et al., 1998; Callan et al., 2003, Skipper et al., 2005, 2007) suggests that such processes are indeed involved in multisensory verbal transformations.\n",
    ""
   ]
  },
  "dohen07_avsp": {
   "authors": [
    [
     "Marion",
     "Dohen"
    ],
    [
     "Hélène",
     "Loevenbruck"
    ]
   ],
   "title": "Auditory-visual perception of acoustically degraded prosodic contrastive focus in French",
   "original": "av07_P20",
   "page_count": 4,
   "order": 37,
   "p1": "paper P20",
   "pn": "",
   "abstract": [
    "Previous studies have shown that visual only perception of prosodic contrastive focus in French is possible. The aim of this study was to determine whether the visual modality could be combined to the auditory one and lead to a perceptual enhancement of prosodic focus. In order to examine this question, we carried out auditory only, audiovisual and visual only perception tests. In order to avoid a ceiling effect, auditory only perception of prosodic focus being very high, we used whispered speech for which the acoustic prosodic information is degraded. The productions of two different speakers were used. This test showed that adding the visual modality enhances auditory perception of prosodic focus when the acoustic prosodic cues are degraded.\n",
    ""
   ]
  },
  "theobald07_avsp": {
   "authors": [
    [
     "Barry-John",
     "Theobald"
    ],
    [
     "Nicholas",
     "Wilkinson"
    ]
   ],
   "title": "A real-time speech-driven talking head using active appearance models",
   "original": "av07_P22",
   "page_count": 6,
   "order": 38,
   "p1": "paper P22",
   "pn": "",
   "abstract": [
    "In this paper we describe a real-time speech-driven method for synthesising realistic video sequences of a subject enunciating arbitrary phrases. In an offline training phase an active appearance model (AAM) is constructed from hand-labelled images and is used to encode the face of a subject reciting a few training sentences. Canonical correlation analysis (CCA) coupled with linear regression is then used to model the relationship between auditory and visual features, which is later used to predict visual features from the auditory features for novel utterances.\n",
    "We present results from experiments conducted: 1) to determine the suitability of several auditory features for use in an AAM-based speech-driven talking head, 2) to determine the effect of the size of the training set on the correlation between the auditory and visual features, 3) to determine the influence of context on the degree of correlation, and 4) to determine the appropriate window size from which the auditory features should be calculated. This approach shows promise and a longer term goal is to develop a fully expressive, three-dimensional talking head.\n",
    ""
   ]
  },
  "raidt07_avsp": {
   "authors": [
    [
     "Stephan",
     "Raidt"
    ],
    [
     "Gérard",
     "Bailly"
    ],
    [
     "Frédéric",
     "Elisei"
    ]
   ],
   "title": "Analyzing and modeling gaze during face-to-face interaction",
   "original": "av07_P23",
   "page_count": 6,
   "order": 39,
   "p1": "paper P23",
   "pn": "",
   "abstract": [
    "We present here the analysis of multimodal data gathered during realistic face-to-face interaction of a target speaker with a number of interlocutors. During several dyadic dialogs videos and gaze have been monitored with an original experimental setup using coupled cameras and screens equipped with eye tracking capabilities. For a detailed analysis of gaze patterns we distinguish different regions of interest on the face. With the aim to understand the functions of gaze in social interaction and to develop a coherent gaze control model for our talking heads we investigate the influence of cognitive state and social role on the observed gaze behaviour. Keywords: embodied conversational agents, face-to-face interaction, eye gaze, talking head, gaze model.\n",
    ""
   ]
  },
  "hazan07b_avsp": {
   "authors": [
    [
     "Valerie",
     "Hazan"
    ],
    [
     "Anke",
     "Sennema"
    ]
   ],
   "title": "The impact of visual training on the perception and production of a non-native phonetic contrast",
   "original": "av07_P24",
   "page_count": 6,
   "order": 40,
   "p1": "paper P24",
   "pn": "",
   "abstract": [
    "Many studies have shown that the perception of difficult non-native phonetic contrasts can be improved through auditory training. More recently, studies comparing the effectiveness of auditory and audiovisual training have shown an advantage for audiovisual training at least for contrasts that are sufficiently visually-salient. Audiovisual training also led to improvements in the pronunciation of the trained consonants. The current study, which trained the /l/-/r/ contrast with Japanese learners of English, investigated training effectiveness using visual stimuli alone, i.e. with trainees seeing but not hearing the speakers. Fifteen Japanese students participated in the seven-session training programme and there were eleven controls. Pre/post tests were carried out in auditory (A), visual (V) and audiovisual (AV) test conditions and participants were also recorded before and after the training reading a list of words which included the sounds /l/ and /r/. Visual training was successful in significantly increasing the discriminability of the /l/-/r/ contrast in trainees in V and AV test conditions but there was no carry-over to the A condition. There was generalisation to nonsense words by unknown speakers. Visual influence was also tested by comparing performance in A and AV test conditions, and in a simple McGurk task. AV benefit increased to a greater extent for trainees. In the McGurk test, visual influence in the identification of discrepant (A /ba/- V /ga/) stimuli increased significantly in the post-test but this also occurred for controls, so might be due to a foreignlanguage effect as most participants were attending a phonetics summer school in a foreign country. Results of an identification and rating test evaluating the participants pronunciation of /l/ and /r/ pre- and post- training showed no evidence of any improvements in pronunciation following visual training.\n",
    ""
   ]
  },
  "ouni07_avsp": {
   "authors": [
    [
     "Slim",
     "Ouni"
    ],
    [
     "Kais",
     "Oun"
    ]
   ],
   "title": "Arabic pharyngeals in visual speech",
   "original": "av07_P25",
   "page_count": 5,
   "order": 41,
   "p1": "paper P25",
   "pn": "",
   "abstract": [
    "Many perceptual experiments show that human talkers provide more intelligible visual speech than synthetic talkers. This inferiority of synthetic visual speech might be due to a lack of finer modeling of the parts of the face that are important to lipreading or that some parts of the face that are not generally considered as relevant to visual speech or as not visible in face-to-face communication, might actually provide some information, which humans are capable of decoding. This information might therefore not be modeled accurately in the synthetic speaker. In this paper, we provide evidence from Arabic that some sounds, which are not known as visible, might be recognized correctly visually. We performed a lipreading recognition experiment on Arabic, where a set of consonant-vowel stimuli were presented as visual-only speech and participants were asked to report what they recognized. The resulting consonant confusion matrix shows that some of these pharyngeals were, to some extent, well discriminated. Results are discussed based on the category of phonemes and the vowel context.\n",
    ""
   ]
  },
  "schacht07_avsp": {
   "authors": [
    [
     "Stefan",
     "Schacht"
    ],
    [
     "Oleg",
     "Fallman"
    ],
    [
     "Dietrich",
     "Klakow"
    ]
   ],
   "title": "Fast lip tracking for speech/nonspeech detection",
   "original": "av07_P26",
   "page_count": 1,
   "order": 42,
   "p1": "paper P26",
   "pn": "",
   "abstract": [
    "An efficient speech/nonspeech detection is an important part of any speech recognition system. It allows a good estimation of the background noise, which can be used for noise cancellation techniques like spectral subtraction. Furthermore it avoids the activity of the speech recognizer on unwanted segments of the audio stream. Recently speech recognition has gained popularity on mobile devices, e.g. smartphones. These devices are often used in crowded environments. So the unwanted audio segments often contain speech. This makes a decision, which is based only on audio data, difficult. Since most modern smartphones have a front facing camera we propose a speech/nonspeech detection algorithm that also uses video informations. First we use a modified version of the well known Viola Jones algorithm for face detection to locate the users face in the video stream. Our modifications exploit the typical usage of a mobile device. First we search only for one face in the video stream. The normal distance from the users face to the camera also limits the size of the frame segment which might be classified as a face. If the previous video frame contained a face we start the search in the current frame at the same location. After the localization of the users face we use linear discriminant analysis and polynomial approximation to track the contour of the lips. We combine these informations with the analysis of the audio stream to obtain a robust speech/nonspeech decision. We present an example implementation of this algorithm for a mobile device and evaluation results on a large collection of videos, which were recorded during the SmartWeb project.\n",
    ""
   ]
  },
  "brungart07_avsp": {
   "authors": [
    [
     "Douglas S.",
     "Brungart"
    ],
    [
     "Virginie van",
     "Wassenhove"
    ],
    [
     "Eugene",
     "Brandewie"
    ],
    [
     "Griffin",
     "Romigh"
    ]
   ],
   "title": "The effects of temporal acceleration and deceleration on AV speech perception",
   "original": "av07_P27",
   "page_count": 6,
   "order": 43,
   "p1": "paper P27",
   "pn": "",
   "abstract": [
    "Modern signal processing techniques make it possible to speed up or slow down the apparent speaking rate of an audio-visual (AV) speech stimulus, but little is known about the effect this processing might have on the intelligibility of AV speech signals. In this experiment, AV recordings of phrases from the Modified Rhyme Test (MRT) were accelerated or decelerated by first changing the duration of the audio signal using the PSOLA speech processing algorithm (PRAAT) and then changing the frame rate of the AVI file to maintain synchronization of the audio and visual stimuli. The original speech phrases were recorded at either a fast speaking rate (roughly 5 syllables per second (syl/s)), a normal conversational rate (3.3 syl/s), and a slow rate (1.7 syl/s). The results of a preliminary experiment showed that conversational-rate AV recordings that were shifted in speed to match the slow or fast recordings produced the same audio and audiovisual intelligibility levels as the original recordings. However, some degradation in performance occurred when the fast recordings were slowed down or the slow recordings were speeded up. In the main experiment, the phrases were processed to set their speaking rates to eight different fixed values ranging from 0.6 syl/s to 20 syl/s. The results show that AV advantages were preserved at speaking rates as fast as 12.5 syl/s, but that they disappeared when the rate was increased to 20 syl/s. Notably, the results also failed to show any improvement in AV performance for phrases presented slower than their original speaking rates.\n",
    ""
   ]
  },
  "dean07_avsp": {
   "authors": [
    [
     "David",
     "Dean"
    ],
    [
     "Patrick",
     "Lucey"
    ],
    [
     "Sridha",
     "Sridharan"
    ],
    [
     "Tim",
     "Wark"
    ]
   ],
   "title": "Weighting and normalisation of synchronous HMMs for audio-visual speech recognition",
   "original": "av07_P28",
   "page_count": 6,
   "order": 44,
   "p1": "paper P28",
   "pn": "",
   "abstract": [
    "In this paper, we examine the effect of varying the stream weights in synchronous multi-stream hidden Markov models (HMMs) for audio-visual speech recognition. Rather than considering the stream weights to be the same for training and testing, we examine the effect of different stream weights for each task on the final speech-recognition performance. Evaluating our system under varying levels of audio and video degradation on the XM2VTS database, we show that the final performance is primarily a function of the choice of stream weight used in testing, and that the choice of stream weight used for training has a very minor corresponding effect. By varying the value of the testing stream weights we show that the best average speech recognition performance occurs with the streams weighted at around 80% audio and 20% video. However, by examining the distribution of frame-by-frame scores for each stream on a leftout section of the database, we show that these testing weights chosen primarily serve to normalise the two stream score distributions, rather than indicating the dependence of the final performance on either stream. By using a novel adaption of zero-normalisation to normalise each streams models before performing the weighted-fusion, we show that the actual contribution of the audio and video scores to the best performing speech system is closer to equal that appears to be indicated by the un-normalised stream weighting parameters alone.\n",
    ""
   ]
  },
  "mana07_avsp": {
   "authors": [
    [
     "Nadia",
     "Mana"
    ],
    [
     "Fabio",
     "Pianesi"
    ]
   ],
   "title": "Modelling of emotional facial expressions during speech in synthetic talking heads using a hybrid approach",
   "original": "av07_P29",
   "page_count": 6,
   "order": 45,
   "p1": "paper P29",
   "pn": "",
   "abstract": [
    "Talking heads and virtual characters, able to communicate complex information with human-like expressiveness and naturalness, should be able to display emotional facial expressions. In recent years several works, both rule-based and statistical, have obtained important results in the modelling of emotional facial expressions to be used in synthetic Talking Heads. However, most of the rule-based systems suffer from the drawback of static generation, due to the fact that the set of rules and their combinations are limited. On the other hand, most of works on synthesis using statistical approaches are restricted to speech and lip movements.\n",
    "This paper presents a modelling of the dynamics of emotional facial expressions based on a hybrid statistical/machine learning approach. This approach combines Hidden Markov Models (HMMs) and Recurrent Neural Networks (RNNs), aiming at benefiting from the advantages of both paradigms and overcoming their own limitations.\n",
    ""
   ]
  },
  "krnoul07_avsp": {
   "authors": [
    [
     "Zdenek",
     "Krnoul"
    ],
    [
     "Milos",
     "Zelezný"
    ]
   ],
   "title": "Innovations in Czech audio-visual speech synthesis for precise articulation",
   "original": "av07_P30",
   "page_count": 4,
   "order": 46,
   "p1": "paper P30",
   "pn": "",
   "abstract": [
    "This paper presents new steps toward animation of precise articulation. The acquisition of audio-visual corpus for Czech and new method for parameterization of visual speech was designed to obtain exact speech data. The parameterization method is primarily suitable for training a data driven visual speech synthesis systems. The audio-visual corpus includes also specially designed test part. Furthermore, the paper presents the collection of suitable text material for test of visual speech perception and also the procedure how can be the test performed. The synthesis method based on the selection of visual unit and animation model of talking head is extended. The synthesis system is objectively and subjectively evaluated.\n",
    ""
   ]
  },
  "cisar07_avsp": {
   "authors": [
    [
     "Petr",
     "Císar"
    ],
    [
     "Milos",
     "Zelezný"
    ],
    [
     "Jan",
     "Zelinka"
    ],
    [
     "Jana",
     "Trojanová"
    ]
   ],
   "title": "Development and testing of new combined visual speech parameterization",
   "original": "av07_P31",
   "page_count": 4,
   "order": 47,
   "p1": "paper P31",
   "pn": "",
   "abstract": [
    "Audio-visual speech recognition is used to make speech recognition more robust in case when the acoustic speech signal is affected by increased environmental noise. Since visual speech expression is not affected by the acoustic noise, it is used as supplemental information for the recognition. Usually, the shape of lips of a speaker is described by either pixel-based or shape-based features. Many works were published on these two basic approaches.\n",
    "After experiments with various parameterizations we decided to use expert knowledge about human lip-reading for development of a new parameterization. We used information provided by human lip-reading experts and speech therapists. Based on this information we designed a combined parameterization for description of visual speech part. Experiments with this parameterization were performed on two different databases: English audio-visual database XM2VTS and Czech audio-visual database UWB-05- HSCAVC.\n",
    "The designed parameterization combines both basic approaches and uses shape-based description for outer lip contour and pixel-based description for description of inner part of a mouth. Results obtained in experiments with this parameterization showed that it outperforms the traditionally used parameterizations.\n",
    ""
   ]
  },
  "grauwinkel07_avsp": {
   "authors": [
    [
     "Katja",
     "Grauwinkel"
    ],
    [
     "Sascha",
     "Fagel"
    ]
   ],
   "title": "Visualization of internal articulator dynamics for use in speech therapy for children with Sigmatismus Interdentalis",
   "original": "av07_P32",
   "page_count": 4,
   "order": 48,
   "p1": "paper P32",
   "pn": "",
   "abstract": [
    "This paper presents an ongoing study in which the applicability of a talking head with its three-dimensional animation of internal articulator dynamics is investigated as a method of speech visualization for use in speech therapy for children with sigmatismus interdentalis. Previous work of the authors showed that people with normal hearing, vision and speaking abilities were able to benefit from visual information provided by the internal articulatory movements of the talking head during speech production after a short learning lesson. In the present study a perception and a production test for children with sigmatismus interdentalis were designed. In a learning lesson the correct production of the alveolar sibilants /s/ and /z/ was auditorily presented and visualized by use of the talking head; the correct tongue position was ex-plained and set in contrast to an interdental production of the sounds. Afterwards a visual identification test was performed and the perception and production tests were repeated. In order to determine the effect of the learning lesson, the production data of three of the children were evaluated by 15 raters. The results of the perception test showed that children were able to auditorily differentiate between correct and wrong production of the alveolar fricatives /s/ and /z/. Furthermore, most of the children are able to visually identify correct and wrong productions of the talking head. The evaluations showed that the learning lesson was capable to improve the sibilant production of two of the three children.\n",
    ""
   ]
  },
  "colonius07_avsp": {
   "authors": [
    [
     "Hans",
     "Colonius"
    ],
    [
     "Adele",
     "Diederich"
    ]
   ],
   "title": "A measure of auditory-visual integration efficiency based on fechnerian scaling",
   "original": "av07_P33",
   "page_count": 5,
   "order": 49,
   "p1": "paper P33",
   "pn": "",
   "abstract": [
    "Auditory-visual integration efficiency (IE) is a presumed skill employed by subjects independently from their ability to extract information from auditory and visual speech inputs. Currently there are no established methods for determining a subjects IE. Here we present a novel measurement technique to address this issue without requiring explicit assumptions about the underlying audiovisual processing. It is based on a version of the theory of Fechnerian Scaling developed by Dzhafarov and Colonius (Dzhafarov, E.N. and Colonius, H. (2006)). Reconstructing distances among objects from their discriminability, Psychometrika, 72(2), 365-386), that permits the reconstruction of subjective distances among stimuli of arbitrary complexity from their pairwise discriminability.\n",
    ""
   ]
  },
  "behne07_avsp": {
   "authors": [
    [
     "Dawn",
     "Behne"
    ],
    [
     "Yue",
     "Wang"
    ],
    [
     "Magnus",
     "Alm"
    ],
    [
     "Ingrid",
     "Arntsen"
    ],
    [
     "Ragnhild",
     "Eg"
    ],
    [
     "Ane",
     "Valsø"
    ]
   ],
   "title": "Changes in audio-visual speech perception during adulthood",
   "original": "av07_P34",
   "page_count": 5,
   "order": 50,
   "p1": "paper P34",
   "pn": "",
   "abstract": [
    "Audiovisual speech perception research has shown an increasing use of visual information from infancy to young adulthood. The current study extends these findings by examining audiovisual speech perception from young adulthood to mid-adulthood by addressing the extent to which audio, visual and audiovisual cues are used for place of articulation identification. Responses were gathered with young adults (19-30 yrs) and mid-aged adults (49-60 yrs) for voiceless and voiced audiovisual consonant-vowel syllables differing in consonant place of articulation. Materials were presented in quiet and in café noise (SNR=0dB). Results show that mid-aged adults made greater use of visual information than young adults in both quiet and noise, suggesting more than compensation for natural changes in hearing. This was evident across places of articulation and voicing conditions where mid-aged adults showed further indications for using visual cues. Findings indicate that the processing of sensory information continues to change in the course of adulthood with the use of visual cues in audiovisual speech perception increasing with the experience that comes with age.\n",
    ""
   ]
  },
  "wang07_avsp": {
   "authors": [
    [
     "Yue",
     "Wang"
    ],
    [
     "Dawn",
     "Behne"
    ],
    [
     "Haisheng",
     "Jiang"
    ],
    [
     "Angela",
     "Feehan"
    ]
   ],
   "title": "Effect of native language experience on audio-visual perception of English fricatives by Korean and Mandarin natives",
   "original": "av07_P35",
   "page_count": 5,
   "order": 51,
   "p1": "paper P35",
   "pn": "",
   "abstract": [
    "This study examines how native language (L1) experience affects audio-visual (AV) perception of nonnative (L2) speech. Mandarin, Korean, and English perceivers were presented English CV syllables containing fricatives with 3 places of articulation: labiodentals nonexistent in Korean, interdentals nonexistent in Korean and Mandarin, and alveolars occurring in all L1s. The stimuli were presented as: audio-only (A), visual-only (V), congruent AV, and incongruent AV. Results show that all groups performed better in the AV than A or V condition. The English perceivers outperformed the nonnatives. Mandarin perceivers relied more on V input, and greater AV integration with incongruent AV, whereas Koreans benefited more from A. These findings indicate that nonnatives had less effective AV integration than natives with sounds involving visually unfamiliar places of articulation. The nonnatives AV processing was differentially influenced by L1 experience. Conversely, similarities across groups indicate possible perceptual universals. Together they point to an integrated network in speech processing across modalities and linguistic backgrounds.\n",
    ""
   ]
  },
  "troille07_avsp": {
   "authors": [
    [
     "Emilie",
     "Troille"
    ],
    [
     "Marie-Agnès",
     "Cathiard"
    ],
    [
     "Christian",
     "Abry"
    ]
   ],
   "title": "Consequences on bimodal perception of the timing of the consonant and vowel audiovisual flows",
   "original": "av07_P36",
   "page_count": 6,
   "order": 52,
   "p1": "paper P36",
   "pn": "",
   "abstract": [
    "It is now well known that speech can be seen before it is heard: this has been extensively shown for the vowel rounding anticipatory gesture leading the sound. In this study, the perception of French vowel [y] anticipatory coarticulation was tested throughout a voiced fricative [z] consonant with a gating paradigm. It was found that vowel auditory information, as carried by the noise of the fricative, was ahead of visual and even audiovisual information. Hence the time course of bimodal information in speech cannot be considered to display the same pattern whatever the timing of the coordination of speech gestures. As concerns vowel information only, consonantal coarticulation can carry earlier auditory information than the vowel itself, this depending of the structure of the stimulus. In our fricative-vowel case, it was obvious that the vowel building up movement was audible throughout the fricative noise, whereas the changes in formant grouping (indicating mainly the change from [i] to [y]) occurred later. As concerns the timing of audio and visual information, it becomes more and more obvious that the dates of the delivery of each component of the perceptual flow can change drastically the results obtained in the time course of brain activity for bimodal processing. Hence the flexibility of the phase relationships between the two flows urges to a better knowledge of the natural ecological variability in audiovisual signal production, a flexible coherence not to be violated on pain of breaking the Gestalt laws.\n",
    ""
   ]
  },
  "chetty07_avsp": {
   "authors": [
    [
     "Girija",
     "Chetty"
    ],
    [
     "Michael",
     "Wagner"
    ]
   ],
   "title": "Audiovisual speaker identity verification based on cross modal fusion",
   "original": "av07_P37",
   "page_count": 5,
   "order": 53,
   "p1": "paper P37",
   "pn": "",
   "abstract": [
    "In this paper, we propose the fusion of audio and explicit correlation features for speaker identity verification applications. Experiments performed with the GMM based speaker models with hybrid fusion technique involving late fusion of explicit cross-modal fusion features, with eigen lip and audio MFCC features allow a considerable improvement in EER performance An evaluation of the system performance with different gender specific datasets from controlled VidTIMIT data base and opportunistic UCBN database shows, that is possible to achieve an EER of less than 2% with correlated component hybrid fusion, and improvement of around 22 % over uncorrelated component fusion.\n",
    ""
   ]
  },
  "barbosa07b_avsp": {
   "authors": [
    [
     "Adriano V.",
     "Barbosa"
    ],
    [
     "Hani C.",
     "Yehia"
    ],
    [
     "Eric",
     "Vatikiotis-Bateson"
    ]
   ],
   "title": "MATLAB toolbox for audiovisual speech processing",
   "original": "av07_P38",
   "page_count": 6,
   "order": 54,
   "p1": "paper P38",
   "pn": "",
   "abstract": [
    "Audiovisual speech processing has reached a stage of maturity where there are now numerous computational procedures needed to measure and assess multimodal signals. However, as is often the case, the results of these procedures are better known than the procedures themselves. This paper presents a MATLAB toolbox consisting of an extensive collection of tools we have developed over the past 10 years. These tools are not intended to be the final answer for multimodal speech analysis; rather they are presented as an easy-to-use and welldocumented library whose scope is sufficiently broad to be useful to both experts and novices.\n",
    "The toolbox includes procedures for measuring, organizing, modeling, and validating multiple streams of time-varying data, including acoustics, two- and threedimensional motions of the speaker. In addition to physical and derived (from video) marker data, new functions have been implemented that incorporate optical flow techniques based on the OpenCV library. When complete the toolbox will allow us to track human body gestures during speech from video noninvasively and to quantify the correspondences between different performance modalities within and across speakers.\n",
    ""
   ]
  },
  "tanaka07_avsp": {
   "authors": [
    [
     "Akihiro",
     "Tanaka"
    ],
    [
     "Shuichi",
     "Sakamoto"
    ],
    [
     "Komi",
     "Tsumura"
    ],
    [
     "Yôiti",
     "Suzuki"
    ]
   ],
   "title": "Effects of intermodal timing difference and speed difference on intelligibility of auditory-visual speech in younger and older adults",
   "original": "av07_P39",
   "page_count": 6,
   "order": 55,
   "p1": "paper P39",
   "pn": "",
   "abstract": [
    "Previous studies have revealed a temporal window during which human observers perceive physically desynchronized auditory and visual signals as synchronous. This study investigated effects of intermodal timing differences and speed differences on intelligibility of auditory-visual speech. We used 20 minimal pairs of Japanese four-mora words such as \"mi-zu-a-ge\" (catch landing) versus \"mi-zu-a-me\" (starch syrup) and administered intelligibility tests. Words were presented under visual-only, auditory-only, and auditoryvisual (AV) conditions. Two types of AV conditions were used: asynchronous and expansion conditions. In asynchronous (i.e. timing difference) conditions, the audio lag was 0-400 ms. In expansion (i.e. speed difference) conditions, the auditory signal was time-expanded while the visual signal was kept at the original speed. The amount of expansion was 0-400 ms. Results showed that the word intelligibility declined as the timing difference and speed difference increased. Results of AV benefit (i.e. the superiority of AV performance over auditory-only performance) revealed that the AV benefit at the end of words declined as the speed difference increased, although it did not decline as timing difference increased. These results suggest that intermodal lag recalibration requires a constant timing difference between auditory and visual signals. Older adults recalibrated neither the timing difference nor the speed difference. These results might be useful for design of a multimodal speech-rate conversion system.\n",
    ""
   ]
  },
  "martin07_avsp": {
   "authors": [
    [
     "Jean-Claude",
     "Martin"
    ],
    [
     "Christian",
     "Jacquemin"
    ],
    [
     "Laurent",
     "Pointal"
    ],
    [
     "Brian",
     "Katz"
    ],
    [
     "Christophe",
     "D'Alessandro"
    ],
    [
     "Aurélien",
     "Max"
    ],
    [
     "M.",
     "Courgeon"
    ]
   ],
   "title": "A 3d audio-visual animated agent for expressive conversational question answering",
   "original": "av07_P40",
   "page_count": 4,
   "order": 56,
   "p1": "paper P40",
   "pn": "",
   "abstract": [
    "This paper reports on the ACQA (Animated agent for Conversational Question Answering) project conducted at LIMSI. The aim is to design an expressive animated conversational agent (ACA) for conducting research along two main lines: 1/ perceptual experiments (eg perception of expressivity and 3D movements in both audio and visual channels): 2/ design of human-computer interfaces requiring head models at different resolutions and the integration of the talking head in virtual scenes. The target application of this expressive ACA is a real-time question and answer speech based system developed at LIMSI (RITEL). The architecture of the system is based on distributed modules exchanging messages through a network protocol. The main components of the system are: RITEL a question and answer system searching raw text, which is able to produce a text (the answer) and attitudinal information; this attitudinal information is then processed for delivering expressive tags; the text is converted into phoneme, viseme, and prosodic descriptions. Audio speech is generated by the LIMSI selection-concatenation text-to-speech engine. Visual speech is using MPEG4 keypoint-based animation, and is rendered in real-time by Virtual Choreographer (VirChor), a GPU-based 3D engine. Finally, visual and audio speech is played in a 3D audio and visual scene. The project also puts a lot of effort for realistic visual and audio 3D rendering. A new model of phoneme-dependant human radiation patterns is included in the speech synthesis system, so that the ACA can move in the virtual scene with realistic 3D visual and audio rendering.\n",
    ""
   ]
  },
  "vatikiotisbateson07_avsp": {
   "authors": [
    [
     "Eric",
     "Vatikiotis-Bateson"
    ],
    [
     "Adriano V.",
     "Barbosa"
    ],
    [
     "Cheuk Yi",
     "Chow"
    ],
    [
     "Martin",
     "Oberg"
    ],
    [
     "Johanna",
     "Tan"
    ],
    [
     "Hani C.",
     "Yehia"
    ]
   ],
   "title": "Audiovisual Lombard speech: reconciling production and perception",
   "original": "av07_P41",
   "page_count": 6,
   "order": 57,
   "p1": "paper P41",
   "pn": "",
   "abstract": [
    "An earlier study compared audiovisual perception of speech produced in environmental noise (Lombard speech) and speech produced in quiet with the same environmental noise added. The results and showed that listeners make differential use of the visual information depending on the recording condition, but gave no indication of how or why this might be so. A possible confound in that study was that high audio presentation levels might account for the small visual enhancements observed for Lombard speech. This paper reports results for a second perception study using much lower acoustic presentation levels, compares them with the results of the previous study, and integrates the perception results with analyses of the audiovisual production data: face and head motion, audio amplitude (RMS), and parameters of the spectral acoustics (line spectrum pairs).\n",
    ""
   ]
  },
  "chen07_avsp": {
   "authors": [
    [
     "Yuchun",
     "Chen"
    ],
    [
     "Valerie",
     "Hazan"
    ]
   ],
   "title": "Developmental factor in auditory-visual speech perception - the Mcgurk effect in Mandarin-Chinese and English speakers",
   "original": "av07_P42",
   "page_count": 5,
   "order": 58,
   "p1": "paper P42",
   "pn": "",
   "abstract": [
    "This study investigated the developmental factor in the use of visual information in auditory-visual speech perception in Mandarin-Chinese and English native speakers. Adults and 8- to 9-year-old children from two language backgrounds (Mandarin-Chinese and English) were presented stimuli consisting of /ba/, /da/, /ga/ and were asked to make syllable identifications in auditory-only, visual-only and auditory-visual (congruent and incongruent) conditions in clear and in noise. In order to test the second-language learning effect, an additional Chinese child group with a high degree of English exposure was also included. The results showed a developmental increase in the use of visual information and in the magnitude of McGurk effect in both Chinese and English groups. In addition, a positive correlation between the total visual effect and speechreading performance was found, suggesting that the smaller visual influence in the bimodal condition for children might be accounted for by their less sophisticated lip-reading ability. In regard with the foreign language effect, English listeners showed greater visual influence for tokens produced by Chinese speakers, but there was no difference linked to second-language exposure between the two Chinese child groups.\n",
    ""
   ]
  }
 },
 "sessions": [
  {
   "title": "Oral Sessions",
   "papers": [
    "stekelenburg07_avsp",
    "calabresi07_avsp",
    "mol07_avsp",
    "fagel07_avsp",
    "lucey07_avsp",
    "elisei07_avsp",
    "auerjr07_avsp",
    "jesse07_avsp",
    "knowland07_avsp",
    "traunmuller07_avsp",
    "swerts07_avsp",
    "barbosa07_avsp",
    "barker07_avsp",
    "hu07_avsp",
    "hazan07_avsp",
    "baier07_avsp",
    "kim07_avsp"
   ]
  },
  {
   "title": "Poster Sessions",
   "papers": [
    "gan07_avsp",
    "cave07_avsp",
    "ali07_avsp",
    "sugano07_avsp",
    "vroomen07_avsp",
    "berger07_avsp",
    "rivet07_avsp",
    "davis07_avsp",
    "bertrand07_avsp",
    "pilling07_avsp",
    "irwin07_avsp",
    "thomas07_avsp",
    "melenchon07_avsp",
    "rivet07b_avsp",
    "batliner07_avsp",
    "milner07_avsp",
    "almajai07_avsp",
    "sakamoto07_avsp",
    "basirat07_avsp",
    "dohen07_avsp",
    "theobald07_avsp",
    "raidt07_avsp",
    "hazan07b_avsp",
    "ouni07_avsp",
    "schacht07_avsp",
    "brungart07_avsp",
    "dean07_avsp",
    "mana07_avsp",
    "krnoul07_avsp",
    "cisar07_avsp",
    "grauwinkel07_avsp",
    "colonius07_avsp",
    "behne07_avsp",
    "wang07_avsp",
    "troille07_avsp",
    "chetty07_avsp",
    "barbosa07b_avsp",
    "tanaka07_avsp",
    "martin07_avsp",
    "vatikiotisbateson07_avsp",
    "chen07_avsp"
   ]
  }
 ]
}