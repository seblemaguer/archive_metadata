{
  "title": "Interspeech 2015",
  "location": "Dresden, Germany",
  "startDate": "6/9/2015",
  "endDate": "10/9/2015",
  "chair": "General Chair: Sebastian M\u00f6ller; Co-Chair: Hermann Ney",
  "conf": "Interspeech",
  "year": "2015",
  "name": "interspeech_2015",
  "series": "Interspeech",
  "SIG": "",
  "title1": "Interspeech 2015",
  "date": "6-10 September 2015",
  "booklet": "interspeech_2015.pdf",
  "papers": {
    "beckman15_interspeech": {
      "authors": [
        [
          "Mary E.",
          "Beckman"
        ]
      ],
      "title": "The emergence of compositional structure in language evolution and development",
      "original": "i15_4001",
      "page_count": 0,
      "order": 1,
      "p1": "0",
      "pn": "",
      "abstract": [
        "Spoken language has a complex multi-dimensional compositional structure that enables the rich productivity so characteristic of this most important human biosignal. Even the utterance of a single vowel sound, such as [o] or [i], requires the talker to coordinate gestures of the lip and tongue with gestures of the respiratory-laryngeal system, to combine the timbre properties of the vowel with a specific voice quality and a specific melodic pattern. The latter can be chosen from some set of morphemes that differentiate utterances, as in the English sentences Oh? versus Oh!, or from some set of tonemes that differentiate words, as in the Mandarin Chinese words y\u012b `one' versus y\u01d0 `ant'.   In either case, the utterance will have a kind of \u201csimultaneous compositionality\u201d that can be found also in the vocal communication systems of many other primates. Its importance for the human biosignal is evident in the fact that human infants begin to develop the capacity for it as soon as the larynx disengages from the nasopharynx at about 2 months. Of course, most of the words of all spoken languages have an even richer internal compositional structure that depends on the coordination of gestural complexes for contrasting timbre properties, to create a series of alternating consonant and vowel sounds, as in the \u201ccanonical\u201d CV syllable type that infants with normal hearing begin to produce between 6 and 8 months of age, or the even more complex rhythmic structures that are used to make words in many languages. The development of writing systems was the first technical innovation for modeling this property of \u201cserial compositionality\u201d that is shared by all human languages, as well as by the vocal communication systems of some other primates (such as those of many species of gibbon). The history of the Interspeech conference series is closely intertwined with many other important technical developments that have greatly increased our ability to model both types of compositionality. Now is a fruitful time to apply these more recent technical developments in a concerted way to achieve a better understanding of the emergence of compositionality in phylogeny and ontogeny.\n",
        ""
      ]
    },
    "sarikaya15_interspeech": {
      "authors": [
        [
          "Ruhi",
          "Sarikaya"
        ]
      ],
      "title": "The technology powering personal digital assistants",
      "original": "i15_4002",
      "page_count": 0,
      "order": 2,
      "p1": "0",
      "pn": "",
      "abstract": [
        "We have long envisioned that one day computers will understand natural language and anticipate what we need and when we need it to proactively complete tasks on our behalf. As computers get smaller and more pervasive, how humans interact with them is becoming a crucial issue. Despite numerous attempts over the past 40 years to make language understanding an effective and robust natural user interface for computer interaction, success was limited and scoped to applications that are not particularly central to everyday use. However, advances in speech recognition and machine learning, coupled with the emergence of structured data served by content providers and increased computational power have broadened the application of natural language understanding to a wide spectrum of everyday tasks that are central to the user's productivity. We believe that as computers become smaller and more ubiquitous (eg wearable computers) and as the number of applications increases, both system-initiated and user initiated task completion across various applications and services will become indispensable for personal life management and work productivity. There has been already a tremendous investment in the industry (particularly Microsoft, Google, Apple, Amazon and Nuance) around digital personal assistants during the last couple of years. Each of the major companies in the speech and language technology space has a version of their personal assistants (Cortana, Google Now, Siri, Echo, and Dragon, respectively) deployed in production. Yet there is not much talked about these technologies and products in any of the speech and language technology conferences. In this talk, we give an overview of personal digital assistants, describe the system design, architecture and the key components behind them. We will highlight challenges and describe best practices related to the bringing personal assistants from laboratories to the real-world and discuss their potential to fully redefine the human-computer interaction moving forward.\n",
        ""
      ]
    },
    "amunts15_interspeech": {
      "authors": [
        [
          "Katrin",
          "Amunts"
        ]
      ],
      "title": "The HBP-atlas \u2014 concept, perspectives, and application for language and speech research",
      "original": "i15_4003",
      "page_count": 0,
      "order": 3,
      "p1": "0",
      "pn": "",
      "abstract": [
        "Studying the human brain remains one of the greatest scientific challenges. A comprehensive understanding of the structural and functional organization of the brain is not only of great importance for basic science, but also for the development of new approaches that improve diagnosis and the treatment of neurological and psychiatric diseases. With this mindset, the Human Brain Project (HBP) started its work in October 2013 with the aim of creating a European ICT infrastructure for neuroscience. The immense complexity of the brain, with its approximately 86 billion nerve cells, makes it essential to include modeling and simulation approaches, combined with methods of high performance computing (HPC), in order to analyze the organizational principles of the brain.   One of the central elements of the HBP is the Human Brain Atlas. It includes data from different aspects of brain organization, eg, cytoarchitectonics, fibre architecture, molecular architecture and results from fMRI studies revealing the functional segregation of the brain. Such multi-level atlas allows analyzing the neural underpinnings of language processes with unprecedented detail, and studying structural-functional relationships at the level of cortical areas.   Conversely, the understanding of neural mechanisms might inspire new advancements for HPC. Those insights into the brain provide simulation, and give computer scientists the opportunity to develop a new generation of computers and software that are inspired by the functional principles of the brain. HPC opens up new avenues for neuroscientists to develop virtual brain models, such as the BigBrain model, which connects the macroscopic with the microscopic organization level for the first time in a reference system. In such models, data from the genetic, molecular, and cellular levels up to cognitive systems could be combined together for a subsequent analysis at different scales.\n",
        ""
      ]
    },
    "scherer15_interspeech": {
      "authors": [
        [
          "Klaus",
          "Scherer"
        ]
      ],
      "title": "Voices of power, passion, and personality",
      "original": "i15_4004",
      "page_count": 0,
      "order": 4,
      "p1": "0",
      "pn": "",
      "abstract": [
        "Many species of animals use vocal communication in mating rituals, warning conspecifics, conveying location of food sources, and social learning. Not surprisingly, the human species has perfected this system of communication by developing first spoken and then written language. I will argue that the expression of emotion has been an important motor for this evolutionary advancement. In many species we find multimodal \u201caffect bursts\u201d which communicate reactions to environmental events and behavioral intentions to conspecifics through synchronized vocal, facial and bodily expression. It becomes increasingly plausible that both speech and music evolved from such affect bursts. In this talk, I will highlight the major strengths of vocal communication, especially voice quality, as compared to facial expression. While the face is a relatively discrete signaling system for specific reactions and messages, in large part restricted to human communication, the voice is a phylogenetically old and continuous carrier of information about the vocalizer's physique, enduring dispositions, strategic intention and current emotional state. The dynamic nature of voice delivery, including changes in voice quality, rhythm, intonation, and timing, is a major asset in communicating the unfolding of emotional reactions continuously in real time, allowing for instantaneous adaptation. In addition to theoretical considerations, including the suggestion of a path model for vocal communication, I will present recent empirical research from our laboratory, for both the speaking and the singing voice. Specifically, the signaling of speaker, power, passion and personality will be addressed. In addition, a variety of potential applications in different domains will be discussed.\n",
        ""
      ]
    },
    "sainath15_interspeech": {
      "authors": [
        [
          "Tara N.",
          "Sainath"
        ],
        [
          "Ron J.",
          "Weiss"
        ],
        [
          "Andrew",
          "Senior"
        ],
        [
          "Kevin W.",
          "Wilson"
        ],
        [
          "Oriol",
          "Vinyals"
        ]
      ],
      "title": "Learning the speech front-end with raw waveform CLDNNs",
      "original": "i15_0001",
      "page_count": 5,
      "order": 5,
      "p1": "1",
      "pn": "5",
      "abstract": [
        "Learning an acoustic model directly from the raw waveform has been an active area of research. However, waveform-based models have not yet matched the performance of log-mel trained neural networks. We will show that raw waveform features match the performance of log-mel filterbank energies when used with a state-of-the-art CLDNN acoustic model trained on over 2,000 hours of speech. Specifically, we will show the benefit of the CLDNN, namely the time convolution layer in reducing temporal variations, the frequency convolution layer for preserving locality and reducing frequency variations, as well as the LSTM layers for temporal modeling. In addition, by stacking raw waveform features with log-mel features, we achieve a 3% relative reduction in word error rate.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-1",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition \u2013 Signal Processing, Acoustic Modeling, Robustness, and Adaptation"
    },
    "bhargava15_interspeech": {
      "authors": [
        [
          "Mayank",
          "Bhargava"
        ],
        [
          "Richard",
          "Rose"
        ]
      ],
      "title": "Architectures for deep neural network based acoustic models defined over windowed speech waveforms",
      "original": "i15_0006",
      "page_count": 5,
      "order": 6,
      "p1": "6",
      "pn": "10",
      "abstract": [
        "This paper investigates acoustic models for automatic speech recognition (ASR) using deep neural networks (DNNs) whose input is taken directly from windowed speech waveforms (WSW). After demonstrating the ability of these networks to automatically acquire internal representations that are similar to mel-scale filter-banks, an investigation into efficient DNN architectures for exploiting WSW features is performed. First, a modified bottleneck DNN architecture is investigated to capture dynamic spectrum information that is not well represented in the time domain signal. Second,the redundancies inherent in WSW based DNNs are considered. The performance of acoustic models defined over WSW features is compared to that obtained from acoustic models defined over mel frequency spectrum coefficient (MFSC) features on the Wall Street Journal (WSJ) speech corpus. It is shown that using WSW features results in a 3.0 percent increase in WER relative to that resulting from MFSC features on the WSJ corpus. However, when combined with MFSC features, a reduction in WER of 4.1 percent is obtained with respect to the best evaluated MFSC based DNN acoustic model.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-2"
    },
    "palaz15_interspeech": {
      "authors": [
        [
          "Dimitri",
          "Palaz"
        ],
        [
          "Mathew",
          "Magimai-Doss"
        ],
        [
          "Ronan",
          "Collobert"
        ]
      ],
      "title": "Analysis of CNN-based speech recognition system using raw speech as input",
      "original": "i15_0011",
      "page_count": 5,
      "order": 7,
      "p1": "11",
      "pn": "15",
      "abstract": [
        "Automatic speech recognition systems typically model the relationship between the acoustic speech signal and the phones in two separate steps: feature extraction and classifier training. In our recent works, we have shown that, in the framework of convolutional neural networks (CNN), the relationship between the raw speech signal and the phones can be directly modeled and ASR systems competitive to standard approach can be built. In this paper, we first analyze and show that, between the first two convolutional layers, the CNN learns (in parts) and models the phone-specific spectral envelope information of 2-4 ms speech. Given that we show that the CNN-based approach yields ASR trends similar to standard short-term spectral based ASR system under mismatched (noisy) conditions, with the CNN-based approach being more robust.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-3",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition \u2013 Signal Processing, Acoustic Modeling, Robustness, and Adaptation"
    },
    "ogawa15_interspeech": {
      "authors": [
        [
          "Tetsuji",
          "Ogawa"
        ],
        [
          "Kenshiro",
          "Ueda"
        ],
        [
          "Kouichi",
          "Katsurada"
        ],
        [
          "Tetsunori",
          "Kobayashi"
        ],
        [
          "Tsuneo",
          "Nitta"
        ]
      ],
      "title": "Bilinear map of filter-bank outputs for DNN-based speech recognition",
      "original": "i15_0016",
      "page_count": 5,
      "order": 8,
      "p1": "16",
      "pn": "20",
      "abstract": [
        "Filter-bank outputs are extended into tensors to yield precise acoustic features for speech recognition using deep neural networks (DNNs). The filter-bank outputs with temporal contexts form a time-frequency pattern of speech and have been shown to be effective as a feature parameter for DNN-based acoustic models. We attempt to project the filter-bank outputs onto a tensor product space using decorrelation followed by a bilinear map to improve acoustic separability in feature extraction. This extension makes extracting a more precise structure of the time-frequency pattern possible because the bilinear map yields higher-order correlations of features. Experimental comparisons carried out in phoneme recognition demonstrate that the tensor feature provides comparable results to the filter-bank feature, and the fusion of the two features yields an improvement over each feature.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-4",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition \u2013 Signal Processing, Acoustic Modeling, Robustness, and Adaptation"
    },
    "lin15_interspeech": {
      "authors": [
        [
          "Payton",
          "Lin"
        ],
        [
          "Dau-Cheng",
          "Lyu"
        ],
        [
          "Yun-Fan",
          "Chang"
        ],
        [
          "Yu",
          "Tsao"
        ]
      ],
      "title": "Speech recognition with temporal neural networks",
      "original": "i15_0021",
      "page_count": 5,
      "order": 9,
      "p1": "21",
      "pn": "25",
      "abstract": [
        "Raw temporal features were derived from extracted temporal envelope bank (referred to as \u201cTbank\u201d). Tbank features were used with deep neural networks (DNNs) to greatly increase the amount of detailed information about the past to be carried forward to help in the interpretation of the future.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-5",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition \u2013 Signal Processing, Acoustic Modeling, Robustness, and Adaptation"
    },
    "golik15_interspeech": {
      "authors": [
        [
          "Pavel",
          "Golik"
        ],
        [
          "Zolt\u00e1n",
          "T\u00fcske"
        ],
        [
          "Ralf",
          "Schl\u00fcter"
        ],
        [
          "Hermann",
          "Ney"
        ]
      ],
      "title": "Convolutional neural networks for acoustic modeling of raw time signal in LVCSR",
      "original": "i15_0026",
      "page_count": 5,
      "order": 10,
      "p1": "26",
      "pn": "30",
      "abstract": [
        "In this paper we continue to investigate how the deep neural network (DNN) based acoustic models for automatic speech recognition can be trained without hand-crafted feature extraction. Previously, we have shown that a simple fully connected feedforward DNN performs surprisingly well when trained directly on the raw time signal. The analysis of the weights revealed that the DNN has learned a kind of short-time time-frequency decomposition of the speech signal. In conventional feature extraction pipelines this is done manually by means of a filter bank that is shared between the neighboring analysis windows.   Following this idea, we show that the performance gap between DNNs trained on spliced hand-crafted features and DNNs trained on raw time signal can be strongly reduced by introducing 1D-convolutional layers. Thus, the DNN is forced to learn a short-time filter bank shared over a longer time span. This also allows us to interpret the weights of the second convolutional layer in the same way as 2D patches learned on critical band energies by typical convolutional neural networks.   The evaluation is performed on an English LVCSR task. Trained on the raw time signal, the convolutional layers allow to reduce the WER on the test set from 25.5% to 23.4%, compared to an MFCC based result of 22.1% using fully connected layers.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-6",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition \u2013 Signal Processing, Acoustic Modeling, Robustness, and Adaptation"
    },
    "glavitsch15_interspeech": {
      "authors": [
        [
          "Ulrike",
          "Glavitsch"
        ],
        [
          "Lei",
          "He"
        ],
        [
          "Volker",
          "Dellwo"
        ]
      ],
      "title": "Stable and unstable intervals as a basic segmentation procedure of the speech signal",
      "original": "i15_0031",
      "page_count": 5,
      "order": 11,
      "p1": "31",
      "pn": "35",
      "abstract": [
        "The concept of acoustically stable and unstable intervals to structure continuous speech is introduced. We present a method to compute stable intervals efficiently and reliably as a bottom-up approach at an early processing stage. We argue that such intervals stand in close relation to the rhythm of speech as they contribute to the overall temporal organization of the speech production process and the acoustic signal (stable intervals = intervals of reduced movement of certain articulators; unstable intervals = intervals of enhanced movement of certain articulators). To test the relationship of stability intervals with speech rhythm we investigated the between-speaker variability of stable and unstable intervals in the TEVOID corpus. Results revealed that significant between-speaker variability exists. We hypothesize from our findings that the basic segmentation of speech into stable and unstable intervals is a process that might play a role in human perception and processing of speech.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-7",
      "author_area_id": "2",
      "author_area_label": "Phonetics, Phonology, and Prosody"
    },
    "windmann15_interspeech": {
      "authors": [
        [
          "Andreas",
          "Windmann"
        ],
        [
          "Juraj",
          "\u0160imko"
        ],
        [
          "Petra",
          "Wagner"
        ]
      ],
      "title": "Polysyllabic shortening and word-final lengthening in English",
      "original": "i15_0036",
      "page_count": 5,
      "order": 12,
      "p1": "36",
      "pn": "40",
      "abstract": [
        "We investigate Polysyllabic shortening effects in three prosodic domains, the word, the inter-stress interval (ISI) and the narrow rhythm unit (NRU), in a large corpus of English broadcast speech. Results confirm and extend earlier findings, indicating that these effects are interpretable as artifacts of word-final lengthening. We do, however, find effects compatible with the assumption of eurhythmic principles in speech production.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-8",
      "author_area_id": "2",
      "author_area_label": "Phonetics, Phonology, and Prosody"
    },
    "eriksson15_interspeech": {
      "authors": [
        [
          "Anders",
          "Eriksson"
        ],
        [
          "Mattias",
          "Heldner"
        ]
      ],
      "title": "The acoustics of word stress in English as a function of stress level and speaking style",
      "original": "i15_0041",
      "page_count": 5,
      "order": 13,
      "p1": "41",
      "pn": "45",
      "abstract": [
        "This study of lexical stress in English is part of a series of studies, the goal of which is to describe the acoustics of lexical stress for a number of typologically different languages. When fully developed the methodology should be applicable to any language. The database of recordings so far includes Brazilian Portuguese, English (U.K.), Estonian, German, French, Italian and Swedish. The acoustic parameters examined are f0 -level, f0 -variation, Duration, and Spectral Emphasis. Values for these parameters, computed for all vowels, are the data upon which the analyses are based. All parameters are tested with respect to their correlation with stress level (primary, secondary, unstressed) and speaking style (wordlist reading, phrase reading, spontaneous speech). For the English data, the most robust results concerning stress level are found for Duration and Spectral Emphasis. f0 -level is also significantly correlated but not quite to the same degree. The acoustic effect of phonological secondary stress was significantly different from primary stress only for Duration. In the statistical tests, speaker sex turned out as significant in most cases. Detailed examination showed, however, that the difference was mainly in the degree to which a given parameter was used, not how it was used to signal lexical stress contrasts.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-9",
      "author_area_id": "2",
      "author_area_label": "Phonetics, Phonology, and Prosody"
    },
    "zahner15_interspeech": {
      "authors": [
        [
          "Katharina",
          "Zahner"
        ],
        [
          "Muna",
          "Pohl"
        ],
        [
          "Bettina",
          "Braun"
        ]
      ],
      "title": "Pitch accent distribution in German infant-directed speech",
      "original": "i15_0046",
      "page_count": 5,
      "order": 14,
      "p1": "46",
      "pn": "50",
      "abstract": [
        "Infant-directed speech exhibits slower speech rate, higher pitch and larger F0 excursions than adult-directed speech. Apart from these phonetic properties established in many languages, little is known on the intonational phonological structure in individual languages, i.e. pitch accents and boundary tones and their frequency distribution. Here, we investigated the intonation of infant-directed speech in German. We extracted all turns from the CHILDES database directed towards infants younger than one year (n=585). Two annotators labeled pitch accents and boundary tones according to the autosegmental-metrical intonation system GToBI. Additionally, the tonal movement surrounding the accentual syllable was analyzed. Main results showed a) that 45% of the words carried a pitch accent, b) that phrases ending in a low tone were most frequent, c) that H* accents were generally more frequent than L* accents, d) that H*, L+H* and L* are the most frequent pitch accent types in IDS, and e) that a pattern consisting of an accentual low-pitched syllable preceded by a low tone and followed by a rise or a high tone constitutes the most frequent single pattern. The analyses reveal that the IDS intonational properties lead to a speech style with many tonal alternations, particularly in the vicinity of accented syllables.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-10",
      "author_area_id": "2",
      "author_area_label": "Phonetics, Phonology, and Prosody"
    },
    "mixdorff15_interspeech": {
      "authors": [
        [
          "Hansj\u00f6rg",
          "Mixdorff"
        ],
        [
          "Christian",
          "Cossio-Mercado"
        ],
        [
          "Angelika",
          "H\u00f6nemann"
        ],
        [
          "Jorge",
          "Gurlekian"
        ],
        [
          "Diego",
          "Evin"
        ],
        [
          "Humberto",
          "Torres"
        ]
      ],
      "title": "Acoustic correlates of perceived syllable prominence in German",
      "original": "i15_0051",
      "page_count": 5,
      "order": 15,
      "p1": "51",
      "pn": "55",
      "abstract": [
        "This paper explores the relationship between perceived syllable prominence and the acoustic properties of a speech utterance. It is aimed at establishing a link between the linguistic meaning of an utterance in terms of sentence modality and focus and its underlying prosodic features. Applications of such knowledge can be found in computer-based pronunciation training as well as general automatic speech recognition and understanding. Our acoustic analysis confirms earlier results in that focus and sentence mode modify the fundamental frequency contour, syllabic durations and intensity. However, we could not find consistent differences between utterances produced with non-contrastive and contrastive focus, respectively. Only one third of utterances with broad focus were identified as such. Ratings of syllable prominence are strongly correlated with the amplitude   Aa of underlying accent commands, syllable duration, maximum intensity and mean harmonics-to-noise ratio.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-11",
      "author_area_id": "2",
      "author_area_label": "Phonetics, Phonology, and Prosody"
    },
    "simonetti15_interspeech": {
      "authors": [
        [
          "Simone",
          "Simonetti"
        ],
        [
          "Jeesun",
          "Kim"
        ],
        [
          "Chris",
          "Davis"
        ]
      ],
      "title": "Cross-modality matching of linguistic and emotional prosody",
      "original": "i15_0056",
      "page_count": 4,
      "order": 16,
      "p1": "56",
      "pn": "59",
      "abstract": [
        "Talkers can express different meanings or emotions without changing what is said by changing how it is said (by using both auditory and/or visual speech cues). Typically, cue strength differs between the auditory and visual channels: linguistic prosody (expression) is clearest in audition; emotional prosody is clearest visually. We investigated how well perceivers can match auditory and visual linguistic and emotional prosodic signals. Previous research showed that perceivers can match linguistic visual and auditory prosody reasonably well. The current study extended this by also testing how well auditory and visual spoken emotion expressions could be matched. Participants were presented a pair of sentences (consisting of the same segmental content) spoken by the same talker and were required to decide whether the pair had the same prosody. Twenty sentences were tested with two types of prosody (emotional vs. linguistic), two talkers, and four matching conditions: auditory-auditory (AA); visual-visual (VV); auditory-visual (AV); and visual-auditory (VA). Linguistic prosody was accurately matched in all conditions. Matching emotional expressions was excellent for VV, poorer for VA, and near chance for AA and AV presentations. These differences are discussed in terms of the relationship between types of auditory and visual cues and task effects.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-12",
      "author_area_id": "2",
      "author_area_label": "Phonetics, Phonology, and Prosody"
    },
    "michalsky15_interspeech": {
      "authors": [
        [
          "Jan",
          "Michalsky"
        ]
      ],
      "title": "Pitch scaling as a perceptual cue for questions in German",
      "original": "i15_0924",
      "page_count": 5,
      "order": 17,
      "p1": "924",
      "pn": "928",
      "abstract": [
        "Recent studies on the intonation of German suggest that the phonetic realization may contribute to the signaling of questions. In a previous production study polar questions, alternative questions and continuous statements were found to differ by a gradual increase in pitch scaling of a phonologically identical final rising contour [1]. Based on similar findings for Dutch Haan [2] concludes that the meaning signaled by the phonetic realization indicates an attitude rather than a categorical function. This is supported by Chen's [3] perception studies on question intonation in Dutch, Hungarian and Mandarin Chinese as well as early findings for German by Batliner [4]. This paper investigates whether the phonetic realization of intonation in questions signals the categorical pragmatic function of `interrogativity' or rather a `questioning' attitude. Additionally, we investigate, which phonetic parameter is the decisive cue to this meaning. Three perception studies are reported: a combination of an identification and discrimination task, an imitation task, and a semantic rating task. Results suggest that the phonetic implementation of intonation in German questions signals an attitude rather than a linguistic category and that this function is primarily signaled by the offset of the final rising pitch movement.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-13",
      "author_area_id": "2",
      "author_area_label": "Phonetics, Phonology, and Prosody"
    },
    "reichel15_interspeech": {
      "authors": [
        [
          "Uwe D.",
          "Reichel"
        ],
        [
          "Katalin",
          "M\u00e1dy"
        ],
        [
          "\u0160tefan",
          "Be\u0148u\u0161"
        ]
      ],
      "title": "Parameterization of prosodic headedness",
      "original": "i15_0929",
      "page_count": 5,
      "order": 18,
      "p1": "929",
      "pn": "933",
      "abstract": [
        "Prosodic headedness generally refers to the location of relevant prosodic events at the left or right end of prosodic constituents. In a bottom-up procedure based on a computational F0 stylization we tested several measures to quantify headedness in parametrical and categorical terms for intonation in the accentual phrase (AP) domain. These measures refer to F0 level and range trends as well as to F0 contour patterns within APs. We tested the suitability of this framework for Hungarian and French known to be left- and right-headed, respectively, and applied it to Slovak whose headedness status is yet less clear. The prosodic differences of Hungarian and French were well captured by several of the proposed parameters, so that from their values for Slovak it can be concluded that Slovak tends to be a left-headed language.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-14",
      "author_area_id": "2",
      "author_area_label": "Phonetics, Phonology, and Prosody"
    },
    "sarma15_interspeech": {
      "authors": [
        [
          "Biswajit Dev",
          "Sarma"
        ],
        [
          "Priyankoo",
          "Sarmah"
        ],
        [
          "Wendy",
          "Lalhminghlui"
        ],
        [
          "S. R. Mahadeva",
          "Prasanna"
        ]
      ],
      "title": "Detection of mizo tones",
      "original": "i15_0934",
      "page_count": 4,
      "order": 19,
      "p1": "934",
      "pn": "937",
      "abstract": [
        "Mizo is a tone language of the Kuki-Chin subfamily of the Tibeto-Burman language family. It is a under-studied language and not much resources are available for the language. Moreover, it is a tone language with four different tones, namely, high, low, falling and rising. While designing a speech recognition system it becomes imperative that tonal variations are taken into consideration. Hence, a tone detection method for Mizo is designed using quantitative analysis of acoustic features of Mizo tones. Traditional methods of modelling requires large data for training. As such database is not available for Mizo, we relied only on the slope and height for detecting Mizo tones. In this method, we first converted the pitch values to z-score values. Then the z-score values are fitted to a line. An analysis is made on the distributions of the variance of the pitch contour, represented by z-scores, to classify the tone as High/Low or Falling/Rising. Then depending on the slope and height values the tone is further classified into High or Low and Rising or Falling, respectively.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-15",
      "author_area_id": "2",
      "author_area_label": "Phonetics, Phonology, and Prosody"
    },
    "repp15_interspeech": {
      "authors": [
        [
          "Sophie",
          "Repp"
        ],
        [
          "Lena",
          "Rosin"
        ]
      ],
      "title": "The intonation of echo wh-questions",
      "original": "i15_0938",
      "page_count": 5,
      "order": 20,
      "p1": "938",
      "pn": "942",
      "abstract": [
        "The acoustic characteristics of German echo questions are explored in a production study. It is shown that there are prosodic differences (F0, duration, intensity) between echo questions signalling a high level of emotional arousal, echo questions signalling that the speaker did not understand the previous utterance, and questions requesting completely new information. The findings are largely compatible with earlier findings on utterances with different levels of emotional arousal, where e.g. a higher F0 signals a higher emotional arousal but do not confirm expectations with respect to phonological differences formulated on the basis of suggestions in the linguistic literature on echo questions.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-16",
      "author_area_id": "2",
      "author_area_label": "Phonetics, Phonology, and Prosody"
    },
    "jabeen15_interspeech": {
      "authors": [
        [
          "Farhat",
          "Jabeen"
        ],
        [
          "Tina",
          "B\u00f6gel"
        ],
        [
          "Miriam",
          "Butt"
        ]
      ],
      "title": "Immediately postverbal questions in urdu",
      "original": "i15_0943",
      "page_count": 5,
      "order": 21,
      "p1": "943",
      "pn": "947",
      "abstract": [
        "This production study investigates the interaction of prosody, word order and information structure with respect to wh-constituents in Urdu. We contrasted immediately preverbal wh-constituents with immediately postverbal ones. The preverbal position is the default focus position in Urdu; the appearance of wh-constituents in the immediately postverbal position within the verbal complex is not well understood. In order to test various possible factors governing the appearance of immediately postverbal wh-constituents, target sentences with wh-constituents in both pre- and postverbal positions were presented in different pragmatic contexts and given to native speakers to pronounce. The results show a clear difference in prosodic realization between the pre- and the postverbal position. The preverbal position is consistent with focus prosody, the postverbal wh-phrases appear to occur when the verb is in focus.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-17",
      "author_area_id": "2",
      "author_area_label": "Phonetics, Phonology, and Prosody"
    },
    "mady15_interspeech": {
      "authors": [
        [
          "Katalin",
          "M\u00e1dy"
        ]
      ],
      "title": "Prosodic (non-)realisation of broad, narrow and contrastive focus in Hungarian: a production and a perception study",
      "original": "i15_0948",
      "page_count": 5,
      "order": 22,
      "p1": "948",
      "pn": "952",
      "abstract": [
        "In languages with variable focus positions, prominent elements tend to be emphasised by prosodic cues (e.g. English). If a language prefers a given prosodic pattern, i.e. sentence-final nuclear accents, like Spanish, the prosodic realisation of broad focus might not differ from that of narrow and contrastive focus. The relevance of prosodic focus marking was tested in Hungarian were focus typically appears in front of the finite verb. Prosodic cues such as F0 maximum, F0 peak alignment, segment duration and post-verbal deaccentuation were tested in an experiment with read question and answer sequences. While narrow and contrastive focus triggered post-verbal deaccentuation, none of the gradual measures distinguished focus types consistently from each other. A subsequent perception experiment was conducted in which the same sentences without post-verbal units were to be judged for their naturalness. F0 maximum, F0 peak alignment and accent duration were manipulated. Naturalness scores revealed a sequence narrow > contrastive > broad focus, i.e. a preference for narrow focus contexts compared to contrastive and broad focus ones, while the manipulated prosodic parameters had no effect on the scores. It is concluded that prosodic focus marking in Hungarian is optional and pragmatic rather than grammatical and syntax-related.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-18",
      "author_area_id": "2",
      "author_area_label": "Phonetics, Phonology, and Prosody"
    },
    "benus15_interspeech": {
      "authors": [
        [
          "\u0160tefan",
          "Be\u0148u\u0161"
        ],
        [
          "Uwe D.",
          "Reichel"
        ],
        [
          "Juraj",
          "\u0160imko"
        ]
      ],
      "title": "F0 discontinuity as a marker of prosodic boundary strength in lombard speech",
      "original": "i15_0953",
      "page_count": 5,
      "order": 23,
      "p1": "953",
      "pn": "957",
      "abstract": [
        "Prosodic boundary strength (PBS) refers to the degree of disjuncture between two chunks of speech. It is affected by both linguistic and para-linguistic communicative intentions playing thus an important role in both speech generation and recognition tasks. Among several PBS signals, we focus in this paper on pitch-related discontinuities in boundaries conveying linguistically meaningful contrasts produced in increasing levels of ambient noise. We compare several measures of local and global pitch reset and use classifiers in an effort to better understand the relationship between the degree of ambient noise and F0 marking of PBS. Our results include a positive effect of some noise on boundary classification, better performance of local than global reset features, and more systematic behavior of F0 falls compared to rises.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-19",
      "author_area_id": "2",
      "author_area_label": "Phonetics, Phonology, and Prosody"
    },
    "gendrot15_interspeech": {
      "authors": [
        [
          "C\u00e9dric",
          "Gendrot"
        ],
        [
          "Martine",
          "Adda-Decker"
        ],
        [
          "Yaru",
          "Wu"
        ]
      ],
      "title": "Comparing journalistic and spontaneous speech: prosodic and spectral analysis",
      "original": "i15_0958",
      "page_count": 5,
      "order": 24,
      "p1": "958",
      "pn": "962",
      "abstract": [
        "In this study we compare the ESTER corpus of journalistic speech [1] and the NCCF corpus of spontaneous speech [2] in terms of duration, F0 and spectral reduction in productions automatically detected as speech units between pauses. Continuation F0 rises are overall absent in spontaneous speech and speech units reveal a declination slope with less amplitude than in journalistic speech. For both corpora, lengthening starts around 60% of the sequence duration, but significantly less in spontaneous speech. Lengthening in the initial part of the sequence is observed in journalistic speech only. As expected we measure a faster speech rate in spontaneous speech with shorter vowel durations implying \u2014 partly only \u2014 a more important vowel reduction.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-20",
      "author_area_id": "2",
      "author_area_label": "Phonetics, Phonology, and Prosody"
    },
    "schauffler15_interspeech": {
      "authors": [
        [
          "Nadja",
          "Schauffler"
        ],
        [
          "Katrin",
          "Schweitzer"
        ]
      ],
      "title": "Rhythm influences the tonal realisation of focus",
      "original": "i15_0963",
      "page_count": 5,
      "order": 25,
      "p1": "963",
      "pn": "967",
      "abstract": [
        "Several studies suggest that rhythm affects different aspects in speech production and perception. For example, in German, discourse structure is normally marked by pitch accent placement and pitch accent type, however, there is variation that cannot be explained by purely semantic or syntactic factors. Prosody-inherent factors, like rhythm, can contribute to this variation. This becomes evident in prosodically more complex environments: while the prosody of utterances containing one focused constituent is well investigated and rather clear-cut, the prosodic organisation of multiple contrastive foci is less clear. In double-focus constructions, for example, two focused constituents demand prominence, possibly resulting in the realisation of two pitch accents. If these pitch accents are required on adjacent syllables they conflict with rhythmic preferences. We present a sentence reading experiment investigating the tonal realisation of two focused constituents and how their contours affect each other in different rhythmic environments. Specifically, we tested whether a potential pitch accent clash in a sentence with two corrective foci influences the pitch excursion and the absolute peak height of the accented syllables. The results demonstrate that rhythmic constraints affect the organisation of the tonal marking of corrective focus.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-21",
      "author_area_id": "2",
      "author_area_label": "Phonetics, Phonology, and Prosody"
    },
    "andreeva15_interspeech": {
      "authors": [
        [
          "Bistra",
          "Andreeva"
        ],
        [
          "Bernd",
          "M\u00f6bius"
        ],
        [
          "Grazyna",
          "Demenko"
        ],
        [
          "Frank",
          "Zimmerer"
        ],
        [
          "Jeanin",
          "J\u00fcgler"
        ]
      ],
      "title": "Linguistic measures of pitch range in slavic and Germanic languages",
      "original": "i15_0968",
      "page_count": 5,
      "order": 26,
      "p1": "968",
      "pn": "972",
      "abstract": [
        "Based on specific linguistic landmarks in the speech signal, this study investigates pitch level and pitch span differences in English, German, Bulgarian and Polish. The analysis is based on 22 speakers per language (11 males and 11 females). Linear mixed models were computed that include various linguistic measures of pitch level and span, revealing characteristic differences across languages and between language groups. Pitch level appeared to have significantly higher values for the female speakers in the Slavic than the Germanic group. The male speakers showed slightly different results, with only the Polish speakers displaying significantly higher mean values for pitch level than the German males. Overall, the results show that the Slavic speakers tend to have a wider pitch span than the German speakers. But for the linguistic measure, namely for span between the initial peaks and the non-prominent valleys, we only find the difference between Polish and German speakers. We found a flatter intonation contour in German than in Polish, Bulgarian and English male and female speakers and differences in the frequency of the landmarks between languages. Concerning \u201cspeaker liveliness\u201d we found that the speakers from the Slavic group are significantly livelier than the speakers from the Germanic group.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-22",
      "author_area_id": "2",
      "author_area_label": "Phonetics, Phonology, and Prosody"
    },
    "qiu15_interspeech": {
      "authors": [
        [
          "Chunan",
          "Qiu"
        ],
        [
          "Jie",
          "Liang"
        ]
      ],
      "title": "The effect of stress on vowel space in daxi hakka Chinese",
      "original": "i15_0973",
      "page_count": 5,
      "order": 27,
      "p1": "973",
      "pn": "977",
      "abstract": [
        "The present study examined the effect of stress on vowels, specifically duration, formant frequency and acoustic vowel space in CV syllable in Daxi Hakka Chinese. F1 and F2 values were measured at three equidistant time locations. Results show that the absence of stress results in the reduction of vowel duration, which is shortened by 23% in the unstressed condition. The presence of stress affects formants by raising F2 for front vowels, raising F1 for low vowels, and lowering F2 for back vowels. Space areas in the stressed condition are significantly greater than their unstressed counterparts in all three measurement points. The mean space areas are compressed by 17% from the stressed to the unstressed condition. The mean space areas in 50% point are the largest in the two stress conditions. There is a positive correlation between interspeaker duration and vowel space areas.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-23",
      "author_area_id": "2",
      "author_area_label": "Phonetics, Phonology, and Prosody"
    },
    "oreilly15_interspeech": {
      "authors": [
        [
          "Maria",
          "O'Reilly"
        ],
        [
          "Ailbhe N\u00ed",
          "Chasaide"
        ]
      ],
      "title": "Declination, peak height and pitch level in declaratives and questions of south connaught irish",
      "original": "i15_0978",
      "page_count": 5,
      "order": 28,
      "p1": "978",
      "pn": "982",
      "abstract": [
        "As South Connaught Irish typically uses the same (falling) tune type in both questions and declaratives, this paper examines whether sentence mode might be differentiated in this dialect by other aspects of the contour realization, namely declination slope, peak height and pitch level. A set of matched declaratives (DEC), wh- questions (WHQ) and yes/no questions (YNQ) of two phrase lengths (with 2 and 3 accent groups) was analysed. The results indicate that sentence type is reflected in the measured F0 parameters. Compared to declaratives, WHQ exhibit markedly steeper declination slopes and somewhat higher IP-initial peaks, while YNQ raise the pitch level and the IP-initial peaks. Phrase length influences declination slope, but does not appear to affect peak height.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-24",
      "author_area_id": "2",
      "author_area_label": "Phonetics, Phonology, and Prosody"
    },
    "sarmah15_interspeech": {
      "authors": [
        [
          "Priyankoo",
          "Sarmah"
        ],
        [
          "Leena",
          "Dihingia"
        ],
        [
          "Wendy",
          "Lalhminghlui"
        ]
      ],
      "title": "Contextual variation of tones in mizo",
      "original": "i15_0983",
      "page_count": 4,
      "order": 29,
      "p1": "983",
      "pn": "986",
      "abstract": [
        "Mizo is a Tibeto-Burman language belonging to the Kuki-Chin subfamily and it has four lexical tones, namely, high, low, rising and falling. Contextual influence on tones of Mizo is investigated in this study. Trisyllabic Mizo phrases are recorded with the four Mizo tones in H_H, R_R, L_L and F_F contexts. The target word is also recorded in isolation. Both carryover and anticipatory influences were found in various degrees. In case of low tone targets, preceding tones with high offset raises the target low tone. In case of rising tone targets, following tones with high onset reduce the tone to L tone. The preceding tone does not affect the target tone, in this case. Finally, the results of this study are discussed in comparison to contextual variations reported in other tone languages such as Cantonese, Mandarin Chinese and Thai.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-25",
      "author_area_id": "2",
      "author_area_label": "Phonetics, Phonology, and Prosody"
    },
    "wochner15_interspeech": {
      "authors": [
        [
          "Daniela",
          "Wochner"
        ],
        [
          "Jana",
          "Schlegel"
        ],
        [
          "Nicole",
          "Deh\u00e9"
        ],
        [
          "Bettina",
          "Braun"
        ]
      ],
      "title": "The prosodic marking of rhetorical questions in German",
      "original": "i15_0987",
      "page_count": 5,
      "order": 30,
      "p1": "987",
      "pn": "991",
      "abstract": [
        "This study investigates the prosody of rhetorical questions in German. In an interaction study we examined how speakers use boundary tones, pitch, duration and voice quality to mark syntactically ambiguous questions as rhetorical or information-seeking. To this end, speakers produced identical interrogatives (polar and wh-questions) in rhetorical and information-seeking contexts. The results show that, phonologically, rhetorical questions end in low boundary tones more often, but the difference is minimal (5%), especially compared to the effect of interrogative type on boundary tones (polar questions are rising and wh-questions are falling in more than 90% of the cases, respectively). Phonetically, rhetorical questions are characterized by longer utterance durations and lower initial pitch. Furthermore, the first constituent (wh-word or verb) is produced with a softer voice compared to information-seeking questions. Overall the results show that there are prosodic differences between RQs and ISQs for both polar and wh-questions. There seems to be a tendency for rhetoricity to be signalled early-on in the utterance in both polar and wh-questions. Furthermore, the phonetic cues for disambiguation are stronger in wh-questions than in polar questions.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-26",
      "author_area_id": "2",
      "author_area_label": "Phonetics, Phonology, and Prosody"
    },
    "zorila15_interspeech": {
      "authors": [
        [
          "Tudor-C\u0103t\u0103lin",
          "Zoril\u0103"
        ],
        [
          "Yannis",
          "Stylianou"
        ]
      ],
      "title": "A fast algorithm for improved intelligibility of speech-in-noise based on frequency and time domain energy reallocation",
      "original": "i15_0060",
      "page_count": 5,
      "order": 31,
      "p1": "60",
      "pn": "64",
      "abstract": [
        "This paper presents a fast and effective algorithm for enhancing speech intelligibility in additive noise conditions under the constraint of equal signal power before and after enhancement. Speech energy is reallocated in time, using dynamic range compression, and in frequency by boosting the signal to noise ratio in high frequencies, increasing the contrast between consecutive spectral peaks and valleys for the mid-frequencies, while maintaining the spectral energy in low frequencies. The algorithm has 90% lower computational load than similar and recently suggested state-of-the art approaches, while in large formal speech-in-noise intelligibility tests, the algorithm has shown to perform equally well to these methods in terms of intelligibility gains.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-27",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "koutsogiannaki15_interspeech": {
      "authors": [
        [
          "Maria",
          "Koutsogiannaki"
        ],
        [
          "Petko N.",
          "Petkov"
        ],
        [
          "Yannis",
          "Stylianou"
        ]
      ],
      "title": "Intelligibility enhancement of casual speech for reverberant environments inspired by clear speech properties",
      "original": "i15_0065",
      "page_count": 5,
      "order": 32,
      "p1": "65",
      "pn": "69",
      "abstract": [
        "Clear speech has been shown to have an intelligibility advantage over casual speech in noisy and reverberant environments. This work validates spectral and time domain modifications to increase the intelligibility of casual speech in reverberant environments by compensating particular differences between the two speaking styles. To compensate spectral differences, a frequency-domain filtering approach is applied to casual speech. In time domain, two techniques for time-scaling casual speech are explored: (1) uniform time-scaling and (2) pause insertion and phoneme elongation based on loudness and modulation criteria. The effect of the proposed modifications is evaluated through subjective listening tests in two reverberant conditions with reverberation time 0.8s and 2s. The combination of spectral transformation and uniform time-scaling is shown to be the most successful in increasing the intelligibility of casual speech. The evaluation results support the conclusion that modifications inspired by clear speech can be beneficial for the intelligibility enhancement of speech in reverberant environments.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-28",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "jemaa15_interspeech": {
      "authors": [
        [
          "A. Ben",
          "Jemaa"
        ],
        [
          "N.",
          "Mechergui"
        ],
        [
          "G.",
          "Courtois"
        ],
        [
          "A.",
          "Mudry"
        ],
        [
          "S.",
          "Djaziri-Larbi"
        ],
        [
          "M.",
          "Turki"
        ],
        [
          "H.",
          "Lissek"
        ],
        [
          "M.",
          "Jaidane"
        ]
      ],
      "title": "Intelligibility enhancement of vocal announcements for public address systems: a design for all through a presbycusis pre-compensation filter",
      "original": "i15_0070",
      "page_count": 5,
      "order": 33,
      "p1": "70",
      "pn": "74",
      "abstract": [
        "Listeners suffering from presbycusis (age-related hearing loss) often report difficulties when attempting to understand vocal announcements in public spaces. Current solutions that improve speech intelligibility for hearing-impaired subjects mainly consist of customized solutions, such as hearing aids. This study proposes a more generic strategy, which would enhance speech perception for both normal-hearing and hearing-impaired listeners, i.e. For All. It provides the early stages of such an approach. Digital filters with different degrees of hearing-loss compensation have been designed, getting inspired by the way hearing aids process speech signals. Subjective tests conducted on normal-hearing and presbycusis subjects confirmed that it is possible to improve speech intelligibility for both types of population simultaneously.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-29",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "schepker15_interspeech": {
      "authors": [
        [
          "Henning",
          "Schepker"
        ],
        [
          "David",
          "H\u00fclsmeier"
        ],
        [
          "Jan",
          "Rennies"
        ],
        [
          "Simon",
          "Doclo"
        ]
      ],
      "title": "Model-based integration of reverberation for noise-adaptive near-end listening enhancement",
      "original": "i15_0075",
      "page_count": 5,
      "order": 34,
      "p1": "75",
      "pn": "79",
      "abstract": [
        "Speech intelligibility is an important factor for successful speech communication in today's society. So-called near-end listening enhancement (NELE) algorithms aim at improving speech intelligibility in conditions where the (clean) speech signal is accessible and can be modified prior to its presentation. However, many of these algorithms only consider the detrimental effect of noise and disregard the effect of reverberation. Therefore, in this paper we propose to additionally incorporate the detrimental effects of reverberation into noise-adaptive near-end listening enhancement algorithms. Based on the Speech Transmission Index (STI), which is widely used for speech intelligibility prediction, the effect of reverberation is effectively accounted for as an additional noise power term. This combined noise power term is used in a state-of-the-art noise-adaptive NELE algorithm. Simulations using two objective measures, the STI and the short-time objective intelligibility (STOI) measure demonstrate the potential of the proposed approach to improve the predicted speech intelligibility in noisy and reverberant conditions.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-30",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "rottschafer15_interspeech": {
      "authors": [
        [
          "Sebastian",
          "Rottsch\u00e4fer"
        ],
        [
          "Hendrik",
          "Buschmeier"
        ],
        [
          "Herwin van",
          "Welbergen"
        ],
        [
          "Stefan",
          "Kopp"
        ]
      ],
      "title": "Online Lombard adaptation in incremental speech synthesis",
      "original": "i15_0080",
      "page_count": 5,
      "order": 35,
      "p1": "80",
      "pn": "84",
      "abstract": [
        "The `Lombard effect' consists of various speech adaptation mechanisms human speakers use involuntarily to counter influences that a noisy environment has on their speech intelligibility. These adaptations are highly dependent on the characteristics of the noise and happen rapidly. Modelling the effect for the output side of speech interfaces is therefore difficult: the noise characteristics need to be evaluated continuously and speech synthesis adaptations need to take effect immediately. This paper describes and evaluates an online system consisting of a module that analyses the acoustic environment and a module that adapts the speech parameters of an incremental speech synthesis system in a timely manner. In an evaluation with human listeners the system had a similar effect on intelligibility as had human speakers in offline studies. Furthermore, during noise the Lombard-adapted speech was rated more natural than standard speech.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-31",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "jokinen15_interspeech": {
      "authors": [
        [
          "Emma",
          "Jokinen"
        ],
        [
          "Ulpu",
          "Remes"
        ],
        [
          "Paavo",
          "Alku"
        ]
      ],
      "title": "Comparison of Gaussian process regression and Gaussian mixture models in spectral tilt modelling for intelligibility enhancement of telephone speech",
      "original": "i15_0085",
      "page_count": 5,
      "order": 36,
      "p1": "85",
      "pn": "89",
      "abstract": [
        "Intelligibility enhancement can be applied in mobile communications as a post-processing step when the background noise conditions are adverse. In this study, post-processing methods aiming to model the Lombard effect are investigated. More specifically, the study focuses on mapping the spectral tilt of normal speech to that of Lombard speech to improve intelligibility of telephone speech in near-end noise conditions. Two different modelling techniques, Gaussian mixture models (GMMs) and Gaussian processes (GPs), are evaluated with different amounts of training data. Normal-to-Lombard conversions implemented by GMMs and GPs are then compared objectively as well as in subjective intelligibility and quality tests with unprocessed speech in different noise conditions. All GMMs and GPs evaluated in the subjective tests were able to improve intelligibility without significant decrease in quality compared to unprocessed speech. While the best intelligibility results were obtained with a GP model, other GMM and GP alternatives were rated higher in quality. Based on the results, determining the best modelling technique for normal-to-Lombard mapping is challenging and calls for further studies.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-32",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "kumar15_interspeech": {
      "authors": [
        [
          "Naveen",
          "Kumar"
        ],
        [
          "Shrikanth S.",
          "Narayanan"
        ]
      ],
      "title": "A discriminative reliability-aware classification model with applications to intelligibility classification in pathological speech",
      "original": "i15_0090",
      "page_count": 5,
      "order": 37,
      "p1": "90",
      "pn": "94",
      "abstract": [
        "Many computational paralinguistic tasks need to work with noisy human annotations that are inherently challenging for the human annotator to provide. In this paper, we propose a discriminative model to account for the inherent heterogeneity in the reliability of annotations associated with a sample while training automatic classification models. Reliability is modeled as a latent factor that governs the dependence between the observed features and its corresponding annotated class label. We propose an expectation-maximization algorithm to learn the latent reliability scores using maximum entropy models in a mixture-of-experts like framework. In addition, two models \u2014 a feature dependent reliable model and a feature independent unreliable model are also learned. We test the proposed method on classifying the intelligibility of pathological speech. The results show that the method is able to exploit latent reliability information on feature sets that are noisy. Comparing against a baseline of reliability-blind maximum entropy model, we show that there is merit to reliability-aware classification when the feature set is unreliable.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-33",
      "author_area_id": "3",
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "orozcoarroyave15_interspeech": {
      "authors": [
        [
          "J. R.",
          "Orozco-Arroyave"
        ],
        [
          "Florian",
          "H\u00f6nig"
        ],
        [
          "J. D.",
          "Arias-Londo\u00f1o"
        ],
        [
          "J. F.",
          "Vargas-Bonilla"
        ],
        [
          "Sabine",
          "Skodda"
        ],
        [
          "J.",
          "Rusz"
        ],
        [
          "Elmar",
          "N\u00f6th"
        ]
      ],
      "title": "Voiced/unvoiced transitions in speech as a potential bio-marker to detect Parkinson's disease",
      "original": "i15_0095",
      "page_count": 5,
      "order": 38,
      "p1": "95",
      "pn": "99",
      "abstract": [
        "Several studies have addressed the automatic classification of speakers with Parkinson's disease (PD) and healthy controls (HC). Most of the studies are based on speech recordings of sustained vowels, isolated words, and single sentences. Only few investigations have considered read texts and/or spontaneous speech. This paper addresses two main questions still open regarding the automatic analysis speech in patients with PD, (a) \u201cIs it possible to classify PD patients and HC through running speech signals in multiple languages?\u201d, and (b) \u201cwhere is the information to discriminate between speech recordings of PD patients and HC?\u201d In this paper speech recordings of read texts and monologues spoken in three different languages are considered. The energy content of the borders between voiced and unvoiced sounds is modeled. According to the results with read texts it is possible to achieve accuracies ranging from 91% to 98% depending on the language. With respect to the results on monologues, the accuracies are above 98% in all of the three languages. The presence of discriminant information in the voiced/unvoiced and unvoiced/voiced transitions is validated here, evidencing the problems of PD patients to stop/start the vocal folds movement during the production of running speech.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-34",
      "author_area_id": "3",
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "villacanas15_interspeech": {
      "authors": [
        [
          "T.",
          "Villa-Ca\u00f1as"
        ],
        [
          "J. D.",
          "Arias-Londo\u00f1o"
        ],
        [
          "J. R.",
          "Orozco-Arroyave"
        ],
        [
          "J. F.",
          "Vargas-Bonilla"
        ],
        [
          "Elmar",
          "N\u00f6th"
        ]
      ],
      "title": "Low-frequency components analysis in running speech for the automatic detection of Parkinson's disease",
      "original": "i15_0100",
      "page_count": 5,
      "order": 39,
      "p1": "100",
      "pn": "104",
      "abstract": [
        "This paper explores the analysis of low-frequency components of continuous speech signals from people with Parkinson's disease, in order to detect changes in the spectrum that could be associated to the presence of tremor in the speech. Different time-frequency (TF) techniques are used for the characterization of the low frequency content of the speech signals, by paying special attention on the ability to work in non-stationary frameworks, due to the need for the analysis of long enough time segments, where the assumptions of stationary can not be met. The set of variables extracted from the TF representations includes centroids and the energy content of different frequency bands, along with entropy measures and nonlinear energy operators, which are used as features for the automatic detection of people with Parkinson's disease vs healthy controls. The discrimination capability of the estimated features is evaluated using three different classification strategies: GMM, GMM-UBM, and SVM. Furthermore, the information provided by different TF techniques is combined using a second classification stage. The results show that the changes in the low frequency components are able to discriminate between people with Parkinson's and healthy speakers with an accuracy of 77%, using one single sentence.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-35",
      "author_area_id": "3",
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "vasquezcorrea15_interspeech": {
      "authors": [
        [
          "J. C.",
          "V\u00e1squez-Correa"
        ],
        [
          "T.",
          "Arias-Vergara"
        ],
        [
          "J. R.",
          "Orozco-Arroyave"
        ],
        [
          "J. F.",
          "Vargas-Bonilla"
        ],
        [
          "J. D.",
          "Arias-Londo\u00f1o"
        ],
        [
          "Elmar",
          "N\u00f6th"
        ]
      ],
      "title": "Automatic detection of Parkinson's disease from continuous speech recorded in non-controlled noise conditions",
      "original": "i15_0105",
      "page_count": 5,
      "order": 40,
      "p1": "105",
      "pn": "109",
      "abstract": [
        "Automatic classification of Parkinson's disease (PD) speakers and healthy controls (HC) is performed considering speech recordings collected in non-controlled noise conditions. The speech tasks include six sentences and a read text. The recording is performed using an open source portable device and a commercial microphone. A speech enhancement (SE) technique is applied to improve the quality of the signals. Voiced and unvoiced frames are segmented from the speech tasks and characterized separately. The discrimination of speakers with PD and HC is performed using a support vector machine with soft margin. The results indicate that it is possible to discriminate between PD and HC speakers using recordings collected in non-controlled noise conditions. The accuracies obtained with the voiced features range from 64% to 86%. For unvoiced features the accuracies range from 78% to 99%. The SE algorithm improves the accuracies of the unvoiced frames in up to 11 percentage points, while the accuracies decrease in the voiced frames when the SE algorithm is applied. This work is a step forward to the development of portable devices to assess the speech of people with PD.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-36",
      "author_area_id": "3",
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "cummins15_interspeech": {
      "authors": [
        [
          "Nicholas",
          "Cummins"
        ],
        [
          "Vidhyasaharan",
          "Sethu"
        ],
        [
          "Julien",
          "Epps"
        ],
        [
          "Jarek",
          "Krajewski"
        ]
      ],
      "title": "Relevance vector machine for depression prediction",
      "original": "i15_0110",
      "page_count": 5,
      "order": 41,
      "p1": "110",
      "pn": "114",
      "abstract": [
        "The objective and automated monitoring of depression using behavioral signals is confounded by the wide clinical profile of this commonly occurring mood disorder. This paper introduces Relevance Vector Machines, a novel method for predicting clinical depression scores from paralinguistic cues. It highlights many of the advantages RVM can offer depression prediction; sparsity, implicit noise characterization, an explicit probabilistic output and heterogeneous mapping property which allow one or more arbitrary, non-linear, transform to be used in conjunction with a RVM. Results indicate that RVMs can perform as strongly as Support Vector Regression in a brute-forcing paradigm. Of particular interest is the heterogeneous mapping property which improves RVM performance without requiring an expensive, in terms of data and time, search of the operating parameter space.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-37",
      "author_area_id": "3",
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "marchi15_interspeech": {
      "authors": [
        [
          "Erik",
          "Marchi"
        ],
        [
          "Bj\u00f6rn",
          "Schuller"
        ],
        [
          "Simon",
          "Baron-Cohen"
        ],
        [
          "Ofer",
          "Golan"
        ],
        [
          "Sven",
          "B\u00f6lte"
        ],
        [
          "Prerna",
          "Arora"
        ],
        [
          "Reinhold",
          "H\u00e4b-Umbach"
        ]
      ],
      "title": "Typicality and emotion in the voice of children with autism spectrum condition: evidence across three languages",
      "original": "i15_0115",
      "page_count": 5,
      "order": 42,
      "p1": "115",
      "pn": "119",
      "abstract": [
        "Only a few studies exist on automatic emotion analysis of speech from children with Autism Spectrum Conditions (ASC). Out of these, some preliminary studies have recently focused on comparing the relevance of selected acoustic features against large sets of prosodic, spectral, and cepstral features; however, no study so far provided a comparison of performances across different languages. The present contribution aims to fill this white spot in the literature and provide insight by extensive evaluations carried out on three databases of prompted phrases collected in English, Swedish, and Hebrew, inducing nine emotion categories embedded in short-stories. The datasets contain speech of children with ASC and typically developing children under the same conditions. We evaluate automatic diagnosis and recognition of emotions in atypical children's voice over the nine categories including binary valence/arousal discrimination.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-38",
      "author_area_id": "3",
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "liu15_interspeech": {
      "authors": [
        [
          "Chunxi",
          "Liu"
        ],
        [
          "Puyang",
          "Xu"
        ],
        [
          "Ruhi",
          "Sarikaya"
        ]
      ],
      "title": "Deep contextual language understanding in spoken dialogue systems",
      "original": "i15_0120",
      "page_count": 5,
      "order": 43,
      "p1": "120",
      "pn": "124",
      "abstract": [
        "We describe a unified multi-turn multi-task spoken language understanding (SLU) solution capable of handling multiple context sensitive classification (intent determination) and sequence labeling (slot filling) tasks simultaneously. The proposed architecture is based on recurrent convolutional neural networks (RCNN) with shared feature layers and globally normalized sequence modeling components. The temporal dependencies within and across different tasks are encoded succinctly as recurrent connections. The dialog system responses beyond SLU component are also exploited as effective external features. We show with extensive experiments on a number of datasets that the proposed joint learning framework generates state-of-the-art results for both classification and tagging, and the contextual modeling based on recurrent and external features significantly improves the context sensitivity of SLU models.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-39",
      "author_area_id": "11",
      "author_area_label": "Spoken Language Processing \u2013 Dialogue, Summarization, Understanding"
    },
    "tam15_interspeech": {
      "authors": [
        [
          "Yik-Cheung",
          "Tam"
        ],
        [
          "Yangyang",
          "Shi"
        ],
        [
          "Hunk",
          "Chen"
        ],
        [
          "Mei-Yuh",
          "Hwang"
        ]
      ],
      "title": "RNN-based labeled data generation for spoken language understanding",
      "original": "i15_0125",
      "page_count": 5,
      "order": 44,
      "p1": "125",
      "pn": "129",
      "abstract": [
        "In spoken language understanding, getting manually labeled data such as domain, intent and slot labels is usually required for training classifiers. Starting with some manually labeled data, we propose a data generation approach to augment the training set with synthetic data sampled from a joint distribution between an input query and an output label. We propose using a recurrent neural network to model the joint distribution and sample synthetic data for classifier training. Evaluated on ATIS and live logs of Cortana, a Microsoft voice personal assistant, we showed consistent performance improvement on domain classification, intent classification, and slot tagging on multiple languages.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-40",
      "author_area_id": "11",
      "author_area_label": "Spoken Language Processing \u2013 Dialogue, Summarization, Understanding"
    },
    "vukotic15_interspeech": {
      "authors": [
        [
          "Vedran",
          "Vukotic"
        ],
        [
          "Christian",
          "Raymond"
        ],
        [
          "Guillaume",
          "Gravier"
        ]
      ],
      "title": "Is it time to switch to word embedding and recurrent neural networks for spoken language understanding?",
      "original": "i15_0130",
      "page_count": 5,
      "order": 45,
      "p1": "130",
      "pn": "134",
      "abstract": [
        "Recently, word embedding representations have been investigated for slot filling in Spoken Language Understanding, along with the use of Neural Networks as classifiers. Neural Networks, especially Recurrent Neural Networks, that are specifically adapted to sequence labeling problems, have been applied successfully on the popular ATIS database. In this work, we make a comparison of this kind of models with the previously state-of-the-art Conditional Random Fields (CRF) classifier on a more challenging SLU database. We show that, despite efficient word representations used within these Neural Networks, their ability to process sequences is still significantly lower than for CRF, while also having a drawback of higher computational costs, and that the ability of CRF to model output label dependencies is crucial for SLU.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-41",
      "author_area_id": "11",
      "author_area_label": "Spoken Language Processing \u2013 Dialogue, Summarization, Understanding"
    },
    "ravuri15_interspeech": {
      "authors": [
        [
          "Suman",
          "Ravuri"
        ],
        [
          "Andreas",
          "Stolcke"
        ]
      ],
      "title": "Recurrent neural network and LSTM models for lexical utterance classification",
      "original": "i15_0135",
      "page_count": 5,
      "order": 46,
      "p1": "135",
      "pn": "139",
      "abstract": [
        "Utterance classification is a critical pre-processing step for many speech understanding and dialog systems. In multi-user settings, one needs to first identify if an utterance is even directed at the system, followed by another level of classification to determine the intent of the user's input. In this work, we propose RNN and LSTM models for both these tasks. We show how both models outperform baselines based on ngram-based language models (LMs), feedforward neural network LMs, and boosting classifiers. To deal with the high rate of singleton and out-of-vocabulary words in the data, we also investigate a word input encoding based on character ngrams, and show how this representation beats the standard one-hot vector word encoding. Overall, these proposed approaches achieve over 30% relative reduction in equal error rate compared to boosting classifier baseline on an ATIS utterance intent classification task, and over 3.9% absolute reduction in equal error rate compared to a the maximum entropy LM baseline of 27.0% on an addressee detection task. We find that RNNs work best when utterances are short, while LSTMs are best when utterances are longer.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-42",
      "author_area_id": "11",
      "author_area_label": "Spoken Language Processing \u2013 Dialogue, Summarization, Understanding"
    },
    "lu15_interspeech": {
      "authors": [
        [
          "Hung-tsung",
          "Lu"
        ],
        [
          "Yuan-ming",
          "Liou"
        ],
        [
          "Hung-yi",
          "Lee"
        ],
        [
          "Lin-shan",
          "Lee"
        ]
      ],
      "title": "Semantic retrieval of personal photos using a deep autoencoder fusing visual features with speech annotations represented as word/paragraph vectors",
      "original": "i15_0140",
      "page_count": 5,
      "order": 47,
      "p1": "140",
      "pn": "144",
      "abstract": [
        "It is very attractive for the user to retrieve photos from a huge collection using high-level personal queries (e.g. \u201cuncle Bill's house\u201d), but technically very challenging. Previous works proposed a set of approaches toward the goal assuming only 30% of the photos are annotated by sparse spoken descriptions when the photos are taken. In this paper, to promote the interaction between different types of features, we use the continuous space word representations to train a paragraph vector model for the speech annotation, and then fuse the paragraph vector with the visual features produced by deep Convolutional Neural Network (CNN) using a Deep AutoEncoder (DAE). The retrieval framework therefore combines the word vectors and paragraph vectors of the speech annotations, the CNN-based visual features, and the DAE-based fused visual/speech features in a three-stage process including a two-layer random walk. The retrieval performance was significantly improved in the preliminary experiments.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-43",
      "author_area_id": "11",
      "author_area_label": "Spoken Language Processing \u2013 Dialogue, Summarization, Understanding"
    },
    "morchid15_interspeech": {
      "authors": [
        [
          "Mohamed",
          "Morchid"
        ],
        [
          "Richard",
          "Dufour"
        ],
        [
          "Driss",
          "Matrouf"
        ]
      ],
      "title": "A comparison of normalization techniques applied to latent space representations for speech analytics",
      "original": "i15_0145",
      "page_count": 5,
      "order": 48,
      "p1": "145",
      "pn": "149",
      "abstract": [
        "In the context of noisy environments, Automatic Speech Recognition (ASR) systems usually produce poor transcription quality which also negatively impact performance of speech analytics. Various methods have then been proposed to compensate the bad effect of ASR errors, mainly by projecting transcribed words in an abstract space. In this paper, we seek to identify themes from dialogues of telephone conversation services using latent topic-spaces estimated from a latent Dirichlet allocation (LDA). As an outcome, a document can be represented with a vector containing probabilities to be associated to each topic estimated with LDA. This vector should nonetheless be normalized to condition document representations. We propose to compare the original LDA vector representation (without normalization) with two normalization approaches, the Eigen Factor Radial (EFR) and the Feature Warping (FW) methods, already successfully applied in speaker recognition field, but never compared and evaluated in the context of a speech analytic task. Results show the interest of these normalization techniques for theme identification tasks using automatic transcriptions The EFR normalization approach allows a gain of 3.67 and 3.06 points respectively in comparison to the absence of normalization and to the FW normalization technique.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-44",
      "author_area_id": "11",
      "author_area_label": "Spoken Language Processing \u2013 Dialogue, Summarization, Understanding"
    },
    "sheikh15_interspeech": {
      "authors": [
        [
          "Imran",
          "Sheikh"
        ],
        [
          "Irina",
          "Illina"
        ],
        [
          "Dominique",
          "Fohr"
        ]
      ],
      "title": "Study of entity-topic models for OOV proper name retrieval",
      "original": "i15_1344",
      "page_count": 5,
      "order": 49,
      "p1": "1344",
      "pn": "1348",
      "abstract": [
        "Retrieving Proper Names (PNs) relevant to an audio document can improve speech recognition and content based audio-video indexing. Latent Dirichlet Allocation (LDA) topic model has been used to retrieve Out-Of-Vocabulary (OOV) PNs relevant to an audio document with good recall rates. However, retrieval of OOV PNs using LDA is affected by two issues, which we study in this paper: (1) Word Frequency Bias (less frequent OOV PNs are ranked lower); (2) Loss of Specificity (the reduced topic space representation loses lexical context). Entity-Topic models have been proposed as extensions of LDA to specifically learn relations between words, entities (PNs) and topics. We study OOV PN retrieval with Entity-Topic models and show that they are also affected by word frequency bias and loss of specificity. We evaluate our proposed methods for rare OOV PN re-ranking and lexical context re-ranking for LDA as well as for Entity-Topic models. The results show an improvement in both Recall and the Mean Average Precision.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-45",
      "author_area_id": "11",
      "author_area_label": "Spoken Language Processing \u2013 Dialogue, Summarization, Understanding"
    },
    "boutin15_interspeech": {
      "authors": [
        [
          "Simon",
          "Boutin"
        ],
        [
          "R\u00e9al",
          "Tremblay"
        ],
        [
          "Patrick",
          "Cardinal"
        ],
        [
          "Doug",
          "Peters"
        ],
        [
          "Pierre",
          "Dumouchel"
        ]
      ],
      "title": "Audio quotation marks for natural language understanding",
      "original": "i15_1349",
      "page_count": 4,
      "order": 50,
      "p1": "1349",
      "pn": "1352",
      "abstract": [
        "Detecting the presence of quotations in speech is a difficult task for automatic natural language understanding. This paper presents a study on the correlation between three prosodic features present in a voice command and the presence or absence of quotations. These features consist of intra-word pause durations, F0 reset and F0 continuity. A combination of lexical and prosodic extraction tools was used to extract these features. The two-sample Kolmogorov-Smirnov test was then used to compare the distributions of the collected measures. The results show a correlation between these features and the presence or absence of quotations. Moreover, the results show that it is possible to use these features to differentiate direct from indirect quotations.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-46",
      "author_area_id": "11",
      "author_area_label": "Spoken Language Processing \u2013 Dialogue, Summarization, Understanding"
    },
    "yang15_interspeech": {
      "authors": [
        [
          "Xiaohao",
          "Yang"
        ],
        [
          "Jia",
          "Liu"
        ]
      ],
      "title": "Using word confusion networks for slot filling in spoken language understanding",
      "original": "i15_1353",
      "page_count": 5,
      "order": 51,
      "p1": "1353",
      "pn": "1357",
      "abstract": [
        "Semantic slot filling is one of the most challenging problems in spoken language understanding (SLU) because of automatic speech recognition (ASR) errors. To improve the performance of slot filling, a successful approach is to use a statistical model that is trained on ASR one-best hypotheses. The state of the art models for slot filling rely on using discriminative sequence modeling methods, such as conditional random fields (CRFs), recurrent neural networks (RNNs) and the recent recurrent CRF (R-CRF) model. In our previous work, we have also proposed the combination model of CRF and deep belief network (CRF-DBN). However, they are mostly trained with the one-best hypotheses from the ASR system. In this paper, we propose to exploit word confusion networks (WCNs) by taking the word bins in a WCN as training or testing units instead of the independent words. The units are represented by vectors composed of multiple aligned ASR hypotheses and the corresponding posterior probabilities. Before training the model, we cluster similar units that may originate from the same word. We apply our proposed method to the CRF, CRF-DBN and R-CRF models. The experiments on ATIS corpus show consistent improvements of the performance by using WCNs.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-47",
      "author_area_id": "11",
      "author_area_label": "Spoken Language Processing \u2013 Dialogue, Summarization, Understanding"
    },
    "chiu15_interspeech": {
      "authors": [
        [
          "Justin",
          "Chiu"
        ],
        [
          "Yajie",
          "Miao"
        ],
        [
          "Alan W.",
          "Black"
        ],
        [
          "Alexander I.",
          "Rudnicky"
        ]
      ],
      "title": "Distributed representation-based spoken word sense induction",
      "original": "i15_1358",
      "page_count": 5,
      "order": 52,
      "p1": "1358",
      "pn": "1362",
      "abstract": [
        "Spoken Term Detection (STD) or Keyword Search (KWS) techniques can locate keyword instances but do not differentiate between meanings. Spoken Word Sense Induction (SWSI) differentiates target instances by clustering according to context, providing a more useful result. In this paper we present a fully unsupervised SWSI approach based on distributed representations of spoken utterances. We compare this approach to several others, including the state-of-the-art Hierarchical Dirichlet Process (HDP). To determine how ASR performance affects SWSI, we used three different levels of Word Error Rate (WER), 40%, 20% and 0%; 40% WER is representative of online video, 0% of text. We show that the distributed representation approach outperforms all other approaches, regardless of the WER. Although LDA-based approaches do well on clean data, they degrade significantly with WER. Paradoxically, lower WER does not guarantee better SWSI performance, due to the influence of common locutions.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-48",
      "author_area_id": "11",
      "author_area_label": "Spoken Language Processing \u2013 Dialogue, Summarization, Understanding"
    },
    "shen15_interspeech": {
      "authors": [
        [
          "Sheng-syun",
          "Shen"
        ],
        [
          "Hung-yi",
          "Lee"
        ],
        [
          "Shang-wen",
          "Li"
        ],
        [
          "Victor",
          "Zue"
        ],
        [
          "Lin-shan",
          "Lee"
        ]
      ],
      "title": "Structuring lectures in massive open online courses (MOOCs) for efficient learning by linking similar sections and predicting prerequisites",
      "original": "i15_1363",
      "page_count": 5,
      "order": 53,
      "p1": "1363",
      "pn": "1367",
      "abstract": [
        "The increasing popularity of Massive Open Online Courses (MOOCs) has resulted in huge number of courses available over the Internet. Typically, a learner can type a search query into the look-up window of a MOOC platform and receive a set of course suggestions. But it is difficult for the learner to select lectures out of those suggested courses and learn the desired information efficiently. In this paper, we propose to structure the lectures of the various suggested courses into a map (graph) for each query entered by the learner, indicating the lectures with very similar content and reasonable sequence order of learning. In this way the learner can define his own learning path on the map based on his interests and backgrounds, and learn the desired information from lectures in different courses without too much difficulties in minimum time. We propose a series of approaches for linking lectures of very similar content and predicting the prerequisites for this purpose. Preliminary results show that the proposed approaches have the potential to achieve the above goal.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-49",
      "author_area_id": "11",
      "author_area_label": "Spoken Language Processing \u2013 Dialogue, Summarization, Understanding"
    },
    "charlet15_interspeech": {
      "authors": [
        [
          "Delphine",
          "Charlet"
        ],
        [
          "G\u00e9raldine",
          "Damnati"
        ],
        [
          "J\u00e9r\u00e9my",
          "Trione"
        ]
      ],
      "title": "News talk-show chaptering with journalistic genres",
      "original": "i15_1368",
      "page_count": 5,
      "order": 54,
      "p1": "1368",
      "pn": "1372",
      "abstract": [
        "Modern TV or radio news talk-shows include a variety of sequences which comply with specific journalistic patterns, including debates, interviews, reports. The paper deals with automatic chapter generation for TV news talk-shows, according to these different journalistic genres. It is shown that linguistic and speaker-distribution based features can lead to an efficient characterization of these genres when the boundaries of the chapters are known, and that a speaker-distribution based segmentation is suitable for segmenting contents into these different genres. Evaluations on a collection of 42 episodes of a news talk-show provided by the French evaluation campaign REPERE show promising performance.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-50",
      "author_area_id": "11",
      "author_area_label": "Spoken Language Processing \u2013 Dialogue, Summarization, Understanding"
    },
    "ramanarayanan15_interspeech": {
      "authors": [
        [
          "Vikram",
          "Ramanarayanan"
        ],
        [
          "Lei",
          "Chen"
        ],
        [
          "Chee Wee",
          "Leong"
        ],
        [
          "Gary",
          "Feng"
        ],
        [
          "David",
          "Suendermann-Oeft"
        ]
      ],
      "title": "An analysis of time-aggregated and time-series features for scoring different aspects of multimodal presentation data",
      "original": "i15_1373",
      "page_count": 5,
      "order": 55,
      "p1": "1373",
      "pn": "1377",
      "abstract": [
        "We present a technique for automated assessment of public speaking and presentation proficiency based on the analysis of concurrently recorded speech and motion capture data. With respect to Kinect motion capture data, we examine both time-aggregated as well as time-series based features. While the former is based on statistical functionals of body-part position and/or velocity computed over the entire series, the latter feature set, dubbed histograms of cooccurrences, captures how often different broad postural configurations co-occur within different time lags of each other over the evolution of the multimodal time series. We examine the relative utility of these features, along with curated features derived from the speech stream, in predicting human-rated scores of different aspects of public speaking and presentation proficiency. We further show that these features outperform the human inter-rater agreement baseline for a subset of the analyzed aspects.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-51",
      "author_area_id": "11",
      "author_area_label": "Spoken Language Processing \u2013 Dialogue, Summarization, Understanding"
    },
    "racca15_interspeech": {
      "authors": [
        [
          "David N.",
          "Racca"
        ],
        [
          "Gareth J. F.",
          "Jones"
        ]
      ],
      "title": "Incorporating prosodic prominence evidence into term weights for spoken content retrieval",
      "original": "i15_1378",
      "page_count": 5,
      "order": 56,
      "p1": "1378",
      "pn": "1382",
      "abstract": [
        "We present an extended technique for spoken content retrieval (SCR) that exploits the prosodic characteristics of spoken terms in order to improve retrieval effectiveness. Our method promotes the rank of speech segments containing a high number of prosodically prominent terms. Given a set of queries and examples of relevant speech segments, we train a classifier to learn differences in the prosodic realisation of spoken terms mentioned in relevant and non-relevant segments. The classifier is trained with a set of lexical and prosodic features that capture local variations of prosodic prominence. For an unseen query, we perform SCR by using an extension of the Okapi BM25 function of probabilistic retrieval that incorporates the prosodic classifier's predictions into the computation of term weights. Experiments with the speech data from the SDPWS corpus of Japanese oral presentations, and the queries and relevance assessment data from the NTCIR SpokenDoc task show that our approach provides improvements over purely text-based SCR approaches.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-52",
      "author_area_id": "11",
      "author_area_label": "Spoken Language Processing \u2013 Dialogue, Summarization, Understanding"
    },
    "chen15_interspeech": {
      "authors": [
        [
          "Kuan-Yu",
          "Chen"
        ],
        [
          "Shih-Hung",
          "Liu"
        ],
        [
          "Hsin-Min",
          "Wang"
        ],
        [
          "Berlin",
          "Chen"
        ],
        [
          "Hsin-Hsi",
          "Chen"
        ]
      ],
      "title": "Leveraging word embeddings for spoken document summarization",
      "original": "i15_1383",
      "page_count": 5,
      "order": 57,
      "p1": "1383",
      "pn": "1387",
      "abstract": [
        "Owing to the rapidly growing multimedia content available on the Internet, extractive spoken document summarization, with the purpose of automatically selecting a set of representative sentences from a spoken document to concisely express the most important theme of the document, has been an active area of research and experimentation. On the other hand, word embedding has emerged as a newly favorite research subject because of its excellent performance in many natural language processing (NLP)-related tasks. However, as far as we are aware, there are relatively few studies investigating its use in extractive text or speech summarization. A common thread of leveraging word embeddings in the summarization process is to represent the document (or sentence) by averaging the word embeddings of the words occurring in the document (or sentence). Then, intuitively, the cosine similarity measure can be employed to determine the relevance degree between a pair of representations. Beyond the continued efforts made to improve the representation of words, this paper focuses on building novel and efficient ranking models based on the general word embedding methods for extractive speech summarization. Experimental results demonstrate the effectiveness of our proposed methods, compared to existing state-of-the-art methods.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-53",
      "author_area_id": "11",
      "author_area_label": "Spoken Language Processing \u2013 Dialogue, Summarization, Understanding"
    },
    "renkens15_interspeech": {
      "authors": [
        [
          "Vincent",
          "Renkens"
        ],
        [
          "Hugo",
          "Van hamme"
        ]
      ],
      "title": "Mutually exclusive grounding for weakly supervised non-negative matrix factorisation",
      "original": "i15_1388",
      "page_count": 5,
      "order": 58,
      "p1": "1388",
      "pn": "1392",
      "abstract": [
        "Non-negative Matrix Factorisation (NMF) has been successfully applied for learning the meaning of a small set of vocal commands without any prior knowledge of the language. This kind of learning is useful if flexibility in terms of the acoustic and language model is required, for example in assistive technologies for dysarthric speakers because they do not comply with common models. Vocal commands are grounded through the addition of semantic labels that represent the action corresponding to the command. The Kullback Leibler Divergence (KLD) is used to evaluate the acoustic model. The KLD is optimal for Poisson distributed data making it an appropriate metric for the acoustic features because they are a count of acoustic events. The semantic labels are however activations, so a multinomial likelihood function seems more appropriate because they are mutually exclusive. In this paper a cost function to evaluate the semantic model based on the multinomial likelihood function is proposed that aims to better suit its distribution. To minimise the proposed cost function a new set of update rules and a new normalisation scheme are proposed.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-54",
      "author_area_id": "11",
      "author_area_label": "Spoken Language Processing \u2013 Dialogue, Summarization, Understanding"
    },
    "bastianelli15_interspeech": {
      "authors": [
        [
          "Emanuele",
          "Bastianelli"
        ],
        [
          "Danilo",
          "Croce"
        ],
        [
          "Roberto",
          "Basili"
        ],
        [
          "Daniele",
          "Nardi"
        ]
      ],
      "title": "Using semantic maps for robust natural language interaction with robots",
      "original": "i15_1393",
      "page_count": 5,
      "order": 59,
      "p1": "1393",
      "pn": "1397",
      "abstract": [
        "Modern robotic architectures are equipped with sensors enabling a deep analysis of the environment. In this work, we aim at demonstrating that such perceptual information (here modeled through semantic maps) can be effectively used to enhance the language understanding capabilities of the robot. A robust lexical mapping function based on the Distributional Semantics paradigm is here proposed as a basic model of grounding language towards the environment. We show that making such information available to the underlying language understanding algorithms improves the accuracy throughout the entire interpretation process.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-55",
      "author_area_id": "11",
      "author_area_label": "Spoken Language Processing \u2013 Dialogue, Summarization, Understanding"
    },
    "luan15_interspeech": {
      "authors": [
        [
          "Yi",
          "Luan"
        ],
        [
          "Shinji",
          "Watanabe"
        ],
        [
          "Bret",
          "Harsham"
        ]
      ],
      "title": "Efficient learning for spoken language understanding tasks with word embedding based pre-training",
      "original": "i15_1398",
      "page_count": 5,
      "order": 60,
      "p1": "1398",
      "pn": "1402",
      "abstract": [
        "Spoken language understanding (SLU) tasks such as goal estimation and intention identification from user's commands are essential components in spoken dialog systems. In recent years, neural network approaches have shown great success in various SLU tasks. However, one major difficulty of SLU is that the annotation of collected data can be expensive. Often this results in insufficient data being available for a task. The performance of a neural network trained in low resource conditions is usually inferior because of over-training. To improve the performance, this paper investigates the use of unsupervised training methods with large-scale corpora based on word embedding and latent topic models to pre-train the SLU networks. In order to capture long-term characteristics over the entire dialog, we propose a novel Recurrent Neural Network (RNN) architecture. The proposed RNN uses two sub-networks to model the different time scales represented by word and turn sequences. The combination of pre-training and RNN gives us a 18% relative error reduction compared to a baseline system.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-56",
      "author_area_id": "11",
      "author_area_label": "Spoken Language Processing \u2013 Dialogue, Summarization, Understanding"
    },
    "ferreira15_interspeech": {
      "authors": [
        [
          "Emmanuel",
          "Ferreira"
        ],
        [
          "Bassam",
          "Jabaian"
        ],
        [
          "Fabrice",
          "Lef\u00e8vre"
        ]
      ],
      "title": "Zero-shot semantic parser for spoken language understanding",
      "original": "i15_1403",
      "page_count": 5,
      "order": 61,
      "p1": "1403",
      "pn": "1407",
      "abstract": [
        "Machine learning algorithms are now common in the state-of-the-art spoken language understanding models. But to reach good performance they must be trained on a potentially large amount of data which are not available for a variety of tasks and languages of interest. In this work, we present a novel zero-shot learning method, based on word embeddings, allowing to derive a full semantic parser for spoken language understanding.   No annotated in-context data are needed, the ontological description of the target domain and generic word embedding features (learned from freely available general domain data) suffice to derive the model. Two versions are studied with respect to how the model parameters and decoding step are handled, including an extension of the proposed approach in the context of conditional random fields. We show that this model, with very little supervision, can reach instantly performance comparable to those obtained by either state-of-the-art carefully handcrafted rule-based or trained statistical models for extraction of dialog acts on the Dialog State Tracking test datasets (DSTC2 and 3).\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-57",
      "author_area_id": "11",
      "author_area_label": "Spoken Language Processing \u2013 Dialogue, Summarization, Understanding"
    },
    "tafforeau15_interspeech": {
      "authors": [
        [
          "Jeremie",
          "Tafforeau"
        ],
        [
          "Thierry",
          "Artieres"
        ],
        [
          "Benoit",
          "Favre"
        ],
        [
          "Frederic",
          "Bechet"
        ]
      ],
      "title": "Adapting lexical representation and OOV handling from written to spoken language with word embedding",
      "original": "i15_1408",
      "page_count": 5,
      "order": 62,
      "p1": "1408",
      "pn": "1412",
      "abstract": [
        "Word embeddings have become ubiquitous in NLP, especially when using neural networks. One of the assumptions of such representations is that words with similar properties have similar representation, allowing for better generalization from subsequent models. In the standard setting, two kinds of training corpora are used: a very large unlabeled corpus for learning the word embedding representations; and an in-domain training corpus with gold labels for training classifiers on the target NLP task. Because of the amount of data required to learn embeddings, they are trained on large corpus of written text. This can be an issue when dealing with non-canonical language, such as spontaneous speech: embeddings have to be adapted to fit the particularities of spoken transcriptions. However the adaptation corpus available for a given speech application can be limited, resulting in a high number of words from the embedding space not occurring in the adaptation space. We present in this paper a method for adapting an embedding space trained on written text to a spoken corpus of limited size. In particular we deal with words from the embedding space not occurring in the adaptation data. We report experiments done on a Part-Of-Speech task on spontaneous speech transcriptions collected in a call-centre. We show that our word embedding adaptation approach outperforms state-of-the-art Conditional Random Field approach when little in-domain adaptation data is available.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-58",
      "author_area_id": "11",
      "author_area_label": "Spoken Language Processing \u2013 Dialogue, Summarization, Understanding"
    },
    "yang15b_interspeech": {
      "authors": [
        [
          "Xiaohao",
          "Yang"
        ],
        [
          "Jia",
          "Liu"
        ]
      ],
      "title": "Dialog state tracking using long short-term memory neural networks",
      "original": "i15_1800",
      "page_count": 5,
      "order": 63,
      "p1": "1800",
      "pn": "1804",
      "abstract": [
        "Neural network based approaches have recently shown state-of-art performance in the Dialog State Tracking Challenge (DSTC). In DSTC, a tracker is used to assign a label to the state at each moment in an input sequence of a dialog. Specifically, deep neural networks (DNNs) and simple recurrent neural networks (RNNs) have significantly improved the performance of the dialog state tracking. In this paper, we investigate exploiting long short-term memory (LSTM) neural networks, which contain forgetting, input and output gates and are more advanced than simple RNNs, for the dialog state tracking task. To explicitly model the dependence of the output labels, we propose two different models on top of the LSTM un-normalized scores. One is a regression model, the other is a conditional random field (CRF) model. We also apply a deep LSTM to the task. The method is evaluated on the second Dialog State Tracking Challenge (DSTC2) corpus and the results demonstrate that our proposed models can improve the performances of the task.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-59",
      "author_area_id": "11",
      "author_area_label": "Spoken Language Processing \u2013 Dialogue, Summarization, Understanding"
    },
    "lopes15_interspeech": {
      "authors": [
        [
          "Jos\u00e9",
          "Lopes"
        ],
        [
          "Giampiero",
          "Salvi"
        ],
        [
          "Gabriel",
          "Skantze"
        ],
        [
          "Alberto",
          "Abad"
        ],
        [
          "Joakim",
          "Gustafson"
        ],
        [
          "Fernando",
          "Batista"
        ],
        [
          "Raveesh",
          "Meena"
        ],
        [
          "Isabel",
          "Trancoso"
        ]
      ],
      "title": "Detecting repetitions in spoken dialogue systems using phonetic distances",
      "original": "i15_1805",
      "page_count": 5,
      "order": 64,
      "p1": "1805",
      "pn": "1809",
      "abstract": [
        "Repetitions in Spoken Dialogue Systems can be a symptom of problematic communication. Such repetitions are often due to speech recognition errors, which in turn makes it harder to use the output of the speech recognizer to detect repetitions. In this paper, we combine the alignment score obtained using phonetic distances with dialogue-related features to improve repetition detection. To evaluate the method proposed we compare several alignment techniques from edit distance to DTW-based distance, previously used in Spoken-Term detection tasks. We also compare two different methods to compute the phonetic distance: the first one using the phoneme sequence, and the second one using the distance between the phone posterior vectors. Two different datasets were used in this evaluation: a bus-schedule information system (in English) and a call routing system (in Swedish). The results show that approaches using phoneme distances over-perform approaches using Levenshtein distances between ASR outputs for repetition detection.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-60",
      "author_area_id": "11",
      "author_area_label": "Spoken Language Processing \u2013 Dialogue, Summarization, Understanding"
    },
    "crook15_interspeech": {
      "authors": [
        [
          "Paul A.",
          "Crook"
        ],
        [
          "Jean-Philippe",
          "Robichaud"
        ],
        [
          "Ruhi",
          "Sarikaya"
        ]
      ],
      "title": "Multi-language hypotheses ranking and domain tracking for open domain dialogue systems",
      "original": "i15_1810",
      "page_count": 5,
      "order": 65,
      "p1": "1810",
      "pn": "1814",
      "abstract": [
        "Hypothesis ranking (HR) is an approach for improving the accuracy of both domain detection and tracking in multi-domain, multi-turn dialogue systems. This paper presents the results of applying a universal HR model to multiple dialogue systems, each of which are using a different language. It demonstrates that as the set of input features used by HR models are largely language independent a single, universal HR model can be used in place of language specific HR models with only a small loss in accuracy (average absolute gain of +3.55% versus +4.54%), and also such a model can generalise well to new unseen languages, especially related languages (achieving an average absolute gain of +2.8% in domain accuracy on held out locales fr-fr, es-es, it-it; an average of 66% of the gain that could be achieve by training language specific HR models). That the latter is achieved without retraining significantly eases expansion of existing dialogue systems to new locales/languages.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-61",
      "author_area_id": "11",
      "author_area_label": "Spoken Language Processing \u2013 Dialogue, Summarization, Understanding"
    },
    "solanki15_interspeech": {
      "authors": [
        [
          "Vijay",
          "Solanki"
        ],
        [
          "Alessandro",
          "Vinciarelli"
        ],
        [
          "Jane",
          "Stuart-Smith"
        ],
        [
          "Rachel",
          "Smith"
        ]
      ],
      "title": "Measuring mimicry in task-oriented conversations: degree of mimicry is related to task difficulty",
      "original": "i15_1815",
      "page_count": 5,
      "order": 66,
      "p1": "1815",
      "pn": "1819",
      "abstract": [
        "The tendency to unconsciously imitate others in conversations has been referred to as mimicry, accommodation, interpersonal adaptation, etc. During the last few years, the computing community has made significant efforts towards the automatic detection of the phenomenon, but a widely accepted approach is still missing. Given that mimicry is the unconscious tendency to imitate others, this article proposes the adoption of speaker verification methodologies that were originally conceived to spot people trying to forge the voice of others. Preliminary experiments suggest that mimicry can be detected using this methodology by measuring how much speakers converge or diverge with respect to one another in terms of acoustic evidence. As a validation of the approach, the experiments show that convergence (speakers becoming more similar in terms of acoustic properties) tends to appear more frequently when the DiapixUK task requires more time to be completed and, therefore, is more difficult. This is interpreted as an attempt to improve communication through increased coherence.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-62",
      "author_area_id": "11",
      "author_area_label": "Spoken Language Processing \u2013 Dialogue, Summarization, Understanding"
    },
    "laskowski15_interspeech": {
      "authors": [
        [
          "Kornel",
          "Laskowski"
        ]
      ],
      "title": "Auto-imputing radial basis functions for neural-network turn-taking models",
      "original": "i15_1820",
      "page_count": 5,
      "order": 67,
      "p1": "1820",
      "pn": "1824",
      "abstract": [
        "A stochastic turn-taking (STT) model is a per-frame predictor of incipient speech activity. Its ability to make predictions at any instant in time makes it particularly well-suited to the analysis and synthesis of interactive conversation. At the current time, however, STT models are limited by their inability to accept features which may frequently be undefined. Rather than attempting to impute such features, this work proposes and evaluates a mechanism which implicitly conditions Gaussian-distributed features on Bernoulli-distributed indicator features, making prior imputation unnecessary. Experiments indicate that the proposed mechanisms achieve predictive parity with standard model structures, while at the same time offering more direct interpretability and the desired insensitivity to missing feature values.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-63",
      "author_area_id": "11",
      "author_area_label": "Spoken Language Processing \u2013 Dialogue, Summarization, Understanding"
    },
    "llimona15_interspeech": {
      "authors": [
        [
          "Quim",
          "Llimona"
        ],
        [
          "Jordi",
          "Luque"
        ],
        [
          "Xavier",
          "Anguera"
        ],
        [
          "Zoraida",
          "Hidalgo"
        ],
        [
          "Souneil",
          "Park"
        ],
        [
          "Nuria",
          "Oliver"
        ]
      ],
      "title": "Effect of gender and call duration on customer satisfaction in call center big data",
      "original": "i15_1825",
      "page_count": 5,
      "order": 68,
      "p1": "1825",
      "pn": "1829",
      "abstract": [
        "Customer center call data is typically collected by organizations and corporations in order to improve customer experience through the analysis of such call data. In this paper, we report our findings when analysing more than 26 thousand calls to the call centers of a large corporation in a Latin American country. We focus on the impact of gender and call duration on self-reported customer satisfaction. Speech-based gender detection technology is employed to automatically detect the gender of the customer and the agent involved in the calls. A significant correlation is found between self-reported customer satisfaction at the end of the call and gender homophily between the customer and the call center's agent. Interestingly, we do not find any significant effect of call duration on satisfaction.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-64",
      "author_area_id": "11",
      "author_area_label": "Spoken Language Processing \u2013 Dialogue, Summarization, Understanding"
    },
    "callejas15_interspeech": {
      "authors": [
        [
          "Zoraida",
          "Callejas"
        ],
        [
          "David",
          "Griol"
        ]
      ],
      "title": "Using profile similarity to measure agreement in personality perception",
      "original": "i15_1830",
      "page_count": 5,
      "order": 69,
      "p1": "1830",
      "pn": "1834",
      "abstract": [
        "Personality plays a very important role to generate conversational agents with believable interaction capabilities, as well as to build rich user models that consider their peculiarities and behavioural styles. To render or recognize personality, it is necessary to reliably compute agreement across ratings of personality, either to evaluate whether an agent's personality is perceived as intended by human observers, to measure the reliability of the annotation of personality corpora, or to assess if users personality is correctly and/or consistently predicted by automatic recognizers. However, in the literature the automatic measurement of agreement is usually computed using observed agreement or kappa measures. This leads to a loss of information as agreement is generally computed trait-wise and then averaged, instead of considering personality as a profile with several dimensions and comparing the full profiles over all traits. In this paper we discuss the possibilities offered by a repertoire of profile similarity coefficients that can be used either with traits defined ad-hoc for the application domain or with standardized personality trait scores. To show the suitability of our proposal, we have used it with the SSPNet Speaker Personality Corpus, and show that these coefficients provide valuable information that complements state-of-the-art evaluation approaches.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-65",
      "author_area_id": "11",
      "author_area_label": "Spoken Language Processing \u2013 Dialogue, Summarization, Understanding"
    },
    "nakamura15_interspeech": {
      "authors": [
        [
          "Shizuka",
          "Nakamura"
        ],
        [
          "Miki",
          "Watanabe"
        ],
        [
          "Yuichiro",
          "Yoshikawa"
        ],
        [
          "Kohei",
          "Ogawa"
        ],
        [
          "Hiroshi",
          "Ishiguro"
        ]
      ],
      "title": "Relieving mental stress of speakers using a tele-operated robot in foreign language speech education",
      "original": "i15_1835",
      "page_count": 4,
      "order": 70,
      "p1": "1835",
      "pn": "1838",
      "abstract": [
        "In an effort to relieve the mental stress experienced by speakers in foreign language speech education classes, an experiment using a tele-operated robot was conducted and its effect was evaluated. The robot was utilized in the following class environment: the teacher and classmates were not able to see the actual appearance of the speaker; however, the speaker could visually recognize the tele-operated robot speaking as its proxy, and was aware of the fact that the teacher and classmates also recognized the situation. The results of statistical analysis of subjective evaluation by the speakers to evaluate the effect on stress show that the degree of stress in speakers was lower when conversations with the teacher were conducted indirectly via the robot than doing so directly. A similar tendency is indicated by the results of subjective evaluation by the listeners in this experiment to imagine the case that they themselves are speakers. These results confirm that it is possible to relieve the mental stress experienced by speakers by utilizing a tele-operated robot.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-66",
      "author_area_id": "11",
      "author_area_label": "Spoken Language Processing \u2013 Dialogue, Summarization, Understanding"
    },
    "gravano15_interspeech": {
      "authors": [
        [
          "Agust\u00edn",
          "Gravano"
        ],
        [
          "\u0160tefan",
          "Be\u0148u\u0161"
        ],
        [
          "Rivka",
          "Levitan"
        ],
        [
          "Julia",
          "Hirschberg"
        ]
      ],
      "title": "Backward mimicry and forward influence in prosodic contour choice in standard American English",
      "original": "i15_1839",
      "page_count": 5,
      "order": 71,
      "p1": "1839",
      "pn": "1843",
      "abstract": [
        "Entrainment is the tendency of speakers engaged in conversation to align different aspects of their communicative behavior. In this study we explore in more detail a measure of prosodic entrainment defined in previous work, which uses a discrete parametrization of intonational contours defined by the ToBI conventions for prosodic description. We divide this measure into two asymmetric variants: backward mimicry (in which a speaker uses a contour used previously by the interlocutor) and forward influence (in which a speaker's contour appears later in the speech of the interlocutor). This distinction sheds new light on significant correlations with a number of social variables related to the level of engagement of speakers in a corpus of task-oriented dialogues in Standard American English.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-67",
      "author_area_id": "11",
      "author_area_label": "Spoken Language Processing \u2013 Dialogue, Summarization, Understanding"
    },
    "chowdhury15_interspeech": {
      "authors": [
        [
          "Shammur Absar",
          "Chowdhury"
        ],
        [
          "Morena",
          "Danieli"
        ],
        [
          "Giuseppe",
          "Riccardi"
        ]
      ],
      "title": "The role of speakers and context in classifying competition in overlapping speech",
      "original": "i15_1844",
      "page_count": 5,
      "order": 72,
      "p1": "1844",
      "pn": "1848",
      "abstract": [
        "Overlapping speech is one of the most frequently occurring events in the course of human-human conversations. Understanding the dynamics of overlapping speech is crucial for conversational analysis and for modeling human-machine dialog. Overlapping speech may signal the speaker's intention to grab the floor with a competitive vs non-competitive act. In this paper, we study the role of speakers, whether they initiate ( overlapper) or not ( overlappee) the overlap, and the context of the event. The speech overlap may be explained and predicted by the dialog context, the linguistic or acoustic descriptors. Our goal is to understand whether the competitiveness of the overlap is best predicted by the overlapper, the overlappee, the context or by their combinations. For each overlap and its context we have extracted acoustic, linguistic, and psycholinguistic features and combined decisions from the best classification models. The evaluation of the classifier has been carried out over call center human-human conversations. The results show that the complete knowledge of speakers' role and context highly contribute to the classification results when using acoustic and psycholinguistic features. Our findings also suggest that the lexical selections of the overlapper are good indicators of speaker's competitive or non-competitive intentions.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-68",
      "author_area_id": "11",
      "author_area_label": "Spoken Language Processing \u2013 Dialogue, Summarization, Understanding"
    },
    "christodoulides15_interspeech": {
      "authors": [
        [
          "George",
          "Christodoulides"
        ],
        [
          "Mathieu",
          "Avanzi"
        ]
      ],
      "title": "Automatic detection and annotation of disfluencies in spoken French corpora",
      "original": "i15_1849",
      "page_count": 5,
      "order": 73,
      "p1": "1849",
      "pn": "1853",
      "abstract": [
        "In this paper we propose a multi-step system for the semi-automatic detection and annotation of disfluencies in spoken corpora. A set of rules, statistical models and machine learning techniques are applied to the input, which is a transcription aligned to the speech signal. The system uses the results of an automatic estimation of prosodic, part-of-speech and shallow syntactic features. We present a detailed coding scheme for simple disfluencies (filled pauses, mispronunciations, false starts, drawls and intra-word pauses), structured disfluencies (repetitions, deletions, substitutions, insertions) and complex disfluencies. The system is trained and evaluated on a transcribed corpus of spontaneous French speech, consisting of 112 different speakers and balanced for speaker age and sex, covering 14 different varieties of French spoken in Belgium, France and Switzerland.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-69",
      "author_area_id": "11",
      "author_area_label": "Spoken Language Processing \u2013 Dialogue, Summarization, Understanding"
    },
    "hakkanitur15_interspeech": {
      "authors": [
        [
          "Dilek",
          "Hakkani-T\u00fcr"
        ],
        [
          "Yun-Cheng",
          "Ju"
        ],
        [
          "Geoffrey",
          "Zweig"
        ],
        [
          "Gokhan",
          "Tur"
        ]
      ],
      "title": "Clustering novel intents in a conversational interaction system with semantic parsing",
      "original": "i15_1854",
      "page_count": 5,
      "order": 74,
      "p1": "1854",
      "pn": "1858",
      "abstract": [
        "Spoken language understanding (SLU) in today's conversational systems focuses on recognizing a set of domains, intents, and associated arguments, that are determined by application developers. User requests that are not covered by these are usually directed to search engines, and may remain unhandled. We propose a method that aims to find common user intents amongst these uncovered, out-of-domain utterances, with the goal of supporting future phases of dialog system design. Our approach relies on finding common semantic patterns in uncovered user utterances using an Abstract Meaning Representation based semantic parser. We represent the corpus as a graph and find subgraphs that represent clusters, by pruning the corpus graph according to frequency and entropy. We employ crowd-workers to select and label the resulting clusters and compare resulting clusters with two baselines. Experimental analyses show that we obtain higher coverage and accuracy with the semantic parsing based clustering method. Furthermore, since the intents and candidate slots are already induced, these utterances can also be used in unsupervised SLU modeling. In intent classification experiments, we show that the statistical model trained using the clusters formed by this approach results in higher classification F-measure (showing about 25% relative improvement) in comparison to the alternatives.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-70",
      "author_area_id": "11",
      "author_area_label": "Spoken Language Processing \u2013 Dialogue, Summarization, Understanding"
    },
    "despotovic15_interspeech": {
      "authors": [
        [
          "Vladimir",
          "Despotovic"
        ],
        [
          "Oliver",
          "Walter"
        ],
        [
          "Reinhold",
          "Haeb-Umbach"
        ]
      ],
      "title": "Semantic analysis of spoken input using Markov logic networks",
      "original": "i15_1859",
      "page_count": 5,
      "order": 75,
      "p1": "1859",
      "pn": "1863",
      "abstract": [
        "We present a semantic analysis technique for spoken input using Markov Logic Networks (MLNs). MLNs combine graphical models with first-order logic. They are particularly suitable for providing inference in the presence of inconsistent and incomplete data, which are typical of an automatic speech recognizer's (ASR) output in the presence of degraded speech. The target application is a speech interface to a home automation system to be operated by people with speech impairments, where the ASR output is particularly noisy. In order to cater for dysarthric speech with non-canonical phoneme realizations, acoustic representations of the input speech are learned in an unsupervised fashion. While training data transcripts are not required for the acoustic model training, the MLN training requires supervision, however, at a rather loose and abstract level. Results on two databases, one of them for dysarthric speech, show that MLN-based semantic analysis clearly outperforms baseline approaches employing non-negative matrix factorization, multinomial naive Bayes models, or support vector machines.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-71",
      "author_area_id": "11",
      "author_area_label": "Spoken Language Processing \u2013 Dialogue, Summarization, Understanding"
    },
    "svec15_interspeech": {
      "authors": [
        [
          "Jan",
          "\u0160vec"
        ],
        [
          "Adam",
          "Ch\u00fdlek"
        ],
        [
          "Lubo\u0161",
          "\u0160m\u00eddl"
        ]
      ],
      "title": "Hierarchical discriminative model for spoken language understanding based on convolutional neural network",
      "original": "i15_1864",
      "page_count": 5,
      "order": 76,
      "p1": "1864",
      "pn": "1868",
      "abstract": [
        "This paper presents a novel method for processing automatic speech recognition (ASR) lattices (and generally weighted finite state acceptors) in feed-forward artificial neural networks. It is based on the existing work focused on the text classification using convolutional neural networks (CNNs). The presented method generalizes the convolutional layer of the neural network so that it is able to process both the posterior probabilities and the lexical information contained in an ASR lattice. The convolutional layer was used in a CNN-based implementation of a hierarchical discriminative model (HDM). The method was evaluated using two semantically annotated corpora and the CNN-based HDM improves performance of a spoken language understanding module in comparison with an original HDM based on Support Vector Machines (SVM).\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-72",
      "author_area_id": "11",
      "author_area_label": "Spoken Language Processing \u2013 Dialogue, Summarization, Understanding"
    },
    "chen15b_interspeech": {
      "authors": [
        [
          "Yun-Nung",
          "Chen"
        ],
        [
          "William Yang",
          "Wang"
        ],
        [
          "Alexander I.",
          "Rudnicky"
        ]
      ],
      "title": "Learning semantic hierarchy with distributed representations for unsupervised spoken language understanding",
      "original": "i15_1869",
      "page_count": 5,
      "order": 77,
      "p1": "1869",
      "pn": "1873",
      "abstract": [
        "We study the problem of unsupervised ontology learning for semantic understanding in spoken dialogue systems, in particular, learning the hierarchical semantic structure from the data. Given unlabelled conversations, we augment a frame-semantic based unsupervised slot induction approach with hierarchical agglomerative clustering to merge topically-related slots (e.g., both slots \u201c direction\u201d and \u201c locale\u201d convey location-related information) for building a coherent semantic hierarchy, and then estimate the slot importance at different levels. The high-level semantic estimation involves not only within-slot but also cross-slot relations. The experiments show that high-level semantic information can accurately estimate the prominence of slots, significantly improving the slot induction performance; furthermore, a semantic decoder trained on the data with automatically extracted slots achieves about 68% F-measure, which is close to the one from hand-crafted grammars.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-73",
      "author_area_id": "11",
      "author_area_label": "Spoken Language Processing \u2013 Dialogue, Summarization, Understanding"
    },
    "szekely15_interspeech": {
      "authors": [
        [
          "\u00c9va",
          "Sz\u00e9kely"
        ],
        [
          "Mark T.",
          "Keane"
        ],
        [
          "Julie",
          "Carson-Berndsen"
        ]
      ],
      "title": "The effect of soft, modal and loud voice levels on entrainment in noisy conditions",
      "original": "i15_0150",
      "page_count": 5,
      "order": 78,
      "p1": "150",
      "pn": "154",
      "abstract": [
        "Conversation partners have a tendency to adapt their vocal intensity to each other and to other social and environmental factors. A socially adequate vocal intensity level by a speech synthesiser that goes beyond mere volume adjustment is highly desirable for a rewarding and successful human-machine or machine mediated human-human interaction. This paper examines the interaction of the Lombard effect and speaker entrainment in a controlled experiment conducted with a confederate interlocutor. The interlocutor was asked to maintain either a soft, a modal or a loud voice level during the dialogues. Through half of the trials, subjects were exposed to a cocktail party noise through headphones. The analytical results suggest that both the background noise and the interlocutor's voice level affect the dynamics of speaker entrainment. Speakers appear to still entrain to the voice level of their interlocutor in noisy conditions, though to a lesser extent, as strategies of ensuring intelligibility affect voice levels as well. These findings could be leveraged in spoken dialogue systems and speech generating devices to help choose a vocal effort level for the synthetic voice that is both intelligible and socially suited to a specific interaction.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-74",
      "author_area_id": "1",
      "author_area_label": "Speech Perception, Production, and Acquisition"
    },
    "cowan15_interspeech": {
      "authors": [
        [
          "Benjamin R.",
          "Cowan"
        ],
        [
          "Holly P.",
          "Branigan"
        ]
      ],
      "title": "Does voice anthropomorphism affect lexical alignment in speech-based human-computer dialogue?",
      "original": "i15_0155",
      "page_count": 5,
      "order": 79,
      "p1": "155",
      "pn": "159",
      "abstract": [
        "A common observation in dialogue research is that people tend to entrain, or align, linguistically with their interlocutors. This phenomenon offers a potentially important way to shape user behavior in human-computer dialogue interactions but little is known about the mechanisms that underlie it and how they may be affected by interlocutor design. We report a Wizard of Oz study that explored how voice anthropomorphism impacts lexical alignment in speech-based human-computer dialogue. In a referential communication task, speakers showed a very strong tendency to align lexical choices with their interlocutors, whether human or computer, but this tendency was not affected by voice anthropomorphism. These results highlight the robustness of lexical alignment effects in speech based human-computer dialogues, and suggests that this effect may be impervious to at least some design cues. They also suggest that automatic priming may be an influential mechanism in explaining why we align lexically with automated dialogue partners.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-75",
      "author_area_id": "1",
      "author_area_label": "Speech Perception, Production, and Acquisition"
    },
    "ma15_interspeech": {
      "authors": [
        [
          "Ning",
          "Ma"
        ],
        [
          "Guy J.",
          "Brown"
        ],
        [
          "Jose A.",
          "Gonzalez"
        ]
      ],
      "title": "Exploiting top-down source models to improve binaural localisation of multiple sources in reverberant environments",
      "original": "i15_0160",
      "page_count": 5,
      "order": 80,
      "p1": "160",
      "pn": "164",
      "abstract": [
        "Relatively few systems for machine hearing exploit top-down information in source localisation, despite there being clear evidence for top-down (e.g., attentional) effects in biological spatial hearing. This paper addresses this issue by proposing a framework for binaural sound localisation that exploits top-down knowledge about the source spectral characteristics in the acoustic scene. Information from source models is used to improve the localisation process by selectively weighting binaural cues. The system therefore combines top-down and bottom-up information flow within a single computational framework. Our experiments show that by exploiting source models in this way, sound localisation performance can be improved substantially under challenging conditions in which multiple sources and room reverberation are present.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-76",
      "author_area_id": "1",
      "author_area_label": "Speech Perception, Production, and Acquisition"
    },
    "schymura15_interspeech": {
      "authors": [
        [
          "Christopher",
          "Schymura"
        ],
        [
          "Fiete",
          "Winter"
        ],
        [
          "Dorothea",
          "Kolossa"
        ],
        [
          "Sascha",
          "Spors"
        ]
      ],
      "title": "Binaural sound source localisation and tracking using a dynamic spherical head model",
      "original": "i15_0165",
      "page_count": 5,
      "order": 81,
      "p1": "165",
      "pn": "169",
      "abstract": [
        "This paper introduces a binaural model for the localisation and tracking of a moving sound source's azimuth in the horizontal plane. The model uses a nonlinear state space representation of the sound source dynamics including the current position of the listener's head. The state is estimated via an unscented Kalman Filter by comparing the interaural level and time differences of the binaural signal with semi-analytically derived localisation cues from a spherical head model. The localisation performance of the model is evaluated in combination with two different head movement approaches based on open- and closed-loop control strategies. The results show that adaptive strategies outperform non-adaptive ones and are able to compensate systematic deviations between the spherical head model and human heads.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-77",
      "author_area_id": "1",
      "author_area_label": "Speech Perception, Production, and Acquisition"
    },
    "may15_interspeech": {
      "authors": [
        [
          "Tobias",
          "May"
        ],
        [
          "Thomas",
          "Bentsen"
        ],
        [
          "Torsten",
          "Dau"
        ]
      ],
      "title": "The role of temporal resolution in modulation-based speech segregation",
      "original": "i15_0170",
      "page_count": 5,
      "order": 82,
      "p1": "170",
      "pn": "174",
      "abstract": [
        "This study is concerned with the challenge of automatically segregating a target speech signal from interfering background noise. A computational speech segregation system is presented which exploits logarithmically-scaled amplitude modulation spectrogram (AMS) features to distinguish between speech and noise activity on the basis of individual time-frequency (T-F) units. One important parameter of the segregation system is the window duration of the analysis-synthesis stage, which determines the lower limit of modulation frequencies that can be represented but also the temporal acuity with which the segregation system can manipulate individual T-F units. To clarify the consequences of this trade-off on modulation-based speech segregation performance, the influence of the window duration was systematically investigated.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-78",
      "author_area_id": "1",
      "author_area_label": "Speech Perception, Production, and Acquisition"
    },
    "kayser15_interspeech": {
      "authors": [
        [
          "Hendrik",
          "Kayser"
        ],
        [
          "Constantin",
          "Spille"
        ],
        [
          "Daniel",
          "Marquardt"
        ],
        [
          "Bernd T.",
          "Meyer"
        ]
      ],
      "title": "Improving automatic speech recognition in spatially-aware hearing aids",
      "original": "i15_0175",
      "page_count": 5,
      "order": 83,
      "p1": "175",
      "pn": "179",
      "abstract": [
        "In the context of ambient assisted living, automatic speech recognition (ASR) has the potential to provide textual support for hearing aid users in challenging acoustic conditions. In this paper we therefore investigate possibilities to improve ASR based on binaural hearing aid signals in complex acoustic scenes. Particularly, information about the spatial configuration of sound sources is exploited and estimated using a recently developed method that employs probabilistic information about the location of a target speaker (and a simultaneous localized masker) for robust real-time localization. Two different strategies are investigated: straightforward better-ear listening and a multi-channel beamforming system aiming at enhancement of a target speech source with additional suppression of localized masking sound. The latter method is also complemented by better-ear listening. Both approaches are evaluated in different acoustic scenarios containing moving target and interfering speakers or noise sources. Compared to using non-preprocessed signals, we obtain average relative reductions in word error rate of 28.4% in the presence of a localized interfering noise, 19.2% in the case of a concurrent talker and 23.7% in presence of a concurrent talker in spatially diffuse noise. A post-analysis assesses the relation of localization performance and beamforming for improved speech recognition in complex acoustic scenes.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-79",
      "author_area_id": "1",
      "author_area_label": "Speech Perception, Production, and Acquisition"
    },
    "gomez15_interspeech": {
      "authors": [
        [
          "Randy",
          "Gomez"
        ],
        [
          "Levko",
          "Ivanchuk"
        ],
        [
          "Keisuke",
          "Nakamura"
        ],
        [
          "Takeshi",
          "Mizumoto"
        ],
        [
          "Kazuhiro",
          "Nakadai"
        ]
      ],
      "title": "Dereverberation for active human-robot communication robust to speaker's face orientation",
      "original": "i15_0180",
      "page_count": 5,
      "order": 84,
      "p1": "180",
      "pn": "184",
      "abstract": [
        "Reverberation poses a problem to the active robot audition system. The change in speaker's face orientation relative to the robot perturbs the room acoustics and alters the reverberation condition at runtime, which degrades the automatic speech recognition (ASR) performance. In this paper, we present a method to mitigate this problem in the context of the ASR. First, filter coefficients are derived to correct the Room Transfer Function (RTF) per change in face orientation. We treat the change in the face orientation as a filtering mechanism that captures the room acoustics. Then, joint dynamics between the filter and the observed reverberant speech is investigated in consideration with the ASR system. Second, we introduce a gain correction scheme to compensate the change in power as a function of the face orientation. This scheme is also linked to the ASR, in which gain parameters are derived via the Viterbi algorithm. Experimental results using Hidden Markov Model-Deep Neural Network (HMM-DNN) ASR in a reverberant robot environment, show that proposed method is robust to the change in face orientation and outperforms state-of-the-art dereverberation techniques.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-80"
    },
    "chen15c_interspeech": {
      "authors": [
        [
          "Nanxin",
          "Chen"
        ],
        [
          "Yanmin",
          "Qian"
        ],
        [
          "Kai",
          "Yu"
        ]
      ],
      "title": "Multi-task learning for text-dependent speaker verification",
      "original": "i15_0185",
      "page_count": 5,
      "order": 85,
      "p1": "185",
      "pn": "189",
      "abstract": [
        "Text-dependent speaker verification uses short utterances and verifies both speaker identity and text contents. Due to this nature, traditional state-of-the-art speaker verification approaches, such as i-vector, may not work well. Recently, there has been interest of applying deep learning to speaker verification, however in previous works, standalone deep learning systems have not achieved state-of-the-art performance and they have to be used in system combination or as tandem features to obtain gains. In this paper, a novel multi-task deep learning framework is proposed for text-dependent speaker verification. First, multi-task deep learning is employed to learn both speaker identity and text information. With the learned network, utterance level average of the outputs of the last hidden layer, referred to as j-vector, means joint-vector, is extracted. Discriminant function, with classes defined as multi-task labels on both speaker and text, is then applied to the j-vectors as the decision function for the closed-set recognition, and Probabilistic Linear Discriminant Analysis (PLDA), with classes defined as on the multi-task labels, is applied to the j-vectors for the verification. Experiments on the RSR2015 corpus showed that the j-vector approach leads to good result on the evaluation data. The proposed multi-task deep learning system achieved 0.54% EER, 0.14% EER for the closed-set condition.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-81",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "stafylakis15_interspeech": {
      "authors": [
        [
          "Themos",
          "Stafylakis"
        ],
        [
          "Patrick",
          "Kenny"
        ],
        [
          "Md. Jahangir",
          "Alam"
        ],
        [
          "Marcel",
          "Kockmann"
        ]
      ],
      "title": "JFA for speaker recognition with random digit strings",
      "original": "i15_0190",
      "page_count": 5,
      "order": 86,
      "p1": "190",
      "pn": "194",
      "abstract": [
        "In this paper, we examine the use of Joint Factor Analysis methods on RSR2015 digits. A tied-mixture model is used for segmentation of the utterances into digits, while Joint Factor Analysis and a Joint Density model are deployed for features and backend, respectively. A novel approach for digit-dependent fusion of UBM-component log-likelihood ratios is introduced, yielding the best results so far. The fusion of 5 different JFA features gives an equal-error rate of 3.6%, compared to 6.3% attained by the a baseline GMM-UBM model with score normalization.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-82",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "knyazeva15_interspeech": {
      "authors": [
        [
          "Elena",
          "Knyazeva"
        ],
        [
          "Guillaume",
          "Wisniewski"
        ],
        [
          "Herv\u00e9",
          "Bredin"
        ],
        [
          "Fran\u00e7ois",
          "Yvon"
        ]
      ],
      "title": "Structured prediction for speaker identification in TV series",
      "original": "i15_0195",
      "page_count": 5,
      "order": 87,
      "p1": "195",
      "pn": "199",
      "abstract": [
        "Though radio and TV broadcast are highly structured documents, state-of-the-art speaker identification algorithms do not take advantage of this information to improve prediction performance: speech turns are usually identified independently from each other, using unstructured multi-class classification approaches. In this work, we propose to address speaker identification as a sequence labeling task and use two structured prediction techniques to account for the inherent temporal structure of interactions between speakers: the first one relies on Conditional Random Field and can take into account local relations between two consecutive speech turns; the second one, based on the Searn framework, sacrifices exact inference for the sake of the expressiveness of the model and is able to incorporate rich structure information during prediction. Experiments performed on The Big Bang Theory TV series show that structured prediction techniques outperform the standard unstructured approach.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-83",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "cumani15_interspeech": {
      "authors": [
        [
          "Sandro",
          "Cumani"
        ],
        [
          "Pietro",
          "Laface"
        ],
        [
          "Farzana",
          "Kulsoom"
        ]
      ],
      "title": "Speaker recognition by means of acoustic and phonetically informed GMMs",
      "original": "i15_0200",
      "page_count": 5,
      "order": 88,
      "p1": "200",
      "pn": "204",
      "abstract": [
        "In this work we assess the recently proposed hybrid Deep Neural Network/Gaussian Mixture Model (DNN/GMM) approach for speaker recognition considering the effects of the granularity of the phonetic DNN model, and of the precision of the corresponding GMM models, which will be referred to as the phonetic GMMs. The aim of this work is to better understand the contributions of the phonetic information provided by the DNN model with respect to the accuracy of the acoustic GMMs in fitting the distribution of the features associated to a given context-dependent phone state. The testbed for this work was the text-independent speaker recognition task defined by NIST for the 2012 Speaker Recognition Evaluation. Our experiment confirms that the acoustic and the phonetic GMMs are complementary. Thus, their score combination yields very good results if the DNN is trained on data collected in an environment similar to the one that is used for testing. We show, however, that using a single Gaussian per DNN state is not the best choice: the best single system has been obtained balancing the phonetic and acoustic precision of a DNN/GMM system.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-84",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "panda15_interspeech": {
      "authors": [
        [
          "Ashish",
          "Panda"
        ]
      ],
      "title": "A fast approach to psychoacoustic model compensation for robust speaker recognition in additive noise",
      "original": "i15_0205",
      "page_count": 5,
      "order": 89,
      "p1": "205",
      "pn": "209",
      "abstract": [
        "This paper addresses the problem of speaker verification in the presence of additive noise. We propose a fast implementation of Psychoacoustic Model Compensation (Psy-Comp) scheme for static features along with model domain mean and variance normalization for robust speaker recognition in noisy conditions. The proposed algorithms are validated through experiments on noise corrupted NIST-2000 speaker recognition database. We show that the Psy-Comp scheme along with model domain mean and variance normalization provide significant performance gain compared to the Vector Taylor Series (VTS) scheme and feature domain cepstral mean and variance normalization scheme. Moreover, the computational cost of the proposed method is significantly less than the VTS scheme.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-85",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "doroshin15_interspeech": {
      "authors": [
        [
          "Danila",
          "Doroshin"
        ],
        [
          "Nikolay",
          "Lubimov"
        ],
        [
          "Marina",
          "Nastasenko"
        ],
        [
          "Mikhail",
          "Kotov"
        ]
      ],
      "title": "Blind score normalization method for PLDA based speaker recognition",
      "original": "i15_0210",
      "page_count": 4,
      "order": 90,
      "p1": "210",
      "pn": "213",
      "abstract": [
        "Probabilistic Linear Discriminant Analysis (PLDA) has become state-of-the-art method for modeling i-vector space in speaker recognition task. However the performance degradation is observed if enrollment data size differs from one speaker to another. This paper presents a solution to such problem by introducing new PLDA scoring normalization technique. Normalization parameters are derived in a blind way, so that, unlike traditional   ZT-norm, no extra development data is required. Moreover, proposed method has shown to be optimal in terms of detection cost function. The experiments conducted on NIST SRE 2014 database demonstrate an improved accuracy in a mixed enrollment number condition.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-86",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "novoselov15_interspeech": {
      "authors": [
        [
          "Sergey",
          "Novoselov"
        ],
        [
          "Timur",
          "Pekhovsky"
        ],
        [
          "Oleg",
          "Kudashev"
        ],
        [
          "Valentin S.",
          "Mendelev"
        ],
        [
          "Alexey",
          "Prudnikov"
        ]
      ],
      "title": "Non-linear PLDA for i-vector speaker verification",
      "original": "i15_0214",
      "page_count": 5,
      "order": 91,
      "p1": "214",
      "pn": "218",
      "abstract": [
        "Two approaches are presented for non-linear PLDA to be used in speaker verification. In NIST 2010 speaker recognition evaluation (SRE) tests under DET-5 conditions, the two methods and particularly their combination provided significant improvements in equal error rates and minDCF values over a standard PLDA scheme. The proposed schemes were also applied within a speaker verification system that employs DNN-based sufficient statistics calculation resulting in a 45% reduction in minDCF relative to a conventional GMM based system.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-87",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "vaquero15_interspeech": {
      "authors": [
        [
          "Carlos",
          "Vaquero"
        ],
        [
          "Patricia",
          "Rodr\u00edguez"
        ]
      ],
      "title": "On the need of template protection for voice authentication",
      "original": "i15_0219",
      "page_count": 5,
      "order": 92,
      "p1": "219",
      "pn": "223",
      "abstract": [
        "In this work we study the need of template protection to provide security and privacy in text-dependent pass-phrase voice authentication systems. For this purpose, we analyze the robustness of two state-of-the-art speaker verification systems against attacks performed using input data generated from a compromised voice template. This analysis shows that compromised templates can be used to gain unauthorized access to authentication systems, when these systems use the same speaker verification technology, background models and pass-phrase as the one from which the compromised template was stolen. However we also show that the compromised template may not be helpful to attack an authentication system which uses a speaker verification technology or pass-phrase different from those considered in the system the template was obtained from. This fact facilitates the fulfillment of the main requirements that a protected template should meet to guarantee user privacy: irreversibility and unlinkability.   Finally we propose a set of guidelines for the design of voice authentication systems that enable the preservation of user privacy and provide revocability measures in case a template is compromised.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-88",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "kelly15_interspeech": {
      "authors": [
        [
          "Finnian",
          "Kelly"
        ],
        [
          "John H. L.",
          "Hansen"
        ]
      ],
      "title": "Evaluation and calibration of short-term aging effects in speaker verification",
      "original": "i15_0224",
      "page_count": 5,
      "order": 93,
      "p1": "224",
      "pn": "228",
      "abstract": [
        "A speaker verification evaluation is presented on the Multi-session Audio Research Project (MARP) corpus, for which speakers were recorded at regular intervals, in consistent conditions, over a period of three years. It is observed that the performance of an i-vector system with probabilistic linear discriminant analysis (PLDA) modelling decreases progressively, in terms of both discrimination and calibration, as the time intervals between train and test sessions increase. For male speakers, the equal error rate (EER) increases from 2.4% to 4.4% when the interval between sessions grows from several months to three years. An extension to conventional linear score calibration is proposed, whereby short-term aging information is incorporated as an additional factor in the score transformation. This new approach improves discrimination and calibration performance in the presence of increasing time intervals between train and test sessions, compared with score-only calibration.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-89",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "chen15d_interspeech": {
      "authors": [
        [
          "Liping",
          "Chen"
        ],
        [
          "Kong Aik",
          "Lee"
        ],
        [
          "Bin",
          "Ma"
        ],
        [
          "Wu",
          "Guo"
        ],
        [
          "Haizhou",
          "Li"
        ],
        [
          "Li-Rong",
          "Dai"
        ]
      ],
      "title": "Phone-centric local variability vector for text-constrained speaker verification",
      "original": "i15_0229",
      "page_count": 5,
      "order": 94,
      "p1": "229",
      "pn": "233",
      "abstract": [
        "This paper investigates the use of frame alignment given by a deep neural network (DNN) for text-constrained speaker verification task, where the lexical contents of the test utterances are limited to a finite set of vocabulary. The DNN makes use of information carried by the target and its contextual frames to assign it probabilistically to one of the phonetic states. The frame alignment is therefore more precise and less ambiguous than that generated by a Gaussian mixture model (GMM). Using the DNN alignment, we show that an i-vector can be decomposed into segments of local variability vectors, each corresponding to a monophone, where each local vector models session variability given the phonetic context. Based on the local vectors, the content matching between the utterances for comparison can be accomplished in the PLDA scoring. Experiments conducted on the RSR2015 database shows that the proposed phone-centric local variability vector achieves a better performance compared to the i-vector.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-90",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "george15_interspeech": {
      "authors": [
        [
          "Kuruvachan K.",
          "George"
        ],
        [
          "C. Santhosh",
          "Kumar"
        ],
        [
          "K I",
          "Ramachandran"
        ],
        [
          "Ashish",
          "Panda"
        ]
      ],
      "title": "Cosine distance features for robust speaker verification",
      "original": "i15_0234",
      "page_count": 5,
      "order": 95,
      "p1": "234",
      "pn": "238",
      "abstract": [
        "We use similarities with people we know already as a means to enhance the speaker verification accuracy. Motivated by this, we use cosine distance similarities with a set of reference speakers, cosine distance features (CDF), to improve the performance of speaker verification systems for clean and additive noise test conditions. We used mel frequency cepstral coefficients, power normalized cepstral coefficients, or delta spectral cepstral coefficients for deriving CDF. We then input CDF to a support vector machine (SVM) backend classifier (CDF-SVM). The performance of CDF-SVM was then compared with an i-vector with cosine distance scoring (i-CDS), and an i-vector with a backend SVM classifier (i-SVM) for stationary and non-stationary noises at different signal to noise ratio (SNR) levels. The experimental results show that, the CDF-SVM outperforms all other systems at high SNR and clean environments. However, in certain low SNR cases, i-CDS was found to be better. Finally, we fused the CDF-SVM with i-CDS and results show that the noise robustness of the combined system is significantly better than the individual systems for both high and low SNR levels.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-91",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "shiota15_interspeech": {
      "authors": [
        [
          "Sayaka",
          "Shiota"
        ],
        [
          "Fernando",
          "Villavicencio"
        ],
        [
          "Junichi",
          "Yamagishi"
        ],
        [
          "Nobutaka",
          "Ono"
        ],
        [
          "Isao",
          "Echizen"
        ],
        [
          "Tomoko",
          "Matsui"
        ]
      ],
      "title": "Voice liveness detection algorithms based on pop noise caused by human breath for automatic speaker verification",
      "original": "i15_0239",
      "page_count": 5,
      "order": 96,
      "p1": "239",
      "pn": "243",
      "abstract": [
        "This paper proposes a novel countermeasure framework to detect spoofing attacks to reduce the vulnerability of automatic speaker verification (ASV) systems. Recently, ASV systems have reached equivalent performances equivalent to those of other biometric modalities. However, spoofing techniques against these systems have also progressed drastically. Experimentation using advanced speech synthesis and voice conversion techniques has showed unacceptable false acceptance rates and several new countermeasure algorithms have been explored to detect spoofing materials accurately. However, the countermeasures proposed so far are based on the acoustic differences between natural speech signals and artificial speech signals, expected to become gradually smaller in the near future. In this paper, we focus on voice liveness detection, which aims to validate whether the presented speech signals originated from a live human. We use the phenomenon of pop noise, which is a distortion that happens when human breath reaches a microphone, as liveness evidence. This paper proposes pop noise detection algorithms and shows through an experimental study that they can be used to discriminate live voice signals from artificial ones generated by means of speech synthesis techniques.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-92",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "hurmalainen15_interspeech": {
      "authors": [
        [
          "Antti",
          "Hurmalainen"
        ],
        [
          "Rahim",
          "Saeidi"
        ],
        [
          "Tuomas",
          "Virtanen"
        ]
      ],
      "title": "Noise robust speaker recognition with convolutive sparse coding",
      "original": "i15_0244",
      "page_count": 5,
      "order": 97,
      "p1": "244",
      "pn": "248",
      "abstract": [
        "Recognition and classification of speech content in everyday environments is challenging due to the large diversity of real-world noise sources, which may also include competing speech. At signal-to-noise ratios below 0 dB, a majority of features may become corrupted, severely degrading the performance of classifiers built upon clean observations of a target class. As the energy and complexity of competing sources increase, their explicit modelling becomes integral for successful detection and classification of target speech. We have previously demonstrated how non-negative compositional modelling in a spectrogram space is suitable for robust recognition of speech and speakers even at low SNRs. In this work, the sparse coding approach is extended to cover the whole separation and classification chain to recognise the speaker of short utterances in difficult noise environments. A convolutive matrix factorisation and coding system is evaluated on 2nd CHiME Track 1 data. Over 98% average speaker recognition accuracy is achieved for shorter than three second utterances at +9 \u2026 -6 dB SNR, illustrating the system's performance in challenging conditions.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-93",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "alam15_interspeech": {
      "authors": [
        [
          "Md. Jahangir",
          "Alam"
        ],
        [
          "Patrick",
          "Kenny"
        ],
        [
          "Themos",
          "Stafylakis"
        ]
      ],
      "title": "Combining amplitude and phase-based features for speaker verification with short duration utterances",
      "original": "i15_0249",
      "page_count": 5,
      "order": 98,
      "p1": "249",
      "pn": "253",
      "abstract": [
        "Due to the increasing use of fusion in speaker recognition systems, one trend of current research activity focuses on new features that capture complementary information to the MFCC (Mel-frequency cepstral coefficients) for improving speaker recognition performance. The goal of this work is to combine (or fuse) amplitude and phase-based features to improve speaker verification performance. Based on the amplitude and phase spectra we investigate some possible variations to the extraction of cepstral coefficients that produce diversity with respect to fused subsystems. Among the amplitude-based features we consider widely used MFCC, Linear frequency cepstral coefficients, and multitaper spectrum estimation-based MFCC (denoted here as MMFCC). To compute phase-based features we choose modified group delay- and all-pole group delay-, linear prediction residual phase-based features. We also consider product spectrum-based cepstral coefficients features that are influenced by both the amplitude and phase spectra. For performance evaluation, text-dependent speaker verification experiments are conducted on the a proprietary dataset known as Voice Trust-Pakistan (VT-Pakistan) corpus. Experimental results show that the fused system provide reduced error rate compared to both the amplitude and phase-based features. On the average fused system provided a relative improvement of 37% over the baseline MFCC systems in terms of EER, DCF (detection cost function) of SRE 2008 and DCF of SRE 2010.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-94",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "lee15_interspeech": {
      "authors": [
        [
          "Kong Aik",
          "Lee"
        ],
        [
          "Anthony",
          "Larcher"
        ],
        [
          "Guangsen",
          "Wang"
        ],
        [
          "Patrick",
          "Kenny"
        ],
        [
          "Niko",
          "Br\u00fcmmer"
        ],
        [
          "David van",
          "Leeuwen"
        ],
        [
          "Hagai",
          "Aronowitz"
        ],
        [
          "Marcel",
          "Kockmann"
        ],
        [
          "Carlos",
          "Vaquero"
        ],
        [
          "Bin",
          "Ma"
        ],
        [
          "Haizhou",
          "Li"
        ],
        [
          "Themos",
          "Stafylakis"
        ],
        [
          "Md. Jahangir",
          "Alam"
        ],
        [
          "Albert",
          "Swart"
        ],
        [
          "Javier",
          "Perez"
        ]
      ],
      "title": "The reddots data collection for speaker recognition",
      "original": "i15_2996",
      "page_count": 5,
      "order": 99,
      "p1": "2996",
      "pn": "3000",
      "abstract": [
        "This paper describes data collection efforts conducted as part of the RedDots project which is dedicated to the study of speaker recognition under conditions where test utterances are of short duration and of   variable phonetic content. At the current stage, we focus on English speakers, both native and non-native, recruited worldwide. This is made possible through the use of a recording front-end consisting of an application running on mobile devices communicating with a centralized web server at the back-end. Speech recordings are collected by having speakers read text prompts displayed on the screen of the mobile devices. We aim to collect a large number of sessions from each speaker over a long time span, typically one session per week over a one year period. The corpus is expected to include rich inter-speaker and intra-speaker variations, both intrinsic and extrinsic (that is, due to recording channel and acoustic environment).\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-95",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "he15_interspeech": {
      "authors": [
        [
          "Yongjun",
          "He"
        ],
        [
          "Chen",
          "Chen"
        ],
        [
          "Jiqing",
          "Han"
        ]
      ],
      "title": "Noise-robust speaker recognition based on morphological component analysis",
      "original": "i15_3001",
      "page_count": 5,
      "order": 100,
      "p1": "3001",
      "pn": "3005",
      "abstract": [
        "Speaker recognition suffers severe performance degradation under noisy environments. To solve this problem, we propose a novel method based on morphological component analysis. This method employs a universal background dictionary (UBD) to model common variability of all speakers, a speech dictionary of each speaker to model special variability of this speaker and a noise dictionary to model variability of environmental noise. These three dictionaries are concatenated to be a big dictionary, over which test speech is sparsely represented and classified. To improve the discriminability of speaker dictionaries, we optimize the speaker dictionaries by removing speaker atoms which are close to the UBD atoms. To ensure varying noises can be tracked, we design an algorithm to update the noise dictionary with the noisy speech. We finally conduct experiments under various noise conditions and the results show that the proposed method can obviously improve the robustness of speaker recognition under noisy environments.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-96",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "nautsch15_interspeech": {
      "authors": [
        [
          "Andreas",
          "Nautsch"
        ],
        [
          "Rahim",
          "Saeidi"
        ],
        [
          "Christian",
          "Rathgeb"
        ],
        [
          "Christoph",
          "Busch"
        ]
      ],
      "title": "Analysis of mutual duration and noise effects in speaker recognition: benefits of condition-matched cohort selection in score normalization",
      "original": "i15_3006",
      "page_count": 5,
      "order": 101,
      "p1": "3006",
      "pn": "3010",
      "abstract": [
        "The biometric and forensic performance of automatic speaker recognition systems degrades under noisy and short probe utterance conditions. Score normalization is an effective tool taking into account the mismatch of reference and probe utterances. In an adaptive symmetric score normalization scheme for state-of-the-art i-vector recognition systems, a set of cohort speakers are employed to calculate the mean and variance of impostor scores when compared to reference and probe i-vectors. In dealing with real-life conditions where the quality of audio recordings in test phase does not match enrolment utterance(s) of speakers, we demonstrate the effectiveness of utilizing a condition-matched cohort set for score normalization. The cohort set audio material is shortened and degraded by noise in different reasonable and controlled signal-to-noise ratios according to expected test conditions, yielding in multiple set of cohorts. Further, we propose automatic cohort pre-selection based on modeling each degradation category. For each i-vector, a quality vector is assigned as the posterior probability of degradation classes. The cohort set is then formed by i-vectors representing small KL-divergence of respective quality vectors when compared to reference and probe. Further gains are observed by including this quality vector also into the score calibration.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-97",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "fredes15_interspeech": {
      "authors": [
        [
          "Josu\u00e9",
          "Fredes"
        ],
        [
          "Jos\u00e9",
          "Novoa"
        ],
        [
          "Victor",
          "Poblete"
        ],
        [
          "Simon",
          "King"
        ],
        [
          "Richard M.",
          "Stern"
        ],
        [
          "N\u00e9stor Becerra",
          "Yoma"
        ]
      ],
      "title": "Robustness to additive noise of locally-normalized cepstral coefficients in speaker verification",
      "original": "i15_3011",
      "page_count": 5,
      "order": 102,
      "p1": "3011",
      "pn": "3015",
      "abstract": [
        "In this paper the performance of a new feature set, Locally Normalized Cepstral Coefficients (LNCC) is evaluated for a speaker verification task with short testing utterances in additive noise. The results presented here show that LNCC outperforms baseline MFCC features when SNR is lower than 15 dB. The average relative reduction in EER achieved by LNCC is 33%. The use of LNCC in combination with spectral subtraction provides a reduction in EER averaging 18% when compared to MFCC features also with spectral subtraction. In addition, sub-band LNCC is proposed to improve the estimation of noise energy and hence the effectiveness of spectral subtraction. When compared with MFCC features, the use of sub-band LNCC led to greater reductions in EER than LNCC with non-stationary noise.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-98",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "shokouhi15_interspeech": {
      "authors": [
        [
          "Navid",
          "Shokouhi"
        ],
        [
          "John H. L.",
          "Hansen"
        ]
      ],
      "title": "Probabilistic linear discriminant analysis for robust speaker identification in co-channel speech",
      "original": "i15_3016",
      "page_count": 5,
      "order": 103,
      "p1": "3016",
      "pn": "3020",
      "abstract": [
        "Co-channel speech refers to a monophonic audio recording in which at least two speakers are present. Meeting and telephone conversations recorded on a single channel are examples of co-channel speech. In this study, we address the problem of speaker identification (SID) for trials that contain co-channel speech in the train and/or test sessions. The assumption here is that there is access to i-vectors for all the recordings and we would like to compensate for interfering speech without requiring any changes or enhancements on the audio. This is an attractive approach, since state-of-the-art SID systems are developed on i-vectors and thereby solutions that do not require alterations in the i-vector extraction stage are more convenient. We propose modifications to the standard PLDA formulation that enables extracting more accurate estimates of the eigenvoice matrix in the presence of interfering speech and consequently more accurate statistics for speaker dependent latent variables. The proposed co-channel PLDA formulation results in 30% relative drop in equal error rate when compared to the standard PLDA system for co-channel sessions with signal-to-interference ratios as low as 0dB.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-99",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "wang15_interspeech": {
      "authors": [
        [
          "Hongcui",
          "Wang"
        ],
        [
          "Di",
          "Jin"
        ],
        [
          "Lantian",
          "Li"
        ],
        [
          "Jianwu",
          "Dang"
        ]
      ],
      "title": "Community detection with manifold learning on speaker i-vector space for Chinese",
      "original": "i15_3021",
      "page_count": 5,
      "order": 104,
      "p1": "3021",
      "pn": "3025",
      "abstract": [
        "Speaker recognition with clustering speech signals of the same speaker is an important speech analysis task in various applications. Recent works have shown that there was an underlying manifold on which speaker utterances live in the model-parameter space. However, most speaker clustering methods work on the Euclidean space, and hence often fail to discover the intrinsic geometrical structure of the data space. For this problem, we consider to convert the speaker i-vector representation of utterances in the Euclidean space into a network structure constructed based on the local (k) nearest neighbor relationship of these signals. We then propose a community detection model on the network for clustering signals. The new model is based on the probabilistic community memberships, and is further refined with the idea that: if two connected nodes have a high similarity, their community membership distributions in the model should be made close. This refinement enhances the local invariance assumption, and thus better respects the structure of the underlying manifold than the existing community detection methods. Some experiments are conducted on speaker content network built from a Chinese speaker recognition database. The results confirmed the effectiveness of this new method.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-100",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "yella15_interspeech": {
      "authors": [
        [
          "Sree Harsha",
          "Yella"
        ],
        [
          "Andreas",
          "Stolcke"
        ]
      ],
      "title": "A comparison of neural network feature transforms for speaker diarization",
      "original": "i15_3026",
      "page_count": 5,
      "order": 105,
      "p1": "3026",
      "pn": "3030",
      "abstract": [
        "Speaker diarization finds contiguous speaker segments in an audio stream and clusters them by speaker identity, without using a-priori knowledge about the number of speakers or enrollment data. Diarization typically clusters speech segments based on short-term spectral features. In prior work, we showed that neural networks can serve as discriminative feature transformers for diarization by training them to perform same/different speaker comparisons on speech segments, yielding improved diarization accuracy when combined with standard MFCC-based models. In this work, we explore a wider range of neural network architectures for feature transformation, by adding additional layers and nonlinearities, and by varying the objective function during training. We find that the original speaker comparison network can be improved by adding a nonlinear transform layer, and that further gains are possible by training the network to perform speaker classification rather than comparison. Overall we achieve relative reductions in speaker error between 18% and 34% on a variety of test data from the AMI, ICSI, and NIST-RT corpora.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-101",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "shapiro15_interspeech": {
      "authors": [
        [
          "Ilya",
          "Shapiro"
        ],
        [
          "Neta",
          "Rabin"
        ],
        [
          "Irit",
          "Opher"
        ],
        [
          "Itshak",
          "Lapidot"
        ]
      ],
      "title": "Clustering short push-to-talk segments",
      "original": "i15_3031",
      "page_count": 5,
      "order": 106,
      "p1": "3031",
      "pn": "3035",
      "abstract": [
        "We present a method for clustering short push-to-talk speech segments in the presence of different numbers of speakers. Iterative Mean Shift algorithm based on the cosine distance is used to perform speaker clustering on i-vectors generated from many short speech segments. We report results as measured by the Accuracy, the average number of detected speakers (ANDS), the average cluster purity (ACP), the average speaker purity (ASP) and K . We achieve clustering accuracy of: 90.0%, 86.9% and 72.1% for 3, 15 and 60 speakers respectively.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-102",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "fedorova15_interspeech": {
      "authors": [
        [
          "Anna",
          "Fedorova"
        ],
        [
          "Ond\u0159ej",
          "Glembek"
        ],
        [
          "Tomi",
          "Kinnunen"
        ],
        [
          "Pavel",
          "Mat\u011bjka"
        ]
      ],
      "title": "Exploring ANN back-ends for i-vector based speaker age estimation",
      "original": "i15_3036",
      "page_count": 5,
      "order": 107,
      "p1": "3036",
      "pn": "3040",
      "abstract": [
        "We address the problem of speaker age estimation using i-vectors. We first compare different i-vector extraction setups and then focus on (shallow) artificial neural net (ANN) back-ends. We explore ANN architecture, training algorithm and ANN ensembles. The results on NIST 2008 and 2010 SRE data indicate that, after extensive parameter optimization, ANN back-end in combination with i-vectors reaches mean absolute errors (MAEs) of 5.49 (females) and 6.35 (males), which are 4.5% relative improvement in comparison to our support-vector regression (SVR) baseline. Hence, the choice of back-end did not affect the accuracy much; a suggested future direction is therefore focusing more on front-end processing.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-103",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "banse15_interspeech": {
      "authors": [
        [
          "D\u00e9sir\u00e9",
          "Bans\u00e9"
        ],
        [
          "George R.",
          "Doddington"
        ],
        [
          "Daniel",
          "Garcia-Romero"
        ],
        [
          "John J.",
          "Godfrey"
        ],
        [
          "Craig S.",
          "Greenberg"
        ],
        [
          "Jaime",
          "Hern\u00e1ndez-Cordero"
        ],
        [
          "John M.",
          "Howard"
        ],
        [
          "Alvin F.",
          "Martin"
        ],
        [
          "Lisa P.",
          "Mason"
        ],
        [
          "Alan",
          "McCree"
        ],
        [
          "Douglas A.",
          "Reynolds"
        ]
      ],
      "title": "Analysis of the second phase of the 2013-2014 i-vector machine learning challenge",
      "original": "i15_3041",
      "page_count": 5,
      "order": 108,
      "p1": "3041",
      "pn": "3045",
      "abstract": [
        "In late 2013 and 2014, the National Institute of Standards and Technology (NIST) coordinated an i-vector challenge utilizing data from previous NIST Speaker Recognition Evaluations. Following the evaluation period, a second phase of the challenge was held, where speaker labels were made available for system development. The second phase included system submissions from 23 participants representing 13 different countries, of which 18 also participated in the first phase of the challenge. The top 10 systems participating in both of the challenge phases demonstrated an average relative improvement of approximately 26% between the first and second phases, which represents the value of having access to the speaker labels. The top five participants submitted a system that outperformed the oracle system from the first phase on the evaluation data.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-104",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "martin15_interspeech": {
      "authors": [
        [
          "Alvin F.",
          "Martin"
        ],
        [
          "Craig S.",
          "Greenberg"
        ],
        [
          "John M.",
          "Howard"
        ],
        [
          "D\u00e9sir\u00e9",
          "Bans\u00e9"
        ],
        [
          "George R.",
          "Doddington"
        ],
        [
          "Jaime",
          "Hern\u00e1ndez-Cordero"
        ],
        [
          "Lisa P.",
          "Mason"
        ]
      ],
      "title": "NIST language recognition evaluation \u2014 plans for 2015",
      "original": "i15_3046",
      "page_count": 5,
      "order": 109,
      "p1": "3046",
      "pn": "3050",
      "abstract": [
        "We discuss two NIST coordinated evaluations of automatic language recognition technology planned for calendar year 2015 along with possible additional plans for the future. The first is the Language Recognition i-Vector Machine Learning Challenge, largely modeled on the 2013-2014 Speaker Recognition i-Vector Machine Learning Challenge. This online challenge, emphasizing the language identification task, is particularly intended to attract interest from the machine learning community and others beyond the audio processing community. The second is the next NIST Language Recognition Evaluation, following in the series of NIST evaluations previously held in 1996, 2003, 2005, 2007, 2009, and 2011. This evaluation will emphasize language detection in the context of closely related language pairs and is open to all interested in participating.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-105",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "desplanques15_interspeech": {
      "authors": [
        [
          "Brecht",
          "Desplanques"
        ],
        [
          "Kris",
          "Demuynck"
        ],
        [
          "Jean-Pierre",
          "Martens"
        ]
      ],
      "title": "Factor analysis for speaker segmentation and improved speaker diarization",
      "original": "i15_3081",
      "page_count": 5,
      "order": 110,
      "p1": "3081",
      "pn": "3085",
      "abstract": [
        "Speaker diarization includes two steps: speaker segmentation and speaker clustering. Speaker segmentation searches for speaker boundaries, whereas speaker clustering aims at grouping speech segments of the same speaker. In this work, the segmentation is improved by replacing the Bayesian Information Criterion (BIC) with a new iVector-based approach. Unlike BIC-based methods which trigger on any acoustic dissimilarities, the proposed method suppresses phonetic variations and accentuates speaker differences. More specifically our method generates boundaries based on the distance between two speaker factor vectors that are extracted on a frame-by-frame basis. The extraction relies on an eigenvoice matrix so that large differences between speaker factor vectors indicate a different speaker. A Mahalanobis-based distance measure, in which the covariance matrix compensates for the remaining and detrimental phonetic variability, is shown to generate accurate boundaries. The detected segments are clustered by a state-of-the-art iVector Probabilistic Linear Discriminant Analysis system. Experiments on the COST278 multilingual broadcast news database show relative reductions of 50% in boundary detection errors. The speaker error rate is reduced by 8% relative.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-106",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "inoue15_interspeech": {
      "authors": [
        [
          "Koji",
          "Inoue"
        ],
        [
          "Yukoh",
          "Wakabayashi"
        ],
        [
          "Hiromasa",
          "Yoshimoto"
        ],
        [
          "Katsuya",
          "Takanashi"
        ],
        [
          "Tatsuya",
          "Kawahara"
        ]
      ],
      "title": "Enhanced speaker diarization with detection of backchannels using eye-gaze information in poster conversations",
      "original": "i15_3086",
      "page_count": 5,
      "order": 111,
      "p1": "3086",
      "pn": "3090",
      "abstract": [
        "We propose multi-modal speaker diarization using acoustic and eye-gaze information in poster conversations. Eye-gaze information plays an important role in turn-taking, thus it is useful for predicting speech activity. In this paper, a variety of eye-gaze features are elaborated and combined with the acoustic information by the multi-modal integration model. Moreover, we introduce another model to detect backchannels, which involve different eye-gaze behaviors. This enhances the diarization result by filtering meaningful utterances such as questions and comments. Experimental evaluations in real poster sessions demonstrate that eye-gaze information contributes to improvement of diarization accuracy under noisy environments, and its weight is automatically determined according to the Signal-to-Noise Ratio (SNR).\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-107",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "delgado15_interspeech": {
      "authors": [
        [
          "H\u00e9ctor",
          "Delgado"
        ],
        [
          "Xavier",
          "Anguera"
        ],
        [
          "Corinne",
          "Fredouille"
        ],
        [
          "Javier",
          "Serrano"
        ]
      ],
      "title": "Novel clustering selection criterion for fast binary key speaker diarization",
      "original": "i15_3091",
      "page_count": 5,
      "order": 112,
      "p1": "3091",
      "pn": "3095",
      "abstract": [
        "Speaker diarization has become an important building block in many speech-related systems. Given the great increase of audiovisual media, fast systems are required in order to process large amounts of data in a reasonable time. In this regard, the recently proposed speaker diarization system based on binary key speaker modeling provides a very fast alternative to state-of-the-art systems at the cost of a slight decrease in performance. This decrease is mainly due to drawbacks in the final clustering selection algorithm, which is far from returning the optimum clustering the system is actually able to generate. At the same time, we have identified potential points of our system which can be further sped up. This paper aims to face these two issues by first lightening the processing at the main identified bottleneck, and second by proposing an alternative clustering selection technique capable of providing near-optimum clustering outputs. Experimental results on the REPERE test database validate the effectiveness of the proposed improvements, obtaining a relative performance gain of 20% and execution times of 0.037 xRT (being xRT the Real-Time factor).\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-108",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "sell15_interspeech": {
      "authors": [
        [
          "Gregory",
          "Sell"
        ],
        [
          "Daniel",
          "Garcia-Romero"
        ],
        [
          "Alan",
          "McCree"
        ]
      ],
      "title": "Speaker diarization with i-vectors from DNN senone posteriors",
      "original": "i15_3096",
      "page_count": 4,
      "order": 113,
      "p1": "3096",
      "pn": "3099",
      "abstract": [
        "Motivated by recent gains in speaker identification by incorporating senone posteriors from deep neural networks (DNNs) into i-vector extraction, we examine similar enhancements to speaker diarization with i-vector clustering. We examine two DNNs with different numbers of senone targets in combination with a diagonal or full covariance universal background model (UBM) in the context of the multilingual corpus CALLHOME. Results show that the larger DNN with a full covariance UBM gives the best performance. The improvements appear to have a strong dependence on number of speakers in a conversation, and a lesser dependence on language. Overall, when combined with resegmentation, the proposed system improves CALLHOME performance to 10.3% DER.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-109",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "woubie15_interspeech": {
      "authors": [
        [
          "Abraham",
          "Woubie"
        ],
        [
          "Jordi",
          "Luque"
        ],
        [
          "Javier",
          "Hernando"
        ]
      ],
      "title": "Using voice-quality measurements with prosodic and spectral features for speaker diarization",
      "original": "i15_3100",
      "page_count": 5,
      "order": 114,
      "p1": "3100",
      "pn": "3104",
      "abstract": [
        "Jitter and shimmer voice-quality measurements have been successfully used to detect voice pathologies and classify different speaking styles. In this paper, we investigate the usefulness of jitter and shimmer voice measurements in the framework of the speaker diarization task. The combination of jitter and shimmer voice-quality features with the long-term prosodic and short-term spectral features is explored in a subset of the Augmented Multi-party Interaction (AMI) corpus, a multi-party and spontaneous speech set of recordings. The best results have been obtained by fusing the voice-quality features with the prosodic ones at the feature level, and then fusing them with the spectral features at the score level. Experimental results show more than 20% relative DER improvement compared to the spectral baseline system.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-110",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "madikeri15_interspeech": {
      "authors": [
        [
          "Srikanth",
          "Madikeri"
        ],
        [
          "Ivan",
          "Himawan"
        ],
        [
          "Petr",
          "Motlicek"
        ],
        [
          "Marc",
          "Ferras"
        ]
      ],
      "title": "Integrating online i-vector extractor with information bottleneck based speaker diarization system",
      "original": "i15_3105",
      "page_count": 5,
      "order": 115,
      "p1": "3105",
      "pn": "3109",
      "abstract": [
        "Conventional approaches to speaker diarization use short-term features such asMel Frequency Cepstral Co-efficients (MFCC). Features such as i-vectors have been used on longer segments (minimum 2.5 seconds of speech). Using i-vectors for speaker diarization has been shown to be beneficial as it models speaker information explicitly. In this paper, the i-vector modelling technique is adapted to be used as short term features for diarization by estimating i-vectors over a short window of MFCCs. The Information Bottleneck (IB) approach provides a convenient platform to integrate multiple features together for fast and accurate diarization of speech. Speaker models are estimated over a window of 10 frames of speech and used as features in the IB system. Experiments on the NIST RT datasets show an absolute improvement of 3.9% in the best case when i-vectors are used as auxiliary features to MFCCs. Further, discriminative training algorithms such as LDA and PLDA are applied on the i-vectors. A best case performance improvement of 5% in absolute terms is obtained on the RT datasets.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-111",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "raitio15_interspeech": {
      "authors": [
        [
          "Tuomo",
          "Raitio"
        ],
        [
          "Lauri",
          "Juvela"
        ],
        [
          "Antti",
          "Suni"
        ],
        [
          "Martti",
          "Vainio"
        ],
        [
          "Paavo",
          "Alku"
        ]
      ],
      "title": "Phase perception of the glottal excitation of vocoded speech",
      "original": "i15_0254",
      "page_count": 5,
      "order": 116,
      "p1": "254",
      "pn": "258",
      "abstract": [
        "While the characteristics of the amplitude spectrum of the voiced excitation have been studied widely both in natural and synthetic speech, the role of the excitation phase has remained less explored. Especially in speech synthesis, the phase information is often omitted for simplicity. This study investigates the impact of phase information of the excitation signal of voiced speech. The experiments in the study involve analysis-synthesis of speech using a vocoder that utilizes natural glottal flow pulses for reconstructing the voiced excitation. Firstly, the phase spectra of the glottal flow waveforms are converted to either zero-phase or random-phase. Secondly, the quality of vocoded speech using the two phase-modified pulses is compared in subjective listening tests to the corresponding signal excited with the natural-phase pulse. The results indicate that phase has a perceptually relevant effect in vocoded speech and the use of natural phase improves the synthesis quality.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-112",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "sitaram15_interspeech": {
      "authors": [
        [
          "Sunayana",
          "Sitaram"
        ],
        [
          "Serena",
          "Jeblee"
        ],
        [
          "Alan W.",
          "Black"
        ]
      ],
      "title": "Using acoustics to improve pronunciation for synthesis of low resource languages",
      "original": "i15_0259",
      "page_count": 5,
      "order": 117,
      "p1": "259",
      "pn": "263",
      "abstract": [
        "Some languages have very consistent mappings between graphemes and phonemes, while in other languages, this mapping is more ambiguous. Consonantal writing systems prove to be a challenge for Text to Speech Systems (TTS) because they do not indicate short vowels, which creates an ambiguity in pronunciation. Special letter-to-sound rules may be needed for some cases in languages that otherwise have a good correspondence between graphemes and phonemes. In the low-resource scenario, we may not have linguistic resources such as diacritizers or hand-written rules for the language. We propose a technique to automatically learn pronunciations iteratively from acoustics during TTS training and predict pronunciations from text during synthesis time. We conduct experiments on dialects of Arabic for disambiguating homographs and Hindi for discovering the schwa-deletion rules. We evaluate our systems using objective and subjective metrics of TTS and show significant improvements for dialects of Arabic. Our methods can be generalized to other languages that exhibit similar phenomena.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-113",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "inai15_interspeech": {
      "authors": [
        [
          "Tadashi",
          "Inai"
        ],
        [
          "Sunao",
          "Hara"
        ],
        [
          "Masanobu",
          "Abe"
        ],
        [
          "Yusuke",
          "Ijima"
        ],
        [
          "Noboru",
          "Miyazaki"
        ],
        [
          "Hideyuki",
          "Mizuno"
        ]
      ],
      "title": "Sub-band text-to-speech combining sample-based spectrum with statistically generated spectrum",
      "original": "i15_0264",
      "page_count": 5,
      "order": 118,
      "p1": "264",
      "pn": "268",
      "abstract": [
        "As described in this paper, we propose a sub-band speech synthesis approach to develop a high quality Text-to-Speech (TTS) system: a sample-based spectrum is used in the high-frequency band and spectrum generated by HMM-based TTS is used in the low-frequency band. Herein, sample-based spectrum means spectrum selected from a phoneme database such that it is the most similar to spectrum generated by HMM-based speech synthesis. A key idea is to compensate over-smoothing caused by statistical procedures by introducing a sample-based spectrum, especially in the high-frequency band. Listening test results show that the proposed method has better performance than HMM-based speech synthesis in terms of clarity. It is at the same level as HMM-based speech synthesis in terms of smoothness. In addition, preference test results among the proposed method, HMM-based speech synthesis, and waveform speech synthesis using 80 min speech data reveal that the proposed method is the most liked.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-114"
    },
    "lu15b_interspeech": {
      "authors": [
        [
          "Heng",
          "Lu"
        ],
        [
          "Wei",
          "Zhang"
        ],
        [
          "Xu",
          "Shao"
        ],
        [
          "Quan",
          "Zhou"
        ],
        [
          "Wenhui",
          "Lei"
        ],
        [
          "Hongbin",
          "Zhou"
        ],
        [
          "Andrew",
          "Breen"
        ]
      ],
      "title": "Pruning redundant synthesis units based on static and delta unit appearance frequency",
      "original": "i15_0269",
      "page_count": 5,
      "order": 119,
      "p1": "269",
      "pn": "273",
      "abstract": [
        "In order to reduce the footprint of concatenative speech synthesis systems for embedded devices, a novel method for pruning redundant units is introduced in this work. Instead of using only a unit appearance frequency-based pruning criterion, as in the conventional method, the new method introduces the concept of \u201cdelta unit appearance frequency\u201d which indicates whether a unit is replaceable or not. Both static and delta unit appearance frequency are included in this proposed method as pruning criteria. Only units with comparatively high appearance frequency and which cannot be replaced by other units are preserved in the database. Experiments show that the new method can reduce the footprint of our speech synthesis system greatly without losing much synthesis voice quality.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-115",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "ohtani15_interspeech": {
      "authors": [
        [
          "Yamato",
          "Ohtani"
        ],
        [
          "Yu",
          "Nasu"
        ],
        [
          "Masahiro",
          "Morita"
        ],
        [
          "Masami",
          "Akamine"
        ]
      ],
      "title": "Emotional transplant in statistical speech synthesis based on emotion additive model",
      "original": "i15_0274",
      "page_count": 5,
      "order": 120,
      "p1": "274",
      "pn": "278",
      "abstract": [
        "This paper proposes a novel method to transplant emotions to a new speaker in statistical speech synthesis based on an emotion additive model (EAM), which represents the differences between emotional and neutral voices. This method trains EAM using neutral and emotional speech data of multiple speakers and applies it to a neutral voice model of a new speaker (target). There is some degradation in speech quality due to a mismatch in speakers between the EAM and the target neutral voice model. To alleviate the mismatch, we introduce an eigenvoice technique to this framework. We build neutral voice models and EAMs using multiple speakers, and construct an eigenvoice space consisting the neutral voice models and EAMs. To transplant the emotion to the target speaker, the proposed method estimates weights of eigenvoices for the target neutral speech data based on a maximum likelihood criteria. The EAM of the target speaker is obtained by applying the estimated weights to the EAM parameters of the eigenvoice space. Emotional speech is generated using the EAM and the neutral voice model. Experimental results show that the proposed method performs emotional speech synthesis with reasonable emotions and high speech quality.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-116",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "xie15_interspeech": {
      "authors": [
        [
          "Xurong",
          "Xie"
        ],
        [
          "Xunying",
          "Liu"
        ],
        [
          "Lan",
          "Wang"
        ],
        [
          "Rongfeng",
          "Su"
        ]
      ],
      "title": "Generalized variable parameter HMMs based acoustic-to-articulatory inversion",
      "original": "i15_0279",
      "page_count": 5,
      "order": 121,
      "p1": "279",
      "pn": "283",
      "abstract": [
        "Acoustic-to-articulatory inversion is useful for a range of related research areas including language learning, speech production, speech coding, speech recognition and speech synthesis. HMM-based generative modelling methods and DNN-based approaches have become dominant approaches in recent years. In this paper, a novel acoustic-to-articulatory inversion technique based on generalized variable parameter HMMs (GVP-HMMs) is proposed. It leverages the strengths of both generative and neural network based modelling frameworks. On a Mandarin speech inversion task, a tandem GVP-HMM system using DNN bottleneck features as auxiliary inputs significantly outperformed the baseline HMM, multiple regression HMM (MR-HMM), DNN and deep mixture density network (MDN) systems by 0.20mm, 0.16mm, 0.12mm and 0.10mm respectively in terms of electromagnetic articulography (EMA) root mean square error (RMSE).\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-117",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "mohammadi15_interspeech": {
      "authors": [
        [
          "Seyed Hamidreza",
          "Mohammadi"
        ],
        [
          "Alexander",
          "Kain"
        ]
      ],
      "title": "Semi-supervised training of a voice conversion mapping function using a joint-autoencoder",
      "original": "i15_0284",
      "page_count": 5,
      "order": 122,
      "p1": "284",
      "pn": "288",
      "abstract": [
        "Recently, researchers have begun to investigate Deep Neural Network (DNN) architectures as mapping functions in voice conversion systems. In this study, we propose a novel Stacked-Joint-Autoencoder (SJAE) architecture, which aims to find a common encoding of parallel source and target features. The SJAE is initialized from a Stacked-Autoencoder (SAE) that has been trained on a large general-purpose speech database. We also propose to train the SJAE using unrelated speakers that are similar to the source and target speaker, instead of using only the source and target speakers. The final DNN is constructed from the source-encoding part and the target-decoding part of the SJAE, and then fine-tuned using back-propagation. The use of this semi-supervised training approach allows us to use multiple frames during mapping, since we have previously learned the general structure of the acoustic space and also the general structure of similar source-target speaker mappings. We train two speaker conversions and compare several system configurations objectively and subjectively while varying the number of available training sentences. The results show that each of the individual contributions of SAE, SJAE, and using unrelated speakers to initialize the mapping function increases conversion performance.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-118"
    },
    "huber15_interspeech": {
      "authors": [
        [
          "Stefan",
          "Huber"
        ],
        [
          "Axel",
          "Roebel"
        ]
      ],
      "title": "On glottal source shape parameter transformation using a novel deterministic and stochastic speech analysis and synthesis system",
      "original": "i15_0289",
      "page_count": 5,
      "order": 123,
      "p1": "289",
      "pn": "293",
      "abstract": [
        "In this paper we present a flexible deterministic plus stochastic model (DSM) approach for parametric speech analysis and synthesis with high quality. The novelty of the proposed speech processing system lies in its extended means to estimate the unvoiced stochastic component and to robustly handle the transformation of the glottal excitation source. It is therefore well suited as speech system within the context of Voice Transformation and Voice Conversion. The system is evaluated in the context of a voice quality transformation on natural human speech. The voice quality of a speech phrase is altered by means of re-synthesizing the deterministic component with different pulse shapes of the glottal excitation source. A subjective listening test suggests that the speech processing system is able to successfully synthesize and arise to a listener the perceptual sensation of different voice quality characteristics. Additionally, improvements of the speech synthesis quality compared to a baseline method are demonstrated.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-119",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "huang15_interspeech": {
      "authors": [
        [
          "Yi-Chin",
          "Huang"
        ],
        [
          "Chung-Hsien",
          "Wu"
        ],
        [
          "Ming-Ge",
          "Shie"
        ]
      ],
      "title": "Fluent personalized speech synthesis with prosodic word-level spontaneous speech generation",
      "original": "i15_0294",
      "page_count": 5,
      "order": 124,
      "p1": "294",
      "pn": "298",
      "abstract": [
        "This paper proposes an automatic approach to generating speech with fluency at the prosodic word level based on a small-sized speech database of the target speaker, consisting of read and fluent speech. First, an auto-segmentation algorithm is employed to automatically segment and label the database of the target speaker. A pre-trained average voice model is adapted to the voice model of the target speaker by using the auto-segmented data. For synthesizing fluent speech, a prosodic model is proposed to smooth the prosodic word-level parameters to improve the fluency in a prosodic word. Finally, a postfilter method based on the modulation spectrum is adopted to alleviate over-smoothing problem of the synthesized speech and thus improve the speaker similarity. Experimental results showed that the proposed method can effectively improve the speech fluency and speaker likeliness of the synthesized speech for a target speaker compared to the MLLR-based model adaptation method.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-120",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "oshima15_interspeech": {
      "authors": [
        [
          "Yuji",
          "Oshima"
        ],
        [
          "Shinnosuke",
          "Takamichi"
        ],
        [
          "Tomoki",
          "Toda"
        ],
        [
          "Graham",
          "Neubig"
        ],
        [
          "Sakriani",
          "Sakti"
        ],
        [
          "Satoshi",
          "Nakamura"
        ]
      ],
      "title": "Non-native speech synthesis preserving speaker individuality based on partial correction of prosodic and phonetic characteristics",
      "original": "i15_0299",
      "page_count": 5,
      "order": 125,
      "p1": "299",
      "pn": "303",
      "abstract": [
        "This paper presents a novel non-native speech synthesis technique that preserves the individuality of a non-native speaker. Cross-lingual speech synthesis based on voice conversion or HMM-based speech synthesis, which synthesizes foreign language speech of a specific non-native speaker reflecting the speaker-dependent acoustic characteristics extracted from the speaker's natural speech in his/her mother tongue, tends to cause a degradation of speaker individuality in synthetic speech compared to intra-lingual speech synthesis. This paper proposes a new approach to cross-lingual speech synthesis that preserves speaker individuality by explicitly using non-native speech spoken by the target speaker. Although the use of non-native speech makes it possible to preserve the speaker individuality in the synthesized target speech, naturalness is significantly degraded as the speech is directly affected by unnatural prosody and pronunciation often caused by differences in the linguistic systems of the source and target languages. To improve naturalness while preserving speaker individuality, we propose (1) a prosodic correction method based on model adaptation, and (2) a phonetic correction method based on spectrum replacement for unvoiced consonants. The experimental results demonstrate that these proposed methods are capable of significantly improving naturalness while preserving the speaker individuality in synthetic speech.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-121",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "toman15_interspeech": {
      "authors": [
        [
          "Markus",
          "Toman"
        ],
        [
          "Michael",
          "Pucher"
        ]
      ],
      "title": "Evaluation of state mapping based foreign accent conversion",
      "original": "i15_0304",
      "page_count": 5,
      "order": 126,
      "p1": "304",
      "pn": "308",
      "abstract": [
        "We present an evaluation of the perception of foreign-accented natural and synthetic speech in comparison to accent-reduced synthetic speech. Our method for foreign accent conversion is based on mapping of Hidden Semi-Markov Model states between accented and non-accented voice models and does not need an average voice model of accented speech. We employ the method on recorded data of speakers with first language (L1) from different European countries and second language (L2) being Austrian German. Results from a subjective evaluation show that the proposed method is able to significantly reduce the perceived accent. It also retains speaker similarity when an average voice model of the same gender is used. Accentedness of synthetic speech was rated significantly lower than natural speech by the participants and listeners were unable to identify accents correctly for 81% of the natural and 85% of the synthesized samples. Our evaluation shows the feasibility of accent conversion with a limited amount of speech resources.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-122",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "wu15_interspeech": {
      "authors": [
        [
          "Zhizheng",
          "Wu"
        ],
        [
          "Simon",
          "King"
        ]
      ],
      "title": "Minimum trajectory error training for deep neural networks, combined with stacked bottleneck features",
      "original": "i15_0309",
      "page_count": 5,
      "order": 127,
      "p1": "309",
      "pn": "313",
      "abstract": [
        "Recently, Deep Neural Networks (DNNs) have shown promise as an acoustic model for statistical parametric speech synthesis. Their ability to learn complex mappings from linguistic features to acoustic features has advanced the naturalness of synthesis speech significantly. However, because DNN parameter estimation methods typically attempt to minimise the mean squared error of each individual frame in the training data, the dynamic and continuous nature of speech parameters is neglected. In this paper, we propose a training criterion that minimises speech parameter trajectory errors, and so takes dynamic constraints from a wide acoustic context into account during training. We combine this novel training criterion with our previously proposed stacked bottleneck features, which provide wide linguistic context. Both objective and subjective evaluation results confirm the effectiveness of the proposed training criterion for improving model accuracy and naturalness of synthesised speech.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-123",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "wang15b_interspeech": {
      "authors": [
        [
          "Yang",
          "Wang"
        ],
        [
          "Minghao",
          "Yang"
        ],
        [
          "Zhengqi",
          "Wen"
        ],
        [
          "Jianhua",
          "Tao"
        ]
      ],
      "title": "Combining extreme learning machine and decision tree for duration prediction in HMM based speech synthesis",
      "original": "i15_2197",
      "page_count": 5,
      "order": 128,
      "p1": "2197",
      "pn": "2201",
      "abstract": [
        "Hidden Markov Model (HMM) based speech synthesis using Decision Tree (DT) for duration prediction is known to produce over-averaged rhythm. To alleviate this problem, this paper proposes a two level duration prediction method together with outlier removal. This method takes advantages of accurate regression capability by Extreme Learning Machine (ELM) for phone level duration prediction, and the capability of distributing state durations by DT for state level duration prediction. Experimental results showed that the method decreased RMSE of phone duration, increased the fluctuation of syllable duration, and achieved 63.75% in preference evaluation. Furthermore, this method does not incur laborious manual alignment on training corpus.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-124",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "ninh15_interspeech": {
      "authors": [
        [
          "Duy Khanh",
          "Ninh"
        ],
        [
          "Yoichi",
          "Yamashita"
        ]
      ],
      "title": "F0 parameterization of glottalized tones for HMM-based vietnamese TTS",
      "original": "i15_2202",
      "page_count": 5,
      "order": 129,
      "p1": "2202",
      "pn": "2206",
      "abstract": [
        "A conventional HMM-based TTS system for Hanoi Vietnamese often suffers from the hoarse quality due to the incomplete F0 parameterization of glottalized tones. As estimating F0 in glottalization is rather problematic for usual F0 extractors, we propose a pitch marking algorithm where the pitch marks are propagated from regular regions of speech signal to glottalized one, from which the complete F0 contour of a glottalized tone is derived. The proposed F0 parameterization scheme was confirmed to significantly reduce the hoarseness whilst improve the tone naturalness of synthetic speech by both objective and listening tests. The pitch marking algorithm works as a refinement step based on the results of an F0 extractor. Therefore, the proposed scheme can be combined with any F0 extractor.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-125",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "merritt15_interspeech": {
      "authors": [
        [
          "Thomas",
          "Merritt"
        ],
        [
          "Junichi",
          "Yamagishi"
        ],
        [
          "Zhizheng",
          "Wu"
        ],
        [
          "Oliver",
          "Watts"
        ],
        [
          "Simon",
          "King"
        ]
      ],
      "title": "Deep neural network context embeddings for model selection in rich-context HMM synthesis",
      "original": "i15_2207",
      "page_count": 5,
      "order": 130,
      "p1": "2207",
      "pn": "2211",
      "abstract": [
        "This paper introduces a novel form of parametric synthesis that uses context embeddings produced by the bottleneck layer of a deep neural network to guide the selection of models in a rich-context HMM-based synthesiser. Rich-context synthesis \u2014 in which Gaussian distributions estimated from single linguistic contexts seen in the training data are used for synthesis, rather than more conventional decision tree-tied models \u2014 was originally proposed to address over-smoothing due to averaging across contexts. Our previous investigations have confirmed experimentally that averaging across different contexts is indeed one of the largest factors contributing to the limited quality of statistical parametric speech synthesis. However, a possible weakness of the rich context approach as previously formulated is that a conventional tied model is still used to guide selection of Gaussians at synthesis time. Our proposed approach replaces this with context embeddings derived from a neural network.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-126",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "chen15e_interspeech": {
      "authors": [
        [
          "Bo",
          "Chen"
        ],
        [
          "Zhehuai",
          "Chen"
        ],
        [
          "Jiachen",
          "Xu"
        ],
        [
          "Kai",
          "Yu"
        ]
      ],
      "title": "An investigation of context clustering for statistical speech synthesis with deep neural network",
      "original": "i15_2212",
      "page_count": 5,
      "order": 131,
      "p1": "2212",
      "pn": "2216",
      "abstract": [
        "The state-of-the-art DNN speech synthesis system directly maps linguistic input to acoustic output and voice quality improvement over the conventional MSD-GMM-HMM synthesis system has been reported. DNN-based speech synthesis system does not require context clustering as in GMM-HMM systems and this was believed to be the main advantage and contributor to performance improvement. Our previous work has demonstrated that F0 interpolation, rather than context clustering, is the actual contributor for performance improvement. However, it remains unknown whether the use of unclustered context is a beneficial characteristic of DNN-based synthesis or not. In this paper, this issue is investigated in detail. Decision tree clustered contexts are used as linguistic input for DNN and compared to unclustered context input. A novel approach for inputting context clusters is proposed. Here, the decision tree question indicators are used as input instead of the clustered contexts. Experiments showed that DNN with clustered contexts significantly outperformed DNN with unclustered contexts and the proposed question indicator input approach obtained the best performance. The investigation of this paper reveals the limitation of DNN-based speech synthesis and implies that context clustering is also an important issue for DNN-based speech synthesis with limited training data.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-127",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "watts15_interspeech": {
      "authors": [
        [
          "Oliver",
          "Watts"
        ],
        [
          "Zhizheng",
          "Wu"
        ],
        [
          "Simon",
          "King"
        ]
      ],
      "title": "Sentence-level control vectors for deep neural network speech synthesis",
      "original": "i15_2217",
      "page_count": 5,
      "order": 132,
      "p1": "2217",
      "pn": "2221",
      "abstract": [
        "This paper describes the use of a low-dimensional vector representation of sentence acoustics to control the output of a feed-forward deep neural network text-to-speech system on a sentence-by-sentence basis. Vector representations for sentences in the training corpus are learned during network training along with other parameters of the model. Although the network is trained on a frame-by-frame basis, the standard frame-level inputs representing linguistic features are supplemented by features from a projection layer which outputs a learned representation of sentence-level acoustic characteristics. The projection layer contains dedicated parameters for each sentence in the training data which are optimised jointly with the standard network weights. Sentence-specific parameters are optimised on all frames of the relevant sentence \u2014 these parameters therefore allow the network to account for sentence-level variation in the data which is not predictable from the standard linguistic inputs. Results show that the global prosodic characteristics of synthetic speech can be controlled simply and robustly at run time by supplementing basic linguistic features with sentence-level control vectors which are novel but designed to be consistent with those observed in the training corpus.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-128",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "betz15_interspeech": {
      "authors": [
        [
          "Simon",
          "Betz"
        ],
        [
          "Petra",
          "Wagner"
        ],
        [
          "David",
          "Schlangen"
        ]
      ],
      "title": "Micro-structure of disfluencies: basics for conversational speech synthesis",
      "original": "i15_2222",
      "page_count": 5,
      "order": 133,
      "p1": "2222",
      "pn": "2226",
      "abstract": [
        "Incremental dialogue systems can produce fast responses and can interact in a human-like fashion. However, these systems occasionally produce erroneous material or run out of things to say. Humans in such situations use disfluencies to remedy their ongoing production and signal this to the listener. We devised a new model for inserting disfluencies into synthesis and evaluated this approach in a perception test. It showed that lengthenings and silent pauses can be built for speech synthesis with low effort and high output quality. Synthesized word fragments and filled pauses, while potentially useful in incremental dialogue systems, appear more difficult to handle for listeners. While we were able to get consistently high ratings for certain types of disfluencies, the need for more basic research on their micro structure became apparent in order to be able to synthesize the fine phonetic detail of disfluencies. For this, we analysed corpus data with regard to distributional and durational aspects of lengthenings, word fragments and pauses. Based on these natural speaking strategies, we explored further to what extent speech can be delayed using disfluency strategies, and how to handle difficult disfluency elements by determining the appropriate amount of durational variation applicable.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-129",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "szaszak15_interspeech": {
      "authors": [
        [
          "Gy\u00f6rgy",
          "Szasz\u00e1k"
        ],
        [
          "Andr\u00e1s",
          "Beke"
        ],
        [
          "G\u00e1bor",
          "Olaszy"
        ],
        [
          "B\u00e1lint P\u00e1l",
          "T\u00f3th"
        ]
      ],
      "title": "Using automatic stress extraction from audio for improved prosody modelling in speech synthesis",
      "original": "i15_2227",
      "page_count": 5,
      "order": 134,
      "p1": "2227",
      "pn": "2231",
      "abstract": [
        "Generating proper and natural sounding prosody is one of the key interests of today's speech synthesis research. An important factor in this effort is the availability of a precisely labelled speech corpus with adequate prosodic stress marking. Obtaining such a labelling constitutes a huge effort, whereas inter-annotator agreement scores are usually found far below 100%. Stress marking based on phonetic transcription is an alternative, but yields even poorer quality than human annotation. Applying an automatic labelling may help overcoming these difficulties. The current paper presents an automatic approach for stress detection based purely on audio, which is used to derive an automatic, layered labelling of stress events and link them to syllables. For proof of concept, a speech corpus was extended by the output of the stress detection algorithm and a HMM-TTS system was trained with the extended corpus. Results are compared to a baseline system, trained on the same database, but with stress marking obtained from textual transcripts after applying a set of linguistic rules. The evaluation includes CMOS tests and the analysis of the decision trees. Results show an overall improvement in prosodic properties of the synthesized speech. Subjective ratings reveal a voice perceived as more natural.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-130",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "lanchantin15_interspeech": {
      "authors": [
        [
          "Pierre",
          "Lanchantin"
        ],
        [
          "Christophe",
          "Veaux"
        ],
        [
          "Mark J. F.",
          "Gales"
        ],
        [
          "Simon",
          "King"
        ],
        [
          "Junichi",
          "Yamagishi"
        ]
      ],
      "title": "Reconstructing voices within the multiple-average-voice-model framework",
      "original": "i15_2232",
      "page_count": 5,
      "order": 135,
      "p1": "2232",
      "pn": "2236",
      "abstract": [
        "Personalisation of voice output communication aids (VOCAs) allows to preserve the vocal identity of people suffering from speech disorders. This can be achieved by the adaptation of HMM-based speech synthesis systems using a small amount of adaptation data. When the voice has begun to deteriorate, reconstruction is still possible in the statistical domain by correcting the parameters of the models associated with the speech disorder. This can be done by substituting those with parameters from a donor's voice, at risk of losing part of the identity of the patient. Recently, the Multiple-Average-Voice-Model (Multiple AVM) framework has been proposed for speaker adaptation. Adaptation is performed via interpolation into a speaker eigenspace spanned by the mean vectors of speaker-adapted AVMs which can be tuned to the individual speaker. In this paper, we present the benefits of this framework for voice reconstruction: it requires only a very small amount of adaptation data, interpolation can be performed in a clean speech eigenspace and the resulting voice can be easily fine-tuned by acting on the interpolation weights. We illustrate our points with a subjective assessment of the reconstructed voice.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-131",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "thu15_interspeech": {
      "authors": [
        [
          "Ye Kyaw",
          "Thu"
        ],
        [
          "Win Pa",
          "Pa"
        ],
        [
          "Jinfu",
          "Ni"
        ],
        [
          "Yoshinori",
          "Shiga"
        ],
        [
          "Andrew",
          "Finch"
        ],
        [
          "Chiori",
          "Hori"
        ],
        [
          "Hisashi",
          "Kawai"
        ],
        [
          "Eiichiro",
          "Sumita"
        ]
      ],
      "title": "HMM based myanmar text to speech system",
      "original": "i15_2237",
      "page_count": 5,
      "order": 136,
      "p1": "2237",
      "pn": "2241",
      "abstract": [
        "This paper presents a complete statistical speech synthesizer for Myanmar which includes a syllable segmenter, text normalizer, grapheme-to-phoneme convertor, and an HMM-based speech synthesis engine. We believe this is the first such system for the Myanmar language. We performed a thorough human evaluation of the synthesizer relative to human and re-synthesized baselines. Our results show that our system is able to synthesize speech at a quality comparable with similar state-of-the-art synthesizers for other languages.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-132",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "takaki15_interspeech": {
      "authors": [
        [
          "Shinji",
          "Takaki"
        ],
        [
          "SangJin",
          "Kim"
        ],
        [
          "Junichi",
          "Yamagishi"
        ],
        [
          "JongJin",
          "Kim"
        ]
      ],
      "title": "Multiple feed-forward deep neural networks for statistical parametric speech synthesis",
      "original": "i15_2242",
      "page_count": 5,
      "order": 137,
      "p1": "2242",
      "pn": "2246",
      "abstract": [
        "In this paper, we investigate a combination of several feed-forward deep neural networks (DNNs) for a high-quality statistical parametric speech synthesis system. Recently, DNNs have significantly improved the performance of essential components in the statistical parametric speech synthesis, e.g. spectral feature extraction, acoustic modeling and spectral post-filter. In this paper our proposed technique combines these feed-forward DNNs so that the DNNs can perform all standard steps of the statistical speech synthesis from end to end, including the feature extraction from STRAIGHT spectral amplitudes, acoustic modeling, smooth trajectory generation and spectral post-filter. The proposed DNN-based speech synthesis system is then compared to the state-of-the-art speech synthesis systems, i.e. conventional HMM-based, DNN-based and unit selection ones.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-133",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "yao15_interspeech": {
      "authors": [
        [
          "Kaisheng",
          "Yao"
        ],
        [
          "Geoffrey",
          "Zweig"
        ]
      ],
      "title": "Sequence-to-sequence neural net models for grapheme-to-phoneme conversion",
      "original": "i15_3330",
      "page_count": 5,
      "order": 138,
      "p1": "3330",
      "pn": "3334",
      "abstract": [
        "Sequence-to-sequence translation methods based on generation with a side-conditioned language model have recently shown promising results in several tasks. In machine translation, models conditioned on source side words have been used to produce target-language text, and in image captioning, models conditioned images have been used to generate caption text. Past work with this approach has focused on large vocabulary tasks, and measured quality in terms of BLEU. In this paper, we explore the applicability of such models to the qualitatively different grapheme-to-phoneme task. Here, the input and output side vocabularies are small, plain n-gram models do well, and credit is only given when the output is exactly correct. We find that the simple side-conditioned generation approach is able to rival the state-of-the-art, and we are able to significantly advance the stat-of-the-art with bi-directional long short-term memory (LSTM) neural networks that use the same alignment information that is used in conventional approaches.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-134",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "kay15_interspeech": {
      "authors": [
        [
          "Rosie",
          "Kay"
        ],
        [
          "Oliver",
          "Watts"
        ],
        [
          "Roberto Barra",
          "Chicote"
        ],
        [
          "Cassie",
          "Mayo"
        ]
      ],
      "title": "Knowledge versus data in TTS: evaluation of a continuum of synthesis systems",
      "original": "i15_3335",
      "page_count": 5,
      "order": 139,
      "p1": "3335",
      "pn": "3339",
      "abstract": [
        "Grapheme-based models have been proposed for both ASR and TTS as a way of circumventing the lack of expert-compiled pronunciation lexicons in under-resourced languages. It is a common observation that this should work well in languages employing orthographies with a transparent letter-to-phoneme relationship, such as Spanish. Our experience has shown, however, that there is still a significant difference in intelligibility between grapheme-based systems and conventional ones for this language. This paper explores the contribution of different levels of linguistic annotation to system intelligibility, and the trade-off between those levels and the quantity of data used for training. Ten systems spaced across these two continua of knowledge and data were subjectively evaluated for intelligibility.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-135",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "eger15_interspeech": {
      "authors": [
        [
          "Steffen",
          "Eger"
        ]
      ],
      "title": "Improving G2p from wiktionary and other (web) resources",
      "original": "i15_3340",
      "page_count": 5,
      "order": 140,
      "p1": "3340",
      "pn": "3344",
      "abstract": [
        "We consider the problem of integrating supplemental information strings in the grapheme-to-phoneme (G2P) conversion task. In particular, we investigate whether we can improve the performance of a G2P system by making it aware of corresponding transductions of an external knowledge source, such as transcriptions in other dialects or languages, transcriptions provided by other datasets, or transcriptions obtained from crowd-sourced knowledge bases such as Wiktionary. Our main methodological paradigm is that of  multiple monotone many-to-many alignments of input strings, supplemental information strings, and desired transcriptions. Subsequently, we apply a discriminative sequential transducer to the multiply aligned data, using subsequences of the supplemental information strings as additional features.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-136",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "ding15_interspeech": {
      "authors": [
        [
          "Chuang",
          "Ding"
        ],
        [
          "Pengcheng",
          "Zhu"
        ],
        [
          "Lei",
          "Xie"
        ]
      ],
      "title": "BLSTM neural networks for speech driven head motion synthesis",
      "original": "i15_3345",
      "page_count": 5,
      "order": 141,
      "p1": "3345",
      "pn": "3349",
      "abstract": [
        "Head motion naturally occurs in synchrony with speech and carries important intention, attitude and emotion factors. This paper aims to synthesize head motions from natural speech for talking avatar applications. Specifically, we study the feasibility of learning speech-to-head-motion regression models by two types of popular neural networks, i.e., feed-forward and bidirectional long short-term memory (BLSTM). We discover that the BLSTM networks apparently outperform the feed-forward ones in this task because of their capacity of learning long-range speech dynamics. More interestingly, we observe that stacking different networks, i.e., inserting a feed-forward layer into two BLSTM layers, achieves the best performance. Subjective evaluation shows that this hybrid network can produce more plausible head motions from speech.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-137",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "tobing15_interspeech": {
      "authors": [
        [
          "Patrick Lumban",
          "Tobing"
        ],
        [
          "Kazuhiro",
          "Kobayashi"
        ],
        [
          "Tomoki",
          "Toda"
        ],
        [
          "Graham",
          "Neubig"
        ],
        [
          "Sakriani",
          "Sakti"
        ],
        [
          "Satoshi",
          "Nakamura"
        ]
      ],
      "title": "Articulatory controllable speech modification based on Gaussian mixture models with direct waveform modification using spectrum differential",
      "original": "i15_3350",
      "page_count": 5,
      "order": 142,
      "p1": "3350",
      "pn": "3354",
      "abstract": [
        "In our previous work, we have developed a speech modification system capable of manipulating unobserved articulatory movements by sequentially performing speech-to-articulatory inversion mapping and articulatory-to-speech production mapping based on a Gaussian mixture model (GMM)-based statistical feature mapping technique. One of the biggest issues to be addressed in this system is quality degradation of the synthetic speech caused by modeling and conversion errors in a vocoder-based waveform generation framework. To address this issue, we propose several implementation methods of direct waveform modification. The proposed methods directly filter an input speech waveform with a time sequence of spectral differential parameters calculated between unmodified and modified spectral envelop parameters in order to avoid using vocoder-based excitation signal generation. The experimental results show that the proposed direct waveform modification methods yield significantly larger quality improvements in the synthetic speech while also keeping a capability of intuitively modifying phoneme sounds by manipulating the unobserved articulatory movements.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-138",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "cornu15_interspeech": {
      "authors": [
        [
          "Thomas Le",
          "Cornu"
        ],
        [
          "Ben",
          "Milner"
        ]
      ],
      "title": "Reconstructing intelligible audio speech from visual speech features",
      "original": "i15_3355",
      "page_count": 5,
      "order": 143,
      "p1": "3355",
      "pn": "3359",
      "abstract": [
        "This work describes an investigation into the feasibility of producing intelligible audio speech from only visual speech features. The proposed method aims to estimate a spectral envelope from visual features which is then combined with an artificial excitation signal and used within a model of speech production to reconstruct an audio signal. Different combinations of audio and visual features are considered, along with both a statistical method of estimation and a deep neural network. The intelligibility of the reconstructed audio speech is measured by human listeners, and then compared to the intelligibility of the video signal only and when combined with the reconstructed audio.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-139",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "sitaram15b_interspeech": {
      "authors": [
        [
          "Sunayana",
          "Sitaram"
        ],
        [
          "Alok",
          "Parlikar"
        ],
        [
          "Gopala Krishna",
          "Anumanchipalli"
        ],
        [
          "Alan W.",
          "Black"
        ]
      ],
      "title": "Universal grapheme-based speech synthesis",
      "original": "i15_3360",
      "page_count": 5,
      "order": 144,
      "p1": "3360",
      "pn": "3364",
      "abstract": [
        "Grapheme-to-phoneme conversion follows the text processing step in speech synthesis. Typically, lexicons or Letter-to-Sound rules are used to map graphemes to phonemes. However, in some languages, such resources may not be readily available. In this paper, we describe a universal front end that supports using grapheme information alone to build usable speech synthesis systems. This work takes advantage of an explicit mapping of Unicode characters from a wide range of scripts to a single phoneset to create support for building speech synthesizers for most languages in the world. We compare the efficacy of this front end to the baseline approach of treating every single grapheme as a separate phoneme for synthesis by building voices for twelve languages across several language families and to front ends with linguistic knowledge in languages with higher resources. In addition, we improve our models by using Random Forests as opposed to using single Classification and Regression Trees. We find that the common universal front end performs better than the raw graphemes in general. We also find that using Random Forests lead to a significant improvement in synthesis quality, which is better than the quality of the knowledge based front end in many cases.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-140",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "wester15_interspeech": {
      "authors": [
        [
          "Mirjam",
          "Wester"
        ],
        [
          "Matthew",
          "Aylett"
        ],
        [
          "Marcus",
          "Tomalin"
        ],
        [
          "Rasmus",
          "Dall"
        ]
      ],
      "title": "Artificial personality and disfluency",
      "original": "i15_3365",
      "page_count": 5,
      "order": 145,
      "p1": "3365",
      "pn": "3369",
      "abstract": [
        "The focus of this paper is artificial voices with different personalities. Previous studies have shown links between an individual's use of disfluencies in their speech and their perceived personality. Here, filled pauses ( uh and um) and discourse markers ( like, you know, I mean) have been included in synthetic speech as a way of creating an artificial voice with different personalities. We discuss the automatic insertion of filled pauses and discourse markers (i.e., fillers) into otherwise fluent texts. The automatic system is compared to a ground truth of human \u201cacted\u201d filler insertion. Perceived personality (as defined by the big five personality dimensions) of the synthetic speech is assessed by means of a standardised questionnaire. Synthesis without fillers is compared to synthesis with either spontaneous or synthetic fillers. Our findings explore how the inclusion of disfluencies influences the way in which subjects rate the perceived personality of an artificial voice.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-141",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "evrard15_interspeech": {
      "authors": [
        [
          "Marc",
          "Evrard"
        ],
        [
          "Samuel",
          "Delalez"
        ],
        [
          "Christophe",
          "d'Alessandro"
        ],
        [
          "Albert",
          "Rilliard"
        ]
      ],
      "title": "Comparison of chironomic stylization versus statistical modeling of prosody for expressive speech synthesis",
      "original": "i15_3370",
      "page_count": 5,
      "order": 146,
      "p1": "3370",
      "pn": "3374",
      "abstract": [
        "Chironomic stylization is the process of real-time modification of intonation contours (f0 and tempo) using drawing/writing gestures with a stylus on a graphic tablet. The question addressed in this research is whether hand-made intonation stylization could improve or degrade expressivity and overall quality, compared to statistical modeling of prosody. A system for expressive TTS in French based on HMM was designed. A neutral corpus and six expressive speech corpora were used ( anger, fear, joy, sadness, sensuality, surprise). Five sentences were synthesized with the six types of expressivity through CMLLR adaptation. Using a chironomic system, three trained subjects were asked to modify synthetic sentences, aiming at improving their expressive quality. Natural, HMM-TTS, and HMM-TTS-Chironomic sentences were evaluated in an expressivity recognition test and a MOS test. The results show that chironomic modification brings significant improvements in both recognition and MOS tests. These results are discussed in detail, together with the effects of voice quality on the perception of HMM-TTS expressive speech. The two main conclusions are: (i) intonation of HMM-TTS can be significantly improved; (ii) hand-corrected TTS improves expressivity and overall quality. Chironomic stylization is a powerful tool lying between fully automatic TTS and recorded speech.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-142",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "ardaillon15_interspeech": {
      "authors": [
        [
          "Luc",
          "Ardaillon"
        ],
        [
          "Gilles",
          "Degottex"
        ],
        [
          "Axel",
          "Roebel"
        ]
      ],
      "title": "A multi-layer F0 model for singing voice synthesis using a b-spline representation with intuitive controls",
      "original": "i15_3375",
      "page_count": 5,
      "order": 147,
      "p1": "3375",
      "pn": "3379",
      "abstract": [
        "In singing voice, the fundamental frequency (F0) carries not only melody, but also music style, personal expressivity and other characteristics specific to voice production mechanism. The F0 modeling is therefore critical for a natural-sounding and expressive synthesis. In addition, for artistic purposes, composers also need to have control over expressive parameters of the F0 curve, which is missing in many current approaches. This paper presents a novel parametric F0 model for singing voice synthesis with intuitive control of expressive parameters. The proposed approach considers the various F0 variations of the singing voice as separate layers using B-splines to model the melodic component. This model has been implemented in a concatenative singing voice synthesis system and its perceived naturalness has been evaluated through listening tests. The validity of each layer is first evaluated independently, and the full model is then compared to real F0 curves from professional singers. The results of these tests suggest that the model is suitable to produce natural and expressive F0 contours.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-143",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "jauk15_interspeech": {
      "authors": [
        [
          "Igor",
          "Jauk"
        ],
        [
          "Antonio",
          "Bonafonte"
        ],
        [
          "Paula",
          "Lopez-Otero"
        ],
        [
          "Laura",
          "Docio-Fernandez"
        ]
      ],
      "title": "Creating expressive synthetic voices by unsupervised clustering of audiobooks",
      "original": "i15_3380",
      "page_count": 5,
      "order": 148,
      "p1": "3380",
      "pn": "3384",
      "abstract": [
        "In this work we design an approach for automatic feature selection and voice creation for expressive synthesis. Our approach is guided by two main goals: (1) increasing the flexibility of expressive voice creation and (2) overcoming the limitations of speaking styles in expressive synthesis. We define a novel set of features, combining traditionally used prosodic features with spectral features and proposing the use of iVectors. With these features we perform unsupervised clustering of an audiobook excerpt and, from these clusters, we create synthetic voices using the SAT technique. To evaluate the clustering performance we propose an objective evaluation of the unsupervised clustering results technique based on perplexity reduction. This objective evaluation indicates that both prosodic and spectral features contribute to separate speaking styles and emotions, achieving the best results when including iVectors in the feature set, leading to a perplexity reduction of the expressions and audiobook characters by factors 14 and 2, respectively. We also designed a novel subjective evaluation method where the participants have to edit a small excerpt of an audiobook using synthetic voices created from clusters. The results suggest that our feature set is effective in the task of expressiveness and character detection.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-144",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "aryal15_interspeech": {
      "authors": [
        [
          "Sandesh",
          "Aryal"
        ],
        [
          "Ricardo",
          "Gutierrez-Osuna"
        ]
      ],
      "title": "Articulatory-based conversion of foreign accents with deep neural networks",
      "original": "i15_3385",
      "page_count": 5,
      "order": 149,
      "p1": "3385",
      "pn": "3389",
      "abstract": [
        "We present an articulatory-based method for real-time accent conversion using deep neural networks (DNN). The approach consists of two steps. First, we train a DNN articulatory synthesizer for the non-native speaker that estimates acoustics from contextualized articulatory gestures. Then we drive the DNN with articulatory gestures from a reference native speaker \u2014 mapped to the nonnative articulatory space via a Procrustes transform. We evaluate the accent-conversion performance of the DNN through a series of listening tests of intelligibility, voice identity and nonnative accentedness. Compared to a baseline method based on Gaussian mixture models, the DNN accent conversions were found to be 31% more intelligible, and were perceived more native-like in 68% of the cases. The DNN also succeeded in preserving the voice identity of the nonnative speaker.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-145",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "matousek15_interspeech": {
      "authors": [
        [
          "Jind\u0159ich",
          "Matou\u0161ek"
        ],
        [
          "Daniel",
          "Tihelka"
        ]
      ],
      "title": "Anomaly-based annotation errors detection in TTS corpora",
      "original": "i15_0314",
      "page_count": 5,
      "order": 150,
      "p1": "314",
      "pn": "318",
      "abstract": [
        "In this paper we adopt several anomaly detection methods to detect annotation errors in single-speaker read-speech corpora used for text-to-speech (TTS) synthesis. Correctly annotated words are considered as normal examples on which the detection methods are trained. Misannotated words are then taken as anomalous examples which do not conform to normal patterns of the trained detection models. Word-level feature sets including basic features derived from forced alignment, and various acoustic, spectral, phonetic, and positional features were examined. Dimensionality reduction techniques were also applied to reduce the number of features. The first results with F1 score being almost 89% show that anomaly detection could help in detecting annotation errors in read-speech corpora for TTS synthesis.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-146",
      "author_area_id": "12",
      "author_area_label": "Spoken Language Processing \u2013 Translation, Information Retrieval, and Resources"
    },
    "schweitzer15_interspeech": {
      "authors": [
        [
          "Katrin",
          "Schweitzer"
        ],
        [
          "Markus",
          "G\u00e4rtner"
        ],
        [
          "Arndt",
          "Riester"
        ],
        [
          "Ina",
          "R\u00f6siger"
        ],
        [
          "Kerstin",
          "Eckart"
        ],
        [
          "Jonas",
          "Kuhn"
        ],
        [
          "Grzegorz",
          "Dogil"
        ]
      ],
      "title": "Analysing automatic descriptions of intonation with ICARUS",
      "original": "i15_0319",
      "page_count": 5,
      "order": 151,
      "p1": "319",
      "pn": "323",
      "abstract": [
        "We present ICARUS for intonation \u2014 a graphical tool which allows to access automatically derived F0 features in an intuitive and user-friendly way. It can be used for data exploration and search. Tonal features can be accessed together with information from other linguistic levels; this is exemplified in two search queries where we combine the search for a specific tonal contour with a) coreference annotations and b) automatically derived syntactic annotations. Thereby we demonstrate how ICARUS for intonation bridges the gap between manual/semi-automatic analysis of relatively small, manually annotated data sets and automatic analysis of larger corpora with automatically derived features.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-147",
      "author_area_id": "12",
      "author_area_label": "Spoken Language Processing \u2013 Translation, Information Retrieval, and Resources"
    },
    "chen15f_interspeech": {
      "authors": [
        [
          "Nancy F.",
          "Chen"
        ],
        [
          "Rong",
          "Tong"
        ],
        [
          "Darren",
          "Wee"
        ],
        [
          "Peixuan",
          "Lee"
        ],
        [
          "Bin",
          "Ma"
        ],
        [
          "Haizhou",
          "Li"
        ]
      ],
      "title": "iCALL corpus: Mandarin Chinese spoken by non-native speakers of European descent",
      "original": "i15_0324",
      "page_count": 5,
      "order": 152,
      "p1": "324",
      "pn": "328",
      "abstract": [
        "We present iCALL, a speech corpus designed to evaluate Mandarin Chinese pronunciation patterns of non-native speakers of European descent, developed at the Institute for Infocomm Research (I2R) in Singapore. To the best of our knowledge, iCALL is larger than any reported non-native corpora to date in terms of utterance number, duration, and number of speakers: iCALL consists of 90,841 utterances from 305 speakers with a total duration of 142 hours. The speakers are gender-balanced, from a diverse native language background, and represent a realistic sampling of the adult age of Mandarin learners. The read utterances are phonetically balanced and are of varying lengths (words, phrases, and sentences). The spoken utterances are phonetically transcribed and perceptually rated with fluency scores by trained native speakers of Mandarin. In this work, we share our experience in corpus design, data collection, and human annotation and analyze phonetic and tonal error patterns, in particular their relationship with speaker demographics and utterance length. Potential applications of the iCALL corpus include computer-assisted pronunciation training (CAPT), lexical tone recognition, automatic fluency assessment, accent recognition, and accented Mandarin speech recognition.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-148",
      "author_area_id": "12",
      "author_area_label": "Spoken Language Processing \u2013 Translation, Information Retrieval, and Resources"
    },
    "wong15_interspeech": {
      "authors": [
        [
          "Ka Ho",
          "Wong"
        ],
        [
          "Yu Ting",
          "Yeung"
        ],
        [
          "Edwin H. Y.",
          "Chan"
        ],
        [
          "Patrick C. M.",
          "Wong"
        ],
        [
          "Gina-Anne",
          "Levow"
        ],
        [
          "Helen",
          "Meng"
        ]
      ],
      "title": "Development of a Cantonese dysarthric speech corpus",
      "original": "i15_0329",
      "page_count": 5,
      "order": 153,
      "p1": "329",
      "pn": "333",
      "abstract": [
        "Dysarthria is a neurogenic communication disorder affecting speech production. Significant differences in phonemic inventories and phonological patterns across the world's languages render generalization of disordered speech patterns from one language (e.g, English) to another (e.g., Cantonese) difficult. Capitalizing on existing methods in developing English-language dysarthric speech corpora, we develop a Cantonese corpus in order to investigate articulatory and prosodic characteristics of Cantonese dysarthric speech, focusing on speaking rate and pitch and loudness control. Currently, we have collected 7.5 and 2.5 hours of speech data from 11 dysarthric subjects and 5 control speakers respectively. Our preliminary analysis reveals the characteristics of Cantonese dysarthric speech are consistent with general properties of motor speech disorders found in other languages.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-149",
      "author_area_id": "12",
      "author_area_label": "Spoken Language Processing \u2013 Translation, Information Retrieval, and Resources"
    },
    "arsikere15_interspeech": {
      "authors": [
        [
          "Harish",
          "Arsikere"
        ],
        [
          "Sonal",
          "Patil"
        ],
        [
          "Ranjeet",
          "Kumar"
        ],
        [
          "Kundan",
          "Shrivastava"
        ],
        [
          "Om",
          "Deshmukh"
        ]
      ],
      "title": "Stylex: a corpus of educational videos for research on speaking styles and their impact on engagement and learning",
      "original": "i15_0334",
      "page_count": 5,
      "order": 154,
      "p1": "334",
      "pn": "338",
      "abstract": [
        "In the context of learning through educational videos, the material chosen for a given topic must not only be relevant but also engaging to the consumer \u2014 ensuring better understanding and retention of content. This paper focuses on the speaking style of instructors, which is an important aspect driving student engagement. We present StyleX, a corpus of 450 1-minute video clips featuring 50 instructors, 10 topics in engineering and various accents of English. With the help of a large student population (304 in total), we study the impact of four speaking-style dimensions (liveliness, clarity, fluency and formality) on engagement and learning. Based on the in-classroom evaluations of 250 clips (> 20 simultaneous evaluators per clip), we find that liveliness and clarity are the most important dimensions (correlation with engagement and learning > 0.8), followed by fluency and formality. Familiarity with topics has a significant effect on the evaluators' ratings, while the instructors' accent and gender do not. StyleX represents the first large-scale effort of its kind in terms of the clip duration used and the number of topics, instructors and evaluators involved. This is also the first study, to our knowledge, on the explicit relationship between speaking style and engagement.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-150",
      "author_area_id": "12",
      "author_area_label": "Spoken Language Processing \u2013 Translation, Information Retrieval, and Resources"
    },
    "can15_interspeech": {
      "authors": [
        [
          "Do\u011fan",
          "Can"
        ],
        [
          "David C.",
          "Atkins"
        ],
        [
          "Shrikanth S.",
          "Narayanan"
        ]
      ],
      "title": "A dialog act tagging approach to behavioral coding: a case study of addiction counseling conversations",
      "original": "i15_0339",
      "page_count": 5,
      "order": 155,
      "p1": "339",
      "pn": "343",
      "abstract": [
        "Motivational Interviewing (MI) is a goal-oriented psychotherapy, employed in cases such as addiction, that helps clients explore and resolve their ambivalence about the problem at hand in a dialog setting. MI session quality is typically assessed with behavioral coding \u2014 a time consuming and labor intensive manual annotation system. This paper examines a computational approach to modeling and assessing the quality of MI sessions. Specifically, we pose the utterance level behavioral coding task as a sequence tagging problem and use linear chain CRF models trained on coded session transcripts and Switchboard DAMSL dataset to predict utterance level behavioral codes as well as dialog acts. We then use those utterance level predictions to predict session level behavioral codes of clinical interest characterizing the quality and efficacy of psychotherapy. We experiment with different feature parameterizations and reduced code sets and present an analysis of how standard dialog acts relate to behavioral codes.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-151",
      "author_area_id": "12",
      "author_area_label": "Spoken Language Processing \u2013 Translation, Information Retrieval, and Resources"
    },
    "vapnarsky15_interspeech": {
      "authors": [
        [
          "Valentina",
          "Vapnarsky"
        ],
        [
          "Claude",
          "Barras"
        ],
        [
          "C\u00e9dric",
          "Becquey"
        ],
        [
          "David",
          "Doukhan"
        ],
        [
          "Martine",
          "Adda-Decker"
        ],
        [
          "Lori",
          "Lamel"
        ]
      ],
      "title": "Analysing rhythm in ritual discourse in yucatec maya using automatic speech alignment",
      "original": "i15_0344",
      "page_count": 5,
      "order": 156,
      "p1": "344",
      "pn": "348",
      "abstract": [
        "Over the years, research in ethno-linguistics contributed to gather corpora in a wide range of languages, cultures and topics. In the present work, we are investigating ritual speech in Yucatec Maya. The ritual discourse tends to have a cyclic structure with repetitive patterns and various types of parallelisms between speech sections. Previous studies have revealed an intricate connexion between a speech's structure and vocal productions, in particular through temporal aspects including rhythm, pauses and durations of different speech sections. To further investigate our findings by relying more strongly on the acoustic recordings, automatic speech recognition tools may become of great help, in particular to test various linguistic and ethnolinguistic hypotheses. Unfortunately, Yucatec Maya, with less than one million native speakers, is an under-resourced language with respect to digital resources. As a total, 24 minutes of ritual speech from three performances were manually transcribed by expert linguists in Yucatec and a basic pronunciation dictionary for Yucatec was created accordingly. The transcribed acoustic recordings were then automatically time-aligned on a phonetic and lexical basis. Automatic segmentations were used to measure tempo changes, durations of breath units as well as to examine their link with the structure of the ritual text.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-152",
      "author_area_id": "12",
      "author_area_label": "Spoken Language Processing \u2013 Translation, Information Retrieval, and Resources"
    },
    "hasan15_interspeech": {
      "authors": [
        [
          "Madina",
          "Hasan"
        ],
        [
          "Rama",
          "Doddipatla"
        ],
        [
          "Thomas",
          "Hain"
        ]
      ],
      "title": "Noise-matched training of CRF based sentence end detection models",
      "original": "i15_0349",
      "page_count": 5,
      "order": 157,
      "p1": "349",
      "pn": "353",
      "abstract": [
        "Sentence end detection (SED) is an important task for many applications and has been studied on written text and automatic speech recognition (ASR) transcripts. In previous work it was shown that conditional random fields models gave best SED performance on a range of tasks, with and without the inclusion of prosodic features. So far, true transcripts were used for both training and evaluation of SED models. However, in the context of noisy ASR transcripts the performance degrades significantly, especially at medium to high ASR error rates. In this work we demonstrate the correlation of SED performance with word error rate (WER), at different ASR system performance levels. A new method is introduced for transferring SED labels onto noisy ASR transcripts for model training of noise-matched SED models. The proposed method significantly improves the performance of SED models, and provides 11% relative gain in slot error rate when compared with models trained on true transcripts. This paper further investigates the effect of noise-matched trained SED with different features. It is observed that the impact of textual features reduces significantly with low ASR performance. However, prosodic features still have noticeable impact.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-153"
    },
    "kuang15_interspeech": {
      "authors": [
        [
          "Jianjing",
          "Kuang"
        ],
        [
          "Mark",
          "Liberman"
        ]
      ],
      "title": "The effect of spectral slope on pitch perception",
      "original": "i15_0354",
      "page_count": 5,
      "order": 158,
      "p1": "354",
      "pn": "358",
      "abstract": [
        "This study aims to explore whether listeners integrate spectral cues in pitch-range perceptions. A forced-choice pitch classification experiment with four spectral conditions was conducted to investigate whether spectral cue manipulation can affect pitch-height perceptions. The participants in this experiment include tonal vs. non-tonal language speakers and musicians vs. non-musicians. The results show that the pitch classification function significantly shifted under different spectral conditions. Listeners generally hear higher pitches when the spectrum includes more high-frequency energy (i.e., tenser phonation). This study strongly supports the hypothesis that voice quality cues and F0 interact in pitch perceptions. Moreover, language experience and musical training can affect the magnitude of shifts.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-154",
      "author_area_id": "2",
      "author_area_label": "Phonetics, Phonology, and Prosody"
    },
    "bao15_interspeech": {
      "authors": [
        [
          "Honghao",
          "Bao"
        ],
        [
          "Wenhuan",
          "Lu"
        ],
        [
          "Kiyoshi",
          "Honda"
        ],
        [
          "Jianguo",
          "Wei"
        ],
        [
          "Qiang",
          "Fang"
        ],
        [
          "Jianwu",
          "Dang"
        ]
      ],
      "title": "Combined cine- and tagged-MRI for tracking landmarks on the tongue surface",
      "original": "i15_0359",
      "page_count": 5,
      "order": 159,
      "p1": "359",
      "pn": "363",
      "abstract": [
        "Magnetic resonance imaging (MRI) techniques have been a promising way in recent speech production studies, and dynamic magnetic imaging with repetitive or real-time MRI scans has been widely used to acquire motions of all the articulators and measure their deformation during speech. While MRI is capable to visualize the entire surfaces of those organs, it lacks landmarks for motion tracking that are available with other techniques such as magnetic sensing methods. One possible solution to have both surface contours and landmarks of the articulators is to combine different imaging techniques. In this paper, we propose a new method to add surface markers on the dynamic MRI of the tongue by combining cine- and tagged-MRI data together. To do so, analysis was done on the images from the two types of scans conducted in the same session. The intersection points of the tag lines with the tongue surface contour were extracted from tagged-MRI data and they were mapped onto cine-MRI data. After minimizing the minute mapping errors, the result showed that marker tracking on both oral and pharyngeal surfaces of the tongue was successful.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-155",
      "author_area_id": "1",
      "author_area_label": "Speech Perception, Production, and Acquisition"
    },
    "barbier15_interspeech": {
      "authors": [
        [
          "Guillaume",
          "Barbier"
        ],
        [
          "Louis-Jean",
          "Bo\u00eb"
        ],
        [
          "Guillaume",
          "Captier"
        ],
        [
          "Rafael",
          "Laboissi\u00e8re"
        ]
      ],
      "title": "Human vocal tract growth: a longitudinal study of the development of various anatomical structures",
      "original": "i15_0364",
      "page_count": 5,
      "order": 160,
      "p1": "364",
      "pn": "368",
      "abstract": [
        "The growth of the head and neck and its components, including that of the vocal tract, is not homothetic but appears rather as an anamorphosis. The growth of various structures presents a phenomenon of heterochrony. Another important issue in vocal tract growth is sexual dimorphism. It was first claimed that sexual dimorphism appears at puberty, but a recent study has suggested that some prepubertal differences exist. To study these two phenomena, we used longitudinal radiographic data of sixty-eight typical subjects (966 radiographs, taken from 1 month to 25 years) and twelve fetuses (anatomical sections). In this study, we analyzed the growth curves and growth types of the hard and soft palate, the pharyngeal cavity and the estimated length of the whole vocal tract using non-linear mixed-effect models, in order to take advantage of our unique longitudinal dataset. Results indicate that most of the structures follow a neural/somatic growth type, while the pharyngeal cavity follows a more somatic growth type. As concerns sexual dimorphism, no prepubertal differences were found, suggesting that the sexual dimorphism is likely to begin at puberty. These results have implications for the acoustics of speech production during development and should lead to improvements in vocal tract growth modeling.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-156",
      "author_area_id": "1",
      "author_area_label": "Speech Perception, Production, and Acquisition"
    },
    "sivaraman15_interspeech": {
      "authors": [
        [
          "Ganesh",
          "Sivaraman"
        ],
        [
          "Vikramjit",
          "Mitra"
        ],
        [
          "Mark K.",
          "Tiede"
        ],
        [
          "Elliot",
          "Saltzman"
        ],
        [
          "Louis",
          "Goldstein"
        ],
        [
          "Carol",
          "Espy-Wilson"
        ]
      ],
      "title": "Analysis of coarticulated speech using estimated articulatory trajectories",
      "original": "i15_0369",
      "page_count": 5,
      "order": 161,
      "p1": "369",
      "pn": "373",
      "abstract": [
        "Speech acoustic patterns vary significantly as a result of coarticulation and lenition processes that are shaped by segmental context or by performance factors such as production rate and degree of casualness. The resultant acoustic variability continues to offer serious challenges for the development of automatic speech recognition (ASR) systems. Articulatory phonology provides a formalism to understand coarticulation through spatiotemporal changes in the patterns of underlying gestures. This paper studies the coarticulation occurring in certain fast spoken utterances using articulatory constriction tract-variables (TVs) estimated from acoustic features. The TV estimators are trained on the University of Wisconsin X-ray Microbeam (XRMB) database. The utterances analyzed are from a different corpus containing simultaneous acoustic and Electromagnetic Articulograph (EMA) data. Plots of the estimated TVs show that the estimation procedure successfully detected the articulatory constrictions even in the case of highly coarticulated utterances that a state-of-the-art phone recognition system failed to detect. These results highlight the potential of TV trajectory estimation methods for improving the performance of phone recognition systems, particularly when sounds are reduced or deleted.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-157",
      "author_area_id": "1",
      "author_area_label": "Speech Perception, Production, and Acquisition"
    },
    "barbier15b_interspeech": {
      "authors": [
        [
          "Guillaume",
          "Barbier"
        ],
        [
          "Pascal",
          "Perrier"
        ],
        [
          "Lucie",
          "M\u00e9nard"
        ],
        [
          "Yohan",
          "Payan"
        ],
        [
          "Mark K.",
          "Tiede"
        ],
        [
          "Joseph S.",
          "Perkell"
        ]
      ],
      "title": "Speech planning in 4-year-old children versus adults: acoustic and articulatory analyses",
      "original": "i15_0374",
      "page_count": 5,
      "order": 162,
      "p1": "374",
      "pn": "378",
      "abstract": [
        "This study investigates speech motor control in 4-year-old Canadian French children in comparison with adults. It focuses on measures of token-to-token variability in the production of isolated vowels and on anticipatory extra-syllabic coarticulation within V1-C-V2 sequences. Acoustic and ultrasound articulatory data were recorded. Acoustic data from 20 children and 10 adults have been analyzed. Thus far, ultrasound data have been analyzed from a subset of these participants: 6 children and 2 adults. In agreement with former studies, token-to-token variability was greater in children than in adults. Strong anticipation of V2 in V1 was found in all adults, but not in children. Most of the children showed no anticipation at all and some of them showed a small amount of anticipation along the antero-posterior dimension only, manifested in the acoustic F2 dimension. These results are interpreted as evidence for the immaturity of children's speech motor control from two perspectives: insufficiently stable motor control patterns for vowel production, and a lack of effectiveness in anticipating forthcoming gestures. In line with theories of optimal motor control, anticipatory coarticulation is assumed to be based on the use of internal models of the speech apparatus and the increasing maturation of these representations as speech develops.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-158",
      "author_area_id": "1",
      "author_area_label": "Speech Perception, Production, and Acquisition"
    },
    "kaburagi15_interspeech": {
      "authors": [
        [
          "Tokihiko",
          "Kaburagi"
        ]
      ],
      "title": "Morphological and acoustic analysis of the vocal tract using a multi-speaker volumetric MRI dataset",
      "original": "i15_0379",
      "page_count": 5,
      "order": 163,
      "p1": "379",
      "pn": "383",
      "abstract": [
        "The shape of the vocal tract was analyzed from both morphological and acoustic perspectives for ten male speakers of Japanese. A volumetric MRI (magnetic resonance imaging) measurement was performed while each speaker uttered each of the five Japanese vowels. The cross-sectional vocal-tract area function was computed from the MRI dataset and the resulting 50 vocal-tract shapes were analyzed statistically to determine the principal deformation patterns. A perturbation of the vocal-tract shape was then given for each vowel to examine the effect on the first and second formant frequencies. When the perturbation was given by changing the coefficient values of the first and second principal modes, a local region on the coefficient plane was observed where the formant change was small. In other words, this region was acoustically insensitive to the perturbation of the vocal-tract shape. When the vocal-tract shapes of the ten speakers were marked on the same plot, it was also found that marked vocal-tract shapes were located in the vicinity of the acoustically insensitive region. From these numerical investigations, it was considered how the individual differences in the vocal-tract shape can be resolved to generate phonetically relevant speech sounds.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-159",
      "author_area_id": "1",
      "author_area_label": "Speech Perception, Production, and Acquisition"
    },
    "skordilis15_interspeech": {
      "authors": [
        [
          "Zisis Iason",
          "Skordilis"
        ],
        [
          "Vikram",
          "Ramanarayanan"
        ],
        [
          "Louis",
          "Goldstein"
        ],
        [
          "Shrikanth S.",
          "Narayanan"
        ]
      ],
      "title": "Experimental assessment of the tongue incompressibility hypothesis during speech production",
      "original": "i15_0384",
      "page_count": 5,
      "order": 164,
      "p1": "384",
      "pn": "388",
      "abstract": [
        "The human tongue is an important organ for speech production. Its deformation and motion control the shape of the vocal tract significantly and thereby the acoustic properties of the speech signal produced. Thus, much effort in the speech research community has been directed towards its biomechanical modeling. A common assumption incorporated into many models of the human tongue is the tissue incompressibility hypothesis: the tongue is considered a muscular hydrostat and therefore its volume should remain constant regardless of its posture. To the best of our knowledge, experimental assessment of the constant volume hypothesis during actual speech production is limited. In this work, the aim is to experimentally assess the incompressibility hypothesis during actual speech production using a dataset of volumetric Magnetic Resonance (MR) images of 17 subjects sustaining contextualized continuants (27 continuants per subject). A seeded region growing based algorithm is used to segment the tongue and calculate its volume. Then, the intra-subject variability of the tongue volume along the different tongue postures is examined. Within the accuracy of our tongue volume measurements, our empirical results seem consistent with the incompressibility hypothesis.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-160",
      "author_area_id": "1",
      "author_area_label": "Speech Perception, Production, and Acquisition"
    },
    "fer15_interspeech": {
      "authors": [
        [
          "Radek",
          "F\u00e9r"
        ],
        [
          "Pavel",
          "Mat\u011bjka"
        ],
        [
          "Franti\u0161ek",
          "Gr\u00e9zl"
        ],
        [
          "Old\u0159ich",
          "Plchot"
        ],
        [
          "Jan",
          "\u010cernock\u00fd"
        ]
      ],
      "title": "Multilingual bottleneck features for language recognition",
      "original": "i15_0389",
      "page_count": 5,
      "order": 165,
      "p1": "389",
      "pn": "393",
      "abstract": [
        "In this paper, we investigate Multilingual Stacked Bottleneck Features (SBN) in language recognition domain. These features are extracted using bottleneck neural networks trained on data from multiple languages. Previous results have shown benefits of multilingual training of SBN feature extractor for speech recognition. Here we focus on its impact on language recognition. We present results obtained with monolingual and multilingual networks, and their fusions. Using multilingual features, we obtain 16% relative improvement on 3 s condition of NIST LRE09 dataset with respect to features trained on a single language.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-161",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "mccree15_interspeech": {
      "authors": [
        [
          "Alan",
          "McCree"
        ],
        [
          "Daniel",
          "Garcia-Romero"
        ]
      ],
      "title": "DNN senone MAP multinomial i-vectors for phonotactic language recognition",
      "original": "i15_0394",
      "page_count": 4,
      "order": 166,
      "p1": "394",
      "pn": "397",
      "abstract": [
        "Deep neural networks have recently shown great promise for language recognition. In particular, the expected counts of clustered context-dependent phone states (senones) can serve as a simple but effective phonotactic system. This paper introduces multinomial i-vectors applied to senone counts and shows that they work better than current PCA approaches. In addition, we show that a new approach using a standard normal prior and MAP multinomial i-vector estimation further improves performance, particularly for shorter test durations. Finally, we present a reduced-complexity version of Newton's method to greatly accelerate multinomial i-vector extraction. Experimental results on the NIST LRE11 task show that this approach performs significantly better than top-performing acoustic and phonotactic systems from that evaluation.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-162",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "song15_interspeech": {
      "authors": [
        [
          "Yan",
          "Song"
        ],
        [
          "Xinhai",
          "Hong"
        ],
        [
          "Bing",
          "Jiang"
        ],
        [
          "Ruilian",
          "Cui"
        ],
        [
          "Ian",
          "McLoughlin"
        ],
        [
          "Li-Rong",
          "Dai"
        ]
      ],
      "title": "Deep bottleneck network based i-vector representation for language identification",
      "original": "i15_0398",
      "page_count": 5,
      "order": 167,
      "p1": "398",
      "pn": "402",
      "abstract": [
        "This paper presents a unified i-vector framework for language identification (LID) based on deep bottleneck networks (DBN) trained for automatic speech recognition (ASR). The framework covers both front-end feature extraction and back-end modeling stages.The output from different layers of a DBN are exploited to improve the effectiveness of the i-vector representation through incorporating a mixture of acoustic and phonetic information. Furthermore, a universal model is derived from the DBN with a LID corpus. This is a somewhat inverse process to the GMM-UBM method, in which the GMM of each language is mapped from a GMM-UBM. Evaluations on specific dialect recognition tasks show that the DBN based i-vector can achieve significant and consistent performance gains over conventional GMM-UBM and DNN based i-vector methods [1][2]. The generalization capability of this framework is also evaluated using DBNs trained on Mandarin and English corpuses.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-163",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "lozanodiez15_interspeech": {
      "authors": [
        [
          "Alicia",
          "Lozano-Diez"
        ],
        [
          "Ruben",
          "Zazo-Candil"
        ],
        [
          "Javier",
          "Gonzalez-Dominguez"
        ],
        [
          "Doroteo T.",
          "Toledano"
        ],
        [
          "Joaquin",
          "Gonzalez-Rodriguez"
        ]
      ],
      "title": "An end-to-end approach to language identification in short utterances using convolutional neural networks",
      "original": "i15_0403",
      "page_count": 5,
      "order": 168,
      "p1": "403",
      "pn": "407",
      "abstract": [
        "In this work, we propose an end-to-end approach to the language identification (LID) problem based on Convolutional Deep Neural Networks (CDNNs). The use of CDNNs is mainly motivated by the ability they have shown when modeling speech signals, and their relatively low-cost with respect to other deep architectures in terms of number of free parameters. We evaluate different configurations in a subset of 8 languages within the NIST Language Recognition Evaluation 2009 Voice of America (VOA) dataset, for the task of short test durations (segments up to 3 seconds of speech). The proposed CDNN-based systems achieve comparable performances to our baseline i-vector system, while reducing drastically the number of parameters to tune (at least 100 times fewer parameters). Then, we combine these CDNN-based systems and the i-vector baseline with a simple fusion at score level. This combination outperforms our best standalone system (up to 11% of relative improvement in terms of EER).\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-164",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "hautamaki15_interspeech": {
      "authors": [
        [
          "Ville",
          "Hautam\u00e4ki"
        ],
        [
          "Sabato Marco",
          "Siniscalchi"
        ],
        [
          "Hamid",
          "Behravan"
        ],
        [
          "Valerio Mario",
          "Salerno"
        ],
        [
          "Ivan",
          "Kukanov"
        ]
      ],
      "title": "Boosting universal speech attributes classification with deep neural network for foreign accent characterization",
      "original": "i15_0408",
      "page_count": 5,
      "order": 169,
      "p1": "408",
      "pn": "412",
      "abstract": [
        "We have recently proposed a universal acoustic characterisation to foreign accent recognition, in which any spoken foreign accent was described in terms of a common set of fundamental speech attributes. Although experimental evidence demonstrated the feasibility of our approach, we believe that speech attributes, namely manner and place of articulation, can be better modelled by a deep neural network. In this work, we propose the use of deep neural network trained on telephone bandwidth material from different languages to improve the proposed universal acoustic characterisation. We demonstrate that deeper neural architectures enhance the attribute classification accuracy. Furthermore, we show that improvements in attribute classification carry over to foreign accent recognition by producing a 21% relative improvement over previous baseline on spoken Finnish, and a 5.8% relative improvement on spoken English.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-165",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "geng15_interspeech": {
      "authors": [
        [
          "Wang",
          "Geng"
        ],
        [
          "Jie",
          "Li"
        ],
        [
          "Shanshan",
          "Zhang"
        ],
        [
          "Xinyuan",
          "Cai"
        ],
        [
          "Bo",
          "Xu"
        ]
      ],
      "title": "Multilingual tandem bottleneck feature for language identification",
      "original": "i15_0413",
      "page_count": 5,
      "order": 170,
      "p1": "413",
      "pn": "417",
      "abstract": [
        "The deep bottleneck (BN) feature based ivector solution has been recognized as a popular pipeline for language identification (LID) recently. However, issues such as how to extract more effective BN features and how to fully utilize features extracted from deep neural networks (DNN) are still not well investigated. In this paper, these issues are empirically tackled by means as follows: First, two novel types of deep features, phone-discriminant and triphone-discriminate are extracted. Then, DNNs are trained both separately and jointly on multilingual corpuses to produce different BN features. Finally, tandem fashion on deep BN features is applied to build enhanced deep features. Experiment results show that systems built on top of tandem deep features obtain 19% and 42% relative equal error rate reduction on average on NIST LRE 2007 over the counterpart built on traditional deep BN features and the cepstral feature based LID system, respectively.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-166",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "asaei15_interspeech": {
      "authors": [
        [
          "Afsaneh",
          "Asaei"
        ],
        [
          "Milos",
          "Cernak"
        ],
        [
          "Herv\u00e9",
          "Bourlard"
        ]
      ],
      "title": "On compressibility of neural network phonological features for low bit rate speech coding",
      "original": "i15_0418",
      "page_count": 5,
      "order": 171,
      "p1": "418",
      "pn": "422",
      "abstract": [
        "Phonological features extracted by neural network have shown interesting potential for low bit rate speech vocoding. The time span of phonological features is wider than that of the phonetic features, and thus fewer frames need to be transmitted. Moreover, the binary nature of phonological features enables a higher compression ratio at minor quality cost.   In this paper, we study the compressibility and structured sparsity of the phonological features. We propose a compressive sampling framework for speech coding and sparse reconstruction for decoding prior to synthesis. Compressive sampling is found to be a principled way for compression in contrast to the conventional pruning approach; it leads to 50% reduction in the bit-rate for better or equal quality of the decoded speech. Furthermore, exploiting the structured sparsity and binary characteristic of these features have shown to enable very low bit-rate coding at 700 bps with negligible quality loss; this coding scheme imposes no latency. If we consider a latency of 256 ms for supra-segmental structures, the rate of 250-350 bps is achieved.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-167",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "lenarczyk15_interspeech": {
      "authors": [
        [
          "Micha\u0142",
          "Lenarczyk"
        ]
      ],
      "title": "Robust and accurate LSF location with laguerre method",
      "original": "i15_0423",
      "page_count": 5,
      "order": 172,
      "p1": "423",
      "pn": "427",
      "abstract": [
        "A new algorithm for finding line spectral frequencies, LSF, is introduced, based on Laguerre method of root approximation. The method allows to assuredly find all roots one by one without recourse to polynomial deflation, which allows approximation to a high precision. Error bounds can be estimated by approximating from two sides with added margin. An improved variant of Laguerre recursion scheme is proposed to deal with unfavourable starting points, resulting in faster convergence.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-168",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "issing15_interspeech": {
      "authors": [
        [
          "Jochen",
          "Issing"
        ],
        [
          "Nikolaus",
          "F\u00e4rber"
        ],
        [
          "Reinhard",
          "German"
        ]
      ],
      "title": "Interactivity-aware playout adaptation",
      "original": "i15_0428",
      "page_count": 5,
      "order": 173,
      "p1": "428",
      "pn": "432",
      "abstract": [
        "Adaptive Playout is a solution in IP-based communication clients to compensate network issues using a dynamic receiver buffer. Whereas small buffers provoke loss artefacts due to delayed packets, large buffers result in high delay and loss of interactivity. Existing Voice over IP clients balance the trade-off between low delay and loss artefacts based on empirical values. Most often the degree of interactivity is not taken into account and adaptation parameters remain the same for long monologues and lively discussions. We present a novel playout adaptation scheme that not only adapts to changing network conditions but also to conversational interactivity. Using an integrated approach we improve conversations with high interactivity significantly while at the same time preserve high audio quality. Using a full client implementation we verify our findings by conducting conversation tests and obtain gains of up to 0.9 Mean Opinion Score.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-169",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "issing15b_interspeech": {
      "authors": [
        [
          "Jochen",
          "Issing"
        ],
        [
          "Nikolaus",
          "F\u00e4rber"
        ],
        [
          "Reinhard",
          "German"
        ]
      ],
      "title": "Advanced time shrinking using a drop classifier based on codec features",
      "original": "i15_0433",
      "page_count": 5,
      "order": 174,
      "p1": "433",
      "pn": "437",
      "abstract": [
        "We present an integrated approach of full-band audio time scale modification for Voice over IP communication. The concept is based on a low complexity adaptive playout method that uses frame dropping and audio concealment for time shrinking and stretching, respectively. The existing version of this method is improved using a classifier that assists in choosing which audio frames can be dropped with the least subjective impact on audio quality. To maintain low complexity, we exclusively use audio signal features that are available in the audio codec. The classification of audio frames improves audio quality of the existing method without classification by 0.5 Mean Opinion Score points while requiring significantly less computational complexity by a factor of ca 10^4.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-170",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "hines15_interspeech": {
      "authors": [
        [
          "Andrew",
          "Hines"
        ],
        [
          "Eoin",
          "Gillen"
        ],
        [
          "Naomi",
          "Harte"
        ]
      ],
      "title": "Measuring and monitoring speech quality for voice over IP with POLQA, viSQOL and p.563",
      "original": "i15_0438",
      "page_count": 5,
      "order": 175,
      "p1": "438",
      "pn": "442",
      "abstract": [
        "There are many types of degradation which can occur in Voice over IP (VoIP) calls. Of interest in this work are degradations which occur independently of the codec, hardware or network in use. Specifically, their effect on the subjective and objective quality of the speech is examined. Since no dataset suitable for this purpose exists, a new dataset (TCD-VoIP) has been created and has been made publicly available. The dataset contains speech clips suffering from a range of common call quality degradations, as well as a set of subjective opinion scores on the clips from 24 listeners. The performances of three objective quality metrics: POLQA, ViSQOL and P.563, have been evaluated using the dataset. The results show that full reference metrics are capable of accurately predicting a variety of common VoIP degradations. They also highlight the outstanding need for a wideband, single-ended, no-reference metric to monitor accurately speech quality for degradations common in VoIP scenarios.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-171",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "gallardo15_interspeech": {
      "authors": [
        [
          "Laura Fern\u00e1ndez",
          "Gallardo"
        ],
        [
          "Sebastian",
          "M\u00f6ller"
        ]
      ],
      "title": "Towards the prediction of human speaker identification performance from measured speech quality",
      "original": "i15_0443",
      "page_count": 5,
      "order": 176,
      "p1": "443",
      "pn": "447",
      "abstract": [
        "Speech communication channels and their components (e.g. codecs) are generally designed for optimum perceived speech quality. However, transmission channels should also preserve principal speaker-specific characteristics that enable acceptable speaker identification performance by end listeners. This paper proposes a first step towards effective approaches for the prediction of the human speaker identification performance from instrumental quality measures. Correspondences between speech quality and speaker identification accuracy are shown by fitting linear curves to data points involving different channel transmissions. Narrowband, wideband, and super-wideband channels are considered, with other typically associated distortions. Our analyses show that Coloration, one of the perceptual quality dimensions, can be a better predictor of the human speaker identification performance than overall quality predictions in terms of Mean Opinion Scores. This suggests that the speaker-specific properties of the voice are mainly impaired by the distortion of frequency components in the transmission path.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-172",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "levit15_interspeech": {
      "authors": [
        [
          "M.",
          "Levit"
        ],
        [
          "Andreas",
          "Stolcke"
        ],
        [
          "R.",
          "Subba"
        ],
        [
          "S.",
          "Parthasarathy"
        ],
        [
          "S.",
          "Chang"
        ],
        [
          "S.",
          "Xie"
        ],
        [
          "T.",
          "Anastasakos"
        ],
        [
          "Benoit",
          "Dumoulin"
        ]
      ],
      "title": "Personalization of word-phrase-entity language models",
      "original": "i15_0448",
      "page_count": 5,
      "order": 177,
      "p1": "448",
      "pn": "452",
      "abstract": [
        "We continue our investigations of Word-Phrase-Entity (WPE) Language Models that unify words, phrases and classes, such as named entities, into a single probabilistic framework for the purpose of language modeling. In the present study we show how WPE LMs can be adapted to work in a personalized scenario where class definitions change from user to user or even from utterance to utterance. Compared to traditional class-based LMs in various conditions, WPE LMs exhibited comparable or better modeling potential without requiring pre-tagged training material. We also significantly scaled the experimental setup by widening the target domain, amplifying the amount of training material and increasing the number of classes.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-173",
      "author_area_id": "9",
      "author_area_label": "Speech Recognition \u2013 Architecture, Search, and Linguistic Components"
    },
    "kobayashi15_interspeech": {
      "authors": [
        [
          "Akio",
          "Kobayashi"
        ],
        [
          "Manon",
          "Ichiki"
        ],
        [
          "Takahiro",
          "Oku"
        ],
        [
          "Kazuo",
          "Onoe"
        ],
        [
          "Shoei",
          "Sato"
        ]
      ],
      "title": "Discriminative bilinear language modeling for broadcast transcriptions",
      "original": "i15_0453",
      "page_count": 5,
      "order": 178,
      "p1": "453",
      "pn": "457",
      "abstract": [
        "A discriminative bilinear language model (DBLM) estimated on the basis of Bayes risk minimization is described. The discriminative language model (DLM) is conventionally trained by using n-gram features. However, given a large amount of training data, the DLM is not necessarily trained efficiently because of the increasing number of unique features. In addition, though some of the n-grams share the same word sequences as contexts, the DLM never reflects this kind of information in that they are not designed to work in a coordinated manner. These disadvantages of utilizing n-gram features could lead to a loss of DLM robustness. We solve these issues by introducing a bilinear network structure to the features aimed at factorizing the contexts shared among the n-grams and estimating the model more robustly. In our proposed language modeling, all the model parameters, such as weight matrices, are estimated according to the objective based on the Bayes risk to be minimized on the training lattices. The experimental results show that our DBLM trained in the lightly-supervised manner significantly reduced the word error rate compared with that of the trigram LM, while the conventional DLM does not yield a significant reduction.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-174",
      "author_area_id": "9",
      "author_area_label": "Speech Recognition \u2013 Architecture, Search, and Linguistic Components"
    },
    "ma15b_interspeech": {
      "authors": [
        [
          "Xi",
          "Ma"
        ],
        [
          "Xiaoxi",
          "Wang"
        ],
        [
          "Dong",
          "Wang"
        ],
        [
          "Zhiyong",
          "Zhang"
        ]
      ],
      "title": "Recognize foreign low-frequency words with similar pairs",
      "original": "i15_0458",
      "page_count": 5,
      "order": 179,
      "p1": "458",
      "pn": "462",
      "abstract": [
        "Low-frequency words place a major challenge for automatic speech recognition (ASR). The probabilities of these words, which are often important name entities, are generally under-estimated by the language model (LM) due to their limited occurrences in the training data. Recently, we proposed a word-pair approach to deal with the problem, which borrows information of frequent words to enhance the probabilities of low-frequency words. This paper presents an extension to the word-pair method by involving multiple `predicting words' to produce better estimation for low-frequency words. We also employ this approach to deal with out-of-language words in the task of multi-lingual speech recognition.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-175",
      "author_area_id": "9",
      "author_area_label": "Speech Recognition \u2013 Architecture, Search, and Linguistic Components"
    },
    "masumura15_interspeech": {
      "authors": [
        [
          "Ryo",
          "Masumura"
        ],
        [
          "Taichi",
          "Asami"
        ],
        [
          "Takanobu",
          "Oba"
        ],
        [
          "Hirokazu",
          "Masataki"
        ],
        [
          "Sumitaka",
          "Sakauchi"
        ],
        [
          "Akinori",
          "Ito"
        ]
      ],
      "title": "Combinations of various language model technologies including data expansion and adaptation in spontaneous speech recognition",
      "original": "i15_0463",
      "page_count": 5,
      "order": 180,
      "p1": "463",
      "pn": "467",
      "abstract": [
        "This paper demonstrates combinations of various language model (LM) technologies simultaneously, not only modeling techniques but also those for training data expansion based on external language resources and unsupervised adaptation for spontaneous speech recognition. Although forming combinations of various LM technologies has been examined, previous works focused on only modeling techniques. In fact, the previous works did not consider other important functionalities in practical spontaneous language modeling; a use of external language resources and an unsupervised LM adaptation. Therefore, our examination employs not only manual transcriptions of target domain speech but also out-of-domain text resources for spontaneous language modeling. In addition, the unsupervised LM adaptation based on multi-pass decoding is aggressively introduced to the combination. Our experimental results show a significant word error rate reduction by combining various technologies compared to using each technology individually in Japanese spontaneous speech recognition task. Furthermore, we also reveal relationships between the technologies.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-176",
      "author_area_id": "9",
      "author_area_label": "Speech Recognition \u2013 Architecture, Search, and Linguistic Components"
    },
    "aleksic15_interspeech": {
      "authors": [
        [
          "Petar",
          "Aleksic"
        ],
        [
          "Mohammadreza",
          "Ghodsi"
        ],
        [
          "Assaf",
          "Michaely"
        ],
        [
          "Cyril",
          "Allauzen"
        ],
        [
          "Keith",
          "Hall"
        ],
        [
          "Brian",
          "Roark"
        ],
        [
          "David",
          "Rybach"
        ],
        [
          "Pedro",
          "Moreno"
        ]
      ],
      "title": "Bringing contextual information to google speech recognition",
      "original": "i15_0468",
      "page_count": 5,
      "order": 181,
      "p1": "468",
      "pn": "472",
      "abstract": [
        "In automatic speech recognition on mobile devices, very often what a user says strongly depends on the particular context he or she is in. The n-grams relevant to the context are often not known in advance. The context can depend on, for example, particular dialog state, options presented to the user, conversation topic, location, etc. Speech recognition of sentences that include these n-grams can be challenging, as they are often not well represented in a language model (LM) or even include out-of-vocabulary (OOV) words.   In this paper, we propose a solution for using contextual information to improve speech recognition accuracy. We utilize an on-the-fly rescoring mechanism to adjust the LM weights of a small set of n-grams relevant to the particular context during speech decoding.   Our solution handles out of vocabulary words. It also addresses efficient combination of multiple sources of context and it even allows biasing class based language models. We show significant speech recognition accuracy improvements on several datasets, using various types of contexts, without negatively impacting the overall system. The improvements are obtained in both offline and live experiments.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-177",
      "author_area_id": "9",
      "author_area_label": "Speech Recognition \u2013 Architecture, Search, and Linguistic Components"
    },
    "vasserman15_interspeech": {
      "authors": [
        [
          "Lucy",
          "Vasserman"
        ],
        [
          "Vlad",
          "Schogol"
        ],
        [
          "Keith",
          "Hall"
        ]
      ],
      "title": "Sequence-based class tagging for robust transcription in ASR",
      "original": "i15_0473",
      "page_count": 5,
      "order": 182,
      "p1": "473",
      "pn": "477",
      "abstract": [
        "We present a method of modeling non-lexical vocabulary items such as numbers, times, dates, monetary amounts and address components that avoids the data sparsity and out-of-vocabulary problems of written-domain language models. Like previous approaches, we use a class-based language model and efficient finite-state class grammars during run-time decoding. We mitigate the problem of context-independent replacement of class items by employing a contextual sequence labeling model to identify which class instances should be replaced, leaving others to appear in their original form. Applied to the task of general voice-search audio transcription, our method achieves 10% relative error reduction (on the numeric error rate metric) compared to the previous system (based on a verbalizer transducer). On a numeric entity recognition task, our method achieves a 23% relative error reduction on the same metric. In both cases, word error rate remains the same or is reduced.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-178",
      "author_area_id": "9",
      "author_area_label": "Speech Recognition \u2013 Architecture, Search, and Linguistic Components"
    },
    "schuller15_interspeech": {
      "authors": [
        [
          "Bj\u00f6rn",
          "Schuller"
        ],
        [
          "Stefan",
          "Steidl"
        ],
        [
          "Anton",
          "Batliner"
        ],
        [
          "Simone",
          "Hantke"
        ],
        [
          "Florian",
          "H\u00f6nig"
        ],
        [
          "J. R.",
          "Orozco-Arroyave"
        ],
        [
          "Elmar",
          "N\u00f6th"
        ],
        [
          "Yue",
          "Zhang"
        ],
        [
          "Felix",
          "Weninger"
        ]
      ],
      "title": "The INTERSPEECH 2015 computational paralinguistics challenge: nativeness, Parkinson's & eating condition",
      "original": "i15_0478",
      "page_count": 5,
      "order": 183,
      "p1": "478",
      "pn": "482",
      "abstract": [
        "The INTERSPEECH 2015 Computational Paralinguistics Challenge addresses three different problems for the first time in research competition under well-defined conditions: the estimation of the degree of nativeness, the neurological state of patients with Parkinson's condition, and the eating conditions of speakers, i.e., whether and which food type they are eating in a seven-class problem. In this paper, we describe these sub-challenges, their conditions, and the baseline feature extraction and classifiers, as provided to the participants.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-179",
      "author_area_id": "3",
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "honig15_interspeech": {
      "authors": [
        [
          "Florian",
          "H\u00f6nig"
        ]
      ],
      "title": "The degree of nativeness sub-challenge: the data",
      "original": "i15_4101",
      "page_count": 0,
      "order": 184,
      "p1": "0",
      "pn": "",
      "abstract": [
        "As an introduction to the degree of nativeness sub-challenge, we will give information on the speakers, the text material, and procedures that have been used to elicit speech. We will also give details on the annotation of the data, and comment on the partitioning used for development and test.\n",
        ""
      ]
    },
    "montacie15_interspeech": {
      "authors": [
        [
          "Claude",
          "Montaci\u00e9"
        ],
        [
          "Marie-Jos\u00e9",
          "Caraty"
        ]
      ],
      "title": "Phrase accentuation verification and phonetic variation measurement for the degree of nativeness sub-challenge",
      "original": "i15_0483",
      "page_count": 5,
      "order": 185,
      "p1": "483",
      "pn": "487",
      "abstract": [
        "The Degree of Nativeness Sub-Challenge consists in the automatic grading of the pronunciation quality of non-native English utterances. In this paper, we investigate the phrase accentuation and the phonetic acoustic variability for the prediction of the grades. Two prediction systems have been developed: the Extended Baseline System (EBS) and the Pronunciation Feature based System (PFS). The EBS system was designed to take into account the cross-corpus specificities such as recording conditions and the sentence variability. The speech files were segmented using Automatic Speech Recognition methods (ASR). Audio features were selected on both the training and development sets using the Regressional ReliefF method. New audio features were developed for the PFS system to take into account the mispronunciations: unusual prosody and/or phonetic variation. These systems have been assessed using the Spearman's correlation coefficient with expert annotations. The PFS system has significantly improved of 0.05 the Official Baseline performance.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-180",
      "author_area_id": "3",
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "ribeiro15_interspeech": {
      "authors": [
        [
          "Eug\u00e9nio",
          "Ribeiro"
        ],
        [
          "Jaime",
          "Ferreira"
        ],
        [
          "Julia",
          "Olcoz"
        ],
        [
          "Alberto",
          "Abad"
        ],
        [
          "Helena",
          "Moniz"
        ],
        [
          "Fernando",
          "Batista"
        ],
        [
          "Isabel",
          "Trancoso"
        ]
      ],
      "title": "Combining multiple approaches to predict the degree of nativeness",
      "original": "i15_0488",
      "page_count": 5,
      "order": 186,
      "p1": "488",
      "pn": "492",
      "abstract": [
        "Automatic speaker nativeness assessment has multiple applications, such as second language learning and IVR systems. In this paper we view this as a regression problem, since the available labels are on a continuous scale. Multiple approaches were applied, such as phonotactic models, i-vectors, and goodness of pronunciation, covering both segmental and suprasegmental features. Different phonotactic models were adopted, either trained with the challenge data, or using additional multilingual data from other domains. The obtained values were later combined in multiple ways and fed to a support vector machine regressor. Results on the test set surpass the provided baseline and are in line with the results obtained on the remaining sets. This suggests that our models generalize well to other datasets.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-181",
      "author_area_id": "3",
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "black15_interspeech": {
      "authors": [
        [
          "Matthew P.",
          "Black"
        ],
        [
          "Daniel",
          "Bone"
        ],
        [
          "Zisis Iason",
          "Skordilis"
        ],
        [
          "Rahul",
          "Gupta"
        ],
        [
          "Wei",
          "Xia"
        ],
        [
          "Pavlos",
          "Papadopoulos"
        ],
        [
          "Sandeep Nallan",
          "Chakravarthula"
        ],
        [
          "Bo",
          "Xiao"
        ],
        [
          "Maarten Van",
          "Segbroeck"
        ],
        [
          "Jangwon",
          "Kim"
        ],
        [
          "Panayiotis G.",
          "Georgiou"
        ],
        [
          "Shrikanth S.",
          "Narayanan"
        ]
      ],
      "title": "Automated evaluation of non-native English pronunciation quality: combining knowledge- and data-driven features at multiple time scales",
      "original": "i15_0493",
      "page_count": 5,
      "order": 187,
      "p1": "493",
      "pn": "497",
      "abstract": [
        "Automatically evaluating pronunciation quality of non-native speech has seen tremendous success in both research and commercial settings, with applications in L2 learning. In this paper, submitted for the INTERSPEECH 2015 Degree of Nativeness Sub-Challenge, this problem is posed under a challenging cross-corpora setting using speech data drawn from multiple speakers from a variety of language backgrounds (L1) reading different English sentences. Since the perception of non-nativeness is realized at the segmental and suprasegmental linguistic levels, we explore a number of acoustic cues at multiple time scales. We experiment with both data-driven and knowledge-inspired features that capture degree of nativeness from pauses in speech, speaking rate, rhythm/stress, and goodness of phone pronunciation. One promising finding is that highly accurate automated assessment can be attained using a small diverse set of intuitive and interpretable features. Performance is further boosted by smoothing scores across utterances from the same speaker; our best system significantly outperforms the challenge baseline.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-182",
      "author_area_id": "3",
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "orozcoarroyave15b_interspeech": {
      "authors": [
        [
          "J. R.",
          "Orozco-Arroyave"
        ]
      ],
      "title": "The Parkinson's condition sub-challenge: the data",
      "original": "i15_4102",
      "page_count": 0,
      "order": 188,
      "p1": "0",
      "pn": "",
      "abstract": [
        "A general description of the speech tasks and the Parkinson's patients recorded to build the database is provided. Details of the speakers in the test subset will also be given.\n",
        ""
      ]
    },
    "sztaho15_interspeech": {
      "authors": [
        [
          "D\u00e1vid",
          "Sztah\u00f3"
        ],
        [
          "G\u00e1bor",
          "Kiss"
        ],
        [
          "Kl\u00e1ra",
          "Vicsi"
        ]
      ],
      "title": "Estimating the severity of Parkinson's disease from speech using linear regression and database partitioning",
      "original": "i15_0498",
      "page_count": 5,
      "order": 189,
      "p1": "498",
      "pn": "502",
      "abstract": [
        "Parkinson's disease (PD) is one of the most common neurodegenerative disorders. PD is referred as idiopathic, that is, as having no known cause; its main symptoms are tremor, rigidity and general loss of muscle control. Research shows that speech may be a useful indicator for discriminating patients with PD from healthy controls. The paper describes our contribution to the INTERSPEECH 2015 Special Session \u201cComputational Paralinguistics Challenge (ComParE): Parkinson's Condition Sub-Challenge\u201d. The main goal of the challenge is to perform automatic classification (regression) on speech produced by patients with Parkinson's disease. The paper presents our method of linear regression models on a set of extracted acoustic features from the middle of vowels in words, sentences and continuous speech, and the partitioning of the speech samples according to their total length into parts with long, medium and short duration.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-183",
      "author_area_id": "3",
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "zlotnik15_interspeech": {
      "authors": [
        [
          "Alexander",
          "Zlotnik"
        ],
        [
          "Juan M.",
          "Montero"
        ],
        [
          "Rub\u00e9n",
          "San-Segundo"
        ],
        [
          "Ascensi\u00f3n",
          "Gallardo-Antol\u00edn"
        ]
      ],
      "title": "Random forest-based prediction of Parkinson's disease progression using acoustic, ASR and intelligibility features",
      "original": "i15_0503",
      "page_count": 5,
      "order": 190,
      "p1": "503",
      "pn": "507",
      "abstract": [
        "The Interspeech ComParE 2015 PC Sub-Challenge consists of automatically determining the degree of Parkinson's condition using exclusively the patient's voice. In this paper, we face this problem as a regression task and in order to succeed, we propose the use of an ensemble learning method, Random Forest (RF), in combination with features of different nature: acoustic characteristics, features derived from the output of an Automatic Speech Recognition system (ASR) and non-intrusive intelligibility measures. The system outperforms the baseline results achieving a relative improvement higher than 19% in the development set.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-184",
      "author_area_id": "3",
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "an15_interspeech": {
      "authors": [
        [
          "Guozhen",
          "An"
        ],
        [
          "David Guy",
          "Brizan"
        ],
        [
          "Min",
          "Ma"
        ],
        [
          "Michelle",
          "Morales"
        ],
        [
          "Ali Raza",
          "Syed"
        ],
        [
          "Andrew",
          "Rosenberg"
        ]
      ],
      "title": "Automatic recognition of unified Parkinson's disease rating from speech with acoustic, i-vector and phonotactic features",
      "original": "i15_0508",
      "page_count": 5,
      "order": 191,
      "p1": "508",
      "pn": "512",
      "abstract": [
        "Parkinson's Disease is a neurodegenerative disease affecting millions of people globally, most of whom present difficulties producing speech sounds. In this paper, we describe a system to identify the degree to which a person suffers from the disease. We use a number of automatic phone recognition-based features and we augment these with i-vector features and utterance-level acoustic aggregations. On the Interspeech 2015 ComParE challenge corpus, we find that these features allow for prediction well above the challenge baseline, particularly under cross-validation evaluation.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-185",
      "author_area_id": "3",
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "hahm15_interspeech": {
      "authors": [
        [
          "Seongjun",
          "Hahm"
        ],
        [
          "Jun",
          "Wang"
        ]
      ],
      "title": "Parkinson's condition estimation using speech acoustic and inversely mapped articulatory data",
      "original": "i15_0513",
      "page_count": 5,
      "order": 192,
      "p1": "513",
      "pn": "517",
      "abstract": [
        "Parkinson's disease is a neurological disorder that affects patient's motor function including speech articulation. There is no cure for Parkinson's disease. Speech and motor function declines as the disease progresses. Automatic assessment of the disease condition may advance the treatment of Parkinson's disease with objective, inexpensive measures. Speech acoustics, which can be easily obtained from patients, has been used for automatic assessment. The use of information in motor function of articulator (e.g., jaw, tongue, or lips) has rarely been investigated. In this paper, we proposed an approach of automatic assessment of Parkinson's condition using both acoustic data and acoustically-inverted articulatory data. The quasi-articulatory features were obtained from the Parkinson's acoustic speech data using acoustic-to-articulatory inverse mapping. Support vector regression (SVR) and deep neural network (DNN) regression were used in the experiment. Results indicated adding articulatory data to acoustic data can improve the performance of using acoustic data only, for both SVR and DNN. In addition, deep neural network outperformed support vector regression on the same data features measured with Pearson correlation but not with Spearman correlation. The implications of our approach with further improvement were discussed.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-186",
      "author_area_id": "3",
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "williamson15_interspeech": {
      "authors": [
        [
          "James R.",
          "Williamson"
        ],
        [
          "Thomas F.",
          "Quatieri"
        ],
        [
          "Brian S.",
          "Helfer"
        ],
        [
          "Joseph",
          "Perricone"
        ],
        [
          "Satrajit S.",
          "Ghosh"
        ],
        [
          "Gregory",
          "Ciccarelli"
        ],
        [
          "Daryush D.",
          "Mehta"
        ]
      ],
      "title": "Segment-dependent dynamics in predicting Parkinson's disease",
      "original": "i15_0518",
      "page_count": 5,
      "order": 193,
      "p1": "518",
      "pn": "522",
      "abstract": [
        "Early, accurate detection of Parkinson's disease may aid in possible intervention and rehabilitation. Thus, simple noninvasive biomarkers are desired for determining severity. In this study, a novel set of acoustic speech biomarkers are introduced and fused with conventional features for predicting clinical assessment of Parkinson's disease. We introduce acoustic biomarkers reflecting the segment dependence of changes in speech production components, motivated by disturbances in underlying neural motor, articulatory, and prosodic brain centers of speech. Such changes occur at phonetic and larger time scales, including multi-scale perturbations in formant frequency and pitch trajectories, in phoneme durations and their frequency of occurrence, and in temporal waveform structure. We also introduce articulatory features based on a neural computational model of speech production, the Directions into Velocities of Articulators (DIVA) model. The database used is from the Interspeech 2015 Computational Paralinguistic Challenge. By fusing conventional and novel speech features, we obtain Spearman correlations between predicted scores and clinical assessments of r = 0.63 on the training set (four-fold cross validation), r = 0.70 on a held-out development set, and r = 0.97 on a held-out test set.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-187",
      "author_area_id": "3",
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "batliner15_interspeech": {
      "authors": [
        [
          "Anton",
          "Batliner"
        ]
      ],
      "title": "The eating condition sub-challenge: the data",
      "original": "i15_4103",
      "page_count": 0,
      "order": 194,
      "p1": "0",
      "pn": "",
      "abstract": [
        "The automatic processing of the acoustics of eating in connection with speech (eating while speaking) is a fairly new topic. Thus, we will start by giving some examples of possible and promising applications. We then introduce the database used for this sub-challenge (design, recording, partitioning into train and development (cross-validation) and test sets.\n",
        ""
      ]
    },
    "prasad15_interspeech": {
      "authors": [
        [
          "Abhay",
          "Prasad"
        ],
        [
          "Prasanta Kumar",
          "Ghosh"
        ]
      ],
      "title": "Automatic classification of eating conditions from speech using acoustic feature selection and a set of hierarchical support vector machine classifiers",
      "original": "i15_0884",
      "page_count": 5,
      "order": 195,
      "p1": "884",
      "pn": "888",
      "abstract": [
        "The problem of automatic classification of seven types of eating conditions from speech is considered. Based on the confusion among different eating conditions from a seven class support vector machine (SVM) classifier, a hierarchical SVM classifier is designed. Experiments on the iHEARu-EAT database show that the hierarchical classifier results in a better classification accuracy compared to a seven class classifier. We also perform a feature selection for each of the classifiers in the hierarchical approach. This further improves the unweighted average recall (UAR) to 73.7% compared to an UAR of 60.9% obtained from the baseline scheme of a direct seven-way classification.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-188",
      "author_area_id": "3",
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "wagner15_interspeech": {
      "authors": [
        [
          "Johannes",
          "Wagner"
        ],
        [
          "Andreas",
          "Seiderer"
        ],
        [
          "Florian",
          "Lingenfelser"
        ],
        [
          "Elisabeth",
          "Andr\u00e9"
        ]
      ],
      "title": "Combining hierarchical classification with frequency weighting for the recognition of eating conditions",
      "original": "i15_0889",
      "page_count": 5,
      "order": 196,
      "p1": "889",
      "pn": "893",
      "abstract": [
        "Though parents regularly remind their children not to do so, talking while eating is a typical everyday situation automatic speech analysis systems should be able to deal with. The Paralinguistic Eating Condition (EC) Challenge at Interspeech 2015 sets the task to classify whether a speaker is eating or not, and if so, which type of food the speaker is currently tasting. The approach we follow in this paper is rather unusual: instead of suppressing the influence of noise to enhance the intelligibility of a spoken message, we try to emphasize the noisy parts of the spectrum to improve the recognition of food classes. To allow for a fine-grained adaption to the characteristic spectrum of single food types we adopt a hierarchical tree structure and decompose the classification task into a sequence of binary decisions. At each node we apply frequency-dependent weighting to tune the spectrum to the involved target classes. With our approach we are able to improve results in a 7-class recognition problem (6 types of food and no food) by more than 7% on the training set (using leave-one-eater-out cross validation) and 4% on the test set, respectively.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-189",
      "author_area_id": "3",
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "pir15_interspeech": {
      "authors": [
        [
          "Dara",
          "Pir"
        ],
        [
          "Theodore",
          "Brown"
        ]
      ],
      "title": "Acoustic group feature selection using wrapper method for automatic eating condition recognition",
      "original": "i15_0894",
      "page_count": 5,
      "order": 197,
      "p1": "894",
      "pn": "898",
      "abstract": [
        "In this paper, we present a wrapper-based acoustic group feature selection system for the INTERSPEECH 2015 Computational Paralinguistics Challenge (ComParE) 2015, Eating Condition (EC) Sub-challenge. The wrapper-based method has two components: the feature subset evaluation and the feature space search. The feature subset evaluation is performed using Support Vector Machine (SVM) classifiers. The wrapper method combined with complex algorithms such as SVM is computationally intensive. To address this, the feature space search uses Best Incremental Ranked Subset (BIRS), a fast and efficient algorithm. Moreover, we investigate considering the feature space in meaningful groups rather than individually. The acoustic feature space is partitioned into groups with each group representing a Low Level Descriptor (LLD). This partitioning reduces the time complexity of the search algorithm and makes the problem more tractable while attempting to gain insight into the relevant acoustic feature groups. Our wrapper-based system achieves improvement over the challenge baseline on the EC Sub-challenge test set using a variant of BIRS algorithm and LLD groups.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-190",
      "author_area_id": "3",
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "pellegrini15_interspeech": {
      "authors": [
        [
          "Thomas",
          "Pellegrini"
        ]
      ],
      "title": "Comparing SVM, softmax, and shallow neural networks for eating condition classification",
      "original": "i15_0899",
      "page_count": 5,
      "order": 198,
      "p1": "899",
      "pn": "903",
      "abstract": [
        "This paper reports experiments on Eating Condition (EC) classification in the context of the INTERSPEECH 2015 Paralinguistic EC sub-challenge. Several techniques were compared: Support Vector Machines, Softmax classifiers and single hidden-layer neural nets using the ReLu activation function. Although eating noise and speech overlap in the recordings most of the time, performance improvements were obtained with all the tested techniques, by using the baseline features augmented with the same features but extracted on audio frames with low energy only. This led to a total of 12K features. With the Softmax classifier, for instance, UAR increased from 58.3% to 64.3% in the Leave-One-Speaker-Out (LOSO) cross-validation configuration. As expected, the `Biscuit' and `Crisp' categories benefited the most from using low-energy frames, with UAR improvements between 10% and 15% absolute. Indeed, these noises are high-frequency noises with low energy. SVM and Softmax showed similar performance, with Softmax slightly outperforming SVMs. Our best performance of 68.4% UAR on the test set was obtained by averaging the scores of several neural nets trained in the LOSO configuration. We also report a performance comparison of three different weight update rules used with batch gradient descent: the sgd, momentum and rmsprop rules.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-191",
      "author_area_id": "3",
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "milde15_interspeech": {
      "authors": [
        [
          "Benjamin",
          "Milde"
        ],
        [
          "Chris",
          "Biemann"
        ]
      ],
      "title": "Using representation learning and out-of-domain data for a paralinguistic speech task",
      "original": "i15_0904",
      "page_count": 5,
      "order": 199,
      "p1": "904",
      "pn": "908",
      "abstract": [
        "In this work, we study the paralinguistic speech task of eating condition classification and present our submitted classification system for the INTERSPEECH 2015 Computational Paralinguistics challenge. We build upon a deep learning language identification system, which we repurpose for general audio sequence classification. The main idea is that we train local convolutional neural network classifiers that automatically learn representations on smaller windows of the full sequence's spectrum and to aggregate multiple local classifications towards a full sequence classification. A particular challenge of the task is training data scarcity and the resulting overfitting of neural network methods, which we tackle with dropout, synthetic data augmentation and transfer learning with out-of-domain data from a language identification task. Our final submitted system achieved an UAR score of 75.9% for 7-way eating condition classification, which is a relative improvement of 15% over the baseline.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-192",
      "author_area_id": "3",
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "kaya15_interspeech": {
      "authors": [
        [
          "Heysem",
          "Kaya"
        ],
        [
          "Alexey A.",
          "Karpov"
        ],
        [
          "Albert Ali",
          "Salah"
        ]
      ],
      "title": "Fisher vectors with cascaded normalization for paralinguistic analysis",
      "original": "i15_0909",
      "page_count": 5,
      "order": 200,
      "p1": "909",
      "pn": "913",
      "abstract": [
        "Computational Paralinguistics has several unresolved issues, one of which is coping with large variability due to speakers, spoken content and corpora. In this paper, we address the variability compensation issue by proposing a novel method composed of i) Fisher vector encoding of low level descriptors extracted from the signal, ii) speaker z-normalization applied after speaker clustering iii) non-linear normalization of features and iv) classification based on Kernel Extreme Learning Machines and Partial Least Squares regression. For experimental validation, we apply the proposed method on INTERSPEECH 2015 Computational Paralinguistics Challenge (ComParE 2015), Eating Condition sub-challenge, which is a seven-class classification task. In our preliminary experiments, the proposed method achieves an Unweighted Average Recall (UAR) score of 83.1%, outperforming the challenge test set baseline UAR (65.9%) by a large margin.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-193",
      "author_area_id": "3",
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "kim15_interspeech": {
      "authors": [
        [
          "Jangwon",
          "Kim"
        ],
        [
          "Md.",
          "Nasir"
        ],
        [
          "Rahul",
          "Gupta"
        ],
        [
          "Maarten Van",
          "Segbroeck"
        ],
        [
          "Daniel",
          "Bone"
        ],
        [
          "Matthew P.",
          "Black"
        ],
        [
          "Zisis Iason",
          "Skordilis"
        ],
        [
          "Zhaojun",
          "Yang"
        ],
        [
          "Panayiotis G.",
          "Georgiou"
        ],
        [
          "Shrikanth S.",
          "Narayanan"
        ]
      ],
      "title": "Automatic estimation of Parkinson's disease severity from diverse speech tasks",
      "original": "i15_0914",
      "page_count": 5,
      "order": 201,
      "p1": "914",
      "pn": "918",
      "abstract": [
        "The need for reliable, scalable and efficient diagnosis of Parkinson's Disease (PD) is a major clinical need. Automating the diagnosis can lead to more accurate and objective predictions as well as provide insights regarding the nature of Parkinson's condition. This paper proposes a fully automated system to rate the severity (UPDRS-III scale) of PD from patients' speech. Specifically, the system captures atypicalities in an individual's voice when performing multiple diverse speaking tasks and makes a unified prediction of the PD severity. The performance is tested in a cross-data setting, with different subjects and dissimilar recording conditions. Results indicate that (i) effective features vary depending on the nature of the specific speech task, (ii) additional novel feature sets to detect distortions in Parkinson's speech significantly improve the prediction accuracy from the Interspeech15 Challenge baseline system and (iii) our fusion system based on an unsupervised clustering technique also improves the accuracy. Our system incorporates i-vector and functionals for segmental features, non-linear time series features, speech rhythm and automatic speech recognition decoding based features. By its application on the Interspeech15 eating condition challenge, the system also shows its potential for detecting other sources of speech variability.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-194",
      "author_area_id": "3",
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "grosz15_interspeech": {
      "authors": [
        [
          "Tam\u00e1s",
          "Gr\u00f3sz"
        ],
        [
          "R\u00f3bert",
          "Busa-Fekete"
        ],
        [
          "G\u00e1bor",
          "Gosztolya"
        ],
        [
          "L\u00e1szl\u00f3",
          "T\u00f3th"
        ]
      ],
      "title": "Assessing the degree of nativeness and Parkinson's condition using Gaussian processes and deep rectifier neural networks",
      "original": "i15_0919",
      "page_count": 5,
      "order": 202,
      "p1": "919",
      "pn": "923",
      "abstract": [
        "The Interspeech 2015 Computational Paralinguistics Challenge includes two regression learning tasks, namely the Parkinson's Condition Sub-Challenge and the Degree of Nativeness Sub-Challenge. We evaluated two state-of-the-art machine learning methods on the tasks, namely Deep Neural Networks (DNN) and Gaussian Processes Regression (GPR).We also experimented with various classifier combination and feature selection methods. For the Degree of Nativeness sub-challenge we obtained a far better Spearman correlation value than the one presented in the baseline paper. As regards the Parkinson's Condition Sub-Challenge, we showed that both DNN and GPR are competitive with the baseline SVM, and that the results can be improved further by combining the classifiers. However, we obtained by far the best results when we applied a speaker clustering method to identify the files that belong to the same speaker.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-195",
      "author_area_id": "3",
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "steidl15_interspeech": {
      "authors": [
        [
          "Stefan",
          "Steidl"
        ]
      ],
      "title": "The INTERSPEECH 2015 computational paralinguistics challenge: a summary of results",
      "original": "i15_4104",
      "page_count": 0,
      "order": 203,
      "p1": "0",
      "pn": "",
      "abstract": [
        "During the two special sessions at INTERSPEECH 2015, all participants of the ComParE 2015 Challenge get the chance to presented their systems and how they approached the classification and regression tasks. At the end of the second session, we will give an overview of the submitted results to the three Sub-Challenges on the official test set and will present the winner for each Sub-Challenge.\n",
        ""
      ]
    },
    "batliner15b_interspeech": {
      "authors": [
        [
          "Anton",
          "Batliner"
        ]
      ],
      "title": "Wrapping up: the story of the compare challenges, what we learned and where to go",
      "original": "i15_4105",
      "page_count": 0,
      "order": 204,
      "p1": "0",
      "pn": "",
      "abstract": [
        "We will try to give a birds' eye view of the present challenge against the background of past challenges, and discuss possible ways to go for future challenges.\n",
        ""
      ]
    },
    "houghton15_interspeech": {
      "authors": [
        [
          "S. M.",
          "Houghton"
        ],
        [
          "Colin J.",
          "Champion"
        ],
        [
          "Philip",
          "Weber"
        ]
      ],
      "title": "Recognition of voiced sounds with a continuous state HMM",
      "original": "i15_0523",
      "page_count": 5,
      "order": 205,
      "p1": "523",
      "pn": "527",
      "abstract": [
        "Many current speech recognition systems use very large statistical models using many thousands, perhaps millions, of parameters to account for variability in speech signals observed in large training corpora, and represent speech as sequences of discrete, independent events. The mechanisms of speech production are, however, conceptually very simple and involve continuous smooth movement of a small number of speech articulators. We report progress towards a practical implementation of a parsimonious continuous state hidden Markov model for recovery of voiced phoneme sequences from trajectories of such continuous, dynamic speech production features, using of the order of several hundred parameters. We describe automated training of the parameters using a forced alignment procedure, and results for training and testing on an individual speaker.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-196",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition \u2013 Signal Processing, Acoustic Modeling, Robustness, and Adaptation"
    },
    "zeng15_interspeech": {
      "authors": [
        [
          "Xiangyu",
          "Zeng"
        ],
        [
          "Shi",
          "Yin"
        ],
        [
          "Dong",
          "Wang"
        ]
      ],
      "title": "Learning speech rate in speech recognition",
      "original": "i15_0528",
      "page_count": 5,
      "order": 206,
      "p1": "528",
      "pn": "532",
      "abstract": [
        "A significant performance reduction is often observed in speech recognition when the rate of speech (ROS) is too low or too high. Most of present approaches to addressing the ROS variation focus on the change of speech signals in dynamic properties caused by ROS, and accordingly modify the dynamic model, e.g., the transition probabilities of the hidden Markov model (HMM). However, an abnormal ROS changes not only the dynamic but also the static property of speech signals, and thus can not be compensated for purely by modifying the dynamic model.   This paper proposes an ROS learning approach based on deep neural networks (DNN), which involves an ROS feature as the input of the DNN model and so the spectrum distortion caused by ROS can be learned and compensated for. The experimental results show that this approach can deliver better performance for too slow and too fast utterances, demonstrating our conjecture that ROS impacts both the dynamic and the static property of speech. In addition, the proposed approach can be combined with the conventional HMM transition adaptation method, offering additional performance gains.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-197",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition \u2013 Signal Processing, Acoustic Modeling, Robustness, and Adaptation"
    },
    "chen15g_interspeech": {
      "authors": [
        [
          "Guoguo",
          "Chen"
        ],
        [
          "Hainan",
          "Xu"
        ],
        [
          "Minhua",
          "Wu"
        ],
        [
          "Daniel",
          "Povey"
        ],
        [
          "Sanjeev",
          "Khudanpur"
        ]
      ],
      "title": "Pronunciation and silence probability modeling for ASR",
      "original": "i15_0533",
      "page_count": 5,
      "order": 207,
      "p1": "533",
      "pn": "537",
      "abstract": [
        "In this paper we evaluate the WER improvement from modeling pronunciation probabilities and word-specific silence probabilities in speech recognition. We do this in the context of Finite State Transducer (FST)-based decoding, where pronunciation and silence probabilities are encoded in the lexicon (L) transducer. We describe a novel way to model word-dependent silence probabilities, where in addition to modeling the probability of silence following each individual word, we also model the probability of each word appearing after silence. All of these probabilities are estimated from aligned training data, with suitable smoothing. We conduct our experiments on four commonly used automatic speech recognition datasets, namelyWall Street Journal, Switchboard, TED-LIUM, and Librispeech. The improvement from modeling pronunciation and silence probabilities is small but fairly consistent across datasets.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-198",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition \u2013 Signal Processing, Acoustic Modeling, Robustness, and Adaptation"
    },
    "davel15_interspeech": {
      "authors": [
        [
          "Marelie",
          "Davel"
        ],
        [
          "Etienne",
          "Barnard"
        ],
        [
          "Charl van",
          "Heerden"
        ],
        [
          "William",
          "Hartmann"
        ],
        [
          "Damianos",
          "Karakos"
        ],
        [
          "Richard",
          "Schwartz"
        ],
        [
          "Stavros",
          "Tsakalidis"
        ]
      ],
      "title": "Exploring minimal pronunciation modeling for low resource languages",
      "original": "i15_0538",
      "page_count": 5,
      "order": 208,
      "p1": "538",
      "pn": "542",
      "abstract": [
        "Pronunciation lexicons can range from fully graphemic (modeling each word using the orthography directly) to fully phonemic (first mapping each word to a phoneme string). Between these two options lies a continuum of modeling options. We analyze techniques that can improve the accuracy of a graphemic system without requiring significant effort to design or implement. The analysis is performed in the context of the IARPA Babel project, which aims to develop spoken term detection systems for previously unseen languages rapidly, and with minimal human effort. We consider techniques related to letter-to-sound mapping and language-independent syllabification of primarily graphemic systems, and discuss results obtained for six languages: Cebuano, Kazakh, Kurmanji Kurdish, Lithuanian, Telugu and Tok Pisin.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-199",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition \u2013 Signal Processing, Acoustic Modeling, Robustness, and Adaptation"
    },
    "zheng15_interspeech": {
      "authors": [
        [
          "Hao",
          "Zheng"
        ],
        [
          "Zhanlei",
          "Yang"
        ],
        [
          "Liwei",
          "Qiao"
        ],
        [
          "Jianping",
          "Li"
        ],
        [
          "Wenju",
          "Liu"
        ]
      ],
      "title": "Attribute knowledge integration for speech recognition based on multi-task learning neural networks",
      "original": "i15_0543",
      "page_count": 5,
      "order": 209,
      "p1": "543",
      "pn": "547",
      "abstract": [
        "It has been demonstrated that the speech recognition performance can be improved by adding extra articulatory information, and subsequently, how to use such information effectively becomes a challenging problem. In this paper, we propose an attribute-based knowledge integration architecture which is realized by modeling and learning both acoustic and articulatory cues simultaneously in a uniform framework. The framework promotes the performance by providing attribute-based knowledge in both feature and model domains. In model domain, the attribute classification is used as the secondary task to improve the performance of an MTL-DNN used for speech recognition by lifting the discriminative ability on pronunciation. In feature domain, an attribute-based feature is extracted from an MTL-DNN trained with attribute classification as its primary task and phonetic/tri-phone state classification as the secondary task. Experiments on TIMIT and WSJ corpuses show that the proposed framework achieves significant performance improvements compared with the baseline DNN-HMM systems.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-200",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition \u2013 Signal Processing, Acoustic Modeling, Robustness, and Adaptation"
    },
    "marcheret15_interspeech": {
      "authors": [
        [
          "Etienne",
          "Marcheret"
        ],
        [
          "Gerasimos",
          "Potamianos"
        ],
        [
          "Josef",
          "Vopicka"
        ],
        [
          "Vaibhava",
          "Goel"
        ]
      ],
      "title": "Detecting audio-visual synchrony using deep neural networks",
      "original": "i15_0548",
      "page_count": 5,
      "order": 210,
      "p1": "548",
      "pn": "552",
      "abstract": [
        "In this paper, we address the problem of automatically detecting whether the audio and visual speech modalities in frontal pose videos are synchronous or not. This is of interest in a wide range of applications, for example spoof detection in biometrics, lip-syncing, speaker detection and diarization in multi-subject videos, and video data quality assurance. In our adopted approach, we investigate the use of deep neural networks (DNNs) for this purpose. The proposed synchrony DNNs operate directly on audio and visual features over relatively wide contexts, or, alternatively, on appropriate hidden (bottleneck) or output layers of DNNs trained for single-modal or audio-visual automatic speech recognition. In all cases, the synchrony DNN classes consist of the \u201cin-sync\u201d and a number of \u201cout-of-sync\u201d targets, the latter considered at multiples of \u00b1 30 msec steps of overall asynchrony between the two modalities. We apply the proposed approach on two multi-subject audio-visual databases, one of high-quality data recorded in studio-like conditions, and one of data recorded by smart cell-phone devices. On both sets, and under a speaker-independent experimental framework, we are able to achieve very low equal-error-rates in distinguishing \u201cin-sync\u201d from \u201cout-of-sync\u201d data.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-201",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition \u2013 Signal Processing, Acoustic Modeling, Robustness, and Adaptation"
    },
    "kalantari15_interspeech": {
      "authors": [
        [
          "Shahram",
          "Kalantari"
        ],
        [
          "David",
          "Dean"
        ],
        [
          "Houman",
          "Ghaemmaghami"
        ],
        [
          "Sridha",
          "Sridharan"
        ],
        [
          "Clinton",
          "Fookes"
        ]
      ],
      "title": "Cross database training of audio-visual hidden Markov models for phone recognition",
      "original": "i15_0553",
      "page_count": 5,
      "order": 211,
      "p1": "553",
      "pn": "557",
      "abstract": [
        "Speech recognition can be improved by using visual information in the form of lip movements of the speaker in addition to audio information. To date, state-of-the-art techniques for audio-visual speech recognition continue to use audio and visual data of the same database for training their models. In this paper, we present a new approach to make use of one modality of an external dataset in addition to a given audio-visual dataset. By so doing, it is possible to create more powerful models from other extensive audio-only databases and adapt them on our comparatively smaller multi-stream databases. Results show that the presented approach outperforms the widely adopted synchronous hidden Markov models (HMM) trained jointly on audio and visual data of a given audio-visual database for phone recognition by 29% relative. It also outperforms the external audio models trained on extensive external audio datasets and also internal audio models by 5.5% and 46% relative respectively. We also show that the proposed approach is beneficial in noisy environments where the audio source is affected by the environmental noise.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-202",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition \u2013 Signal Processing, Acoustic Modeling, Robustness, and Adaptation"
    },
    "kalantari15b_interspeech": {
      "authors": [
        [
          "Shahram",
          "Kalantari"
        ],
        [
          "David",
          "Dean"
        ],
        [
          "Sridha",
          "Sridharan"
        ]
      ],
      "title": "Incorporating visual information for spoken term detection",
      "original": "i15_0558",
      "page_count": 5,
      "order": 212,
      "p1": "558",
      "pn": "562",
      "abstract": [
        "Spoken term detection (STD) is the task of looking up a spoken term in a large volume of speech segments. In order to provide fast search, speech segments are first indexed into an intermediate representation using speech recognition engines which provide multiple hypotheses for each speech segment. Approximate matching techniques are usually applied at the search stage to compensate the poor performance of automatic speech recognition engines during indexing. Recently, using visual information in addition to audio information has been shown to improve phone recognition performance, particularly in noisy environments. In this paper, we will make use of visual information in the form of lip movements of the speaker in indexing stage and will investigate its effect on STD performance. Particularly, we will investigate if gains in phone recognition accuracy will carry through the approximate matching stage to provide similar gains in the final audio-visual STD system over a traditional audio only approach. We will also investigate the effect of using visual information on STD performance in different noise environments.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-203",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition \u2013 Signal Processing, Acoustic Modeling, Robustness, and Adaptation"
    },
    "ninomiya15_interspeech": {
      "authors": [
        [
          "Hiroshi",
          "Ninomiya"
        ],
        [
          "Norihide",
          "Kitaoka"
        ],
        [
          "Satoshi",
          "Tamura"
        ],
        [
          "Yurie",
          "Iribe"
        ],
        [
          "Kazuya",
          "Takeda"
        ]
      ],
      "title": "Integration of deep bottleneck features for audio-visual speech recognition",
      "original": "i15_0563",
      "page_count": 5,
      "order": 213,
      "p1": "563",
      "pn": "567",
      "abstract": [
        "Recent interest in \u201cdeep learning\u201d, which can be defined as the use of algorithms to model high-level abstractions in data, using models composed of multiple non-linear transformations, has resulted in an increase in the number of studies investigating the use of deep learning with automatic speech recognition (ASR) systems. Some of these studies have found that bottleneck features extracted from deep neural networks (DNNs), sometimes called \u201cdeep bottleneck features\u201d (DBNFs), can reduce the word error rates of ASR systems. However, there has been little research on audio-visual speech recognition (AVSR) systems using DBNFs. In this paper, we propose a method of integrating DBNFs using multi-stream HMMs in order to improve the performance of AVSRs under both clean and noisy conditions. We evaluate our method using a continuously spoken, Japanese digit recognition task under matched and mismatched conditions. Relative word error reduction rates of roughly 68.7%, 47.4%, and 51.9% were achieved, compared with an audio-only ASR system and two feature-fusion models, which employed DBNFs and single-stream HMMs, respectively.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-204",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition \u2013 Signal Processing, Acoustic Modeling, Robustness, and Adaptation"
    },
    "kakouros15_interspeech": {
      "authors": [
        [
          "Sofoklis",
          "Kakouros"
        ],
        [
          "Okko",
          "R\u00e4s\u00e4nen"
        ]
      ],
      "title": "Automatic detection of sentence prominence in speech using predictability of word-level acoustic features",
      "original": "i15_0568",
      "page_count": 5,
      "order": 214,
      "p1": "568",
      "pn": "572",
      "abstract": [
        "Automatic detection of prominence in speech is an important task for many spoken language applications. However, most previous approaches rely on the availability of a corpus that is annotated with prosodic labels in order to train classifiers, therefore lacking generality beyond high-resourced languages. In this paper, we propose an algorithm for the automatic detection of sentence prominence that does not require explicit prominence labels for training. The method is based on the finding that human perception of prominence correlates with the (un)predictability of prosodic trajectories. The proposed system takes speech as input and combines information from automatically detected syllabic nuclei and three prosodic features in order to provide estimates of the prominent words. Results are reported using a speech corpus with manually assigned prominence labels from twenty annotators, showing that the algorithmic output converges with the annotators' prominence responses with 86% accuracy.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-205",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition \u2013 Signal Processing, Acoustic Modeling, Robustness, and Adaptation"
    },
    "cernak15_interspeech": {
      "authors": [
        [
          "Milos",
          "Cernak"
        ],
        [
          "Pierre-Edouard",
          "Honnet"
        ]
      ],
      "title": "An empirical model of emphatic word detection",
      "original": "i15_0573",
      "page_count": 5,
      "order": 215,
      "p1": "573",
      "pn": "577",
      "abstract": [
        "The paper presents an empirical model of emphatic word detection, as an alternative to conventional machine-learning-based methods. The model is based on the Probabilistic Amplitude Demodulation (PAD) that is iteratively applied for getting syllable and stress modulations, i.e., using the cascaded PAD method. The emphatic words are detected by prominent peaks of the stress modulation and by considering the peaks that are stressed or accented. The cascaded demodulation steered with general purpose values derived from 200ms long average syllable duration, yields to detection accuracy of 81%-83%. Speaker-dependent cascaded demodulation, considering specific speaking rate of the speakers, yields to detection accuracy of 86%-91%. The advantages of the proposed empirical detection model are (i) noise-robustness, (ii) language-independence and (iii) it does not require a training phase.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-206",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition \u2013 Signal Processing, Acoustic Modeling, Robustness, and Adaptation"
    },
    "ning15_interspeech": {
      "authors": [
        [
          "Yishuang",
          "Ning"
        ],
        [
          "Zhiyong",
          "Wu"
        ],
        [
          "Xiaoyan",
          "Lou"
        ],
        [
          "Helen",
          "Meng"
        ],
        [
          "Jia",
          "Jia"
        ],
        [
          "Lianhong",
          "Cai"
        ]
      ],
      "title": "Using tilt for automatic emphasis detection with Bayesian networks",
      "original": "i15_0578",
      "page_count": 5,
      "order": 216,
      "p1": "578",
      "pn": "582",
      "abstract": [
        "This paper proposes a new framework for emphasis detection from natural speech, where emphasis refers to a word or part of a word perceived as standing out from its surrounding words. Labeling emphatic words from speech recordings plays a significant role not only in human-computer interactions, but also in building speech corpus for expressive speech synthesis. Many previous researches use the global features to train their models, neglecting the efficiency of the local ones. In this paper, we introduce the tilt parameters which correspond to the phonetic prominence of an intonation event to our task. Besides, traditional approaches such as emphasis detection with support vector machines (SVMs) neglect the correlations between features, thus degrading the accuracy of emphasis detection. In this paper, we use Bayesian networks (BNs) which consider the dependency between features as detector. Experimental results demonstrate that BNs outperform the baseline and SVMs for the task. Specifically, by combining the tilt feature with the traditional segmental features and semitone, the proposed method yields an 11.6% improvement in emphasis detection accuracy as compared with the baseline and 2.2%-3.1% improvement with other feature combinations.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-207",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition \u2013 Signal Processing, Acoustic Modeling, Robustness, and Adaptation"
    },
    "bai15_interspeech": {
      "authors": [
        [
          "Linxue",
          "Bai"
        ],
        [
          "Peter",
          "Jan\u010dovi\u010d"
        ],
        [
          "Martin",
          "Russell"
        ],
        [
          "Philip",
          "Weber"
        ]
      ],
      "title": "Analysis of a low-dimensional bottleneck neural network representation of speech for modelling speech dynamics",
      "original": "i15_0583",
      "page_count": 5,
      "order": 217,
      "p1": "583",
      "pn": "587",
      "abstract": [
        "This paper presents an analysis of a low-dimensional representation of speech for modelling speech dynamics, extracted using bottleneck neural networks. The input to the neural network is a set of spectral feature vectors. We explore the effect of various designs and training of the network, such as varying the size of context in the input layer, size of the bottleneck and other hidden layers, and using input reconstruction or phone posteriors as targets. Experiments are performed on TIMIT. The bottleneck features are employed in a conventional HMM-based phoneme recognition system, with recognition accuracy of 70.6% on the core test achieved using only 9-dimensional features. We also analyse how the bottleneck features fit the assumptions of dynamic models of speech. Specifically, we employ the continuous-state hidden Markov model (CS-HMM), which considers speech as a sequence of dwell and transition regions. We demonstrate that the bottleneck features preserve well the trajectory continuity over time and can provide a suitable representation for CS-HMM.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-208",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "uchida15_interspeech": {
      "authors": [
        [
          "Hidetsugu",
          "Uchida"
        ],
        [
          "Daisuke",
          "Saito"
        ],
        [
          "Nobuaki",
          "Minematsu"
        ],
        [
          "Keikichi",
          "Hirose"
        ]
      ],
      "title": "Statistical acoustic-to-articulatory mapping unified with speaker normalization based on voice conversion",
      "original": "i15_0588",
      "page_count": 5,
      "order": 218,
      "p1": "588",
      "pn": "592",
      "abstract": [
        "This paper proposes a model of speaker-normalized acoustic-to-articulatory mapping using statistical voice conversion. A mapping function from acoustic parameters to articulatory parameters is usually developed with a single speaker's parallel data. Hence the constructed mapping model can work appropriately only for this specific speaker, and applying this model to other speakers degrades the performance of acoustic-to-articulatory mapping. In this paper, two models of speaker conversion and acoustic-to-articulatory mapping are implemented using Gaussian Mixture Models (GMM), and by integrating these two models, we propose two methods of speaker-normalized acoustic-to-articulatory mapping. One is concatenating these models sequentially, and the other integrates the two models into a unified model, where acoustic parameters of a speaker can be converted directly to articulatory parameters of another speaker. Experiments show that both methods can improve the mapping accuracy and that the latter method works better than the former method. Especially in the case of velar stop consonants, the mapping accuracy is higher by 0.6 mm.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-209",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "pappagari15_interspeech": {
      "authors": [
        [
          "Raghavendra Reddy",
          "Pappagari"
        ],
        [
          "Karthika",
          "Vijayan"
        ],
        [
          "K. Sri Rama",
          "Murty"
        ]
      ],
      "title": "Analysis of features from analytic representation of speech using MP-ABX measures",
      "original": "i15_0593",
      "page_count": 5,
      "order": 219,
      "p1": "593",
      "pn": "597",
      "abstract": [
        "The significance of features derived from complex analytic domain representation of speech, for different applications, is investigated. Frequency domain linear prediction (FDLP) coefficients are derived from analytic magnitude and instantaneous frequency (IF) coefficients are derived from analytic phase of speech signals. Minimal pair ABX (MP-ABX) tasks are used to analyse different features and develop insights into the nature of information in them. The performance of the features derived from analytic representation are compared with performance of the Mel-Frequency Cepstral Coefficients (MFCC). It is noticed that the magnitude based features- FDLP and MFCC delivered promising PaC, PaT and CaT scores in MP-ABX tasks, demonstrating their phoneme discrimination abilities. Combining FDLP features with MFCC had proven beneficial in phoneme discrimination tasks. The IF features performed well in TaP mode of MP-ABX tasks, emphasizing the existence of speaker specific information in them. The IF significantly outperformed FDLP, MFCC and their combination in speaker discrimination task.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-210",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "loweimi15_interspeech": {
      "authors": [
        [
          "Erfan",
          "Loweimi"
        ],
        [
          "Jon",
          "Barker"
        ],
        [
          "Thomas",
          "Hain"
        ]
      ],
      "title": "Source-filter separation of speech signal in the phase domain",
      "original": "i15_0598",
      "page_count": 5,
      "order": 220,
      "p1": "598",
      "pn": "602",
      "abstract": [
        "Deconvolution of the speech excitation (source) and vocal tract (filter) components through log-magnitude spectral processing is well-established and has led to the well-known cepstral features used in a multitude of speech processing tasks. This paper presents a novel source-filter decomposition based on processing in the phase domain. We show that separation between source and filter in the log-magnitude spectra is far from perfect, leading to loss of vital vocal tract information. It is demonstrated that the same task can be better performed by trend and fluctuation analysis of the phase spectrum of the minimum-phase component of speech, which can be computed via the Hilbert transform. Trend and fluctuation can be separated through low-pass filtering of the phase, using additivity of vocal tract and source in the phase domain. This results in separated signals which have a clear relation to the vocal tract and excitation components. The effectiveness of the method is put to test in a speech recognition task. The vocal tract component extracted in this way is used as the basis of a feature extraction algorithm for speech recognition on the Aurora-2 database. The recognition results shows upto 8.5% absolute improvement in comparison with MFCC features on average (0-20dB).\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-211",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "maia15_interspeech": {
      "authors": [
        [
          "Ranniery",
          "Maia"
        ],
        [
          "Yannis",
          "Stylianou"
        ],
        [
          "Masami",
          "Akamine"
        ]
      ],
      "title": "A maximum likelihood approach to the detection of moments of maximum excitation and its application to high-quality speech parameterization",
      "original": "i15_0603",
      "page_count": 5,
      "order": 221,
      "p1": "603",
      "pn": "607",
      "abstract": [
        "This paper presents an algorithm to detect moments of maximum excitation (MME) in speech. It assumes a model in which speech can be represented as a sequence of pulses located at the MME convolved with a time-varying minimum-phase impulse response. By considering that in the glottal cycle speech concentrates more energy at the MME than at other instants, the locations and amplitudes of the excitation pulses are determined through maximum likelihood estimation. The suggested approach provides a fully automatic and consistent method for the detection of MME in speech without relying on ad hoc procedures which usually do not work well across different speech styles without a required amount of adjustments. Experiments with speech parameterization, in the context of complex cepstrum analysis and synthesis, have shown that the proposed MME-based processing can improve signal to error reconstruction ratio up to 10%, when compared to the use of glottal closure instant estimations provided by a well-known algorithm.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-212",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "liberatore15_interspeech": {
      "authors": [
        [
          "Christopher",
          "Liberatore"
        ],
        [
          "Sandesh",
          "Aryal"
        ],
        [
          "Zelun",
          "Wang"
        ],
        [
          "Seth",
          "Polsley"
        ],
        [
          "Ricardo",
          "Gutierrez-Osuna"
        ]
      ],
      "title": "SABR: sparse, anchor-based representation of the speech signal",
      "original": "i15_0608",
      "page_count": 5,
      "order": 222,
      "p1": "608",
      "pn": "612",
      "abstract": [
        "We present SABR (Sparse, Anchor-Based Representation), an analysis technique to decompose the speech signal into speaker-dependent and speaker-independent components. Given a collection of utterances for a particular speaker, SABR uses the centroid for each phoneme as an acoustic \u201canchor,\u201d then applies Lasso regularization to represent each speech frame as a sparse non-negative combination of the anchors. We illustrate the performance of the method on a speaker-independent phoneme recognition task and a voice conversion task. Using a linear classifier, SABR weights achieve significantly higher phoneme recognition rates than Mel frequency Cepstral coefficients. SABR weights can also be used directly to perform accent conversion without the need to train a speaker-to-speaker regression model.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-213",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "csapo15_interspeech": {
      "authors": [
        [
          "Tam\u00e1s G\u00e1bor",
          "Csap\u00f3"
        ],
        [
          "G\u00e9za",
          "N\u00e9meth"
        ]
      ],
      "title": "Automatic transformation of irregular to regular voice by residual analysis and synthesis",
      "original": "i15_0613",
      "page_count": 5,
      "order": 223,
      "p1": "613",
      "pn": "617",
      "abstract": [
        "This paper presents an automatic speech transformation method of non-ideal phonation of speech (irregular or creaky voice). The irregular-to-regular transformation is performed by analyzing and resynthesizing the residual. A recent continuous pitch estimation algorithm is used for interpolating F0 in regions of irregular voice. The linear prediction residual of irregular sections of speech is replaced by overlap-added frames from a codebook of pitch-synchronous residuals. Finally, speech is reconstructed from the residual. A listening experiment showed that by transforming natural speech samples containing irregular voice, the perceived roughness of the transformed speech is decreased.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-214",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "preu15_interspeech": {
      "authors": [
        [
          "Simon",
          "Preu\u00df"
        ],
        [
          "Peter",
          "Birkholz"
        ]
      ],
      "title": "Optical sensor calibration for electro-optical stomatography",
      "original": "i15_0618",
      "page_count": 5,
      "order": 224,
      "p1": "618",
      "pn": "622",
      "abstract": [
        "We are currently developing a technology called \u201celectro-optical stomatography\u201d to measure and visualize articulatory movements within the vocal tract using electrical contact sensors and optical proximity sensors. To measure tongue movements with the optical sensors in this system, a mapping between the raw sensor values and actual tongue positions has to be determined. This mapping is non-linear and different for every tongue and sensor. The lack of an accurate, reliable calibration method has so far prevented wide-spread use of optical measurements within the vocal tract. Here, we present a calibration method based on a multi-linear regression model that maps the sensor value at a single distance of 0mm to calibration values at 0, 5, 10, 15, 20, 25, and 30mm. The coefficients of the model are determined by a least-squares regression in 25 training data sets (recorded with 5 subjects and 5 sensors). Evaluation in a leave-one-out cross-validation and on five more data sets recorded with another, different subject on 5 additional sensors yields very good results with maximum median position errors close to 1mm. The calibration of the optical sensors can therefore be semi-automatically accomplished based on a single, easily obtainable measurement during direct tongue contact.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-215",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "abari15_interspeech": {
      "authors": [
        [
          "K\u00e1lm\u00e1n",
          "Abari"
        ],
        [
          "Tam\u00e1s G\u00e1bor",
          "Csap\u00f3"
        ],
        [
          "B\u00e1lint P\u00e1l",
          "T\u00f3th"
        ],
        [
          "G\u00e1bor",
          "Olaszy"
        ]
      ],
      "title": "From text to formants \u2014 indirect model for trajectory prediction based on a multi-speaker parallel speech database",
      "original": "i15_0623",
      "page_count": 5,
      "order": 225,
      "p1": "623",
      "pn": "627",
      "abstract": [
        "An indirect model is presented, capable of estimating formant trajectories from text only (Text-to-Formants, TTF). The result is a phonetically correct formant trajectory flow of any virtual speech signal, i.e. one that has never been uttered. The focus is on the pattern forms inside the given sound, taking into account the sound environment (up to quinphone), and not on individual formant value measurements. The model is based on a multi-speaker parallel speech database with precise manual corrections and a HMM-based formant trajectory predictor. The validation of the TTF model shows that formant trajectories can be predicted with good accuracy from text. The model indirectly gives information about a theoretically possible articulation flow of the sentence. Thus it gives a general `formantprint' of the language.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-216",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "hsu15_interspeech": {
      "authors": [
        [
          "Chung-Chien",
          "Hsu"
        ],
        [
          "Jen-Tzung",
          "Chien"
        ],
        [
          "Tai-Shih",
          "Chi"
        ]
      ],
      "title": "Layered nonnegative matrix factorization for speech separation",
      "original": "i15_0628",
      "page_count": 5,
      "order": 226,
      "p1": "628",
      "pn": "632",
      "abstract": [
        "This paper proposes a layered nonnegative matrix factorization (L-NMF) algorithm for speech separation. The standard NMF method extracts parts-based bases out of nonnegative training data and is often used to separate mixed spectrograms. The proposed L-NMF algorithm comprises of several layers of standard NMF blocks. During training, each layer of the L-NMF is initialized separately and then fine-tuned by minimizing the propagated reconstruction error. More complicated bases of the training data are emerged in deeper layers of the L-NMF by progressively combining parts-based bases extracted in the first layer. In other words, these complicated bases contain collective information of the parts-based bases. The bases deciphered by all layers are then used to separate spectrograms in the conventional NMF way. Simulation results show the proposed L-NMF outperforms the standard NMF in terms of the source-to-distortion ratio (SDR).\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-217",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "laporte15_interspeech": {
      "authors": [
        [
          "Catherine",
          "Laporte"
        ],
        [
          "Lucie",
          "M\u00e9nard"
        ]
      ],
      "title": "Robust tongue tracking in ultrasound images: a multi-hypothesis approach",
      "original": "i15_0633",
      "page_count": 5,
      "order": 227,
      "p1": "633",
      "pn": "637",
      "abstract": [
        "Ultrasound (US) imaging is an excellent means of observing tongue motion during speech. Tracking the tongue contour in US video is required for analysis of this motion, but most currently available techniques suffer from either a lack of temporal consistency or a lack of robustness to difficult conditions such as a rapidly deforming tongue or momentarily poor image quality. This paper proposes a new algorithm combining active contours, active shape models and particle filtering that addresses these shortcomings. The strength of this approach lies in the fact that it maintains multiple tongue shape hypotheses simultaneously. Experimental results show that this approach outperforms a classic active contour algorithm as well as a shape-constrained variant thereof, particularly in difficult tracking conditions.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-218",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "websdale15_interspeech": {
      "authors": [
        [
          "Danny",
          "Websdale"
        ],
        [
          "Thomas Le",
          "Cornu"
        ],
        [
          "Ben",
          "Milner"
        ]
      ],
      "title": "Objective measures for predicting the intelligibility of spectrally smoothed speech with artificial excitation",
      "original": "i15_0638",
      "page_count": 5,
      "order": 228,
      "p1": "638",
      "pn": "642",
      "abstract": [
        "A study is presented on how well objective measures of speech quality and intelligibility can predict the subjective intelligibility of speech that has undergone spectral envelope smoothing and simplification of its excitation. Speech modifications are made by resynthesising speech that has been spectrally smoothed. Objective measures are applied to the modified speech and include measures of speech quality, signal-to-noise ratio and intelligibility, as well as proposing the normalised frequency-weighted spectral distortion (NFD) measure. The measures are compared to subjective intelligibility scores where it is found that several have high correlation (|r| \u2265 0.7), with NFD achieving the highest correlation (r = -0.81).\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-219",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "mertens15_interspeech": {
      "authors": [
        [
          "Christophe",
          "Mertens"
        ],
        [
          "Francis",
          "Grenez"
        ],
        [
          "Fran\u00e7ois",
          "Viallet"
        ],
        [
          "Alain",
          "Ghio"
        ],
        [
          "Sabine",
          "Skodda"
        ],
        [
          "Jean",
          "Schoentgen"
        ]
      ],
      "title": "Vocal tremor analysis via AM-FM decomposition of empirical modes of the glottal cycle length time series",
      "original": "i15_0766",
      "page_count": 5,
      "order": 229,
      "p1": "766",
      "pn": "770",
      "abstract": [
        "The presentation concerns a method that obtains the size and frequency of vocal tremor in speech sounds sustained by normal speakers and patients suffering from neurological disorders. The glottal cycle lengths are tracked in the temporal domain via salience analysis and dynamic programming. The cycle length time series is then decomposed into a sum of oscillating components by empirical mode decomposition the instantaneous envelopes and frequencies of which are obtained via an AM-FM decomposition. Based on their average instantaneous frequencies, the empirical modes are then assigned to four categories (intonation, physiological tremor, neurological tremor as well as jitter) and added within each. The within-category size of the cycle length perturbations is estimated via the standard deviation of the empirical mode sum divided by the average cycle length. The tremor frequency within the neurological tremor category is obtained via a weighted instantaneous average of the mode frequencies followed by a weighted temporal average. The method is applied to two corpora of vowels sustained by 123 and 74 control and 456 and 205 Parkinson speakers respectively.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-220",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "godoy15_interspeech": {
      "authors": [
        [
          "Elizabeth",
          "Godoy"
        ],
        [
          "Nicolas",
          "Malyska"
        ],
        [
          "Thomas F.",
          "Quatieri"
        ]
      ],
      "title": "Estimating lower vocal tract features with closed-open phase spectral analyses",
      "original": "i15_0771",
      "page_count": 5,
      "order": 230,
      "p1": "771",
      "pn": "775",
      "abstract": [
        "Previous studies have shown that, in addition to being speaker-dependent yet context-independent, lower vocal tract acoustics significantly impact the speech spectrum at mid-to-high frequencies (e.g 3-6kHz). The present work automatically estimates spectral features that exhibit acoustic properties of the lower vocal tract. Specifically aiming to capture the cyclicity property of the epilarynx tube, a novel multi-resolution approach to spectral analyses is presented that exploits significant differences between the closed and open phases of a glottal cycle. A prominent null linked to the piriform fossa is also estimated. Examples of the feature estimation on natural speech of the VOICES multi-speaker corpus illustrate that a salient spectral pattern indeed emerges between 3-6kHz across all speakers. Moreover, the observed pattern is consistent with that canonically shown for the lower vocal tract in previous works. Additionally, an instance of a speaker's formant (i.e. spectral peak around 3kHz that has been well-established as a characteristic of voice projection) is quantified here for the VOICES template speaker in relation to epilarynx acoustics. The corresponding peak is shown to be double the power on average compared to the other speakers (20 vs 10 dB).\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-221",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "houghton15b_interspeech": {
      "authors": [
        [
          "S. M.",
          "Houghton"
        ],
        [
          "Colin J.",
          "Champion"
        ]
      ],
      "title": "Inductive implementation of segmental HMMs as CS-HMMs",
      "original": "i15_0776",
      "page_count": 5,
      "order": 231,
      "p1": "776",
      "pn": "780",
      "abstract": [
        "Segmental models have been used in speech recognition to reduce the effect of the counterfactual assumptions of statistical independence which are made in more conventional systems. They have achieved their aim at the cost of a large increase in computational load arising from making assumptions on entire segments rather than on individual frames. In this paper we show how segmental algorithms can be refactored as iterative calculations, removing most of additional computational burden they impose. We also show that the iterative implementation leads naturally to increased flexibility in the handling of timing, allowing an arbitrary timing model to be incorporated at no extra cost.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-222"
    },
    "meenakshi15_interspeech": {
      "authors": [
        [
          "G. Nisha",
          "Meenakshi"
        ],
        [
          "Prasanta Kumar",
          "Ghosh"
        ]
      ],
      "title": "A discriminative analysis within and across voiced and unvoiced consonants in neutral and whispered speech in multiple indian languages",
      "original": "i15_0781",
      "page_count": 5,
      "order": 232,
      "p1": "781",
      "pn": "785",
      "abstract": [
        "Whispered speech lacks the vocal chord vibration which is typically used to distinguish voiced and unvoiced consonants, making their discrimination a challenging task. In this work, we objectively and subjectively quantify the amount of discrimination between a voiced (V) consonant and its unvoiced (UV) counterpart using seven V-UV consonant pairs in six Indian languages, in neutral and whispered speech. We also quantify the extent to which the voicing characteristics in a consonant changes from neutral to whispered speech. Experiments using vowel-consonant-vowel (VCV) stimuli demonstrate that the V-UV discrimination reduces from neutral to whispered speech in a consonant specific manner with highest reduction for /\u0261/-/k/ pair and least reduction for /z/-/s/ pair. Interestingly, this reduction in objectively measured discrimination does not directly correlate with the reduction in the V-UV classification accuracy obtained from subjective evaluation. Results from listening test show that the maximum and minimum reduction in the V-UV classification accuracy occur for /\u02a4/-/\u02a7/ and /v/-/f/ pairs when whispered. Whispered Tamil and Telugu VCV achieve the highest (85.71%) and lowest (58.93%) subjective V-UV classification accuracy respectively, demonstrating the variability in the production and perception whispered consonants across languages.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-223",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "tsai15_interspeech": {
      "authors": [
        [
          "T. J.",
          "Tsai"
        ],
        [
          "Andreas",
          "Stolcke"
        ]
      ],
      "title": "Aligning meeting recordings via adaptive fingerprinting",
      "original": "i15_0786",
      "page_count": 5,
      "order": 233,
      "p1": "786",
      "pn": "790",
      "abstract": [
        "This paper proposes a robust and efficient way to temporally align a set of unsynchronized meeting recordings, such as might be collected by participants' cell phones. We propose an adaptive audio fingerprint which is learned on-the-fly in a completely unsupervised manner to adapt to the characteristics of a given set of unaligned recordings. The design of the adaptive audio fingerprint is formulated as a series of optimization problems which can be solved very efficiently using eigenvector routines. We also propose a method of aligning sets of files which uses the cumulative evidence from previous alignments to help align the weakest matches. Based on challenging alignment scenarios extracted from the ICSI meeting corpus, the proposed alignment system is able to achieve > 99% alignment accuracy at a 100ms error tolerance.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-224",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "zohrer15_interspeech": {
      "authors": [
        [
          "Matthias",
          "Z\u00f6hrer"
        ],
        [
          "Robert",
          "Peharz"
        ],
        [
          "Franz",
          "Pernkopf"
        ]
      ],
      "title": "On representation learning for artificial bandwidth extension",
      "original": "i15_0791",
      "page_count": 5,
      "order": 234,
      "p1": "791",
      "pn": "795",
      "abstract": [
        "Recently, sum-product networks (SPNs) showed convincing results on the ill-posed task of artificial bandwidth extension (ABE). However, SPNs are just one type of many architectures which can be summarized as representational models. In this paper, using ABE as benchmark task, we perform a comparative study of Gauss Bernoulli restricted Boltzmann machines, conditional restricted Boltzmann machines, higher order contractive autoencoders, SPNs and generative stochastic networks (GSNs). Especially the latter ones are promising architectures in terms of its reconstruction capabilities. Our experiments show impressive results of GSNs, achieving on average an improvement of 3.90dB and 4.08dB in segmental SNR on a speaker dependent (SD) and speaker independent (SI) scenario compared to SPNs, respectively.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-225",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "gowda15_interspeech": {
      "authors": [
        [
          "Dhananjaya",
          "Gowda"
        ],
        [
          "Rahim",
          "Saeidi"
        ],
        [
          "Paavo",
          "Alku"
        ]
      ],
      "title": "AM-FM based filter bank analysis for estimation of spectro-temporal envelopes and its application for speaker recognition in noisy reverberant environments",
      "original": "i15_1166",
      "page_count": 5,
      "order": 235,
      "p1": "1166",
      "pn": "1170",
      "abstract": [
        "In this paper, a new AM-FM based filter bank analysis for the estimation of spectro-temporal envelope (STE) of speech signals is proposed. The filter bank is simulated by filtering a frequency translated signal using a single resonator centered around the Nyquist frequency. The proposed design of using a single fixed resonator provides distinct advantages over the traditional methods of filter bank design. First, it provides a simple IIR filter with a smooth frequency response with no ripples. Second, the bandwidth of the resonator can be easily controlled by the multiplicity of poles and their proximity to the unit circle on the z-plane. Third, the resonator fixed at the highest possible center frequency provides the best separation between the AM and FM components of the filtered signal. Speaker recognition experiments on noisy and reverberant speech with short test segments show that the proposed AM-FM based filter bank analysis for STE estimation provides consistent improvement over a recently proposed discrete cosine transform based filter bank approach.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-226",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "drugman15_interspeech": {
      "authors": [
        [
          "Thomas",
          "Drugman"
        ],
        [
          "Yannis",
          "Stylianou"
        ]
      ],
      "title": "Fast and accurate phase unwrapping",
      "original": "i15_1171",
      "page_count": 5,
      "order": 236,
      "p1": "1171",
      "pn": "1175",
      "abstract": [
        "More and more speech technology and signal processing applications make use of the phase information. A proper estimation and representation of the phase goes inextricably along with a correct phase unwrapping, which refers to the problem of finding the instance of the phase function chosen to ensure continuity. This paper proposes a new technique of phase unwrapping which is based on two mathematical considerations:   i) a property of the unwrapped phase at Nyquist frequency, ii) the modified Schur-Cohn's algorithm which allows a fast calculation of the root distribution of polynomials with respect to the unit circle. The proposed method is compared to five state-of-the-art phase unwrappers on a large dataset of both synthetic random and real speech signals. By leveraging the two aforementioned considerations, the proposed approach is shown to perform an exact estimation of the unwrapped phase at a reduced computational load.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-227",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "lu15c_interspeech": {
      "authors": [
        [
          "Xugang",
          "Lu"
        ],
        [
          "Peng",
          "Shen"
        ],
        [
          "Yu",
          "Tsao"
        ],
        [
          "Chiori",
          "Hori"
        ],
        [
          "Hisashi",
          "Kawai"
        ]
      ],
      "title": "Sparse representation with temporal max-smoothing for acoustic event detection",
      "original": "i15_1176",
      "page_count": 5,
      "order": 237,
      "p1": "1176",
      "pn": "1180",
      "abstract": [
        "In order to incorporate long temporal-frequency structure for acoustic event detection, we have proposed a spectral patch based learning and representation method. The learned spectral patches were regarded as acoustic words which were further used in sparse encoding for acoustic feature representation and modeling. In our previous study, during feature encoding stage, each spectral patch was encoded independently. Considering that spectral patches taken from a time sequence should keep similar representations for neighboring patches after encoding, in this study, we propose to enhance the temporal correlation of feature representation using a temporal max-smoothing algorithm. The max-smoothing tries to pick up the maximum response in a local time window as the representative feature for detection task. We tested the new feature for automatic detection of acoustic events which were selected from lecture audio data. Experimental results showed that the temporal max-smoothing significantly improved the performance.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-228",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "anushiyarachel15_interspeech": {
      "authors": [
        [
          "G",
          "Anushiya Rachel"
        ],
        [
          "P",
          "Vijayalakshmi"
        ],
        [
          "T",
          "Nagarajan"
        ]
      ],
      "title": "Estimation of glottal closure instants from telephone speech using a group delay-based approach that considers speech signal as a spectrum",
      "original": "i15_1181",
      "page_count": 5,
      "order": 238,
      "p1": "1181",
      "pn": "1185",
      "abstract": [
        "Glottal closure instants (GCIs) are characterized by a strong negative valley in the speech signal and an abrupt change in the amplitude. In this paper, an algorithm that exploits these two properties of a GCI is proposed to estimate the location of GCIs, specifically from telephone speech. The algorithm considers a symmetrized voiced segment as the Fourier transform of an even signal. In such a case, the negative valleys in the spectrum correspond to zeros that lie outside the unit circle in the z-plane. The angular location of these zeros indicate the location of the GCIs. The angular location can be estimated from the group delay spectrum of the even signal, since a phase change of 2\u03c0, between adjacent frequency bins, occurs at the location of a zero that lies outside the unit circle. The performance of the algorithm is evaluated on a simulated speech corpora derived from CMU and CSTR databases and the NTIMIT database, in terms of identification, false alarm, and miss rates. The proposed algorithm is compared with DYPSA, YAGA, and SEDREAMS, and is found to outperform all the algorithms when used on telephone speech.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-229",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "montano15_interspeech": {
      "authors": [
        [
          "Ra\u00fal",
          "Monta\u00f1o"
        ],
        [
          "Francesc",
          "Al\u00edas"
        ]
      ],
      "title": "The role of prosody and voice quality in text-dependent categories of storytelling across languages",
      "original": "i15_1186",
      "page_count": 5,
      "order": 239,
      "p1": "1186",
      "pn": "1190",
      "abstract": [
        "In contrast to full-blown emotions, storytelling speech entails a particular speaking style that contains subtle expressive nuances of which little is known. In the present work, we study the role of prosody and voice quality while searching for cross-linguistic acoustic similarities in two categories of storytelling speech that are defined by their lexical components: the descriptive mode and sentences that specify a character intervention, together with a third neutral category (perceptually validated as reference). The study addresses four narrators using four different European languages (English, French, German and Spanish) expressing the same story. After conducting several statistical and discriminant analyses, we find that all narrators under analysis exploit some acoustic parameters in a similar way to differentiate among the analysed storytelling categories. Specifically, we observe that three prosodic features (mean fundamental frequency, mean intensity and number of silent pauses) and two voice quality parameters (mean Harmonic-to-Noise Ratio and Maxima Dispersion Quotient) explain a relatively similar proportion of the variance among storytelling categories in all languages. Moreover, the classification results obtained from the discriminant analysis are comparable for the three considered storytelling categories across languages.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-230",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "hyafil15_interspeech": {
      "authors": [
        [
          "Alexandre",
          "Hyafil"
        ],
        [
          "Milos",
          "Cernak"
        ]
      ],
      "title": "Neuromorphic based oscillatory device for incremental syllable boundary detection",
      "original": "i15_1191",
      "page_count": 5,
      "order": 240,
      "p1": "1191",
      "pn": "1195",
      "abstract": [
        "Syllables are considered as basic supra-segmental units, used mainly in prosodic modelling. It has long been thought that efficient syllabification algorithms may also provide valuable cues for improved segmental (acoustic) modelling. However, the best current syllabification methods work offline, considering the power envelope of whole utterance.   In this paper we introduce a new method for detection of syllable boundaries based on a model of speech parsing into syllables by neural oscillations in human auditory cortex. Neural oscillations automatically lock to speech slow fluctuations that convey the syllabic rhythm. Similarly as humans encode speech incrementally, i.e., not considering future temporal context, the proposed method works incrementally as well. In addition, it is highly robust to noise. Syllabification performance for English and different noise conditions was compared to the existing Mermelstein and group delay algorithms. While the performance of the existing methods depend on the type of noise and signal to noise ratio, the performance of the proposed method is constant under all noise conditions.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-231",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "lee15b_interspeech": {
      "authors": [
        [
          "Ann",
          "Lee"
        ],
        [
          "James",
          "Glass"
        ]
      ],
      "title": "Mispronunciation detection without nonnative training data",
      "original": "i15_0643",
      "page_count": 5,
      "order": 241,
      "p1": "643",
      "pn": "647",
      "abstract": [
        "Conventional mispronunciation detection systems that have the capability of providing corrective feedback typically require a set of common error patterns that are known beforehand, obtained either by consulting with experts, or from a human-annotated nonnative corpus. In this paper, we propose a mispronunciation detection framework that does not rely on nonnative training data. We first discover an individual learner's possible pronunciation error patterns by analyzing the acoustic similarities across their utterances. With the discovered error candidates, we iteratively compute forced alignments and decode learner-specific context-dependent error patterns in a greedy manner. We evaluate the framework on a Chinese University of Hong Kong (CUHK) corpus containing both Cantonese and Mandarin speakers reading English. Experimental results show that the proposed framework effectively detects mispronunciations and also has a good ability to prioritize feedback.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-232",
      "author_area_id": "10",
      "author_area_label": "Speech Recognition \u2013 Technologies and Systems for New Applications"
    },
    "rasipuram15_interspeech": {
      "authors": [
        [
          "Ramya",
          "Rasipuram"
        ],
        [
          "Milos",
          "Cernak"
        ],
        [
          "Alexandre",
          "Nachen"
        ],
        [
          "Mathew",
          "Magimai-Doss"
        ]
      ],
      "title": "Automatic accentedness evaluation of non-native speech using phonetic and sub-phonetic posterior probabilities",
      "original": "i15_0648",
      "page_count": 5,
      "order": 242,
      "p1": "648",
      "pn": "652",
      "abstract": [
        "Automatic evaluation of non-native speech accentedness has potential implications for not only language learning and accent identification systems but also for speaker and speech recognition systems. From the perspective of speech production, the two primary factors influencing the accentedness are the phonetic and prosodic structure. In this paper, we propose an approach for automatic accentedness evaluation based on comparison of instances of native and non-native speakers at the acoustic-phonetic level. Specifically, the proposed approach measures accentedness by comparing phone class conditional probability sequences corresponding to the instances of native and non-native speakers, respectively. We evaluate the proposed approach on the EMIME bilingual and EMIME Mandarin bilingual corpora, which contains English speech from native English speakers and various non-native English speakers, namely Finnish, German and Mandarin. We also investigate the influence of the granularity of the phonetic unit representation on the performance of the proposed accentedness measure. Our results indicate that the accentedness ratings by the proposed approach correlate consistently with the human ratings of accentedness. In addition, our studies show that the granularity of the phonetic unit representation that yields the best correlation with the human accentedness ratings varies with respect to the native language of the non-native speakers.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-233",
      "author_area_id": "10",
      "author_area_label": "Speech Recognition \u2013 Technologies and Systems for New Applications"
    },
    "ma15c_interspeech": {
      "authors": [
        [
          "Min",
          "Ma"
        ],
        [
          "Keelan",
          "Evanini"
        ],
        [
          "Anastassia",
          "Loukina"
        ],
        [
          "Xinhao",
          "Wang"
        ],
        [
          "Klaus",
          "Zechner"
        ]
      ],
      "title": "Using F0 contours to assess nativeness in a sentence repeat task",
      "original": "i15_0653",
      "page_count": 5,
      "order": 243,
      "p1": "653",
      "pn": "657",
      "abstract": [
        "In this paper, we conduct experiments using F0 contour features to assess the nativeness of responses provided by speakers from India and China to a Sentence Repeat task in an assessment of English speaking proficiency for non-native speakers. The results show that the coefficients from polynomial models of the pitch contours help distinguish between native and non-native speakers, especially among females. We find that the F0 contour can be represented adequately by using only basic statistical variables and the first three orders of polynomial coefficients. In addition, the most important features for classification are presented for each group of speakers. Finally, we discuss the differences among the gender-specific groups of the speakers.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-234",
      "author_area_id": "10",
      "author_area_label": "Speech Recognition \u2013 Technologies and Systems for New Applications"
    },
    "lunsford15_interspeech": {
      "authors": [
        [
          "Rebecca",
          "Lunsford"
        ],
        [
          "Peter A.",
          "Heeman"
        ]
      ],
      "title": "Using linguistic indicators of difficulty to identify mild cognitive impairment",
      "original": "i15_0658",
      "page_count": 5,
      "order": 244,
      "p1": "658",
      "pn": "662",
      "abstract": [
        "Speaking is a complex task, and it is to be expected that speech will be effected when a speaker is faced with cognitive difficulties. To explore how cognitive impairment is manifested in a persons' speech, we compared the speech of elders diagnosed with Mild Cognitive Impairment (MCI) to others who are cognitively intact, while the speakers attempt to retell a story they just heard. We found that the speakers with impairment, as compared to those who are cognitively intact, spent more time engaged in verbalized hesitations (e.g., \u201cand um \u2026\u201d) prior to speaking story content, and that these verbalized hesitations accounted for a larger ratio of the time spent retelling. In addition, we found that a higher percentage of the impaired speakers used phrases such as \u201cI guess\u201d and \u201cI can't recall\u201d to qualify content they were unsure of, or to replace details they couldn't recall. These results provide insight into how speakers manage cognitive impairment, suggesting that these indicators of difficulty could be used to assist in early diagnosis of MCI.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-235",
      "author_area_id": "10",
      "author_area_label": "Speech Recognition \u2013 Technologies and Systems for New Applications"
    },
    "fontan15_interspeech": {
      "authors": [
        [
          "Lionel",
          "Fontan"
        ],
        [
          "J\u00e9r\u00f4me",
          "Farinas"
        ],
        [
          "Isabelle",
          "Ferran\u00e9"
        ],
        [
          "Julien",
          "Pinquier"
        ],
        [
          "Xavier",
          "Aumont"
        ]
      ],
      "title": "Automatic intelligibility measures applied to speech signals simulating age-related hearing loss",
      "original": "i15_0663",
      "page_count": 5,
      "order": 245,
      "p1": "663",
      "pn": "667",
      "abstract": [
        "This research work forms the first part of a long-term project designed to provide a framework for facilitating hearing aids tuning. The present study focuses on the setting up of automatic measures of speech intelligibility for the recognition of isolated words and sentences. Both materials were degraded in order to simulate presbycusis effects on speech perception. Automatic measures based on an Automatic Speech Recognition (ASR) system were applied to an audio corpus simulating the effects of presbycusis at nine severity stages. The results are compared to reference intelligibility scores collected from 60 French listeners. The aim of this system being to produce measures as close as possible to human behaviour, good performances were achieved since strong correlations between subjective and objective scores are observed.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-236",
      "author_area_id": "10",
      "author_area_label": "Speech Recognition \u2013 Technologies and Systems for New Applications"
    },
    "chakravarthula15_interspeech": {
      "authors": [
        [
          "Sandeep Nallan",
          "Chakravarthula"
        ],
        [
          "Bo",
          "Xiao"
        ],
        [
          "Zac E.",
          "Imel"
        ],
        [
          "David C.",
          "Atkins"
        ],
        [
          "Panayiotis G.",
          "Georgiou"
        ]
      ],
      "title": "Assessing empathy using static and dynamic behavior models based on therapist's language in addiction counseling",
      "original": "i15_0668",
      "page_count": 5,
      "order": 246,
      "p1": "668",
      "pn": "672",
      "abstract": [
        "Empathy by the counselor is an important measure of treatment quality in psychotherapy. It is a behavioral process that involves understanding and sharing the experiences and emotions of a person over the course of an interaction. While a complex phenomenon, human behavior can at moments be perceived as strongly empathetic or non-empathetic. Currently, manual coding of behavior and behavioral signal processing models of empathy often pose the unnatural assumption that empathy is constant throughout the interaction. In this work we investigate two models: Static Behavior Model (SBM) that assumes a fixed degree of empathy throughout an interaction; and a context-dependent Dynamic Behavior Model (DBM), which assumes a Hidden Markov Model, allowing transitions between high- and low- empathy states. Through the non-causal human perception mechanisms, these states can be perceived and integrated as high- or low- gestalt empathy. We show that the DBM performs better than the SBM, while as a byproduct, generating local labels that may be of use to domain experts. We also demonstrate the robustness of both SBM and DBM to transcription errors stemming from ASR rather than human transcriptions. Our results suggest that empathy manifests itself in different forms over time and is best captured by context-dependent models.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-237",
      "author_area_id": "10",
      "author_area_label": "Speech Recognition \u2013 Technologies and Systems for New Applications"
    },
    "liu15b_interspeech": {
      "authors": [
        [
          "Yuzong",
          "Liu"
        ],
        [
          "Rishabh",
          "Iyer"
        ],
        [
          "Katrin",
          "Kirchhoff"
        ],
        [
          "Jeff",
          "Bilmes"
        ]
      ],
      "title": "SVitchboard II and fiSVer i: high-quality limited-complexity corpora of conversational English speech",
      "original": "i15_0673",
      "page_count": 5,
      "order": 247,
      "p1": "673",
      "pn": "677",
      "abstract": [
        "In this paper, we introduce a set of benchmark corpora of conversational English speech derived from the Switchboard-I and Fisher datasets. Traditional ASR research requires considerable computational resources and has slow experimental turnaround times. Our goal is to introduce these new datasets to researchers in the ASR and machine learning communities (especially in academia), in order to facilitate the development of novel acoustic modeling techniques on smaller but acoustically rich corpora. We select these corpora to maximize an acoustic quality criterion while limiting the vocabulary size (from 10 words up to 10,000 words) with different state-of-the-art submodular function optimization algorithms. We provide baseline word recognition results for both GMM and DNN-based systems and release the corpora definitions and Kaldi training recipes to the public.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-238",
      "author_area_id": "10",
      "author_area_label": "Speech Recognition \u2013 Technologies and Systems for New Applications"
    },
    "kamper15_interspeech": {
      "authors": [
        [
          "Herman",
          "Kamper"
        ],
        [
          "Aren",
          "Jansen"
        ],
        [
          "Sharon",
          "Goldwater"
        ]
      ],
      "title": "Fully unsupervised small-vocabulary speech recognition using a segmental Bayesian model",
      "original": "i15_0678",
      "page_count": 5,
      "order": 248,
      "p1": "678",
      "pn": "682",
      "abstract": [
        "Current supervised speech technology relies heavily on transcribed speech and pronunciation dictionaries. In settings where unlabelled speech data alone is available, unsupervised methods are required to discover categorical linguistic structure directly from the audio. We present a novel Bayesian model which segments unlabelled input speech into word-like units, resulting in a complete unsupervised transcription of the speech in terms of discovered word types. In our approach, a potential word segment (of arbitrary length) is embedded in a fixed-dimensional space; the model (implemented as a Gibbs sampler) then builds a whole-word acoustic model in this space while jointly doing segmentation. We report word error rates in a connected digit recognition task by mapping the unsupervised output to ground truth transcriptions. Our model outperforms a previously developed HMM-based system, even when the model is not constrained to discover only the 11 word types present in the data.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-239",
      "author_area_id": "10",
      "author_area_label": "Speech Recognition \u2013 Technologies and Systems for New Applications"
    },
    "tilk15_interspeech": {
      "authors": [
        [
          "Ottokar",
          "Tilk"
        ],
        [
          "Tanel",
          "Alum\u00e4e"
        ]
      ],
      "title": "LSTM for punctuation restoration in speech transcripts",
      "original": "i15_0683",
      "page_count": 5,
      "order": 249,
      "p1": "683",
      "pn": "687",
      "abstract": [
        "The output of automatic speech recognition systems is generally an unpunctuated stream of words which is hard to process for both humans and machines. We present a two-stage recurrent neural network based model using long short-term memory units to restore punctuation in speech transcripts. In the first stage, textual features are learned on a large text corpus. The second stage combines textual features with pause durations and adapts the model to speech domain. Our approach reduces the number of punctuation errors by up to 16.9% when compared to a decision tree that combines hidden-event language model posteriors with inter-word pause information, having largest improvements in period restoration.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-240",
      "author_area_id": "10",
      "author_area_label": "Speech Recognition \u2013 Technologies and Systems for New Applications"
    },
    "ylmaz15_interspeech": {
      "authors": [
        [
          "Emre",
          "Y\u0131lmaz"
        ],
        [
          "Deepak",
          "Baby"
        ],
        [
          "Hugo",
          "Van hamme"
        ]
      ],
      "title": "Noise robust exemplar matching for speech enhancement: applications to automatic speech recognition",
      "original": "i15_0688",
      "page_count": 5,
      "order": 250,
      "p1": "688",
      "pn": "692",
      "abstract": [
        "We present a novel automatic speech recognition (ASR) scheme which uses the recently proposed noise robust exemplar matching framework for speech enhancement in the front-end. The proposed system employs a GMM-HMM back-end to recognize the enhanced speech signals unlike the prior work focusing on template matching only. Speech enhancement is achieved using multiple dictionaries containing speech exemplars representing a single speech unit and several noise exemplars of the same length. These combined dictionaries are used to approximate the noisy segments and the speech component is obtained as a linear combination of the speech exemplars in the combined dictionaries yielding the minimum total reconstruction error. The performance of the proposed system is evaluated on the small vocabulary track of the 2nd CHiME Challenge and the AURORA-2 database and the results have shown the effectiveness of the proposed approach in improving the noise robustness of a conventional ASR system.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-241",
      "author_area_id": "10",
      "author_area_label": "Speech Recognition \u2013 Technologies and Systems for New Applications"
    },
    "gao15_interspeech": {
      "authors": [
        [
          "Yingming",
          "Gao"
        ],
        [
          "Yanlu",
          "Xie"
        ],
        [
          "Wen",
          "Cao"
        ],
        [
          "Jinsong",
          "Zhang"
        ]
      ],
      "title": "A study on robust detection of pronunciation erroneous tendency based on deep neural network",
      "original": "i15_0693",
      "page_count": 4,
      "order": 251,
      "p1": "693",
      "pn": "696",
      "abstract": [
        "Compared with scoring feedbacks, instructive feedbacks are more demanded by language learners using computer aided pronunciation training (CAPT) systems, which require detailed information about erroneous pronunciations along with phone errors. Pronunciation erroneous tendency (PET) defines a set of incorrect articulation configurations regarding main articulators and uttering manners for the phones respectively, and its robust detection contributes to the provision of appropriate instructive feedbacks. In our previous works, we designed a set of PET labels for CSL (Chinese as a second language) by Japanese learners, and conducted a preliminary detection study with GMM-HMM. This study is aimed at achieving a more robust detection of PETs by two approaches: employing DNN-HMM as the acoustic modeling, and comparing three kinds of acoustic features: MFCC, PLP, and filter-bank. Experimental results showed that the DNN-HMM PET modeling achieved more robust detection accuracies than the previous GMM-HMM, and the three kinds of features behaved differently. A lattice combination of the results of three feature systems led to the best PET results: FRR of 5.5%, FAR of 35.6%, and DA of 88.6%, which showed its efficiency.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-242",
      "author_area_id": "10",
      "author_area_label": "Speech Recognition \u2013 Technologies and Systems for New Applications"
    },
    "joshi15_interspeech": {
      "authors": [
        [
          "Shrikant",
          "Joshi"
        ],
        [
          "Nachiket",
          "Deo"
        ],
        [
          "Preeti",
          "Rao"
        ]
      ],
      "title": "Vowel mispronunciation detection using DNN acoustic models with cross-lingual training",
      "original": "i15_0697",
      "page_count": 5,
      "order": 252,
      "p1": "697",
      "pn": "701",
      "abstract": [
        "We address the automatic detection of phone-level mispronunciation for feedback in a computer-aided language learning task where the target language data (Indian English) is limited. Based on the recent success of DNN acoustic models on limited resource recognition tasks, we compare different methods of utilizing the limited target language data in the training of acoustic models that are initialized with multilingual data. Frame-level DNN posteriors obtained by the different training methods are compared in a phone classification task with a baseline GMM/HMM system. A judicious use of domain knowledge in terms of L2 phonology and L1 interference, that includes influence on phone quality and duration, are applied to the design of confidence scores for mispronunciation detection of vowels of Indian English as spoken by Gujarati L1 learners. We also show that the pronunciation error detection system benefits from a more precise signal-based segmentation of the test speech vowels, as would be expected due to the now more reliable frame-based confidence scores.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-243",
      "author_area_id": "10",
      "author_area_label": "Speech Recognition \u2013 Technologies and Systems for New Applications"
    },
    "kumar15b_interspeech": {
      "authors": [
        [
          "Kshitiz",
          "Kumar"
        ],
        [
          "Ziad Al",
          "Bawab"
        ],
        [
          "Yong",
          "Zhao"
        ],
        [
          "Chaojun",
          "Liu"
        ],
        [
          "Benoit",
          "Dumoulin"
        ],
        [
          "Yifan",
          "Gong"
        ]
      ],
      "title": "Confidence-features and confidence-scores for ASR applications in arbitration and DNN speaker adaptation",
      "original": "i15_0702",
      "page_count": 5,
      "order": 253,
      "p1": "702",
      "pn": "706",
      "abstract": [
        "Speech recognition confidence-scores quantitatively represent correctness of decoded utterances in a [0,1] range. Confidences have primarily been used to filter out recognitions with scores below a threshold. They have also been used in other speech applications in e.g. arbitration, ROVER, and high-quality data selection for model training etc. Confidence-scores are computed from a rich set of confidence-features in the speech recognition engine. While many speech applications consume confidence scores, we haven't seen adequate focus on directly consuming confidence-features in applications. In this work we build a thesis that additionally consuming confidence-features can provide big gains across confidence-related tasks. We demonstrate this for arbitration application, where we obtain 31% relative reduction in arbitration metric. We additionally demonstrate a novel application of confidence-scores in deep-neural-network (DNN) adaptation, where we strongly improve the relative reduction in word-error-rate (WER) for speaker adaptation on limited data.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-244",
      "author_area_id": "10",
      "author_area_label": "Speech Recognition \u2013 Technologies and Systems for New Applications"
    },
    "liu15c_interspeech": {
      "authors": [
        [
          "Pengfei",
          "Liu"
        ],
        [
          "Shoaib",
          "Jameel"
        ],
        [
          "Wai",
          "Lam"
        ],
        [
          "Bin",
          "Ma"
        ],
        [
          "Helen",
          "Meng"
        ]
      ],
      "title": "Topic modeling for conference analytics",
      "original": "i15_0707",
      "page_count": 5,
      "order": 254,
      "p1": "707",
      "pn": "711",
      "abstract": [
        "This work presents our attempt to understand the research topics that characterize the papers submitted to a conference, by using topic modeling and data visualization techniques. We infer the latent topics from the abstracts of all the papers submitted to Interspeech2014 by means of Latent Dirichlet Allocation. Per-topic word distributions thus obtained are visualized through word clouds. We also compare the automatically inferred topics against the expert-defined topics (also known as tracks for Interspeech2014). The comparison is based on an information retrieval framework, where we use each latent topic as a query and each track as a document. For each latent topic, we retrieve a ranked list of tracks scored by the degree of word overlap. Each latent topic is associated with the top-scoring track. This analytic procedure was applied to all submissions to Interspeech2014 and sheds some interesting light in terms of providing an overview of topic categorization in the conference, popular versus unpopular topics, emerging topics and topic compositions. Such insights are potentially valuable for understanding the technical content of a field and planning the future development of its conference(s).\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-245",
      "author_area_id": "10",
      "author_area_label": "Speech Recognition \u2013 Technologies and Systems for New Applications"
    },
    "sharma15_interspeech": {
      "authors": [
        [
          "Pulkit",
          "Sharma"
        ],
        [
          "Vinayak",
          "Abrol"
        ],
        [
          "A. D.",
          "Dileep"
        ],
        [
          "Anil Kumar",
          "Sao"
        ]
      ],
      "title": "Sparse coding based features for speech units classification",
      "original": "i15_0712",
      "page_count": 4,
      "order": 255,
      "p1": "712",
      "pn": "715",
      "abstract": [
        "In this paper a sparse representation based feature is proposed for the tasks in speech recognition. Dictionary plays an important role in order to get a good sparse representation. Therefore instead of using a single over complete dictionary, multiple signal adaptive dictionaries are used. A novel principal component analysis (PCA) based method is proposed to learn multiple dictionaries for each speech unit. For a given speech frame, first minimum distance criterion is employed to select appropriate dictionary and then a sparse solver is used to compute sparse feature for acoustic modeling. Experiments are performed using different datasets, which shows the proposed feature outperforms the existing features in recognition of isolated utterances.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-246",
      "author_area_id": "10",
      "author_area_label": "Speech Recognition \u2013 Technologies and Systems for New Applications"
    },
    "niculescu15_interspeech": {
      "authors": [
        [
          "Andreea I.",
          "Niculescu"
        ],
        [
          "Ngoc Thuy Huong",
          "Thai"
        ],
        [
          "Chongjia",
          "Ni"
        ],
        [
          "Boon Pang",
          "Lim"
        ],
        [
          "Kheng Hui",
          "Yeo"
        ],
        [
          "Rafael E.",
          "Banchs"
        ]
      ],
      "title": "Smarter driving with IDA, the intelligent driving assistant for singapore",
      "original": "i15_0716",
      "page_count": 2,
      "order": 256,
      "p1": "716",
      "pn": "717",
      "abstract": [
        "In this paper we present our works towards creating a natural language platform for an intelligent driving assistant ( IDA) for smart parking in Singapore. In particular, we are focusing on the challenges of designing and implementing reliable spoken dialogue components that enable drivers to communicate hands-free with the system. These components require: spoken language dialogue design, data collection, as well as training of speech recognition (ASR) and natural language understanding (NLU) modules. The main objective of IDA is to help drivers to find suitable parking, online monitor car park availability and redirect drivers when the number of free available spots drops to a critical level. As such, this speech-enabled application contributes to a more sustainable city by decreasing traffic congestion, fuel expenses and time waste for all drivers on the road.\n",
        ""
      ]
    },
    "yeo15_interspeech": {
      "authors": [
        [
          "Kheng Hui",
          "Yeo"
        ],
        [
          "Rafael E.",
          "Banchs"
        ]
      ],
      "title": "Talk it out: adding speech interaction to support informational and transactional applications on public touch-screen kiosks",
      "original": "i15_0718",
      "page_count": 2,
      "order": 257,
      "p1": "718",
      "pn": "719",
      "abstract": [
        "In this paper we present a method that enables people to interact with large touch-screen displays using spoken language. Such panels are commonly used in public areas for information and advertisement. A personal mobile device is used to convert the speech signal into text, which is then sent to the panel via an Internet connection. This allows speaker adaptation and background noise issues to be mitigated. Information may also be sent to the user for later reference on their personal device.\n",
        ""
      ]
    },
    "dharo15_interspeech": {
      "authors": [
        [
          "Luis Fernando",
          "D'Haro"
        ],
        [
          "Seokhwan",
          "Kim"
        ],
        [
          "Rafael E.",
          "Banchs"
        ]
      ],
      "title": "Conversational agent and management tools for conference and tourism domain",
      "original": "i15_0720",
      "page_count": 2,
      "order": 258,
      "p1": "720",
      "pn": "721",
      "abstract": [
        "In this paper we describe a platform and a set of useful tools that allow the fast creation of conversation agents for two different domain applications: a conference information system and local tourist guide. The paper provides detailed descriptions of the implemented tools for developer management and automatic extraction of conference-dependent information. Finally, we provide some usage statistics of the agent that was included in the mobile APP used last year at Interspeech conference.\n",
        ""
      ]
    },
    "salimbajevs15_interspeech": {
      "authors": [
        [
          "Askars",
          "Salimbajevs"
        ],
        [
          "Jevgenijs",
          "Strigins"
        ]
      ],
      "title": "Latvian speech-to-text transcription service",
      "original": "i15_0722",
      "page_count": 2,
      "order": 259,
      "p1": "722",
      "pn": "723",
      "abstract": [
        "In this demonstration paper, we introduce the first publicly available Speech-To-Text transcription service for the Latvian language. We present its main features, the details of automatic speech recognition (ASR) system used in this service, software architecture, and an evaluation of recognition quality. The service will provide regular people with the opportunity to transcribe their own audio files for various purposes, such as lectures, meetings, etc. Also, the users will be given an opportunity to give their evaluation and feedback about the quality and usability of this service, which will be used by developers to make changes in the ASR in order to improve it.\n",
        ""
      ]
    },
    "gaka15_interspeech": {
      "authors": [
        [
          "Jakub",
          "Ga\u0142ka"
        ],
        [
          "Joanna",
          "Grzybowska"
        ],
        [
          "Magdalena",
          "Igras"
        ],
        [
          "Pawe\u0142",
          "Jaci\u00f3w"
        ],
        [
          "Kamil",
          "Wajda"
        ],
        [
          "Marcin",
          "Witkowski"
        ],
        [
          "Mariusz",
          "Zi\u00f3\u0142ko"
        ]
      ],
      "title": "System supporting speaker identification in emergency call center",
      "original": "i15_0724",
      "page_count": 2,
      "order": 260,
      "p1": "724",
      "pn": "725",
      "abstract": [
        "A supporting system of voice analysis for emergency call centers is being developed at AGH University of Science and Technology in Krakow. The aim of our work is to provide an innovative supporting tool for rapid and accurate assessment of caller profile. The project covers not only speaker identification (when speaker's speech sample is known), but also speaker's gender and age detection, recognition of emotions, recognition of acoustic background. The system consists of: speech signal analysis, voiceprints learning, adaptation and classification.\n",
        ""
      ]
    },
    "abdelali15_interspeech": {
      "authors": [
        [
          "Ahmed",
          "Abdelali"
        ],
        [
          "Ahmed",
          "Ali"
        ],
        [
          "Francisco",
          "Guzm\u00e1n"
        ],
        [
          "Felix",
          "Stahlberg"
        ],
        [
          "Stephan",
          "Vogel"
        ],
        [
          "Yifan",
          "Zhang"
        ]
      ],
      "title": "QAT<SUP>2</SUP> \u2014 the QCRI advanced transcription and translation system",
      "original": "i15_0726",
      "page_count": 2,
      "order": 261,
      "p1": "726",
      "pn": "727",
      "abstract": [
        "QAT2 is a multimedia content translation web service developed by QCRI to help content provider to reach audiences and viewers speaking different languages. It is built with establishing open source technologies such as KALDI, Moses and MaryTTS, to provide a complete translation experience for web users. It translates text content in its original format, and produce translated videos with speech-to-speech translation. The result is a complete native language experience for end users on foreign language websites. The system currently supports translation from Arabic to English.\n",
        ""
      ]
    },
    "stadtschnitzer15_interspeech": {
      "authors": [
        [
          "Michael",
          "Stadtschnitzer"
        ],
        [
          "Christoph",
          "Schmidt"
        ]
      ],
      "title": "Implementation of a live dialectal media subtitling system",
      "original": "i15_0728",
      "page_count": 2,
      "order": 262,
      "p1": "728",
      "pn": "729",
      "abstract": [
        "Subtitling is a useful technique to fulfil the information needs of deaf and hearing impaired people. Live subtitling is needed especially for live events and is not restricted to television, but can also be provided to persons on site, e.g. to a deaf politician during a parliamentary debate. Live subtitling is demanding since the audio information has to be transformed into text within a few seconds. In this demonstration we present the Fraunhofer IAIS audio and video live subtitling system for standard German and Bavarian. The system was developed in the \u201cLive-Caption\u201d project and consists of an online speech recognition and speaker diarisation system. We employ our entire large annotated standard German broadcast corpus for training. In addition, we apply the system to Bavarian dialect by adapting the acoustic models and the pronunciation lexicon, exploiting a Bavarian media corpus. Due to the real-time restrictions and the spontaneous character of dialectal speech, the system performance is far from perfect, but encouraging.\n",
        ""
      ]
    },
    "bell15_interspeech": {
      "authors": [
        [
          "Peter",
          "Bell"
        ],
        [
          "Catherine",
          "Lai"
        ],
        [
          "Clare",
          "Llewellyn"
        ],
        [
          "Alexandra",
          "Birch"
        ],
        [
          "Mark",
          "Sinclair"
        ]
      ],
      "title": "A system for automatic broadcast news summarisation, geolocation and translation",
      "original": "i15_0730",
      "page_count": 2,
      "order": 263,
      "p1": "730",
      "pn": "731",
      "abstract": [
        "An increasing amount of news content is produced in audio-video form every day. To effectively analyse and monitoring this multilingual data stream, we require methods to extract and present audio content in accessible ways. In this paper, we describe an end-to-end system for processing and browsing audio news data. This fully automated system brings together our recent research on audio scene analysis, speech recognition, summarisation, named entity detection, geolocation, and machine translation. The graphical interface allows users to visualise the distribution of news content by entity names and story location. Browsing of news events is facilitated through extractive summaries and the ability to view transcripts in multiple languages.\n",
        ""
      ]
    },
    "znotins15_interspeech": {
      "authors": [
        [
          "Art\u016brs",
          "Znoti\u0146\u0161"
        ],
        [
          "Kaspars",
          "Polis"
        ],
        [
          "Roberts",
          "Dar\u0123is"
        ]
      ],
      "title": "Media monitoring system for latvian radio and TV broadcasts",
      "original": "i15_0732",
      "page_count": 2,
      "order": 264,
      "p1": "732",
      "pn": "733",
      "abstract": [
        "Media monitoring allows to capture media exposure of people, organizations and other important topics. This paper presents a media monitoring system for Latvian radio and television broadcasts. This system uses an automatic speech recognition (ASR) module to convert audio and video files to text and to extract keywords of interest. The system has been developed in close cooperation with Latvian information agency LETA.\n",
        ""
      ]
    },
    "assayag15_interspeech": {
      "authors": [
        [
          "Michel",
          "Assayag"
        ],
        [
          "Jonathan",
          "Huang"
        ],
        [
          "Jonathan",
          "Mamou"
        ],
        [
          "Oren",
          "Pereg"
        ],
        [
          "Saurav",
          "Sahay"
        ],
        [
          "Oren",
          "Shamir"
        ],
        [
          "Georg",
          "Stemmer"
        ],
        [
          "Moshe",
          "Wasserblat"
        ]
      ],
      "title": "Meeting assistant application",
      "original": "i15_0734",
      "page_count": 2,
      "order": 265,
      "p1": "734",
      "pn": "735",
      "abstract": [
        "This paper describes the Meeting Assistant application developed at Intel. Unlike existing human-to-machine solutions, the challenges induced by human-to-human conversations are currently poorly addressed by the industry. In this paper, we describe the capabilities of in-house speech and NLP technologies: online automatic speech recognition, speaker diarization, keyphrase extraction and sentiment detection. These technologies has been adapted to conversational speech domain and integrated into the Meeting Assistant.\n",
        ""
      ]
    },
    "zioko15_interspeech": {
      "authors": [
        [
          "Bartosz",
          "Zi\u00f3\u0142ko"
        ],
        [
          "Tomasz",
          "Jadczyk"
        ],
        [
          "Dawid",
          "Skurzok"
        ],
        [
          "Piotr",
          "\u017belasko"
        ],
        [
          "Jakub",
          "Ga\u0142ka"
        ],
        [
          "Tomasz",
          "P\u0229dzim\u0105\u017c"
        ],
        [
          "Ireneusz",
          "Gawlik"
        ],
        [
          "Szymon",
          "Pa\u0142ka"
        ]
      ],
      "title": "SARMATA 2.0 automatic Polish language speech recognition system",
      "original": "i15_1062",
      "page_count": 2,
      "order": 266,
      "p1": "1062",
      "pn": "1063",
      "abstract": [
        "A speech recognition system for the Polish language is described. The presentation will focus on an adjustment of the Kaldi toolkit for Polish, our own grapheme to phoneme conversion tool and a corpus of Polish we collected. The approaches to commercial applications will also be described.\n",
        ""
      ]
    },
    "faria15_interspeech": {
      "authors": [
        [
          "Arlo",
          "Faria"
        ],
        [
          "Korbinian",
          "Riedhammer"
        ]
      ],
      "title": "Remeeting \u2014 get more out of meetings",
      "original": "i15_1064",
      "page_count": 2,
      "order": 267,
      "p1": "1064",
      "pn": "1065",
      "abstract": [
        "Remeeting is a tool that helps you get more out of in-person meetings. Calendar integration and a special email address allow users to email agenda items prior to a certain meeting. A discrete notification at the time of the meeting reminds the user to start the recording. During the meeting, the user focuses on the conversation, or can add notes and photos if desired. After the meeting, every participant gets notified by an automated email that lists the participants along with automatically extracted keywords, notes and photos. This stimulates collaboration, and keeps follow-up contributions at a central place: Just reply to add further notes to the meeting. The resulting meeting \u201cdocument\u201d can be shared with others and reviewed using a web app that acts as a visual index to the meeting. This makes Remeeting the perfect tool for regular group meetings, standups and interviews, where people typically track progress and follow up on. Remeeting is leveraging, promoting and contributing to open source projects including kaldi and docker.\n",
        ""
      ]
    },
    "masudakatsuse15_interspeech": {
      "authors": [
        [
          "Ikuyo",
          "Masuda-Katsuse"
        ]
      ],
      "title": "Web application system for pronunciation practice by children with disabilities and to support cooperation of teachers and medical workers",
      "original": "i15_1066",
      "page_count": 2,
      "order": 268,
      "p1": "1066",
      "pn": "1067",
      "abstract": [
        "We developed a Web application system for children with pronunciation difficulties to practice pronunciation. The system users are assigned one of the following authorizations: student, teacher, medical worker, or speech evaluator. The teachers, medical workers, and speech evaluators are grouped by the student to whom they are linked and can access exercise records and student speech sounds by the Internet. The teachers can individually tailor practice words to each child's pronunciation needs. The medical workers and speech evaluators confirm the accuracy of the student's pronunciation and can share information with teachers. Thus, our system will encourage students to practice their pronunciation and promote the cooperation of teachers and medical workers for more effective instruction.\n",
        ""
      ]
    },
    "kaufhold15_interspeech": {
      "authors": [
        [
          "Caroline",
          "Kaufhold"
        ],
        [
          "Vadim",
          "Gamidov"
        ],
        [
          "Andreas",
          "Kiessling"
        ],
        [
          "Klaus",
          "Reinhard"
        ],
        [
          "Elmar",
          "N\u00f6th"
        ]
      ],
      "title": "PATSY \u2014 it's all about pronunciation!",
      "original": "i15_1068",
      "page_count": 2,
      "order": 269,
      "p1": "1068",
      "pn": "1069",
      "abstract": [
        "PATSY is an abbreviation for its German name \u201cPiloten/ATC Trainingssystem f\u00fcr den Sprechfunk\u201d which translates to pilot/ air traffic control (ATC) training system for radio communication. The phraseology training system is intended to be a stand-alone, platform-independent, multi-user e-Learning framework for learning and practicing the ATC radio communication wordings and at the same time improving intelligibility of the speaker by pronunciation scoring. A serious gaming approach is aimed at, which allows the user to practice his or her communication skills in almost real-life scenarios. At the Show and Tell session at Interspeech 2015, we would like to present a subsystem of PATSY in which we only concentrate on pronunciation scoring. The speaker is prompted to record a sequence of words of the NATO phonetic alphabet and he is given back a visually enhanced feedback regarding his pronunciation score. Further aspects of PATSY are the verification of correct syntax and the assessment of the speaker's \u201ccomfort level\u201d which states how familiar the speaker is with the topic in question.\n",
        ""
      ]
    },
    "azarov15_interspeech": {
      "authors": [
        [
          "Elias",
          "Azarov"
        ],
        [
          "Maxim",
          "Vashkevich"
        ],
        [
          "Denis",
          "Likhachov"
        ],
        [
          "Alexander",
          "Petrovsky"
        ]
      ],
      "title": "Real-time pitch modification system for speech and singing voice",
      "original": "i15_1070",
      "page_count": 2,
      "order": 270,
      "p1": "1070",
      "pn": "1071",
      "abstract": [
        "A real-time pitch modification system has been developed. The implemented processing scheme is based on hybrid deterministic/stochastic decomposition of the signal and includes extraction of instantaneous pitch, pitch-synchronous time-frequency analysis, parametrical morphing and synthesis. The scheme provides high quality output with considerably high naturalness. The aim of the presentation is to show capabilities of the designed real-time signal processing framework. The system implements speech-specific intonation change routines such as lowering, uplifting, tremolo etc. In order to make the presentation more expressive we designed a special singing mode in which the system automatically corrects wrong notes. The target melody and voice effects are specified using musical instruments digital interface (MIDI).\n",
        ""
      ]
    },
    "duplessis15_interspeech": {
      "authors": [
        [
          "Guillaume Dubuisson",
          "Duplessis"
        ],
        [
          "Lucile",
          "B\u00e9chade"
        ],
        [
          "Mohamed A.",
          "Sehili"
        ],
        [
          "Agn\u00e8s",
          "Delaborde"
        ],
        [
          "Vincent",
          "Letard"
        ],
        [
          "Anne-Laure",
          "Ligozat"
        ],
        [
          "Paul",
          "Del\u00e9glise"
        ],
        [
          "Yannick",
          "Est\u00e8ve"
        ],
        [
          "Sophie",
          "Rosset"
        ],
        [
          "Laurence",
          "Devillers"
        ]
      ],
      "title": "Nao is doing humour in the CHIST-ERA joker project",
      "original": "i15_1072",
      "page_count": 2,
      "order": 271,
      "p1": "1072",
      "pn": "1073",
      "abstract": [
        "We present automatic systems that implement multimodal social dialogues involving humour with the humanoid robot Nao for the 16th Interspeech conference. Humorous capabilities of the systems are based on three main techniques: riddles, challenging the human participant, and punctual interventions. The presented prototypes will automatically record and analyse audio and video streams to provide a real-time feedback. Using these systems, we expect to observe rich and varied reactions from international English-speaking volunteers to humorous stimuli triggered by Nao.\n",
        ""
      ]
    },
    "lange15_interspeech": {
      "authors": [
        [
          "Lisa",
          "Lange"
        ],
        [
          "Bartholom\u00e4us",
          "Pfeiffer"
        ],
        [
          "Daniel",
          "Duran"
        ]
      ],
      "title": "ABIMS \u2014 auditory bewildered interaction measurement system",
      "original": "i15_1074",
      "page_count": 2,
      "order": 272,
      "p1": "1074",
      "pn": "1075",
      "abstract": [
        "We present a novel computer game-based framework for phonetic perception experiments. We employ the game environment of a first-person shooter game where players interact with and respond to animated creatures in different virtual environments. Whenever the player encounters such an animated creature, a sound stimulus is played and the creature changes its color depending on the type of stimulus. Throughout the game, colors are getting harder to distinguish until the two creature types (corresponding to two stimulus types) are virtually identical in color. Data can be collected in various ways and without the need for specific additional tests. A first pilot study was conducted in which the game was well received by the subjects and which already highlights the potential of this investigative framework.\n",
        ""
      ]
    },
    "berkling15_interspeech": {
      "authors": [
        [
          "Kay",
          "Berkling"
        ],
        [
          "Nadine",
          "Pflaumer"
        ],
        [
          "Alexei",
          "Coyplove"
        ]
      ],
      "title": "Phontasia \u2014 a game for training German orthography",
      "original": "i15_1874",
      "page_count": 2,
      "order": 273,
      "p1": "1874",
      "pn": "1875",
      "abstract": [
        "For the English language, decades of research have gone into the study of phonics, the systematic instruction of letter to sound connections in context, culminating in the National Reading Panel in 2000 that anchored phonics in the US elementary curriculum. It has affirmed that phonics is an important ingredient in standard teaching methodologies for English. No similar approach exists today in German elementary school books. Recently, the syllable method is starting to grow next to other popular methods like whole-word approach or teaching that there is a 1-1 Grapheme-Phoneme correspondence. In our work, we have started to look at structured teaching of German orthography through a game that uses speech synthesis and pre-structured syllables in order to teach regular patterns of grapheme usage to children. Previous publications have shown some success. The game will be available for users to test out their German skills.\n",
        ""
      ]
    },
    "wong15b_interspeech": {
      "authors": [
        [
          "Ka Ho",
          "Wong"
        ],
        [
          "Wai Kim",
          "Leung"
        ],
        [
          "Helen",
          "Meng"
        ]
      ],
      "title": "E-commu-book: an assistive technology for users with speech impairments",
      "original": "i15_1876",
      "page_count": 2,
      "order": 274,
      "p1": "1876",
      "pn": "1877",
      "abstract": [
        "We are developing a voice-enabled, mobile and customizable electronic communication book (e-Commu-Book) as an aid for people with speech impairment. When a user touches an image icon on the e-Commu-Book, the caption under the image icon will be read out by speech synthesis. The caregivers of the users can help customize contents to fit the users' needs. The record of a user's behavior can be uploaded to the cloud for future analysis so as to improve the user experience of the e-Commu-Book.\n",
        ""
      ]
    },
    "rothlisberger15_interspeech": {
      "authors": [
        [
          "Martina",
          "R\u00f6thlisberger"
        ],
        [
          "Iliana I.",
          "Karipidis"
        ],
        [
          "Georgette",
          "Pleisch"
        ],
        [
          "Volker",
          "Dellwo"
        ],
        [
          "Ulla",
          "Richardson"
        ],
        [
          "Silvia",
          "Brem"
        ]
      ],
      "title": "Swiss graphogame: concept and design presentation of a computerised reading intervention for children with high risk for poor reading outcomes",
      "original": "i15_1878",
      "page_count": 2,
      "order": 275,
      "p1": "1878",
      "pn": "1879",
      "abstract": [
        "Developmental dyslexia is found in 30 to 65% of the children from high risk families (parent or sibling affected). The computerised GraphoGame training program aims to improve reading acquisition for children with a high risk for developing reading and spelling problems. GraphoGame is a learning platform developed at the University of Jyv\u00e4skyl\u00e4 in Finland to support poor reading children during reading acquisition. Here, we present the concept and design of the new Swiss GraphoGame, which at a linguistic level has been developed to especially suit the needs of children speaking an orthographically semi-transparent language (German).   The efficacy of this computerised intervention is currently being evaluated with (Swiss) German speaking children at risk for reading and spelling disabilities in the middle and the end of first grade. Upon successful evaluation of this intervention, the computerised training game may be used by parents, teachers and therapists to help children with special needs with regard to reading acquisition.\n",
        ""
      ]
    },
    "pfab15_interspeech": {
      "authors": [
        [
          "Jakob",
          "Pfab"
        ],
        [
          "Hanna",
          "Jakob"
        ],
        [
          "Mona",
          "Sp\u00e4th"
        ],
        [
          "Christoph",
          "Draxler"
        ]
      ],
      "title": "Neolexon \u2014 a therapy app for patients with aphasia",
      "original": "i15_1880",
      "page_count": 2,
      "order": 276,
      "p1": "1880",
      "pn": "1881",
      "abstract": [
        "Neolexon is an app for self-paced language training for patients suffering from aphasia. Auditory and reading comprehension as well as oral and written naming tasks are the most common types of exercises in aphasia therapy. For best effects, the exercises must be repeated with high frequency, and they must be adapted to the specific requirements of each patient.   The app was designed specifically for tablet devices and implements the four common types of exercises. Due to the patients' cognitive and motor restrictions it features a radically reduced graphical user interface that does not distract from the task and requires only minimal gestural interaction. The content of exercises is dynamically specified by the therapist according to the specific needs of a patient; the material is selected from a deeply annotated database of exercise items.   The app was developed as a student project and successfully evaluated by patients and therapists.\n",
        ""
      ]
    },
    "patil15_interspeech": {
      "authors": [
        [
          "Sonal",
          "Patil"
        ],
        [
          "Harish",
          "Arsikere"
        ],
        [
          "Om",
          "Deshmukh"
        ]
      ],
      "title": "Acoustic stress detection for improved navigation of educational videos",
      "original": "i15_1882",
      "page_count": 2,
      "order": 277,
      "p1": "1882",
      "pn": "1883",
      "abstract": [
        "This paper presents a system that uses acoustic stress detection to identify important concepts in educational videos. The proposed system is part of a non-linear navigation system that contains additional features like dynamic word cloud and 2-D timeline. An important feature of the word cloud is that the color used to represent the word depicts its spoken emphasis. This emphasis is estimated by quantifying the acoustic stress of each word. Stressed instances of a given word are also highlighted on the 2-D timeline using different colors. The primary focus of this paper is to detect words spoken with higher acoustic stress and provide an efficient means to navigate to corresponding instances. In the training phase, words are labeled manually as `stressed' or `unstressed' by speech experts. An SVM classifier is trained using three types of acoustic features: intensity-based, pitch-based and duration-based. Considering the data imbalance in terms of the ratio of `stressed' to `unstressed' words, the performance achieved (70% correct detection at a false-alarm rate of 19%) is satisfactory. The usability studies show that the time taken to detect and navigate to stressed instances of words is significantly less (p < 0.01) than that using a youtube-type baseline system.\n",
        ""
      ]
    },
    "anguera15_interspeech": {
      "authors": [
        [
          "Xavier",
          "Anguera"
        ]
      ],
      "title": "Multimodal read-aloud ebooks for language learning",
      "original": "i15_1884",
      "page_count": 2,
      "order": 278,
      "p1": "1884",
      "pn": "1885",
      "abstract": [
        "In this show and tell description paper we present the read-aloud electronic books (eBooks) we create at Sinkronigo.com. Read-aloud eBooks incorporate both audio and text into a single epub3-compliant eBook, which can be read in any compatible eBook reader, where the user can listen to the audio narration while the words are highlighted as spoken. Read-aloud eBooks are a perfect companion for people with reading difficulties and for those learning a new language.\n",
        ""
      ]
    },
    "besacier15_interspeech": {
      "authors": [
        [
          "Laurent",
          "Besacier"
        ],
        [
          "Elodie",
          "Gauthier"
        ],
        [
          "Mathieu",
          "Mangeot"
        ],
        [
          "Philippe",
          "Bretier"
        ],
        [
          "Paul",
          "Bagshaw"
        ],
        [
          "Olivier",
          "Rosec"
        ],
        [
          "Thierry",
          "Moudenc"
        ],
        [
          "Fran\u00e7ois",
          "Pellegrino"
        ],
        [
          "Sylvie",
          "Voisin"
        ],
        [
          "Egidio",
          "Marsico"
        ],
        [
          "Pascal",
          "Nocera"
        ]
      ],
      "title": "Speech technologies for african languages: example of a multilingual calculator for education",
      "original": "i15_1886",
      "page_count": 2,
      "order": 279,
      "p1": "1886",
      "pn": "1887",
      "abstract": [
        "This paper presents our achievements after 18 months of the ALFFA project dealing with African languages technologies. We focus on a multilingual calculator (Android app) that will be demonstrated during the Show and Tell session.\n",
        ""
      ]
    },
    "lee15c_interspeech": {
      "authors": [
        [
          "Kong Aik",
          "Lee"
        ],
        [
          "Guangsen",
          "Wang"
        ],
        [
          "Kam Pheng",
          "Ng"
        ],
        [
          "Hanwu",
          "Sun"
        ],
        [
          "Trung Hieu",
          "Nguyen"
        ],
        [
          "Ngoc Thuy Huong",
          "Thai"
        ],
        [
          "Bin",
          "Ma"
        ],
        [
          "Haizhou",
          "Li"
        ]
      ],
      "title": "The reddots platform for mobile crowd-sourcing of speech data",
      "original": "i15_2603",
      "page_count": 2,
      "order": 280,
      "p1": "2603",
      "pn": "2604",
      "abstract": [
        "With ever increasing computational power and availability of broadband connectivity to the Internet, mobile devices have become pervasive and gaining more popularity. Capitalizing on the mobile-Internet infrastructure, a speech data collection platform was developed as part of the RedDots project which is dedicated to the study of speaker recognition over mobile devices. The RedDots platform consists of a centralized Web server receiving inputs from the crowd through mobile devices. As of the time of this writing, we have recruited speakers from 17 countries in three-month period, showing the potential of the RedDots platform to collect speech data from the worldwide population.\n",
        ""
      ]
    },
    "arai15_interspeech": {
      "authors": [
        [
          "Takayuki",
          "Arai"
        ]
      ],
      "title": "Two extensions of umeda and teranishi's physical models of the human vocal tract",
      "original": "i15_2605",
      "page_count": 2,
      "order": 281,
      "p1": "2605",
      "pn": "2606",
      "abstract": [
        "The physical model designed by Umeda and Teranishi simulating an arbitrary shape of the human vocal tract was a straight tube with a set of plastic plates inserted from one side. Although this model has the advantage that users can configure any shape of the vocal tract, manually manipulating several plates simultaneously is difficult. In this study, we present two models extending Umeda and Teranishi's work to overcome this disadvantage. The first model has a straight tube similar to the original Umeda and Teranishi's model, but the weight of the plates enables them to return to resting position automatically. The second model has a bent tube with the oral and pharyngeal cavities connected at 90 degrees. This feature simulates the actual human vocal tract. The plates move back to their original positions by means of spring coils. In both cases, the plates' automatic return movement facilitates manual manipulation as compared to the original Umeda and Teranishi's model.\n",
        ""
      ]
    },
    "budnik15_interspeech": {
      "authors": [
        [
          "Matheuz",
          "Budnik"
        ],
        [
          "Laurent",
          "Besacier"
        ],
        [
          "Johann",
          "Poignant"
        ],
        [
          "Herv\u00e9",
          "Bredin"
        ],
        [
          "Claude",
          "Barras"
        ],
        [
          "Mickael",
          "Stefas"
        ],
        [
          "Pierrick",
          "Bruneau"
        ],
        [
          "Thomas",
          "Tamisier"
        ]
      ],
      "title": "Collaborative annotation for person identification in TV shows",
      "original": "i15_2607",
      "page_count": 2,
      "order": 282,
      "p1": "2607",
      "pn": "2608",
      "abstract": [
        "This paper presents a collaborative annotation framework for person identification in TV shows. The web annotation frontend will be demonstrated during the Show and Tell session. All the code for annotation is made available on github. The tool can also be used in a crowd-sourcing environment.\n",
        ""
      ]
    },
    "kisler15_interspeech": {
      "authors": [
        [
          "Thomas",
          "Kisler"
        ],
        [
          "Florian",
          "Schiel"
        ],
        [
          "Uwe D.",
          "Reichel"
        ],
        [
          "Christoph",
          "Draxler"
        ]
      ],
      "title": "Phonetic/linguistic web services at BAS",
      "original": "i15_2609",
      "page_count": 2,
      "order": 283,
      "p1": "2609",
      "pn": "2610",
      "abstract": [
        "We present recent developments in the collection of phonetic-linguistic web services provided by the Bavarian Archive of Speech Signals (BAS). The BAS back end web services are REST based and can be easily integrated into user applications. Several public web interfaces have been implemented that utilize these back end services to provide easy-to-use access to high-end linguistic and phonetic processing (front end services). In this show&tell we demonstrate the latest front end services of BAS: automatic phonetic segmentation & labelling using the MAUS technique (14 languages), text-to-phoneme conversion (13 languages), automatic phonetic transcription (6 languages), phonetic syllabification (13 languages), and speech synthesis.\n",
        ""
      ]
    },
    "winkelmann15_interspeech": {
      "authors": [
        [
          "Raphael",
          "Winkelmann"
        ]
      ],
      "title": "Managing speech databases with emur and the EMU-webapp",
      "original": "i15_2611",
      "page_count": 2,
      "order": 284,
      "p1": "2611",
      "pn": "2612",
      "abstract": [
        "As is the nature of the discipline, a majority of speech and language researchers spend a large amount of their time acquiring and transforming data into analyzable and interpretable forms to gain a better understanding of a certain subject matter. In this paper we present a collection of tools that aid the researcher in this sometimes tedious and error-prone process. The tools presented here are part of the next iteration of the EMU speech database management system which aims to be as close to an all-in-one solution for generating, manipulating, querying, analyzing and managing speech databases as possible.\n",
        ""
      ]
    },
    "wankerl15_interspeech": {
      "authors": [
        [
          "Sebastian",
          "Wankerl"
        ],
        [
          "Florian",
          "H\u00f6nig"
        ],
        [
          "Anton",
          "Batliner"
        ],
        [
          "J. R.",
          "Orozco-Arroyave"
        ],
        [
          "Elmar",
          "N\u00f6th"
        ]
      ],
      "title": "Visual comparison of speaker groups",
      "original": "i15_2613",
      "page_count": 2,
      "order": 285,
      "p1": "2613",
      "pn": "2614",
      "abstract": [
        "We describe a generic tool for visualising differences between two groups of speakers who produce a given word sequence. We do this by first time-aligning all recordings and then aggregating time-varying information within each group. By that, we can display prototypical loudness and tempo contours, and also spectrograms, together with information on variability and group effect size over time. An optional user-supplied segmentation (just needed for one of the recordings) can be used to relate local differences to individual phonemes. The system is validated with a group of speakers with Parkinson's disease and an age-matched control group. It will be provided as an open-source software package to the community.\n",
        ""
      ]
    },
    "kumar15c_interspeech": {
      "authors": [
        [
          "Rohit",
          "Kumar"
        ],
        [
          "Matthew E.",
          "Roy"
        ],
        [
          "Sanjika",
          "Hewavitharana"
        ],
        [
          "Dennis N.",
          "Mehay"
        ],
        [
          "Nina",
          "Zinovieva"
        ]
      ],
      "title": "Tools for rapid customization of S2s systems for emergent domains",
      "original": "i15_2615",
      "page_count": 2,
      "order": 286,
      "p1": "2615",
      "pn": "2616",
      "abstract": [
        "Component models of speech-to-speech translation (S2S) systems need to be customized to emerging needs. In this demonstration, we will showcase the technical functionality of BBN's domain customization tools for S2S systems that allow subject matter experts to augment an existing S2S system with new vocabulary items and translation rules using a web-based user interface. To reduce the user effort and time, our tools leverage Wikipedia as a linguistic resource for enrichment of domain profiles by finding lexical items and translations related to the domain. In a recent evaluation of BBN S2S system customized for using these tools, we found 15% (relative) reduction in word error rate as well as 30% (relative) reduction in untranslatable words when used within customized conversational domains.\n",
        ""
      ]
    },
    "metze15_interspeech": {
      "authors": [
        [
          "Florian",
          "Metze"
        ],
        [
          "Eric",
          "Riebling"
        ],
        [
          "Eric",
          "Fosler-Lussier"
        ],
        [
          "Andrew",
          "Plummer"
        ],
        [
          "Rebecca",
          "Bates"
        ]
      ],
      "title": "The speech recognition virtual kitchen turns one",
      "original": "i15_2617",
      "page_count": 2,
      "order": 287,
      "p1": "2617",
      "pn": "2618",
      "abstract": [
        "This paper describes recent developments in the Speech Recognition Virtual Kitchen. The core of the research infrastructure is the use of Virtual Machines or Linux containers. We liken these to a \u201ckitchen\u201d, because they provide \u201cappliances\u201d (e.g., speech recognition tool-kits), \u201crecipes\u201d (scripts for creating state-of-the-art systems), and \u201cingredients\u201d (language data). Users can now download a number of pre-configured Virtual Machines at http://speechkitchen.org/, and open-source versions will be made available on Github soon. Several workshops and evaluation campaigns are distributing baseline systems as Virtual Machines, furthering the goals of promoting community sharing of research techniques, fostering innovative experimentation, and providing solid reference systems as a tool for education, research, and evaluation in the speech and language area. A web-based community platform complements the VMs, to allow physically disconnected users to jointly explore VMs, learn from each other, and collaborate in research.\n",
        ""
      ]
    },
    "rennies15_interspeech": {
      "authors": [
        [
          "Jan",
          "Rennies"
        ],
        [
          "Andreas",
          "Volgenandt"
        ],
        [
          "Henning",
          "Schepker"
        ],
        [
          "Simon",
          "Doclo"
        ]
      ],
      "title": "Model-based adaptive pre-processing of speech for enhanced intelligibility in noise and reverberation",
      "original": "i15_2619",
      "page_count": 2,
      "order": 288,
      "p1": "2619",
      "pn": "2620",
      "abstract": [
        "In this demonstrator we present the most recent advances in the development of the near-end listening enhancement algorithm AdaptDRC. The algorithm uses short-time estimates of the speech intelligibility index to control spectral shaping, dynamic range compression and/or level adjustment to achieve an adaptive enhancement of speech intelligibility in adverse listening conditions. Depending on the application scenario, the algorithm framework can take background noise and reverberation as well as different boundary conditions into account. The show and tell contribution comprises a real-time setup of the algorithm to demonstrate the sound modifications and the impact of the different parameters and boundary conditions. An accompanying poster shows results of formal listening tests evaluating the speech intelligibility improvement achieved by the algorithm for normal-hearing and hearing-impaired listeners.\n",
        ""
      ]
    },
    "moller15_interspeech": {
      "authors": [
        [
          "Sebastian",
          "M\u00f6ller"
        ],
        [
          "Tilo",
          "Westermann"
        ]
      ],
      "title": "Experiences with and new application ideas for the interspeech app",
      "original": "i15_2621",
      "page_count": 2,
      "order": 289,
      "p1": "2621",
      "pn": "2622",
      "abstract": [
        "For Interspeech 2013 in Lyon, we launched a first version of a mobile phone app, which should support conference participants prior to and during the conference. We anticipated that the app will be of use to conference organizers alike, and that it could be turned into an open-source tool for the spoken language technology community. In this contribution, we will demonstrate usage experiences collected during the 2013 and 2014 Interspeech conferences, and propose and implement ideas for carrying out speech research with such an app.\n",
        ""
      ]
    },
    "sityaev15_interspeech": {
      "authors": [
        [
          "Dmitry",
          "Sityaev"
        ],
        [
          "Praphul",
          "Kumar"
        ],
        [
          "Rajesh",
          "Ramchander"
        ]
      ],
      "title": "Traditional IVR and visual IVR \u2014 killing two birds with one stone",
      "original": "i15_2623",
      "page_count": 2,
      "order": 290,
      "p1": "2623",
      "pn": "2624",
      "abstract": [
        "This paper describes a novel solution which allows to quickly build and develop multi-channel applications. Due to the popularity of a smartphone, a new paradigm of applications called Visual IVR has been emerging recently, where visual navigation replaces the traditional DTMF or voice-enabled dialogue control. The described solution brings a unified approach for the creation of traditional IVR systems as well as Visual IVR systems.\n",
        ""
      ]
    },
    "itakura15_interspeech": {
      "authors": [
        [
          "Kousuke",
          "Itakura"
        ],
        [
          "Izaya",
          "Nishimuta"
        ],
        [
          "Yoshiaki",
          "Bando"
        ],
        [
          "Katsutoshi",
          "Itoyama"
        ],
        [
          "Kazuyoshi",
          "Yoshii"
        ]
      ],
      "title": "Bayesian integration of sound source separation and speech recognition: a new approach to simultaneous speech recognition",
      "original": "i15_0736",
      "page_count": 5,
      "order": 291,
      "p1": "736",
      "pn": "740",
      "abstract": [
        "This paper presents a novel Bayesian method that can directly recognize overlapping utterances without explicitly separating mixture signals into their independent components in advance of speech recognition. The conventional approach to contaminated speech recognition in real environments uniquely extracts the clean isolated signals of individual sources ( e.g., by noise reduction, dereverberation, and source separation). One of the main limitations of this cascading approach is that the accuracy of speech recognition is upper bounded by the accuracy of preprocessing. To overcome this limitation, our method marginalizes out uncertain isolated speech signals by integrating source separation and speech recognition in a Bayesian manner. A sufficient number of samples are drawn from the posterior distribution of isolated speech signals by using a Markov chain Monte Carlo method, and then the posterior distributions of uttered texts for those samples are integrated. Under a certain condition, this Monte Carlo integration is shown to reduce to the well-known method called ROVER that integrates recognized texts obtained from sampled speech signals. Results of simultaneous speech recognition experiments showed that in terms of word accuracy the proposed method significantly outperformed conventional cascading methods.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-247",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition \u2013 Signal Processing, Acoustic Modeling, Robustness, and Adaptation"
    },
    "himawan15_interspeech": {
      "authors": [
        [
          "Ivan",
          "Himawan"
        ],
        [
          "Petr",
          "Motlicek"
        ],
        [
          "Sridha",
          "Sridharan"
        ],
        [
          "David",
          "Dean"
        ],
        [
          "Dian",
          "Tjondronegoro"
        ]
      ],
      "title": "Channel selection in the short-time modulation domain for distant speech recognition",
      "original": "i15_0741",
      "page_count": 5,
      "order": 292,
      "p1": "741",
      "pn": "745",
      "abstract": [
        "Automatic speech recognition from multiple distant microphones poses significant challenges because of noise and reverberations. The quality of speech acquisition may vary between microphones because of movements of speakers and channel distortions. This paper proposes a channel selection approach for selecting reliable channels based on selection criterion operating in the short-term modulation spectrum domain. The proposed approach quantifies the relative strength of speech from each microphone and speech obtained from beamforming modulations. The new technique is compared experimentally in the real reverb conditions in terms of perceptual evaluation of speech quality (PESQ) measures and word error rate (WER). Overall improvement in recognition rate is observed using delay-sum and superdirective beamformers compared to the case when the channel is selected randomly using circular microphone arrays.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-248",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition \u2013 Signal Processing, Acoustic Modeling, Robustness, and Adaptation"
    },
    "dekkers15_interspeech": {
      "authors": [
        [
          "Gert",
          "Dekkers"
        ],
        [
          "Toon van",
          "Waterschoot"
        ],
        [
          "Bart",
          "Vanrumste"
        ],
        [
          "Bert Van Den",
          "Broeck"
        ],
        [
          "Jort F.",
          "Gemmeke"
        ],
        [
          "Hugo",
          "Van hamme"
        ],
        [
          "Peter",
          "Karsmakers"
        ]
      ],
      "title": "A multi-channel speech enhancement framework for robust NMF-based speech recognition for speech-impaired users",
      "original": "i15_0746",
      "page_count": 5,
      "order": 293,
      "p1": "746",
      "pn": "750",
      "abstract": [
        "In this paper a multi-channel speech enhancement framework for distant speech acquisition in noisy and reverberant environments for Non-negative Matrix Factorization (NMF)-based Automatic Speech Recognition (ASR) is proposed. The system is evaluated for its use in an assistive vocal interface for physically impaired and speech-impaired users. The framework utilises the Spatially Pre-processed Speech Distortion Weighted Multi-channel Wiener Filter (SP-SDW-MWF) in combination with a postfilter to reduce noise and reverberation. Additionally, the estimation uncertainty of the speech enhancement framework is propagated through the Mel-Frequency Cepstrum Coefficients (MFCC) feature extraction to allow for feature compensation in a later stage. Results indicate that a) using a trade-off parameter between noise reduction and speech distortion has a positive effect on the recognition performance with respect to the well-known GSC and MWF and b) the addition of a post-filter and the feature compensation increases performance with respect to several baselines for a non-pathological and pathological speaker.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-249",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition \u2013 Signal Processing, Acoustic Modeling, Robustness, and Adaptation"
    },
    "kim15b_interspeech": {
      "authors": [
        [
          "Chanwoo",
          "Kim"
        ],
        [
          "Kean K.",
          "Chin"
        ]
      ],
      "title": "Sound source separation algorithm using phase difference and angle distribution modeling near the target",
      "original": "i15_0751",
      "page_count": 5,
      "order": 294,
      "p1": "751",
      "pn": "755",
      "abstract": [
        "In this paper we present a novel two-microphone sound source separation algorithm, which selects the signal from the target direction while suppressing signals from other directions. In this algorithm, which is referred to as Power Angle Information Near Target (PAINT), we first calculate phase difference for each time-frequency bin. From the phase difference, the angle of a sound source is estimated. For each frame, we represent the source angle distribution near the expected target location as a mixture of a Gaussian and a uniform distributions and obtain binary masks using hypothesis testing. Continuous masks are calculated from the binary masks using the Channel Weighting (CW) technique, and processed speech is synthesized using IFFT and the OverLap-Add (OLA) method. We demonstrate that the algorithm described in this paper shows better speech recognition accuracy compared to conventional approaches and our previous approaches.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-250"
    },
    "ravanelli15_interspeech": {
      "authors": [
        [
          "Mirco",
          "Ravanelli"
        ],
        [
          "Maurizio",
          "Omologo"
        ]
      ],
      "title": "Contaminated speech training methods for robust DNN-HMM distant speech recognition",
      "original": "i15_0756",
      "page_count": 5,
      "order": 295,
      "p1": "756",
      "pn": "760",
      "abstract": [
        "Despite the significant progress made in the last years, state-of-the-art speech recognition technologies provide a satisfactory performance only in the close-talking condition. Robustness of distant speech recognition in adverse acoustic conditions, on the other hand, remains a crucial open issue for future applications of human-machine interaction. To this end, several advances in speech enhancement, acoustic scene analysis as well as acoustic modeling, have recently contributed to improve the state-of-the-art in the field. One of the most effective approaches to derive a robust acoustic modeling is based on using contaminated speech, which proved helpful in reducing the acoustic mismatch between training and testing conditions.   In this paper, we revise this classical approach in the context of modern DNN-HMM systems, and propose the adoption of three methods, namely, asymmetric context windowing, close-talk based supervision, and close-talk based pre-training. The experimental results, obtained using both real and simulated data, show a significant advantage in using these three methods, overall providing a 15% error rate reduction compared to the baseline systems. The same trend in performance is confirmed either using a high-quality training set of small size, and a large one.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-251",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition \u2013 Signal Processing, Acoustic Modeling, Robustness, and Adaptation"
    },
    "miao15_interspeech": {
      "authors": [
        [
          "Yajie",
          "Miao"
        ],
        [
          "Florian",
          "Metze"
        ]
      ],
      "title": "Distance-aware DNNs for robust speech recognition",
      "original": "i15_0761",
      "page_count": 5,
      "order": 296,
      "p1": "761",
      "pn": "765",
      "abstract": [
        "Distant speech recognition (DSR) remains to be an open challenge, even for the state-of-the-art deep neural network (DNN) models. Previous work has attempted to improve DNNs under constantly distant speech. However, in real applications, the speaker-microphone distance (SMD) can be quite dynamic, varying even within a single utterance. This paper investigates how to alleviate the impact of dynamic SMD on DNN models. Our solution is to incorporate the frame-level SMD information into DNN training. Generation of the SMD information relies on a universal extractor that is learned on a meeting corpus. We study the utility of different architectures in instantiating the SMD extractor. On our target acoustic modeling task, two approaches are proposed to build distance-aware DNN models using the SMD information: simple concatenation and distance adaptive training (DAT). Our experiments show that in the simplest case, incorporating the SMD descriptors improves word error rates of DNNs by 5.6% relative. Further optimizing SMD extraction and integration results in more gains.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-252",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition \u2013 Signal Processing, Acoustic Modeling, Robustness, and Adaptation"
    },
    "levy15_interspeech": {
      "authors": [
        [
          "Helena",
          "Levy"
        ]
      ],
      "title": "Perception and production of vowel contrasts in German learners of English",
      "original": "i15_0796",
      "page_count": 5,
      "order": 297,
      "p1": "796",
      "pn": "800",
      "abstract": [
        "This paper discusses the relationship between the perception and the production of vowel contrasts in German learners of English. The underlying study investigates whether German subjects' ability to distinguish between the English front vowels /e/ and /\u00e6/ on the level of production improved after training only perception. The main hypothesis was that perception training would improve production. One perception test and one production task were followed by a session of perception training during which the subjects did not speak. After the training, the two modalities were tested again. Results supported the hypothesis, maintaining that the production of the intended contrast improved after training. Perception, however, remained almost the same. Overall results suggest that training perception in an EFL (English as a foreign language) context may have positive effects on pronunciation.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-253",
      "author_area_id": "1",
      "author_area_label": "Speech Perception, Production, and Acquisition"
    },
    "tong15_interspeech": {
      "authors": [
        [
          "Rong",
          "Tong"
        ],
        [
          "Nancy F.",
          "Chen"
        ],
        [
          "Bin",
          "Ma"
        ],
        [
          "Haizhou",
          "Li"
        ]
      ],
      "title": "Goodness of tone (GOT) for non-native Mandarin tone recognition",
      "original": "i15_0801",
      "page_count": 5,
      "order": 298,
      "p1": "801",
      "pn": "805",
      "abstract": [
        "Lexical tone is one of the most challenging pronunciation problems in tonal language acquisition. Accurate lexical tone production is especially challenging for people whose native language is not a tonal one. In this paper, we propose Goodness of Tone (GOT), a confidence measure inspired from goodness of pronunciation (GOP) for tone recognition. GOT is a vector representation of the confidence of each lexical tone of the given speech segment. The proposed GOT confidence measure is useful in tone recognition due to the following: 1) Unlike other tonal features such as pitch or fundamental frequency variation, GOT integrates both phonetic and tonal information. 2) GOT exploits competing tonal phones which differ only in tonal label but are the same in phonetic labels as a reference to conduct cohort normalization. 3) GOT is a vector that concatenates confidence scores from all the possible lexical tones, making it easier to characterize error patterns of non-native tonal production.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-254",
      "author_area_id": "1",
      "author_area_label": "Speech Perception, Production, and Acquisition"
    },
    "jugler15_interspeech": {
      "authors": [
        [
          "Jeanin",
          "J\u00fcgler"
        ],
        [
          "Frank",
          "Zimmerer"
        ],
        [
          "Bernd",
          "M\u00f6bius"
        ],
        [
          "Christoph",
          "Draxler"
        ]
      ],
      "title": "The effect of high-variability training on the perception and production of French stops by German native speakers",
      "original": "i15_0806",
      "page_count": 5,
      "order": 299,
      "p1": "806",
      "pn": "810",
      "abstract": [
        "We investigated the effect of high-variability training (HVT) on the production and perception of French bilabial voiced and voiceless stops by German native speakers. Stop consonants in the two languages differ with respect to several articulatory and acoustic features. German learners of French (Experiment Group) trained the perception of word-initial bilabial stops spoken by six French native speakers using identification tests, whereas subjects of a Control Group did not perform a training. Additional perception and production tests of French words including bilabial, alveolar, and velar stops in all word positions were performed to capture the impact of HVT. Subjects were found to be quite good at distinguishing voiced and voiceless stops. However, voiceless stops received lower correctness scores than voiced ones and subjects of the Experiment group were able to further increase their scores after training. Results for production are mirror-inverted showing that subjects of the Experiment Group successfully produced longer negative VOT values but did not show an improvement for voiceless stops.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-255",
      "author_area_id": "1",
      "author_area_label": "Speech Perception, Production, and Acquisition"
    },
    "bao15b_interspeech": {
      "authors": [
        [
          "Wenfu",
          "Bao"
        ],
        [
          "Hui",
          "Feng"
        ],
        [
          "Jianwu",
          "Dang"
        ],
        [
          "Zhilei",
          "Liu"
        ],
        [
          "Yang",
          "Yu"
        ],
        [
          "Siyu",
          "Wang"
        ]
      ],
      "title": "Perception of Mandarin tones by native tibetan speakers",
      "original": "i15_0811",
      "page_count": 4,
      "order": 300,
      "p1": "811",
      "pn": "814",
      "abstract": [
        "Previous studies have demonstrated that the second language (L2) learners' linguistic backgrounds and L2 proficiency have an effect on their perception of L2 sounds. This paper attempts to investigate the assimilation patterns of Mandarin and Tibetan tones, and the influences of first language (L1) backgrounds and Mandarin proficiency on the perception of Mandarin tones. A total of 46 Tibetan participants, including 14 Khams and 32 U-Tsang speakers, were instructed to assimilate the Mandarin tones they've heard to their most similar native tones. Results suggest that the four-tone system U-Tsang speakers match M155 to T155, M451 to T452, with great disparities in mapping M235 and M3214 to T213 and T3132, while the two-tone system Khams speakers tend to assimilate M155 and M451 to the high tone, and M235 and M3214 to the low tone. Mandarin Chinese proficiency does show the progressive tone-mapping patterns, that is, the higher the learners' proficiency is, the more possible he or she will have the tone mapping which could be predicted by the tone values. As proficiency level increases, standard deviation of learners' mapping tends to get smaller, especially among Khams speakers.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-256",
      "author_area_id": "1",
      "author_area_label": "Speech Perception, Production, and Acquisition"
    },
    "saha15_interspeech": {
      "authors": [
        [
          "Shambhu Nath",
          "Saha"
        ],
        [
          "Shyamal Kr. Das",
          "Mandal"
        ]
      ],
      "title": "Study of acoustic correlates of English lexical stress produced by native (L1) bengali speakers compared to native (L1) English speakers",
      "original": "i15_0815",
      "page_count": 5,
      "order": 301,
      "p1": "815",
      "pn": "819",
      "abstract": [
        "English lexical stress is multidimensional in nature and acoustically related to combination of fundamental frequency (F0), duration, intensity and vowel quality. Errors in any or all of these correlates could interfere with perception of the stress contrast, but it is unknown which correlates are most difficult for Bengali speakers to acquire. This study compares the use of these correlates in the production of English lexical stress contrasts by 10 L1 English and 20 L1 Bengali speakers. Results showed that although Bengali speakers used all four acoustic correlates in similar manner like English speakers, but they produced significantly less native like stress patterns. English speakers reduced vowel duration significantly more in the unstressed vowels compared to Bengali speakers and degree of intensity and F0 increase in stressed vowels by English speakers was higher than that by Bengali speakers. There were also significant differences in formant patterns across speaker groups, such that Bengali speakers produced English like vowel reduction in certain unstressed syllables, but in other cases Bengali speakers have tendency to either not reduce or incorrectly reduce vowels in unstressed syllables. Results suggest that Bengali speakers' production of English lexical stress contrast is influenced by L1 language experience and L1 phonology.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-257",
      "author_area_id": "1",
      "author_area_label": "Speech Perception, Production, and Acquisition"
    },
    "naganomadsen15_interspeech": {
      "authors": [
        [
          "Yasuko",
          "Nagano-Madsen"
        ]
      ],
      "title": "Prosodic phrasing unique to the acquisition of L2 intonation \u2014 an analysis of L2 Japanese intonation by L1 Swedish learners",
      "original": "i15_0820",
      "page_count": 4,
      "order": 302,
      "p1": "820",
      "pn": "823",
      "abstract": [
        "This paper examines the prosodic organization of L2 Japanese produced by L1 Swedish at the beginner level. Japanese and Swedish have been well studied for their prosodic structures and some well-defined prosodic phrases have been proposed. However, these existing prosodic phrases are found to be inadequate in analyzing L2 intonation seen as interlanguage. Instead, it consists of some unique phrasing showing the characteristics of interlanguage, i.e. a language that has its own system and it changes continuously during the acquisition process. Studies on interlanguage are mostly on grammar and not much is known about the acquisition of L2 intonation. The results reveal that the beginner level L2 intonation is characterized by many pauses that are inserted at every grammatical phrase boundary. Such a phasing is unique as interlanguage and presumably universal in the less fluent speech at the beginner level. While a typical prosodic phrasing in Japanese uses downstep to group APs to iPs, a typical phrasing in L2 Japanese produced by L1 Swedish uses upstep and some other patterns instead. They are considered to be L1 prosodic transfer.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-258",
      "author_area_id": "1",
      "author_area_label": "Speech Perception, Production, and Acquisition"
    },
    "sar15_interspeech": {
      "authors": [
        [
          "Leda",
          "Sar\u0131"
        ],
        [
          "Batuhan",
          "G\u00fcndo\u011fdu"
        ],
        [
          "Murat",
          "Sara\u00e7lar"
        ]
      ],
      "title": "Fusion of LVCSR and posteriorgram based keyword search",
      "original": "i15_0824",
      "page_count": 5,
      "order": 303,
      "p1": "824",
      "pn": "828",
      "abstract": [
        "The aim of this work is to improve the performance of an existing KWS system by merging the search results produced by two additional KWS systems. The existing baseline system is based on large vocabulary continuous speech recognition (LVCSR) and weighted finite state transducers (WFST). The first proposed KWS system is based on searching a symbolic WFST index which is generated by quantizing the posteriorgram representation of the audio. The second proposed KWS system is based on subsequence dynamic time warping (sDTW) algorithm which is commonly used in the query-by-example spoken term detection (QbE-STD) tasks. We also investigate using average posteriorgrams for query generation. Experimental results show that when combined with the existing KWS system, the proposed systems improve the performance of the KWS system especially for the out-of-vocabulary (OOV) queries.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-259",
      "author_area_id": "12",
      "author_area_label": "Spoken Language Processing \u2013 Translation, Information Retrieval, and Resources"
    },
    "mendels15_interspeech": {
      "authors": [
        [
          "Gideon",
          "Mendels"
        ],
        [
          "Erica",
          "Cooper"
        ],
        [
          "Victor",
          "Soto"
        ],
        [
          "Julia",
          "Hirschberg"
        ],
        [
          "Mark J. F.",
          "Gales"
        ],
        [
          "Kate M.",
          "Knill"
        ],
        [
          "Anton",
          "Ragni"
        ],
        [
          "Haipeng",
          "Wang"
        ]
      ],
      "title": "Improving speech recognition and keyword search for low resource languages using web data",
      "original": "i15_0829",
      "page_count": 5,
      "order": 304,
      "p1": "829",
      "pn": "833",
      "abstract": [
        "We describe the use of text data scraped from the web to augment language models for Automatic Speech Recognition and Keyword Search for Low Resource Languages. We scrape text from multiple genres including blogs, online news, translated TED talks, and subtitles. Using linearly interpolated language models, we find that blogs and movie subtitles are more relevant for language modeling of conversational telephone speech and obtain large reductions in out-of-vocabulary keywords. Furthermore, we show that the web data can improve Term Error Rate Performance by 3.8% absolute and Maximum Term-Weighted Value in Keyword Search by 0.0076-0.1059 absolute points. Much of the gain comes from the reduction of out-of-vocabulary items.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-260",
      "author_area_id": "12",
      "author_area_label": "Spoken Language Processing \u2013 Translation, Information Retrieval, and Resources"
    },
    "domoto15_interspeech": {
      "authors": [
        [
          "Kentaro",
          "Domoto"
        ],
        [
          "Takehito",
          "Utsuro"
        ],
        [
          "Naoki",
          "Sawada"
        ],
        [
          "Hiromitsu",
          "Nishizaki"
        ]
      ],
      "title": "Two-step spoken term detection using SVM classifier trained with pre-indexed keywords based on ASR result",
      "original": "i15_0834",
      "page_count": 5,
      "order": 305,
      "p1": "834",
      "pn": "838",
      "abstract": [
        "This paper presents a novel two-step spoken term detection (STD) method that uses the same STD engine twice and a support vector machine (SVM)-based classifier to verify detected terms from the output of the second STD engine. In the first STD process, pre-indexing of the target spoken documents from a keyword list built from the results of automatic speech recognition of the speeches is performed. The first STD process result includes a set of keywords and their detection intervals (positions) in the spoken documents. For the keywords that have competitive intervals, we rank them on the basis of the matching cost of STD and select the best one with the longest duration among competitive detections. The selected keywords are registered in the pre-index. In the second STD process, a query is searched by the same STD engine, and then, the outputted candidates are verified by an SVM classifier. Our proposed two-step STD method was evaluated using the NTCIR-10 SpokenDoc-2 STD task and it drastically outperformed the traditional STD method based on dynamic time warping and the confusion network-based index.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-261",
      "author_area_id": "12",
      "author_area_label": "Spoken Language Processing \u2013 Translation, Information Retrieval, and Resources"
    },
    "zhang15_interspeech": {
      "authors": [
        [
          "Le",
          "Zhang"
        ],
        [
          "Damianos",
          "Karakos"
        ],
        [
          "William",
          "Hartmann"
        ],
        [
          "Roger",
          "Hsiao"
        ],
        [
          "Richard",
          "Schwartz"
        ],
        [
          "Stavros",
          "Tsakalidis"
        ]
      ],
      "title": "Enhancing low resource keyword spotting with automatically retrieved web documents",
      "original": "i15_0839",
      "page_count": 5,
      "order": 306,
      "p1": "839",
      "pn": "843",
      "abstract": [
        "Keyword Spotting (KWS) systems developed for low resource languages with very little transcribed audio suffer due to a small vocabulary (high out-of-vocabulary (OOV) rate) and a weak language model. In this paper, we propose to augment such systems using automatically retrieved web documents. Our procedure can find large volumes of web documents similar to a small pool of training transcriptions within a few hours, by querying a search engine with automatically generated query terms. We then use simple language identification to extract high-confidence text for lexicon expansion and language modeling. Experiments using six very limited language packs (VLLP) from the IARPA-Babel program show web documents can cut the OOV rate by half on the development set, and on average improve keyword spotting performance by 2.8 points absolute measured by the Actual Term Weighted Value (ATWV). In particular, we find most of the gains (8.7 points on average) are from keywords that were OOV in the baseline system, and are converted into in-vocabulary (IV) through lexicon expansion. These gains are obtained even after using subword units (unsupervised syllable-like units and sequences of phones), which are known to greatly enhance OOV keyword search performance.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-262",
      "author_area_id": "12",
      "author_area_label": "Spoken Language Processing \u2013 Translation, Information Retrieval, and Resources"
    },
    "bertero15_interspeech": {
      "authors": [
        [
          "Dario",
          "Bertero"
        ],
        [
          "Linlin",
          "Wang"
        ],
        [
          "Ho Yin",
          "Chan"
        ],
        [
          "Pascale",
          "Fung"
        ]
      ],
      "title": "A comparison between a DNN and a CRF disfluency detection and reconstruction system",
      "original": "i15_0844",
      "page_count": 5,
      "order": 307,
      "p1": "844",
      "pn": "848",
      "abstract": [
        "We propose to compare between a Deep Neural Network and a Conditional Random Field disfluency detection and reconstruction system, both trained on the same features. Deep Neural Networks, despite an increasing popularity in a multitude of speech and language related tasks, were never applied to disfluency recognition. One of the most difficult classes of disfluency is false starts. We are interested in comparing these two approaches on recognition of different types of disfluency. Our experimental results over the SSR v2 corpus show that the DNN approach outperforms CRF slightly on repetition disfluencies. However, DNN exhibits a very low recall (17.6% compared to 26.3% of the CRF) over the more difficult false start recognition subtask. When applied to a corpus of sentences with only false starts, the two methods give both higher results (46.7% F-score for CRF and 44.2% F-score for DNN). We also propose to improve the overall results on false start by training our classifiers in two stages: the first to recognize non-false start errors, the second for false start only, and combining the two approaches using a simple voting algorithm. This allows us to obtain the best result of 52.2% F-score over false start.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-263",
      "author_area_id": "12",
      "author_area_label": "Spoken Language Processing \u2013 Translation, Information Retrieval, and Resources"
    },
    "hough15_interspeech": {
      "authors": [
        [
          "Julian",
          "Hough"
        ],
        [
          "David",
          "Schlangen"
        ]
      ],
      "title": "Recurrent neural networks for incremental disfluency detection",
      "original": "i15_0849",
      "page_count": 5,
      "order": 308,
      "p1": "849",
      "pn": "853",
      "abstract": [
        "For dialogue systems to become robust, they must be able to detect disfluencies accurately and with minimal latency. To meet this challenge, here we frame incremental disfluency detection as a word-by-word tagging task and, following their recent success in Spoken Language Understanding tasks, we test the performance of Recurrent Neural Networks (RNNs). We experiment with different inputs for RNNs to explore the effect of context on their ability to detect edit terms and repair disfluencies effectively. Although not eclipsing the state of the art in terms of utterance-final performance, RNNs achieve good detection results, requiring no feature engineering and using simple input vectors representing the incoming utterance as their training input. Furthermore, RNNs show very good incremental properties with low latency and very good output stability, surpassing previously reported results in these measures.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-264",
      "author_area_id": "12",
      "author_area_label": "Spoken Language Processing \u2013 Translation, Information Retrieval, and Resources"
    },
    "hu15_interspeech": {
      "authors": [
        [
          "Qiong",
          "Hu"
        ],
        [
          "Zhizheng",
          "Wu"
        ],
        [
          "Korin",
          "Richmond"
        ],
        [
          "Junichi",
          "Yamagishi"
        ],
        [
          "Yannis",
          "Stylianou"
        ],
        [
          "Ranniery",
          "Maia"
        ]
      ],
      "title": "Fusion of multiple parameterisations for DNN-based sinusoidal speech synthesis with multi-task learning",
      "original": "i15_0854",
      "page_count": 5,
      "order": 309,
      "p1": "854",
      "pn": "858",
      "abstract": [
        "It has recently been shown that deep neural networks (DNN) can improve the quality of statistical parametric speech synthesis (SPSS) when using a source-filter vocoder. Our own previous work has furthermore shown that a dynamic sinusoidal model (DSM) is also highly suited to DNN-based SPSS, whereby sinusoids may either be used themselves as a \u201cdirect parameterisation\u201d (DIR), or they may be encoded using an \u201cintermediate spectral parameterisation\u201d (INT). The approach in that work was effectively to replace a decision tree with a neural network. However, waveform parameterisation and synthesis steps that have been developed to suit HMMs may not fully exploit DNN capabilities. Here, in contrast, we investigate ways to combine INT and DIR at the levels of both DNN modelling and waveform generation. For DNN training, we propose to use multi-task learning to model cepstra (from INT) and log amplitudes (from DIR) as primary and secondary tasks. Our results show combining these improves modelling accuracy for both tasks. Next, during synthesis, instead of discarding parameters from the second task, a fusion method using harmonic amplitudes derived from both tasks is applied. Preference tests show the proposed method gives improved performance, and that this applies to synthesising both with and without global variance parameters.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-265",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "achanta15_interspeech": {
      "authors": [
        [
          "Sivanand",
          "Achanta"
        ],
        [
          "Tejas",
          "Godambe"
        ],
        [
          "Suryakanth V.",
          "Gangashetty"
        ]
      ],
      "title": "An investigation of recurrent neural network architectures for statistical parametric speech synthesis",
      "original": "i15_0859",
      "page_count": 5,
      "order": 310,
      "p1": "859",
      "pn": "863",
      "abstract": [
        "In this paper, we investigate two different recurrent neural network (RNN) architectures: Elman RNN and recently proposed clockwork RNN [1] for statistical parametric speech synthesis (SPSS). Of late, deep neural networks are being used for SPSS which involve predicting every frame independent of the previous predictions, and hence requires post-processing for ensuring smooth evolution of speech parameters. RNNs, on the other hand, are intuitively better suited for the task as they inherently model temporal dependencies, but were restricted in use because of the difficulty in training. Lately, techniques such as sparse initialization, Nesterov's accelerated gradient, gradient clipping and leaky integration (LI) have been shown to overcome this difficulty. We study the utility of these techniques for SPSS task. In addition, we show that clockwork RNN is equivalent to an Elman RNN with a particular form of LI. This perspective enables us to understand the reason why a simple Elman RNN with LI units performs well on sequential tasks.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-266",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "fan15_interspeech": {
      "authors": [
        [
          "Yuchen",
          "Fan"
        ],
        [
          "Yao",
          "Qian"
        ],
        [
          "Frank K.",
          "Soong"
        ],
        [
          "Lei",
          "He"
        ]
      ],
      "title": "Sequence generation error (SGE) minimization based deep neural networks training for text-to-speech synthesis",
      "original": "i15_0864",
      "page_count": 5,
      "order": 311,
      "p1": "864",
      "pn": "868",
      "abstract": [
        "Feed-forward deep neural networks (DNNs) based text-to-speech (TTS) synthesis, which employs a multi-layered structure to exploit the statistical correlations between rich contextual information and the corresponding acoustic features, has been shown to outperform a decision tree based, GMM-HMM counterpart. However, the DNN-based TTS training has not taken the whole sequence, i.e., sentence, into account in optimization, hence results in some intrinsic inconsistency between training and testing. In this paper we propose a \u201csequence generation error\u201d (SGE) minimization criterion for DNN-based TTS training. By incorporating the whole sequence parameter generation directly into the training process, the mismatch between training and testing is eliminated and the original constraints between the static and dynamic features are naturally embedded in the optimization process. Experimental results performed on a speech database of 5 hours show that DNN-based TTS trained with this new SGE minimization criterion can further improve the DNN baseline performance, particularly, in subjective listening tests.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-267",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "valentinibotinhao15_interspeech": {
      "authors": [
        [
          "Cassia",
          "Valentini-Botinhao"
        ],
        [
          "Zhizheng",
          "Wu"
        ],
        [
          "Simon",
          "King"
        ]
      ],
      "title": "Towards minimum perceptual error training for DNN-based speech synthesis",
      "original": "i15_0869",
      "page_count": 5,
      "order": 312,
      "p1": "869",
      "pn": "873",
      "abstract": [
        "We propose to use a perceptually-oriented domain to improve the quality of text-to-speech generated by deep neural networks (DNNs). We train a DNN that predicts the parameters required for speech reconstruction but whose cost function is calculated in another domain. In this paper, to represent this perceptual domain we extract an approximated version of the Spectro-Temporal Excitation Pattern that was originally proposed as part of a model of hearing speech in noise. We train DNNs that predict band aperiodicity, fundamental frequency and Mel cepstral coefficients and compare generated speech when the spectral cost function is defined in the Mel cepstral, warped log spectrum or perceptual domains. Objective results indicate that the perceptual domain system achieves the highest quality.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-268",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "song15b_interspeech": {
      "authors": [
        [
          "Eunwoo",
          "Song"
        ],
        [
          "Hong-Goo",
          "Kang"
        ]
      ],
      "title": "Deep neural network-based statistical parametric speech synthesis system using improved time-frequency trajectory excitation model",
      "original": "i15_0874",
      "page_count": 5,
      "order": 313,
      "p1": "874",
      "pn": "878",
      "abstract": [
        "This paper proposes a deep neural network (DNN)-based statistical parametric speech synthesis system using an improved time-frequency trajectory excitation (ITFTE) model. The ITFTE model, which efficiently reduces the parametric redundancy of a TFTE model, improved the perceptual quality of the vocoding process and the estimation accuracy of the training process. However, there remain problems related to training ITFTE parameters in a hidden Markov model (HMM) framework, such as inefficiency of representing cross-dimensional correlations between ITFTE parameters, over-smoothed outputs caused by statistical averaging, and an over-fitted model due to a decision tree-based state clustering paradigm. To alleviate these limitations, a centralized DNN replaces the decision trees of the HMM training process. Analysis of trainability confirms that the DNN training process improves the model accuracy, which results in improved perceptual quality of synthesized speech. Objective and subjective test results also verify that the proposed system performs better than the conventional HMM-based system.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-269",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "wu15b_interspeech": {
      "authors": [
        [
          "Zhizheng",
          "Wu"
        ],
        [
          "Pawel",
          "Swietojanski"
        ],
        [
          "Christophe",
          "Veaux"
        ],
        [
          "Steve",
          "Renals"
        ],
        [
          "Simon",
          "King"
        ]
      ],
      "title": "A study of speaker adaptation for DNN-based speech synthesis",
      "original": "i15_0879",
      "page_count": 5,
      "order": 314,
      "p1": "879",
      "pn": "883",
      "abstract": [
        "A major advantage of statistical parametric speech synthesis (SPSS) over unit-selection speech synthesis is its adaptability and controllability in changing speaker characteristics and speaking style. Recently, several studies using deep neural networks (DNNs) as acoustic models for SPSS have shown promising results. However, the adaptability of DNNs in SPSS has not been systematically studied. In this paper, we conduct an experimental analysis of speaker adaptation for DNN-based speech synthesis at different levels. In particular, we augment a low-dimensional speaker-specific vector with linguistic features as input to represent speaker identity, perform model adaptation to scale the hidden activation weights, and perform a feature space transformation at the output layer to modify generated acoustic features. We systematically analyse the performance of each individual adaptation technique and that of their combinations. Experimental results confirm the adaptability of the DNN, and listening tests demonstrate that the DNN can achieve significantly better adaptation performance than the hidden Markov model (HMM) baseline in terms of naturalness and speaker similarity.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-270",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "wang15c_interspeech": {
      "authors": [
        [
          "Yannan",
          "Wang"
        ],
        [
          "Jun",
          "Du"
        ],
        [
          "Li-Rong",
          "Dai"
        ],
        [
          "Chin-Hui",
          "Lee"
        ]
      ],
      "title": "High-resolution acoustic modeling and compact language modeling of language-universal speech attributes for spoken language identification",
      "original": "i15_0992",
      "page_count": 5,
      "order": 315,
      "p1": "992",
      "pn": "996",
      "abstract": [
        "We propose a framework to automatically construct a collection of high-resolution (HR) language-universal units for spoken language identification (LID). Based on the popular phone recognition language modeling (PRLM) approach to LID, a set of universal attribute recognizers (UARs) is first established to replace phone recognizers (PRs) using manner and place of articulation as attribute units and context-dependent (CD) attribute models are then built to achieve high-performance attribute transcription. To alleviate the difficulty of data sparsity in n-gram language modeling (LM) of these CD units, a clustering algorithm is proposed to compact the number of utilized attribute units in LM. Tested on the 2009 National Institute of Standards and Technology Language Recognition Evaluation for the 30-sec task using the same English Switchboard-I training data for acoustic modeling, our proposed approach achieves an equal error rate (EER) of 2.34%, representing a relative EER reduction of over 20% from the results of 2.88% obtained with the conventional PRLM techniques. To the best of our knowledge, this is the first time a single UAR based LID system significantly outperforms a signal PR based system with the same set of training data from a single language.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-271",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "irtza15_interspeech": {
      "authors": [
        [
          "Saad",
          "Irtza"
        ],
        [
          "Vidhyasaharan",
          "Sethu"
        ],
        [
          "Phu Ngoc",
          "Le"
        ],
        [
          "Eliathamby",
          "Ambikairajah"
        ],
        [
          "Haizhou",
          "Li"
        ]
      ],
      "title": "Phonemes frequency based PLLR dimensionality reduction for language recognition",
      "original": "i15_0997",
      "page_count": 5,
      "order": 316,
      "p1": "997",
      "pn": "1001",
      "abstract": [
        "This paper presents a new approach to reduce the dimensionality of Phone Log likelihood Ratio (PLLR) features, which have been shown to be effective for language recognition, by removing the likelihoods corresponding to less frequent phonemes. In this work, phoneme frequencies are estimated using a suitable phoneme recogniser. Following this, an i-vector framework is used to represent the total variability in the reduced dimensional PLLR feature space. This paper also proposes the use of Gaussian probabilistic linear discriminant analysis (GPLDA) as a backend for Language Recognition Evaluation (LRE) tasks. The suitability of both, the proposed dimensionality reductions technique and the GPLDA back-end has been evaluated on NIST 2007 and 2011 LRE tasks. The results show that the novel dimensionality reduction method outperforms PCA based dimensionality reduction by 7%. Further the results also show that GPLDA outperform generatively trained Gaussian back-ends, which have previously been used in conjunction with PLLR feature, by 14.6%.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-272",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "cumani15b_interspeech": {
      "authors": [
        [
          "Sandro",
          "Cumani"
        ],
        [
          "Old\u0159ich",
          "Plchot"
        ],
        [
          "Radek",
          "F\u00e9r"
        ]
      ],
      "title": "Exploiting i-vector posterior covariances for short-duration language recognition",
      "original": "i15_1002",
      "page_count": 5,
      "order": 317,
      "p1": "1002",
      "pn": "1006",
      "abstract": [
        "Linear models in i-vector space have shown to be an effective solution not only for speaker identification, but also for language recognition. The i-vector extraction process, however, is affected by several factors, such as noise level, the acoustic content of the utterance and the duration of the spoken segments. These factors influence both the i-vector estimate and its uncertainty, represented by the i-vector posterior covariance matrix. Modeling of i-vector uncertainty with Probabilistic Linear Discriminant Analysis has shown to be effective for short-duration speaker identification. This paper extends the approach to language recognition, analyzing the effects of i-vector covariances on a state-of-the-art Gaussian classifier, and proposes an effective solution for the reduction of the average detection cost (Cavg) for short segments.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-273",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "lykartsis15_interspeech": {
      "authors": [
        [
          "Athanasios",
          "Lykartsis"
        ],
        [
          "Stefan",
          "Weinzierl"
        ]
      ],
      "title": "Using the beat histogram for speech rhythm description and language identification",
      "original": "i15_1007",
      "page_count": 5,
      "order": 318,
      "p1": "1007",
      "pn": "1011",
      "abstract": [
        "In this paper we present a novel approach for the description of speech rhythm and the extraction of rhythm-related features for automatic language identification (LID). Previous methods have extracted speech rhythm through the calculation of features based on salient elements of speech such as consonants, vowels and syllables. We present how an automatic rhythm extraction method borrowed from music information retrieval, the beat histogram, can be adapted for the analysis of speech rhythm by defining the most relevant novelty functions in the speech signal and extracting features describing their periodicities. We have evaluated those features in a rhythm-based LID task for two multilingual speech corpora using support vector machines, including feature selection methods to identify the most informative descriptors. Results suggest that the method is successful in describing speech rhythm and provides LID classification accuracy comparable to or better than that of other approaches, without the need for a preceding segmentation or annotation of the speech signal. Concerning rhythm typology, the rhythm class hypothesis in its original form seems to be only partly confirmed by our results.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-274",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "saeidi15_interspeech": {
      "authors": [
        [
          "Rahim",
          "Saeidi"
        ],
        [
          "Tuija",
          "Niemi"
        ],
        [
          "Hanna",
          "Karppelin"
        ],
        [
          "Jouni",
          "Pohjalainen"
        ],
        [
          "Tomi",
          "Kinnunen"
        ],
        [
          "Paavo",
          "Alku"
        ]
      ],
      "title": "Speaker recognition for speech under face cover",
      "original": "i15_1012",
      "page_count": 5,
      "order": 319,
      "p1": "1012",
      "pn": "1016",
      "abstract": [
        "Speech under face cover constitute a case that is increasingly met by forensic speech experts. Wearing face cover mostly happens when an individual strives to conceal his or her identity. Based on the material of face cover and the level of contact with speech production organs, speech production becomes affected by face mask and a part of speech energy gets absorbed in the mask. There has been little research on how speech acoustics is affected by different face masks and how face covers might affect performance of automatic speaker recognition systems. In the present paper, we have collected speech under face mask with the aim of studying the effects of wearing different masks on state-of-the-art text-independent automatic speaker recognition system. The preliminary speaker recognition rates along with mask identification experiments are presented in this paper.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-275",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "rahman15_interspeech": {
      "authors": [
        [
          "Md. Hafizur",
          "Rahman"
        ],
        [
          "Ahilan",
          "Kanagasundaram"
        ],
        [
          "David",
          "Dean"
        ],
        [
          "Sridha",
          "Sridharan"
        ]
      ],
      "title": "Dataset-invariant covariance normalization for out-domain PLDA speaker verification",
      "original": "i15_1017",
      "page_count": 5,
      "order": 320,
      "p1": "1017",
      "pn": "1021",
      "abstract": [
        "In this paper we introduce a novel domain-invariant covariance normalization (DICN) technique to relocate both in-domain and out-domain i-vectors into a third dataset-invariant space, providing an improvement for out-domain PLDA speaker verification with a very small number of unlabelled in-domain adaptation i-vectors. By capturing the dataset variance from a global mean using both development out-domain i-vectors and limited unlabelled in-domain i-vectors, we could obtain domain-invariant representations of PLDA training data. The DICN-compensated out-domain PLDA system is shown to perform as well as in-domain PLDA training with as few as 500 unlabelled in-domain i-vectors for NIST-2010 SRE and 2000 unlabelled in-domain i-vectors for NIST-2008 SRE, and considerable relative improvement over both out-domain and in-domain PLDA development if more are available.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-276",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "xu15_interspeech": {
      "authors": [
        [
          "Longting",
          "Xu"
        ],
        [
          "Kong Aik",
          "Lee"
        ],
        [
          "Haizhou",
          "Li"
        ],
        [
          "Zhen",
          "Yang"
        ]
      ],
      "title": "Sparse coding of total variability matrix",
      "original": "i15_1022",
      "page_count": 5,
      "order": 321,
      "p1": "1022",
      "pn": "1026",
      "abstract": [
        "In text-independent speaker verification, it has been shown effective to represent the variable-length and information rich speech utterances using fixed-dimensional vectors, for instance, in the form of i-vectors. An i-vector is a low-dimensional vector in the so-called total variability space represented with a thin and tall rectangular matrix. Taking each row of the total variability matrix as a random vector, we look into the redundancy in representing the total variability space. We show that the total variability matrix is compressible and such characteristic could be exploited to reduce the memory and computational requirement in i-vector extraction. We also show that the existing sparse coding and dictionary learning techniques could be easily adapted for this purpose. Experiments on NIST SRE'10 dataset confirm that the total variability matrix could be represented with a smaller matrix without affecting the performance.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-277",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "cai15_interspeech": {
      "authors": [
        [
          "Weicheng",
          "Cai"
        ],
        [
          "Ming",
          "Li"
        ],
        [
          "Lin",
          "Li"
        ],
        [
          "QingYang",
          "Hong"
        ]
      ],
      "title": "Duration dependent covariance regularization in PLDA modeling for speaker verification",
      "original": "i15_1027",
      "page_count": 5,
      "order": 322,
      "p1": "1027",
      "pn": "1031",
      "abstract": [
        "In this paper, we present a covariance regularized probabilistic linear discriminant analysis (CR-PLDA) model for text independent speaker verification. In the conventional simplified PLDA modeling, the covariance matrix used to capture the residual energies is globally shared for all i-vectors. However, we believe that the point estimated i-vectors from longer speech utterances may be more accurate and their corresponding covariances in the PLDA modeling should be smaller. Similar to the inverse 0th order statistics weighted covariance in the i-vector model training, we propose a duration dependent normalized exponential term containing the duration normalizing factor \u00b5 and duration extent factor \u03bd to regularize the covariance in the PLDA modeling. Experimental results are reported on the NIST SRE 2010 common condition 5 female part task and the NIST 2014 i-vector machine learning challenge, respectively. For both tasks, the proposed covariance regularized PLDA system outperforms the baseline PLDA system by more than 13% relatively in terms of equal error rate (EER) and norm minDCF values.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-278",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "aronowitz15_interspeech": {
      "authors": [
        [
          "Hagai",
          "Aronowitz"
        ]
      ],
      "title": "Exploiting supervector structure for speaker recognition trained on a small development set",
      "original": "i15_1032",
      "page_count": 5,
      "order": 323,
      "p1": "1032",
      "pn": "1036",
      "abstract": [
        "Nowadays state-of-the-art speaker recognition systems obtain quite satisfactory results for both text-independent and text-dependent tasks as long as they are trained on a fair amount of development data from the target domain (assuming clean speech). In this work, we investigate the ability to build accurate speaker recognition systems using small amounts of data from the target domain without using out-of-domain data at all. Our method is based on exploiting the structural nature of GMM supervectors. Knowledge on the way GMM supervectors are created (namely a concatenation of statistics obtained for a set of Gaussians over the feature space) is used to guide modeling in high dimensional supervector space. We report experiments on both text-dependent and text-independent tasks which validate our method and show large error reductions.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-279",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "hong15_interspeech": {
      "authors": [
        [
          "QingYang",
          "Hong"
        ],
        [
          "Lin",
          "Li"
        ],
        [
          "Ming",
          "Li"
        ],
        [
          "Ling",
          "Huang"
        ],
        [
          "Lihong",
          "Wan"
        ],
        [
          "Jun",
          "Zhang"
        ]
      ],
      "title": "Modified-prior PLDA and score calibration for duration mismatch compensation in speaker recognition system",
      "original": "i15_1037",
      "page_count": 5,
      "order": 324,
      "p1": "1037",
      "pn": "1041",
      "abstract": [
        "To deal with the performance degradation of speaker recognition due to duration mismatch between enrollment and test utterances, a novel strategy to modify the standard normal prior distribution of the i-vector during probabilistic linear discriminant analysis (PLDA) modeling is employed. This new modified-prior PLDA model incorporates the covariance matrix scaled with duration of each utterance for each speaker, which achieves more discriminative characteristics by learning the duration variability as well as session variation in the i-vector space. Furthermore, an efficient Quality Measure Function (QMF) method which adopts duration variation as a compensation technique is employed to eliminate the linear shift in the score domain. To evaluate the robustness of the proposed approach, experiments were conducted on the NIST SRE10 core-core task in condition-5 with varying test utterance duration, in which the i-vectors of test utterances were extracted from full segment and randomly truncated segments of duration 10s and 20s. The results demonstrated the efficiency of modified-prior PLDA in different duration conditions, and the combined score calibration further improved the performance of speaker recognition.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-280",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "jelil15_interspeech": {
      "authors": [
        [
          "Sarfaraz",
          "Jelil"
        ],
        [
          "Rohan Kumar",
          "Das"
        ],
        [
          "Rohit",
          "Sinha"
        ],
        [
          "S. R. Mahadeva",
          "Prasanna"
        ]
      ],
      "title": "Speaker verification using Gaussian posteriorgrams on fixed phrase short utterances",
      "original": "i15_1042",
      "page_count": 5,
      "order": 325,
      "p1": "1042",
      "pn": "1046",
      "abstract": [
        "This work explores the speaker verification using fixed phrase short utterances. A novel speaker verification system using Gaussian posteriorgrams is proposed in which the posteriorgram vectors are computed from speaker specific Gaussian mixture model (GMM). The enrollment utterances for each of the target speakers are labeled with GMM trained on the corresponding speaker's data. The test trials are then labeled with the claimed speaker's GMM model. Dynamic time warping (DTW) is used to find a match score between the posteriorgrams of the claimed speaker and that of test trial. The proposed approach is evaluated on the fixed pass phrase subset of the recent RSR2015 database. For contrast purpose, we have also developed state-of-the-art i-vector system including probabilistic linear discriminant analysis (PLDA) classifier. The proposed framework is found to result in highly improved performance when compared with the i-vector based contrast system. We hypothesize that the cause of this large improvement lies in the use of speaker specific variances information in generation of the posteriorgram representations. On evaluating the proposed framework with non-speaker specific variances, it resulted in significant performance degradation which confirmed our hypothesis.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-281",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "gallardo15b_interspeech": {
      "authors": [
        [
          "Laura Fern\u00e1ndez",
          "Gallardo"
        ],
        [
          "Sebastian",
          "M\u00f6ller"
        ],
        [
          "Michael",
          "Wagner"
        ]
      ],
      "title": "Importance of intelligible phonemes for human speaker recognition in different channel bandwidths",
      "original": "i15_1047",
      "page_count": 5,
      "order": 326,
      "p1": "1047",
      "pn": "1051",
      "abstract": [
        "It is known that nasal consonants and vowels are more effective than other phonemes for human speaker recognition. However, the influence of channel transmissions on the speaker-discriminative capabilities of phonemes has not yet been examined. Specifically, the speech bandwidth has a strong effect on the human speaker recognition performance and also on the speech intelligibility. The phonemes that permit more accurate human speaker recognition are determined in this study by means of a speaker verification auditory test, focusing on the differences in performance when the stimuli are presented to listeners in narrowband and in wideband. The speech intelligibility is also investigated via an intelligibility test employing the same speech stimuli. Finally, the possible relationship between phonemes offering better human speaker recognition and more intelligible phonemes in the transition to an enhanced bandwidth is discussed.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-282",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "yamamoto15_interspeech": {
      "authors": [
        [
          "Hitoshi",
          "Yamamoto"
        ],
        [
          "Takafumi",
          "Koshinaka"
        ]
      ],
      "title": "Denoising autoencoder-based speaker feature restoration for utterances of short duration",
      "original": "i15_1052",
      "page_count": 5,
      "order": 327,
      "p1": "1052",
      "pn": "1056",
      "abstract": [
        "This paper describes a speaker feature restoration method for improving text-independent speaker recognition with short utterances. The method employs a denoising autoencoder (DAE) to compensate speaker features of a short utterance which contains limited phonetic information. It first estimates phonetic distribution in the utterance as posteriors based on speech models and then transforms an i-vector of the utterance using DAE along with the phonetic posteriors. The DAE-based transformation is able to produce a reliable speaker feature with help of supervised training using pairs of long and short speech segments. Speaker recognition experiments on an NIST SRE task demonstrate a 37.9% error reduction.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-283",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "ribas15_interspeech": {
      "authors": [
        [
          "Dayana",
          "Ribas"
        ],
        [
          "Emmanuel",
          "Vincent"
        ],
        [
          "Jos\u00e9 Ram\u00f3n",
          "Calvo"
        ]
      ],
      "title": "Full multicondition training for robust i-vector based speaker recognition",
      "original": "i15_1057",
      "page_count": 5,
      "order": 328,
      "p1": "1057",
      "pn": "1061",
      "abstract": [
        "Multicondition training (MCT) is an established technique to handle noisy and reverberant conditions. Previous works in the field of i-vector based speaker recognition have applied MCT to linear discriminant analysis (LDA) and probabilistic LDA (PLDA), but not to the universal background model (UBM) and the total variability (T) matrix, arguing that this would be too much time consuming due to the increase of the size of the training set by the number of noise and reverberation conditions. In this paper, we propose a full MCT approach which consists of applying MCT in all stages of training, including the UBM and the T matrix, while keeping the size of the training set fixed. Experiments in highly nonstationary noise conditions show a decrease of the equal error rate (EER) to 14.16% compared to 17.90% for clean training and 18.08% for MCT of LDA and PLDA only. We also evaluate the impact of state-of-the-art multichannel speech enhancement and show further reduction of the EER down to 10.47%.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-284",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "huang15b_interspeech": {
      "authors": [
        [
          "Zhen",
          "Huang"
        ],
        [
          "Sabato Marco",
          "Siniscalchi"
        ],
        [
          "I-Fan",
          "Chen"
        ],
        [
          "Jinyu",
          "Li"
        ],
        [
          "Jiadong",
          "Wu"
        ],
        [
          "Chin-Hui",
          "Lee"
        ]
      ],
      "title": "Maximum a posteriori adaptation of network parameters in deep models",
      "original": "i15_1076",
      "page_count": 5,
      "order": 329,
      "p1": "1076",
      "pn": "1080",
      "abstract": [
        "We present a Bayesian approach to adapting parameters of a well-trained context-dependent, deep-neural-network, hidden Markov model (CD-DNN-HMM) to improve automatic speech recognition performance. Given an abundance of DNN parameters but with only a limited amount of data, the effectiveness of the adapted DNN model can often be compromised. We formulate maximum a posteriori (MAP) adaptation of parameters of a specially designed CD-DNN-HMM with an augmented linear hidden networks connected to the output tied states, or senones, and compare it to feature space MAP linear regression previously proposed. Experimental evidences on the 20,000-word open vocabulary Wall Street Journal task demonstrate the feasibility of the proposed framework. In supervised adaptation, the proposed MAP adaptation approach provides more than 10% relative error reduction and consistently outperforms the conventional transformation based methods. Furthermore, we present an initial attempt to generate hierarchical priors to improve adaptation efficiency and effectiveness with limited adaptation data by exploiting similarities among senones.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-285",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition \u2013 Signal Processing, Acoustic Modeling, Robustness, and Adaptation"
    },
    "huang15c_interspeech": {
      "authors": [
        [
          "Yan",
          "Huang"
        ],
        [
          "Yifan",
          "Gong"
        ]
      ],
      "title": "Regularized sequence-level deep neural network model adaptation",
      "original": "i15_1081",
      "page_count": 5,
      "order": 330,
      "p1": "1081",
      "pn": "1085",
      "abstract": [
        "We propose a regularized sequence-level (SEQ) deep neural network (DNN) model adaptation methodology as an extension of the previous KL-divergence regularized cross-entropy (CE) adaptation [1]. In this approach, the negative KL-divergence between the baseline and the adapted model is added to the maximum mutual information (MMI) as regularization in the sequence-level adaptation.   We compared eight different adaptation setups specified by the baseline training criterion, the adaptation criterion, and the regularization methodology. We found that the proposed sequence-level adaptation consistently outperforms the cross-entropy adaptation. For both of them, regularization is critical. We further introduced a unified formulation in which the regularized CE and SEQ adaptation are the special cases.   We applied the proposed approach to speaker adaptation and accent adaptation in a mobile short message dictation task. For the speaker adaptation, with 25 or 100 utterances, the proposed approach yields 13.72% or 23.18% WER reduction when adapting from the CE baseline, comparing to 11.87% or 20.18% for the CE adaptation. For the accent adaptation, with 1K utterances, the proposed approach yields 18.74% or 19.50% WER reduction when adapting from the CE-DNN or the SEQ-DNN. The WER reduction using the regularized CE adaptation is 15.98% and 15.69%, respectively.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-286",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition \u2013 Signal Processing, Acoustic Modeling, Robustness, and Adaptation"
    },
    "li15_interspeech": {
      "authors": [
        [
          "Xiangang",
          "Li"
        ],
        [
          "Xihong",
          "Wu"
        ]
      ],
      "title": "Modeling speaker variability using long short-term memory networks for speech recognition",
      "original": "i15_1086",
      "page_count": 5,
      "order": 331,
      "p1": "1086",
      "pn": "1090",
      "abstract": [
        "Speaker adaptation of deep neural networks (DNNs) based acoustic models is still a challenging area of research. Considering that long short-term memory (LSTM) recurrent neural networks (RNNs) have been successfully applied to many sequence prediction and sequence labeling tasks, we propose to use LSTM RNNs for modeling speaker variability in automatic speech recognition (ASR). Firstly, the LSTM RNNs are used for extracting d-vectors (deep vector), which are then concatenated with the raw features for acoustic models. The speaker information provided by d-vectors helps DNNs based acoustic models figure out the speaker normalization during training. Furthermore, motivated by the idea that speech message can also be useful for speaker recognition, a new network called as cross-LSTM is proposed, which consist of two LSTMs: one for classifying speakers and the other for classifying senones. As a result, the speaker recognition and speech recognition are conducted simultaneously. Experiments are conducted on a conversational telephone speech corpus. Experimental results show the proposed models are effective for alleviating speaker variability in ASR, and yield 6% relative improvement for the LSTMP RNNs based systems.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-287",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition \u2013 Signal Processing, Acoustic Modeling, Robustness, and Adaptation"
    },
    "kumar15d_interspeech": {
      "authors": [
        [
          "Kshitiz",
          "Kumar"
        ],
        [
          "Chaojun",
          "Liu"
        ],
        [
          "Kaisheng",
          "Yao"
        ],
        [
          "Yifan",
          "Gong"
        ]
      ],
      "title": "Intermediate-layer DNN adaptation for offline and session-based iterative speaker adaptation",
      "original": "i15_1091",
      "page_count": 5,
      "order": 332,
      "p1": "1091",
      "pn": "1095",
      "abstract": [
        "In this work we present intermediate-layer deep neural network adaptation (DNN) techniques upon which we build offline as well as iterative speaker adaptation for online applications. We motivate our online work for task completion in Microsoft personal voice assistant, where we present different adaptation styles in a speech session e.g., (a) adapt the speaker-independent (SI) model on the current utterance, (b) recursively adapt an incremental speaker-dependent (SD) model in the session for just the previous utterance, (c) adapt the SI model for all past utterances in the session. We considered a number of adaptation techniques and demonstrated that the intermediate-layer approach with inserting-and-adapting a linear layer on top of an intermediate singular-value-decomposition layer provides the best results for offline adaptation, where we obtained respectively 22.6% and 12% relative reduction in word-error-rate (WER) for supervised and unsupervised adaptation on 100-utterances. An alternative intermediate-layer recursive adaptation in a 5-utterances session provided 6% relative-reduction in WER for online applications.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-288",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition \u2013 Signal Processing, Acoustic Modeling, Robustness, and Adaptation"
    },
    "karthickb15_interspeech": {
      "authors": [
        [
          "Murali",
          "Karthick B."
        ],
        [
          "Prateek",
          "Kolhar"
        ],
        [
          "S.",
          "Umesh"
        ]
      ],
      "title": "Speaker adaptation of convolutional neural network using speaker specific subspace vectors of SGMM",
      "original": "i15_1096",
      "page_count": 5,
      "order": 333,
      "p1": "1096",
      "pn": "1100",
      "abstract": [
        "The recent success of convolutional neural network (CNN) in speech recognition is due to its ability to capture translational variance in spectral features while performing discrimination. The CNN architecture requires correlated features as input and thus fMLLR transform which is estimated in de-correlated feature space fails to give significant improvement. In this paper, we propose two methods for extracting speaker adapted features in a correlated space using SGMMs. First, we estimate fMLLR transforms for correlated features by full covariance Gaussians using SGMM approach. Second, we augment speaker specific subspace vectors with acoustic features to provide speaker information in CNN models. Finally we propose a bottleneck - joint CNN/DNN framework to exploit the effects of both (fMLLR+ivectors) and (SGMM-fMLLR+speaker vectors) features. Experiments on TIMIT task show that our proposed features give 5.7% relative improvement over the log-mel features. Furthermore experiments on switchboard task show that the bottleneck - joint CNN/DNN model achieves 12.2% relative improvement over baseline joint CNN/DNN framework.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-289",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition \u2013 Signal Processing, Acoustic Modeling, Robustness, and Adaptation"
    },
    "miao15b_interspeech": {
      "authors": [
        [
          "Yajie",
          "Miao"
        ],
        [
          "Florian",
          "Metze"
        ]
      ],
      "title": "On speaker adaptation of long short-term memory recurrent neural networks",
      "original": "i15_1101",
      "page_count": 5,
      "order": 334,
      "p1": "1101",
      "pn": "1105",
      "abstract": [
        "Long Short-Term Memory (LSTM) is a recurrent neural network (RNN) architecture specializing in modeling long-range temporal dynamics. On acoustic modeling tasks, LSTM-RNNs have shown better performance than DNNs and conventional RNNs. In this paper, we conduct an extensive study on speaker adaptation of LSTM-RNNs. Speaker adaptation helps to reduce the mismatch between acoustic models and testing speakers. We have two main goals for this study. First, on a benchmark dataset, the existing DNN adaptation techniques are evaluated on the adaptation of LSTM-RNNs. We observe that LSTM-RNNs can be effectively adapted by using speaker-adaptive (SA) front-end, or by inserting speaker-dependent (SD) layers. Second, we propose two adaptation approaches that implement the SD-layer-insertion idea specifically for LSTM-RNNs. Using these approaches, speaker adaptation improves word error rates by 3-4% relative over a strong LSTM-RNN baseline. This improvement is enlarged to 6-7% if we exploit SA features for further adaptation.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-290",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition \u2013 Signal Processing, Acoustic Modeling, Robustness, and Adaptation"
    },
    "parisotto15_interspeech": {
      "authors": [
        [
          "Emilio",
          "Parisotto"
        ],
        [
          "Youness A.",
          "Ghassabeh"
        ],
        [
          "Matt J.",
          "MacDonald"
        ],
        [
          "Adelina",
          "Cozma"
        ],
        [
          "Elizabeth W.",
          "Pang"
        ],
        [
          "Frank",
          "Rudzicz"
        ]
      ],
      "title": "Automatic identification of received language in MEG",
      "original": "i15_1106",
      "page_count": 5,
      "order": 335,
      "p1": "1106",
      "pn": "1110",
      "abstract": [
        "We identify the language being received during English and Romanian auditory stimuli in 11 subjects before and after a period of learning 50 words in the latter using only magnetoencephalographic measures. To accomplish this, we extract on the order of 100,000 features (based on wavelets and descriptive statistics over windowed signals), and identify the most salient features. While we achieve very high accuracy in pre-training (up to 90% mean accuracy across 10-fold cross-validation for some subjects), it is significantly more difficult to tell received languages apart after training. We also identify significant effects of semantic word category and the subject's ability to play a musical instrument on classification accuracy.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-291",
      "author_area_id": "1",
      "author_area_label": "Speech Perception, Production, and Acquisition"
    },
    "werff15_interspeech": {
      "authors": [
        [
          "Laurens van der",
          "Werff"
        ],
        [
          "J\u00f3n",
          "Gu\u00f0nason"
        ],
        [
          "Kamilla R\u00fan",
          "J\u00f3hannsd\u00f3ttir"
        ]
      ],
      "title": "Detection of cardiovascular reactivity in speech",
      "original": "i15_1111",
      "page_count": 5,
      "order": 336,
      "p1": "1111",
      "pn": "1115",
      "abstract": [
        "The feasibility of automatically detecting cardiovascular reactivity from speech was investigated. There are studies that have shown success in detecting heart rate in the speech signal before but cardiovascular reactivity has not been looked at as well. Gender-specific, speaker-independent Gaussian mixture models were trained on speech during high and low cardiovascular reactivity and classification implemented using a cosine distance scoring (ivector) approach. Using five distinct criteria to determine whether classification was meaningful, we found clear indication that cardiovascular reactivity affects the voice in a manner that makes it automatically detectable in speech. As such it may become a powerful new information source for estimating various physiological conditions from speech.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-292",
      "author_area_id": "1",
      "author_area_label": "Speech Perception, Production, and Acquisition"
    },
    "francoisnienaber15_interspeech": {
      "authors": [
        [
          "Alex",
          "Francois-Nienaber"
        ],
        [
          "Jed A.",
          "Meltzer"
        ],
        [
          "Frank",
          "Rudzicz"
        ]
      ],
      "title": "Lateralization in emotional speech perception following transcranial direct current stimulation",
      "original": "i15_1116",
      "page_count": 5,
      "order": 337,
      "p1": "1116",
      "pn": "1120",
      "abstract": [
        "The degree to which the perception of spoken emotion is lateralized in the brain remains a controversial topic. This work examines hemispheric differences in the perception of emotion in speech by applying tDCS, a neurostimulation protocol, to the T-RES speech emotion rating paradigm. We find several significant effects, including a strong interaction of prosody and neurostimulation for perceptual ratings when considering only lexical content, and that the perception of happiness does not appear to be affected by tDCS, but anger and (to a large extent) fear appear less intense after stimulation.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-293",
      "author_area_id": "1",
      "author_area_label": "Speech Perception, Production, and Acquisition"
    },
    "yang15c_interspeech": {
      "authors": [
        [
          "Minda",
          "Yang"
        ],
        [
          "Sameer A.",
          "Sheth"
        ],
        [
          "Catherine A.",
          "Schevon"
        ],
        [
          "Guy M.",
          "McKhann"
        ],
        [
          "Nima",
          "Mesgarani"
        ]
      ],
      "title": "Speech reconstruction from human auditory cortex with deep neural networks",
      "original": "i15_1121",
      "page_count": 5,
      "order": 338,
      "p1": "1121",
      "pn": "1125",
      "abstract": [
        "We examined the accuracy of the reconstructed speech spectrograms from neural responses recorded intracranially in human auditory cortex. Electrodes were implanted over the cortex of epilepsy patients for the localization of seizures, and neural responses were recorded as the subjects passively listened to continuous speech. We compared the reconstructed spectrograms estimated with two different models: a linear regression model and a deep neural network. Compared with linear regression model, the reconstructed spectrograms from the deep neural network achieved a higher average correlation with the original spectrograms. In addition, the reconstructed spectrograms from the neural network better preserved the average acoustic features of phones. We further investigated how changing the number of hidden layers in the network affects the reconstruction accuracy and found a better performance with deeper networks, particularly in the reconstruction of spectrotemporal modulation content of speech. These findings reveal the efficacy of deep neural network models in decoding speech signals from neural responses and provide a method for improving the performance of brain computer interfaces with prosthetic applications.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-294",
      "author_area_id": "1",
      "author_area_label": "Speech Perception, Production, and Acquisition"
    },
    "brumberg15_interspeech": {
      "authors": [
        [
          "Jonathan S.",
          "Brumberg"
        ],
        [
          "Nichol",
          "Castro"
        ],
        [
          "Akshatha",
          "Rao"
        ]
      ],
      "title": "Temporal dynamics of the speech readiness potential, and its use in a neural decoder of speech-motor intention",
      "original": "i15_1126",
      "page_count": 5,
      "order": 339,
      "p1": "1126",
      "pn": "1130",
      "abstract": [
        "We investigated the temporal dynamics of brain activity related to speech and motor preparation. Previous electroencephalography (EEG) studies of speech production have identified a slow wave negativity that occurs as early as 2 seconds prior to articulation, known as the readiness potential (RP). This EEG potential is commonly described as having two phases, an early slow component and a late fast component. In our study, we collected RPs from 15 subjects and fit a linear spline with four fixed knots and their locations as free parameters. The result was a piecewise linear approximation to the subject RPs from which it was possible to identify the onset and termination of both the slow and fast components, as well as their slopes. In addition, we used the collected RPs to train an adaptive filter neural decoding algorithm to predict occurrences of RPs from trial-based epochs of imagined speech and motor movements for use in a brain-computer interface for speech communication. The initial spline analysis will help to determine the contribution of each RP phase to the representations of intended speech-motor behavior, reducing the complexity of the adaptive filter for more efficient use in real-time.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-295",
      "author_area_id": "1",
      "author_area_label": "Speech Perception, Production, and Acquisition"
    },
    "heger15_interspeech": {
      "authors": [
        [
          "Dominic",
          "Heger"
        ],
        [
          "Christian",
          "Herff"
        ],
        [
          "Adriana de",
          "Pesters"
        ],
        [
          "Dominic",
          "Telaar"
        ],
        [
          "Peter",
          "Brunner"
        ],
        [
          "Gerwin",
          "Schalk"
        ],
        [
          "Tanja",
          "Schultz"
        ]
      ],
      "title": "Continuous speech recognition from ECoG",
      "original": "i15_1131",
      "page_count": 5,
      "order": 340,
      "p1": "1131",
      "pn": "1135",
      "abstract": [
        "Continuous speech production is a highly complex process involving many parts of the human brain. To date, no fundamental representation that allows for decoding of continuous speech from neural signals has been presented. Here we show that techniques from automatic speech recognition can be applied to decode a textual representation of spoken words from neural signals. We model phones as the fundamental unit of the speech process in invasively measured brain activity (intracranial electrocorticographic (ECoG)) recordings. These phone models give insights into timings and locations of neural processes associated with the continuous production of speech and can be used in a speech recognizer to decode the neural data into their textual representations. When restricting the dictionary to small subsets, Word Error Rates as low as 25% can be achieved. As the brain activity data sets are fairly small, alternative approaches to Gaussian models are investigated by relying on robust, regularized discriminative models.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-296",
      "author_area_id": "1",
      "author_area_label": "Speech Perception, Production, and Acquisition"
    },
    "chen15h_interspeech": {
      "authors": [
        [
          "Yu-hsin",
          "Chen"
        ],
        [
          "Ignacio",
          "Lopez-Moreno"
        ],
        [
          "Tara N.",
          "Sainath"
        ],
        [
          "Mirk\u00f3",
          "Visontai"
        ],
        [
          "Raziel",
          "Alvarez"
        ],
        [
          "Carolina",
          "Parada"
        ]
      ],
      "title": "Locally-connected and convolutional neural networks for small footprint speaker recognition",
      "original": "i15_1136",
      "page_count": 5,
      "order": 341,
      "p1": "1136",
      "pn": "1140",
      "abstract": [
        "This work compares the performance of deep Locally-Connected Networks (LCN) and Convolutional Neural Networks (CNN) for text-dependent speaker recognition. These topologies model the local time-frequency correlations of the speech signal better, using only a fraction of the number of parameters of a fully connected Deep Neural Network (DNN) used in previous works. We show that both a LCN and CNN can reduce the total model footprint to 30% of the original size compared to a baseline fully-connected DNN, with minimal impact in performance or latency. In addition, when matching parameters, the LCN improves speaker verification performance, as measured by equal error rate (EER), by 8% relative over the baseline without increasing model size or computation. Similarly, a CNN improves EER by 10% relative over the baseline for the same model size but with increased computation.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-297",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "garciaromero15_interspeech": {
      "authors": [
        [
          "Daniel",
          "Garcia-Romero"
        ],
        [
          "Alan",
          "McCree"
        ]
      ],
      "title": "Insights into deep neural networks for speaker recognition",
      "original": "i15_1141",
      "page_count": 5,
      "order": 342,
      "p1": "1141",
      "pn": "1145",
      "abstract": [
        "Traditional i-vector speaker recognition systems use a Gaussian mixture model (GMM) to collect sufficient statistics. Recently, replacing this GMM with a deep neural network (DNN) has shown promising results. In this paper, we study a number of open issues that relate to performance, computational complexity, and applicability of DNNs as part of the full speaker recognition pipeline. The experimental validation is performed on the female part of the SRE12 telephone condition 2, where our DNN-based system produces the best published results. The insights gained by our study indicate that, for the purpose of speaker recognition, not using fMLLR speaker adaptation and early stopping of the DNN training allow significant computational reduction without sacrificing performance. Also, using a full covariance universal background model (UBM) and a large set of senones produces important performance gains. Finally, the DNN-based approach does not exhibit a strong language dependence as a DNN trained on Spanish data outperforms the conventional GMM-based system on our English task.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-298",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "richardson15_interspeech": {
      "authors": [
        [
          "Fred",
          "Richardson"
        ],
        [
          "Douglas A.",
          "Reynolds"
        ],
        [
          "Najim",
          "Dehak"
        ]
      ],
      "title": "A unified deep neural network for speaker and language recognition",
      "original": "i15_1146",
      "page_count": 5,
      "order": 343,
      "p1": "1146",
      "pn": "1150",
      "abstract": [
        "Significant performance gains have been reported separately for speaker recognition (SR) and language recognition (LR) tasks using either DNN posteriors of sub-phonetic units or DNN feature representations, but the two techniques have not been compared on the same SR or LR task or across SR and LR tasks using the same DNN. In this work we present the application of a single DNN for both tasks using the 2013 Domain Adaptation Challenge speaker recognition (DAC13) and the NIST 2011 language recognition evaluation (LRE11) benchmarks. Using a single DNN trained on Switchboard data we demonstrate large gains in performance on both benchmarks: a 55% reduction in EER for the DAC13 out-of-domain condition and a 48% reduction in Cavg on the LRE11 30s test condition. Score fusion and feature fusion are also investigated as is the performance of the DNN technologies at short durations for SR.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-299",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "tian15_interspeech": {
      "authors": [
        [
          "Yao",
          "Tian"
        ],
        [
          "Meng",
          "Cai"
        ],
        [
          "Liang",
          "He"
        ],
        [
          "Jia",
          "Liu"
        ]
      ],
      "title": "Investigation of bottleneck features and multilingual deep neural networks for speaker verification",
      "original": "i15_1151",
      "page_count": 5,
      "order": 344,
      "p1": "1151",
      "pn": "1155",
      "abstract": [
        "Recently, the integration of deep neural networks (DNNs) with i-vector systems is proved to be effective for speaker verification. This method uses the DNN with senone outputs to produce frame alignments for sufficient statistics extraction. However, two types of data mismatch may degrade the performance of the DNN-based speaker verification systems. First, the DNN requires transcribed training data, while the data sets used for i-vector training and extraction are mostly untranscribed. Second, the language of the training data for DNN is limited by the pronunciation lexicon, making the model unsuitable for multilingual tasks. In this paper, we propose to use bottleneck features and multilingual DNNs to narrow the gap caused by the data mismatch. In our method, a DNN is first trained with senone labels to extract bottleneck features. Then a Gaussian mixture model (GMM) is trained with the bottleneck features to produce frame alignments. Additionally, bottleneck features based on multilingual DNNs are explored for multilingual speaker verification. Experiments on the NIST SRE 2008 female short2-short3 telephone task (multilingual) and the NIST SRE 2010 female core-extended telephone task (English) demonstrate the effectiveness of the proposed method.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-300",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "xing15_interspeech": {
      "authors": [
        [
          "Hua",
          "Xing"
        ],
        [
          "Gang",
          "Liu"
        ],
        [
          "John H. L.",
          "Hansen"
        ]
      ],
      "title": "Frequency offset correction in single sideband (SSB) speech by deep neural network for speaker verification",
      "original": "i15_1156",
      "page_count": 5,
      "order": 345,
      "p1": "1156",
      "pn": "1160",
      "abstract": [
        "Communication system mismatch represents a major influence for loss in speaker recognition performance. This paper considers a type of nonlinear communication system mismatch- modulation/ demodulation (Mod/DeMod) carrier drift in single sideband (SSB) speech signals. We focus on the problem of estimating frequency offset in SSB speech in order to improve speaker verification performance of the drifted speech. Based on a two-step framework from previous work, we propose using a multi-layered neural network architecture, stacked denoising autoencoder (SDA), to determine the unique interval of the offset value in the first step. Experimental results demonstrate that the SDA based system can produce up to a +16.1% relative improvement in frequency offset estimation accuracy. A speaker verification evaluation shows a +65.9% relative improvement in EER when SSB speech signal is compensated with the frequency offset value estimated by the proposed method.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-301"
    },
    "zheng15b_interspeech": {
      "authors": [
        [
          "Hao",
          "Zheng"
        ],
        [
          "Shanshan",
          "Zhang"
        ],
        [
          "Wenju",
          "Liu"
        ]
      ],
      "title": "Exploring robustness of DNN/RNN for extracting speaker baum-welch statistics in mismatched conditions",
      "original": "i15_1161",
      "page_count": 5,
      "order": 346,
      "p1": "1161",
      "pn": "1165",
      "abstract": [
        "This work explores the use of DNN/RNN for extracting Baum-Welch sufficient statistics in place of the conventional GMM-UBM in speaker recognition. In this framework, the DNN/RNN is trained for automatic speech recognition (ASR) and each of the output unit corresponds to a component of GMM-UBM. Then the outputs of network are combined with acoustic features to calculate sufficient statistics for speaker recognition. We evaluate and analyze the performance of networks with different configurations and training corpuses in this paper. Experimental results on text-independent SRE NIST 2008 and text-dependent RSR2015 speaker verification tasks show the robustness of DNN/RNN for extracting statistics in mismatched evaluation conditions compared with GMM-UBM system. Particularly, Long Short-Term Memory (LSTM) RNN realized in this work outperforms traditional DNN and GMM-UBM in most mismatched conditions.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-302",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "yoshimura15_interspeech": {
      "authors": [
        [
          "Takenori",
          "Yoshimura"
        ],
        [
          "Kei",
          "Hashimoto"
        ],
        [
          "Yoshihiko",
          "Nankaku"
        ],
        [
          "Keiichi",
          "Tokuda"
        ]
      ],
      "title": "Simultaneous optimization of multiple tree structures for factor analyzed HMM-based speech synthesis",
      "original": "i15_1196",
      "page_count": 5,
      "order": 347,
      "p1": "1196",
      "pn": "1200",
      "abstract": [
        "Some speech synthesis approaches are based on an assumption that voice characteristics, e.g., speaker, speaking style, and emotion, are represented in a low-dimensional subspace. In these approaches, the model structures of the basis vectors which span the subspace are typically constructed with decision trees, and are important to synthesize high-quality speech. However, since it is difficult to evaluate all the candidates of the model structures, some strong constraints are usually applied in the model construction to reduce the huge computational complexity. To overcome this problem, this paper presents a new technique that simultaneously construct the model structures with multiple tree structures without the constraints. The proposed technique enables to find the more optimal model structures because the more complex model structure candidates can be evaluated by using some computational complexity reduction algorithms. Experimental results show that the proposed method improves the naturalness of the synthesized speech from the conventional one.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-303",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "pouget15_interspeech": {
      "authors": [
        [
          "Ma\u00ebl",
          "Pouget"
        ],
        [
          "Thomas",
          "Hueber"
        ],
        [
          "G\u00e9rard",
          "Bailly"
        ],
        [
          "Timo",
          "Baumann"
        ]
      ],
      "title": "HMM training strategy for incremental speech synthesis",
      "original": "i15_1201",
      "page_count": 5,
      "order": 348,
      "p1": "1201",
      "pn": "1205",
      "abstract": [
        "Incremental speech synthesis aims at delivering the synthetic voice while the sentence is still being typed. One of the main challenges is the online estimation of the target prosody from a partial knowledge of the sentence's syntactic structure. In the context of HMM-based speech synthesis, this typically results in missing segmental and suprasegmental features, which describe the linguistic context of each phoneme. This study describes a voice training procedure which integrates explicitly a potential uncertainty on some contextual features. The proposed technique is compared to a baseline approach (previously published), which consists in substituting a missing contextual feature by a default value calculated on the training set. Both techniques were implemented in a HMM-based Text-To-Speech system for French, and compared using objective and perceptual measurements. Experimental results show that the proposed strategy outperforms the baseline technique for this language.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-304",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "takamichi15_interspeech": {
      "authors": [
        [
          "Shinnosuke",
          "Takamichi"
        ],
        [
          "Tomoki",
          "Toda"
        ],
        [
          "Alan W.",
          "Black"
        ],
        [
          "Satoshi",
          "Nakamura"
        ]
      ],
      "title": "Modulation spectrum-constrained trajectory training algorithm for HMM-based speech synthesis",
      "original": "i15_1206",
      "page_count": 5,
      "order": 349,
      "p1": "1206",
      "pn": "1210",
      "abstract": [
        "This paper presents a novel training algorithm for Hidden Markov Model (HMM)-based speech synthesis. One of the biggest issues causing significant quality degradation in synthetic speech is the over-smoothing effect often observed in generated speech parameter trajectories. Recently, we have found that a Modulation Spectrum (MS) of the generated speech parameters is sensitively correlated with the over-smoothing effect, and have proposed the parameter generation algorithm considering the MS. The over-smoothing effect is effectively alleviated by the proposed parameter generation algorithm. On the other hand, it loses the computationally-efficient generation processing of the conventional generation algorithm. In this paper, the MS is integrated into the training stage instead of the parameter generation stage in a similar manner as our previous work on Gaussian Mixture Model (GMM)-based spectral parameter trajectory conversion. The trajectory HMM is trained with a novel objective function consisting of both the conventional trajectory HMM likelihood and a newly implemented MS likelihood. This training framework is further extended to the F0 component. The experimental results demonstrate that the proposed algorithm yields improvements in synthetic speech quality while preserving a capability of the computationally-efficient generation processing.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-305",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "black15b_interspeech": {
      "authors": [
        [
          "Alan W.",
          "Black"
        ],
        [
          "Prasanna Kumar",
          "Muthukumar"
        ]
      ],
      "title": "Random forests for statistical speech synthesis",
      "original": "i15_1211",
      "page_count": 5,
      "order": 350,
      "p1": "1211",
      "pn": "1215",
      "abstract": [
        "The world of statistical parametric speech synthesis continues to improve with recent investigations of different machine learning techniques to better model spectrum, F0 and duration from corpora of natural speech. Traditional techniques rely on decision trees alone. This paper shows the advantages of modeling with random forests of decision trees over single trees. Improvements equivalent to more than doubling the data can be achieved, offering end users significantly better synthesis from the same data size. These techniques give proportionally more improvements on smaller datasets, particularly with voices with only 30 minutes of speech. These techniques have been tested over a wide range of voices and languages of various sizes and quality, producing significant improvements in all cases. These techniques are documented, and robustly implemented for others to use through the Dec 2014 release of the Festvox voice building toolkit, thereby directly allowing these benefits to be used in standard voices build for the Festival Speech Synthesis System and CMU Flite.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-306",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "hong15b_interspeech": {
      "authors": [
        [
          "Doo Hwa",
          "Hong"
        ],
        [
          "Joun Yeop",
          "Lee"
        ],
        [
          "Se Young",
          "Jang"
        ],
        [
          "Nam Soo",
          "Kim"
        ]
      ],
      "title": "Speaker adaptation using relevance vector regression for HMM-based expressive TTS",
      "original": "i15_1216",
      "page_count": 5,
      "order": 351,
      "p1": "1216",
      "pn": "1220",
      "abstract": [
        "The conventional maximum likelihood linear regression (MLLR)-based adaptation algorithm employed to acoustic hidden Markov models (HMMs) is too restricted in linear regression to represent the details of mapping charateristics. To overcome this problem, we propose the relevance vector regression (RVR)-based model parameter adaptation technique. In this framework, the conventional technique is extended to have much more basis functions. Also, the weights for conducting a transform matrix are obtained by sparse Bayesian learning, in which most of the weights become zero due to the definition of the prior with the precision hyper-parameters. Furthermore, by using the appropriate kernel functions, RVR can take both of the advantages of linear and nonlinear regression. In the experiments, the emotional speech database is used for adaptation to evaluate the proposed method compared with the conventional constrained MLLR. From the experimental results, we conclude that the RVR adaption method performs better than the conventional method.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-307",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "tsiaras15_interspeech": {
      "authors": [
        [
          "Vassilis",
          "Tsiaras"
        ],
        [
          "Ranniery",
          "Maia"
        ],
        [
          "Vassilis",
          "Diakoloukas"
        ],
        [
          "Yannis",
          "Stylianou"
        ],
        [
          "Vassilis",
          "Digalakis"
        ]
      ],
      "title": "Towards a linear dynamical model based speech synthesizer",
      "original": "i15_1221",
      "page_count": 5,
      "order": 352,
      "p1": "1221",
      "pn": "1225",
      "abstract": [
        "We present recent developments towards building a speech synthesis system completely based on Linear Dynamical Models (LDMs). Specifically, we describe a decision tree-based context clustering approach to LDM-based speech synthesis and an algorithm for parameter generation using global variance with LDMs. In order to capture the speech dynamics, LDMs need coarser phoneme segmentation than the 5-state segmentation usually used in Hidden Markov Model (HMM)-based speech synthesis. Therefore, using LDMs to evaluate the clustering of longer phoneme segments improves the linguistic-to-acoustic mapping and leads to trajectories of synthetic speech parameters without discontinuities and closer to the natural ones. It also decreases the footprint of the system since the total number of decision tree leaves is smaller than the total number of leaves usually produced in a typical HMM-based synthesizer. On the other hand, global variance greatly improves the naturalness of the synthesized speech. According to subjective evaluation, the proposed LDM-based system with only 25% of the parameters of a baseline HMM-based synthesizer is able to produce synthetic speech of similar quality.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-308",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "looze15_interspeech": {
      "authors": [
        [
          "C\u00e9line De",
          "Looze"
        ],
        [
          "Brian",
          "Vaughan"
        ],
        [
          "Finnian",
          "Kelly"
        ],
        [
          "Alison",
          "Kay"
        ]
      ],
      "title": "Providing objective metrics of team communication skills via interpersonal coordination mechanisms",
      "original": "i15_1226",
      "page_count": 5,
      "order": 353,
      "p1": "1226",
      "pn": "1230",
      "abstract": [
        "Being able to communicate efficiently has been acknowledged as a vital skill in many different domains. In particular, team communication skills are of key importance in the operation of complex machinery such as aircrafts, maritime vessels and such other, highly-specialized, civilian or military vehicles, as well as the performance of complex tasks in the medical domain. In this paper, we propose to use prosodic accommodation and turn-taking organisation to provide objective metrics of communication skills. To do this, human-factors evaluations, via a coordination Demand Analysis (CDA), were used in conjunction with a dynamic model of prosodic accommodation and turn-taking organisation. Using conversational speech from airline pilots involved in a collaborative task (decision-making exercise), our study reveals that interpersonal coordination mechanisms are indicative of human evaluation of pilots' communication skills. We discuss our results in terms of relevance for training simulation for personnel in safety or mission critical environments.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-309",
      "author_area_id": "10",
      "author_area_label": "Speech Recognition \u2013 Technologies and Systems for New Applications"
    },
    "lee15d_interspeech": {
      "authors": [
        [
          "Donghyeon",
          "Lee"
        ],
        [
          "Jinsik",
          "Lee"
        ],
        [
          "Eun-Kyoung",
          "Kim"
        ],
        [
          "Jaewon",
          "Lee"
        ]
      ],
      "title": "Dialog act modeling for virtual personal assistant applications using a small volume of labeled data and domain knowledge",
      "original": "i15_1231",
      "page_count": 5,
      "order": 354,
      "p1": "1231",
      "pn": "1235",
      "abstract": [
        "Recently, virtual personal assistant (VPA) applications have been employed in mobile devices, which provide a natural and convenient interface between human and machines. As the VPA services become popular, consumers demand for a wider service than their scope, so the rapid development becomes more important. This paper introduces a dialog act modeling approach for VPA applications, which is an extension of a Latent Dirichlet Allocation model. This approach enables the rapid and cost-effective development by reducing human efforts for manual labeling and the development of a fail-safe product by incorporating domain knowledge such as dictionaries, cross-lingual data, and logic rules. The experimental results showed that a reliable and high-performance dialog act model was built only with a small volume of labeled data and domain knowledge.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-310",
      "author_area_id": "10",
      "author_area_label": "Speech Recognition \u2013 Technologies and Systems for New Applications"
    },
    "zainko15_interspeech": {
      "authors": [
        [
          "Csaba",
          "Zaink\u00f3"
        ],
        [
          "M\u00e1ty\u00e1s",
          "Bartalis"
        ],
        [
          "G\u00e9za",
          "N\u00e9meth"
        ],
        [
          "G\u00e1bor",
          "Olaszy"
        ]
      ],
      "title": "A polyglot domain optimised text-to-speech system for railway station announcements",
      "original": "i15_1236",
      "page_count": 5,
      "order": 355,
      "p1": "1236",
      "pn": "1240",
      "abstract": [
        "Announcements at railway stations are a major information source for passengers. In order to ensure high intelligibility, the traditional solution is to use recorded prompts with \u201cslot filling\u201d of variable data. If a data type (e.g. train name) changes new recordings have to be made. Even with careful design the quality of the system will gradually deteriorate due to change of the voice of the voice talent, speech rate, etc.   Advances in corpus-based technology have allowed the introduction of text-to-speech solutions into this application domain. In this paper our solution for a flexible, single voice based polyglot system is described. It is currently implemented for Hungarian and English with plans underway for German. Hungary, being at the geographic center of Europe is at the crossroads of rail connections to more than 15 countries. The Hungarian system announces the Hungarian variant of station names while the English system shall read them in the official language of the country (e.g. Venice is `Velence' in Hungarian and `Venezia' in Italian).   The system has been in operation at the largest passenger railway station of Hungary since June 2014 and has been installed for more than 60 other stations and stops.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-311",
      "author_area_id": "10",
      "author_area_label": "Speech Recognition \u2013 Technologies and Systems for New Applications"
    },
    "mandal15_interspeech": {
      "authors": [
        [
          "Partho",
          "Mandal"
        ],
        [
          "Shalini",
          "Jain"
        ],
        [
          "Gaurav",
          "Ojha"
        ],
        [
          "Anupam",
          "Shukla"
        ]
      ],
      "title": "Development of hindi speech recognition system of agricultural commodities using deep neural network",
      "original": "i15_1241",
      "page_count": 5,
      "order": 356,
      "p1": "1241",
      "pn": "1245",
      "abstract": [
        "To create a system for speech recognition customized for services in a particular domain, it is very important to add more and more languages to the `supported languages' database of the system. In this study, we have collected speech data from a sample of the population we were targeting the system for i.e. tasks for agricultural commodities. We performed the acoustic modelling of this data using a combination of Deep Neural Network (DNN) and Hidden Markov model (HMM) in which the HMM state likelihoods are taken from the outputs of the DNN. We have performed a three stage training: RBM pre-training, frame cross-entropy training, and sequence-training optimizing MMI/sMBR. After extensive experimentation, the accuracy of our system comes to about 82%. This study motivates further research for fine-tuning of such systems.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-312"
    },
    "feher15_interspeech": {
      "authors": [
        [
          "Thomas",
          "Feh\u00e9r"
        ],
        [
          "Michael",
          "Freitag"
        ],
        [
          "Christian",
          "Gruber"
        ]
      ],
      "title": "Real-time audio signal enhancement for hands-free speech applications",
      "original": "i15_1246",
      "page_count": 5,
      "order": 357,
      "p1": "1246",
      "pn": "1250",
      "abstract": [
        "Automatic speech recognition in a home environment is currently of large interest for many practical applications. It is also challenging for signal processing, since several problems have to be solved satisfactorily. First, a large degradation of speech recognition performance is caused by room reverberations. Second, environmental noises like a radio, television or kitchen devices may further degrade speech recognition. And third, the problem of concurrent and possibly moving speakers has to be addressed.   In this paper we present our results of an extensive investigation of state-of-the-art multi-channel signal processing algorithms used for dereverberation, noise reduction and speaker localization. The investigation is based on a microphone array with 16 microphones which are part of a fixed ceiling-mounted device. All signal processing algorithms are implemented on a PC platform to run in real-time and produce an enhanced signal, which is finally used for speech recognition.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-313",
      "author_area_id": "10",
      "author_area_label": "Speech Recognition \u2013 Technologies and Systems for New Applications"
    },
    "erro15_interspeech": {
      "authors": [
        [
          "D.",
          "Erro"
        ],
        [
          "Inma",
          "Hernaez"
        ],
        [
          "Agustin",
          "Alonso"
        ],
        [
          "D.",
          "Garc\u00eda-Lorenzo"
        ],
        [
          "Eva",
          "Navas"
        ],
        [
          "J.",
          "Ye"
        ],
        [
          "H.",
          "Arzelus"
        ],
        [
          "Igor",
          "Jauk"
        ],
        [
          "N. Q.",
          "Hy"
        ],
        [
          "C.",
          "Magari\u00f1os"
        ],
        [
          "R.",
          "P\u00e9rez-Ram\u00f3n"
        ],
        [
          "M.",
          "Sul\u00edr"
        ],
        [
          "Xiaohai",
          "Tian"
        ],
        [
          "X.",
          "Wang"
        ]
      ],
      "title": "Personalized synthetic voices for speaking impaired: website and app",
      "original": "i15_1251",
      "page_count": 4,
      "order": 358,
      "p1": "1251",
      "pn": "1254",
      "abstract": [
        "This paper describes the current state of the work that is being carried out in the framework of the ZureTTS project to give a personalized voice to people who cannot speak in their own. Despite the availability of tools and algorithms to synthesize speech and adapt it to new speakers, this process is affordable only for experts. To overcome this problem, we recently developed a web interface that assists users in doing so. At this point only healthy users can fully personalize the synthetic voice via adaptation, while impaired users can just manually tune a few dimensions of it. As a complement, we have launched an Android application that connects to the ZureTTS server and makes use of its functionalities in an intuitive way. Although many parts of the system need to be improved, its current version is publicly accessible and ready to be used.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-314",
      "author_area_id": "10",
      "author_area_label": "Speech Recognition \u2013 Technologies and Systems for New Applications"
    },
    "sahraeian15_interspeech": {
      "authors": [
        [
          "Reza",
          "Sahraeian"
        ],
        [
          "Dirk Van",
          "Compernolle"
        ],
        [
          "Febe de",
          "Wet"
        ]
      ],
      "title": "Under-resourced speech recognition based on the speech manifold",
      "original": "i15_1255",
      "page_count": 5,
      "order": 359,
      "p1": "1255",
      "pn": "1259",
      "abstract": [
        "Conventional acoustic modeling involves estimating many parameters to effectively model feature distributions. The sparseness of speech and text data, however, degrades the reliability of the estimation process and makes speech recognition a challenging task. In this paper, we propose to use a nonlinear feature transformation based on the speech manifold called Intrinsic Spectral Analysis (ISA) for under-resourced speech recognition. First, we investigate the usefulness of ISA features in low resource scenarios for both Gaussian mixture and deep neural network (DNN) acoustic modeling. Moreover, due to the connection of ISA features to the articulatory configuration space, this feature space is potentially less language dependent than other typical spectral-based features, and therefore exploiting out-of-language data in this feature space is beneficial. We demonstrate the positive effect of ISA in the frame work of multilingual DNN systems where Flemish and Afrikaans are used as donor and under-resourced target languages respectively. We compare the performance of ISA with conventional features in both multilingual and under-resourced monolingual conditions.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-315",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition \u2013 Signal Processing, Acoustic Modeling, Robustness, and Adaptation"
    },
    "golik15b_interspeech": {
      "authors": [
        [
          "Pavel",
          "Golik"
        ],
        [
          "Zolt\u00e1n",
          "T\u00fcske"
        ],
        [
          "Ralf",
          "Schl\u00fcter"
        ],
        [
          "Hermann",
          "Ney"
        ]
      ],
      "title": "Multilingual features based keyword search for very low-resource languages",
      "original": "i15_1260",
      "page_count": 5,
      "order": 360,
      "p1": "1260",
      "pn": "1264",
      "abstract": [
        "In this paper we describe RWTH Aachen's system for keyword search (KWS) with very limited amount of transcribed audio data available in the target language. This setting has become this year's primary condition within the Babel project [1], seeking to minimize the amount of human effort while retaining a reasonable KWS performance. Thus the highlights presented in this paper include graphemic acoustic modeling; multilingual features trained on language data from the previous project periods; comparison of tandem and hybrid DNN-HMM acoustic models; processing of large amounts of text data available on the web and the morphological KWS based on automatically derived word fragments.   The evaluation is performed using two training sets for each of the six current project period's languages \u2014 full language pack (FLP), consisting of 30 hours and very limited language pack (VLLP), comprising less than 3 hours of transcribed audio data. We put our focus on the latter of the two, which is clearly more challenging. The methods described in this work allowed us to exceed 0.3 MTWV on five out of six languages using development queries.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-316",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition \u2013 Signal Processing, Acoustic Modeling, Robustness, and Adaptation"
    },
    "wang15d_interspeech": {
      "authors": [
        [
          "Xiaoyun",
          "Wang"
        ],
        [
          "Seiichi",
          "Yamamoto"
        ]
      ],
      "title": "Second language speech recognition using multiple-pass decoding with lexicon represented by multiple reduced phoneme sets",
      "original": "i15_1265",
      "page_count": 5,
      "order": 361,
      "p1": "1265",
      "pn": "1269",
      "abstract": [
        "Considering that the pronunciation of second language speech is usually influenced by the mother tongue, we previously proposed using a reduced phoneme set for second language when the mother tongue of speakers is known. However, the proficiency of second language speakers varies widely, as does the influence of mother tongue on their pronunciation. Consequently, the optimal phoneme set is dependent on the proficiency of the second language speaker. In this work, we examine the relation between the proficiency of speakers and a reduced phoneme set customized for them. We propose a novel speech recognition method which is multiple-pass decoding using a lexicon represented by multiple reduced phoneme sets based on experimental results for speech recognition of second language speakers with various proficiencies. The relative error reduction obtained with the multiple reduced phoneme sets is 26.8% compared with the canonical one.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-317",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition \u2013 Signal Processing, Acoustic Modeling, Robustness, and Adaptation"
    },
    "juan15_interspeech": {
      "authors": [
        [
          "Sarah Samson",
          "Juan"
        ],
        [
          "Laurent",
          "Besacier"
        ],
        [
          "Benjamin",
          "Lecouteux"
        ],
        [
          "Mohamed",
          "Dyab"
        ]
      ],
      "title": "Using resources from a closely-related language to develop ASR for a very under-resourced language: a case study for iban",
      "original": "i15_1270",
      "page_count": 5,
      "order": 362,
      "p1": "1270",
      "pn": "1274",
      "abstract": [
        "This paper presents our strategies for developing an automatic speech recognition system for Iban, an under-resourced language. We faced several challenges such as no pronunciation dictionary and lack of training material for building acoustic models. To overcome these problems, we proposed approaches which exploit resources from a closely-related language (Malay). We developed a semi-supervised method for building the pronunciation dictionary and applied cross-lingual strategies for improving acoustic models trained with very limited training data. Both approaches displayed very encouraging results, which show that data from a closely-related language, if available, can be exploited to build ASR for a new language. In the final part of the paper, we present a zero-shot ASR using Malay resources that can be used as an alternative method for transcribing Iban speech.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-318",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition \u2013 Signal Processing, Acoustic Modeling, Robustness, and Adaptation"
    },
    "korenevsky15_interspeech": {
      "authors": [
        [
          "Maxim L.",
          "Korenevsky"
        ],
        [
          "Andrey B.",
          "Smirnov"
        ],
        [
          "Valentin S.",
          "Mendelev"
        ]
      ],
      "title": "Prediction of speech recognition accuracy for utterance classification",
      "original": "i15_1275",
      "page_count": 5,
      "order": 363,
      "p1": "1275",
      "pn": "1279",
      "abstract": [
        "The paper deals with the problem of predicting speech recognition quality and filtering poorly recognized utterances in the case when no reference transcripts are available. In the proposed system, word error rate (WER) predictions for individual utterances are made using conditional random fields (CRF), and classification based on a given threshold is performed afterwards. We propose using a boosting technique, which significantly increases recall for high precision values. We also apply Recurrent Neural Networks (RNN) directly to the utterance classification task and obtain comparable results but with a much simpler system. All experiments were carried out on Russian spontaneous conversational speech.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-319",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition \u2013 Signal Processing, Acoustic Modeling, Robustness, and Adaptation"
    },
    "beck15_interspeech": {
      "authors": [
        [
          "Eugen",
          "Beck"
        ],
        [
          "Ralf",
          "Schl\u00fcter"
        ],
        [
          "Hermann",
          "Ney"
        ]
      ],
      "title": "Error bounds for context reduction and feature omission",
      "original": "i15_1280",
      "page_count": 5,
      "order": 364,
      "p1": "1280",
      "pn": "1284",
      "abstract": [
        "In language processing applications like speech recognition, printed/handwritten character recognition, or statistical machine translation, the language model usually has a major influence on the performance, by introducing context. An increase of context length usually improves perplexity and increases the accuracy of a classifier using such a language model. In this work, the effect of context reduction, i.e. the accuracy difference between a context sensitive, and a context-insensitive classifier is considered. Context reduction is shown to be related to feature omission in the case of single symbol classification. Therefore, the simplest non-trivial case of feature omission will be analyzed by comparing a feature-aware classifier that uses an emission model to a prior-only classifier that statically infers the prior maximizing class only for all observations. Upper and lower tight bounds are presented for the accuracy difference of these model classifiers. The corresponding analytic proofs, though not presented here, were supported by an extensive simulation analysis of the problem, which gave empirical estimates of the accuracy difference bounds. Further, it is shown that the same bounds, though not tightly, also apply to the original case of context reduction. This result is supported by further simulation experiments for symbol string classification.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-320",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition \u2013 Signal Processing, Acoustic Modeling, Robustness, and Adaptation"
    },
    "itoh15_interspeech": {
      "authors": [
        [
          "Nobuyasu",
          "Itoh"
        ],
        [
          "Gakuto",
          "Kurata"
        ],
        [
          "Ryuki",
          "Tachibana"
        ],
        [
          "Masafumi",
          "Nishimura"
        ]
      ],
      "title": "A metric for evaluating speech recognizer output based on human-perception model",
      "original": "i15_1285",
      "page_count": 4,
      "order": 365,
      "p1": "1285",
      "pn": "1288",
      "abstract": [
        "Word error rate or character error rate are usually used as the metrics for evaluating the accuracy of speech recognition. These are naturally-defined objective metrics and are helpful for comparing recognition methods fairly. However the overall performance of the recognition systems and the usefulness of the results are not necessarily considered. To address this problem, we study and propose a metric which replicates human-annotated scores using their perception to the recognition results. The features that we use are the numbers of insertion errors, deletion errors, and substitution errors in the characters and the syllables. In addition we studied the numbers of consecutive errors, the misrecognized keywords, and the locations of errors. We created models using linear regression and random forest, predicted human-perceived scores, and compared them with the actual scores using Spearman's rank-based correlation. According to our experiments the correlation of human perceived scores with character error rates is 0.456, while those with the predicted scores by using a random forest of 10 features is 0.715. The latter is close to the averaged correlation between the scores of the human subjects, 0.765, which suggests that we can predict the human-perceived scores using those features and that we can leverage human perception model for evaluating speech recognition performance. The important factors (features) for the prediction are the numbers of substitution errors and consecutive errors.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-321",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition \u2013 Signal Processing, Acoustic Modeling, Robustness, and Adaptation"
    },
    "jannet15_interspeech": {
      "authors": [
        [
          "Mohamed Ameur Ben",
          "Jannet"
        ],
        [
          "Olivier",
          "Galibert"
        ],
        [
          "Martine",
          "Adda-Decker"
        ],
        [
          "Sophie",
          "Rosset"
        ]
      ],
      "title": "How to evaluate ASR output for named entity recognition?",
      "original": "i15_1289",
      "page_count": 5,
      "order": 366,
      "p1": "1289",
      "pn": "1293",
      "abstract": [
        "The standard metric to evaluate automatic speech recognition (ASR) systems is the word error rate (WER). WER has proven very useful in stand-alone ASR systems. Nowadays, these systems are often embedded in complex natural language processing systems to perform tasks like speech translation, man-machine dialogue, or information retrieval from speech. This exacerbates the need for the speech processing community to design a new evaluation metric to estimate the quality of automatic transcriptions within their larger applicative context.   We introduce a new measure to evaluate ASR in the context of named entity recognition, which makes use of a probabilistic model to estimate the risk of ASR errors inducing downstream errors in named entity detection. Our evaluation, on the ETAPE data, shows that ATENE achieves a higher correlation than WER between the performances in named entities recognition and in automatic speech transcription.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-322",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition \u2013 Signal Processing, Acoustic Modeling, Robustness, and Adaptation"
    },
    "mixdorff15b_interspeech": {
      "authors": [
        [
          "Hansj\u00f6rg",
          "Mixdorff"
        ],
        [
          "Angelika",
          "H\u00f6nemann"
        ],
        [
          "Albert",
          "Rilliard"
        ]
      ],
      "title": "Acoustic-prosodic analysis of attitudinal expressions in German",
      "original": "i15_1294",
      "page_count": 5,
      "order": 367,
      "p1": "1294",
      "pn": "1298",
      "abstract": [
        "This paper presents results from the prosodic analysis of short utterances of German produced with varying attitudinal expressions. It is based on the framework developed by Rilliard et al. eliciting 16 different kinds of social and/or propositional attitudes which place the subjects in various social interactions with a partner of inferior, equal or superior status, respectively as well as positive, neutral or negative, valence. Prosodic variations are analyzed in the framework of the Fujisaki model with respect to F0, as well as other prosodic features, such as duration, intensity and measures related to changes of voice quality. An analysis regarding the features that set apart two attitudes is presented. Expressive changes are discussed in light of previous results on US-English, and relative to universal codes proposed in the literature.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-323",
      "author_area_id": "3",
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "khaki15_interspeech": {
      "authors": [
        [
          "Hossein",
          "Khaki"
        ],
        [
          "Engin",
          "Erzin"
        ]
      ],
      "title": "Continuous emotion tracking using total variability space",
      "original": "i15_1299",
      "page_count": 5,
      "order": 368,
      "p1": "1299",
      "pn": "1303",
      "abstract": [
        "Automatic continuous emotion tracking (CET) has received increased attention with expected applications in medical, robotic, and human-machine interaction areas. The speech signal carries useful clues to estimate the affective state of the speaker. In this paper, we present Total Variability Space (TVS) for CET from speech data. TVS is a widely used framework in speaker and language recognition applications. In this study, we applied TVS as an unsupervised emotional feature extraction framework. Assuming a low temporal variation in the affective space, we discretize the continuous affective state and extract i-vectors. Experimental evaluations are performed on the CreativeIT dataset and fusion results with pool of statistical functions over mel frequency cepstral coefficients (MFCCs) show a 2% improvement for the emotion tracking from speech.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-324",
      "author_area_id": "3",
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "lee15e_interspeech": {
      "authors": [
        [
          "Chi-Chun",
          "Lee"
        ],
        [
          "Daniel",
          "Bone"
        ],
        [
          "Shrikanth S.",
          "Narayanan"
        ]
      ],
      "title": "An analysis of the relationship between signal-derived vocal arousal score and human emotion production and perception",
      "original": "i15_1304",
      "page_count": 5,
      "order": 369,
      "p1": "1304",
      "pn": "1308",
      "abstract": [
        "Bone et al. recently proposed an unsupervised signal-derived vocal arousal score (VC-AS) based on fusion of three intuitive acoustic features, i.e., pitch, intensity, and HF500, and have shown the effectiveness of quantifying human perceptual ratings of arousal robustly across multiple corpora. Due to the readily-applicable nature of the system, this objective quantification scheme could foresee-ably be used in multiple fields of behavioral science as an objective measure of affect. In this work, we investigate in detail the relationship of this signal-derived measure to both intended arousal expression (i.e., production aspect) and perceived arousal rating (i.e., perception aspect). On the perception side, our results on three databases (EMA, VAM, and IEMOCAP) indicate that VC-AS agrees with mean perception at least as well as an average individual rater does. Regarding production, we observe that intended arousal correlates more with VC-AS than mean perception (EMA and IEMOCAP), and that VC-AS correlates more with intended arousal than perceived arousal (EMA); these findings are surprising given that the framework is motivated by extensive affective perception studies, although there is physiological backing. Implications for the use of VC-AS for novel scientific study (e.g., to mitigate subjectivity) is further discussed.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-325",
      "author_area_id": "3",
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "mori15_interspeech": {
      "authors": [
        [
          "Hiroki",
          "Mori"
        ]
      ],
      "title": "Morphology of vocal affect bursts: exploring expressive interjections in Japanese conversation",
      "original": "i15_1309",
      "page_count": 5,
      "order": 370,
      "p1": "1309",
      "pn": "1313",
      "abstract": [
        "Expressive interjection (EI) is defined as non-lexical speech sound which indicates the speaker's cognitive/affective state changes. It is a type of vocal affect burst, i.e., brief and sudden nonverbal expressions that are produced spontaneously and unconsciously. Although EI as a social signal is assumed to play an important part in speech communication, very little is known about its linguistic, paralinguistic, and pragmatic nature. The goal of this study is to unveil the structure and functions of vocal affect bursts in human interactions. This paper focuses on the surface structure of EIs, which is an indispensable foundation for further analysis and modeling. Based on linguistic/acoustic analyses of a natural, spontaneous dialog corpus, the distinctiveness of EI was revealed as: (1) less variation in transcribed expression, (2) may have very short duration, and (3) higher F0 and intensity. In addition, it was revealed that an apparent correlation between formant frequencies and perceived paralinguistic information was observed only for EIs with the vowel /a/, which suggests that the vowel /a/ as an EI can accommodate richer paralinguistic information than other vowels.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-326",
      "author_area_id": "3",
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "mehrabani15_interspeech": {
      "authors": [
        [
          "Mahnoosh",
          "Mehrabani"
        ],
        [
          "Ozlem",
          "Kalinli"
        ],
        [
          "Ruxin",
          "Chen"
        ]
      ],
      "title": "Emotion clustering based on probabilistic linear discriminant analysis",
      "original": "i15_1314",
      "page_count": 5,
      "order": 371,
      "p1": "1314",
      "pn": "1318",
      "abstract": [
        "This study proposes an emotion clustering method based on Probabilistic Linear Discriminant Analysis (PLDA). Each emotional utterance is modeled as a GMM mean supervector. Hierarchical clustering is applied to cluster supervectors that represent similar emotions using a likelihood ratio from a PLDA model. The PLDA model can be trained with a different emotional database from the test data, with different emotion categories, speakers, or even languages. The advantage of using a PLDA model is that it identifies emotion dependent subspaces of the GMM mean supervector space. Our proposed emotion clustering based on PLDA likelihood distance improves 5-emotion clustering accuracy by 37.1% absolute compared to a baseline with Euclidean distance when PLDA model is trained with a separate set of speakers from the same database. Even when PLDA model is trained using a different database with a different language, clustering performance is improved by 11.2%.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-327",
      "author_area_id": "3",
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "albin15_interspeech": {
      "authors": [
        [
          "Aaron",
          "Albin"
        ],
        [
          "Elliot",
          "Moore"
        ]
      ],
      "title": "Objective study of the performance degradation in emotion recognition through the AMR-WB+ codec",
      "original": "i15_1319",
      "page_count": 5,
      "order": 372,
      "p1": "1319",
      "pn": "1323",
      "abstract": [
        "Research in speech emotion recognition often involves features that are extracted in lab settings or scenarios where speech quality is high. However, a great deal of communication occurs through speech codecs, which alters the speech signal and features extracted from it. The purpose of this study is to report on the performance degradation in emotion recognition systems when speech is passed through a codec and to provide insight on features that are affected in relation to their relevance in emotion classification. Using two emotional databases and the AMR-WB+ codec, features that are the most and least significantly affected by the codec are investigated and classifier performances are compared among them in multiple experiments. The results show that clean-trained classifiers drop significantly in accuracy on codec speech, and vice versa for codec-trained classifiers on clean speech in a full feature set task. However, using an intersection feature set between two databases that is resilient to the codec process can provide comparable performance for clean and codec-trained classifiers on either type of speech. The results suggest that these sets of features seem to capture more relevant information about emotion classes, since the perception of emotion should not be altered by a codec.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-328",
      "author_area_id": "3",
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "kadiri15_interspeech": {
      "authors": [
        [
          "Sudarsana Reddy",
          "Kadiri"
        ],
        [
          "P.",
          "Gangamohan"
        ],
        [
          "Suryakanth V.",
          "Gangashetty"
        ],
        [
          "B.",
          "Yegnanarayana"
        ]
      ],
      "title": "Analysis of excitation source features of speech for emotion recognition",
      "original": "i15_1324",
      "page_count": 5,
      "order": 373,
      "p1": "1324",
      "pn": "1328",
      "abstract": [
        "During production of emotional speech there are deviations in the components of speech production mechanism when compared to normal speech. The objective of this study is to capture the deviations in features related to the excitation source component of speech, and to develop a system for automatic recognition of emotions based on these deviations. The emotions considered for this study are: anger, happy, neutral and sad. The study shows that there are useful features in the deviations of the excitation source features at subsegmental level, and they can be exploited to develop an emotion recognition system. A hierarchical binary decision tree approach is used for classification.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-329",
      "author_area_id": "3",
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "huang15d_interspeech": {
      "authors": [
        [
          "Zhaocheng",
          "Huang"
        ],
        [
          "Julien",
          "Epps"
        ],
        [
          "Eliathamby",
          "Ambikairajah"
        ]
      ],
      "title": "An investigation of emotion change detection from speech",
      "original": "i15_1329",
      "page_count": 5,
      "order": 374,
      "p1": "1329",
      "pn": "1333",
      "abstract": [
        "Emotion recognition based on speech plays an important role in Human Computer Interaction (HCI), which has motivated extensive recent investigation into this area. However, current research on emotion recognition is focused on recognizing emotion on a per-file basis and mostly does not provide insight into emotion changes. In this paper, we report on an initial investigation into detecting the instant of emotion change using Gaussian Mixture Models (GMM) based methods, either without or with prior knowledge of emotions: the Generalized Likelihood Ratio and Emotion Pair Likelihood Ratios, together with a novel normalization scheme to improve emotion change detection accuracy. Experimental results based on the IEMOCAP corpus are presented that demonstrate a promising baseline. Despite the challenging nature of the problem, this work provides a path towards systems that detect and understand emotion changes, and also presents very interesting questions for further investigation.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-330",
      "author_area_id": "3",
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "gu15_interspeech": {
      "authors": [
        [
          "Wentao",
          "Gu"
        ],
        [
          "Ping",
          "Tang"
        ],
        [
          "Keikichi",
          "Hirose"
        ],
        [
          "V\u00e9ronique",
          "Auberg\u00e9"
        ]
      ],
      "title": "Crosslinguistic comparison on the perception of Mandarin attitudinal speech",
      "original": "i15_1334",
      "page_count": 5,
      "order": 375,
      "p1": "1334",
      "pn": "1338",
      "abstract": [
        "This work investigated crosslinguistic perception of Mandarin utterances conveying six classes of attitudes, i.e., dominant/submissive, friendly/hostile, polite/rude, serious/joking, praising/blaming, and sincere/insincere. Five groups of subjects were tested: native Mandarin speakers, Japanese L2 learners of Mandarin, French L2 learners of Mandarin, na\u00efve Japanese without Mandarin ability, and na\u00efve French without Mandarin ability. A set of Mandarin attitudinal utterances elicited in role-play dialogues were used as stimuli. Perceptual experiments showed that native subjects performed the best in identifying attitudes, and L2 learners judged better than na\u00efve foreigners. A complex interaction was found between attitude and the listener's L1/L2 experience which was also closely related to culture. Also, the correlations between perceptual patterns and prosodic features were examined, and the results suggested that the prosodic cues for certain attitudes might be dependent on the listener's language/culture background.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-331",
      "author_area_id": "3",
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "gosztolya15_interspeech": {
      "authors": [
        [
          "G\u00e1bor",
          "Gosztolya"
        ]
      ],
      "title": "Conflict intensity estimation from speech using Greedy forward-backward feature selection",
      "original": "i15_1339",
      "page_count": 5,
      "order": 376,
      "p1": "1339",
      "pn": "1343",
      "abstract": [
        "In the recent years extracting non-trivial information from audio sources has become possible. The resulting data has induced a new area in speech technology known as computational paralinguistics. A task in this area was presented at the ComParE 2013 Challenge (using the SSPNet Conflict Corpus), where the task was to determine the intensity of conflicts arising in speech recordings, based only on the audio information. Most authors approached this task by following standard paralinguistic practice, where we extract a huge number of potential features and perform the actual classification or regression process in the hope that the machine learning method applied is able to completely ignore irrelevant features. Although current state-of-the-art methods can indeed handle an overcomplete feature set, studies show that they can still be aided by feature selection. We opted for a simple greedy feature selection algorithm, by which we were able to outperform all previous scores on the SSPNet Conflict dataset, achieving a UAR score of 85.6%.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-332",
      "author_area_id": "3",
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "chong15_interspeech": {
      "authors": [
        [
          "Chee Seng",
          "Chong"
        ],
        [
          "Jeesun",
          "Kim"
        ],
        [
          "Chris",
          "Davis"
        ]
      ],
      "title": "Exploring acoustic differences between Cantonese (tonal) and English (non-tonal) spoken expressions of emotions",
      "original": "i15_1522",
      "page_count": 5,
      "order": 377,
      "p1": "1522",
      "pn": "1526",
      "abstract": [
        "It has been claimed that tone language speakers use less F0 related cues in the production of verbal expressions of emotions. This is because F0 is used in the production of lexical tones. This study investigated this claim by examining how F0 and various other acoustic parameters are used in the production of verbal emotion expressions in Cantonese (tone language) compared to English (non-tone language). Acoustic measurements (e.g., mean F0, F0 range) were extracted from the verbal expressions of five emotions (angry, happy, sad, surprise and disgust) and a neutral expression produced by five male native speakers of Cantonese and English. They were analyzed using K-means clustering to see how different acoustic properties are grouped and how this varies as a function of language. The results showed some difference between the two languages in how F0 related cues are used in the production of emotions. The results are discussed in terms of the general acoustic characteristics of spoken emotion expressions and in relation to behavioral data from perceptual studies.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-333",
      "author_area_id": "3",
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "palogiannidi15_interspeech": {
      "authors": [
        [
          "Elisavet",
          "Palogiannidi"
        ],
        [
          "Elias",
          "Iosif"
        ],
        [
          "Polychronis",
          "Koutsakis"
        ],
        [
          "Alexandros",
          "Potamianos"
        ]
      ],
      "title": "Valence, arousal and dominance estimation for English, German, Greek, Portuguese and Spanish lexica using semantic models",
      "original": "i15_1527",
      "page_count": 5,
      "order": 378,
      "p1": "1527",
      "pn": "1531",
      "abstract": [
        "We propose and evaluate the use of an affective-semantic model to expand the affective lexica of German, Greek, English, Spanish and Portuguese. Motivated by the assumption that semantic similarity implies affective similarity, we use word level semantic similarity scores as semantic features to estimate their corresponding affective scores. Various context-based semantic similarity metrics are investigated using contextual features that include both words and character n-grams. The model produces continuous affective ratings in three dimensions (valence, arousal and dominance) for all five languages, achieving consistent performance. We achieve classification accuracy (valence polarity task) between 85% and 91% for all five languages. For morphologically rich languages the proposed use of character n-grams is shown to improve performance.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-334"
    },
    "xu15b_interspeech": {
      "authors": [
        [
          "Xinzhou",
          "Xu"
        ],
        [
          "Jun",
          "Deng"
        ],
        [
          "Wenming",
          "Zheng"
        ],
        [
          "Li",
          "Zhao"
        ],
        [
          "Bj\u00f6rn",
          "Schuller"
        ]
      ],
      "title": "Dimensionality reduction for speech emotion features by multiscale kernels",
      "original": "i15_1532",
      "page_count": 5,
      "order": 379,
      "p1": "1532",
      "pn": "1536",
      "abstract": [
        "To achieve efficient and compact low-dimensional features for speech emotion recognition, this paper proposes a novel feature reduction method using multiscale kernels in the framework of graph embedding. With Fisher discriminant embedding graph, multiscale Gaussian kernels are used in constructing optimal linear combination of Gram matrices for multiple kernel learning. To evaluate the proposed method, comprehensive experiments, using different public feature sets from the open-source toolbox openSMILE on various corpora, show that the proposed method achieves better performance compared with conventional linear dimensionality reduction methods and single-kernel methods.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-335",
      "author_area_id": "3",
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "lee15f_interspeech": {
      "authors": [
        [
          "Jinkyu",
          "Lee"
        ],
        [
          "Ivan",
          "Tashev"
        ]
      ],
      "title": "High-level feature representation using recurrent neural network for speech emotion recognition",
      "original": "i15_1537",
      "page_count": 4,
      "order": 380,
      "p1": "1537",
      "pn": "1540",
      "abstract": [
        "This paper presents a speech emotion recognition system using a recurrent neural network (RNN) model trained by an efficient learning algorithm. The proposed system takes into account the long-range contextual effect and the uncertainty of emotional label expressions. To extract high-level representation of emotional states with regard to its temporal dynamics, a powerful learning method with a bidirectional long short-term memory (BLSTM) structure is adopted. To overcome the uncertainty of emotional labels, such that all frames in the same utterance are mapped to the same emotional label, it is assumed that the label of each frame is regarded as a sequence of random variables. The sequences are then trained by the proposed learning algorithm. The weighted accuracy of the proposed emotion recognition system is improved up to 12% compared to the DNN-ELM-based emotion recognition system used as a baseline.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-336",
      "author_area_id": "3",
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "kim15c_interspeech": {
      "authors": [
        [
          "Myung Jong",
          "Kim"
        ],
        [
          "Joohong",
          "Yoo"
        ],
        [
          "Younggwan",
          "Kim"
        ],
        [
          "Hoirin",
          "Kim"
        ]
      ],
      "title": "Speech emotion classification using tree-structured sparse logistic regression",
      "original": "i15_1541",
      "page_count": 5,
      "order": 381,
      "p1": "1541",
      "pn": "1545",
      "abstract": [
        "The extraction and selection of acoustic features are crucial steps in the development of a system for classifying emotions in speech. Most works in the field use some kind of prosodic features, often in combination with spectral and glottal features, and select appropriate features in classifying emotions. In the methods, feature choices are mostly made regardless of existing relationships and structures between features. However, considering them can be beneficial, potentially both for interpretability and to improve classification performance. To this end, a structured sparse logistic regression model incorporated with the hierarchical structure of features derived from prosody, spectral envelope, and glottal information is proposed in this paper. The proposed model simultaneously addresses tree-structured sparse feature selection and emotion classification. Evaluation of the proposed model on Berlin emotional database showed substantial improvement over the conventional sparse logistic regression model.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-337",
      "author_area_id": "3",
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "vlasenko15_interspeech": {
      "authors": [
        [
          "Bogdan",
          "Vlasenko"
        ],
        [
          "Andreas",
          "Wendemuth"
        ]
      ],
      "title": "Annotators' agreement and spontaneous emotion classification performance",
      "original": "i15_1546",
      "page_count": 5,
      "order": 382,
      "p1": "1546",
      "pn": "1550",
      "abstract": [
        "The combination of various types of data can significantly increase the amount of emotional material for training of more reliable real-life emotion classifiers. There are two well-known schemes of annotation utilized for emotional speech: multi-dimensional and categories-based. Multi-dimensional annotation is usually applied for labeling spontaneous emotional events, and categorial-based annotation is used for specification of the acted \u201cfull blown\u201d emotional chunks. In order to simulate real-life conditions we used a cross-corpora evaluation strategy for datasets with different schemes of emotional annotation. Emotional models were trained on acted material from the EMO-DB (categories based annotation) dataset and evaluated on spontaneous data from the VAM dataset (multi-dimensional annotation). The best emotion classification performance was obtained on real-life emotional instances with the most intense arousal labels provided by a majority voting strategy (out of 17 annotators). We find that the corresponding spontaneous speech samples containing the most intensive emotional content are comparable with acted instances. The importance of employing a larger number of emotional annotators was finally addressed in our article.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-338",
      "author_area_id": "3",
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "arisoy15_interspeech": {
      "authors": [
        [
          "Ebru",
          "Arisoy"
        ],
        [
          "Murat",
          "Sara\u00e7lar"
        ]
      ],
      "title": "Multi-stream long short-term memory neural network language model",
      "original": "i15_1413",
      "page_count": 5,
      "order": 383,
      "p1": "1413",
      "pn": "1417",
      "abstract": [
        "Long Short-Term Memory (LSTM) neural networks are recurrent neural networks that contain memory units that can store contextual information from past inputs for arbitrary amounts of time. A typical LSTM neural network language model is trained by feeding an input sequence. i.e., a stream of words, to the input layer of the network and the output layer predicts the probability of the next word given the past inputs in the sequence. In this paper we introduce a multi-stream LSTM neural network language model where multiple asynchronous input sequences are fed to the network as parallel streams while predicting the output word sequence. For our experiments, we use a sub-word sequence in addition to a word sequence as the input streams, which allows joint training of the LSTM neural network language model using both information sources.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-339",
      "author_area_id": "9",
      "author_area_label": "Speech Recognition \u2013 Architecture, Search, and Linguistic Components"
    },
    "hall15_interspeech": {
      "authors": [
        [
          "Keith",
          "Hall"
        ],
        [
          "Eunjoon",
          "Cho"
        ],
        [
          "Cyril",
          "Allauzen"
        ],
        [
          "Fran\u00e7oise",
          "Beaufays"
        ],
        [
          "Noah",
          "Coccaro"
        ],
        [
          "Kaisuke",
          "Nakajima"
        ],
        [
          "Michael",
          "Riley"
        ],
        [
          "Brian",
          "Roark"
        ],
        [
          "David",
          "Rybach"
        ],
        [
          "Linda",
          "Zhang"
        ]
      ],
      "title": "Composition-based on-the-fly rescoring for salient n-gram biasing",
      "original": "i15_1418",
      "page_count": 5,
      "order": 384,
      "p1": "1418",
      "pn": "1422",
      "abstract": [
        "We introduce a technique for dynamically applying contextually-derived language models to a state-of-the-art speech recognition system. These generally small-footprint models can be seen as a generalization of cache-based models [1], whereby contextually salient n-grams are derived from relevant sources (not just user generated language) to produce a model intended for combination with the baseline language model. The derived models are applied during first-pass decoding as a form of on-the-fly composition between the decoder search graph and the set of weighted contextual n-grams. We present a construction algorithm which takes a trie representing the contextual n-grams and produces a weighted finite state automaton which is more compact than a standard n-gram machine. Finally, we present a set of empirical results on the recognition of spoken search queries where a contextual model encoding recent trending queries is applied using the proposed technique.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-340",
      "author_area_id": "9",
      "author_area_label": "Speech Recognition \u2013 Architecture, Search, and Linguistic Components"
    },
    "marin15_interspeech": {
      "authors": [
        [
          "Alex",
          "Marin"
        ],
        [
          "Mari",
          "Ostendorf"
        ],
        [
          "Ji",
          "He"
        ]
      ],
      "title": "Learning phrase patterns for ASR name error detection using semantic similarity",
      "original": "i15_1423",
      "page_count": 5,
      "order": 385,
      "p1": "1423",
      "pn": "1427",
      "abstract": [
        "This paper addresses the problem of detecting name errors in automatic speech recognition (ASR) output, when available ASR output training data is sparse and covers only a limited subset of scenarios from a broad-domain task. We reduce the dimensionality of the lexical feature set using a related task (sentence-level name detection) and improve feature coverage by expanding single words to classes using semantic similarity measures derived from both WordNet and neural network word embeddings. Phrase patterns are learned over the selected word features using a frequency-based pattern selection algorithm. In experiments on English dialogs, we find that adding sentence-level features performs better than using local n-gram context. Automatically-learned seed features perform better than handcrafted patterns, and their class expansions generalize better. Finally, the embedding-based class expansions yield better results than the corresponding WordNet-based configurations.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-341",
      "author_area_id": "9",
      "author_area_label": "Speech Recognition \u2013 Architecture, Search, and Linguistic Components"
    },
    "shazeer15_interspeech": {
      "authors": [
        [
          "Noam",
          "Shazeer"
        ],
        [
          "Joris",
          "Pelemans"
        ],
        [
          "Ciprian",
          "Chelba"
        ]
      ],
      "title": "Sparse non-negative matrix language modeling for skip-grams",
      "original": "i15_1428",
      "page_count": 5,
      "order": 386,
      "p1": "1428",
      "pn": "1432",
      "abstract": [
        "We present a novel family of language model (LM) estimation techniques named Sparse Non-negative Matrix (SNM) estimation.   A first set of experiments empirically evaluating these techniques on the One Billion Word Benchmark [3] shows that with skip-gram features SNMLMs are able to match the state-of-the-art recurrent neural network (RNN) LMs; combining the two modeling techniques yields the best known result on the benchmark.   The computational advantages of SNM over both maximum entropy and RNNLM estimation are probably its main strength, promising an approach that has the same flexibility in combining arbitrary features effectively and yet should scale to very large amounts of data as gracefully as n-gram LMs do.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-342",
      "author_area_id": "9",
      "author_area_label": "Speech Recognition \u2013 Architecture, Search, and Linguistic Components"
    },
    "pelemans15_interspeech": {
      "authors": [
        [
          "Joris",
          "Pelemans"
        ],
        [
          "Noam",
          "Shazeer"
        ],
        [
          "Ciprian",
          "Chelba"
        ]
      ],
      "title": "Pruning sparse non-negative matrix n-gram language models",
      "original": "i15_1433",
      "page_count": 5,
      "order": 387,
      "p1": "1433",
      "pn": "1437",
      "abstract": [
        "In this paper we present a pruning algorithm and experimental results for our recently proposed Sparse Non-negative Matrix (SNM) family of language models (LMs). We show that when trained with only n-gram features SNMLM pruning based on a mutual information criterion yields the best known pruned model on the One Billion Word Language Model Benchmark, reducing perplexity with 18% and 57% over Katz and Kneser-Ney LMs, respectively. We also present a method for converting an SNMLM to ARPA back-off format which can be readily used in a single-pass decoder for Automatic Speech Recognition.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-343",
      "author_area_id": "9",
      "author_area_label": "Speech Recognition \u2013 Architecture, Search, and Linguistic Components"
    },
    "chelba15_interspeech": {
      "authors": [
        [
          "Ciprian",
          "Chelba"
        ],
        [
          "Xuedong",
          "Zhang"
        ],
        [
          "Keith",
          "Hall"
        ]
      ],
      "title": "Geo-location for voice search language modeling",
      "original": "i15_1438",
      "page_count": 5,
      "order": 388,
      "p1": "1438",
      "pn": "1442",
      "abstract": [
        "We investigate the benefit of augmenting with geo-location information the language model used in speech recognition for voice-search.   We observe reductions in perplexity of up to 15% relative on test sets obtained from both typed query data, as well as transcribed voice search data; on a subset of the test data consisting of \u201clocal\u201d queries \u2014 search results displaying a restaurant, some address, or similar \u2014 the reduction in perplexity is even higher, up to 30% relative.   Automatic speech recognition experiments confirm the utility of geo-location information for improved language modeling. Significant reductions in word error rate are observed both on general voice search traffic, as well as \u201clocal\u201d traffic, up to 2% and 8% relative, respectively.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-344",
      "author_area_id": "9",
      "author_area_label": "Speech Recognition \u2013 Architecture, Search, and Linguistic Components"
    },
    "botros15_interspeech": {
      "authors": [
        [
          "Rami",
          "Botros"
        ],
        [
          "Kazuki",
          "Irie"
        ],
        [
          "Martin",
          "Sundermeyer"
        ],
        [
          "Hermann",
          "Ney"
        ]
      ],
      "title": "On efficient training of word classes and their application to recurrent neural network language models",
      "original": "i15_1443",
      "page_count": 5,
      "order": 389,
      "p1": "1443",
      "pn": "1447",
      "abstract": [
        "In this paper, we investigated various word clustering methods, by studying two clustering algorithms: Brown clustering and exchange algorithm, and three objective functions derived from different class-based language models (CBLM): two-sided, predictive and conditional models. In particular, we focused on the implementation of the exchange algorithm with improved speed. In total, we compared six clustering methods in terms of runtime and perplexity (PP) of the CBLM on a French corpus, and show that our accelerated implementation of exchange algorithm is up to 114 times faster than the original and around 6 times faster than the best implementation of Brown clustering we could find, while performing about the same (slightly better) in PP. In addition, we conducted a keyword search experiment on the Babel Lithuanian task (IARPA-babel304b-v1.0b), which showed that CBLM improves the word error rate (WER) but not the keyword search performance. Furthermore, we used these clustering techniques for the output layer of a recurrent neural network (RNN) language model (LM) and we show that in terms of PP of the RNN LM, word classes trained under the predictive model perform slightly better than those trained under other criteria we considered.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-345",
      "author_area_id": "9",
      "author_area_label": "Speech Recognition \u2013 Architecture, Search, and Linguistic Components"
    },
    "bayer15_interspeech": {
      "authors": [
        [
          "Ali Orkan",
          "Bayer"
        ],
        [
          "Giuseppe",
          "Riccardi"
        ]
      ],
      "title": "Deep semantic encodings for language modeling",
      "original": "i15_1448",
      "page_count": 5,
      "order": 390,
      "p1": "1448",
      "pn": "1452",
      "abstract": [
        "Word error rate (WER) is not an appropriate metric for spoken language systems (SLS) because lower WER does not necessarily yield better understanding performance. Therefore, language models (LMs) that are used in SLS should be trained to jointly optimize transcription and understanding performance. Semantic LMs (SELMs) are based on the theory of frame semantics and incorporate features of frames and meaning bearing words ( target words) as semantic context when training LMs. The performance of SELMs is affected by the errors on the ASR and the semantic parser output. In this paper we address the problem of coping with such noise in the training phase of the neural network-based architecture of LMs. We propose the use of deep autoencoders for the encoding of semantic context while accounting for ASR errors. We investigate the optimization of SELMs both for transcription and understanding by using deep semantic encodings. Deep semantic encodings suppress the noise introduced by the ASR module, and enable SELMs to be optimized adequately. We assess the understanding performance by measuring the errors made on target words and we achieve 3.7% relative improvement over recurrent neural network LMs.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-346",
      "author_area_id": "9",
      "author_area_label": "Speech Recognition \u2013 Architecture, Search, and Linguistic Components"
    },
    "sun15_interspeech": {
      "authors": [
        [
          "Ming",
          "Sun"
        ],
        [
          "Yun-Nung",
          "Chen"
        ],
        [
          "Alexander I.",
          "Rudnicky"
        ]
      ],
      "title": "Learning OOV through semantic relatedness in spoken dialog systems",
      "original": "i15_1453",
      "page_count": 5,
      "order": 391,
      "p1": "1453",
      "pn": "1457",
      "abstract": [
        "Ensuring language coverage in dialog systems can be a challenge, since the language in a domain may drift over time, creating a mismatch between the original training data and current input. This in turn degrades performance by increasing misunderstanding and eventually leading to task failure. Without the capability of adapting the vocabulary and the language model based on certain domains or users, recognition errors may degrade the understanding performance, and even lead to a task failure, which incurs more time and effort to recover. This paper investigates how coverage can be maintained by automatically acquiring potential out-of-vocabulary (OOV) words by leveraging different types of relatedness between vocabulary items and words retrieved from web-based resources. Our experiments show that both recognition and semantic parsing accuracy can thereby be improved.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-347",
      "author_area_id": "9",
      "author_area_label": "Speech Recognition \u2013 Architecture, Search, and Linguistic Components"
    },
    "chong15b_interspeech": {
      "authors": [
        [
          "Tze Yuang",
          "Chong"
        ],
        [
          "Rafael E.",
          "Banchs"
        ],
        [
          "Eng Siong",
          "Chng"
        ],
        [
          "Haizhou",
          "Li"
        ]
      ],
      "title": "TDTO language modeling with feedforward neural networks",
      "original": "i15_1458",
      "page_count": 5,
      "order": 392,
      "p1": "1458",
      "pn": "1462",
      "abstract": [
        "In this paper, we describe the use of feedforward neural networks to improve the term-distance term-occurrence (TDTO) language model, previously proposed in [1]-[3]. The main idea behind the TDTO model proposition is to model separately both position and occurrence information of words in the history-context to better estimate n-gram probabilities. Neural networks have been shown to offer a better generalization property than other conventional smoothing methods. We take advantage of such property for a better smoothing mechanism for the TDTO model, referred to as the continuous space TDTO (cTDTO). The newly proposed model has reported an improved perplexity over the baseline TDTO model of up to 9.2%, at history length of ten, as evaluated on the Wall Street Journal (WSJ) corpus. Also, in the Aurora-4 speech recognition N-best re-ranking task, the cTDTO outperformed the TDTO model by reducing the word error rate (WER) up to 12.9% relatively.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-348",
      "author_area_id": "9",
      "author_area_label": "Speech Recognition \u2013 Architecture, Search, and Linguistic Components"
    },
    "paulik15_interspeech": {
      "authors": [
        [
          "Matthias",
          "Paulik"
        ]
      ],
      "title": "Improvements to the pruning behavior of DNN acoustic models",
      "original": "i15_1463",
      "page_count": 5,
      "order": 393,
      "p1": "1463",
      "pn": "1467",
      "abstract": [
        "This paper examines two strategies that improve the beam pruning behavior of DNN acoustic models with only a negligible increase in model complexity. By augmenting the boosted MMI loss function used in sequence training with the weighted cross-entropy error, we achieve a real time factor (RTF) reduction of more than 13%. By directly incorporating a transition model into the DNN, which leads to a parameter size increase of less than 0.017%, we achieve a RTF reduction of 16%. Combining both techniques results in a RTF reduction of more than 23%. Both strategies, and their combination, also lead to small but statistically significant word error rate reductions.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-349",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition \u2013 Signal Processing, Acoustic Modeling, Robustness, and Adaptation"
    },
    "sak15_interspeech": {
      "authors": [
        [
          "Ha\u015fim",
          "Sak"
        ],
        [
          "Andrew",
          "Senior"
        ],
        [
          "Kanishka",
          "Rao"
        ],
        [
          "Fran\u00e7oise",
          "Beaufays"
        ]
      ],
      "title": "Fast and accurate recurrent neural network acoustic models for speech recognition",
      "original": "i15_1468",
      "page_count": 5,
      "order": 394,
      "p1": "1468",
      "pn": "1472",
      "abstract": [
        "We have recently shown that deep Long Short-Term Memory (LSTM) recurrent neural networks (RNNs) outperform feed forward deep neural networks (DNNs) as acoustic models for speech recognition. More recently, we have shown that the performance of sequence trained context dependent (CD) hidden Markov model (HMM) acoustic models using such LSTM RNNs can be equaled by sequence trained phone models initialized with connectionist temporal classification (CTC). In this paper, we present techniques that further improve performance of LSTM RNN acoustic models for large vocabulary speech recognition. We show that frame stacking and reduced frame rate lead to more accurate models and faster decoding. CD phone modeling leads to further improvements. We also present initial results for LSTM RNN models outputting words directly.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-350",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition \u2013 Signal Processing, Acoustic Modeling, Robustness, and Adaptation"
    },
    "nakkiran15_interspeech": {
      "authors": [
        [
          "Preetum",
          "Nakkiran"
        ],
        [
          "Raziel",
          "Alvarez"
        ],
        [
          "Rohit",
          "Prabhavalkar"
        ],
        [
          "Carolina",
          "Parada"
        ]
      ],
      "title": "Compressing deep neural networks using a rank-constrained topology",
      "original": "i15_1473",
      "page_count": 5,
      "order": 395,
      "p1": "1473",
      "pn": "1477",
      "abstract": [
        "We present a general approach to reduce the size of feedforward deep neural networks (DNNs). We propose a rank-constrained topology, which factors the weights in the input layer of the DNN in terms of a low-rank representation: unlike previous work, our technique is applied at the level of the filters learned at individual hidden layer nodes, and exploits the natural two-dimensional time-frequency structure in the input. These techniques are applied on a small-footprint DNN-based keyword spotting task, where we find that we can reduce model size by 75% relative to the baseline, without any loss in performance. Furthermore, we find that the proposed approach is more effective at improving model performance compared to other popular dimensionality reduction techniques, when evaluated with a comparable number of parameters.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-351",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition \u2013 Signal Processing, Acoustic Modeling, Robustness, and Adaptation"
    },
    "sainath15b_interspeech": {
      "authors": [
        [
          "Tara N.",
          "Sainath"
        ],
        [
          "Carolina",
          "Parada"
        ]
      ],
      "title": "Convolutional neural networks for small-footprint keyword spotting",
      "original": "i15_1478",
      "page_count": 5,
      "order": 396,
      "p1": "1478",
      "pn": "1482",
      "abstract": [
        "We explore using Convolutional Neural Networks (CNNs) for a small-footprint keyword spotting (KWS) task. CNNs are attractive for KWS since they have been shown to outperform DNNs with far fewer parameters. We consider two different applications in our work, one where we limit the number of multiplications of the KWS system, and another where we limit the number of parameters. We present new CNN architectures to address the constraints of each applications. We find that the CNN architectures offer between a 27-44% relative improvement in false reject rate compared to a DNN, while fitting into the constraints of each application.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-352",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition \u2013 Signal Processing, Acoustic Modeling, Robustness, and Adaptation"
    },
    "berg15_interspeech": {
      "authors": [
        [
          "Ewout van den",
          "Berg"
        ],
        [
          "Daniel",
          "Brand"
        ],
        [
          "Rajesh",
          "Bordawekar"
        ],
        [
          "Leonid",
          "Rachevsky"
        ],
        [
          "Bhuvana",
          "Ramabhadran"
        ]
      ],
      "title": "Efficient GPU implementation of convolutional neural networks for speech recognition",
      "original": "i15_1483",
      "page_count": 5,
      "order": 397,
      "p1": "1483",
      "pn": "1487",
      "abstract": [
        "Deep learning has enjoyed tremendous success in speech recognition in recent years. Despite their widespread use, training large and complex architectures remains very time consuming. A prime example of this are convolutional neural networks (CNNs), which have provided state-of-the-art results, but are also among the most computationally intensive networks to train. In this paper, we study four different methods for GPU acceleration of CNNs: a native implementation using cuBLAS, two implementations based on NVIDIA's recently released deep-learning cuDNN library, and an implementation based on cuFFT. We analyze the performance of each of these approaches on the forward operation, the gradient computation, and the backward propagation. The overall best performance is obtained using the custom native implementation, which was found to be up to 6.9 times faster than cuDNN. The paper concludes with results on the end-to-end training speed of our CNN network on an LVCSR task.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-353",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition \u2013 Signal Processing, Acoustic Modeling, Robustness, and Adaptation"
    },
    "strom15_interspeech": {
      "authors": [
        [
          "Nikko",
          "Strom"
        ]
      ],
      "title": "Scalable distributed DNN training using commodity GPU cloud computing",
      "original": "i15_1488",
      "page_count": 5,
      "order": 398,
      "p1": "1488",
      "pn": "1492",
      "abstract": [
        "We introduce a new method for scaling up distributed Stochastic Gradient Descent (SGD) training of Deep Neural Networks (DNN). The method solves the well-known communication bottleneck problem that arises for data-parallel SGD because compute nodes frequently need to synchronize a replica of the model. We solve it by purposefully controlling the rate of weight-update per individual weight, which is in contrast to the uniform update-rate customarily imposed by the size of a mini-batch. It is shown empirically that the method can reduce the amount of communication by three orders of magnitude while training a typical DNN for acoustic modelling. This reduction in communication bandwidth enables efficient scaling to more parallel GPU nodes than any other method that we are aware of, and it can be achieved with neither loss in convergence rate nor accuracy in the resulting DNN. Furthermore, the training can be performed on commodity cloud infrastructure and networking.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-354",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition \u2013 Signal Processing, Acoustic Modeling, Robustness, and Adaptation"
    },
    "kalkur15_interspeech": {
      "authors": [
        [
          "Sachin N.",
          "Kalkur"
        ],
        [
          "Sandeep",
          "Reddy C."
        ],
        [
          "Rajesh M.",
          "Hegde"
        ]
      ],
      "title": "Joint source localization and separation in spherical harmonic domain using a sparsity based method",
      "original": "i15_1493",
      "page_count": 5,
      "order": 399,
      "p1": "1493",
      "pn": "1497",
      "abstract": [
        "In this paper, we address the problem of source localization and separation using sparse methods over a spherical microphone array. A sparsity based method is developed from the observed data in spherical harmonic domain. A solution to the sparse model formulated herein is obtained by imposing orthonormal constraint on the sparsity matrix. Subsequently, a splitting method based on bregman iteration is used to jointly localize and separate the sources from the mixtures of sources. A joint estimate of location and the separated sources is finally obtained after fixed number of iterations. Experiments on source localization and separation are conducted at different SNRs on the grid database. Experimental results based on RMSE analysis and objective evaluation indicate a reasonable performance improvement when compared to other methods in literature.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-355",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "zhang15b_interspeech": {
      "authors": [
        [
          "Shaofei",
          "Zhang"
        ],
        [
          "Dong-Yan",
          "Huang"
        ],
        [
          "Lei",
          "Xie"
        ],
        [
          "Eng Siong",
          "Chng"
        ],
        [
          "Haizhou",
          "Li"
        ],
        [
          "Minghui",
          "Dong"
        ]
      ],
      "title": "Regularized non-negative matrix factorization using alternating direction method of multipliers and its application to source separation",
      "original": "i15_1498",
      "page_count": 5,
      "order": 400,
      "p1": "1498",
      "pn": "1502",
      "abstract": [
        "Non-negative matrix factorization (NMF) aims at finding non-negative representations of nonnegative data. Among different NMF algorithms, alternating direction method of multipliers (ADMM) is a popular one with superior performance. However, we find that ADMM shows instability and inferior performance on real-world data like speech signals. In this paper, to solve this problem, we develop a class of advanced regularized ADMM algorithms for NMF. Efficient and robust learning rules are achieved by incorporating l1-norm and the Frobenius norm regularization. The prior information of Laplacian distribution of data is used to solve the problem with a unique solution. We evaluate this class of ADMM algorithms using both synthetic and real speech signals for a source separation task at different cost functions, i.e., Euclidean distance (EUD), Kullback-Leibler (KL) divergence and Itakura-Saito (IS) divergence. Results demonstrate that the proposed algorithms converge faster and yield more stable and accurate results than the original ADMM algorithm.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-356",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "nie15_interspeech": {
      "authors": [
        [
          "Shuai",
          "Nie"
        ],
        [
          "Shan",
          "Liang"
        ],
        [
          "Wei",
          "Xue"
        ],
        [
          "Xueliang",
          "Zhang"
        ],
        [
          "Wenju",
          "Liu"
        ],
        [
          "Like",
          "Dong"
        ],
        [
          "Hong",
          "Yang"
        ]
      ],
      "title": "Two-stage multi-target joint learning for monaural speech separation",
      "original": "i15_1503",
      "page_count": 5,
      "order": 401,
      "p1": "1503",
      "pn": "1507",
      "abstract": [
        "Recently, supervised speech separation has been extensively studied and shown considerable promise. Due to the temporal continuity of speech, speech auditory features and separation targets present prominent spectro-temporal structures and strong correlations over the time-frequency (T-F) domain, which can be exploited for speech separation. However, many supervised speech separation methods independently model each T-F unit with only one target and much ignore these useful information. In this paper, we propose a two-stage multi-target joint learning method to jointly model the related speech separation targets at the frame level. Systematic experiments show that the proposed approach consistently achieves better separation and generalization performances in the low signal-to-noise ratio (SNR) conditions.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-357",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "xu15c_interspeech": {
      "authors": [
        [
          "Yong",
          "Xu"
        ],
        [
          "Jun",
          "Du"
        ],
        [
          "Zhen",
          "Huang"
        ],
        [
          "Li-Rong",
          "Dai"
        ],
        [
          "Chin-Hui",
          "Lee"
        ]
      ],
      "title": "Multi-objective learning and mask-based post-processing for deep neural network based speech enhancement",
      "original": "i15_1508",
      "page_count": 5,
      "order": 402,
      "p1": "1508",
      "pn": "1512",
      "abstract": [
        "We propose a multi-objective framework to learn both secondary targets not directly related to the intended task of speech enhancement (SE) and the primary target of the clean log-power spectra (LPS) features to be used directly for constructing the enhanced speech signals. In deep neural network (DNN) based SE we introduce an auxiliary structure to learn secondary continuous features, such as mel-frequency cepstral coefficients (MFCCs), and categorical information, such as the ideal binary mask (IBM), and integrate it into the original DNN architecture for joint optimization of all the parameters. This joint estimation scheme imposes additional constraints not available in the direct prediction of LPS, and potentially improves the learning of the primary target. Furthermore, the learned secondary information as a byproduct can be used for other purposes, e.g., the IBM-based post-processing in this work. A series of experiments show that joint LPS and MFCC learning improves the SE performance, and IBM-based post-processing further enhances listening quality of the reconstructed speech.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-358",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "kwon15_interspeech": {
      "authors": [
        [
          "Kisoo",
          "Kwon"
        ],
        [
          "Jong Won",
          "Shin"
        ],
        [
          "Hyung Yong",
          "Kim"
        ],
        [
          "Nam Soo",
          "Kim"
        ]
      ],
      "title": "Discriminative nonnegative matrix factorization using cross-reconstruction error for source separation",
      "original": "i15_1513",
      "page_count": 4,
      "order": 403,
      "p1": "1513",
      "pn": "1516",
      "abstract": [
        "Non-negative matrix factorization (NMF) is a dimensionality reduction method that usually leads to a part-based representation, and it is shown to be effective for source separation. However, the performance of the source separation degrades when one signal can be described with the bases for the other source signals. In this paper, we propose a discriminative NMF (DNMF) algorithm which exploits the reconstruction error for the other signals as well as the target signal based on target bases. The objective function to train the basis matrix is constructed to reward high reconstruction error for the other source signals in addition to low reconstruction error for the signal from the corresponding source. Experiments showed that the proposed method outperformed the standard NMF by about 0.26 in perceptual evaluation of speech quality score and 1.95 dB in signal-to-distortion ratio when it is applied to speech enhancement at input SNR of 0 dB.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-359",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "khan15_interspeech": {
      "authors": [
        [
          "Faheem",
          "Khan"
        ],
        [
          "Ben",
          "Milner"
        ]
      ],
      "title": "Using audio and visual information for single channel speaker separation",
      "original": "i15_1517",
      "page_count": 5,
      "order": 404,
      "p1": "1517",
      "pn": "1521",
      "abstract": [
        "This work proposes a method to exploit both audio and visual speech information to extract a target speaker from a mixture of competing speakers. The work begins by taking an effective audio-only method of speaker separation, namely the soft mask method, and modifying its operation to allow visual speech information to improve the separation process. The audio input is taken from a single channel and includes the mixture of speakers, and a separate set of visual features is extracted from each speaker. This allows modification of the separation process to include not only the audio speech but also visual speech from each speaker in the mixture. Experimental results are presented that compare the proposed audio-visual speaker separation with audio-only and visual-only methods using both speech quality and speech intelligibility metrics.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-360",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "hoge15_interspeech": {
      "authors": [
        [
          "Harald",
          "H\u00f6ge"
        ]
      ],
      "title": "On the nature of the features generated in the human auditory pathway for phone recognition",
      "original": "i15_1551",
      "page_count": 5,
      "order": 405,
      "p1": "1551",
      "pn": "1555",
      "abstract": [
        "The features used by human phone recognition are generated along the auditory pathway by several transformations. In the first stages `modulation features' are generated in lamina of neurons building a 3 dimensional strongly quantized structure, where each point of the structure corresponds to a feature component. One dimension concerns the different critical bands originated by bundles of inner hair cells. The second dimensions correspond to different locations of the acoustic field around the head. The third dimension is the modulation depth for different spectral and temporal modulation frequencies for each critical band and location. This structure is repeated in the auditory cortex, where a transformation to `phone features' occurs. Due to insufficient neurophysiologic knowledge of these features, we conclude indirectly on their nature based on measurements of the accuracy of perceiving phones. We conclude that the phone features are statistic independent across adjacent phones. This implies that no acoustic context of neighbored phones is used to perceive phones. From these findings we speculate, that a phone oriented segment model is implemented in the auditory cortex. This model has the potential to model correctly the statistic dependencies of all phone features constituting an utterance.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-361",
      "author_area_id": "1",
      "author_area_label": "Speech Perception, Production, and Acquisition"
    },
    "yamamoto15b_interspeech": {
      "authors": [
        [
          "Kodai",
          "Yamamoto"
        ],
        [
          "Toshio",
          "Irino"
        ],
        [
          "Ryuichi",
          "Nisimura"
        ],
        [
          "Hideki",
          "Kawahara"
        ],
        [
          "Roy D.",
          "Patterson"
        ]
      ],
      "title": "How the slope of the speech spectrum affects the perception of speaker size",
      "original": "i15_1556",
      "page_count": 5,
      "order": 406,
      "p1": "1556",
      "pn": "1560",
      "abstract": [
        "We performed a behavioral experiment to demonstrate the effect of spectral slope on the perception of speaker size, and we developed an auditory model based on the dynamic compressive gammachirp filterbank (dcGC-FB) to explain the results. STRAIGHT was used to generate \u201cunvoiced\u201d and \u201cwhispered\u201d versions of naturally recorded words; the only difference was that the spectral slope of the whispered words was tilted up 6 dB/octave with respect to that of the unvoiced words. The experiment confirmed that the whispered words are heard to come from smaller speakers. The auditory model uses the tonotopic excitation pattern, Ep, as the internal representation of speech sounds. The model is found to be much more effective when the gradient of the excitation pattern, \u25bd Ep, is included in the size discrimination process. It is particularly useful for explaining individual subject variability.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-362",
      "author_area_id": "1",
      "author_area_label": "Speech Perception, Production, and Acquisition"
    },
    "rasilo15_interspeech": {
      "authors": [
        [
          "Heikki",
          "Rasilo"
        ],
        [
          "Okko",
          "R\u00e4s\u00e4nen"
        ]
      ],
      "title": "Weakly-supervised word learning is improved by an active online algorithm",
      "original": "i15_1561",
      "page_count": 5,
      "order": 407,
      "p1": "1561",
      "pn": "1565",
      "abstract": [
        "When infants learn words, they do not generally occur in isolation but as parts of continuous utterances and with several possible related meanings. Details of the efficient learning techniques of infants remain unknown. In this paper we introduce a dynamic concept matrix (DCM) algorithm that learns acoustic models for a set of keywords from given pairs of continuous utterances and related keyword labels. DCM is an incremental, active online algorithm. Specifically, each training utterance is first recognized with the current word models, and the recognition result is used to guide training further. In low-noise conditions DCM shows significant improvement in convergence rate and final recognition scores to an earlier passive CM model on TIDIGITS and CAREGIVER UK Y2 datasets. The results suggest that in ambiguous learning situations it may be beneficial for the learner to observe the learning situation, make hard decisions if some known words/objects were recognized and update the models based on the decisions.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-363",
      "author_area_id": "1",
      "author_area_label": "Speech Perception, Production, and Acquisition"
    },
    "lin15b_interspeech": {
      "authors": [
        [
          "Lin",
          "Lin"
        ],
        [
          "Jon",
          "Barker"
        ],
        [
          "Guy J.",
          "Brown"
        ]
      ],
      "title": "The effect of cochlear implant processing on speaker intelligibility: a perceptual study and computer model",
      "original": "i15_1566",
      "page_count": 5,
      "order": 408,
      "p1": "1566",
      "pn": "1570",
      "abstract": [
        "Cochlear implant (CI) users have great difficulty understanding speech in noise. However, some speakers are found to be more intelligible than others. This paper tests whether a glimpse-based model, that was previously used to successfully explain speaker intelligibility in normal-hearing listeners, can also predict speaker intelligibility for CI users. The model employs a front-end that mimics the effects of energetic masking. This is coupled with a back-end that employs statistical models of CI-processed speech to recognise the unmasked glimpses of the target signal. Listening tests were conducted using signals that simulate the effect of hearing speech mixed with speech-shaped noise through a CI, at signal-to-noise ratios ranging from -4 to 6 dB. The intelligibility of 34 different talkers was measured at each noise level. The model is able to explain the variation in the speaker intelligibilities: the correlation between listener and model intelligibilities varies between 0.87 and 0.91 depending on noise level. This is higher than correlations previously found for normal hearing listeners. Our results have the potential to inform future CI signal processing strategies.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-364",
      "author_area_id": "1",
      "author_area_label": "Speech Perception, Production, and Acquisition"
    },
    "cao15_interspeech": {
      "authors": [
        [
          "Mengxue",
          "Cao"
        ],
        [
          "Aijun",
          "Li"
        ],
        [
          "Qiang",
          "Fang"
        ],
        [
          "Bernd J.",
          "Kr\u00f6ger"
        ]
      ],
      "title": "Phonetic-phonological feature emerges by associating phonetic with semantic information \u2014 a GSOM-based modeling study",
      "original": "i15_1571",
      "page_count": 5,
      "order": 409,
      "p1": "1571",
      "pn": "1575",
      "abstract": [
        "Born with an innate neural architecture built specially for language learning, young children have the ability to distinguish sounds in a variety of languages. As they are exposed to native language environment, perceptual reorganization occurs, and native language system gradually establishes. Phonology knowledge, which is language-specific, emerges during this process. In this study, based on the further developed Interconnected Growing Self-Organizing Maps (I-GSOM) model, we present a series of computational modeling experiments which simulate the early language acquisition process for young children. A universal model that has the ability to distinguish phonemes in different languages (two in our case) is built based on English and Standard Chinese data. The native language learning is simulated based on Standard Chinese data with both phonetic and semantic inputs for children between 12 and 18 months old. Experiment results show that the conceptual based top-down process contributes to the reorganization of phonetic knowledge, and phonetic-semantic association helps the emergence of phonetic-phonological knowledge. It can be hypothesized from these findings that the phonetic-phonological interface does not appear as a clean cut within the speech processing system but as a broader zone located between sensorimotor and semantic processing.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-365",
      "author_area_id": "1",
      "author_area_label": "Speech Perception, Production, and Acquisition"
    },
    "bosch15_interspeech": {
      "authors": [
        [
          "L. ten",
          "Bosch"
        ],
        [
          "L.",
          "Boves"
        ],
        [
          "B.",
          "Tucker"
        ],
        [
          "M.",
          "Ernestus"
        ]
      ],
      "title": "DIANA: towards computational modeling reaction times in lexical decision in north American English",
      "original": "i15_1576",
      "page_count": 5,
      "order": 410,
      "p1": "1576",
      "pn": "1580",
      "abstract": [
        "DIANA is an end-to-end computational model of speech processing, which takes as input the speech signal, and provides as output the orthographic transcription of the stimulus, a word/non-word judgment and the associated estimated reaction time. So far, the model has only been tested for Dutch.   In this paper, we extend DIANA such that it can also process North American English. The model is tested by having it simulate human participants in a large scale North American English lexical decision experiment. The simulations show that DIANA can adequately approximate the reaction times of an average participant (r = 0.45). In addition, they indicate that DIANA does not yet adequately model the cognitive processes that take place after stimulus offset.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-366",
      "author_area_id": "1",
      "author_area_label": "Speech Perception, Production, and Acquisition"
    },
    "chen15i_interspeech": {
      "authors": [
        [
          "Qian",
          "Chen"
        ],
        [
          "Zhen-Hua",
          "Ling"
        ],
        [
          "Chen-Yu",
          "Yang"
        ],
        [
          "Li-Rong",
          "Dai"
        ]
      ],
      "title": "Automatic phrase boundary labeling of speech synthesis database using context-dependent HMMs and n-gram prior distributions",
      "original": "i15_1581",
      "page_count": 5,
      "order": 411,
      "p1": "1581",
      "pn": "1585",
      "abstract": [
        "This paper presents an automatic phrase boundary labeling method for speech synthesis database annotation using context-dependent hidden Markov models (CD-HMMs) and n-gram prior distributions. At training stage, CD-HMMs are built to describe the conditional distribution of acoustic features given phonetic label and phrase boundary. In addition, n-gram models are estimated to represent the prior distributions of the phrase boundaries to be predicted. At decoding stage, the CD-HMMs and n-gram models are combined to predict the phrase boundaries by Viterbi decoding under maximum a posteriori (MAP) criterion. In our experiments, the proposed method utilizing context-dependent bigram prior distributions improved the F-score of phrase boundary labeling from 72.2% to 79.6% on the Boston University Radio News Corpus (BURNC), and from 69.6% to 81.0% on the Blizzard Challenge 2007 database respectively, comparing with the method using only acoustic models.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-367",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "ribeiro15b_interspeech": {
      "authors": [
        [
          "Manuel Sam",
          "Ribeiro"
        ],
        [
          "Junichi",
          "Yamagishi"
        ],
        [
          "Robert A. J.",
          "Clark"
        ]
      ],
      "title": "A perceptual investigation of wavelet-based decomposition of f0 for text-to-speech synthesis",
      "original": "i15_1586",
      "page_count": 5,
      "order": 412,
      "p1": "1586",
      "pn": "1590",
      "abstract": [
        "The Continuous Wavelet Transform (CWT) has been recently proposed to model F0 in the context of speech synthesis. It was shown that systems using signal decomposition with the CWT tend to outperform systems that model the signal directly. The F0 signal is typically decomposed into various scales of differing frequency. In these experiments, we reconstruct F0 with selected frequencies and ask native listeners to judge the naturalness of synthesized utterances with respect to natural speech. Results indicate that HMM-generated F0 is comparable to the CWT low frequencies, suggesting it mostly generates utterances with neutral intonation. Middle frequencies achieve very high levels of naturalness, while very high frequencies are mostly noise.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-368",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "moungsri15_interspeech": {
      "authors": [
        [
          "Decha",
          "Moungsri"
        ],
        [
          "Tomoki",
          "Koriyama"
        ],
        [
          "Takao",
          "Kobayashi"
        ]
      ],
      "title": "Duration prediction using multi-level model for GPR-based speech synthesis",
      "original": "i15_1591",
      "page_count": 5,
      "order": 413,
      "p1": "1591",
      "pn": "1595",
      "abstract": [
        "This paper introduces frame-based Gaussian process regression (GPR) into phone/syllable duration modeling for Thai speech synthesis. The GPR model is designed for predicting frame-level acoustic features using corresponding frame information, which includes relative position in each unit of utterance structure and linguistic information such as tone type and part of speech. Although the GPR-based prediction can be applied to a phone duration model, the use of phone duration model only is not always sufficient to generate natural sounding speech. Specifically, in some languages including Thai, syllable durations affect the perception of sentence structure. In this paper, we propose a duration prediction technique using a multi-level model which includes syllable and phone levels for prediction. In the technique, first, syllable durations are predicted, and then they are used as additional contexts in phone-level model to generate phone duration for synthesizing. Objective and subjective evaluation results show that GPR-based modeling with multi-level model for duration prediction outperforms the conventional HMM-based speech synthesis.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-369",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "langarani15_interspeech": {
      "authors": [
        [
          "Mahsa Sadat Elyasi",
          "Langarani"
        ],
        [
          "Jan van",
          "Santen"
        ],
        [
          "Seyed Hamidreza",
          "Mohammadi"
        ],
        [
          "Alexander",
          "Kain"
        ]
      ],
      "title": "Data-driven foot-based intonation generator for text-to-speech synthesis",
      "original": "i15_1596",
      "page_count": 5,
      "order": 414,
      "p1": "1596",
      "pn": "1600",
      "abstract": [
        "We propose a method for generating F0 contours for text-to-speech synthesis. Training speech is automatically annotated in terms of feet, with features indicating start and end times of syllables, foot position, and foot length. During training, we fit a foot-based superpositional intonation model comprising accent curves and phrase curves. During synthesis, the method searches for stored, fitted accent curves associated with feet that optimally match to-be-synthesized feet in the feature space, while minimizing differences between successive accent curve heights. We tested the proposed method against the HMM-based Speech Synthesis System (HTS) by imposing contours generated by these two methods onto natural speech, and obtaining quality ratings. Test sets varied in how well they were covered by the training data. Contours generated by the proposed method were preferred over HTS-generated contours, especially for poorly-covered test items. To test the new method's usefulness for processing marked-up text input, we compared its ability to convey contrastive stress with that of natural speech recordings, and found no difference. We conclude that the new method holds promise for generating comparatively high-quality F0 contours, especially when training data are sparse and when mark-up is required.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-370",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "gerazov15_interspeech": {
      "authors": [
        [
          "Branislav",
          "Gerazov"
        ],
        [
          "Pierre-Edouard",
          "Honnet"
        ],
        [
          "Aleksandar",
          "Gjoreski"
        ],
        [
          "Philip N.",
          "Garner"
        ]
      ],
      "title": "Weighted correlation based atom decomposition intonation modelling",
      "original": "i15_1601",
      "page_count": 5,
      "order": 415,
      "p1": "1601",
      "pn": "1605",
      "abstract": [
        "Intonation modelling is an integral part of text-to-speech systems from their very beginnings. This has led to the proliferation of various intonation models, each with its own relative strengths and weaknesses. Only a few of these intonation models are based on physiology, despite the advantage that such models are language independent. We propose a new intonation model inspired by the physiology of intonation production, which is based on decomposing the F0 contour into elementary atoms. The model, named the Weighted Correlation Atom Decomposition model (WCAD), is a generalisation of the command response (CR) model and has the advantage of having a simple parameter extraction method. The decomposition process follows a matching pursuit approach based on using the perceptually relevant weighted correlation as a cost function. The results have affirmed the plausibility of using the WCAD model to model F0 contours across different languages and speakers. The results have also shown that the WCAD model has good comparative performance to the CR model, giving it practical importance.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-371",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "fernandez15_interspeech": {
      "authors": [
        [
          "Raul",
          "Fernandez"
        ],
        [
          "Asaf",
          "Rendel"
        ],
        [
          "Bhuvana",
          "Ramabhadran"
        ],
        [
          "Ron",
          "Hoory"
        ]
      ],
      "title": "Using deep bidirectional recurrent neural networks for prosodic-target prediction in a unit-selection text-to-speech system",
      "original": "i15_1606",
      "page_count": 5,
      "order": 416,
      "p1": "1606",
      "pn": "1610",
      "abstract": [
        "Deeply-stacked Bidirectional Recurrent Neural Networks (BiRNNs) are able to capture complex, short- and long-term, context dependencies between predictors and targets due to the non-linear dependency they introduce on the entire observation when predicting a target, thanks to the use of recurrent hidden layers that accumulate information from all preceding and future observations. This aspect of the model makes them desirable for tasks such as the prediction of prosodic contours for text-to-speech systems, where the surface prosody can be a result of the interaction between local and non-local features. Although previous work has demonstrated that they attain state-of-the-art performance for this task within a parametric synthesis framework, their use within unit-selection synthesis systems remains unexplored. In this work we deploy this class of models within a unit selection system, investigate their effect on the outcome of the unit search, and perceptually evaluate it against the baseline (decision-tree-based) approach.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-372",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "liao15_interspeech": {
      "authors": [
        [
          "Hank",
          "Liao"
        ],
        [
          "Golan",
          "Pundak"
        ],
        [
          "Olivier",
          "Siohan"
        ],
        [
          "Melissa K.",
          "Carroll"
        ],
        [
          "Noah",
          "Coccaro"
        ],
        [
          "Qi-Ming",
          "Jiang"
        ],
        [
          "Tara N.",
          "Sainath"
        ],
        [
          "Andrew",
          "Senior"
        ],
        [
          "Fran\u00e7oise",
          "Beaufays"
        ],
        [
          "Michiel",
          "Bacchiani"
        ]
      ],
      "title": "Large vocabulary automatic speech recognition for children",
      "original": "i15_1611",
      "page_count": 5,
      "order": 417,
      "p1": "1611",
      "pn": "1615",
      "abstract": [
        "Recently, Google launched YouTube Kids, a mobile application for children, that uses a speech recognizer built specifically for recognizing children's speech. In this paper we present techniques we explored to build such a system. We describe the use of a neural network classifier to identify matched acoustic training data, filtering data for language modeling to reduce the chance of producing offensive results. We also compare long short-term memory (LSTM) recurrent networks to convolutional, LSTM, deep neural networks (CLDNN). We found that a CLDNN acoustic model outperforms an LSTM across a variety of different conditions, but does not specifically model child speech relatively better than adult. Overall, these findings allow us to build a successful, state-of-the-art large vocabulary speech recognizer for both children and adults.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-373",
      "author_area_id": "11",
      "author_area_label": "Spoken Language Processing \u2013 Dialogue, Summarization, Understanding"
    },
    "bone15_interspeech": {
      "authors": [
        [
          "Daniel",
          "Bone"
        ],
        [
          "Matthew P.",
          "Black"
        ],
        [
          "Anil",
          "Ramakrishna"
        ],
        [
          "Ruth",
          "Grossman"
        ],
        [
          "Shrikanth S.",
          "Narayanan"
        ]
      ],
      "title": "Acoustic-prosodic correlates of `awkward' prosody in story retellings from adolescents with autism",
      "original": "i15_1616",
      "page_count": 5,
      "order": 418,
      "p1": "1616",
      "pn": "1620",
      "abstract": [
        "Atypical speech prosody is a primary characteristic of autism spectrum disorders (ASD), yet it is often excluded from diagnostic instrument algorithms due to poor subjective reliability. Robust, objective prosodic cues can enhance our understanding of those aspects which are atypical in autism. In this work, we connect objective signal-derived descriptors of prosody to subjective perceptions of prosodic awkwardness. Subjectively, more awkward speech is less expressive (more monotone) and more often has perceived awkward rate/rhythm, volume, and intonation. We also find expressivity can be quantified through objective intonation variability features, and that speaking rate and rhythm cues are highly predictive of perceived awkwardness. Acoustic-prosodic features are also able to significantly differentiate subjects with ASD from typically developing (TD) subjects in a classification task, emphasizing the potential of automated methods for diagnostic efficiency and clarity.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-374",
      "author_area_id": "11",
      "author_area_label": "Spoken Language Processing \u2013 Dialogue, Summarization, Understanding"
    },
    "fringi15_interspeech": {
      "authors": [
        [
          "Eva",
          "Fringi"
        ],
        [
          "Jill Fain",
          "Lehman"
        ],
        [
          "Martin",
          "Russell"
        ]
      ],
      "title": "Evidence of phonological processes in automatic recognition of children's speech",
      "original": "i15_1621",
      "page_count": 4,
      "order": 419,
      "p1": "1621",
      "pn": "1624",
      "abstract": [
        "Automatic speech recognition (ASR) for children's speech is more difficult than for adults' speech. A plausible explanation is that ASR errors are due to predictable phonological effects associated with language acquisition. We describe phone recognition experiments on hand labelled data for children aged between 5 and 9. A comparison of the resulting confusion matrices with those for adult speech (TIMIT) shows increased phone substitution rates for children, which correspond to some extent to established phonological phenomena. However these errors still only account for a relatively small proportion of the issue. This suggests that attempts to improve ASR accuracy on children's speech by accommodating these phenomena, for example by changing the pronunciation dictionary, cannot solve the whole problem.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-375",
      "author_area_id": "11",
      "author_area_label": "Spoken Language Processing \u2013 Dialogue, Summarization, Understanding"
    },
    "pucher15_interspeech": {
      "authors": [
        [
          "Michael",
          "Pucher"
        ],
        [
          "Markus",
          "Toman"
        ],
        [
          "Dietmar",
          "Schabus"
        ],
        [
          "Cassia",
          "Valentini-Botinhao"
        ],
        [
          "Junichi",
          "Yamagishi"
        ],
        [
          "Bettina",
          "Zillinger"
        ],
        [
          "Erich",
          "Schmid"
        ]
      ],
      "title": "Influence of speaker familiarity on blind and visually impaired children's perception of synthetic voices in audio games",
      "original": "i15_1625",
      "page_count": 5,
      "order": 420,
      "p1": "1625",
      "pn": "1629",
      "abstract": [
        "In this paper we evaluate how speaker familiarity influences the engagement times and performance of blind school children when playing audio games made with different synthetic voices. We developed synthetic voices of school children, their teachers and of speakers that were unfamiliar to them and used each of these voices to create variants of two audio games: a memory game and a labyrinth game. Results show that pupils had significantly longer engagement times and better performance when playing games that used synthetic voices built with their own voices. This result was observed even though the children reported not recognising the synthetic voice as their own after the experiment was over. These findings could be used to improve the design of audio games and lecture books for blind and visually impaired children.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-376",
      "author_area_id": "11",
      "author_area_label": "Spoken Language Processing \u2013 Dialogue, Summarization, Understanding"
    },
    "shahnawazuddin15_interspeech": {
      "authors": [
        [
          "S.",
          "Shahnawazuddin"
        ],
        [
          "Rohit",
          "Sinha"
        ]
      ],
      "title": "Low-memory fast on-line adaptation for acoustically mismatched children's speech recognition",
      "original": "i15_1630",
      "page_count": 5,
      "order": 421,
      "p1": "1630",
      "pn": "1634",
      "abstract": [
        "This work focuses on the issues and the challenges in acoustic adaptation in context of on-line children's speech recognition. When children's speech is decoded on adults' speech trained acoustic models, severely degraded recognition performance is noted on account of extreme acoustic mismatch. Though a number of conventional adaptation techniques are available, they are found to be undesirably latent for an on-line task. For addressing the same, in this work we have combined two low complexity fast adaptation techniques, namely acoustic model interpolation and low-rank feature projection. Two schemes for doing the same are presented in this work. In the first approach, model interpolation is done using weights estimated in unconstrained fashion. The other approach is a hybrid one in which a set mean supervectors are pre-estimated using suitable developmental data. Those are then optimally scaled using the given test data. Though the unconstrained approach results in better improvements over baseline, it has a higher complexity and memory requirements. In case of the hybrid approach, for interpolating M models, the number of parameters to be estimated and memory requirements are reduced by a factor of (M - 1).\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-377",
      "author_area_id": "11",
      "author_area_label": "Spoken Language Processing \u2013 Dialogue, Summarization, Understanding"
    },
    "giuliani15_interspeech": {
      "authors": [
        [
          "Diego",
          "Giuliani"
        ],
        [
          "Bagher",
          "BabaAli"
        ]
      ],
      "title": "Large vocabulary children's speech recognition with DNN-HMM and SGMM acoustic modeling",
      "original": "i15_1635",
      "page_count": 5,
      "order": 422,
      "p1": "1635",
      "pn": "1639",
      "abstract": [
        "In this paper, large vocabulary children's speech recognition is investigated by using the Deep Neural Network - Hidden Markov Model (DNN-HMM) hybrid and the Subspace Gaussian Mixture Model (SGMM) acoustic modeling approach. In the investigated scenario training data is limited to about 7 hours of speech from children in the age range 7-13 and testing data consists in read clean speech from children in the same age range. To tackle inter-speaker acoustic variability, speaker adaptive training, based on feature space maximum likelihood linear regression, as well as vocal tract length normalization are adopted. Experimental results show that with both DNN-HMM and SGMM systems very good recognition results can be achieved although best results are obtained with the DNN-HMM system.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-378",
      "author_area_id": "11",
      "author_area_label": "Spoken Language Processing \u2013 Dialogue, Summarization, Understanding"
    },
    "govender15_interspeech": {
      "authors": [
        [
          "Avashna",
          "Govender"
        ],
        [
          "Febe de",
          "Wet"
        ],
        [
          "Jules-Raymond",
          "Tapamo"
        ]
      ],
      "title": "HMM adaptation for child speech synthesis",
      "original": "i15_1640",
      "page_count": 5,
      "order": 423,
      "p1": "1640",
      "pn": "1644",
      "abstract": [
        "Hidden Markov Model (HMM)-based synthesis in combination with speaker adaptation has proven to be an approach that is well-suited for child speech synthesis [1]. This paper describes the development and evaluation of different HMM-based child speech synthesis systems. The aim is to determine the most suitable combination of initial model and speaker adaptation techniques to synthesize child speech. The results of the study indicate that gender-independent initial models perform better than gender-dependent initial models and Constrained Structural Maximum a Posteriori Linear Regression (CSMAPLR) followed by maximum a posteriori (MAP) is the speaker adaptation technique combination that yields the most natural and intelligible synthesized child speech.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-379",
      "author_area_id": "11",
      "author_area_label": "Spoken Language Processing \u2013 Dialogue, Summarization, Understanding"
    },
    "kim15d_interspeech": {
      "authors": [
        [
          "Jaebok",
          "Kim"
        ],
        [
          "Khiet P.",
          "Truong"
        ],
        [
          "Vicky",
          "Charisi"
        ],
        [
          "Cristina",
          "Zaga"
        ],
        [
          "Manja",
          "Lohse"
        ],
        [
          "Dirk",
          "Heylen"
        ],
        [
          "Vanessa",
          "Evers"
        ]
      ],
      "title": "Vocal turn-taking patterns in groups of children performing collaborative tasks: an exploratory study",
      "original": "i15_1645",
      "page_count": 5,
      "order": 424,
      "p1": "1645",
      "pn": "1649",
      "abstract": [
        "Since children (5-9 years old) are still developing their emotional and social skills, their social interactional behaviors in small groups might differ from adults' interactional behaviors. In order to develop a robot that is able to support children performing collaborative tasks in small groups, it is necessary to gain a better understanding of how children interact with each other. We were interested in investigating vocal turn-taking patterns as we expect these to reveal relations to collaborative and conflict behaviors, especially with children behaviors as previous literature suggests. To that end, we collected an audiovisual corpus of children performing collaborative tasks together in groups of three. Through automatic turn-taking analyses, our results showed that speaker changes with overlaps are more common than without overlaps and children seemed to show smoother turn-taking patterns, i.e., less frequent and longer lasting speaker changes, during collaborative than conflict behaviors.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-380",
      "author_area_id": "11",
      "author_area_label": "Spoken Language Processing \u2013 Dialogue, Summarization, Understanding"
    },
    "sadeghian15_interspeech": {
      "authors": [
        [
          "Roozbeh",
          "Sadeghian"
        ],
        [
          "Stephen A.",
          "Zahorian"
        ]
      ],
      "title": "Towards an automated screening tool for pediatric speech delay",
      "original": "i15_1650",
      "page_count": 5,
      "order": 425,
      "p1": "1650",
      "pn": "1654",
      "abstract": [
        "Speech delay is a childhood language problem that sometimes is resolved on its own but sometimes may cause more serious language difficulties later. This leads therapists to screen children for detection at early ages in order to eliminate future problems. Using the Goldman-Fristoe Test of Articulation (GFTA) method, therapists listen to a child's pronunciation of certain phonemes and phoneme pairs in specified words and judge the child's stage of speech development. The goal of this paper is to develop an Automatic Speech Recognition (ASR) tool and related speech processing methods which emulate the knowledge of speech therapists. In this paper two methods of feature extraction (MFCC and DCTC) were used as the baseline for training an HMM-based utterance verification system which was later used for testing the utterances of 63 young children (ages 4-10), both typically developed and speech delayed. The ASR results show the value of augmenting static spectral information with spectral trajectory information for better prediction of therapist's judgments.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-381",
      "author_area_id": "11",
      "author_area_label": "Spoken Language Processing \u2013 Dialogue, Summarization, Understanding"
    },
    "proenca15_interspeech": {
      "authors": [
        [
          "Jorge",
          "Proen\u00e7a"
        ],
        [
          "Dirce",
          "Celorico"
        ],
        [
          "Sara",
          "Candeias"
        ],
        [
          "Carla",
          "Lopes"
        ],
        [
          "Fernando",
          "Perdig\u00e3o"
        ]
      ],
      "title": "Children's reading aloud performance: a database and automatic detection of disfluencies",
      "original": "i15_1655",
      "page_count": 5,
      "order": 426,
      "p1": "1655",
      "pn": "1659",
      "abstract": [
        "The automatic evaluation of children's reading performance by detecting and analyzing errors and disfluencies in speech is an important tool to build automatic reading tutors and to complement the current method of manual evaluations of overall reading ability in schools. A large amount of speech from children reading aloud plentiful in errors and disfluencies is needed to train acoustic, disfluency and pronunciation models for an automatic reading assessment system. This paper describes the acquisition and analysis of a read-aloud speech database of European Portuguese from children aged 6-10 from the first to fourth school grades. Towards the goal of detecting all reading errors and disfluencies, we apply a decoding process to the utterances using flexible word level lattices that allow syllable based false starts and repetitions of two or more word sequences. The proposed method proved promising in detecting corrections and repetitions in sentences, and provides an improved alignment of the data, helpful for future annotation tasks. The analysis of the database also shows agreement to government defined curricular goals for reading.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-382",
      "author_area_id": "11",
      "author_area_label": "Spoken Language Processing \u2013 Dialogue, Summarization, Understanding"
    },
    "sundar15_interspeech": {
      "authors": [
        [
          "Harshavardhan",
          "Sundar"
        ],
        [
          "Jill Fain",
          "Lehman"
        ],
        [
          "Rita",
          "Singh"
        ]
      ],
      "title": "Keyword spotting in multi-player voice driven games for children",
      "original": "i15_1660",
      "page_count": 5,
      "order": 427,
      "p1": "1660",
      "pn": "1664",
      "abstract": [
        "Word spotting, or keyword identification, is a highly challenging task when there are multiple speakers speaking simultaneously. In the case of a game being controlled by children solely through voice, the task becomes extremely difficult. Children, unlike adults, typically do not await their turn to speak in an orderly fashion. They interrupt and shout at arbitrary times, speak or say things that are not within the purview of the game vocabulary, arbitrarily stretch, contract, distort or rapid-repeat words, and do not stay in one location either horizontally or vertically. Consequently, standard state-of-art keyword spotting systems that work admirably for adults in multiple keyword settings, fail to perform well even in a basic two-word vocabulary keyword spotting task in the case of children. This paper highlights the issues with keyword spotting using a simple two-word game played by children of different age groups, and gives quantitative performance assessments using a novel keyword spotting technique that is especially suited to such scenarios.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-383",
      "author_area_id": "11",
      "author_area_label": "Spoken Language Processing \u2013 Dialogue, Summarization, Understanding"
    },
    "guo15_interspeech": {
      "authors": [
        [
          "Jinxi",
          "Guo"
        ],
        [
          "Rohit",
          "Paturi"
        ],
        [
          "Gary",
          "Yeung"
        ],
        [
          "Steven M.",
          "Lulich"
        ],
        [
          "Harish",
          "Arsikere"
        ],
        [
          "Abeer",
          "Alwan"
        ]
      ],
      "title": "Age-dependent height estimation and speaker normalization for children's speech using the first three subglottal resonances",
      "original": "i15_1665",
      "page_count": 5,
      "order": 428,
      "p1": "1665",
      "pn": "1669",
      "abstract": [
        "This paper proposes an age-dependent scheme for automatic height estimation and speaker normalization of children's speech, using the first three subglottal resonances (SGRs). Similar to previous work, our analysis indicates that children above the age of 11 years show different acoustic properties from those under 11. Therefore, an age-dependent model is investigated. The estimation algorithms for the first three SGRs are motivated by our previous research for adults. The algorithms for the first two SGRs have been applied to children's speech before. This paper proposes a similar approach to estimate Sg3 for children. The algorithm is trained and evaluated on 46 children, aged between 6-17 years, using cross-validation. Average RMS errors in estimating Sg1, Sg2 and Sg3 using the age-dependent model are 51, 128 and 168 Hz, respectively. The height estimation algorithm employs a negative correlation between SGRs and height, and the mean absolute height estimation error was found to be less than 3.8cm for the younger children and 4.9cm for the older children. In addition, using TIDIGITS, a linear frequency warping scheme using age-dependent Sg3 gives statistically-significant word error rate reductions (up to 26%) relative to conventional VTLN.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-384",
      "author_area_id": "11",
      "author_area_label": "Spoken Language Processing \u2013 Dialogue, Summarization, Understanding"
    },
    "leemann15_interspeech": {
      "authors": [
        [
          "Adrian",
          "Leemann"
        ],
        [
          "Camilla",
          "Bernardasci"
        ],
        [
          "Francis",
          "Nolan"
        ]
      ],
      "title": "The effect of speakers' regional varieties on listeners' decision-making",
      "original": "i15_1670",
      "page_count": 5,
      "order": 429,
      "p1": "1670",
      "pn": "1674",
      "abstract": [
        "It has been widely reported that speech provides cues to a speaker's regional background. Little is known about how such cues influence human behavior, however. In the present study we used a matched-guise design to test how speakers' regional accents affect listeners' decision-making. In three scenarios, 72 subjects from three regions in Switzerland were asked to choose either the Standard German, Bern, or Zurich German speaker when asked to select a secretary, surgeon, or travel companion. Results revealed that preferences differed depending on the scenario. We further report two results that have not been described before: (1) the Standard accent was least preferred in all scenarios; (2) in-group favoritism seems to apply only partially to the Swiss context: the Zurich variety was the most preferred variety for all listener groups. We discuss implications from the point of view of accent prestige and social identity theory.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-385",
      "author_area_id": "2",
      "author_area_label": "Phonetics, Phonology, and Prosody"
    },
    "fuchs15_interspeech": {
      "authors": [
        [
          "Robert",
          "Fuchs"
        ]
      ],
      "title": "Word-initial glottal stop insertion, hiatus resolution and linking in British English",
      "original": "i15_1675",
      "page_count": 5,
      "order": 430,
      "p1": "1675",
      "pn": "1679",
      "abstract": [
        "Glottal stop insertion at word boundaries is usually not included in descriptions of the phonology of Southern Standard British English (BrE), and little is known about the factors that determine it. This paper investigates the insertion of glottal stops before vowels at word boundaries (e.g. pronounced as [ta\u028an\u0294\u026az]) in the spontaneous and read speech of 10 speakers of BrE.   Results show that glottal stops are inserted in certain contexts in up to 50% of all cases before vowel-initial words at word boundaries. Glottal stop insertion is least frequent after obstruents (11.8%), followed by sonorants (19.0%) and vowels (29.0%). Preceding high vowels and following low vowels also make glottal stop insertion more likely. In addition, glottal stop insertion is slightly more frequent in read than in spontaneous speech, which suggests that it is a feature of the standard rather than a colloquial feature of BrE.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-386",
      "author_area_id": "2",
      "author_area_label": "Phonetics, Phonology, and Prosody"
    },
    "li15b_interspeech": {
      "authors": [
        [
          "Shanpeng",
          "Li"
        ],
        [
          "Wentao",
          "Gu"
        ]
      ],
      "title": "Acoustic analysis of Mandarin affricates",
      "original": "i15_1680",
      "page_count": 5,
      "order": 431,
      "p1": "1680",
      "pn": "1684",
      "abstract": [
        "This work investigated the relationship between acoustic parameters and phonetic features for six Mandarin affricates. The acoustic parameters under study included normalized duration, normalized amplitude, spectral energy distribution, and F2 onset of the following vowel, while the phonetic features under study included place of articulation, state of aspiration, and identity of the following vowel. Repeated-measures ANOVAs showed that all six affricates were distinguishable from each other in terms of central of gravity or dispersion of spectrum. Discriminant analysis showed that a combination of all nine acoustic parameters resulted in 89.5% rate of recognition for six affricates. Factor analysis showed that the first five components contributed to 86.3% of the information for affricates. In conclusion, spectral energy distribution constitutes the major acoustic parameters for Mandarin affricates, contributing mainly to the place of articulation, while normalized duration/amplitude of frication contributes mainly to the state of aspiration.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-387",
      "author_area_id": "2",
      "author_area_label": "Phonetics, Phonology, and Prosody"
    },
    "leykum15_interspeech": {
      "authors": [
        [
          "Hannah",
          "Leykum"
        ],
        [
          "Sylvia",
          "Moosm\u00fcller"
        ],
        [
          "Wolfgang U.",
          "Dressler"
        ]
      ],
      "title": "Homophonous phonotactic and morphonotactic consonant clusters in word-final position",
      "original": "i15_1685",
      "page_count": 5,
      "order": 432,
      "p1": "1685",
      "pn": "1689",
      "abstract": [
        "Morphonotactic clusters are defined as combinations of phonemes across morpheme boundaries. As they are cognitively processed faster and acquired earlier than phonotactic clusters, it is hypothesised that in speech production, morphonotactic clusters are more robust and highlighted than phonotactic clusters.   The aim of this study was to compare homophonous phonotactic and morphonotactic consonant clusters. Homophonous target words produced in semi-spontaneous speech of 16 Standard Austrian German speakers were analysed.   Word-final morphonotactic and phonotactic consonant clusters were analysed with respect to relative duration and intensity of both the clusters and the cluster-final /t/. Additionally, closure duration of the final-/t/ and the intensity of the burst were measured.   Contrary to the assumption, a global comparison of the clusters revealed no difference in the relative duration of morphonotactic clusters as compared to the phonotactic counterparts. Likewise, concerning the word-final /t/ no differences between phonotactic and morphonotactic clusters occurred. Unexpectedly, gender- and age-specific differences arose, independent of the phonotactic / morphonotactic distinction of the clusters.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-388",
      "author_area_id": "2",
      "author_area_label": "Phonetics, Phonology, and Prosody"
    },
    "gibson15_interspeech": {
      "authors": [
        [
          "Mark",
          "Gibson"
        ],
        [
          "Ana Mar\u00eda Fern\u00e1ndez",
          "Planas"
        ],
        [
          "Adamantios",
          "Gafos"
        ],
        [
          "Emily",
          "Remirez"
        ]
      ],
      "title": "Consonant duration and VOT as a function of syllable complexity and voicing in a sub-set of Spanish clusters",
      "original": "i15_1690",
      "page_count": 5,
      "order": 433,
      "p1": "1690",
      "pn": "1694",
      "abstract": [
        "A series of hypotheses are addressed vis-\u00e0-vis the effects of syllable complexity and voicing on consonant duration and VOT (voice onset time) in a subset of Spanish clusters. Electropalatographic (EPG) and acoustic signals were obtained for native-speakers of Standard Peninsular Spanish producing clusters within and across word boundaries. The results show that VOT patterns for singleton onsets also hold for clusters, VOT in word-final devoiced stops and voiceless stops exhibit a short lag/long lag paradigm similar to German and English onset clusters, C1 duration in clusters is a function of voicing and place but not of syllable complexity, and the duration of /l/ in complex onsets is conditioned by the phonological specifications of the preceding consonant, but linguo-palatal contact is not. Combined results suggest an intimate relationship between the temporal parameters of gestures in onset clusters, but not across word boundaries, and that the temporal and spatial parameters of gestures in onsets, even though related, operate relatively independently from one another.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-389",
      "author_area_id": "2",
      "author_area_label": "Phonetics, Phonology, and Prosody"
    },
    "arai15b_interspeech": {
      "authors": [
        [
          "Takayuki",
          "Arai"
        ]
      ],
      "title": "Hands-on tool producing front vowels for phonetic education: aiming for pronunciation training with tactile sensation",
      "original": "i15_1695",
      "page_count": 5,
      "order": 434,
      "p1": "1695",
      "pn": "1699",
      "abstract": [
        "We applied a physical model of the human vocal tract, which was originally designed for simulating English /r/, and tested whether the model can produce a certain range of vowels, especially mid front vowels. We first confirmed that the model can produce such vowels with high intelligibility. By changing the tongue height of the model, learners can adjust the vowel quality by listening to the output sounds as well as receiving a tactile sensation. Therefore, we further used the model for the pronunciation training as a hands-on tool for phonetic education based on the consideration that tongue and finger movements are related in terms of motor control. We demonstrated the vowel production using the model and received feedbacks from a group of listeners engaged in phonetic education. The synergetic effect of visual, auditory and tactile sensations was pointed out as an advantage. We then conducted a production experiment, where participants were asked to repeat each vowel they heard and produce that vowel by means of manipulating the vocal-tract model. As results, slight training effects were observed when using the physical model. Specifically, formant frequencies approached target frequencies as the experimental session progressed.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-390",
      "author_area_id": "2",
      "author_area_label": "Phonetics, Phonology, and Prosody"
    },
    "dutta15_interspeech": {
      "authors": [
        [
          "Indranil",
          "Dutta"
        ],
        [
          "Ayushi",
          "Pandey"
        ]
      ],
      "title": "Acoustics of articulatory constraints: vowel classification and nasalization",
      "original": "i15_1700",
      "page_count": 5,
      "order": 435,
      "p1": "1700",
      "pn": "1704",
      "abstract": [
        "Nasalization confounds the acoustic space by both crowding acoustic cues and also increasing the bandwidth of F1 which makes vowel identification, and hence classification by machine learning algorithms more complex. We present results from a set of machine learning (ML) algorithms that are trained on 15 acoustic features from the spectral and temporal domains to classify contextually nasalized vowels [1, 2]. The Degree of Articulatory Constraint (DAC) predicts that phonetic segments produced with lesser tongue-dorsum involvement are coarticulatorily more sensitive [3, 4]. We test the predictions of the DAC, using ML algorithms to classify vowels in the context of labials and dentals. Labials /m/ show more coarticulatory sensitivity as compared to dentals /n/. The ML algorithms do align themselves with the predictions of the DAC, showing 12% better accuracy for identification of vowels in the context of labials than in the context of dentals. Based on the DAC predictions, labials will exert less coarticulatory influence on neighboring vowels, and hence the classification of vowels next to labials should be easier, compared to dentals. The Nasal-Vowel (NV) and Vowel-Nasal (VN) contexts also allow us to test the effects of anticipatory and carryover nasal coarticulation on vowel classification.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-391",
      "author_area_id": "2",
      "author_area_label": "Phonetics, Phonology, and Prosody"
    },
    "kraus15_interspeech": {
      "authors": [
        [
          "Janina",
          "Kraus"
        ]
      ],
      "title": "Voice-conditioned allophones of MOUTH and PRICE in bahamian creole",
      "original": "i15_1705",
      "page_count": 5,
      "order": 436,
      "p1": "1705",
      "pn": "1709",
      "abstract": [
        "Allophonic height alternations in MOUTH and PRICE, conditioned by coda voicing, have been confirmed for a wide variety of English accents, especially in the North American context. Realisations may vary in degree but not in direction. Moreton and Thomas [1] suggest that universal processes may underly the varying productions and propose the Asymmetric Assimilation model to unify observations on the effect of voicing. So far, the hypothesis has not been tested on creolised varieties of English. This study aims to describe the spectral nature of voice-conditioned allophones of MOUTH and PRICE in two social varieties of Bahamian Creole. The results of an acoustic analysis of 15 creole speakers from Nassau, Bahamas, support the claim that automatic phonetic processes may underly the initial direction of the allophony, which is then available for sociolinguistic differentiation.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-392",
      "author_area_id": "2",
      "author_area_label": "Phonetics, Phonology, and Prosody"
    },
    "kolly15_interspeech": {
      "authors": [
        [
          "Marie-Jos\u00e9",
          "Kolly"
        ],
        [
          "Adrian",
          "Leemann"
        ],
        [
          "Florian",
          "Matter"
        ]
      ],
      "title": "Analysis of spatial variation with app-based crowdsourced audio data",
      "original": "i15_1710",
      "page_count": 5,
      "order": 437,
      "p1": "1710",
      "pn": "1714",
      "abstract": [
        "Crowdsourcing technologies such as the use of web or smartphone applications enable the collection of large linguistic corpora. Such corpora allow for an analysis of various linguistic features. In the present contribution we analyzed data of >1500 speakers that were crowdsourced with the smartphone application `Dial\u00e4kt \u00c4pp' to examine one such feature: the articulation of /kh/ as [k\u0361x] in Swiss German. We compared our results with findings from historical dialectological surveys. Results showed evidence for sound change in progress in Northern Switzerland with [k\u0361x] diffusing towards Zurich but retreating from the Northeast. We discuss the findings against the backdrop of methodological caveats of crowdsourcing audio data through smartphone applications.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-393",
      "author_area_id": "2",
      "author_area_label": "Phonetics, Phonology, and Prosody"
    },
    "jani15_interspeech": {
      "authors": [
        [
          "M\u00e1ty\u00e1s",
          "Jani"
        ],
        [
          "Catia",
          "Cucchiarini"
        ],
        [
          "Roeland van",
          "Hout"
        ],
        [
          "Helmer",
          "Strik"
        ]
      ],
      "title": "Confusability in L2 vowels: analyzing the role of different features",
      "original": "i15_1715",
      "page_count": 5,
      "order": 438,
      "p1": "1715",
      "pn": "1719",
      "abstract": [
        "The production and perception of L2 vowels are influenced by the L1 vowel system. Most studies on L2 vowel production evaluate the learners' pronunciation using subjective listening tests. In this study we present a novel objective method for investigating learner vowel confusability based on acoustic measurements. Monosyllabic words uttered by Spanish learners of Dutch are analyzed, and basic acoustic features \u2014 formant frequencies and duration \u2014 are extracted. Native Dutch speakers' measurements are used to obtain models for the Dutch vowels, which are employed to compute likelihood ratios and similarity distributions of the Spanish realizations in comparison to the Dutch target vowels. The likelihood ratios are presented in a matrix format similar to a confusion matrix crossing the target vowels by the vowels as classified. Results based on spectral features alone confirm the existence of an attractor effect of L1 vowels on L2 vowels. Overall, including duration in the analyses decreases the number of confusions. Comparing the confusion values on different feature sets helps analyzing the impact of the specific features. The results of the present study suggest that although the Spanish learners' use of duration is not native-like, it does help reduce confusability among Dutch vowels.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-394",
      "author_area_id": "2",
      "author_area_label": "Phonetics, Phonology, and Prosody"
    },
    "zimmerer15_interspeech": {
      "authors": [
        [
          "Frank",
          "Zimmerer"
        ],
        [
          "J\u00fcrgen",
          "Trouvain"
        ]
      ],
      "title": "Perception of French speakers' German vowels",
      "original": "i15_1720",
      "page_count": 5,
      "order": 439,
      "p1": "1720",
      "pn": "1724",
      "abstract": [
        "Despite some similarities in their inventories, German and French exhibit marked differences in the vowel systems. Most notably, German uses both tenseness and length to differentiate vowels, whereas in French, vowel length is not distinctive. Therefore, interferences can be expected when French native speakers learn to speak German. Results of a vowel judgment experiment with vowels in minimal pairs produced by 56 French learners of German, indicate that these learners have indeed problems producing German vowels correctly. Advanced learners manage vowel productions better than beginners. Both groups show lengthening as well as shortening errors. Furthermore, rounded vowels seem to pose more severe problems in L2 acquisition than unrounded vowels. These results have important implications for language learning and teaching, particularly for individualized computer-assisted pronunciation training.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-395",
      "author_area_id": "2",
      "author_area_label": "Phonetics, Phonology, and Prosody"
    },
    "bruni15_interspeech": {
      "authors": [
        [
          "Jagoda",
          "Bruni"
        ],
        [
          "Daniel",
          "Duran"
        ],
        [
          "Grzegorz",
          "Dogil"
        ]
      ],
      "title": "Unintuitive phonetic behavior in tswana post-nasal stops",
      "original": "i15_1725",
      "page_count": 5,
      "order": 440,
      "p1": "1725",
      "pn": "1729",
      "abstract": [
        "This article describes the phonetic process of post-nasal devoicing in Tswana. We propose a multi-agent exemplar model with various interaction schemes which include factors like functional and social biases in order to account for this counter-intuitive phenomenon. Our novel hybrid multi-agent modeling framework facilitates investigation of sound change by combining the sociophonetic model of Nettle [22] and the exemplar-based model of Wedel [26] into a single unified model.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-396",
      "author_area_id": "2",
      "author_area_label": "Phonetics, Phonology, and Prosody"
    },
    "prathosh15_interspeech": {
      "authors": [
        [
          "A. P.",
          "Prathosh"
        ],
        [
          "A. G.",
          "Ramakrishnan"
        ],
        [
          "T. V.",
          "Ananthapadmanabha"
        ]
      ],
      "title": "Classification of place-of-articulation of stop consonants using temporal analysis",
      "original": "i15_2655",
      "page_count": 5,
      "order": 441,
      "p1": "2655",
      "pn": "2659",
      "abstract": [
        "This paper proposes acoustic-phonetic features for classification of place-of-articulation of stop consonants derived from their temporal structures. The speech signal corresponding to a stop is characterized by several temporal features such as sub-band zero-crossings and envelope fits. Classification experiments on the stops from the TIMIT (read speech) and the Buckeye (conversational speech) databases using a support vector machine classifier demonstrate that the performance of the proposed features (84.6%) is comparable to that obtained by MFCCs (85.1%) in many aspects. Further, the classification accuracy is boosted (90.1%) with the combination of temporal and MFCC features, which substantiates their supplementary nature.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-397"
    },
    "barlaz15_interspeech": {
      "authors": [
        [
          "Marissa",
          "Barlaz"
        ],
        [
          "Maojing",
          "Fu"
        ],
        [
          "Zhi-Pei",
          "Liang"
        ],
        [
          "Ryan",
          "Shosted"
        ],
        [
          "Brad",
          "Sutton"
        ]
      ],
      "title": "The emergence of nasal velar codas in Brazilian Portuguese: an rt-MRI study",
      "original": "i15_2660",
      "page_count": 5,
      "order": 442,
      "p1": "2660",
      "pn": "2664",
      "abstract": [
        "Real-time MRI has become an increasingly useful tool in articulatory studies, especially in examining posterior regions of the vocal tract. Previous work on Brazilian Portugese has shown the emergence of coda consonants following word-final nasal vowels, though this has been limited to the discussion of front vowels and coda emergence in anterior regions of the vocal tract. We present rt-MRI evidence showing a narrow constriction between the tongue dorsum and velum emerging at the end of back nasal vowels, patterning similarly with velar stop consonants. Though our present subject does not show evidence of complete occlusion, we believe the small distance between the tongue dorsum and velum makes the emergence of a velar consonant highly likely. The emergence of these consonants is a result of sound change that further distinguishes oral and nasal vowel pairs. We also show the utility of advanced image processing methodologies to give a more accurate and computationally economical way to do speech articulation research.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-398"
    },
    "michon15_interspeech": {
      "authors": [
        [
          "Elise",
          "Michon"
        ],
        [
          "Emmanuel",
          "Dupoux"
        ],
        [
          "Alejandrina",
          "Cristia"
        ]
      ],
      "title": "Salient dimensions in implicit phonotactic learning",
      "original": "i15_2665",
      "page_count": 5,
      "order": 443,
      "p1": "2665",
      "pn": "2669",
      "abstract": [
        "Adults are able to learn sound co-occurrences without conscious knowledge after brief exposures. But which dimensions of sounds are most salient in this process? Using an artificial phonology paradigm, we explored potential learnability differences involving consonant-, speaker-, and tone-vowel co-occurrences. Results revealed that participants, whose native language was not tonal, implicitly encoded consonant-vowel patterns with a high level of accuracy; were above chance for tone-vowel co-occurrences; and were at chance for speaker-vowel co-occurrences. This pattern of results is exactly what would be expected if both language-specific experience and innate biases to encode potentially contrastive linguistic dimensions affect the salience of different dimensions during implicit learning of sound patterns.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-399",
      "author_area_id": "2",
      "author_area_label": "Phonetics, Phonology, and Prosody"
    },
    "howson15_interspeech": {
      "authors": [
        [
          "Phil",
          "Howson"
        ]
      ],
      "title": "An acoustic examination of the three-way sibilant contrast in lower sorbian",
      "original": "i15_2670",
      "page_count": 5,
      "order": 444,
      "p1": "2670",
      "pn": "2674",
      "abstract": [
        "The current paper presents an acoustic study of Lower Sorbian. Four native speakers of Lower Sorbian participated in this study. Center of gravity, standard deviation, skewness, kurtosis and slope measures were taken to measure the fricatives' spectral qualities. Vocalic transitions were also taken to gather finer grained information about the sibilants' articulation. The results indicated there is a three-way contrast in Lower Sorbian. The alveolar and retroflex segments have approximately the same place of articulation, but the overall tongue shape contributes significantly to the difference in spectral properties. The alveolopalatal segment is composed of a tongue-palate contact which extends from the dental place towards the palatal. This suggests that the length of the constriction also greatly affects the spectral properties of the fricatives. However, the phonetic realization of these segments is not the same as in Polish. The COG measures indicate both the Lower Sorbian retroflex and alveolopalatal segments is much lower than Polish. This suggests that the phonetic instantiation of the three-way contrast is variable. The results also indicate that the acoustic-perceptual cues from the vowel transitions and fricative spectra work in conjunction to distinguish the sibilants because no single cue is reliable enough to distinguish all of them.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-400",
      "author_area_id": "2",
      "author_area_label": "Phonetics, Phonology, and Prosody"
    },
    "yuan15_interspeech": {
      "authors": [
        [
          "Jiahong",
          "Yuan"
        ],
        [
          "Mark",
          "Liberman"
        ]
      ],
      "title": "Investigating consonant reduction in Mandarin Chinese with improved forced alignment",
      "original": "i15_2675",
      "page_count": 4,
      "order": 445,
      "p1": "2675",
      "pn": "2678",
      "abstract": [
        "Phonetic reduction has been an important topic in linguistics research. It also presents a great challenge for forced alignment, a technique widely used for automatic phonetic segmentation. In this study, we employed skip-state HMMs to improve forced alignment quality and to make forced alignment applicable to the investigation of phonetic reduction and deletion. With skip-state HMMs, forced alignment accuracy at 10 ms agreement was improved from 73.3% to 75.6% on a corpus of Mandarin Chinese broadcast news speech. Our analysis based on the improved forced alignment of Mandarin broadcast news speech \u2014 verified by hand segmentation of a random sample of cases \u2014 shows that: 1. The durations of frication and aspiration are additive in the production of plosives and affricates; 2. Plosives are more likely to be deleted than affricates; and 3. Plosives and affricates in higher-frequency words and at word-medial position are more likely to be reduced.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-401",
      "author_area_id": "2",
      "author_area_label": "Phonetics, Phonology, and Prosody"
    },
    "pouplier15_interspeech": {
      "authors": [
        [
          "Marianne",
          "Pouplier"
        ],
        [
          "Stefania",
          "Marin"
        ],
        [
          "Alexei",
          "Kochetov"
        ]
      ],
      "title": "Durational characteristics and timing patterns of Russian onset clusters at two speaking rates",
      "original": "i15_2679",
      "page_count": 5,
      "order": 446,
      "p1": "2679",
      "pn": "2683",
      "abstract": [
        "This study presents articulatory data on the durational and timing characteristics of Russian onset clusters and their change as a function of speaking rate. While there is increasing evidence that languages differ systematically in their consonant-to-consonant timing, little is known on whether this difference also entails different implementations of speaking rate changes. Russian contrasts with languages like English or German in that it has little overlap between onset consonants. Relatedly, stop consonants are obligatorily released. We investigate whether these global timing characteristics have implications for the implementation of speech rate changes. We hypothesize that Russian onset timing may vary little as a function of speaking rate, with rate affecting consonant duration rather than C1-C2 timing. Also a cluster's sonority profile (e.g. /bl-/; /lb-/) may factor into the implementation of speech rate changes. Results show, contrary to expectation, that both duration and timing of the consonants in a cluster are subject to change. However, there was a less of a rate effect for clusters with falling sonority, pointing towards their lesser flexibility in timing. Our results also reveal significant differences in the durational control of C1 and C2, challenging current models of durational organization of consonant clusters.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-402",
      "author_area_id": "2",
      "author_area_label": "Phonetics, Phonology, and Prosody"
    },
    "wong15c_interspeech": {
      "authors": [
        [
          "Chun Hoy",
          "Wong"
        ],
        [
          "Tan",
          "Lee"
        ],
        [
          "Yu Ting",
          "Yeung"
        ],
        [
          "P. C.",
          "Ching"
        ]
      ],
      "title": "Modeling temporal dependency for robust estimation of LP model parameters in speech enhancement",
      "original": "i15_1730",
      "page_count": 5,
      "order": 447,
      "p1": "1730",
      "pn": "1734",
      "abstract": [
        "This paper presents a novel approach to robust estimation of linear prediction (LP) model parameters in the application of speech enhancement. The robustness stems from the use of prior knowledge on the clean speech and the interfering noise, which are represented by two separate codebooks of LP model parameters. We propose to model the temporal dependency between short-time model parameters with a composite hidden Markov model (HMM) that is constructed by combining the speech and the noise codebooks. Optimal speech model parameters are estimated from the HMM state sequence that best matches the input observation. To further improve the estimation accuracy, we propose to perform interpolation of multiple HMM state sequences such that the estimated speech parameters would not be limited by the codebook coverage. Experimental results demonstrate the benefits and effectiveness of temporal dependency modeling and states interpolation in improving the segmental signal-to-noise ratio, PESQ and spectral distortion of enhanced speech.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-403",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "vaz15_interspeech": {
      "authors": [
        [
          "Colin",
          "Vaz"
        ],
        [
          "Shrikanth S.",
          "Narayanan"
        ]
      ],
      "title": "Learning a speech manifold for signal subspace speech denoising",
      "original": "i15_1735",
      "page_count": 5,
      "order": 448,
      "p1": "1735",
      "pn": "1739",
      "abstract": [
        "We present a method for learning a low-dimensional manifold for speech from clean speech samples in high-dimensional space. Using this manifold, we perform speech denoising by projecting noisy speech onto the manifold to remove non-speech components. This method of denoising classifies our algorithm as a signal subspace denoising method, where high-dimensional noisy data is projected onto the signal subspace to recover the signal of interest. We ran denoising experiments with different types of additive noise. The proposed method not only recovers the second formant more accurately, but also produces denoised speech with higher quality (as illustrated by PESQ scores) compared to other signal subspace denoising algorithms.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-404",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "elshamy15_interspeech": {
      "authors": [
        [
          "Samy",
          "Elshamy"
        ],
        [
          "Nilesh",
          "Madhu"
        ],
        [
          "Wouter",
          "Tirry"
        ],
        [
          "Tim",
          "Fingscheidt"
        ]
      ],
      "title": "An iterative speech model-based a priori SNR estimator",
      "original": "i15_1740",
      "page_count": 5,
      "order": 449,
      "p1": "1740",
      "pn": "1744",
      "abstract": [
        "In this contribution we propose an a priori signal-to-noise ratio (SNR) estimator based on a probabilistic speech model. Since the a priori SNR is an important means for speech enhancement algorithms, such as weighting rule calculation for noise reduction or speech presence probability computation, its diligent estimation is of wide interest. As a basis for this estimator a Gaussian mixture model (GMM) is trained on clean speech amplitudes and by finding the maximum likelihood (ML) clean speech estimate of the corresponding observed frame the a priori SNR can easily be calculated. Additionally, an iterative scheme is applied to consequently enhance the estimate by repetitively evaluating the GMM. This technique allows to accomplish noise reduction free of musical tones even in non-stationary noise environments and exceeds the quality of the classical decision-directed (DD) approach for typical spectral weighting rules.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-405",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "zhang15c_interspeech": {
      "authors": [
        [
          "Xiao-Lei",
          "Zhang"
        ],
        [
          "DeLiang",
          "Wang"
        ]
      ],
      "title": "Multi-resolution stacking for speech separation based on boosted DNN",
      "original": "i15_1745",
      "page_count": 5,
      "order": 450,
      "p1": "1745",
      "pn": "1749",
      "abstract": [
        "Recent progress in speech separation shows that deep neural networks (DNN) based supervised methods can improve the performance in difficult noise conditions and exhibit good generalization to unseen noise scenarios. However, existing approaches do not explore contextual information sufficiently. In this paper, we focus on exploring contextual information using DNN. The proposed method has two parts \u2014 a multi-resolution stacking (MRS) framework and a boosted DNN (bDNN) classifier. The MRS framework trains a stack of classifier ensembles, where each classifier in an ensemble concatenates the raw acoustic feature and the outputs of its bottom ensemble as a new feature, and different classifiers in an ensemble work with different window lengths. The bDNN classifier first generates multiple base predictions for a frame from a given window that is centered on the frame and contains multiple neighboring frames, and then aggregates the base predictions for the final prediction. Our experimental comparison with DNN based speech separation in difficult noise scenarios demonstrates the effectiveness of the proposed method in terms of both prediction accuracy and objective speech intelligibility.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-406",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "nrholm15_interspeech": {
      "authors": [
        [
          "Sidsel Marie",
          "N\u00f8rholm"
        ],
        [
          "Martin",
          "Krawczyk-Becker"
        ],
        [
          "Timo",
          "Gerkmann"
        ],
        [
          "Steven van de",
          "Par"
        ],
        [
          "Jesper Rindom",
          "Jensen"
        ],
        [
          "Mads Gr\u00e6sb\u00f8ll",
          "Christensen"
        ]
      ],
      "title": "Least squares estimate of the initial phases in STFT based speech enhancement",
      "original": "i15_1750",
      "page_count": 5,
      "order": 451,
      "p1": "1750",
      "pn": "1754",
      "abstract": [
        "In this paper, we consider single-channel speech enhancement in the short time Fourier transform (STFT) domain. We suggest to improve an STFT phase estimate by estimating the initial phases. The method is based on the harmonic model and a model for the phase evolution over time. The initial phases are estimated by setting up a least squares problem between the noisy phase and the model for phase evolution. Simulations on synthetic and speech signals show a decreased error on the phase when an estimate of the initial phase is included compared to using the noisy phase as an initialisation. The error on the phase is decreased at input SNRs from -10 to 10 dB. Reconstructing the signal using the clean amplitude, the mean squared error is decreased and the PESQ score is increased.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-407",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "nrholm15b_interspeech": {
      "authors": [
        [
          "Sidsel Marie",
          "N\u00f8rholm"
        ],
        [
          "Jesper Rindom",
          "Jensen"
        ],
        [
          "Mads Gr\u00e6sb\u00f8ll",
          "Christensen"
        ]
      ],
      "title": "Enhancement of non-stationary speech using harmonic chirp filters",
      "original": "i15_1755",
      "page_count": 5,
      "order": 452,
      "p1": "1755",
      "pn": "1759",
      "abstract": [
        "In this paper, the issue of single channel speech enhancement of non-stationary voiced speech is addressed. The non-stationarity of speech is well known, but state of the art speech enhancement methods assume stationarity within frames of 20-30 ms. We derive optimal distortionless filters that take the non-stationarity nature of voiced speech into account via linear constraints. This is facilitated by imposing a harmonic chirp model on the speech signal. As an implicit part of the filter design, the noise statistics are also estimated based on the observed signal and parameters of the harmonic chirp model. Simulations on real speech show that the chirp based filters perform better than their harmonic counterparts. Further, it is seen that the gain of using the chirp model increases when the estimated chirp parameter is big corresponding to periods in the signal where the instantaneous fundamental frequency changes fast.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-408",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "kinoshita15_interspeech": {
      "authors": [
        [
          "Keisuke",
          "Kinoshita"
        ],
        [
          "Marc",
          "Delcroix"
        ],
        [
          "Atsunori",
          "Ogawa"
        ],
        [
          "Tomohiro",
          "Nakatani"
        ]
      ],
      "title": "Text-informed speech enhancement with deep neural networks",
      "original": "i15_1760",
      "page_count": 5,
      "order": 453,
      "p1": "1760",
      "pn": "1764",
      "abstract": [
        "A speech signal captured by a distant microphone is generally contaminated by background noise, which severely degrades the audible quality and intelligibility of the observed signal. To resolve this issue, speech enhancement has been intensively studied. In this paper, we consider a text-informed speech enhancement, where the enhancement process is guided by the corresponding text information, i.e., a correct transcription of the target utterance. The proposed deep neural network (DNN)-based framework is motivated by the recent success in the text-to-speech (TTS) research in employing DNN as well as high audible-quality output signal of the corpus-based speech enhancement which borrows knowledge from the TTS research field. Taking advantage of the nature of DNN that allows us to utilize disparate features in an inference stage, the proposed method infers the clean speech features by jointly using the observed signal and widely-used TTS features derived from the corresponding text. In this paper, we first introduce the background and the details of the proposed method. Then, we show how the text information can be naturally integrated into speech enhancement by utilizing DNN and improve the enhancement performance.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-409",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "masaya15_interspeech": {
      "authors": [
        [
          "Shogo",
          "Masaya"
        ],
        [
          "Masashi",
          "Unoki"
        ]
      ],
      "title": "Complex tensor factorization in modulation frequency domain for single-channel speech enhancement",
      "original": "i15_1765",
      "page_count": 5,
      "order": 454,
      "p1": "1765",
      "pn": "1769",
      "abstract": [
        "This paper proposes a novel method of speech enhancement using tensor factorization, which is extended from complex non-negative matrix factorization (CMF), in the modulation frequency domain. Non-negative matrix factorization (NMF) has attracted a great deal of attention as a recent approach to speech enhancement for its ease of feature detection in the acoustic frequency domain. However, previous studies have suggested that spectral processing like spectral subtraction in the modulation frequency domain has been an effective scheme for speech enhancement. The use of not only the amplitude information but also the phase information is required in the modulation frequency domain to utilize more information on speech. Thus, we present new tensor factorization on the complex spectrum in the modulation frequency domain for single-channel speech enhancement. The amplitude and phase spectrum in the acoustic frequency domain can be estimated by using the factorized complex spectra in the modulation frequency domain. Numerical experiments were carried out under several noisy conditions to evaluate the effectiveness of the proposed method. The signal to error ratio and signal to noise ratio loss were used as objective measures. The results revealed that the proposed method outperformed the existing methods of speech enhancement based on NMF and CMF.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-410",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "kang15_interspeech": {
      "authors": [
        [
          "Hyeonjoo",
          "Kang"
        ],
        [
          "JeeSok",
          "Lee"
        ],
        [
          "Soonho",
          "Baek"
        ],
        [
          "Hong-Goo",
          "Kang"
        ]
      ],
      "title": "Systematic integration of acoustic echo canceller and noise reduction modules for voice communication systems",
      "original": "i15_1770",
      "page_count": 5,
      "order": 455,
      "p1": "1770",
      "pn": "1774",
      "abstract": [
        "This paper proposes an efficient way of integrating acoustic echo canceller (AEC) and background noise reduction (NR) modules for voice communication systems. The main strategy for designing a standalone AEC or NR module is straightforward, but it is not easy to integrate two modules in a single system because of the undesired effect caused by the nonlinear nature of each module's output. The proposed algorithm directly estimates noise and echo components from the observed signal, then they are utilized in the process of updating AEC module. Since the estimation step is independent of the actual processing of NR module, the nonlinear effect caused by coupling the NR module with the AEC module can be minimized. Experimental results show that the proposed algorithm achieves the performance of standalone AEC in terms of echo-return-loss-enhancement (ERLE) metric while maintaining that of standalone NR module in the spectral distortion aspect.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-411",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "lee15g_interspeech": {
      "authors": [
        [
          "Chul Min",
          "Lee"
        ],
        [
          "Jong Won",
          "Shin"
        ],
        [
          "Nam Soo",
          "Kim"
        ]
      ],
      "title": "DNN-based residual echo suppression",
      "original": "i15_1775",
      "page_count": 5,
      "order": 456,
      "p1": "1775",
      "pn": "1779",
      "abstract": [
        "Due to the limitations of power amplifiers or loudspeakers, the echo signals captured in the microphones are not in a linear relationship with the far-end signals even when the echo path is perfectly linear. The nonlinear components of the echo cannot be successfully removed by a linear acoustic echo canceller. Residual echo suppression (RES) is a technique to suppress the remained echo after acoustic echo suppression (AES). Conventional approaches compute RES gain using Wiener filter or spectral subtraction method based on the estimated statistics on related signals. In this paper, we propose a deep neural network (DNN)-based RES gain estimation based on both the far-end and the AES output signals in all frequency bins. A DNN architecture, which is suitable to model a complicated nonlinear mapping between high-dimensional vectors, is employed as a regression function from these signals to the optimal RES gain. The proposed method can suppress the residual components without any explicit double-talk detectors. The experimental results show that our proposed approach outperforms a conventional method in terms of the echo return loss enhancement (ERLE) for single-talk periods and the perceptual evaluation of speech quality (PESQ) score for double-talk periods.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-412",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "he15b_interspeech": {
      "authors": [
        [
          "Qi",
          "He"
        ],
        [
          "Changchun",
          "Bao"
        ],
        [
          "Feng",
          "Bao"
        ]
      ],
      "title": "Codebook-based speech enhancement using Markov process and speech-presence probability",
      "original": "i15_1780",
      "page_count": 5,
      "order": 457,
      "p1": "1780",
      "pn": "1784",
      "abstract": [
        "This paper presents a codebook-based speech enhancement algorithm by using Markov process and speech-presence probability (SPP). The Markov process is utilized to model the correlation between the adjacent code-vectors in the codebook for optimizing Bayesian minimum mean squared error (MMSE) estimator. Then the proposed estimator is used to estimate spectral shapes and gains of speech and noise. The correlation between adjacent linear prediction (LP) gains is also fully considered during the procedure of parameter estimation. Through the introduction of SPP in the codebook-constrained Wiener filter, the proposed Wiener filter achieves the goal of much more noise reduction and does not result in the speech component distortion. The evaluation results confirm that the proposed algorithm has a much better performance for reducing annoying background noise than the conventional codebook-based algorithms.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-413",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "chinaev15_interspeech": {
      "authors": [
        [
          "Aleksej",
          "Chinaev"
        ],
        [
          "Reinhold",
          "Haeb-Umbach"
        ]
      ],
      "title": "On optimal smoothing in minimum statistics based noise tracking",
      "original": "i15_1785",
      "page_count": 5,
      "order": 458,
      "p1": "1785",
      "pn": "1789",
      "abstract": [
        "Noise tracking is an important component of speech enhancement algorithms. Of the many noise trackers proposed, Minimum Statistics (MS) is a particularly popular one due to its simple parameterization and at the same time excellent performance. In this paper we propose to further reduce the number of MS parameters by giving an alternative derivation of an optimal smoothing constant. At the same time the noise tracking performance is improved as is demonstrated by experiments employing speech degraded by various noise types and at different SNR values.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-414",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "hao15_interspeech": {
      "authors": [
        [
          "Yue",
          "Hao"
        ],
        [
          "Changchun",
          "Bao"
        ],
        [
          "Feng",
          "Bao"
        ],
        [
          "Feng",
          "Deng"
        ]
      ],
      "title": "A data-driven speech enhancement method based on modeled long-range temporal dynamics",
      "original": "i15_1790",
      "page_count": 5,
      "order": 459,
      "p1": "1790",
      "pn": "1794",
      "abstract": [
        "In this paper, a data-driven speech enhancement method based on modeled long-range temporal dynamics (LRTDs) is proposed. First, given speech and noise corpora, Gaussian Mixture Models (GMMs) of the speech and noise can be trained respectively based on the expectation-maximization (EM) algorithm. Then, the LRTDs are obtained from the GMM models. Next, based on the LRTDs, a noise robustness longest segment searching (NRLSS) method combined with the Vector Taylor Series (VTS) approximation algorithm is adopted to search the longest matching speech and noise segments (LMSNS) from speech and noise corpora. Finally, using the obtained LMSNS, the estimation of speech spectrum is achieved. Furthermore, a modified Wiener filter is constructed to further eliminate residual noise. The test results show that the proposed method outperforms the state-of-the-art speech enhancement methods.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-415",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "mayer15_interspeech": {
      "authors": [
        [
          "Florian",
          "Mayer"
        ],
        [
          "Pejman",
          "Mowlaee"
        ]
      ],
      "title": "Improved phase reconstruction in single-channel speech separation",
      "original": "i15_1795",
      "page_count": 5,
      "order": 460,
      "p1": "1795",
      "pn": "1799",
      "abstract": [
        "Conventional single-channel source separation (SCSS) algorithms are mostly focused on estimating the spectral amplitude of the underlying sources extracted from a mixture. The importance of phase information in source separation and its positive impact on improving the achievable performance is not adequately studied yet. In this work, we propose a phase estimation method to enhance the spectral phase of the underlying signals in SCSS framework. The proposed method relies on multi-pitch estimation and phase decomposition followed by applying temporal smoothing filters on the unwrapped mixture phase. We consider the combination of the proposed phase estimator with ideal binary mask and non-negative matrix factorization, as two well-known SCSS methods for separating the spectral amplitudes. Our results show that certain improvements in quality and intelligibility is achievable via replacing the mixture phase with the estimated one when reconstructing the sources.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-416",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "zhao15_interspeech": {
      "authors": [
        [
          "Tuo",
          "Zhao"
        ],
        [
          "Yunxin",
          "Zhao"
        ],
        [
          "Xin",
          "Chen"
        ]
      ],
      "title": "Time-frequency kernel-based CNN for speech recognition",
      "original": "i15_1888",
      "page_count": 5,
      "order": 461,
      "p1": "1888",
      "pn": "1892",
      "abstract": [
        "We propose a novel approach to generate time-frequency kernel based deep convolutional neural networks (CNN) for robust speech recognition. We give different treatments to shifting along the time and the frequency axes of speech feature representations in the 2D convolution, so as to achieve certain invariance in small frequency shifts while expanding time context size for speech input without smearing time positions of phone segments. The 2D-kernel approach allows easy implementation of deep CNNs. We present experimental results on speaker-independent phone recognition tasks of TIMIT and FFMTIMIT, where the latter was acquired using a far-field microphone and the speech data are noisy. Our results demonstrate that the proposed time-frequency kernel-based CNN gives consistent phone error reductions over frequency-domain CNN and DNN for both TIMIT and FFMTIMIT, with more benefits shown for recognizing noisy speech by using clean speech models.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-417",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition \u2013 Signal Processing, Acoustic Modeling, Robustness, and Adaptation"
    },
    "weber15_interspeech": {
      "authors": [
        [
          "Philip",
          "Weber"
        ],
        [
          "Colin J.",
          "Champion"
        ],
        [
          "S. M.",
          "Houghton"
        ],
        [
          "Peter",
          "Jan\u010dovi\u010d"
        ],
        [
          "Martin",
          "Russell"
        ]
      ],
      "title": "Consonant recognition with continuous-state hidden Markov models and perceptually-motivated features",
      "original": "i15_1893",
      "page_count": 5,
      "order": 462,
      "p1": "1893",
      "pn": "1897",
      "abstract": [
        "Research into human perception of consonants has identified phoneme-specific perceptual cues. It has also been shown that the characteristics of the speech signal most useful for recognition depend on the specific speech sound. Typical ASR features and recognisers however neither vary with the type of sound nor relate directly to perceptual cues.   We investigate classification and decoding of non-sonorant consonants using basic perceptually-motivated features \u2014 phoneme durations and energy in a few broad spectral bands. Our classification results using simple classifiers suggest that features optimal for human perception also perform best for machine classification. We show how characteristics of the models learned relate to knowledge of human speech perception.   Recognition results using a continuous-state HMM (CSHMM) show accuracy similar to a discrete-state HMM with similar assumptions. We conclude by outlining how the CSHMM provides a mechanism to make use of other perceptually-important features by integration with similar models for recognition of voiced sounds.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-418",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition \u2013 Signal Processing, Acoustic Modeling, Robustness, and Adaptation"
    },
    "ganapathy15_interspeech": {
      "authors": [
        [
          "Sriram",
          "Ganapathy"
        ],
        [
          "Samuel",
          "Thomas"
        ],
        [
          "Dimitrios",
          "Dimitriadis"
        ],
        [
          "Steven",
          "Rennie"
        ]
      ],
      "title": "Investigating factor analysis features for deep neural networks in noisy speech recognition",
      "original": "i15_1898",
      "page_count": 5,
      "order": 463,
      "p1": "1898",
      "pn": "1902",
      "abstract": [
        "The problem of speaker and channel adaptation in deep neural network (DNN) based automatic speech recognition (ASR) systems is of substantial interest in advancing the performance of these systems. Recently, the speaker identity vectors (i-vectors) have shown improvements for ASR systems in matched conditions. In this paper, we propose the application of the general factor analysis framework for noisy speech recognition tasks. Several methods for deriving speaker and channel factors are explored including joint factor analysis (JFA) and i-vectors derived from DNN posteriors instead of the traditional Universal background model (UBM) approach. We also experiment with the late fusion of i-vector features with bottleneck (BN) features obtained from a previously trained convolutional neural network (CNN) system. The ASR experiments are performed on the Aspire challenge test data which contains noisy far-field speech while the acoustic models are trained with conversational telephone speech (CTS) data from the Fisher corpus. In these experiments, we show that the factor analysis based methods provide significant improvements in the word error rate (relative improvements of about 11% compared to the baseline DNN system trained with speaker adapted features).\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-419",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition \u2013 Signal Processing, Acoustic Modeling, Robustness, and Adaptation"
    },
    "travadi15_interspeech": {
      "authors": [
        [
          "Ruchir",
          "Travadi"
        ],
        [
          "Shrikanth S.",
          "Narayanan"
        ]
      ],
      "title": "Ensemble of Gaussian mixture localized neural networks with application to phone recognition",
      "original": "i15_1903",
      "page_count": 5,
      "order": 464,
      "p1": "1903",
      "pn": "1907",
      "abstract": [
        "In this paper we present Ensemble of Gaussian Mixture Localized Neural Networks (EGMLNNs), a model for the joint probability density of input as well as output variables of any general mapping to be estimated. The model aims at identifying clusters in the input data, thereby replacing one complex classifier with an ensemble of relatively simpler classifiers, each of which is localized to operate within its associated cluster. We present an algorithm for maximum likelihood parameter estimation for this model using Expectation Maximization (EM). The reported results on phone recognition task on TIMIT database show that the model is able to obtain performance improvement over a single complex classifier while also reducing the computational complexity required for testing.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-420",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition \u2013 Signal Processing, Acoustic Modeling, Robustness, and Adaptation"
    },
    "pesan15_interspeech": {
      "authors": [
        [
          "Jan",
          "Pe\u0161\u00e1n"
        ],
        [
          "Luk\u00e1\u0161",
          "Burget"
        ],
        [
          "Hynek",
          "Hermansky"
        ],
        [
          "Karel",
          "Vesel\u00fd"
        ]
      ],
      "title": "DNN derived filters for processing of modulation spectrum of speech",
      "original": "i15_1908",
      "page_count": 4,
      "order": 465,
      "p1": "1908",
      "pn": "1911",
      "abstract": [
        "We propose a novel approach to design modulation frequency filters for the first stage processing of critical band spectrum of speech using deep neural network (DNN). These filters replace conventional modulation frequency filters currently used in state-of-the-art BUT speech recognition system and yield about 10% relative improvement in phoneme recognition accuracy. The resulting filters are consistent with some known temporal properties of higher levels of mammalian auditory processing and suggest more efficient scheme for pre-processing of speech for ASR.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-421",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition \u2013 Signal Processing, Acoustic Modeling, Robustness, and Adaptation"
    },
    "nagamine15_interspeech": {
      "authors": [
        [
          "Tasha",
          "Nagamine"
        ],
        [
          "Michael L.",
          "Seltzer"
        ],
        [
          "Nima",
          "Mesgarani"
        ]
      ],
      "title": "Exploring how deep neural networks form phonemic categories",
      "original": "i15_1912",
      "page_count": 5,
      "order": 466,
      "p1": "1912",
      "pn": "1916",
      "abstract": [
        "Deep neural networks (DNNs) have become the dominant technique for acoustic-phonetic modeling due to their markedly improved performance over other models. Despite this, little is understood about the computation they implement in creating phonemic categories from highly variable acoustic signals. In this paper, we analyzed a DNN trained for phoneme recognition and characterized its representational properties, both at the single node and population level in each layer. At the single node level, we found strong selectivity to distinct phonetic features in all layers. Node selectivity to specific manners and places of articulation appeared from the first hidden layer and became more explicit in deeper layers. Furthermore, we found that nodes with similar phonetic feature selectivity were differentially activated to different exemplars of these features. Thus, each node becomes tuned to a particular acoustic manifestation of the same feature, providing an effective representational basis for the formation of invariant phonemic categories. This study reveals that phonetic features organize the activations in different layers of a DNN, a result that mirrors the recent findings of feature encoding in the human auditory system. These insights may provide better understanding of the limitations of current models, leading to new strategies to improve their performance.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-422",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition \u2013 Signal Processing, Acoustic Modeling, Robustness, and Adaptation"
    },
    "loukina15_interspeech": {
      "authors": [
        [
          "Anastassia",
          "Loukina"
        ],
        [
          "Melissa",
          "Lopez"
        ],
        [
          "Keelan",
          "Evanini"
        ],
        [
          "David",
          "Suendermann-Oeft"
        ],
        [
          "Alexei V.",
          "Ivanov"
        ],
        [
          "Klaus",
          "Zechner"
        ]
      ],
      "title": "Pronunciation accuracy and intelligibility of non-native speech",
      "original": "i15_1917",
      "page_count": 5,
      "order": 467,
      "p1": "1917",
      "pn": "1921",
      "abstract": [
        "This paper investigates the connection between intelligibility and pronunciation accuracy. We compare which words in non-native English speech are likely to be misrecognized and which words are likely to be marked as pronunciation errors. We found that only 16% of the variability in word-level intelligibility can be explained by the presence of obvious mispronunciations. In some cases, a word remained recognizable or could be identified from the context despite obvious pronunciation errors. In many other cases, the annotators were unable to identify the word when listening to the audio but did not perceive it as mispronounced when presented with its transcription. At the same time, we see high agreement when the results are aggregated across all words from the same speaker.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-423",
      "author_area_id": "2",
      "author_area_label": "Phonetics, Phonology, and Prosody"
    },
    "zimmerer15b_interspeech": {
      "authors": [
        [
          "Frank",
          "Zimmerer"
        ],
        [
          "J\u00fcrgen",
          "Trouvain"
        ]
      ],
      "title": "Productions of /h/ in German: French vs. German speakers",
      "original": "i15_1922",
      "page_count": 5,
      "order": 468,
      "p1": "1922",
      "pn": "1926",
      "abstract": [
        "This paper investigates the production of /h/ by French learners of German in comparison to German native speakers. Anecdotally, French speakers are assumed to delete /h/ when speaking German. We investigate the extent to which learners have problems producing /h/ and if advanced learners show different production patterns than beginners. When French speakers produce /h/ our analysis focuses on whether their utterance is comparable to that of native speakers'. The analysis is based on 34 words occurring in a learner corpus with read speech. Our results indicate that complete deletion is quite rare. Beginners sometimes omit /h/ but they predominantly realize it as [\u0294] or other forms of glottalization. Advanced learners are more successful in producing /h/ native-like. Native speakers and most advanced learners realize /h/ as [h] or [\u0266] depending on the voicing status of the left context. Presumably, native(-like) realizations are based on no or a low degree of adduction of the vocal folds whereas beginners apply a high degree of vocal fold adduction, i.e., hyperarticulation. This results in a difference of the duration of the /h/-segments as our measurements show. Our findings may have important implications for language teaching, aiming at the reduction of a foreign accent.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-424",
      "author_area_id": "2",
      "author_area_label": "Phonetics, Phonology, and Prosody"
    },
    "bonneau15_interspeech": {
      "authors": [
        [
          "Anne",
          "Bonneau"
        ],
        [
          "Martine",
          "Cadot"
        ]
      ],
      "title": "German non-native realizations of French voiced fricatives in final position of a group of words",
      "original": "i15_1927",
      "page_count": 5,
      "order": 469,
      "p1": "1927",
      "pn": "1931",
      "abstract": [
        "We analyzed the realizations of French voiced fricatives /z,\u0292/ by German non-native and French native speakers, in final position of an accentual group, a position where German fricatives are devoiced. Three speaker levels (from beginners to advanced speakers) and different boundary types (depending on whether the fricative is followed by a pause, a schwa, or is directly followed by the first phoneme of the subsequent group), are considered. A set of cues, among which periodicity, fricative duration, and intensity in low frequencies, is used for voicing analysis. Results show that German realizations vary significantly with language, speakers' level and boundary type, and argue in favor of an influence of L1 (German) final devoicing on non-native realizations. We discuss these effects as well as the influence of orthography (the presence of a schwa at the end of words).\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-425",
      "author_area_id": "2",
      "author_area_label": "Phonetics, Phonology, and Prosody"
    },
    "best15_interspeech": {
      "authors": [
        [
          "Catherine T.",
          "Best"
        ],
        [
          "Jason A.",
          "Shaw"
        ],
        [
          "Gerard",
          "Docherty"
        ],
        [
          "Bronwen G.",
          "Evans"
        ],
        [
          "Paul",
          "Foulkes"
        ],
        [
          "Jennifer",
          "Hay"
        ],
        [
          "Jalal",
          "Al-Tamimi"
        ],
        [
          "Katharine",
          "Mair"
        ],
        [
          "Karen E.",
          "Mulak"
        ],
        [
          "Sophie",
          "Wood"
        ]
      ],
      "title": "From newcastle MOUTH to aussie ears: australians' perceptual assimilation and adaptation for newcastle UK vowels",
      "original": "i15_1932",
      "page_count": 5,
      "order": 470,
      "p1": "1932",
      "pn": "1936",
      "abstract": [
        "To probe how episodic and abstract processes contribute to flexible perception of phonetically variable speech, we evaluated Australian (Aus) listeners' perception of Aus-accented vowels versus those of an unfamiliar accent: Newcastle UK (Ncl). Aus listeners first heard a round-robin story told by multiple talkers of Aus or Ncl, then categorized multi-talker tokens of 20 vowels in nonce words spoken in the Aus or Ncl accent. Categorization was variable even across Aus nonce vowels (Maccuracy ranged from 21-80%). Perceptual assimilation of Ncl vowels (Aus passage/Ncl nonce) was diverse: Some were categorized very much like the corresponding Aus vowel. Some showed within-category differentiation from Aus; others were heard as a different vowel altogether. Perception of some Ncl vowels changed after Ncl passage exposure, including both positive adaptation (improved categorization: e.g., MOUTH, FLEECE, TRAP) and negative shifts (increased differentiation from the corresponding Aus vowel: e.g., NURSE, FOOT). Assimilation and adaptation patterns were largely consistent with similarities and dissimilarities between the Aus and Ncl vowel spaces. Implications of the results for episodic and abstract contributions to perceptual flexibility are discussed. We also consider the possibility that listeners perceptually adjust to other-accent vowels as a system, rather than treating each vowel as an independent entity.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-426",
      "author_area_id": "2",
      "author_area_label": "Phonetics, Phonology, and Prosody"
    },
    "bundgaardnielsen15_interspeech": {
      "authors": [
        [
          "Rikke Louise",
          "Bundgaard-Nielsen"
        ],
        [
          "Brett",
          "Baker"
        ],
        [
          "Olga",
          "Maxwell"
        ],
        [
          "Janet",
          "Fletcher"
        ]
      ],
      "title": "Wubuy coronal stop perception by speakers of three dialects of bangla",
      "original": "i15_1937",
      "page_count": 5,
      "order": 471,
      "p1": "1937",
      "pn": "1941",
      "abstract": [
        "We tested native speakers from three major dialect groups of Bangla, on their discrimination of a four-way coronal stop contrast from the Australian Indigenous language Wubuy. Bangla is generally assumed to have a two-way contrast in coronal stops, with an additional place distinction in affricates. The results show that Bangla speakers are able to discriminate the Wubuy contrasts, but also that certain contrasts are more difficult to discriminate than others. We discuss these results with respect to the Bangla coronal inventory, and importantly, with respect to the variation in the phonetic realisation of coronals between the dialects of Bangla. We argue that the phonetic realisation of what is regarded to be the `same' phonemic inventory can have implications for the perceptual behaviour of speakers.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-427",
      "author_area_id": "2",
      "author_area_label": "Phonetics, Phonology, and Prosody"
    },
    "hirst15_interspeech": {
      "authors": [
        [
          "Daniel",
          "Hirst"
        ],
        [
          "Hongwei",
          "Ding"
        ]
      ],
      "title": "Using melody metrics to compare English speech read by native speakers and by L2 Chinese speakers from shanghai",
      "original": "i15_1942",
      "page_count": 5,
      "order": 472,
      "p1": "1942",
      "pn": "1946",
      "abstract": [
        "In this study we analyse 18 metrics which were extracted fully automatically from the acoustic signal to describe the melodic characteristics of recordings of English read by L2 Chinese speakers from Shanghai. The metrics were compared to those of native English speakers recording the same material and also to comparable Chinese recordings read by the same speakers from Shanghai and also by other speakers. For the great majority of the metrics the values obtained for the L2 speakers were intermediate between those obtained from the recordings of English and of Chinese by native speakers.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-428",
      "author_area_id": "2",
      "author_area_label": "Phonetics, Phonology, and Prosody"
    },
    "gibson15b_interspeech": {
      "authors": [
        [
          "James",
          "Gibson"
        ],
        [
          "Nikolaos",
          "Malandrakis"
        ],
        [
          "Francisco",
          "Romero"
        ],
        [
          "David C.",
          "Atkins"
        ],
        [
          "Shrikanth S.",
          "Narayanan"
        ]
      ],
      "title": "Predicting therapist empathy in motivational interviews using language features inspired by psycholinguistic norms",
      "original": "i15_1947",
      "page_count": 5,
      "order": 473,
      "p1": "1947",
      "pn": "1951",
      "abstract": [
        "Therapist language plays a critical role in influencing the overall quality of psychotherapy. Notably, it is a major contributor to the perceived level of empathy expressed by therapists, a primary measure for judging their efficacy. We explore psycholinguistics inspired features for predicting therapist empathy. These features model language which conveys information about affective and cognitive processes, which is central to the therapist expressing understanding of the patient's perspective. We describe the dimensional features obtained based on psycholinguisitic norms, and their application to predicting empathy expressed in motivational interviewing sessions for addiction counseling. We compare these to standard lexical features (n-grams) and demonstrate that these features contain complementary information for predicting therapist empathy. The highest empathy prediction results achieved are 75.28% UAR and 0.6112 Spearman's correlation.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-429",
      "author_area_id": "3",
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "malandrakis15_interspeech": {
      "authors": [
        [
          "Nikolaos",
          "Malandrakis"
        ],
        [
          "Shrikanth S.",
          "Narayanan"
        ]
      ],
      "title": "Therapy language analysis using automatically generated psycholinguistic norms",
      "original": "i15_1952",
      "page_count": 5,
      "order": 474,
      "p1": "1952",
      "pn": "1956",
      "abstract": [
        "Lexical norms, normative and usually numeric, ratings of word meaning are popular tools in research domains relating to human expression and perception of language, especially with regards to emotion. In this paper we are proposing an algorithm of psycholinguistic norm expansion capable of generating high quality norms representing aspects of language beyond emotion, including language concreteness and indicators of age and gender association. Starting from small manually annotated norm lexica, continuous norms for new words are estimated using semantic similarity and a simple linear model along eleven expression-related dimensions. The model is shown to achieve state of the art level performance of word norm estimation. To investigate the potential of these norms as analysis tools of more complex phenomena we use them to investigate the differences in therapist speech in sessions conducted by practitioners adhering to the psychoanalytic and client-centered schools of therapy.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-430",
      "author_area_id": "3",
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "xia15_interspeech": {
      "authors": [
        [
          "Wei",
          "Xia"
        ],
        [
          "James",
          "Gibson"
        ],
        [
          "Bo",
          "Xiao"
        ],
        [
          "Brian",
          "Baucom"
        ],
        [
          "Panayiotis G.",
          "Georgiou"
        ]
      ],
      "title": "A dynamic model for behavioral analysis of couple interactions using acoustic features",
      "original": "i15_1957",
      "page_count": 5,
      "order": 475,
      "p1": "1957",
      "pn": "1961",
      "abstract": [
        "Observational therapy is an important element of mental health that relies on a detailed assessment of multiple behavioral cues. Behavioral coding for research in the field is unfortunately often at session-level resolution due to the inherent cost of labeling and human subjectivity. Being able to model the interlocutors' behavior at a fine temporal resolution and analyze the effect of such behavioral changes in the gestalt perception can help psychologists better understand the behavioral mechanism. In this paper, we propose a method to model the dynamically evolving behavior of interlocutors during couple interactions. We firstly present a static behavioral model based on the local decisions with global fusion, and investigate the impact of the frame length to provide effective global evaluations. We then propose a two-layer sequential Hidden Markov Model to capture local state transitions. We use the corpus of Couple Therapy interactions as a case study, finding that an interlocutor does not express a single behavior throughout a conversation, and there are temporal correlations between neighboring frames. We show that dynamic models can achieve up to 10% relative improvement, compared to static models. This suggests that the human behavioral interaction is a non-linear process, and the resulting latent-state labels may provide new insights to domain experts.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-431",
      "author_area_id": "3",
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "gupta15_interspeech": {
      "authors": [
        [
          "Rahul",
          "Gupta"
        ],
        [
          "Theodora",
          "Chaspari"
        ],
        [
          "Panayiotis G.",
          "Georgiou"
        ],
        [
          "David C.",
          "Atkins"
        ],
        [
          "Shrikanth S.",
          "Narayanan"
        ]
      ],
      "title": "Analysis and modeling of the role of laughter in motivational interviewing based psychotherapy conversations",
      "original": "i15_1962",
      "page_count": 5,
      "order": 476,
      "p1": "1962",
      "pn": "1966",
      "abstract": [
        "Motivational interviewing (MI) is a goal oriented psychotherapy involving natural conversation between a counselor and a client to instill motivation towards behavioral change in the client. Often during such an interaction, the counselor and client express themselves through nonverbal cues such as laughter. We analyze the role of laughters during MI sessions. Specifically, we perform a set of three studies to: (i) Investigate the distribution of utterances containing laughters in an MI session using Poisson process models, (ii) Analyze patterns in counselor and client behaviors with respect to laughter occurrences and (iii) Study the association of counselor utterances high in desirable behaviors such as empathy, acceptance and collaboration (referred to as BrowniePoint counselor utterances) with laughters. We quantify the impact of one persons laughter on the laughter rate of the other person. Our results show that the type of laughter (client/counselor stand alone laughter, shared laughter) can be associated with different patterns of counselor/client behaviors and depict unique relations with BrowniePoint utterances.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-432",
      "author_area_id": "3",
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "bonin15_interspeech": {
      "authors": [
        [
          "Francesca",
          "Bonin"
        ],
        [
          "Nick",
          "Campbell"
        ],
        [
          "Carl",
          "Vogel"
        ]
      ],
      "title": "The discourse value of social signals at topic change moments",
      "original": "i15_1967",
      "page_count": 5,
      "order": 477,
      "p1": "1967",
      "pn": "1971",
      "abstract": [
        "The dynamics of social interactions during human-human conversation have attracted the attention of many researchers aiming at improving the naturalness of automatic dialogue systems and creating socially aware machines. While many studies have investigate social dynamics in relation to the emotional sphere of a conversation, in this work we investigate whether these dynamics are correlated with the discourse structure, particularly topic changes. We conduct an observational study in two corpora of differing nature. In both corpora we find that at topic terminations, the interactional entropy, defined as the amount of social signals exchanged by the participants, decreases with the introduction of a new topic. We conclude that a relation exists between topic changes and amount of social interaction and, hence, that social signals have a discourse value in addition to a social function. This value, we suggest, is group acknowledgement that a new topic has gained the floor. Understanding these social dynamics within conversations can be exploited in the development of socially aware dialogue systems.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-433",
      "author_area_id": "3",
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "schrank15_interspeech": {
      "authors": [
        [
          "Tobias",
          "Schrank"
        ],
        [
          "Barbara",
          "Schuppler"
        ]
      ],
      "title": "Automatic detection of uncertainty in spontaneous German dialogue",
      "original": "i15_1972",
      "page_count": 5,
      "order": 478,
      "p1": "1972",
      "pn": "1976",
      "abstract": [
        "Uncertainty is ubiquitous in natural human communication. Human listeners assess the speaker's degree of uncertainty at any time in communication and use this information to shape dialogue. In contrast, currently available computer systems dealing with spoken language are usually not built to perform this task. The ability to detect uncertainty would likely lead to more natural human-computer dialogue. In order to detect uncertainty automatically, we extract linguistic, paralinguistic and dialogue-related features from the Kiel Corpus, a corpus of naturalistic task-oriented spoken German. We then use these features to train a random forests model. Our experimental results show that relatively high classification accuracy can be obtained while employing only 64 well-chosen features (73% accuracy, 69% F1). To our best knowledge, this is the first study of automatic uncertainty detection using German speech data as well as the first achieving good performance on everyday speech.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-434",
      "author_area_id": "3",
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "ringeval15_interspeech": {
      "authors": [
        [
          "Fabien",
          "Ringeval"
        ],
        [
          "Erik",
          "Marchi"
        ],
        [
          "Marc",
          "Mehu"
        ],
        [
          "Klaus",
          "Scherer"
        ],
        [
          "Bj\u00f6rn",
          "Schuller"
        ]
      ],
      "title": "Face reading from speech \u2014 predicting facial action units from audio cues",
      "original": "i15_1977",
      "page_count": 5,
      "order": 479,
      "p1": "1977",
      "pn": "1981",
      "abstract": [
        "The automatic recognition of facial behaviours is usually achieved through the detection of particular FACS Action Unit (AU), which then makes it possible to analyse the affective behaviours expressed in the face. Despite the fact that advanced techniques have been proposed to extract relevant facial descriptors, the processing of real-life data, i. e., recorded in unconstrained environments, makes the automatic detection of FACS AU much more challenging compared to constrained recordings, such as posed faces, and even impossible when the corresponding parts of the face are masked or subject to low or no illumination. We present in this paper the very first attempt in using acoustic cues for the automatic detection of FACS AU, as an alternative way to obtain information from the face when such data are not available. Results show that features extracted from the voice can be effectively used to predict different types of FACS AU, and that the best performance are obtained for the prediction of the apex, in comparison to the prediction of onset, offset and occurrence.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-435",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "nandwana15_interspeech": {
      "authors": [
        [
          "Mahesh Kumar",
          "Nandwana"
        ],
        [
          "Hynek",
          "Bo\u0159il"
        ],
        [
          "John H. L.",
          "Hansen"
        ]
      ],
      "title": "A new front-end for classification of non-speech sounds: a study on human whistle",
      "original": "i15_1982",
      "page_count": 5,
      "order": 480,
      "p1": "1982",
      "pn": "1986",
      "abstract": [
        "Speech/non-speech sound classification is an important problem in audio diarization, audio document retrieval and advanced human interfaces. The focus of this study is on the development of spectral and temporal acoustic features for speech/non-speech sound classification based on production differences in speech versus whistle. Seven time- and frequency-domain based features are investigated. Performance of the proposed feature set for the task of speech/whistle classification is evaluated at frame level. This evaluation utilizes support vector machine (SVM) models and Gaussian mixture models (GMM) for back-end classifiers. At the frame-level, the proposed front-end fusion gives an absolute performance gain of +15.0% and +3.1% over MFCC with SVM and GMM based classifiers, respectively. This research will benefit the development of intelligent speech interfaces for identification, recognition, and speech coding, as a preprocessing step for real world audio streams.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-436",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "dumpala15_interspeech": {
      "authors": [
        [
          "Sri Harsha",
          "Dumpala"
        ],
        [
          "Bhanu Teja",
          "Nellore"
        ],
        [
          "Raghu Ram",
          "Nevali"
        ],
        [
          "Suryakanth V.",
          "Gangashetty"
        ],
        [
          "B.",
          "Yegnanarayana"
        ]
      ],
      "title": "Robust features for sonorant segmentation in continuous speech",
      "original": "i15_1987",
      "page_count": 5,
      "order": 481,
      "p1": "1987",
      "pn": "1991",
      "abstract": [
        "Sonorant segmentation of speech signals is critical in developing Automatic Speech Recognition (ASR) systems, audio search systems and for automatic segmentation of speech corpora. In this work, acoustic features based on excitation source and vocal tract system characteristics of sonorant sounds are proposed for segmentation of sonorant regions in continuous speech. The features are based on zero frequency resonator signal energy, strength of excitation and dominant resonance frequency around epochs. An algorithm is developed to relate these features in hierarchical manner using knowledge-based approach. The performance of the proposed algorithm is studied on three different datasets, at varying levels of degradation. TIMIT database is used to test the validity and AMI meeting corpus and Telugu (an Indian language) dataset are considered to test the utility of the proposed features.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-437",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "gergen15_interspeech": {
      "authors": [
        [
          "Sebastian",
          "Gergen"
        ],
        [
          "Anil",
          "Nagathil"
        ],
        [
          "Rainer",
          "Martin"
        ]
      ],
      "title": "Reduction of reverberation effects in the MFCC modulation spectrum for improved classification of acoustic signals",
      "original": "i15_1992",
      "page_count": 5,
      "order": 482,
      "p1": "1992",
      "pn": "1996",
      "abstract": [
        "The classification of acoustic signals is an important step in many audio signal processing algorithms, e.g. in the context of speech enhancement, speech recognition, and others. Signals which are captured for classification are often degraded by an unknown amount of reverberation in a real environment. If a classifier is trained on clean and anechoic data, a mismatch between training and test conditions results in a reduced classification accuracy. In this paper, we introduce a novel equalization gain matrix which can be applied to modulation domain audio features. This gain is designed to counteract the modifications which originate from reverberation such that the mismatch between clean training data and degraded test data is reduced. Experiments show that the classification accuracy can be increased significantly for reverberant signals.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-438",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "dennis15_interspeech": {
      "authors": [
        [
          "Jonathan",
          "Dennis"
        ],
        [
          "Huy Dat",
          "Tran"
        ],
        [
          "Haizhou",
          "Li"
        ]
      ],
      "title": "Spiking neural networks and the generalised hough transform for speech pattern detection",
      "original": "i15_1997",
      "page_count": 5,
      "order": 483,
      "p1": "1997",
      "pn": "2001",
      "abstract": [
        "This paper proposes a novel spiking neural network (SNN) architecture that integrates with the generalised Hough transform (GHT) framework for the task of detecting specific speech patterns such as command words. The idea is that the GHT can model the geometrical distribution of speech information over the wider temporal context, while the SNN to used learn the discriminative prior weighting in the GHT to provide a spike output indicating a detection decision. The SNN therefore enhances the projection of the GHT from the input acoustic information into the sparse Hough accumulator space for detecting specific sound patterns. Compared using conventional neural network architectures for this task, the GHT-SNN system has the advantage that it does not require a voice activity detection module or an explicit noise model to reject non-target frames. Instead the output of the SNN is a voltage that is trained to exceed a threshold for positive instances of the sound pattern while remaining below this threshold otherwise, requiring no explicit noise model. Experiments are carried out on the challenging Chalearn gesture recognition task where spoken commands must be detected against variable background noise while rejecting a range of out-of-vocabulary words.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-439",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "choi15_interspeech": {
      "authors": [
        [
          "Woohyun",
          "Choi"
        ],
        [
          "Sangwook",
          "Park"
        ],
        [
          "David K.",
          "Han"
        ],
        [
          "Hanseok",
          "Ko"
        ]
      ],
      "title": "Acoustic event recognition using dominant spectral basis vectors",
      "original": "i15_2002",
      "page_count": 5,
      "order": 484,
      "p1": "2002",
      "pn": "2006",
      "abstract": [
        "This paper proposes a novel filter bank composed of dominant Spectral Basis Vectors (SBVs) in a spectrogram. Spectral envelopes represented by the SBVs have shown to be excellent characteristic features for discriminating different acoustic events in noisy environment. Non-negative Matrix Factorization (NMF) and non-negative K-SVD (NKSVD) for part-based and holistic representations extract dominant SBVs from a spectrogram. The effectiveness of the proposed method is demonstrated on a database of real life recordings via experiments, and its robust performance is compared to conventional methods.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-440",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "hwang15_interspeech": {
      "authors": [
        [
          "Inyoung",
          "Hwang"
        ],
        [
          "Jaeseong",
          "Sim"
        ],
        [
          "Sang-Hyeon",
          "Kim"
        ],
        [
          "Kwang-Sub",
          "Song"
        ],
        [
          "Joon-Hyuk",
          "Chang"
        ]
      ],
      "title": "A statistical model-based voice activity detection using multiple DNNs and noise awareness",
      "original": "i15_2277",
      "page_count": 5,
      "order": 485,
      "p1": "2277",
      "pn": "2281",
      "abstract": [
        "In this paper, we propose the ensemble of deep neural networks (DNNs) by using acoustic environment classification for statistical model-based voice activity detection (VAD). Since conventional decision functions for statistical model-based VAD are based on shallow model and it cannot take an advantage of the diversity of the space distribution of features, we present to use the multiple DNNs separately trained on different noise condition as decision function for the statistical model-based VAD. And, environmental noise classification is also performed based on the separate DNN since acoustic environment classification makes it possible to achieve high detection performance at various type of noise environment by using different algorithm according to current noise condition. In the training stage, a number of DNNs are independently trained according to different type of noise environments, and separate DNN is organized to detect one of the environmental conditions. In an online stage, the environmental knowledge on each frame is contributed to allow us to combine the speech presence probabilities, which are derived from the ensemble of the trained DNNs for the individual environment. Our approach for VAD was evaluated in terms of objective measures and showed significant improvement compared to the conventional algorithm.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-441",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "wang15e_interspeech": {
      "authors": [
        [
          "Qing",
          "Wang"
        ],
        [
          "Jun",
          "Du"
        ],
        [
          "Xiao",
          "Bao"
        ],
        [
          "Zi-Rui",
          "Wang"
        ],
        [
          "Li-Rong",
          "Dai"
        ],
        [
          "Chin-Hui",
          "Lee"
        ]
      ],
      "title": "A universal VAD based on jointly trained deep neural networks",
      "original": "i15_2282",
      "page_count": 5,
      "order": 486,
      "p1": "2282",
      "pn": "2286",
      "abstract": [
        "In this paper, we propose a joint training approach to voice activity detection (VAD) to address the issue of performance degradation due to unseen noise conditions. Two key techniques are integrated into this deep neural network (DNN) based VAD framework. First, a regression DNN is trained to map the noisy to clean speech features similar to DNN-based speech enhancement. Second, the VAD part to discriminate speech against noise backgrounds is also a DNN trained with a large amount of diversified noisy data synthesized by a wide range of additive noise types. By stacking the classification DNN on top of the enhancement DNN, this integrated DNN can be jointly trained to perform VAD. The feature mapping DNN serves as a noise normalization module aiming at explicitly generating the \u201cclean\u201d features which are easier to be correctly recognized by the following classification DNN. Our experiment results demonstrate the proposed noise-universal DNN-based VAD algorithm achieves a good generalization capacity to unseen noises, and the jointly trained DNNs consistently and significantly outperform the conventional classification-based DNN for all the noise types and signal-to-noise levels tested.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-442",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "zhan15_interspeech": {
      "authors": [
        [
          "Ge",
          "Zhan"
        ],
        [
          "Zhaoqiong",
          "Huang"
        ],
        [
          "Dongwen",
          "Ying"
        ],
        [
          "Jielin",
          "Pan"
        ],
        [
          "Yonghong",
          "Yan"
        ]
      ],
      "title": "Spectrographic speech mask estimation using the time-frequency correlation of speech presence",
      "original": "i15_2287",
      "page_count": 5,
      "order": 487,
      "p1": "2287",
      "pn": "2291",
      "abstract": [
        "This paper proposes a method to estimate the spectrographic speech mask based on a two-dimensional (2-D) correlation model. The proposed method is motivated by a fact that the time and frequency correlations of speech presence are interwoven with each other in the time-frequency (TF) domain. Conventional Markov chain is incapable of simultaneously modeling the time and frequency correlations in an adaptive way. The 2-D correlation model is presented to describe the correlation of speech presence in the TF domain, where the speech presence and absence are taken as two states of the model. The time correlation is modeled by the time state-transition probability and the forward factor, while the frequency state-transition probability and the corresponding neighbor factor are defined to describe the frequency correlation. The time and frequency correlations are incorporated into the model by maximizing the Q-function. A sequential scheme is presented to online estimate the parameter set. Given the observed spectrum and the parameter set, the state matrix that maximizes the posteriori probability is regarded as the optimal estimate of the speech mask. The proposed method was compared with some well-established methods. The experimental results confirmed its superiority.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-443",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "ghaemmaghami15_interspeech": {
      "authors": [
        [
          "Houman",
          "Ghaemmaghami"
        ],
        [
          "David",
          "Dean"
        ],
        [
          "Shahram",
          "Kalantari"
        ],
        [
          "Sridha",
          "Sridharan"
        ],
        [
          "Clinton",
          "Fookes"
        ]
      ],
      "title": "Complete-linkage clustering for voice activity detection in audio and visual speech",
      "original": "i15_2292",
      "page_count": 5,
      "order": 488,
      "p1": "2292",
      "pn": "2296",
      "abstract": [
        "We propose a novel technique for conducting robust voice activity detection (VAD) in high-noise recordings. We use Gaussian mixture modeling (GMM) to train two generic models; speech and non-speech. We then score smaller segments of a given (unseen) recording against each of these GMMs to obtain two respective likelihood scores for each segment. These scores are used to compute a dissimilarity measure between pairs of segments and to carry out complete-linkage clustering of the segments into speech and non-speech clusters. We compare the accuracy of our method against state-of-the-art and standardised VAD techniques to demonstrate an absolute improvement of 15% in half-total error rate (HTER) over the best performing baseline system and across the QUT-NOISE-TIMIT database. We then apply our approach to the Audio-Visual Database of American English (AVDBAE) to demonstrate the performance of our algorithm in using visual, audio-visual or a proposed fusion of these features.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-444",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "sriskandaraja15_interspeech": {
      "authors": [
        [
          "Kaavya",
          "Sriskandaraja"
        ],
        [
          "Vidhyasaharan",
          "Sethu"
        ],
        [
          "Phu Ngoc",
          "Le"
        ],
        [
          "Eliathamby",
          "Ambikairajah"
        ]
      ],
      "title": "A model based voice activity detector for noisy environments",
      "original": "i15_2297",
      "page_count": 5,
      "order": 489,
      "p1": "2297",
      "pn": "2301",
      "abstract": [
        "This paper presents a model-based voice activity detector (VAD) aimed at operating in low signal to noise ratio conditions and non-stationary noise environments. The proposed system makes use of Gaussian mixture models trained on Mel Frequency Cepstral Coefficients extracted from noisy speech data. In addition, information from smoothed frame based log energy is used to augment the system to detect voice activity accurately. Finally, preliminary decisions made by the system are post processed to remove some false acceptances which further improves the system performance. Experimental results show that the proposed VAD significantly outperforms the system that currently produces state-of-the-art results on the QUT-NOISE-TIMIT database with relative improvements of 34.58%, 17.18% and 3.5% for high, medium and low signal to noise ratio scenarios respectively.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-445",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "tao15_interspeech": {
      "authors": [
        [
          "Fei",
          "Tao"
        ],
        [
          "John H. L.",
          "Hansen"
        ],
        [
          "Carlos",
          "Busso"
        ]
      ],
      "title": "An unsupervised visual-only voice activity detection approach using temporal orofacial features",
      "original": "i15_2302",
      "page_count": 5,
      "order": 490,
      "p1": "2302",
      "pn": "2306",
      "abstract": [
        "Detecting the presence or absence of speech is an important step toward building robust speech-based interfaces. While previous studies have made progress on voice activity detection (VAD), the performance of these systems significantly degrades when subjects employ challenging speech modes that deviate from normal acoustic patterns (e.g., whisper speech), or in noisy/adverse conditions. An appealing approach under these conditions is visual voice activity detection (VVAD), which detects speech using features characterizing the orofacial activity. This study proposes an unsupervised approach that relies only on visual features, and, therefore, is insensitive to vocal style or time-varying background noise. This study proposes an unsupervised approach that relies on visual features. We estimate optical flow variance and geometrical features around lips, extracting the short-time zero crossing rates, short-time variances, and delta features over a small temporal window. These variables are fused using principal component analysis (PCA) to obtain a \u201ccombo\u201d feature, which displays a bimodal distributions (speech versus silence). A threshold is automatically determine using the expectation-maximization (EM) algorithm. The approach can be easily transformed into a supervised VVAD, if needed. We evaluate the system in neutral and whisper speech. While speech based VADs generally fail to detect speech activity in whisper speech, given its important acoustic differences, the proposed VVAD achieves near 80% accuracy in both neutral and whisper speech, highlighting the benefits of the system.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-446",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "raboshchuk15_interspeech": {
      "authors": [
        [
          "Ganna",
          "Raboshchuk"
        ],
        [
          "Peter",
          "Jan\u010dovi\u010d"
        ],
        [
          "Climent",
          "Nadeu"
        ],
        [
          "Alex Peir\u00f3",
          "Lilja"
        ],
        [
          "M\u00fcnevver",
          "K\u00f6k\u00fcer"
        ],
        [
          "Blanca Mu\u00f1oz",
          "Mahamud"
        ],
        [
          "Ana Riverola de",
          "Veciana"
        ]
      ],
      "title": "Automatic detection of equipment alarms in a neonatal intensive care unit environment: a knowledge-based approach",
      "original": "i15_2902",
      "page_count": 5,
      "order": 491,
      "p1": "2902",
      "pn": "2906",
      "abstract": [
        "Alarm sounds triggered by biomedical equipment play a key role in providing healthcare in a neonatal intensive care unit (NICU). This paper presents our work on automatic detection of acoustic alarms in a noisy NICU environment, where knowledge about the particular characteristics of each alarm class is integrated at different stages of the detection system. The feature extraction is based on applying, around alarm-specific frequencies, a method for detection of sinusoidal signals, which employs the normalised short-term magnitude and phase spectrum. Also, the ratios of magnitudes at those frequencies are taken as features. The system consists of a set of GMM-based detectors, each designed to deal with a specific alarm. Temporal structure of alarms, in terms of duration of signal and silence intervals in every alarm period, is incorporated by aggregating the frame-level posterior probabilities. The experimental evaluations are performed with a database recorded in a real-world hospital environment. The performance of the detection system is assessed both at the frame level and at the alarm period level.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-447",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "dai15_interspeech": {
      "authors": [
        [
          "Jia",
          "Dai"
        ],
        [
          "Wenju",
          "Liu"
        ],
        [
          "Chongjia",
          "Ni"
        ],
        [
          "Like",
          "Dong"
        ],
        [
          "Hong",
          "Yang"
        ]
      ],
      "title": "\u201cmultilingual\u201d deep neural network for music genre classification",
      "original": "i15_2907",
      "page_count": 5,
      "order": 492,
      "p1": "2907",
      "pn": "2911",
      "abstract": [
        "Multilingual deep neural network (DNN) has been widely used in low-resource automatic speech recognition (ASR) in order to balance the rich-resource and low-resource speech recognition or to build the low-resource ASR system quickly. Inspired by the idea of using multilingual DNN for ASR, we use a \u201cmultilingual\u201d DNN (Multi-DNN) for music genre classification. However, we do not have \u201cmultilingual\u201d in music, so we use the similar resource instead. In order to obtain the similar resource corresponding to small target database, the nearest neighbor (NN) algorithm is used to re-label the large similar database. Then the re-labeled large similar database is used to train a Multi-DNN, and the small target database is used to further adapt the trained Multi-DNN. By using the Multi-DNN approach, the DNN can be well trained, and be transferred to the small target database quickly. The experiments are evaluated on the benchmark databases, ISMIR database and GTZAN database, which are used as the large similar database and small target database respectively. The experiment results show that the proposed method can achieve 93.4% (10-fold cross-validation) average classification accuracy on GTZAN database, which outperforms the state-of-the-art best performance on this database.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-448",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "liu15d_interspeech": {
      "authors": [
        [
          "Baiyang",
          "Liu"
        ],
        [
          "Bjorn",
          "Hoffmeister"
        ],
        [
          "Ariya",
          "Rastrow"
        ]
      ],
      "title": "Accurate endpointing with expected pause duration",
      "original": "i15_2912",
      "page_count": 5,
      "order": 493,
      "p1": "2912",
      "pn": "2916",
      "abstract": [
        "In an online automatic speech recognition system, the role of the endpoint detector is to infer when a user has finished speaking a query. Accurate and low-latency endpoint detection is crucial for natural voice interaction. Classic voice activity detector (VAD) based approaches monitor the incoming audio and trigger when a sufficiently long pause is detected. Such approaches are typically limited due to their inability to distinguish between within and end-of-sentence pauses. In this paper, we propose an endpoint detection algorithm that is integrated with the speech recognition process, leveraging acoustic and language model information in order to distinguish between within and end-of-sentence pauses. Unlike other integrated approaches that are based on the highest-scoring active recognition hypothesis, the proposed algorithm computes the expected pause duration over all active hypotheses, which leads to a more reliable pause duration prediction. We show that our method achieves significantly higher accuracy and lower latency in a comparison to standard approaches for endpoint detection.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-449",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "liu15e_interspeech": {
      "authors": [
        [
          "Wenbo",
          "Liu"
        ],
        [
          "Zhiding",
          "Yu"
        ],
        [
          "Bhiksha",
          "Raj"
        ],
        [
          "Ming",
          "Li"
        ]
      ],
      "title": "Locality constrained transitive distance clustering on speech data",
      "original": "i15_2917",
      "page_count": 5,
      "order": 494,
      "p1": "2917",
      "pn": "2921",
      "abstract": [
        "The idea of developing unsupervised learning methods has received significant attention in recent years. An important application is whether one can train a high quality speaker verification model given large quantities of unlabeled speech data. Unsupervised learning methods such as data clustering often play a central role since they are able to analyze the underlying latent patterns without any supervision information. In this paper, we focus on developing an effective clustering method for speech data. We propose the locality constrained transitive distance, a distance measure which better models speech data with arbitrarily shaped clusters. We also propose a robust top-down clustering framework on top of the distance measure to generate accurate cluster labels. Experimental results show the good performance of the proposed method.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-450",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "espi15_interspeech": {
      "authors": [
        [
          "Miquel",
          "Espi"
        ],
        [
          "Masakiyo",
          "Fujimoto"
        ],
        [
          "Keisuke",
          "Kinoshita"
        ],
        [
          "Tomohiro",
          "Nakatani"
        ]
      ],
      "title": "Feature extraction strategies in deep learning based acoustic event detection",
      "original": "i15_2922",
      "page_count": 5,
      "order": 495,
      "p1": "2922",
      "pn": "2926",
      "abstract": [
        "Non-speech acoustic events are significantly different between them, and usually require access to detail rich features. That is why directly modeling a real spectrogram can provide a significant advantage, instead of using predefined features that usually compress and downsample detail as typically done in speech recognition. This paper focuses on the importance of feature extraction for deep learning based acoustic event detection, and more specifically on exploiting local spectro-temporal features of sounds. We do this in two ways: (1) outside the model, using multiple resolution spectrogram simultaneously based on the fact that there is a time-frequency detail trade-off that depends on the resolution with which a spectrogram is computed (e.g. `steps' would require a finer time resolution, while sounds that span many frequencies require finer frequency detail); and (2), with a model that implicitly exploits locality, convolutional neural networks, which are a state-of-the-art 2D feature extraction model. An experimental evaluation shows that the presented approaches outperform state-of-the-art deep learning baseline with a noticeable gain in the CNN case, and provides insights regarding CNN-based spectrogram characterization.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-451",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "transfeld15_interspeech": {
      "authors": [
        [
          "Peter",
          "Transfeld"
        ],
        [
          "Simon",
          "Receveur"
        ],
        [
          "Tim",
          "Fingscheidt"
        ]
      ],
      "title": "An acoustic event detection framework and evaluation metric for surveillance in cars",
      "original": "i15_2927",
      "page_count": 5,
      "order": 496,
      "p1": "2927",
      "pn": "2931",
      "abstract": [
        "The protection of cars against burglary and car theft is a major issue for automotive industry. Several techniques exist to stop the thief from driving away or to alert the owner. In this contribution we present an acoustic event detection (AED) framework for surveillance in cars, operating on a continuous audio stream which may be acquired by the existing in-car hands-free microphone. Furthermore we derive a new time-dependent accuracy measure tACC, evaluating both the accuracy and the temporal preciseness of the instant of detection. Operating on continuous audio, we examine a wide range of signal to noise ratios (SNRs) in a realistic parking scenario, showing the effectiveness of both the proposed AED framework and the new time-dependent accuracy measure.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-452",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "bouchekif15_interspeech": {
      "authors": [
        [
          "Abdessalam",
          "Bouchekif"
        ],
        [
          "G\u00e9raldine",
          "Damnati"
        ],
        [
          "Yannick",
          "Est\u00e8ve"
        ],
        [
          "Delphine",
          "Charlet"
        ],
        [
          "Nathalie",
          "Camelin"
        ]
      ],
      "title": "Diachronic semantic cohesion for topic segmentation of TV broadcast news",
      "original": "i15_2932",
      "page_count": 5,
      "order": 497,
      "p1": "2932",
      "pn": "2936",
      "abstract": [
        "This paper proposes a new way to integrate semantic relations into a topic segmentation process by defining the notion of semantic cohesion. In the context of a sliding window based automatic topic segmentation algorithm, semantic relations are incorporated in the similarity measure between adjacent blocs. Additionally, in the context of TV Brodcast News topic segmentation, we propose a new protocole to gather relevant data for semantic relations computation, showing that a small set of diachronic data can be more relevant for the task than using a large amount of general or asynchronous data. Experiments on a corpus of 86 various French TV Broadcast News shows recorded during one week, in conjunction with text articles collected through the Google News homepage at the same period for semantic relation estimation show significant improvement in topic segmentation performance.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-453",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "kraljevski15_interspeech": {
      "authors": [
        [
          "Ivan",
          "Kraljevski"
        ],
        [
          "Zheng-Hua",
          "Tan"
        ],
        [
          "Maria Paola",
          "Bissiri"
        ]
      ],
      "title": "Comparison of forced-alignment speech recognition and humans for generating reference VAD",
      "original": "i15_2937",
      "page_count": 5,
      "order": 498,
      "p1": "2937",
      "pn": "2941",
      "abstract": [
        "This present paper aims to answer the question whether forced-alignment speech recognition can be used as an alternative to humans in generating reference Voice Activity Detection (VAD) transcriptions. An investigation of the level of agreement between automatic/manual VAD transcriptions and the reference ones produced by a human expert was carried out. Thereafter, statistical analysis was employed on the automatically produced and the collected manual transcriptions. Experimental results confirmed that forced-alignment speech recognition can provide accurate and consistent VAD labels.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-454",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "lehner15_interspeech": {
      "authors": [
        [
          "Bernhard",
          "Lehner"
        ],
        [
          "Gerhard",
          "Widmer"
        ],
        [
          "Reinhard",
          "Sonnleitner"
        ]
      ],
      "title": "Improving voice activity detection in movies",
      "original": "i15_2942",
      "page_count": 5,
      "order": 499,
      "p1": "2942",
      "pn": "2946",
      "abstract": [
        "Voice Activity Detection in movies is a non-trivial and challenging task. The different emotional states of the speakers, as well as the variety of soundscapes and noises contribute to the complexity of the task. In this paper, we propose a set of lightweight features that are specifically designed to perform under such conditions, while at the same time preventing confusions of singing voice with speech. For evaluation, we use four full-length movies, previously unseen to the system and painstakingly annotated. We compare our detector to a state-of-the-art reference system. The new approach performs better, yielding just about half the Equal Error Rate (EER). Furthermore, since the ground truth annotation task is extremely tedious, and to help with advancing in this topic, we release the annotations of all four movies to the research community.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-455",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "su15_interspeech": {
      "authors": [
        [
          "Pei-Hao",
          "Su"
        ],
        [
          "David",
          "Vandyke"
        ],
        [
          "Milica",
          "Ga\u0161i\u0107"
        ],
        [
          "Dongho",
          "Kim"
        ],
        [
          "Nikola",
          "Mrk\u0161i\u0107"
        ],
        [
          "Tsung-Hsien",
          "Wen"
        ],
        [
          "Steve",
          "Young"
        ]
      ],
      "title": "Learning from real users: rating dialogue success with neural networks for reinforcement learning in spoken dialogue systems",
      "original": "i15_2007",
      "page_count": 5,
      "order": 500,
      "p1": "2007",
      "pn": "2011",
      "abstract": [
        "To train a statistical spoken dialogue system (SDS) it is essential that an accurate method for measuring task success is available. To date training has relied on presenting a task to either simulated or paid users and inferring the dialogue's success by observing whether this presented task was achieved or not. Our aim however is to be able to learn from real users acting under their own volition, in which case it is non-trivial to rate the success as any prior knowledge of the task is simply unavailable. User feedback may be utilised but has been found to be inconsistent. Hence, here we present two neural network models that evaluate a sequence of turn-level features to rate the success of a dialogue. Importantly these models make no use of any prior knowledge of the user's task. The models are trained on dialogues generated by a simulated user and the best model is then used to train a policy on-line which is shown to perform at least as well as a baseline system using prior knowledge of the user's task. We note that the models should also be of interest for evaluating SDS and for monitoring a dialogue in rule-based SDS.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-456",
      "author_area_id": "11",
      "author_area_label": "Spoken Language Processing \u2013 Dialogue, Summarization, Understanding"
    },
    "griol15_interspeech": {
      "authors": [
        [
          "David",
          "Griol"
        ],
        [
          "Zoraida",
          "Callejas"
        ],
        [
          "Ram\u00f3n",
          "L\u00f3pez-C\u00f3zar"
        ]
      ],
      "title": "A framework to develop context-aware adaptive dialogue system",
      "original": "i15_2012",
      "page_count": 5,
      "order": 501,
      "p1": "2012",
      "pn": "2016",
      "abstract": [
        "In this paper, we propose a general-purpose framework to develop spoken dialogue systems that dynamically adapt their behavior to user requirements and preferences, as well as to the interaction context. A data-driven technique is proposed to build task structures and dialogue models within the framework. Our proposal reduces the effort required for both the implementation of a new system and the adaptation of an existing one to a new task. We have evaluated the framework developing a travel-planning system, and provide a detailed discussion of its positive influence on both the interaction quality and the personalization of provided services.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-457"
    },
    "griol15b_interspeech": {
      "authors": [
        [
          "David",
          "Griol"
        ],
        [
          "Zoraida",
          "Callejas"
        ]
      ],
      "title": "A proposal to develop domain and subtask-adaptive dialog management models",
      "original": "i15_2017",
      "page_count": 5,
      "order": 502,
      "p1": "2017",
      "pn": "2021",
      "abstract": [
        "Statistical dialog management techniques have the main advantage of allowing an easy adaptation of the dialog model to different application domains. In this paper we propose to also adapt the operation of the dialog manager to the different subtasks that conform the structure of the dialog in each domain. To do so, we present a variation of a statistical dialog management technique to address this challenge. The evaluation results with a practical dialog system show that the use of such specific subtask models increase the quality and number of successful interactions with the system.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-458",
      "author_area_id": "11",
      "author_area_label": "Spoken Language Processing \u2013 Dialogue, Summarization, Understanding"
    },
    "khan15b_interspeech": {
      "authors": [
        [
          "Omar Zia",
          "Khan"
        ],
        [
          "Jean-Philippe",
          "Robichaud"
        ],
        [
          "Paul A.",
          "Crook"
        ],
        [
          "Ruhi",
          "Sarikaya"
        ]
      ],
      "title": "Hypotheses ranking and state tracking for a multi-domain dialog system using multiple ASR alternates",
      "original": "i15_2022",
      "page_count": 5,
      "order": 503,
      "p1": "2022",
      "pn": "2026",
      "abstract": [
        "In this paper, we present an approach to improve the accuracy of multi-domain multi-turn spoken dialog system (SDS) by including alternate results from automatic speech recognition (ASR). Often, even if the top ranked result from the ASR is not correct, the correct result may still be available in the NBest list or in the word confusion network (WCN). Thus, the SDS performance can be improved by considering beyond the top ranked choice from the ASR.We employ late binding, such that multiple ASR choices are propagated through the SDS and knowledge fetch so that additional context can be utilized at later stages to determine the top choice that is good for the overall SDS. We rank alternate domain dependent semantic frames, multiple semantic frames per ASR choice, to determine the true SDS output. Using real-world data, extracted from the logs of Cortana personal digital assistant deployed to millions of users, we show that significant gains can be achieved in domain detection, intent determination, and slot tagging, by considering additional results from ASR.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-459",
      "author_area_id": "11",
      "author_area_label": "Spoken Language Processing \u2013 Dialogue, Summarization, Understanding"
    },
    "wu15c_interspeech": {
      "authors": [
        [
          "Ji",
          "Wu"
        ],
        [
          "Miao",
          "Li"
        ],
        [
          "Chin-Hui",
          "Lee"
        ]
      ],
      "title": "An entropy minimization framework for goal-driven dialogue management",
      "original": "i15_2027",
      "page_count": 5,
      "order": 504,
      "p1": "2027",
      "pn": "2031",
      "abstract": [
        "We propose an entropy minimization dialog management (DM) strategy for goal-driven information retrieval (IR). By associating each goal of an IR task with a set of stochastic attributes, reaching a goal can then be accomplished by filling the \u201cattribute slots\u201d corresponding to the goal. Information access can now be cast as a dialog problem that specific information about the attributes is solicited from a user by the system through multiple dialog turns. For a real-world music search task with 38118 songs and 12 attributes corresponding to each song, we demonstrate the concept by designing a simulation game to order song from the above music database. We show that 8.3 dialog turns are needed on the average if random questions are asked by the system, whereas the entropy minimization DM is a very efficient goal seeking method to order a song with the least amount of 3.3 dialog turns among different strategies. Furthermore, the proposed DM techniques can manage the dialog process in a more efficient and flexible manner.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-460",
      "author_area_id": "11",
      "author_area_label": "Spoken Language Processing \u2013 Dialogue, Summarization, Understanding"
    },
    "zukerman15_interspeech": {
      "authors": [
        [
          "Ingrid",
          "Zukerman"
        ],
        [
          "Andisheh",
          "Partovi"
        ],
        [
          "Su Nam",
          "Kim"
        ]
      ],
      "title": "Context-dependent error correction of spoken referring expressions",
      "original": "i15_2032",
      "page_count": 5,
      "order": 505,
      "p1": "2032",
      "pn": "2036",
      "abstract": [
        "We integrate a supervised machine learning mechanism for detecting erroneous words in the output of a speech recognizer with a two-tier error-correction approach that features (1) a noisy-channel model that replaces erroneous words with generic words, and (2) a phonetic-similarity mechanism that refines the generic words based on a short list of candidate interpretations. Our results, obtained on a corpus of 341 referring expressions, show that the first tier improves interpretation performance, and the second tier yields further improvements.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-461",
      "author_area_id": "11",
      "author_area_label": "Spoken Language Processing \u2013 Dialogue, Summarization, Understanding"
    },
    "wu15d_interspeech": {
      "authors": [
        [
          "Zhizheng",
          "Wu"
        ],
        [
          "Tomi",
          "Kinnunen"
        ]
      ],
      "title": "Automatic speaker verification spoofing and countermeasures (ASVspoof 2015): introductory talk by the organizers",
      "original": "i15_4106",
      "page_count": 0,
      "order": 506,
      "p1": "0",
      "pn": "",
      "abstract": [
        "The ASVspoof initiative follows on from the first special session in Spoofing and Countermeasures for Automatic Speaker Verification (ASV) held during the 2013 edition of INTERSPEECH in Lyon, France. ASVspoof 2015, a special session held during the 2015 edition of INTERSPEECH in Dresden, Germany incorporates a standard challenge designed to support, for the first time, independent assessments of vulnerabilities to spoofing and of countermeasure performance. The initiative provides a level playing field to facilitate the comparison of different spoofing countermeasures on a common dataset, with standard protocols and metrics. While preventing as much as possible the inappropriate use of prior knowledge, the challenge also aims to stimulate the development of generalised countermeasures with potential to detect varying and unforeseen spoofing attacks. This talk will describe the challenge, data protocols and metrics and will also provide a brief summary of participation and challenge results.\n",
        ""
      ]
    },
    "wu15e_interspeech": {
      "authors": [
        [
          "Zhizheng",
          "Wu"
        ],
        [
          "Tomi",
          "Kinnunen"
        ],
        [
          "Nicholas",
          "Evans"
        ],
        [
          "Junichi",
          "Yamagishi"
        ],
        [
          "Cemal",
          "Hanil\u00e7i"
        ],
        [
          "Md.",
          "Sahidullah"
        ],
        [
          "Aleksandr",
          "Sizov"
        ]
      ],
      "title": "ASVspoof 2015: the first automatic speaker verification spoofing and countermeasures challenge",
      "original": "i15_2037",
      "page_count": 5,
      "order": 507,
      "p1": "2037",
      "pn": "2041",
      "abstract": [
        "An increasing number of independent studies have confirmed the vulnerability of automatic speaker verification (ASV) technology to spoofing. However, in comparison to that involving other biometric modalities, spoofing and countermeasure research for ASV is still in its infancy. A current barrier to progress is the lack of standards which impedes the comparison of results generated by different researchers. The ASVspoof initiative aims to overcome this bottleneck through the provision of standard corpora, protocols and metrics to support a common evaluation. This paper introduces the first edition, summaries the results and discusses directions for future challenges and research.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-462",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "sanchez15_interspeech": {
      "authors": [
        [
          "Jon",
          "Sanchez"
        ],
        [
          "Ibon",
          "Saratxaga"
        ],
        [
          "Inma",
          "Hernaez"
        ],
        [
          "Eva",
          "Navas"
        ],
        [
          "D.",
          "Erro"
        ]
      ],
      "title": "The AHOLAB RPS SSD spoofing challenge 2015 submission",
      "original": "i15_2042",
      "page_count": 5,
      "order": 508,
      "p1": "2042",
      "pn": "2046",
      "abstract": [
        "This paper introduces the Synthetic Speech Detection system developed by Aholab for the Automatic Speaker Verification Spoofing and Countermeasures Challenge (ASVspoof 2015). The detector is a classifier based on Gaussian Mixture Models that are created using the Relative Phase Shift (RPS) transformation for the phase information. Different strategies have been evaluated: modeling the specific attacks using the information provided by the ASVspoof 2015 organizers, and modeling the vocoders possibly used in the spoofing signals, using data from previous works. The evaluation results show that attack specific models work for known attacks but they do not cope with the unknown attacks correctly. When using vocoder models build with other databases, the results suggest that the followed strategy do not take advantage of the available data and thus model adaptation should be explored.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-463",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "wester15b_interspeech": {
      "authors": [
        [
          "Mirjam",
          "Wester"
        ],
        [
          "Zhizheng",
          "Wu"
        ],
        [
          "Junichi",
          "Yamagishi"
        ]
      ],
      "title": "Human vs machine spoofing detection on wideband and narrowband data",
      "original": "i15_2047",
      "page_count": 5,
      "order": 509,
      "p1": "2047",
      "pn": "2051",
      "abstract": [
        "How well do humans detect spoofing attacks directed at automatic speaker verification systems? This paper investigates the performance of humans at detecting spoofing attacks from speech synthesis and voice conversion systems. Two speaker verification tasks, in which the speakers were either humans or machines, were also conducted. The three tasks were carried out with two types of data: wideband (16kHz) and narrowband (8kHz) telephone line simulated data. Spoofing detection by humans was compared to automatic spoofing detection (ASD) algorithms. Listening tests were carefully constructed to ensure the human and automatic tasks were as similar as possible taking into consideration listener's constraints (e.g., fatigue and memory limitations). Results for human trials show the error rates on narrowband data double compared to on wideband data. The second verification task, which included only artificial speech, showed equal overall acceptance rates for both 8kHz and 16kHz. In the spoofing detection task, there was a drop in performance on most of the artificial trials as well as on human trials. At 8kHz, 20% of human trials were incorrectly classified as artificial, compared to 12% at 16kHz. The ASD algorithms also showed a drop in performance on 8kHz data, but outperformed human listeners across the board.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-464",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "xiao15_interspeech": {
      "authors": [
        [
          "Xiong",
          "Xiao"
        ],
        [
          "Xiaohai",
          "Tian"
        ],
        [
          "Steven",
          "Du"
        ],
        [
          "Haihua",
          "Xu"
        ],
        [
          "Eng Siong",
          "Chng"
        ],
        [
          "Haizhou",
          "Li"
        ]
      ],
      "title": "Spoofing speech detection using high dimensional magnitude and phase features: the NTU approach for ASVspoof 2015 challenge",
      "original": "i15_2052",
      "page_count": 5,
      "order": 510,
      "p1": "2052",
      "pn": "2056",
      "abstract": [
        "Recent improvement in text-to-speech (TTS) and voice conversion (VC) techniques presents a threat to automatic speaker verification (ASV) systems. An attacker can use the TTS or VC systems to impersonate a target speaker's voice. To overcome such a challenge, we study the detection of such synthetic speech (called spoofing speech) in this paper. We propose to use high dimensional magnitude and phase based features and long term temporal information for the task. In total, 2 types of magnitude based features and 5 types of phase based features are used. For each feature type, we build a component system using a multilayer perceptron to predict the posterior probabilities of the input features extracted from spoofing speech. The probabilities of all component systems are averaged to produce the score for final decision. When tested on the ASVspoof 2015 benchmarking task, an equal error rate (EER) of 0.29% is obtained for known spoofing types, which demonstrates the highly effectiveness of the 7 features used. For unknown spoofing types, the EER is much higher at 5.23%, suggesting that future research should be focused on improving the generalization of the techniques.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-465",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "hanilci15_interspeech": {
      "authors": [
        [
          "Cemal",
          "Hanil\u00e7i"
        ],
        [
          "Tomi",
          "Kinnunen"
        ],
        [
          "Md.",
          "Sahidullah"
        ],
        [
          "Aleksandr",
          "Sizov"
        ]
      ],
      "title": "Classifiers for synthetic speech detection: a comparison",
      "original": "i15_2057",
      "page_count": 5,
      "order": 511,
      "p1": "2057",
      "pn": "2061",
      "abstract": [
        "Automatic speaker verification (ASV) systems are highly vulnerable against spoofing attacks, also known as imposture. With recent developments in speech synthesis and voice conversion technology, it has become important to detect synthesized or voice-converted speech for the security of ASV systems. In this paper, we compare five different classifiers used in speaker recognition to detect synthetic speech. Experimental results conducted on the ASVspoof 2015 dataset show that support vector machines with generalized linear discriminant kernel (GLDS-SVM) yield the best performance on the development set with the EER of 0.12% whereas Gaussian mixture model (GMM) trained using maximum likelihood (ML) criterion with the EER of 3.01% is superior for the evaluation set.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-466",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "patel15_interspeech": {
      "authors": [
        [
          "Tanvina B.",
          "Patel"
        ],
        [
          "Hemant A.",
          "Patil"
        ]
      ],
      "title": "Combining evidences from mel cepstral, cochlear filter cepstral and instantaneous frequency features for detection of natural vs. spoofed speech",
      "original": "i15_2062",
      "page_count": 5,
      "order": 512,
      "p1": "2062",
      "pn": "2066",
      "abstract": [
        "Speech synthesis and voice conversion techniques can pose threats to current speaker verification (SV) systems. For this purpose, it is essential to develop front end systems that are able to distinguish human speech vs. spoofed speech (synthesized or voice converted). In this paper, for the ASVspoof 2015 challenge, we propose a detector based on combination of cochlear filter cepstral coefficients (CFCC) and change in instantaneous frequency (IF), (i.e., CFCCIF) to detect natural vs. spoofed speech. The CFCCIF features were extracted at frame-level and Gaussian mixture model (GMM)-based classification system was used. On the development set, the proposed features (i.e., CFCCIF) after fusion with Mel frequency cepstral coefficients (MFCC) features achieved an EER of 1.52%, which is a significant reduction from MFCC (3.26%) and CFCCIF (2.29%) alone using 12-D static features. The EER further decreases to 0.89% and 0.83% for delta and delta-delta features, respectively. Experimental results on evaluation set show that fusion of MFCC and CFCCIF works relatively well with an EER of 0.41% for known attacks and 2.013% EER for unknown attacks. On an average, fusion of MFCC and CFCCIF features provided relatively best EER of 1.211% for the challenge.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-467",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "villalba15_interspeech": {
      "authors": [
        [
          "Jes\u00fas",
          "Villalba"
        ],
        [
          "Antonio",
          "Miguel"
        ],
        [
          "Alfonso",
          "Ortega"
        ],
        [
          "Eduardo",
          "Lleida"
        ]
      ],
      "title": "Spoofing detection with DNN and one-class SVM for the ASVspoof 2015 challenge",
      "original": "i15_2067",
      "page_count": 5,
      "order": 513,
      "p1": "2067",
      "pn": "2071",
      "abstract": [
        "Speaker verification systems have achieved great performance in recent times. However, we usually measure performance on a ideal scenarios with naive impostors that do not modify their voices to impersonate the target speakers. The fact of impersonating a legitimate user is known as spoofing attack. Recent works show the vulnerability of current speaker verification technology to several types of attacks. Most of these works use non-public databases and different performance measures, which makes difficult to compare approaches. The spoofing challenge (ASVspoof 2015) tries to overcome this problem by proposing a common evaluation framework. This paper describes our submission to the challenge. We proposed to use spectral log-filter-bank and relative phase shift features as input to classifiers based on deep neural networks (DNN). The first of our classifiers used DNN posteriors to decide if the trial is spoof or non-spoof. The second used a bottleneck feature from the DNN as input to a one-class SVM. The one-class SVM models the distribution of legitimate speech, not needing spoofing data for training. We fused the score of the different classifiers to produce our final submission. Our system attained very competitive results with EER<0.05% in 9 out of 10 spoofing types.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-468",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "alam15b_interspeech": {
      "authors": [
        [
          "Md. Jahangir",
          "Alam"
        ],
        [
          "Patrick",
          "Kenny"
        ],
        [
          "Gautam",
          "Bhattacharya"
        ],
        [
          "Themos",
          "Stafylakis"
        ]
      ],
      "title": "Development of CRIM system for the automatic speaker verification spoofing and countermeasures challenge 2015",
      "original": "i15_2072",
      "page_count": 5,
      "order": 514,
      "p1": "2072",
      "pn": "2076",
      "abstract": [
        "The automatic speaker verification spoofing and countermeasures challenge 2015 provides a common framework for the evaluation of spoofing countermeasures or anti-spoofing techniques in the presence of various seen and unseen spoofing attacks. This contribution proposes a system consisting of amplitude, phase, linear prediction residual, and combined amplitude - phase-based countermeasures for the detection of spoofing attacks. In this task we use following features: Mel-frequency cepstral coefficients (MFCC), product spectrum-based cepstral coefficients, modified group delay cepstral coefficients, weighted linear prediction group delay cepstral coefficients, linear prediction residual cepstral coefficients, cosine normalized phase-based cepstral features (CNPCC), and a combination of MFCC-CNPCC. The product spectrum-based features are influenced by both the amplitude and phase spectra. The Gaussian Mixture Model (GMM) classifier is used for the discrimination of the human and spoofed speech signals. Our primary submitted system is a linear fusion of the sub-systems based on the features mentioned above with fusion weights trained on the development dataset. Experimental results on the challenge evaluation data provided an average EER (equal error rate) of 0.041%, 5.347%, and 2.69% on the known, unknown and all (known + unknown) spoofing attacks, respectively. Among all the systems product spectrum-based cepstral coefficients- and conventional MFCC (without any feature normalization)-based systems performed the best in terms of EER measure. On the known, unknown and all conditions the EER obtained by the MFCC and product spectrum-based features are 0.78% & 0.65%, 5.39% & 5.37% and 3.09% & 3.01%, respectively.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-469",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "janicki15_interspeech": {
      "authors": [
        [
          "Artur",
          "Janicki"
        ]
      ],
      "title": "Spoofing countermeasure based on analysis of linear prediction error",
      "original": "i15_2077",
      "page_count": 5,
      "order": 515,
      "p1": "2077",
      "pn": "2081",
      "abstract": [
        "In this paper a novel speaker verification spoofing countermeasure based on analysis of linear prediction error is presented. The method analyses the energy of the prediction error, prediction gain and temporal parameters related to the prediction error signal. The idea of the proposed algorithm and its implementation is described in detail. Various binary classifiers were researched to separate human and spoof classes. When tested on the corpora provided for the ASVspoof 2015 Challenge, the proposed countermeasure yielded much better results than the baseline spoofing detector based on local binary patterns (LBP). It is hoped that the proposed method can help in developing a generalised countermeasure able to detect spoofing attacks based on different variants of speech synthesis, voice conversion, and, potentially, also other spoofing algorithms.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-470"
    },
    "liu15f_interspeech": {
      "authors": [
        [
          "Yi",
          "Liu"
        ],
        [
          "Yao",
          "Tian"
        ],
        [
          "Liang",
          "He"
        ],
        [
          "Jia",
          "Liu"
        ],
        [
          "Michael T.",
          "Johnson"
        ]
      ],
      "title": "Simultaneous utilization of spectral magnitude and phase information to extract supervectors for speaker verification anti-spoofing",
      "original": "i15_2082",
      "page_count": 5,
      "order": 516,
      "p1": "2082",
      "pn": "2086",
      "abstract": [
        "Protection from spoofing attacks is an essential component of speaker verification systems. This paper proposes a novel approach to detect such attacks by utilizing supervectors derived from spectral magnitude and phase information. Three countermeasures are chosen to represent these important information. To combine different countermeasures, score fusion and an anti-spoofing supervector (ASSV) are used. Experiments conducted on ASVspoof 2015 show that the combination of magnitude and phase information obtains relative 90% improvement in terms of the equal error rate (EER) compared to the best subsystem in the development set. The two systems can also be fused to further improve the performance. In addition to accuracy improvements, the new supervector framework is extensible and allows for a more flexible interface to the back-end classifier design.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-471",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "sahidullah15_interspeech": {
      "authors": [
        [
          "Md.",
          "Sahidullah"
        ],
        [
          "Tomi",
          "Kinnunen"
        ],
        [
          "Cemal",
          "Hanil\u00e7i"
        ]
      ],
      "title": "A comparison of features for synthetic speech detection",
      "original": "i15_2087",
      "page_count": 5,
      "order": 517,
      "p1": "2087",
      "pn": "2091",
      "abstract": [
        "The performance of biometric systems based on automatic speaker recognition technology is severely degraded due to spoofing attacks with synthetic speech generated using different voice conversion (VC) and speech synthesis (SS) techniques. Various countermeasures are proposed to detect this type of attack, and in this context, choosing an appropriate feature extraction technique for capturing relevant information from speech is an important issue. This paper presents a concise experimental review of different features for synthetic speech detection task. A wide variety of features considered in this study include previously investigated features as well as some other potentially useful features for characterizing real and synthetic speech. The experiments are conducted on recently released ASVspoof 2015 corpus containing speech data from a large number of VC and SS technique. Comparative results using two different classifiers indicate that features representing spectral information in high-frequency region, dynamic information of speech, and detailed information related to subband characteristics are considerably more useful in detecting synthetic speech.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-472",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "wang15f_interspeech": {
      "authors": [
        [
          "Longbiao",
          "Wang"
        ],
        [
          "Yohei",
          "Yoshida"
        ],
        [
          "Yuta",
          "Kawakami"
        ],
        [
          "Seiichi",
          "Nakagawa"
        ]
      ],
      "title": "Relative phase information for detecting human speech and spoofed speech",
      "original": "i15_2092",
      "page_count": 5,
      "order": 518,
      "p1": "2092",
      "pn": "2096",
      "abstract": [
        "The detection of human and spoofed (synthetic/converted) speech has started to receive more attention. In this study, relative phase information extracted from a Fourier spectrum is used to detect human and spoofed speech. Because original/ natural phase information is almost entirely lost in spoofed speech using current synthesis/conversion techniques, a modified group delay based feature, the frequency derivative of the phase spectrum, has been shown effective for detecting human speech and spoofed speech. The modified group delay based phase contains both the magnitude spectrum and phase information. Therefore, the relative phase information, which contains only phase information, is expected to achieve a better spoofing detection performance. In this study, the relative phase information is also combined with the Mel-Frequency Cepstral Coefficient (MFCC) and modified group delay. The proposed method was evaluated using the \u201cASVspoof 2015: Automatic Speaker Verification Spoofing and Countermeasures Challenge\u201d dataset. The results show that the proposed relative phase information significantly outperforms the MFCC and modified group delay. The equal error rate (EER) was reduced from 1.74% of MFCC, 0.83% of modified group delay to 0.013% of relative phase. By combining the relative phase with MFCC and modified group delay, the EER was reduced to 0.002%.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-473",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "chen15j_interspeech": {
      "authors": [
        [
          "Nanxin",
          "Chen"
        ],
        [
          "Yanmin",
          "Qian"
        ],
        [
          "Heinrich",
          "Dinkel"
        ],
        [
          "Bo",
          "Chen"
        ],
        [
          "Kai",
          "Yu"
        ]
      ],
      "title": "Robust deep feature for spoofing detection \u2014 the SJTU system for ASVspoof 2015 challenge",
      "original": "i15_2097",
      "page_count": 5,
      "order": 519,
      "p1": "2097",
      "pn": "2101",
      "abstract": [
        "Recently there have been wide interests in speaker verification for various applications. Although the reported equal error rate (EER) is relatively low, many evidences show that the present speaker verification technologies can be susceptible to malicious spoofing attacks. Inspired by the great success of deep learning in the automatic speech recognition, deep neural network (DNN) based approaches are developed on the spoofing detection for the first time. In this paper, a novel DNN based robust representation is proposed for the spoofing detection to extract the representative spoofing-vector (s-vector). Then the mahalanobis distance and appropriate normalization methods are investigated to get the best system performance. Using the designed deep learning based strategy, our team obtained an impressive result on spoofing detection task, and achieved the 3rd position in the first spoofing detection challenge evaluation, i.e. ASVspoof 2015 Challenge.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-474",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "yamagishi15_interspeech": {
      "authors": [
        [
          "Junichi",
          "Yamagishi"
        ],
        [
          "Nicholas",
          "Evans"
        ]
      ],
      "title": "Automatic speaker verification spoofing and countermeasures (ASVspoof 2015): open discussion and future plans",
      "original": "i15_4107",
      "page_count": 0,
      "order": 520,
      "p1": "0",
      "pn": "",
      "abstract": [
        "This session will take the form of an open discussion with audience participation. It aims to attract feedback on the first edition of ASVspoof and also input to the focus and goals of future editions. Some of these include a focus on the integration of spoofing countermeasures with automatic speaker verification (ASV) systems, the inclusion of more diverse, advanced spoofing algorithms and a transition to text-dependent ASV.\n",
        ""
      ]
    },
    "lee15h_interspeech": {
      "authors": [
        [
          "Kyungmin",
          "Lee"
        ],
        [
          "Chiyoun",
          "Park"
        ],
        [
          "Ilhwan",
          "Kim"
        ],
        [
          "Namhoon",
          "Kim"
        ],
        [
          "Jaewon",
          "Lee"
        ]
      ],
      "title": "Applying GPGPU to recurrent neural network language model based fast network search in the real-time LVCSR",
      "original": "i15_2102",
      "page_count": 5,
      "order": 521,
      "p1": "2102",
      "pn": "2106",
      "abstract": [
        "Recurrent Neural Network Language Models (RNNLMs) have started to be used in various fields of speech recognition due to their outstanding performance. However, the high computational complexity of RNNLMs has been a hurdle in applying the RNNLM to a real-time Large Vocabulary Continuous Speech Recognition (LVCSR). In order to accelerate the speed of RNNLM-based network searches during decoding, we apply the General Purpose Graphic Processing Units (GPGPUs). This paper proposes a novel method of applying GPGPUs to RNNLM-based graph traversals. We have achieved our goal by reducing redundant computations on CPUs and amount of transfer between GPGPUs and CPUs. The proposed approach was evaluated on both WSJ corpus and in-house data. Experiments shows that the proposed approach achieves the real-time speed in various circumstances while maintaining the Word Error Rate (WER) to be relatively 10% lower than that of n-gram models.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-475",
      "author_area_id": "9",
      "author_area_label": "Speech Recognition \u2013 Architecture, Search, and Linguistic Components"
    },
    "oualil15_interspeech": {
      "authors": [
        [
          "Youssef",
          "Oualil"
        ],
        [
          "Marc",
          "Schulder"
        ],
        [
          "Hartmut",
          "Helmke"
        ],
        [
          "Anna",
          "Schmidt"
        ],
        [
          "Dietrich",
          "Klakow"
        ]
      ],
      "title": "Real-time integration of dynamic context information for improving automatic speech recognition",
      "original": "i15_2107",
      "page_count": 5,
      "order": 522,
      "p1": "2107",
      "pn": "2111",
      "abstract": [
        "The use of prior situational/contextual knowledge about a given task can significantly improve Automatic Speech Recognition (ASR) performance. This is typically done through adaptation of acoustic or language models if data is available, or using knowledge-based rescoring. The main adaptation techniques, however, are either domain-specific, which makes them inadequate for other tasks, or static and offline, and therefore cannot deal with dynamic knowledge. To circumvent this problem, we propose a real-time system which dynamically integrates situational context into ASR. The context integration is done either post-recognition, in which case a weighted Levenshtein distance between the ASR hypotheses and the context information, based on the ASR confidence scores, is proposed to extract the most likely sequence of spoken words;, or pre-recognition, where the search space is adjusted to the new situational knowledge through adaptation of the finite state machine modeling the spoken language. Experiments conducted on 3 hours of Air Traffic Control (ATC) data achieved a reduction of the Command Error Rate (CmdER), which is used as evaluation metric in the ATC domain, by a factor of 4 compared to using no contextual knowledge.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-476",
      "author_area_id": "9",
      "author_area_label": "Speech Recognition \u2013 Architecture, Search, and Linguistic Components"
    },
    "allauzen15_interspeech": {
      "authors": [
        [
          "Cyril",
          "Allauzen"
        ],
        [
          "Michael",
          "Riley"
        ]
      ],
      "title": "Rapid vocabulary addition to context-dependent decoder graphs",
      "original": "i15_2112",
      "page_count": 5,
      "order": 523,
      "p1": "2112",
      "pn": "2116",
      "abstract": [
        "We describe how to efficiently add new vocabulary directly to an existing optimized ASR decoder graph. The augmented decoder graph is represented by two weighted finite-state transducers, a primary graph that represents the static portion and a secondary graph that represents the dynamic portion, together with a mapping that specifies that some states in the two graphs are to be merged. Determinism is obtained by excluding from the secondary graph any prefixes already present in the primary graph. Correct context-dependency is obtained by including in the primary graph all prefixes needed to properly merge with the secondary graph. We report experiments comparing this approach to an existing one that requires on-the-fly construction of the decoder graph.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-477",
      "author_area_id": "9",
      "author_area_label": "Speech Recognition \u2013 Architecture, Search, and Linguistic Components"
    },
    "xu15d_interspeech": {
      "authors": [
        [
          "Hainan",
          "Xu"
        ],
        [
          "Guoguo",
          "Chen"
        ],
        [
          "Daniel",
          "Povey"
        ],
        [
          "Sanjeev",
          "Khudanpur"
        ]
      ],
      "title": "Modeling phonetic context with non-random forests for speech recognition",
      "original": "i15_2117",
      "page_count": 5,
      "order": 524,
      "p1": "2117",
      "pn": "2121",
      "abstract": [
        "Modern speech recognition systems typically cluster triphone phonetic contexts using decision trees. In this paper we describe a way to build multiple complementary decision trees from the same data, for the purpose of system combination. We do this by jointly building the decision trees using an objective function that has an added entropy term to encourage diversity among the decision trees. After the trees are built, the systems are built in the standard way and the emission probabilities are combined during decoding. Experiments on multiple datasets show gains from the use of multiple trees, at the expense of evaluating multiple models in test time.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-478",
      "author_area_id": "9",
      "author_area_label": "Speech Recognition \u2013 Architecture, Search, and Linguistic Components"
    },
    "lecouteux15_interspeech": {
      "authors": [
        [
          "Benjamin",
          "Lecouteux"
        ],
        [
          "Didier",
          "Schwab"
        ]
      ],
      "title": "Ant colony algorithm applied to automatic speech recognition graph decoding",
      "original": "i15_2122",
      "page_count": 5,
      "order": 525,
      "p1": "2122",
      "pn": "2126",
      "abstract": [
        "In this article we propose an original approach that allows the decoding of Automatic Speech Recognition Graphs by using a constructive algorithm based on ant colonies. In classical approaches, when a graph is decoded with higher order language models; the algorithm must expand the graph in order to develop each new observed n-gram. This extension process increases the computation time and memory consumption. We propose to use an ant colony algorithm in order to explore ASR graphs with a new language model, without the necessity of expanding it. We first present results based on the TED English corpus where 2-grams graph are decoded with a 4-grams language model. Then, we show that our approach performs better than a conventional Viterbi algorithm when computing time is constrained and allows a highly threaded decoding process with a single graph and a strict control of computation time and memory consumption.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-479",
      "author_area_id": "9",
      "author_area_label": "Speech Recognition \u2013 Architecture, Search, and Linguistic Components"
    },
    "gysel15_interspeech": {
      "authors": [
        [
          "Christophe Van",
          "Gysel"
        ],
        [
          "Leonid",
          "Velikovich"
        ],
        [
          "Ian",
          "McGraw"
        ],
        [
          "Fran\u00e7oise",
          "Beaufays"
        ]
      ],
      "title": "Garbage modeling for on-device speech recognition",
      "original": "i15_2127",
      "page_count": 5,
      "order": 526,
      "p1": "2127",
      "pn": "2131",
      "abstract": [
        "User interactions with mobile devices increasingly depend on voice as a primary input modality. Due to the disadvantages of sending audio across potentially spotty network connections for speech recognition, in recent years there has been growing attention to performing recognition on-device. The limited computational resources, however, typically require additional model constraints. In this work, we explore the task of on-device utterance verification, wherein the recognizer must transcribe an utterance if it is in a target set or reject it as being out of domain. We present a data-driven methodology for mining tens of thousands of target phrases from an existing corpus. We then compare two common garbage-modeling approaches to utterance verification: a sub-word rejection model and a white-listed n-gram model. We examine a deficiency of the sub-word modeling approach and introduce a novel modification that makes use of common prefixes between targeted phrases and non-targeted phrases. We show good performance in the trade-off between recall and word error rate using both the prefix and white-listed n-gram approaches. Finally, we evaluate the prefix-based approach in a hybrid setting where rejected instances are sent to a server-side recognizer.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-480",
      "author_area_id": "9",
      "author_area_label": "Speech Recognition \u2013 Architecture, Search, and Linguistic Components"
    },
    "xu15e_interspeech": {
      "authors": [
        [
          "Haihua",
          "Xu"
        ],
        [
          "Van Hai",
          "Do"
        ],
        [
          "Xiong",
          "Xiao"
        ],
        [
          "Eng Siong",
          "Chng"
        ]
      ],
      "title": "A comparative study of BNF and DNN multilingual training on cross-lingual low-resource speech recognition",
      "original": "i15_2132",
      "page_count": 5,
      "order": 527,
      "p1": "2132",
      "pn": "2136",
      "abstract": [
        "This paper presents a comparative study of bottle-neck feature (BNF) and Deep Neural Network (DNN) multilingual training for low-resource language speech recognition. Besides, we also compared the system performances after fine-tuning. The evaluation was conducted on the IARPA Babel data. The source languages are Cantonese, Pashto, Tagalog, and Turkish, while the target languages are Vietnamese and Tamil. As compared to the monolingual baseline systems, the BNF and DNN methods similarly achieved relative WER reductions of 3.0-5.1% on the Limited Language Pack (LLP) data and 6.1-8.6% on the Very LLP (VLLP) data. By fine-tuning, the BNF method further gained significant performance improvement, while the DNN method obtained marginal gains. Overall, we observed the DNN method performs worse on the smaller size data (VLLP), as well as on the noisier data (Tamil) of the target languages, in the cross-lingual transfer and fine-tuning cases respectively.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-481",
      "author_area_id": "9",
      "author_area_label": "Speech Recognition \u2013 Architecture, Search, and Linguistic Components"
    },
    "ratajczak15_interspeech": {
      "authors": [
        [
          "Martin",
          "Ratajczak"
        ],
        [
          "Sebastian",
          "Tschiatschek"
        ],
        [
          "Franz",
          "Pernkopf"
        ]
      ],
      "title": "Neural higher-order factors in conditional random fields for phoneme classification",
      "original": "i15_2137",
      "page_count": 5,
      "order": 528,
      "p1": "2137",
      "pn": "2141",
      "abstract": [
        "We explore neural higher-order input-dependent factors in linear-chain conditional random fields (LC-CRFs) for sequence labeling. Higher-order LC-CRFs with linear factors are well-established for sequence labeling tasks, but they lack the ability to model non-linear dependencies. These non-linear dependencies, however, can be efficiently modelled by neural higher-order input-dependent factors which map sub-sequences of inputs to sub-sequences of outputs. This mapping is important in many tasks, in particular, for phoneme classification where the phone representations strongly depend on the context phonemes. Experimental results for phoneme classification with LC-CRFs and neural higher-order factors confirm this fact and we achieve the best ever reported phoneme classification performance on TIMIT, i.e. a phoneme error rate of 15.8%. Furthermore, we show that the success is not obvious as linear high-order factors degrade phoneme classification performance on TIMIT.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-482",
      "author_area_id": "9",
      "author_area_label": "Speech Recognition \u2013 Architecture, Search, and Linguistic Components"
    },
    "jalalvand15_interspeech": {
      "authors": [
        [
          "Shahab",
          "Jalalvand"
        ],
        [
          "Daniele",
          "Falavigna"
        ]
      ],
      "title": "Stacked auto-encoder for ASR error detection and word error rate prediction",
      "original": "i15_2142",
      "page_count": 5,
      "order": 529,
      "p1": "2142",
      "pn": "2146",
      "abstract": [
        "Recently, Stacked Auto-Encoders (SAE) have been successfully used for learning imbalanced datasets. In this paper, for the first time, we propose to use a Neural Network classifier furnished by an SAE structure for detecting the errors made by a strong Automatic Speech Recognition (ASR) system. Error detection on an automatic transcription provided by a \u201cstrong\u201d ASR system, i.e. exhibiting a small word error rate, is difficult due to the limited number of \u201cpositive\u201d examples (i.e. words erroneously recognized) available for training a binary classifier. In this paper we investigate and compare different types of classifiers for automatically detecting ASR errors, including the one based on a stacked auto-encoder architecture. We show the effectiveness of the latter by measuring and comparing performance on the automatic transcriptions of an English corpus collected from TED talks. Performance of each investigated classifier is evaluated both via receiving operating curve and via a measure, called mean absolute error, related to the quality in predicting the corresponding word error rate. The results demonstrates that the classifier based on SAE detects the ASR errors better than the other classification methods.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-483",
      "author_area_id": "9",
      "author_area_label": "Speech Recognition \u2013 Architecture, Search, and Linguistic Components"
    },
    "parida15_interspeech": {
      "authors": [
        [
          "Satyabrata",
          "Parida"
        ],
        [
          "Ashok Kumar",
          "Pattem"
        ],
        [
          "Prasanta Kumar",
          "Ghosh"
        ]
      ],
      "title": "Estimation of the air-tissue boundaries of the vocal tract in the mid-sagittal plane from electromagnetic articulograph data",
      "original": "i15_2147",
      "page_count": 5,
      "order": 530,
      "p1": "2147",
      "pn": "2151",
      "abstract": [
        "Electromagnetic articulograph (EMA) provides movement data of sensors attached to a few flesh points on different speech articulators including lips, jaw, and tongue while a subject speaks. In this work, we quantify the amount of information these flesh points provide about the vocal tract (VT) shape in the mid-sagittal plane. VT shape is described by the air-tissue boundaries, which are obtained manually from the recordings by real-time magnetic resonance imaging (rtMRI) of a set of utterances spoken by a subject, from whom the EMA recordings of the same set of utterances are also available. We propose a two-stage approach for reconstructing the VT shape from the EMA data. The first stage involves a co-registration of the EMA data with the VT shape from the rtMRI frames. The second stage involves the estimation of the air-tissue boundaries from the co-registered EMA points. Co-registration is done by a spatio-temporal alignment of the VT shapes from the rtMRI frames and EMA sensor data, while radial basis function (RBF) network is used for estimating the air-tissue boundaries (ATBs). Experiments with the EMA and rtMRI recordings of five sentences spoken by one male and one female speakers show that the VT shape in the mid-sagittal plane can be recovered from the EMA flesh points with an average reconstruction error of 2.55 mm and 2.75 mm respectively.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-484",
      "author_area_id": "1",
      "author_area_label": "Speech Perception, Production, and Acquisition"
    },
    "canevari15_interspeech": {
      "authors": [
        [
          "Claudia",
          "Canevari"
        ],
        [
          "Leonardo",
          "Badino"
        ],
        [
          "Luciano",
          "Fadiga"
        ]
      ],
      "title": "A new Italian dataset of parallel acoustic and articulatory data",
      "original": "i15_2152",
      "page_count": 5,
      "order": 531,
      "p1": "2152",
      "pn": "2156",
      "abstract": [
        "In this paper we introduce a new Italian dataset consisting of simultaneous recordings of continuous speech and trajectories of important vocal tract articulators (i.e. tongue, lips, incisors) tracked by Electromagnetic Articulography (EMA). It includes more than 500 sentences uttered in citation condition by three speakers, one male (cnz) and two females (lls, olm), for approximately 2 hours of speech material.   Such dataset has been designed to be large enough and phonetically balanced so as to be used in speech applications (e.g. speech recognition systems).   We then test our speaker-dependent articulatory Deep-Neural-Network Hidden-Markov-Model (DNN-HMM) phone recognizer on the set of data recorded from the cnz speaker.   We show that phone recognition results are comparable to the ones that we previously obtained using two well-known British-English datasets with EMA data of equivalent vocal tract articulators. That suggests that the new set of data is a equally useful and coherent resource.   The dataset is the session 1 of a larger Italian corpus, called Multi-SPeaKing-style-Articulatory (MSPKA) corpus, including parallel audio and articulatory data in diverse speaking styles (e.g. read, hyperarticulated and hypoarticulated speech). It is freely available at http://www.mspkacorpus.it for research purposes. In the immediate future the whole corpus will be released.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-485",
      "author_area_id": "1",
      "author_area_label": "Speech Perception, Production, and Acquisition"
    },
    "csapo15b_interspeech": {
      "authors": [
        [
          "Tam\u00e1s G\u00e1bor",
          "Csap\u00f3"
        ],
        [
          "Steven M.",
          "Lulich"
        ]
      ],
      "title": "Error analysis of extracted tongue contours from 2d ultrasound images",
      "original": "i15_2157",
      "page_count": 5,
      "order": 532,
      "p1": "2157",
      "pn": "2161",
      "abstract": [
        "The goal of this study was to characterize errors involved in obtaining midsagittal tongue contours from two-dimensional ultrasound image sequences. Toward that end, two basic experiments were conducted. First, manual tongue contours were obtained from 1,145 tongue ultrasound images recorded from four speakers during production of the sentence ` I owe you a yoyo', and the uncertainty associated with the contours was quantified. Second, tongue contours from the same images were obtained using the EdgeTrak, TongueTrack, and AutoTrace algorithms, and these were compared quantitatively with the manual tongue contours. Three basic error types associated with the tongue contours are identified, indicating areas in need of improvement in future algorithmic developments. Depending on the speaker, RMS errors for the algorithmically obtained contours ranged from 1.76 to 7.11 mm, and the standard deviation of manual contours ranged from 0.97 to 2.07 mm.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-486",
      "author_area_id": "1",
      "author_area_label": "Speech Perception, Production, and Acquisition"
    },
    "bandini15_interspeech": {
      "authors": [
        [
          "Andrea",
          "Bandini"
        ],
        [
          "Slim",
          "Ouni"
        ],
        [
          "Piero",
          "Cosi"
        ],
        [
          "Silvia",
          "Orlandi"
        ],
        [
          "Claudia",
          "Manfredi"
        ]
      ],
      "title": "Accuracy of a markerless acquisition technique for studying speech articulators",
      "original": "i15_2162",
      "page_count": 5,
      "order": 533,
      "p1": "2162",
      "pn": "2166",
      "abstract": [
        "The main disadvantages of the existing methods for studying speech articulators (such as electromagnetic and optoelectronic systems) are the high cost and the discomfort to participants or patients. The aim of this work is to introduce a completely markerless low-cost 3D tracking technique in the context of speech articulation, and then compare it with a well-established marker-based one to evaluate the performances. A Kinect-like device was used in conjunction with an existing face tracking algorithm to track lips movements in 3D without markers. The method was tested on two subjects uttering 200 words and 100 sentences. For most of points of the lips the RMSE ranged between 1 and 3 mm. Although the image resolution used in this experiment was low, these results are very promising. Nevertheless, further studies should consider higher video resolutions in order to obtain better results.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-487",
      "author_area_id": "1",
      "author_area_label": "Speech Perception, Production, and Acquisition"
    },
    "chi15_interspeech": {
      "authors": [
        [
          "Yujie",
          "Chi"
        ],
        [
          "Kiyoshi",
          "Honda"
        ],
        [
          "Jianguo",
          "Wei"
        ],
        [
          "Hui",
          "Feng"
        ],
        [
          "Jianwu",
          "Dang"
        ]
      ],
      "title": "Measuring oral and nasal airflow in production of Chinese plosive",
      "original": "i15_2167",
      "page_count": 5,
      "order": 534,
      "p1": "2167",
      "pn": "2171",
      "abstract": [
        "This study reports a new integrated device for measuring oral and nasal airflow with lossless speech recording with preliminary results on Chinese plosives. An acoustically transparent fiber-made mask acts both as a material for airflow resistance and a support for partitioning oral and nasal chambers. Two pressure sensors placed directly on the mask measure air pressure signals that correspond to the airflow rate. With this device, oral-nasal airflow rate and speech sound were recorded from two Chinese speakers. The corpus was made of two-syllable Chinese words (CVCV, C is a plosive) in a carrier sentence. Slow and rapid variations of nasal airflow signals were obtained with clear speech and oral airflow data, which were analyzed to speculate the elevation and vibration of the velum. The results show that (i) AC component of the nasal airflow is greater in /i/ than in /a/, which supports transvelar nasal coupling during high vowels and obstruents; (ii) word-medial plosives tend to be partially or fully voiced, as inferred from augmented AC amplitude of the nasal airflow during the closure; (iii) short rise or drop of the nasal airflow appears to reflect a quick elevation or decline of the velum.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-488"
    },
    "drioli15_interspeech": {
      "authors": [
        [
          "Carlo",
          "Drioli"
        ],
        [
          "Gian Luca",
          "Foresti"
        ]
      ],
      "title": "Enhanced videokymographic data analysis based on vocal folds dynamics modeling",
      "original": "i15_2172",
      "page_count": 5,
      "order": 535,
      "p1": "2172",
      "pn": "2176",
      "abstract": [
        "The automatic analysis of temporal patterns of vocal folds motion and the tracking of glottal cues such as folds edge position or glottal area, has recently become a topic of interest in the field of laryngeal video imaging. We discuss here the use of a numerically simulated model of the folds motion within a video analysis context, for the analysis of videokymographic data and glottal cues segmentation. The proposed algorithm exploits both visual and acoustic data related to the glottal excitation, to estimate the parameters of the model. The trained model is then used to enhance the analysis and segmentation of visual glottal cues, i.e. the folds edge displacement and glottal area. Objective measures are reported of the accuracy with which the visual glottal cues and the acoustic voice emission are represented by the model. The method is illustrated and assessed on data from different subjects.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-489",
      "author_area_id": "1",
      "author_area_label": "Speech Perception, Production, and Acquisition"
    },
    "kolb15_interspeech": {
      "authors": [
        [
          "Andrew J.",
          "Kolb"
        ],
        [
          "Michael T.",
          "Johnson"
        ],
        [
          "Jeffrey",
          "Berry"
        ]
      ],
      "title": "Interpolation of tongue fleshpoint kinematics from combined EMA position and orientation data",
      "original": "i15_2177",
      "page_count": 5,
      "order": 536,
      "p1": "2177",
      "pn": "2181",
      "abstract": [
        "Articulatory data such as that collected via electromagnetic articulography (EMA) are valuable for many speech applications, including speech modeling, recognition, and synthesis. Nearly all current EMA applications and methods focus on the use of positional sensor data, even though modern 3D-EMA systems also capture sensor orientation, which provides significant additional information about articulator posture and vocal tract shape. To address this problem, this paper introduces a new method for interpolating untracked tongue fleshpoint positions from the combined position and orientation data of three EMA sensors on the tongue, using additional reference sensors for evaluating interpolation error. Comparison of interpolated and measured data illustrated effectiveness of the new method in providing additional tongue shape and position features. The results suggest that analytic methods that combine sensor position and orientation data are able to improve the characterization of tongue kinematics even using a small number of EMA sensors.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-490",
      "author_area_id": "1",
      "author_area_label": "Speech Perception, Production, and Acquisition"
    },
    "andrademiranda15_interspeech": {
      "authors": [
        [
          "Gustavo",
          "Andrade-Miranda"
        ],
        [
          "Nathalie Henrich",
          "Bernardoni"
        ],
        [
          "Juan Ignacio",
          "Godino-Llorente"
        ]
      ],
      "title": "A new technique for assessing glottal dynamics in speech and singing by means of optical-flow computation",
      "original": "i15_2182",
      "page_count": 5,
      "order": 537,
      "p1": "2182",
      "pn": "2186",
      "abstract": [
        "The use of high-speed videoendoscopy (HSV) in combination with image-processing techniques is the most promising approach to investigate vocal-folds vibration and laryngeal dynamics in speech and singing. The current challenge is to provide facilitative and informative playbacks for clinical and research purposes. We present three new facilitative playbacks using an optical-flow framework (OF). Optical-flow techniques are widely used in the field of computer vision for tracking unidentified moving objects in video sequences. The application of OF computation to HSV images is investigated. The advantages, drawbacks, and the complementarity to existing methods are discussed. The method has been tested on a database of 60 HSV sequences which covers different voice qualities for spoken and sung vowels. The new data representations have been compared with commonly-used facilitative playbacks. They provide additional information on the temporal dynamics of glottal vibratory movements during glottal closing and opening phases.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-491",
      "author_area_id": "1",
      "author_area_label": "Speech Perception, Production, and Acquisition"
    },
    "kochetov15_interspeech": {
      "authors": [
        [
          "Alexei",
          "Kochetov"
        ],
        [
          "Phil",
          "Howson"
        ]
      ],
      "title": "On the incompatibility of trilling and palatalization: a single-subject study of sustained apical and uvular trills",
      "original": "i15_2187",
      "page_count": 5,
      "order": 538,
      "p1": "2187",
      "pn": "2191",
      "abstract": [
        "The production of trills requires precise articulatory and aerodynamic settings, which appear to be hardly compatible with secondary palatalization \u2014 the raising and fronting of the tongue body. Yet, the precise reasons for this incompatibility are still poorly understood, largely given the paucity of articulatory work on trills. Moreover, previous investigations of palatalized trills have been limited to apicals (alveolars/dentals), raising the question of whether the suggested factors are general to all lingual trills, including uvulars, or are specific to apical trills. This paper presents an exploratory investigation of sustained palatalized and non-palatalized apical (phonemic) and uvular trills (idiolectal) produced by a native speaker of Russian in word-initial and word-final positions. Acoustic, ultrasound, and electropalatography data collected from the speaker reveal some striking similarities between palatalized apical and uvular trills. Although these can be sustained for over a half-second (having 10-20 contacts), they show a gradually increasing rate of vibration compared to non-palatalized trills, often ending in approximant-like realizations. This can be attributed to the gradual raising/fronting of the tongue body for the palatalization gesture, the peak of which tends to occur at the offset the trill constriction for both apicals and uvulars, and regardless of the position.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-492",
      "author_area_id": "1",
      "author_area_label": "Speech Perception, Production, and Acquisition"
    },
    "zhu15_interspeech": {
      "authors": [
        [
          "Pengcheng",
          "Zhu"
        ],
        [
          "Lei",
          "Xie"
        ],
        [
          "Yunlin",
          "Chen"
        ]
      ],
      "title": "Articulatory movement prediction using deep bidirectional long short-term memory based recurrent neural networks and word/phone embeddings",
      "original": "i15_2192",
      "page_count": 5,
      "order": 539,
      "p1": "2192",
      "pn": "2196",
      "abstract": [
        "Automatic prediction of articulatory movements from speech or text can be beneficial for many applications such as speech recognition and synthesis. A recent approach has reported state-of-the-art performance in speech-to-articulatory prediction using feed forward neural networks. In this paper, we investigate the feasibility of using bidirectional long short-term memory based recurrent neural networks (BLSTM-RNNs) in articulatory movement prediction because they have long-context trajectory modeling ability. We show on the MNGU0 dataset that BLSTM-RNN apparently outperforms feed forward networks and pushes the state-of-the-art RMSE from 0.885 mm to 0.565 mm. On the other hand, predicting articulatory information from text heavily relies on handcrafted linguistic and prosodic features, e.g., POS and TOBI labels. In this paper, we propose to use word and phone embeddings to substitute these manual features. Word/phone embedding features are automatically learned from unlabeled text data by a neural network language model. We show that word and phone embeddings can achieve comparable performance without using POS and TOBI features. More promisingly, combining the conventional full feature set with phone embedding, the lowest RMSE is achieved.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-493",
      "author_area_id": "1",
      "author_area_label": "Speech Perception, Production, and Acquisition"
    },
    "ruiz15_interspeech": {
      "authors": [
        [
          "Nicholas",
          "Ruiz"
        ],
        [
          "Qin",
          "Gao"
        ],
        [
          "William",
          "Lewis"
        ],
        [
          "Marcello",
          "Federico"
        ]
      ],
      "title": "Adapting machine translation models toward misrecognized speech with text-to-speech pronunciation rules and acoustic confusability",
      "original": "i15_2247",
      "page_count": 5,
      "order": 540,
      "p1": "2247",
      "pn": "2251",
      "abstract": [
        "In the spoken language translation pipeline, machine translation systems that are trained solely on written bitexts are often unable to recover from speech recognition errors due to the mismatch in training data. We propose a novel technique to simulate the errors generated by an ASR system, using the ASR system's pronunciation dictionary and language model. Lexical entries in the pronunciation dictionary are converted into phoneme sequences using a text-to-speech (TTS) analyzer and stored in a phoneme-to-word translation model. The translation model and ASR language model are combined into a phoneme-to-word MT system that \u201cdamages\u201d clean texts to look like ASR outputs based on acoustic confusions. Training texts are TTS-converted and damaged into synthetic ASR data for use as adaptation data for training a speech translation system. Our proposed technique yields consistent improvements in translation quality on English-French lectures.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-494",
      "author_area_id": "12",
      "author_area_label": "Spoken Language Processing \u2013 Translation, Information Retrieval, and Resources"
    },
    "bechet15_interspeech": {
      "authors": [
        [
          "Frederic",
          "Bechet"
        ],
        [
          "Benoit",
          "Favre"
        ],
        [
          "Mickael",
          "Rouvier"
        ]
      ],
      "title": "\u201cspeech is silver, but silence is golden\u201d: improving speech-to-speech translation performance by slashing users input",
      "original": "i15_2252",
      "page_count": 5,
      "order": 541,
      "p1": "2252",
      "pn": "2256",
      "abstract": [
        "Speech-to-speech translation is a challenging task mixing two of the most ambitious Natural Language Processing challenges: Machine Translation (MT) and Automatic Speech Recognition (ASR). Recent advances in both fields have led to operational systems achieving good performance when used in matching conditions with those of ASR and MT models training. Regardless of the quality of these models, errors are inevitable due to some technical limitations of the systems (e.g. closed vocabulary) and intrinsic ambiguities of spoken languages. However all ASR and MT errors don't have the same impact on the usability of a given speech-to-speech dialog system: some can be very benign, unconsciously corrected by users, some can damage the understanding between users and eventually lead the dialog to a failure. We present in this paper a strategy focusing on ASR error segments that have a high negative impact on MT performance. We propose a method that consists firstly in automatically detecting these erroneous segments then secondly estimating their impact on MT. We show that removing such segments prior to translation can lead to a significant decrease in translation error rate, even without any correction strategy.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-495",
      "author_area_id": "12",
      "author_area_label": "Spoken Language Processing \u2013 Translation, Information Retrieval, and Resources"
    },
    "ng15_interspeech": {
      "authors": [
        [
          "Raymond W. M.",
          "Ng"
        ],
        [
          "Kashif",
          "Shah"
        ],
        [
          "Lucia",
          "Specia"
        ],
        [
          "Thomas",
          "Hain"
        ]
      ],
      "title": "A study on the stability and effectiveness of features in quality estimation for spoken language translation",
      "original": "i15_2257",
      "page_count": 5,
      "order": 542,
      "p1": "2257",
      "pn": "2261",
      "abstract": [
        "A quality estimation (QE) approach informed with machine translation (MT) and speech recognition (ASR) features has recently shown to improve the performance of a spoken language translation (SLT) system in an in-domain scenario. When domain mismatch is progressively introduced in the MT and ASR systems, the SLT system's performance naturally degrades. The use of QE to improve SLT performance has not been studied in this context. In this paper we investigate the effectiveness of QE under this setting. Our experiments showed that across moderate levels of domain mismatches, QE led to consistent translation improvements of around 0.4 in BLEU score. The QE system relies on 116 features derived from the ASR and MT system input and output. Feature analysis was conducted to understand the information sources contributing the most to performance improvements. LDA dimension reduction was used to summarise effective features into sets as small as 3 without affecting the SLT performance. By inspecting the principal components, eight features including the acoustic model scores and count-based word statistics on the bilingual text were found to be critically important, leading to a further boost of around 0.1 BLEU score over the full set of features. These findings provide interesting possibilities for further work by incorporating the effective QE features in SLT system training or decoding.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-496",
      "author_area_id": "12",
      "author_area_label": "Spoken Language Processing \u2013 Translation, Information Retrieval, and Resources"
    },
    "pelemans15b_interspeech": {
      "authors": [
        [
          "Joris",
          "Pelemans"
        ],
        [
          "Tom",
          "Vanallemeersch"
        ],
        [
          "Kris",
          "Demuynck"
        ],
        [
          "Hugo",
          "Van hamme"
        ],
        [
          "Patrick",
          "Wambacq"
        ]
      ],
      "title": "Efficient language model adaptation for automatic speech recognition of spoken translations",
      "original": "i15_2262",
      "page_count": 5,
      "order": 543,
      "p1": "2262",
      "pn": "2266",
      "abstract": [
        "Direct integration of translation model (TM) probabilities into a language model (LM) with the purpose of improving automatic speech recognition (ASR) of spoken translations typically requires a number of complex operations for each sentence. Many if not all of the LM probabilities need to be updated, the model needs to be renormalized and the ASR system needs to load a new, updated LM for each sentence. In computer-aided translation environments the time loss induced by these complex operations seriously reduces the potential of ASR as an efficient input method.   In this paper we present a novel LM adaptation technique that drastically reduces the complexity of each of these operations. The technique consists of LM probability updates using exponential weights based on TM probabilities for each sentence and does not enforce probability renormalization. Instead of storing each resulting language model in its entirety, we only store the update weights which also reduces disk storage and loading time during ASR. Experiments on Dutch read speech translated from English show that both disk storage and recognition time drop dramatically compared to a baseline system that employs a more conventional way of updating the LM.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-497",
      "author_area_id": "12",
      "author_area_label": "Spoken Language Processing \u2013 Translation, Information Retrieval, and Resources"
    },
    "mieno15_interspeech": {
      "authors": [
        [
          "Takashi",
          "Mieno"
        ],
        [
          "Graham",
          "Neubig"
        ],
        [
          "Sakriani",
          "Sakti"
        ],
        [
          "Tomoki",
          "Toda"
        ],
        [
          "Satoshi",
          "Nakamura"
        ]
      ],
      "title": "Speed or accuracy? a study in evaluation of simultaneous speech translation",
      "original": "i15_2267",
      "page_count": 5,
      "order": 544,
      "p1": "2267",
      "pn": "2271",
      "abstract": [
        "Simultaneous speech translation is a technology that attempts to reduce the delay inherent in speech translation by beginning translation before the end of explicit sentence boundaries. Despite best efforts, there is still often a trade-off between speed and accuracy in these systems, with systems with less delay also achieving lower accuracy. However, somewhat surprisingly, there is no previous work examining the relative importance of speed and accuracy, and thus given two systems with various speeds and accuracies, it is difficult to say with certainty which is better. In this paper, we make the first steps towards evaluation of simultaneous speech translation systems in consideration of both speed and accuracy. We collect user evaluations of speech translation results with different levels of accuracy and delay, and using this data to learn the parameters of an evaluation measure that can judge the trade-off between these two factors. Based on these results, we find that considering both accuracy and delay in the evaluation of speech translation results helps improve correlations with human judgements, and that users placed higher relative importance on reducing delay when results were presented through text, rather than speech.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-498",
      "author_area_id": "12",
      "author_area_label": "Spoken Language Processing \u2013 Translation, Information Retrieval, and Resources"
    },
    "junczysdowmunt15_interspeech": {
      "authors": [
        [
          "Marcin",
          "Junczys-Dowmunt"
        ],
        [
          "Pawe\u0142",
          "Przybysz"
        ],
        [
          "Arleta",
          "Staszuk"
        ],
        [
          "Eun-Kyoung",
          "Kim"
        ],
        [
          "Jaewon",
          "Lee"
        ]
      ],
      "title": "Large scale speech-to-text translation with out-of-domain corpora using better context-based models and domain adaptation",
      "original": "i15_2272",
      "page_count": 5,
      "order": 545,
      "p1": "2272",
      "pn": "2276",
      "abstract": [
        "In this paper, we described the process of building a large-scale speech-to-text pipeline. Two target domains, daily conversations and travel-related conversations between two agents, for the English-German language pair (both directions) are examined. The SMT component is built from out-of-domain but freely-available bilingual and monolingual data. We make use of most of the known available resources to examine the effects of unrestricted data and large scale models. A naive baseline delivers solid results in terms of MT-quality. Extending the baseline with context-based translation model features like operations sequence models, higher-order class-based language models, and additional web-scale word-based language models leads to a system that significantly outperforms the baseline. Domain adaption is performed by separately weighting the influence of the out-of-domain subcorpora. This is explored for translation models and language models yielding significant improvements in both cases. Automatic and manual evaluation results are provided for raw MT-quality and ASR+MT-quality.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-499",
      "author_area_id": "12",
      "author_area_label": "Spoken Language Processing \u2013 Translation, Information Retrieval, and Resources"
    },
    "kenny15_interspeech": {
      "authors": [
        [
          "Patrick",
          "Kenny"
        ],
        [
          "Themos",
          "Stafylakis"
        ],
        [
          "Md. Jahangir",
          "Alam"
        ],
        [
          "Marcel",
          "Kockmann"
        ]
      ],
      "title": "An i-vector backend for speaker verification",
      "original": "i15_2307",
      "page_count": 5,
      "order": 546,
      "p1": "2307",
      "pn": "2311",
      "abstract": [
        "We propose a new approach to the problem of uncertainty modeling in text-dependent speaker verification where speaker factors are used as the feature representation. The state-of-the-art backend in this situation consists in using point estimates of speaker factors to model the joint distribution of pairs of enrollment and test feature vectors under the same-speaker hypothesis. We develop a version of this backend that works with Baum-Welch statistics instead of point estimates. The likelihood ratio calculations for speaker verification turn out to be formally equivalent to evidence calculations with i-vector extractors having non-standard normal priors. Experiments show that this i-vector backend performs well on Part III of the RSR2015 dataset.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-500",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "correia15_interspeech": {
      "authors": [
        [
          "Joana",
          "Correia"
        ],
        [
          "Alessio",
          "Brutti"
        ],
        [
          "Alberto",
          "Abad"
        ]
      ],
      "title": "Multi-channel speaker verification based on total variability modelling",
      "original": "i15_2312",
      "page_count": 5,
      "order": 547,
      "p1": "2312",
      "pn": "2316",
      "abstract": [
        "In this work we address the speaker verification task in domestic environments, monitored by multiple distributed microphones. In particular, we focus on the problem of mismatch in the propagation channel between the enrolment stage, which occurs at a fixed position, and the test phase which could happen in any location of a multi-room apartment. Building upon the Total Variability framework and cosine distance scoring, we present two multi-channel solutions: one based on multi-condition training and the other based on several channel-dependent systems. An experimental analysis on a multi-channel multi-room reverberant data-set shows that the proposed solutions are robust against changes in the speaker position and orientation and improve the performance of the single-channel matched baselines.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-501",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "li15c_interspeech": {
      "authors": [
        [
          "Na",
          "Li"
        ],
        [
          "Man-Wai",
          "Mak"
        ]
      ],
      "title": "SNR-invariant PLDA modeling for robust speaker verification",
      "original": "i15_2317",
      "page_count": 5,
      "order": 548,
      "p1": "2317",
      "pn": "2321",
      "abstract": [
        "In spite of the great success of the i-vector/PLDA framework, speaker verification in noisy environments remains a challenge. To compensate for the variability of i-vectors caused by different levels of background noise, this paper proposes a new framework, namely SNR-invariant PLDA, for robust speaker verification. By assuming that i-vectors extracted from utterances falling within a narrow SNR range share similar SNR-specific information, the paper introduces an SNR factor to the conventional PLDA model. Then, the SNR-related variability and the speaker-related variability embedded in the i-vectors are modeled by the SNR factor and the speaker factor, respectively. Accordingly, an i-vector is represented by a linear combination of three components: speaker, SNR, and channel. During verification, the variability due to SNR and channels are marginalized out when computing the marginal likelihood ratio. Experiments based on NIST 2012 SRE show that SNR-invariant PLDA achieves superior performance when compared with the conventional PLDA and SNR-dependent mixture of PLDA.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-502",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "rahman15b_interspeech": {
      "authors": [
        [
          "Md. Hafizur",
          "Rahman"
        ],
        [
          "David",
          "Dean"
        ],
        [
          "Ahilan",
          "Kanagasundaram"
        ],
        [
          "Sridha",
          "Sridharan"
        ]
      ],
      "title": "Investigating in-domain data requirements for PLDA training",
      "original": "i15_2322",
      "page_count": 5,
      "order": 549,
      "p1": "2322",
      "pn": "2326",
      "abstract": [
        "This paper analyzes the limitations upon the amount of in-domain (NIST SREs) data required for training a probabilistic linear discriminant analysis (PLDA) speaker verification system based on out-domain (Switchboard) total variability subspaces. By limiting the number of speakers, the number of sessions per speaker and the length of active speech per session available in the target domain for PLDA training, we investigated the relative effect of these three parameters on PLDA speaker verification performance in the NIST 2008 and NIST 2010 speaker recognition evaluation datasets. Experimental results indicate that while these parameters depend highly on each other, to beat out-domain PLDA training, more than 10 seconds of active speech should be available for at least 4 sessions/speaker for a minimum of 800 speakers. If further data is available, considerable improvement can be made over solely out-domain PLDA training.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-503",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "glembek15_interspeech": {
      "authors": [
        [
          "Ond\u0159ej",
          "Glembek"
        ],
        [
          "Pavel",
          "Mat\u011bjka"
        ],
        [
          "Old\u0159ich",
          "Plchot"
        ],
        [
          "Jan",
          "Pe\u0161\u00e1n"
        ],
        [
          "Luk\u00e1\u0161",
          "Burget"
        ],
        [
          "Petr",
          "Schwarz"
        ]
      ],
      "title": "Migrating i-vectors between speaker recognition systems using regression neural networks",
      "original": "i15_2327",
      "page_count": 5,
      "order": 550,
      "p1": "2327",
      "pn": "2331",
      "abstract": [
        "This paper studies the scenario of migrating from one i-vector-based speaker recognition system (SRE) to another, i.e. comparing the i-vectors produced by one system with those produced by another system. System migration would typically be motivated by deploying a system with improved recognition accuracy, e.g. because of technological upgrade, or because of the necessity of processing new kind of data, etc. Unfortunately, such migration is very likely to result in the incompatibility between the new and the original i-vectors and, therefore, in the inability of comparing the two. This work studies various topologies of Regression Neural Networks for transforming i-vectors from three different systems so that \u2014 with slight loss in the accuracy \u2014 they are compatible with the reference system. We present the results on the NIST SRE 2010 telephone condition.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-504",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "kanagasundaram15_interspeech": {
      "authors": [
        [
          "Ahilan",
          "Kanagasundaram"
        ],
        [
          "David",
          "Dean"
        ],
        [
          "Sridha",
          "Sridharan"
        ]
      ],
      "title": "Improving PLDA speaker verification using WMFD and linear-weighted approaches in limited microphone data conditions",
      "original": "i15_2332",
      "page_count": 5,
      "order": 551,
      "p1": "2332",
      "pn": "2336",
      "abstract": [
        "This paper proposes the addition of a weighted median Fisher discriminator (WMFD) projection prior to length-normalised Gaussian probabilistic linear discriminant analysis (GPLDA) modelling in order to compensate the additional session variation. In limited microphone data conditions, a linear-weighted approach is introduced to increase the influence of microphone speech dataset. The linear-weighted WMFD-projected GPLDA system shows improvements in EER and DCF values over the pooled LDA- and WMFD-projected GPLDA systems in interview-interview condition as WMFD projection extracts more speaker discriminant information with limited number of sessions/ speaker data, and linear-weighted GPLDA approach estimates reliable model parameters with limited microphone data.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-505",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "gobl15_interspeech": {
      "authors": [
        [
          "Christer",
          "Gobl"
        ],
        [
          "Irena",
          "Yanushevskaya"
        ],
        [
          "Ailbhe N\u00ed",
          "Chasaide"
        ]
      ],
      "title": "The relationship between voice source parameters and the maxima dispersion quotient (MDQ)",
      "original": "i15_2337",
      "page_count": 5,
      "order": 552,
      "p1": "2337",
      "pn": "2341",
      "abstract": [
        "This study examines the relationship between the Maxima Dispersion Quotient (MDQ), a recently proposed measure of the tense-lax dimension of voice quality, and voice source parameters manually measured from glottal flow data, where there is linguistically and paralinguistically determined voice source modulation. MDQ was found to correlate most closely to the open quotient (OQ) and the RD parameter. The paralinguistically varying data, which involved more extensive voice source modulation than the linguistic, also showed a higher degree of correlation with these parameters, and higher correlations overall with a range of voice source parameters. The high correlation with OQ and RD found in these analyses would suggest that MDQ can be a useful, additional parameter for the analysis of glottal source dynamics.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-506",
      "author_area_id": "2",
      "author_area_label": "Phonetics, Phonology, and Prosody"
    },
    "airaksinen15_interspeech": {
      "authors": [
        [
          "Manu",
          "Airaksinen"
        ],
        [
          "Tom",
          "B\u00e4ckstr\u00f6m"
        ],
        [
          "Paavo",
          "Alku"
        ]
      ],
      "title": "Glottal inverse filtering based on quadratic programming",
      "original": "i15_2342",
      "page_count": 5,
      "order": 553,
      "p1": "2342",
      "pn": "2346",
      "abstract": [
        "This study presents a novel quadratic programming based approach to glottal inverse filtering. The proposed method aims to jointly model the effect of the vocal tract and lip radiation with a single filter whose coefficients are optimized using the quadratic programming framework. This allows the proposed method to directly estimate the glottal flow of speech, which mitigates the problem of non-flat closed phases in inverse filtering estimates. The proposed method was objectively evaluated using a synthetic Liljencrants-Fant model based test set of sustained vowels containing a wide variety of phonation types and fundamental periods. The results indicate that the proposed method is robust to changes in f0 and state-of-the-art quality results were obtained for high pitch voices, when f0 is in the range 330 to 450Hz.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-507",
      "author_area_id": "2",
      "author_area_label": "Phonetics, Phonology, and Prosody"
    },
    "narendra15_interspeech": {
      "authors": [
        [
          "N. P.",
          "Narendra"
        ],
        [
          "K. Sreenivasa",
          "Rao"
        ]
      ],
      "title": "Automatic detection of creaky voice using epoch parameters",
      "original": "i15_2347",
      "page_count": 5,
      "order": 554,
      "p1": "2347",
      "pn": "2351",
      "abstract": [
        "This paper proposes a method based on epoch parameters for detection of creaky voice in speech signal. The epoch parameters characterizing the source of excitation considered in this work are number of epochs in a frame, strength of excitation of epochs and epoch intervals. Analysis of epoch parameters estimated from zero-frequency filtering method with different window sizes is carried out. Distinct variations in the epoch parameters are observed for modal and creaky voiced regions. Variances of epoch parameters are used as input features to train a neural network classifier for identifying creaky regions. Performance evaluation results indicate that the proposed method performs significantly better than the existing creaky detection methods on different speech databases.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-508",
      "author_area_id": "2",
      "author_area_label": "Phonetics, Phonology, and Prosody"
    },
    "bundgaardnielsen15b_interspeech": {
      "authors": [
        [
          "Rikke Louise",
          "Bundgaard-Nielsen"
        ],
        [
          "Brett",
          "Baker"
        ]
      ],
      "title": "Perception of voicing in the absence of native voicing experience",
      "original": "i15_2352",
      "page_count": 5,
      "order": 555,
      "p1": "2352",
      "pn": "2356",
      "abstract": [
        "50 years of speech perception research provide a rich literature on cross- and second language (L2) perception of stop consonant contrasts such as /p b/, /t d/, and /k g/ which differ systematically in the relative timing of oral stop release and the onset of vocal fold vibration (voice onset time: VOT). This research has focused primarily on two observations: 1) that nonnative listeners automatically use their native VOT contrast boundary in an unfamiliar language, irrespective of whether this language shares the boundary or not, and 2) that even highly proficient L2 language users often perceive L2 VOT-based contrasts in a way that is consistent with their L1, even after decades of L2 acquisition. No work has, hitherto, examined VOT-based contrast discrimination by L1 speakers of languages without any VOT-based stop contrast. In the following, we present two studies of speakers in such a scenario, showing that even extensive L2 experience is insufficient for L2 learners without native (L1) voicing experience to acquire such a distinction.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-509",
      "author_area_id": "2",
      "author_area_label": "Phonetics, Phonology, and Prosody"
    },
    "kreiman15_interspeech": {
      "authors": [
        [
          "Jody",
          "Kreiman"
        ],
        [
          "Soo Jin",
          "Park"
        ],
        [
          "Patricia A.",
          "Keating"
        ],
        [
          "Abeer",
          "Alwan"
        ]
      ],
      "title": "The relationship between acoustic and perceived intraspeaker variability in voice quality",
      "original": "i15_2357",
      "page_count": 4,
      "order": 556,
      "p1": "2357",
      "pn": "2360",
      "abstract": [
        "Little is known about intraspeaker changes in voice across changing speaking situations in everyday life. In this study, we examined acoustic variations between and within 5 talkers and their effect on the likelihood that voice samples would not be identified as coming from the same talker. Talkers were drawn from a large database recorded to capture everyday variations in vocal characteristics. Nine samples of /a/, recorded on three different days, were examined for each talker. Acoustic characteristics were estimated using VoiceSauce and analysis-by-synthesis, and listeners judged whether pairs of voices came from the same or two different talkers. Results indicate that interspeaker variability in voice quality exceeds intraspeaker variability, but differences are smaller than expected. As predicted by models that treat voice quality as an auditory pattern, the acoustic attributes associated with incorrect \u201cdifferent speaker\u201d responses varied from talker to talker, depending on the particular characteristics of the voice in question.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-510",
      "author_area_id": "2",
      "author_area_label": "Phonetics, Phonology, and Prosody"
    },
    "jiao15_interspeech": {
      "authors": [
        [
          "Li",
          "Jiao"
        ],
        [
          "Qiuwu",
          "Ma"
        ],
        [
          "Ting",
          "Wang"
        ],
        [
          "Yi",
          "Xu"
        ]
      ],
      "title": "Perceptual cues of whispered tones: are they really special?",
      "original": "i15_2361",
      "page_count": 5,
      "order": 557,
      "p1": "2361",
      "pn": "2365",
      "abstract": [
        "This paper reports our initial findings on whether Mandarin Chinese has developed effective strategies to convey tonal information in whispered speech. We recorded phonated and whispered tones in monosyllabic words, and analyzed the acoustic properties of the tonal contrasts. We then generated amplitude-modulated noise based on both the phonated and whispered utterances and used them as stimuli in a tone perception experiment, together with the original phonated and whispered speech. Results showed that, once turned into amplitude-modulated noise, originally phonated and whispered speech had similar perceptual patterns, and that these patterns resembled those of whispered tones. The acoustic analysis showed that properties corresponding to tonal contrasts in whispered speech already existed in the phonated tones, and there was no evidence of enhancement of these properties in whispering. Overall, therefore, Mandarin may not have developed highly special strategies to enhance tonal contrast in whispered speech.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-511",
      "author_area_id": "2",
      "author_area_label": "Phonetics, Phonology, and Prosody"
    },
    "morioka15_interspeech": {
      "authors": [
        [
          "Tsuyoshi",
          "Morioka"
        ],
        [
          "Tomoharu",
          "Iwata"
        ],
        [
          "Takaaki",
          "Hori"
        ],
        [
          "Tetsunori",
          "Kobayashi"
        ]
      ],
      "title": "Multiscale recurrent neural network based language model",
      "original": "i15_2366",
      "page_count": 5,
      "order": 558,
      "p1": "2366",
      "pn": "2370",
      "abstract": [
        "We describe a novel recurrent neural network-based language model (RNNLM) dealing with multiple time-scales of contexts. The RNNLM is now a technical standard in language modeling because it remembers some lengths of contexts. However, the RNNLM can only deal with a single time-scale of a context, regardless of the subsequent words and topic of the spoken utterance, even though the optimal time-scale of the context can vary under such conditions. In contrast, our multiscale RNNLM enables incorporating with sufficient flexibility, and it makes use of various time-scales of contexts simultaneously and with proper weights for predicting the next word. Experimental comparisons carried out in large vocabulary spontaneous speech recognition demonstrate that introducing the multiple time-scales of contexts into the RNNLM yielded improvements over existing RNNLMs in terms of the perplexity and word error rate.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-512",
      "author_area_id": "9",
      "author_area_label": "Speech Recognition \u2013 Architecture, Search, and Linguistic Components"
    },
    "irie15_interspeech": {
      "authors": [
        [
          "Kazuki",
          "Irie"
        ],
        [
          "Ralf",
          "Schl\u00fcter"
        ],
        [
          "Hermann",
          "Ney"
        ]
      ],
      "title": "Bag-of-words input for long history representation in neural network-based language models for speech recognition",
      "original": "i15_2371",
      "page_count": 5,
      "order": 559,
      "p1": "2371",
      "pn": "2375",
      "abstract": [
        "In most of previous works on neural network based language models (NNLMs), the words are represented as 1-of-N encoded feature vectors. In this paper we investigate an alternative encoding of the word history, known as bag-of-words (BOW) representation of a word sequence, and use it as an additional input feature to the NNLM. Both the feedforward neural network (FFNN) and the long short-term memory recurrent neural network (LSTM-RNN) language models (LMs) with additional BOW input are evaluated on an English large vocabulary automatic speech recognition (ASR) task. We show that the BOW features significantly improve both the perplexity (PP) and the word error rate (WER) of a standard FFNN LM. In contrast, the LSTM-RNN LM does not benefit from such an explicit long context feature. Therefore the performance gap between feedforward and recurrent architectures for language modeling is reduced. In addition, we revisit the cache based LM, a seeming analog of the BOW for the count based LM, which was unsuccessful for ASR in the past. Although the cache is able to improve the perplexity, we only observe a very small reduction in WER.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-513",
      "author_area_id": "9",
      "author_area_label": "Speech Recognition \u2013 Architecture, Search, and Linguistic Components"
    },
    "emami15_interspeech": {
      "authors": [
        [
          "Ahmad",
          "Emami"
        ]
      ],
      "title": "Efficient machine translation decoding with slow language models",
      "original": "i15_2376",
      "page_count": 4,
      "order": 560,
      "p1": "2376",
      "pn": "2379",
      "abstract": [
        "Efficient decoding has been a fundamental problem in machine translation research. Usually a significant part of the computational complexity is found in the language model cost computations. If slow language models, such as neural network or maximum-entropy models are used, the computational complexity can be so high as to render decoding impractical. In this paper we propose a method to efficiently integrate slow language models in machine translation decoding. We specifically employ neural network language models in a hierarchical phrase-based translation decoder and achieve more than 15 times speed-up versus directly integrating the neural network models. The speed-up is achieved without any noticeable drop in machine translation output quality, as measured by automatic evaluation metrics. Our proposed method is general enough to be applied to a wide variety of models and decoders.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-514",
      "author_area_id": "9",
      "author_area_label": "Speech Recognition \u2013 Architecture, Search, and Linguistic Components"
    },
    "masumura15b_interspeech": {
      "authors": [
        [
          "Ryo",
          "Masumura"
        ],
        [
          "Taichi",
          "Asami"
        ],
        [
          "Takanobu",
          "Oba"
        ],
        [
          "Hirokazu",
          "Masataki"
        ],
        [
          "Sumitaka",
          "Sakauchi"
        ],
        [
          "Akinori",
          "Ito"
        ]
      ],
      "title": "Latent words recurrent neural network language models",
      "original": "i15_2380",
      "page_count": 5,
      "order": 561,
      "p1": "2380",
      "pn": "2384",
      "abstract": [
        "This paper proposes a novel language modeling approach called latent word recurrent neural network language model, which solves the problems present in both recurrent neural network language models (RNNLMs) and latent word language models (LWLMs). The proposed model has a soft class structure based on a latent variable space as well as LWLM, where the latent variable space is modeled using RNNLM. From the viewpoint of RNNLMs, the proposed model can be considered as a soft class RNNLM with a vast latent variable space. In contrast, from the viewpoint of LWLMs, the proposed model can be considered as an LWLM that uses the RNN structure for latent variable modeling instead of the n-gram structure. This paper also details the parameter inference method and two kinds of usages for natural language processing tasks. Our experiments show effectiveness of the proposed model on a perplexity evaluation for the Penn Treebank corpus and an automatic speech recognition evaluation for Japanese spontaneous speech tasks.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-515",
      "author_area_id": "9",
      "author_area_label": "Speech Recognition \u2013 Architecture, Search, and Linguistic Components"
    },
    "chunwijitra15_interspeech": {
      "authors": [
        [
          "Vataya",
          "Chunwijitra"
        ],
        [
          "Ananlada",
          "Chotimongkol"
        ],
        [
          "Chai",
          "Wutiwiwatchai"
        ]
      ],
      "title": "Combining multiple-type input units using recurrent neural network for LVCSR language modeling",
      "original": "i15_2385",
      "page_count": 5,
      "order": 562,
      "p1": "2385",
      "pn": "2389",
      "abstract": [
        "In this paper, we investigate the use of a Recurrent Neural Network (RNN) in combining hybrid input types, namely word and pseudo-morpheme (PM) for Thai LVCSR language modeling. Similar to other neural network frameworks, there is no restriction on RNN input types. To exploit this advantage, the input vector of a proposed hybrid RNN language model (RNNLM) is a concatenated vector of word and PM vectors. After the first-pass decoding with an n-gram LM, a word-based lattice is expanded to include the corresponding PMs of each word. The hybrid RNNLM is then used to re-score the hybrid lattice in the second-pass decoding. We tested our hybrid RNNLM on two recognition tasks: broadcast news transcription and mobile speech-to-speech translation. The proposed model achieved better recognition performance than a baseline word-based RNNLM as hybrid input types provide more flexible unit choices for language model re-scoring. The computational complexity of a full-hybrid RNNLM can be reduced by limiting the input vector to include only frequent words and PMs. In a reduced-hybrid RNNLM, the size of the input vector can be reduced by half which can considerably save both training and decoding time without affecting recognition accuracy.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-516"
    },
    "gangireddy15_interspeech": {
      "authors": [
        [
          "Siva Reddy",
          "Gangireddy"
        ],
        [
          "Steve",
          "Renals"
        ],
        [
          "Yoshihiko",
          "Nankaku"
        ],
        [
          "Akinobu",
          "Lee"
        ]
      ],
      "title": "Prosodically-enhanced recurrent neural network language models",
      "original": "i15_2390",
      "page_count": 5,
      "order": 563,
      "p1": "2390",
      "pn": "2394",
      "abstract": [
        "Recurrent neural network language models have been shown to consistently reduce the word error rates (WERs) of large vocabulary speech recognition tasks. In this work we propose to enhance the RNNLMs with prosodic features computed using the context of the current word. Since it is plausible to compute the prosody features at the word and syllable level we have trained the models on prosody features computed at both these levels. To investigate the effectiveness of proposed models we report perplexity and WER for two speech recognition tasks, Switchboard and TED. We observed substantial improvements in perplexity and small improvements in WER.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-517",
      "author_area_id": "9",
      "author_area_label": "Speech Recognition \u2013 Architecture, Search, and Linguistic Components"
    },
    "janke15_interspeech": {
      "authors": [
        [
          "Matthias",
          "Janke"
        ],
        [
          "Michael",
          "Wand"
        ]
      ],
      "title": "Biosignal-based spoken communication: welcome and introduction",
      "original": "i15_4108",
      "page_count": 0,
      "order": 564,
      "p1": "0",
      "pn": "",
      "abstract": [
        "Based on the increased interest and major progress in biosignal-based speech processing during the last years we organize the Special Session entitled \u201cBiosignal-based Spoken Communication\u201d. We will give a short overview of the submitted papers and the covered topics.\n",
        ""
      ]
    },
    "anderson15_interspeech": {
      "authors": [
        [
          "Peter",
          "Anderson"
        ],
        [
          "Negar M.",
          "Harandi"
        ],
        [
          "Scott",
          "Moisik"
        ],
        [
          "Ian",
          "Stavness"
        ],
        [
          "Sidney",
          "Fels"
        ]
      ],
      "title": "A comprehensive 3d biomechanically-driven vocal tract model including inverse dynamics for speech research",
      "original": "i15_2395",
      "page_count": 5,
      "order": 565,
      "p1": "2395",
      "pn": "2399",
      "abstract": [
        "We introduce a biomechanical model of oropharyngeal structures that adds the soft-palate, pharynx, and larynx to our previous models of jaw, skull, hyoid, tongue, and face in a unified model. The model includes a comprehensive description of the upper airway musculature, using point-to-point muscles that may either be embedded within the deformable structures or operate externally. The airway is described by an air-tight mesh that fits and deforms with the surrounding articulators, which enables dynamic coupling to our articulatory speech synthesizer. We demonstrate that the biomechanics, in conjunction with the skinning, supports a range from physically realistic to simplified vocal tract geometries to investigate different approaches to aeroacoustic modeling of vocal tract. Furthermore, our model supports inverse modeling to support investigation of plausible muscle activation patterns to generate speech.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-518",
      "author_area_id": "1",
      "author_area_label": "Speech Perception, Production, and Acquisition"
    },
    "mcloughlin15_interspeech": {
      "authors": [
        [
          "Ian",
          "McLoughlin"
        ],
        [
          "Yan",
          "Song"
        ]
      ],
      "title": "Low frequency ultrasonic voice activity detection using convolutional neural networks",
      "original": "i15_2400",
      "page_count": 5,
      "order": 566,
      "p1": "2400",
      "pn": "2404",
      "abstract": [
        "Low frequency ultrasonic mouth state detection uses reflected audio chirps from the face in the region of the mouth to determine lip state, whether open, closed or partially open. The chirps are located in a frequency range just above the threshold of human hearing and are thus both inaudible as well as unaffected by interfering speech, yet can be produced and sensed using inexpensive equipment. To determine mouth open or closed state, and hence form a measure of voice activity detection, this recently invented technique relies upon the difference in the reflected chirp caused by resonances introduced by the open or partially open mouth cavity. Voice activity is then inferred from lip state through patterns of mouth movement, in a similar way to video-based lip-reading technologies. This paper introduces a new metric based on spectrogram features extracted from the reflected chirp, with a convolutional neural network classification back-end, that yields excellent performance without needing the periodic resetting of the template closed-mouth reflection required by the original technique.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-519",
      "author_area_id": "1",
      "author_area_label": "Speech Perception, Production, and Acquisition"
    },
    "bocquelet15_interspeech": {
      "authors": [
        [
          "Florent",
          "Bocquelet"
        ],
        [
          "Thomas",
          "Hueber"
        ],
        [
          "Laurent",
          "Girin"
        ],
        [
          "Christophe",
          "Savariaux"
        ],
        [
          "Blaise",
          "Yvert"
        ]
      ],
      "title": "Real-time control of a DNN-based articulatory synthesizer for silent speech conversion: a pilot study",
      "original": "i15_2405",
      "page_count": 5,
      "order": 567,
      "p1": "2405",
      "pn": "2409",
      "abstract": [
        "This article presents a pilot study on the real-time control of an articulatory synthesizer based on deep neural network (DNN), in the context of silent speech interface. The underlying hypothesis is that a silent speaker could benefit from real-time audio feedback to regulate his/her own production. In this study, we use 3D electromagnetic-articulography (EMA) to capture speech articulation, a DNN to convert EMA to spectral trajectories in real-time, and a standard vocoder excited by white noise for audio synthesis. As shown by recent literature on silent speech, adaptation of the articulo-acoustic modeling process is needed to account for possible inconsistencies between the initial training phase and practical usage conditions. In this study, we focus on different sensor setups across sessions (for the same speaker). Model adaptation is performed by cascading another neural network to the DNN used for articulatory-to-acoustic mapping. The intelligibility of the synthetic speech signal converted in real-time is evaluated using both objective and perceptual measurements.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-520",
      "author_area_id": "1",
      "author_area_label": "Speech Perception, Production, and Acquisition"
    },
    "fabre15_interspeech": {
      "authors": [
        [
          "Diandra",
          "Fabre"
        ],
        [
          "Thomas",
          "Hueber"
        ],
        [
          "Florent",
          "Bocquelet"
        ],
        [
          "Pierre",
          "Badin"
        ]
      ],
      "title": "Tongue tracking in ultrasound images using eigentongue decomposition and artificial neural networks",
      "original": "i15_2410",
      "page_count": 5,
      "order": 568,
      "p1": "2410",
      "pn": "2414",
      "abstract": [
        "This paper describes a machine learning approach for extracting automatically the tongue contour in ultrasound images. This method is developed in the context of visual articulatory biofeedback for speech therapy. The goal is to provide a speaker with an intuitive visualization of his/her tongue movement, in real-time, and with minimum human intervention. Contrary to most widely used techniques based on active contours, the proposed method aims at exploiting the information of all image pixels to infer the tongue contour. For that purpose, a compact representation of each image is extracted using a PCA-based decomposition technique (named EigenTongue). Artificial neural networks are then used to convert the extracted visual features into control parameters of a PCA-based tongue contour model. The proposed method is evaluated on 9 speakers, using data recorded with the ultrasound probe hold manually (as in the targeted application). Speaker-dependent experiments demonstrated the effectiveness of the proposed method (with an average error of ~1.3 mm when training from 80 manually annotated images), even when the tongue contour is poorly imaged. The performance was significantly lower in speaker-independent experiments ( i.e. when estimating contours on an unknown speaker), likely due to anatomical differences across speakers.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-521",
      "author_area_id": "1",
      "author_area_label": "Speech Perception, Production, and Acquisition"
    },
    "wang15g_interspeech": {
      "authors": [
        [
          "Jun",
          "Wang"
        ],
        [
          "Seongjun",
          "Hahm"
        ]
      ],
      "title": "Speaker-independent silent speech recognition with across-speaker articulatory normalization and speaker adaptive training",
      "original": "i15_2415",
      "page_count": 5,
      "order": 569,
      "p1": "2415",
      "pn": "2419",
      "abstract": [
        "Silent speech recognition (SSR) converts non-audio information (e.g., articulatory information) to speech. SSR has potential to enable laryngectomees to produce synthesized speech with a natural sounding voice. Despite its recent advances, current SSR research has largely relied on speaker-dependent recognition. High degree of variation in articulatory patterns across different talkers has been a barrier for developing effective speaker-independent SSR approaches. Speaker-independent approaches, however, are critical for reducing the large amount of training data required from each user; only limited articulatory samples are often available for individuals, due to the logistic difficulty of articulatory data collection. In this paper, we investigated speaker-independent silent speech recognition from tongue and lip movement data with two models that address the across-talker variation: Procrustes matching, a physiological approach, to minimize the across-talker physiological differences of articulators, and speaker adaptive training, a data-driven approach. A silent speech data set was collected using an electromagnetic articulograph (EMA) from five English speakers (while they were silently articulating phrases) and was used to evaluate the two speaker-independent SSR approaches. The long-standing Gaussian mixture model-hidden Markov models and recently available deep neural network-hidden Markov model were used as the recognizers. Experimental results showed the effectiveness of both normalization approaches.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-522",
      "author_area_id": "1",
      "author_area_label": "Speech Perception, Production, and Acquisition"
    },
    "diener15_interspeech": {
      "authors": [
        [
          "Lorenz",
          "Diener"
        ],
        [
          "Matthias",
          "Janke"
        ],
        [
          "Tanja",
          "Schultz"
        ]
      ],
      "title": "Codebook clustering for unit selection based EMG-to-speech conversion",
      "original": "i15_2420",
      "page_count": 5,
      "order": 570,
      "p1": "2420",
      "pn": "2424",
      "abstract": [
        "This paper reports on our recent advances in using Unit Selection to directly synthesize speech from facial surface electromyographic (EMG) signals generated by movement of the articulatory muscles during speech production.   We achieve a robust Unit Selection mapping by using a more sophisticated unit codebook. This codebook is generated from a set of base units using a two stage unit clustering process. The units are first clustered based on the audio-, and afterwards on the EMG feature vectors they cover, and a new codebook is generated using these cluster assignments. We evaluate different cluster counts for both stages and revisit our evaluation of unit sizes in light of this clustering approach.   Our final system achieves a significantly better Mel-Cepstral distortion score than the Unit Selection based EMG-to-Speech conversion system from our previous work while, due to the reduced codebook size, taking less time to perform the conversion.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-523",
      "author_area_id": "1",
      "author_area_label": "Speech Perception, Production, and Acquisition"
    },
    "mirbagheri15_interspeech": {
      "authors": [
        [
          "Majid",
          "Mirbagheri"
        ],
        [
          "Bradley",
          "Ekin"
        ],
        [
          "Les",
          "Atlas"
        ],
        [
          "Adrian K. C.",
          "Lee"
        ]
      ],
      "title": "Flexible tracking of auditory attention",
      "original": "i15_2425",
      "page_count": 5,
      "order": 571,
      "p1": "2425",
      "pn": "2429",
      "abstract": [
        "Auditory selective attention plays a central role in the human capacity to reliably process complex sounds in multi-source environments. The ability to track the attentional state of individuals in such environments is of interest to neuroscientists and engineers due to its importance in the study of attention-related disorders and its potential application in the hearing aid and advertising industries. The underlying neural basis of auditory attention is not well established, however evidence exists to suggest that cortical activity entrainment to the temporal envelope of speech is modulated by attention. Leveraging this finding, we introduce a probabilistic approach based on Hidden Markov Model Regression (HMMR) to decode the attentional state of the listener with respect to a given speech stimulus. Our method is novel in that it uses only the target stream to detect the attended segments, while existing methods require knowledge about the target and distractor. This is a particular advantage in real-world applications where the number of sources are often time-variant and unknown to the decoder. We use synthetic data to evaluate robustness and tracking capability, and real electrophysiological data to demonstrate how the proposed method achieves accuracies commensurate to BCI (Brain Computer Interface) systems deployed in the field.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-524",
      "author_area_id": "1",
      "author_area_label": "Speech Perception, Production, and Acquisition"
    },
    "janke15b_interspeech": {
      "authors": [
        [
          "Matthias",
          "Janke"
        ],
        [
          "Michael",
          "Wand"
        ]
      ],
      "title": "Biosignal-based spoken communication: panel and discussion",
      "original": "i15_4109",
      "page_count": 0,
      "order": 572,
      "p1": "0",
      "pn": "",
      "abstract": [
        "After all Special Session participants have presented their papers, we will outline some factors and future work steps that are relevant for all biosignal-based spoken communication topics.\n",
        ""
      ]
    },
    "mirsamadi15_interspeech": {
      "authors": [
        [
          "Seyedmahdad",
          "Mirsamadi"
        ],
        [
          "John H. L.",
          "Hansen"
        ]
      ],
      "title": "A study on deep neural network acoustic model adaptation for robust far-field speech recognition",
      "original": "i15_2430",
      "page_count": 5,
      "order": 573,
      "p1": "2430",
      "pn": "2434",
      "abstract": [
        "Even though deep neural network acoustic models provide an increased degree of robustness in automatic speech recognition, there is still a large performance drop in the task of far-field speech recognition in reverberant and noisy environments. In this study, we explore DNN adaptation techniques to achieve improved robustness to environmental mismatch for far-field speech recognition. In contrast to many recent studies investigating the role of feature processing in DNN-HMM systems, we focus on adaptation of a clean-trained DNN model to speech data captured by a distant-talking microphone in a target environment with substantial reverberation and noise. We show that significant performance gains can be obtained by discriminatively estimating a set of adaptation parameters to compensate the mismatch between a clean-trained model and a small set of noisy and reverberant adaptation data. Using various adaptation strategies, relative word error rate improvements of up to 16% could be obtained on the single-channel task of the recent Aspire challenge.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-525",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition \u2013 Signal Processing, Acoustic Modeling, Robustness, and Adaptation"
    },
    "mimura15_interspeech": {
      "authors": [
        [
          "Masato",
          "Mimura"
        ],
        [
          "Shinsuke",
          "Sakai"
        ],
        [
          "Tatsuya",
          "Kawahara"
        ]
      ],
      "title": "Speech dereverberation using long short-term memory",
      "original": "i15_2435",
      "page_count": 5,
      "order": 574,
      "p1": "2435",
      "pn": "2439",
      "abstract": [
        "Recently, neural networks have been used for not only phone recognition but also denoising and dereverberation. However, the conventional denoising deep autoencoder (DAE) based on the feed-forward structure is not capable of handling very long speech frames of reverberation. LSTM can be effectively trained to reduce the average error between the enhanced signal and the original clean signal by considering the effect of the long past time frames. In this paper, we demonstrate that considering as long as the maximum reverberation time of the database is effective. Since the effect of reverberation varies depending on the phone-class of the whole speech context, we augment the input of the autoencoder with the phone-class information of the past frames as well as the current frame and call this version of the LSTM autoencoder pLSTM. In the speech recognition experiment using the data set of Reverb Challenge 2014, the LSTM front-end reduced the WER of the multi-condition DNN-HMM by 14.5%, and the use of the phone class feature yielded in pLSTM further improvement of 7.5%. The performance with the pLSTM is comparable to that of pDAE, while the number of parameters is only 1/25-1/8.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-526",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition \u2013 Signal Processing, Acoustic Modeling, Robustness, and Adaptation"
    },
    "peddinti15_interspeech": {
      "authors": [
        [
          "Vijayaditya",
          "Peddinti"
        ],
        [
          "Guoguo",
          "Chen"
        ],
        [
          "Daniel",
          "Povey"
        ],
        [
          "Sanjeev",
          "Khudanpur"
        ]
      ],
      "title": "Reverberation robust acoustic modeling using i-vectors with time delay neural networks",
      "original": "i15_2440",
      "page_count": 5,
      "order": 575,
      "p1": "2440",
      "pn": "2444",
      "abstract": [
        "In reverberant environments there are long term interactions between speech and corrupting sources. In this paper a time delay neural network (TDNN) architecture, capable of learning long term temporal relationships and translation invariant representations, is used for reverberation robust acoustic modeling. Further, iVectors are used as an input to the neural network to perform instantaneous speaker and environment adaptation, providing 10% relative improvement in word error rate. By sub-sampling the outputs at TDNN layers across time steps, training time is reduced. Using a parallel training algorithm we show that the TDNN can be trained on ~ 5500 hours of speech data in 3 days using up to 32 GPUs. The TDNN is shown to provide results competitive with state of the art systems in the IARPA ASpIRE challenge, with 27.7% WER on the dev_test set.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-527",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition \u2013 Signal Processing, Acoustic Modeling, Robustness, and Adaptation"
    },
    "kumar15e_interspeech": {
      "authors": [
        [
          "Kshitiz",
          "Kumar"
        ],
        [
          "Chaojun",
          "Liu"
        ],
        [
          "Yifan",
          "Gong"
        ]
      ],
      "title": "Delta-melspectra features for noise robustness to DNN-based ASR systems",
      "original": "i15_2445",
      "page_count": 4,
      "order": 576,
      "p1": "2445",
      "pn": "2448",
      "abstract": [
        "Deep-neural-networks (DNNs) have significantly improved automatic speech recognition (ASR) accuracy over a range of speech scenarios. However noise-robustness is still a challenge to DNNs, where compared to clean, accuracy degrades significantly for noisy environments. Many of the current DNN-based ASR engines use log-MelSpectra features, along with features from temporal-difference in delta and delta-delta features. In this work we introduce delta-MelSpectra features to seek significant gains for DNNs in noisy environments, where we demonstrate that temporal-difference directly in MelSpectra domain can provide superior noise-robust features. We validate our delta-MelSpectra features over a multistyle trained DNN-ASR system; we tested on a large scale WindowsPhone client data, and obtained 17% and 12% relative reduction in word-error-rate (WER) for noisy and clean environments, respectively.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-528",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition \u2013 Signal Processing, Acoustic Modeling, Robustness, and Adaptation"
    },
    "mitra15_interspeech": {
      "authors": [
        [
          "Vikramjit",
          "Mitra"
        ],
        [
          "Julien Van",
          "Hout"
        ],
        [
          "Mitchell",
          "McLaren"
        ],
        [
          "Wen",
          "Wang"
        ],
        [
          "Martin",
          "Graciarena"
        ],
        [
          "Dimitra",
          "Vergyri"
        ],
        [
          "Horacio",
          "Franco"
        ]
      ],
      "title": "Combating reverberation in large vocabulary continuous speech recognition",
      "original": "i15_2449",
      "page_count": 5,
      "order": 577,
      "p1": "2449",
      "pn": "2453",
      "abstract": [
        "Reverberation leads to high word error rates (WERs) for automatic speech recognition (ASR) systems. This work presents robust acoustic features motivated by subspace modeling and human speech perception for use in large vocabulary continuous speech recognition (LVCSR). We explore different acoustic modeling strategies and language modeling techniques, and demonstrate that robust features with acoustic modeling based on deep learning can provide significant reduction in WERs in the task of recognizing reverberated speech compared to mel-cepstral features and acoustic modeling based on Gaussian Mixture Models (GMMs).\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-529",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition \u2013 Signal Processing, Acoustic Modeling, Robustness, and Adaptation"
    },
    "karafiat15_interspeech": {
      "authors": [
        [
          "Martin",
          "Karafi\u00e1t"
        ],
        [
          "Franti\u0161ek",
          "Gr\u00e9zl"
        ],
        [
          "Luk\u00e1\u0161",
          "Burget"
        ],
        [
          "Igor",
          "Sz\u00f6ke"
        ],
        [
          "Jan",
          "\u010cernock\u00fd"
        ]
      ],
      "title": "Three ways to adapt a CTS recognizer to unseen reverberated speech in BUT system for the ASpIRE challenge",
      "original": "i15_2454",
      "page_count": 5,
      "order": 578,
      "p1": "2454",
      "pn": "2458",
      "abstract": [
        "This paper describes several strategies tested in BUT's submission to the IARPA ASpIRE challenge. The ASpIRE task was to develop an automatic speech recognition (ASR) system for wide-band noisy reverberant speech, while only clean CTS (Fisher) data was allowed for ASR training. To solve this task, we have started with augmenting Fisher data with artificially noised and reverberated versions. The most obvious adaptation was (1) to re-train the whole GMM/HMM-based ASR system. Then, two techniques were designed and tested to make the adaptation easier and overcome retraining the whole ASR on large amount of speech: (2) we trained a speech enhancement DNN (also called de-noising auto-encoder), and (3) we adapted the feature extraction based on stacked bottle-neck networks (SBN). While re-training the whole system works the best, only slightly inferior results were obtained with the auto-encoder denoising followed by retraining of the first layers of the SBN hierarchy, letting most of the ASR system trained on clean Fisher unchanged. This shows a promising, efficient and fast way to port ASR systems to new conditions.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-530",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition \u2013 Signal Processing, Acoustic Modeling, Robustness, and Adaptation"
    },
    "harvilla15_interspeech": {
      "authors": [
        [
          "Mark J.",
          "Harvilla"
        ],
        [
          "Richard M.",
          "Stern"
        ]
      ],
      "title": "Robust parameter estimation for audio declipping in noise",
      "original": "i15_2459",
      "page_count": 5,
      "order": 579,
      "p1": "2459",
      "pn": "2463",
      "abstract": [
        "Contemporary audio declipping algorithms often ignore the possibility of the presence of additive channel noise. If and when noise is present, however, the efficacy of any declipping algorithm is critically dependent on the accuracy with which clipped portions of the signal can be detected. This paper introduces an effective technique for inferring the amplitude and percentile values of the clipping threshold, and develops a statistically-optimal classification algorithm for accurately differentiating between clipped and unclipped samples in a noisy speech signal. The overall effectiveness of the clipped sample estimation algorithm is evaluated by the degree to which automatic speech recognition performance is improved when decoding clipped speech that has been declipped with state-of-the-art declipping algorithms paired with the clipped sample estimation algorithm. Up to 35% relative improvements in word error rate have been observed. Beyond the accuracy of the developed techniques, this paper generally underscores the necessity of robust parameter estimation methods for declipping in noise.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-531",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition \u2013 Signal Processing, Acoustic Modeling, Robustness, and Adaptation"
    },
    "huang15e_interspeech": {
      "authors": [
        [
          "Bin",
          "Huang"
        ],
        [
          "Dengfeng",
          "Ke"
        ],
        [
          "Hao",
          "Zheng"
        ],
        [
          "Bo",
          "Xu"
        ],
        [
          "Yanyan",
          "Xu"
        ],
        [
          "Kaile",
          "Su"
        ]
      ],
      "title": "Multi-task learning deep neural networks for speech feature denoising",
      "original": "i15_2464",
      "page_count": 5,
      "order": 580,
      "p1": "2464",
      "pn": "2468",
      "abstract": [
        "Traditional automatic speech recognition (ASR) systems usually get a sharp performance drop when noise presents in speech. To make a robust ASR, we introduce a new model using the multi-task learning deep neural networks (MTL-DNN) to solve the speech denoising task in feature level. In this model, the networks are initialized by pre-training restricted Boltzmann machines (RBM) and fine-tuned by jointly learning multiple interactive tasks using a shared representation. In multi-task learning, we choose a noisy-clean speech pair fitting task as the primary task and separately explore two constraints as the secondary tasks: phone label and phone cluster. In experiments, the denoised speech is reconstructed by the MTL-DNN using the noisy speech as input and it is respectively evaluated by the DNN-hidden Markov model (HMM) based and the Gaussian Mixture Model (GMM)-HMM based ASR systems. Results show that, using the denoised speech, the word error rate (WER) is respectively reduced by 53.14% and 34.84% compared with baselines. The MTL-DNN model also outperforms the general single-task learning deep neural networks (STL-DNN) model with a performance improvement of 4.93% and 3.88% respectively.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-532",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition \u2013 Signal Processing, Acoustic Modeling, Robustness, and Adaptation"
    },
    "wang15h_interspeech": {
      "authors": [
        [
          "Yuxuan",
          "Wang"
        ],
        [
          "Ananya",
          "Misra"
        ],
        [
          "Kean K.",
          "Chin"
        ]
      ],
      "title": "Time-frequency masking for large scale robust speech recognition",
      "original": "i15_2469",
      "page_count": 5,
      "order": 581,
      "p1": "2469",
      "pn": "2473",
      "abstract": [
        "Time-frequency mask estimation has shown considerable success recently. In this paper, we demonstrate its utility as a feature enhancement frontend for large vocabulary conversational speech recognition. Additionally, we investigate how masking compares with feature denoising, which directly reconstructs clean features from noisy ones. We train a mask estimator that predicts ideal ratio masks. Experimental results on Google voice search evaluation sets demonstrate that masking is superior to feature denoising, and a lightweight masking frontend produces significant improvements over a strong baseline. We also show that masking improves performance of a multi-condition trained (MTR) acoustic model.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-533",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition \u2013 Signal Processing, Acoustic Modeling, Robustness, and Adaptation"
    },
    "su15b_interspeech": {
      "authors": [
        [
          "Rongfeng",
          "Su"
        ],
        [
          "Xurong",
          "Xie"
        ],
        [
          "Xunying",
          "Liu"
        ],
        [
          "Lan",
          "Wang"
        ]
      ],
      "title": "Efficient use of DNN bottleneck features in generalized variable parameter HMMs for noise robust speech recognition",
      "original": "i15_2474",
      "page_count": 5,
      "order": 582,
      "p1": "2474",
      "pn": "2478",
      "abstract": [
        "Recently a new approach to incorporate deep neural networks (DNN) bottleneck features into HMM based acoustic models using generalized variable parameter HMMs (GVP-HMMs) was proposed. As Gaussian component level polynomial interpolation is performed for each high dimensional DNN bottleneck feature vector at a frame level, conventional GVP-HMMs are computationally expensive to use in recognition time. To handle this problem, several approaches were exploited in this paper to efficiently use DNN bottleneck features in GVP-HMMs, including model selection techniques to optimally reduce the polynomial degrees; an efficient GMM based bottleneck feature clustering scheme; more compact GVP-HMM trajectory modelling for model space tied linear transformations. These improvements gave a total of 16 time speed up in decoding time over conventional GVP-HMMs using a uniformly assigned polynomial degree. Significant error rate reductions of 15.6% relative were obtained over the baseline tandem HMM system on the secondary microphone channel condition of Aurora 4 task. Consistent improvements were also obtained on other subsets.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-534",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition \u2013 Signal Processing, Acoustic Modeling, Robustness, and Adaptation"
    },
    "baby15_interspeech": {
      "authors": [
        [
          "Deepak",
          "Baby"
        ],
        [
          "Hugo",
          "Van hamme"
        ]
      ],
      "title": "Investigating modulation spectrogram features for deep neural network-based automatic speech recognition",
      "original": "i15_2479",
      "page_count": 5,
      "order": 583,
      "p1": "2479",
      "pn": "2483",
      "abstract": [
        "Deep neural network (DNN) based acoustic modelling has been shown to yield significant improvements over Gaussian Mixture Models (GMM) for a variety of automatic speech recognition (ASR) tasks. In addition, it is also becoming popular to use rich speech representations, such as full-resolution spectrograms and perceptually motivated features, as input to the DNNs as they are less sensitive to the increase in the input dimensionality. In this work, we evaluate the performance of a DNN trained on the perceptually motivated modulation envelope spectrogram features that model the temporal amplitude modulations within sub-band speech signals. The proposed approach is shown to outperform DNNs trained on a variety of conventional features such as Mel, PLP and STFT features on both TIMIT phone recognition and the AURORA-4 word recognition tasks. It is also shown that the approach outperforms a sophisticated auditory model based on Gabor filter bank features on TIMIT and the channel matched conditions of the AURORA-4 database.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-535",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition \u2013 Signal Processing, Acoustic Modeling, Robustness, and Adaptation"
    },
    "han15_interspeech": {
      "authors": [
        [
          "Kun",
          "Han"
        ],
        [
          "Yanzhang",
          "He"
        ],
        [
          "Deblin",
          "Bagchi"
        ],
        [
          "Eric",
          "Fosler-Lussier"
        ],
        [
          "DeLiang",
          "Wang"
        ]
      ],
      "title": "Deep neural network based spectral feature mapping for robust speech recognition",
      "original": "i15_2484",
      "page_count": 5,
      "order": 584,
      "p1": "2484",
      "pn": "2488",
      "abstract": [
        "Automatic speech recognition (ASR) systems suffer from performance degradation under noisy and reverberant conditions. In this work, we explore a deep neural network (DNN) based approach for spectral feature mapping from corrupted speech to clean speech. The DNN based mapping substantially reduces interference and produces estimated clean spectral features for ASR training and decoding. We experiment with several different feature mapping approaches and demonstrate that a DNN trained to predict clean log filterbank coefficients from noisy spectrogram directly can be extremely effective. The experiments show that the ASR systems with these cleaned features perform well under joint noisy and reverberant conditions, and achieve the state-of-the-art results on the CHiME-2 corpus with stereo (corrupted and clean) data.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-536",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition \u2013 Signal Processing, Acoustic Modeling, Robustness, and Adaptation"
    },
    "xiao15b_interspeech": {
      "authors": [
        [
          "Bo",
          "Xiao"
        ],
        [
          "Zac E.",
          "Imel"
        ],
        [
          "David C.",
          "Atkins"
        ],
        [
          "Panayiotis G.",
          "Georgiou"
        ],
        [
          "Shrikanth S.",
          "Narayanan"
        ]
      ],
      "title": "Analyzing speech rate entrainment and its relation to therapist empathy in drug addiction counseling",
      "original": "i15_2489",
      "page_count": 5,
      "order": 585,
      "p1": "2489",
      "pn": "2493",
      "abstract": [
        "A key quality index in drug addiction counseling such as Motivational Interviewing is the degree of therapist's empathy towards the client. Empathy ratings are meant to evaluate the therapist's understanding of the patient's feelings, through their sensitivity and care of response. Empathy is also associated with the manifestation of behavioral entrainment in the interaction. In this paper, we compute a measure of entrainment in speech rate during dyadic interactions, and investigate its relation to perceived empathy. We show that the averaged absolute difference of turn-level speech rates between the therapist and the patient correlates with the ratings of therapist empathy. We also present the correlation of empathy to the statistics of speech and silence durations. Finally we show that in the task of automatically predicting high or low empathy, speech rate cues provide complementary information to previously proposed prosodic cues. These findings suggest speech rate as an important behavioral cue that is modulated by entrainment and contributes to empathy modeling.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-537",
      "author_area_id": "3",
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "ando15_interspeech": {
      "authors": [
        [
          "Atsushi",
          "Ando"
        ],
        [
          "Taichi",
          "Asami"
        ],
        [
          "Manabu",
          "Okamoto"
        ],
        [
          "Hirokazu",
          "Masataki"
        ],
        [
          "Sumitaka",
          "Sakauchi"
        ]
      ],
      "title": "Agreement and disagreement utterance detection in conversational speech by extracting and integrating local features",
      "original": "i15_2494",
      "page_count": 5,
      "order": 586,
      "p1": "2494",
      "pn": "2498",
      "abstract": [
        "This paper presents a novel framework to automatically detect agreement and disagreement utterances in natural conversation. Such a function is critical for conversation understanding such as meeting summarization. One of the difficulties of agreement and disagreement utterance detection in natural conversation is ambiguity in the utterance unit. Utterances are usually segmented by short pauses. However, in conversations, multiple sentences are often uttered in one breath. Such utterances exhibit the characteristics of agreement and disagreement only in some parts, not the whole utterance. This makes conventional methods problematic since they assume each utterance is just one sentence and extract global features from the whole utterance. To deal with this problem, we propose a detection framework that utilizes only local prosodic/lexical features. The local features are extracted from short windows that cover just a few words. Posteriors of agreement, disagreement and others are estimated window-by-window and integrated to yield a final decision. Experiments on free discussion speech show that the proposed method, through its use of local features, offers significantly higher accuracy in detecting agreement and disagreement utterances.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-538"
    },
    "nasir15_interspeech": {
      "authors": [
        [
          "Md.",
          "Nasir"
        ],
        [
          "Wei",
          "Xia"
        ],
        [
          "Bo",
          "Xiao"
        ],
        [
          "Brian",
          "Baucom"
        ],
        [
          "Shrikanth S.",
          "Narayanan"
        ],
        [
          "Panayiotis G.",
          "Georgiou"
        ]
      ],
      "title": "Still together?: the role of acoustic features in predicting marital outcome",
      "original": "i15_2499",
      "page_count": 5,
      "order": 587,
      "p1": "2499",
      "pn": "2503",
      "abstract": [
        "The assessment and prediction of marital outcome in couple therapy has intrigued many clinical psychologists. In this work, we analyze the significance of various acoustic features extracted from couples' spoken interaction in predicting the success or failure of their marriage. We also investigate whether speech acoustic features can provide complementary information to behavioral descriptions or codes provided by human experts ( e.g., relationship satisfaction, blame patterns, global negativity). We formulate marital outcome prediction as both binary (improvement   vs. no improvement) and multiclass (different levels of improvement) classification problem. Our experiments show that acoustic features can predict marital outcome more accurately than those based on behavioral descriptors provided by human experts. We also find that dialog turn-level acoustic features generally perform better than frame-level signal descriptors. This observation supports the notion that the impact of the behavior of one interlocutor on the other is more important than the behavior itself looked in isolation. Finally, acoustic features together with human-derived behavioral codes show the best performance in outcome prediction, suggesting some complementarity in the information captured by these behavioral representations.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-539",
      "author_area_id": "3",
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "gosztolya15b_interspeech": {
      "authors": [
        [
          "G\u00e1bor",
          "Gosztolya"
        ]
      ],
      "title": "On evaluation metrics for social signal detection",
      "original": "i15_2504",
      "page_count": 5,
      "order": 588,
      "p1": "2504",
      "pn": "2508",
      "abstract": [
        "Social signal detection is a task in speech technology which has recently became more popular. In the Interspeech 2013 ComParE Challenge one of the tasks was social signal detection, and since then, new results have been published on the dataset. These studies all used the Area Under Curve (AUC) metric to evaluate the performance; here we argue that this metric is not really suitable for social signals detection. Besides raising some serious theoretical objections, we will also demonstrate this unsuitability experimentally: we will show that applying a very simple smoothing function on the output of the frame-level scores of state-of-the-art classifiers can significantly improve the AUC scores, but perform poorly when employed in a Hidden Markov Model. As the latter is more like real-world applications, we suggest relying on utterance-level evaluation metrics in the future.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-540",
      "author_area_id": "3",
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "kaushik15_interspeech": {
      "authors": [
        [
          "Lakshmish",
          "Kaushik"
        ],
        [
          "Abhijeet",
          "Sangwan"
        ],
        [
          "John H. L.",
          "Hansen"
        ]
      ],
      "title": "Laughter and filler detection in naturalistic audio",
      "original": "i15_2509",
      "page_count": 5,
      "order": 589,
      "p1": "2509",
      "pn": "2513",
      "abstract": [
        "Laughter and fillers are common phenomenon in speech, and play an important role in communication. In this study, we present Deep Neural Network (DNN) and Convolutional Neural Network (CNN) based systems to classify non-verbal cues (laughter and fillers) from verbal speech in naturalistic audio. We propose improvements over a deep learning system proposed in [1]. Particularly, we propose a simple method to combine spectral features with pitch information to capture prosodic and spectral cues for filler/laughter. Additionally, we propose using a wider time context for feature extraction so that the time evolution of the spectral and prosodic structure can also be exploited for classification. Furthermore, we propose to use CNN for classification. The new method is evaluated on conversational telephony speech (CTS, drawn from Switchboard and Fisher) data and UT-Opinion corpus. Our results shows that the new system improves the AUC (area under the curve) metric by 8.15% and 11.9% absolute for laughters, and 4.85% and 6.01% absolute for fillers, over the baseline system, for CTS and UT-Opinion data, respectively. Finally, we analyze the results to explain the difference in performance between traditional CTS data and naturalistic audio (UT-Opinion), and identify challenges that need to be addressed to make systems perform better for practical data.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-541",
      "author_area_id": "3",
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "pappu15_interspeech": {
      "authors": [
        [
          "Aasish",
          "Pappu"
        ],
        [
          "Amanda",
          "Stent"
        ]
      ],
      "title": "Automatic formatted transcripts for videos",
      "original": "i15_2514",
      "page_count": 5,
      "order": 590,
      "p1": "2514",
      "pn": "2518",
      "abstract": [
        "Multimedia content may be supplemented with time-aligned closed captions for accessibility. Often these captions are created manually by professional editors \u2014 an expensive and time-consuming process. In this paper, we present a novel approach to automatic creation of a well-formatted, readable transcript for a video from closed captions or ASR output. Our approach uses acoustic and lexical features extracted from the video and the raw transcription/caption files. We compare our approach with two standard baselines: a) silence segmented transcripts and b) text-only segmented transcripts. We show that our approach outperforms both these baselines based on subjective and objective metrics.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-542",
      "author_area_id": "3",
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "azais15_interspeech": {
      "authors": [
        [
          "Lucas",
          "Aza\u00efs"
        ],
        [
          "Adrien",
          "Payan"
        ],
        [
          "Tianjiao",
          "Sun"
        ],
        [
          "Guillaume",
          "Vidal"
        ],
        [
          "Tina",
          "Zhang"
        ],
        [
          "Eduardo",
          "Coutinho"
        ],
        [
          "Florian",
          "Eyben"
        ],
        [
          "Bj\u00f6rn",
          "Schuller"
        ]
      ],
      "title": "Does my speech rock? automatic assessment of public speaking skills",
      "original": "i15_2519",
      "page_count": 5,
      "order": 591,
      "p1": "2519",
      "pn": "2523",
      "abstract": [
        "In this paper, we introduce results for the task of Automatic Public Speech Assessment (APSA). Given the comparably sparse work carried out on this task up to this point, a novel database was required for training and evaluation of machine learning models. As a basis, the freely available oral presentations of the ICASSP conference in 2011 were selected due to their transcription including non-verbal vocalisations. The data was specifically labelled in terms of the perceived oratory ability of the speakers by five raters according to a 5-point Public Speaking Skill Rating Likert scale. We investigate the feasibility of speaker-independent APSA using different standardised acoustic feature sets computed per fixed chunk of an oral presentation in a series of ternary classification and continuous regression experiments. Further, we compare the relevance of different feature groups related to fluency (speech/hesitation rate), prosody, voice quality and a variety of spectral features. Our results demonstrate that oratory speaking skills can be reliably assessed using supra-segmental audio features, with prosodic ones being particularly suited.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-543",
      "author_area_id": "3",
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "sergienko15_interspeech": {
      "authors": [
        [
          "Roman",
          "Sergienko"
        ],
        [
          "Alexander",
          "Schmitt"
        ]
      ],
      "title": "Verbal intelligence identification based on text classification",
      "original": "i15_2524",
      "page_count": 5,
      "order": 592,
      "p1": "2524",
      "pn": "2528",
      "abstract": [
        "This paper analyses and compares term weighting methods for automatic verbal intelligence identification from speech. Two different corpora are used; the first one contains monologues on the same topic; the second one contains dialogues between two or three people. The problem is described as a text classification task with two classes: low and high verbal intelligence. Seven different term weighting methods were applied for text classification using the k-NN algorithm. The best result is obtained with the ConfidentWeights method as a term weighting method for the dialogue corpus. The best classification accuracy equals 0.80 and the best macro F1-score equals 0.79. The numerical results have shown that highest scores can be obtained when using a very small number of terms which characterize only the class of higher verbal intelligence.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-544",
      "author_area_id": "3",
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "hsiao15_interspeech": {
      "authors": [
        [
          "Shan-Wen",
          "Hsiao"
        ],
        [
          "Hung-Ching",
          "Sun"
        ],
        [
          "Ming-Chuan",
          "Hsieh"
        ],
        [
          "Ming-Hsueh",
          "Tsai"
        ],
        [
          "Hsin-Chih",
          "Lin"
        ],
        [
          "Chi-Chun",
          "Lee"
        ]
      ],
      "title": "A multimodal approach for automatic assessment of school principals' oral presentation during pre-service training program",
      "original": "i15_2529",
      "page_count": 5,
      "order": 593,
      "p1": "2529",
      "pn": "2533",
      "abstract": [
        "Developing automatic recognition systems of subjective rating using behavior data, collected using audio-video recording devices, has been at the forefront of many interdisciplinary research effort between behavior science and engineering in order to provide objective decision-making tools. In the field of education, pre-service training program for school principals has becoming more critical due to the increasingly complex and demanding nature of the job. In this work, we collaborate with researchers from the National Academy for Educational Research to develop a system in order to assess pre-service principals' oral presentation skill. Our recognition framework incorporates multimodal behavioral data, i.e., audio and video information. With proper handling of label normalization and binarization, we achieve an unweighted average recall of (0.63, 0.70, 0.67) or (0.67, 0.68, 0.67) depending on the choice of labeling schemes, i.e., original or rank-normalized, on differentiating between high versus low performing scores. The three oral presentation rating dimensions used in this work are Dim1: content + structure + word, Dim2: prosody, Dim3: total score.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-545"
    },
    "tsai15b_interspeech": {
      "authors": [
        [
          "T. J.",
          "Tsai"
        ]
      ],
      "title": "Are you TED talk material? comparing prosody in professors and TED speakers",
      "original": "i15_2534",
      "page_count": 5,
      "order": 594,
      "p1": "2534",
      "pn": "2538",
      "abstract": [
        "TED talks are the pinnacle of public speaking. They combine compelling content with flawless delivery, and their popularity is attested by the millions of views they attract. In this work, we compare the prosodic voice characteristics of TED speakers and university professors. Our aim is to identify the characteristics that separate TED speakers from other public speakers. Based on a simple set of features derived from pitch and energy, we train a discriminative classifier to predict whether a 5 minute audio sample is from a TED talk or a university lecture. We are able to achieve < 10% equal error rate. We then investigate which features are most discriminative, and discuss conflating factors that might contribute to those features.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-546",
      "author_area_id": "3",
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "akira15_interspeech": {
      "authors": [
        [
          "Hayakawa",
          "Akira"
        ],
        [
          "Fasih",
          "Haider"
        ],
        [
          "Loredana",
          "Cerrato"
        ],
        [
          "Nick",
          "Campbell"
        ],
        [
          "Saturnino",
          "Luz"
        ]
      ],
      "title": "Detection of cognitive states and their correlation to speech recognition performance in speech-to-speech machine translation systems",
      "original": "i15_2539",
      "page_count": 5,
      "order": 595,
      "p1": "2539",
      "pn": "2543",
      "abstract": [
        "An analysis of possible associations between speech recognition performance and three cognitive states that arise in dialogues mediated by a speech-to-speech machine translation system is reported. This analysis is based on a new corpus of interlingual interactions in a map task which includes precisely synchronised speech, video, and physiological data streams (blood-volume pulse, skin conductance, electroencephalogram, and eye movements). While no evidence is found that cognitive states occurring prior to utterances sent to the speech recogniser affect the speech recognition performance, the onset of cognitive states \u2014 especially frustration \u2014 is found to be clearly associated with speech recognition performance. Given this association, methods for automatic detection of these cognitive states were explored by using features of the two physiological signals, features of the speech signal, and combinations of speech and physiological features. Combined biosignals yields detection performance well above the baseline (71% accuracy) when the time window is restricted to the perceived duration of the state. Extending the window to the end of the utterance following the cognitive state yields poor detection on biosignals alone, but improves considerably when features of the speech signal are added, thus showing the potential usefulness of speech features as a biosignal.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-547",
      "author_area_id": "3",
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "koster15_interspeech": {
      "authors": [
        [
          "Friedemann",
          "K\u00f6ster"
        ],
        [
          "Sebastian",
          "M\u00f6ller"
        ]
      ],
      "title": "Perceptual speech quality dimensions in a conversational situation",
      "original": "i15_2544",
      "page_count": 5,
      "order": 596,
      "p1": "2544",
      "pn": "2548",
      "abstract": [
        "Speech telecommunication systems are most frequently used in conversational situations. In this regard, assessing the quality of conversational speech is the fundamental requirement for system developers to classify and evaluate their systems. However, it is not enough to provide information about the overall quality, but also to point out sources for possible quality-losses. We present a follow-up study to analyze such perceptual speech quality dimensions in a conversational situation. Until now, seven perceptual quality dimensions have been determined in separate studies. In this contribution, we review all dimensions and validate them in an extensive conversational experiment. This study leads to a deep analysis of perceptual speech quality in a conversational situation.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-548",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "berger15_interspeech": {
      "authors": [
        [
          "Jens",
          "Berger"
        ],
        [
          "Anna",
          "Llagostera"
        ]
      ],
      "title": "Multidimensional evaluation and predicting overall speech quality",
      "original": "i15_2549",
      "page_count": 4,
      "order": 597,
      "p1": "2549",
      "pn": "2552",
      "abstract": [
        "The quality of speech samples has been traditionally evaluated in subjective listening tests using 5-point Absolute Category Rating (ACR) scales in Listening Only Tests (LOT) as recommended in ITU-T P.800. Those tests provide the listening quality aspect of speech quality.   There are other tests are under discussion and proposed in order to assess in detail individual perceptual dimensions of speech. In this paper we investigate the relationship between the overall listening quality obtained in an ITU-T P.800 ACR subjective test and the rating of the same signals in four dimensions proposed by W\u00e4ltermann, namely noisiness, discontinuity, coloration and loudness. The database we use is composed of conditions and speech signals extracted from an ACR LOT used in the ITU-T P.863 evaluation, processed by simulated and live telecommunication channels. The signals have been re-scored using the four mentioned scales and are foreseen as contribution to the ITU-T P.AMD project.   This paper focuses on the modeling of an ACR LOT score based on individual dimensional ratings under the assumption of orthogonality of the four dimensions.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-549",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "gaich15_interspeech": {
      "authors": [
        [
          "Andreas",
          "Gaich"
        ],
        [
          "Pejman",
          "Mowlaee"
        ]
      ],
      "title": "On speech intelligibility estimation of phase-aware single-channel speech enhancement",
      "original": "i15_2553",
      "page_count": 5,
      "order": 598,
      "p1": "2553",
      "pn": "2557",
      "abstract": [
        "To reduce time and costs in the development process of noise reduction algorithms, an objective intelligibility measure is crucial. Such a measure has to show high correlation with speech intelligibility determined by real listening experiments. In the past several measures were found that perform reliable in a particular scenario when only the spectral amplitude of a noisy signal is modified. Recent studies demonstrate the positive impact of a phase modification in a single-channel speech enhancement showing improved speech intelligibility while conventional methods relying on amplitude-only modification are known for reduced intelligibility. Further, another recent study shows that a distortion metric defined on the spectral phase outperforms state-of-the-art quality metrics when used in phase-aware speech enhancement. This raises two questions we account for in this work; First, to study the reliability of the existing intelligibility measures in predicting the performance of the phase-aware methods, and second to investigate candidates for new phase-aware instrumental metrics and evaluate their reliability in terms of intelligibility prediction. Our objective and subjective evaluations demonstrate that CSII-based and STOI as well as the proposed phase-aware metrics perform as reliable speech intelligibility estimators following the subjective results.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-550",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "marxer15_interspeech": {
      "authors": [
        [
          "Ricard",
          "Marxer"
        ],
        [
          "Martin",
          "Cooke"
        ],
        [
          "Jon",
          "Barker"
        ]
      ],
      "title": "A framework for the evaluation of microscopic intelligibility models",
      "original": "i15_2558",
      "page_count": 5,
      "order": 599,
      "p1": "2558",
      "pn": "2562",
      "abstract": [
        "Traditional intelligibility models are concerned with predicting the average number of words heard correctly in given noise conditions and can be readily tested by comparison with listener data. In contrast, recent `microscopic' intelligibility models, which attempt to make precise predictions about a listener's perception or misperception of specific utterances, have less well-defined goals and are hard to compare. This paper presents a novel evaluation framework that proposes a standardised procedure for microscopic model evaluation. The key problem is how to compare a model prediction made at a sublexical granularity with a set of listeners' lexical responses, especially considering that not all listeners will hear a given noisy target word in the same way. Our approach is to phonetically align the target word to each listener's response and then, for each position in the target word, calculate both the probability of a phonetic error occurring and the probability distribution of possible phonetic substitutions. Models can then be evaluated according to their ability to estimate these probabilities using likelihood as a measure. This approach has been built into a framework for model evaluation and demonstrated using a recently released corpus of consistent speech misperceptions and a set of naive intelligibility models.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-551",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "andersen15_interspeech": {
      "authors": [
        [
          "Asger Heidemann",
          "Andersen"
        ],
        [
          "Jan Mark de",
          "Haan"
        ],
        [
          "Zheng-Hua",
          "Tan"
        ],
        [
          "Jesper",
          "Jensen"
        ]
      ],
      "title": "A binaural short time objective intelligibility measure for noisy and enhanced speech",
      "original": "i15_2563",
      "page_count": 5,
      "order": 600,
      "p1": "2563",
      "pn": "2567",
      "abstract": [
        "Objective intelligibility measures are increasingly being used to assess the performance of speech processing algorithms, e.g. for hearing aids. It has been shown that the short time objective intelligibility (STOI) measure yields good results in this respect. In this paper we propose a binaural extension of the STOI measure, which predicts binaural advantage using a modified equalization cancellation (EC) stage. The proposed method is evaluated for a range of acoustic conditions. Firstly, the method is able to predict the advantage of spatial separation between a speech target and a speech shaped noise (SSN) interferer. Secondly, the method yields results comparable to the monaural STOI measure when presented with noisy speech processed by ideal time-frequency segregation (ITFS). Finally, the method also performs well when presented with a selection of different acoustic conditions combined with beamforming as used in hearing aids.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-552",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "tang15_interspeech": {
      "authors": [
        [
          "Yan",
          "Tang"
        ],
        [
          "Martin",
          "Cooke"
        ],
        [
          "Bruno M.",
          "Fazenda"
        ],
        [
          "Trevor J.",
          "Cox"
        ]
      ],
      "title": "A glimpse-based approach for predicting binaural intelligibility with single and multiple maskers in anechoic conditions",
      "original": "i15_2568",
      "page_count": 5,
      "order": 601,
      "p1": "2568",
      "pn": "2572",
      "abstract": [
        "A distortion-weighted glimpsing metric developed for estimating monaural speech intelligibility is extended to predict binaural speech intelligibility in noise. Two aspects of binaural listening, the better ear effect and the binaural advantage, are taken into account in the new metric, which predicts intelligibility using monaural target and masker signals and their location, and is therefore able to provide intelligibility estimates in situations where binaural signals are not readily available. Perceptual listening experiments were conducted to evaluate the predictive power of the proposed metric for speech in the presence of single and multiple maskers in anechoic conditions, for a range of source/masker azimuth combinations. The binaural metric is highly correlated (\u03c1 > 0.9) with listeners' performance in all conditions tested, but overestimates intelligibility somewhat in conditions where multiple maskers are present and the target speech source location is unknown.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-553",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "chen15k_interspeech": {
      "authors": [
        [
          "Fei",
          "Chen"
        ]
      ],
      "title": "Improving the prediction power of the speech transmission index to account for non-linear distortions introduced by noise-reduction algorithms",
      "original": "i15_2573",
      "page_count": 5,
      "order": 602,
      "p1": "2573",
      "pn": "2577",
      "abstract": [
        "Although the speech transmission index (STI) has been shown to predict successfully the effects of linear distortions introduced by filtering and additive noise, it does not account for non-linear distortions present in noise-suppressed speech. In this study, the normalized covariance metric (NCM), a STI-based intelligibility measure, was modified to reduce the effects of non-linear distortions introduced by most noise-suppression algorithms for intelligibility prediction. This was done by designing a new definition of the output signal-to-noise ratio to compensate the biased estimation of the input SNR prior to the noise-suppression processing. The modified NCM measure was evaluated with intelligibility scores obtained by normal-hearing listeners in 72 noisy conditions involving noise-suppressed speech corrupted by four different maskers (babble, car, train and street interferences). Significantly higher correlation with intelligibility score was obtained from the modified NCM measure, in contrast to those from the original NCM measure.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-554",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "li15d_interspeech": {
      "authors": [
        [
          "Kehuang",
          "Li"
        ],
        [
          "Zhen",
          "Huang"
        ],
        [
          "Yong",
          "Xu"
        ],
        [
          "Chin-Hui",
          "Lee"
        ]
      ],
      "title": "DNN-based speech bandwidth expansion and its application to adding high-frequency missing features for automatic speech recognition of narrowband speech",
      "original": "i15_2578",
      "page_count": 5,
      "order": 603,
      "p1": "2578",
      "pn": "2582",
      "abstract": [
        "We propose a number of enhancement techniques to improve speech quality in bandwidth expansion (BWE) from narrowband to wideband speech, addressing three issues, which could be critical in real-world applications, namely: (1) discontinuity between narrowband spectrum and the estimated high frequency spectrum, (2) energy mismatch between testing and training utterances, and (3) expanding bandwidth of out-of-domain speech signals. With an inherent prediction of missing high frequency features in bandwidth-expanded speech we also explore the feasibility of adding these estimated features to those extracted from narrowband speech in order to improve the system performance for automatic speech recognition (ASR) of narrowband speech. Leveraging upon a recently-proposed deep neural network based speech BWE system intended for hearing quality enhancement these techniques not only improve over the traditionally-adopted objective and subjective measures but also reduce the word error rate (WER) from 8.67% when recognizing narrowband speech to 8.26% when recognizing bandwidth-expanded speech, and almost approaching the WER of 8.12% when recognizing wideband speech in the 20,000-word open-vocabulary Wall Street Journal ASR task.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-555",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "pulakka15_interspeech": {
      "authors": [
        [
          "Hannu",
          "Pulakka"
        ],
        [
          "Ville",
          "Myllyl\u00e4"
        ],
        [
          "Anssi",
          "R\u00e4m\u00f6"
        ],
        [
          "Paavo",
          "Alku"
        ]
      ],
      "title": "Speech quality evaluation of artificial bandwidth extension: comparing subjective judgments and instrumental predictions",
      "original": "i15_2583",
      "page_count": 5,
      "order": 604,
      "p1": "2583",
      "pn": "2587",
      "abstract": [
        "Artificial bandwidth extension (ABE) methods have been developed to enhance the quality and intelligibility of bandlimited speech transmitted over a telephone connection. Subjective listening tests are the most reliable way of evaluating the quality of ABE, but listening tests are time-consuming and expensive to arrange. Instrumental measures have also been used to estimate the subjective quality of ABE. This study extends the results of an earlier subjective evaluation of ABE methods by instrumental quality predictions computed with WB-PESQ (ITU-T Recommendation P.862.2) and POLQA (ITU-T Recommendation P.863). The instrumental quality predictions are compared with the subjective quality scores. The results indicate that POLQA correlates better with the subjective quality than WB-PESQ. Neither WB-PESQ nor POLQA can predict the rank order of the evaluated ABE methods in all conditions.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-556",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "turan15_interspeech": {
      "authors": [
        [
          "M. A. Tu\u011ftekin",
          "Turan"
        ],
        [
          "Engin",
          "Erzin"
        ]
      ],
      "title": "Synchronous overlap and add of spectra for enhancement of excitation in artificial bandwidth extension of speech",
      "original": "i15_2588",
      "page_count": 5,
      "order": 605,
      "p1": "2588",
      "pn": "2592",
      "abstract": [
        "In this paper, a new approach that extends narrow-band excitation signals using synchronous overlap and add (SOLA) of spectra have been proposed. Although artificial bandwidth extension (ABE) of speech has been extensively studied, the role of excitation spectra has not been as widely studied as the spectral envelope extension. In this study ABE is investigated with the widely used source-filter framework, where speech signal is decomposed into excitation signal (source) and spectral envelope (filter). For the spectral envelope extension, our former work based on hidden Markov model has been used. For the excitation signal extension, we propose a SOLA of excitation spectra, where the high end of the excitation spectra is extended by preserving the harmonic structure. In experimental studies, we also apply two other well-known extension techniques for excitation signals. Then comparatively we evaluate the overall performance of proposed system using the PESQ metric. Our findings indicate that the proposed excitation extension method delivers significant quality improvements.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-557",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "wang15i_interspeech": {
      "authors": [
        [
          "Yingxue",
          "Wang"
        ],
        [
          "Shenghui",
          "Zhao"
        ],
        [
          "Wenbo",
          "Liu"
        ],
        [
          "Ming",
          "Li"
        ],
        [
          "Jingming",
          "Kuang"
        ]
      ],
      "title": "Speech bandwidth expansion based on deep neural networks",
      "original": "i15_2593",
      "page_count": 5,
      "order": 606,
      "p1": "2593",
      "pn": "2597",
      "abstract": [
        "This paper proposes a new speech bandwidth expansion method, which uses Deep Neural Networks (DNNs) to build high-order eigenspaces between the low frequency components and the high frequency components of the speech signal. A four-layer DNN is trained layer-by-layer from a cascade of Neural Networks (NNs) and two Gaussian-Bernoulli Restricted Boltzmann Machines (GBRBMs). The GBRBMs are adopted to model the distribution of spectral envelopes of the low frequency and the high frequency respectively. The NNs are used to model the joint distribution of hidden variables extracted from the two GBRBMs. The proposed method takes advantage of the strong modeling ability of GBRBMs in modeling the distribution of the spectral envelopes. And both the objective and subjective test results show that the proposed method outperforms the conventional GMM based method.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-558"
    },
    "liu15g_interspeech": {
      "authors": [
        [
          "Bin",
          "Liu"
        ],
        [
          "Jianhua",
          "Tao"
        ],
        [
          "Zhengqi",
          "Wen"
        ],
        [
          "Ya",
          "Li"
        ],
        [
          "Danish",
          "Bukhari"
        ]
      ],
      "title": "A novel method of artificial bandwidth extension using deep architecture",
      "original": "i15_2598",
      "page_count": 5,
      "order": 607,
      "p1": "2598",
      "pn": "2602",
      "abstract": [
        "This paper presents a novel artificial bandwidth extension (ABE) framework based on deep neural networks (DNNs) with a multiple-layer's deep architecture. It demonstrates the suitability of DNNs for modeling log power spectra of speech signals using the application of ABE. The DNN is used to estimate the log power spectra in the high-band. Two strategies are proposed to improve the performances of the proposed ABE system. First, global variance equalization is proposed to alleviate the over-smoothing issue in generated log spectra. Second, rich acoustic features in the low-band are considered to improve the construction of the log power spectra in the high-band. Experimental results demonstrate that the proposed framework can achieve significant improvements in both objective and subjective measures over the different baseline methods.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-559",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "dalen15_interspeech": {
      "authors": [
        [
          "Rogier C. van",
          "Dalen"
        ],
        [
          "Mark J. F.",
          "Gales"
        ]
      ],
      "title": "Annotating large lattices with the exact word error",
      "original": "i15_2625",
      "page_count": 5,
      "order": 608,
      "p1": "2625",
      "pn": "2629",
      "abstract": [
        "The acoustic model in modern speech recognisers is trained discriminatively, for example with the minimum Bayes risk. This criterion is hard to compute exactly, so that it is normally approximated by a criterion that uses fixed alignments of lattice arcs. This approximation becomes particularly problematic with new types of acoustic models that require flexible alignments. It would be best to annotate lattices with the risk measure of interest, the exact word error. However, the algorithm for this uses finite-state automaton determinisation, which has exponential complexity and runs out of memory for large lattices. This paper introduces a novel method for determinising and minimising finite-state automata incrementally. Since it uses less memory, it can be applied to larger lattices.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-560",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition \u2013 Signal Processing, Acoustic Modeling, Robustness, and Adaptation"
    },
    "manohar15_interspeech": {
      "authors": [
        [
          "Vimal",
          "Manohar"
        ],
        [
          "Daniel",
          "Povey"
        ],
        [
          "Sanjeev",
          "Khudanpur"
        ]
      ],
      "title": "Semi-supervised maximum mutual information training of deep neural network acoustic models",
      "original": "i15_2630",
      "page_count": 5,
      "order": 609,
      "p1": "2630",
      "pn": "2634",
      "abstract": [
        "Maximum Mutual Information (MMI) is a popular discriminative criterion that has been used in supervised training of acoustic models for automatic speech recognition. However, standard discriminative training is very sensitive to the accuracy of the transcription and hence its implementation in a semi-supervised setting requires extensive filtering of data. We will show that if the supervision transcripts are not known, the natural analogue of MMI is to minimize the conditional entropy of the lattice of possible transcripts of the data. This is equivalent to the weighted average of MMI criterion over different reference transcripts, taking those reference transcripts and their weighting from the lattice itself. In this paper we describe experiments where we applied this method to the semi-supervised training of Deep Neural Network acoustic models. In our experimental setup, the proposed method gives up to 0.5% absolute WER improvement over a DNN trained with sMBR only on the transcribed part of the data. This is 37% of the improvement that we would get from doing sMBR training if we had the transcripts for the untranscribed part of the data.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-561",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition \u2013 Signal Processing, Acoustic Modeling, Robustness, and Adaptation"
    },
    "zhang15d_interspeech": {
      "authors": [
        [
          "Shiliang",
          "Zhang"
        ],
        [
          "Hui",
          "Jiang"
        ],
        [
          "Si",
          "Wei"
        ],
        [
          "Li-Rong",
          "Dai"
        ]
      ],
      "title": "Rectified linear neural networks with tied-scalar regularization for LVCSR",
      "original": "i15_2635",
      "page_count": 5,
      "order": 610,
      "p1": "2635",
      "pn": "2639",
      "abstract": [
        "It is known that rectified linear deep neural networks (RL-DNNs) can consistently outperform the conventional pre-trained sigmoid DNNs even with a random initialization. In this paper, we present another interesting and useful property of RL-DNNs that we can learn RL-DNNs with a very large batch size in stochastic gradient descent (SGD). Therefore, the SGD learning can be easily parallelized among multiple computing units for much better training efficiency. Moreover, we also propose a tied-scalar regularization technique to make the large-batch SGD learning of RL-DNNs more stable. Experimental results on the 309-hour Switchboard (SWB) task have shown that we can train RL-DNNs using batch sizes about 100 times larger than those used in the previous work, thus the learning of RL-DNNs can be accelerated by over 10 times when 8 GPUs are used. More importantly, we have achieved a word error rate of 13.8% with a 6-hidden-layer RL-DNN trained by the frame-level cross-entropy criterion with the tied-scalar regularization. To our knowledge, this is the best reported performance on this task under the same experimental settings.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-562",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition \u2013 Signal Processing, Acoustic Modeling, Robustness, and Adaptation"
    },
    "he15c_interspeech": {
      "authors": [
        [
          "Yanzhang",
          "He"
        ],
        [
          "Eric",
          "Fosler-Lussier"
        ]
      ],
      "title": "Segmental conditional random fields with deep neural networks as acoustic models for first-pass word recognition",
      "original": "i15_2640",
      "page_count": 5,
      "order": 611,
      "p1": "2640",
      "pn": "2644",
      "abstract": [
        "Discriminative segmental models, such as segmental conditional random fields (SCRFs), have been successfully applied to speech recognition recently in lattice rescoring to integrate detectors across different levels of units, such as phones and words. However, the lattice generation has been constrained by a baseline decoder, typically a frame-based hybrid HMM-DNN system, which still suffers from the well-known frame independent assumption. In this paper, we propose to use SCRFs with DNNs directly as the acoustic model, a one-pass unified framework that can utilize local phone classifiers, phone transitions and long-span features, in direct word decoding to model phones or sub-phonetic segments with variable length. We describe a WFST-based approach to utilize the proposed acoustic model efficiently with the language model in first-pass word recognition. Our evaluation on the WSJ0 corpus shows our SCRF-DNN system outperforms a hybrid HMM-DNN system and a frame-level CRF-DNN system using the same monophone label space.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-563",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition \u2013 Signal Processing, Acoustic Modeling, Robustness, and Adaptation"
    },
    "chen15l_interspeech": {
      "authors": [
        [
          "Dongpeng",
          "Chen"
        ],
        [
          "Brian",
          "Mak"
        ]
      ],
      "title": "Distinct triphone acoustic modeling using deep neural networks",
      "original": "i15_2645",
      "page_count": 5,
      "order": 612,
      "p1": "2645",
      "pn": "2649",
      "abstract": [
        "To strike a balance between robust parameter estimation and detailed modeling, most automatic speech recognition systems are built using tied-state continuous density hidden Markov models (CDHMM). Consequently, states that are tied together in a tied-state are not distinguishable, introducing quantization errors inevitably. It has been shown that it is possible to model (almost) all distinct triphones effectively by using a basis approach; previously two methods were proposed: eigentriphone modeling and reference model weighting (RMW) in CDHMM using Gaussian-mixture states. In this paper, we investigate distinct triphone modeling under the state-of-the-art deep neural network (DNN) framework. Due to the large number of DNN model parameters, regularization is necessary. Multi-task learning (MTL) is first used to train distinct triphone states together with carefully chosen related tasks which serve as a regularizer. The RMW approach is then applied to linearly combine the neural network weight vectors of member triphones of each tied-state before the output softmax activation for each distinct triphone state. The method successfully improves phoneme recognition in TIMIT and word recognition in the Wall Street Journal task.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-564",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition \u2013 Signal Processing, Acoustic Modeling, Robustness, and Adaptation"
    },
    "gelly15_interspeech": {
      "authors": [
        [
          "Gregory",
          "Gelly"
        ],
        [
          "Jean-Luc",
          "Gauvain"
        ]
      ],
      "title": "Minimum word error training of RNN-based voice activity detection",
      "original": "i15_2650",
      "page_count": 5,
      "order": 613,
      "p1": "2650",
      "pn": "2654",
      "abstract": [
        "Voice Activity Detection (VAD) is critical in speech recognition systems as it can dramatically impact the recognition accuracy especially on noisy data. This paper presents a novel method which applies Minimum Word Error (MWE) training to a Long Short-Term Memory RNN to optimize Voice Activity Detection for speech recognition. Experiments compare speech recognition WERs using RNN VAD with other commonly used VAD methods for two corpora: the conversational Vietnamese corpus used in the NIST OpenKWS13 evaluation and a corpus of French telephone conversations. The proposed VAD method combining MWE training with RNN yields the best ASR results. This MWE training scheme appears to be particularly useful for low resource ASR tasks, as exemplified by the IARPA BABEL data.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-565",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition \u2013 Signal Processing, Acoustic Modeling, Robustness, and Adaptation"
    },
    "quatieri15_interspeech": {
      "authors": [
        [
          "Thomas F.",
          "Quatieri"
        ],
        [
          "James R.",
          "Williamson"
        ],
        [
          "Christopher J.",
          "Smalt"
        ],
        [
          "Tejash",
          "Patel"
        ],
        [
          "Joseph",
          "Perricone"
        ],
        [
          "Daryush D.",
          "Mehta"
        ],
        [
          "Brian S.",
          "Helfer"
        ],
        [
          "Gregory",
          "Ciccarelli"
        ],
        [
          "Darrell",
          "Ricke"
        ],
        [
          "Nicolas",
          "Malyska"
        ],
        [
          "Jeff",
          "Palmer"
        ],
        [
          "Kristin",
          "Heaton"
        ],
        [
          "Marianna",
          "Eddy"
        ],
        [
          "Joseph",
          "Moran"
        ]
      ],
      "title": "Vocal biomarkers to discriminate cognitive load in a working memory task",
      "original": "i15_2684",
      "page_count": 5,
      "order": 614,
      "p1": "2684",
      "pn": "2688",
      "abstract": [
        "Early, accurate detection of cognitive load can help reduce risk of accidents and injuries, and inform intervention and rehabilitation in recovery. Thus, simple noninvasive biomarkers are desired for determining cognitive load under cognitively complex tasks. In this study, a novel set of vocal biomarkers are introduced for detecting different cognitive load conditions. Our vocal biomarkers use phoneme- and pseudosyllable-based measures, and articulatory and source coordination derived from cross-correlation and temporal coherence of formant and creakiness measures. A ~2-hour protocol was designed to induce cognitive load by stressing auditory working memory. This was done by repeatedly requiring the subject to recall a sentence while holding a number of digits in memory. We demonstrate the power of our speech features to discriminate between high and low load conditions. Using a database consisting of audio from 13 subjects, we apply classification models of cognitive load, showing a ~7% detection equal-error rate from features derived from 40 sentence utterances (~4 minutes of audio).\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-566",
      "author_area_id": "3",
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "zhang15e_interspeech": {
      "authors": [
        [
          "Chunlei",
          "Zhang"
        ],
        [
          "Gang",
          "Liu"
        ],
        [
          "Chengzhu",
          "Yu"
        ],
        [
          "John H. L.",
          "Hansen"
        ]
      ],
      "title": "I-vector based physical task stress detection with different fusion strategies",
      "original": "i15_2689",
      "page_count": 5,
      "order": 615,
      "p1": "2689",
      "pn": "2693",
      "abstract": [
        "It is common for subjects to produce speech while performing a physical task where speech technology may be used. Variabilities are introduced to speech since physical task can influence human speech production. These variabilities degrade the performance of most speech systems. It is vital to detect speech under physical stress variabilities for subsequent algorithm processing. This study presents a method for detecting physical task stress from speech. Inspired by the fact that i-vectors can generally model total factors from speech, a state-of-the-art i-vector framework is investigated with MFCCs and our previously formulated TEO-CB-Auto-Env features for neutral/physical task stress detection. Since MFCCs are derived from a linear speech production model and TEO-CB-Auto-Env features employ a nonlinear operator, these two features are believed to have complementary effects on physical task stress detection. Two alternative fusion strategies (feature-level and score-level fusion) are investigated to validate this hypothesis. Experiments over the UT-Scope Physical Corpus demonstrate that a relative accuracy gain of 2.68% is obtained when fusing different feature based i-vectors. An additional relative performance boost with of 6.52% in accuracy is achieved using score level fusion.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-567",
      "author_area_id": "3",
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "toth15_interspeech": {
      "authors": [
        [
          "L\u00e1szl\u00f3",
          "T\u00f3th"
        ],
        [
          "G\u00e1bor",
          "Gosztolya"
        ],
        [
          "Veronika",
          "Vincze"
        ],
        [
          "Ildik\u00f3",
          "Hoffmann"
        ],
        [
          "Gr\u00e9ta",
          "Szatl\u00f3czki"
        ],
        [
          "Edit",
          "Bir\u00f3"
        ],
        [
          "Fruzsina",
          "Zsura"
        ],
        [
          "Magdolna",
          "P\u00e1k\u00e1ski"
        ],
        [
          "J\u00e1nos",
          "K\u00e1lm\u00e1n"
        ]
      ],
      "title": "Automatic detection of mild cognitive impairment from spontaneous speech using ASR",
      "original": "i15_2694",
      "page_count": 5,
      "order": 616,
      "p1": "2694",
      "pn": "2698",
      "abstract": [
        "Mild Cognitive Impairment (MCI), sometimes regarded as a prodromal stage of Alzheimer's disease, is a mental disorder that is difficult to diagnose. However, recent studies reported that MCI causes slight changes in the speech of the patient. Our starting point here is a study that found acoustic correlates of MCI, but extracted the proposed features manually. Here, we automate the extraction of the features by applying automatic speech recognition (ASR). Unlike earlier authors, we use ASR to extract only a phonetic level segmentation and annotation. While the phonetic output allows the calculation of features like the speech rate, it avoids the problems caused by the agrammatical speech frequently produced by the targeted patient group. Furthermore, as hesitation is the most important indicator of MCI, we take special care when handling filled pauses, which usually correspond to hesitation. Using the ASR-based features, we employ machine learning methods to separate the subjects with MCI from the control group. The classification results obtained with ASR-based feature extraction are just slightly worse that those got with the manual method. The F1 value achieved (85.3) is very promising regarding the creation of an automated MCI screening application.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-568",
      "author_area_id": "3",
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "sidorov15_interspeech": {
      "authors": [
        [
          "Maxim",
          "Sidorov"
        ],
        [
          "Christina",
          "Brester"
        ],
        [
          "Alexander",
          "Schmitt"
        ]
      ],
      "title": "Contemporary stochastic feature selection algorithms for speech-based emotion recognition",
      "original": "i15_2699",
      "page_count": 5,
      "order": 617,
      "p1": "2699",
      "pn": "2703",
      "abstract": [
        "In this study a class of Multi-Objective Genetic Algorithms (MOGAs) is proposed to select the most relevant features for the problem of speech-based emotion recognition. The employed evolutionary algorithms are the Strength Pareto Evolutionary Algorithm (or SPEA), the Preference-Inspired CoEvolutionary Algorithm with goal vectors (or PICEA), and the Nondominated Sorting Genetic Algorithm II (or NSGA-II). Performances of the proposed algorithms were compared against conventional feature selection methods on a number of emotional speech corpora. The study revealed that for some of the corpora the proposed approach significantly outperforms the baseline feature selection methods up to 5.4% of relative difference.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-569",
      "author_area_id": "3",
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "ferrer15_interspeech": {
      "authors": [
        [
          "Carlos A.",
          "Ferrer"
        ],
        [
          "Diana",
          "Torres"
        ],
        [
          "Eduardo",
          "Gonz\u00e1lez"
        ],
        [
          "Jos\u00e9 Ram\u00f3n",
          "Calvo"
        ],
        [
          "Eduardo",
          "Castillo"
        ]
      ],
      "title": "Effect of different jitter-induced glottal pulse shape changes in periodicity perturbation measures",
      "original": "i15_2704",
      "page_count": 5,
      "order": 618,
      "p1": "2704",
      "pn": "2708",
      "abstract": [
        "Jitter has long been used to describe period instability in voiced speech signals. In spite of this long history of measuring and modeling jitter, the ways the different glottal pulse phases are affected by jitter differ across studies. The models have quite dissimilar implications, and their selection has been rather arbitrary in the literature. This paper describes different choices for modeling jitter within the glottal pulse, and their implications in several speech processing tasks. Based on the discussion presented, any model selection departs from some accredited standpoint in voiced speech processing. Experiments to evaluate the effect of different models on the performance of periodicity perturbation measures are carried out on synthetic signals. The results obtained show large differences across models, demonstrating the need to evaluate each model's fit to actual data.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-570",
      "author_area_id": "3",
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "kaushik15b_interspeech": {
      "authors": [
        [
          "Lakshmish",
          "Kaushik"
        ],
        [
          "Abhijeet",
          "Sangwan"
        ],
        [
          "John H. L.",
          "Hansen"
        ]
      ],
      "title": "Automatic audio sentiment extraction using keyword spotting",
      "original": "i15_2709",
      "page_count": 5,
      "order": 619,
      "p1": "2709",
      "pn": "2713",
      "abstract": [
        "Most existing methods for audio sentiment analysis use automatic speech recognition to convert speech to text, and feed the textual input to text-based sentiment classifiers. This study shows that such methods may not be optimal, and proposes an alternate architecture where a single keyword spotting system (KWS) is developed for sentiment detection. In the new architecture, the text-based sentiment classifier is utilized to automatically determine the most powerful sentiment-bearing terms, which is then used as the term list for KWS. In order to obtain a compact yet powerful term list, a new method is proposed to reduce text-based sentiment classifier model complexity while maintaining good classification accuracy. Finally, the term list information is utilized to build a more focused language model for the speech recognition system. The result is a single integrated solution which is focused on vocabulary that directly impacts classification. The proposed solution is evaluated on videos from YouTube.com and UT-Opinion corpus (which contains naturalistic opinionated audio collected in real-world conditions). Our experimental results show that the KWS based system significantly outperforms the traditional architecture in difficult practical tasks.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-571",
      "author_area_id": "3",
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "pasupat15_interspeech": {
      "authors": [
        [
          "Panupong",
          "Pasupat"
        ],
        [
          "Dilek",
          "Hakkani-T\u00fcr"
        ]
      ],
      "title": "Unsupervised relation detection using automatic alignment of query patterns extracted from knowledge graphs and query click logs",
      "original": "i15_2714",
      "page_count": 5,
      "order": 620,
      "p1": "2714",
      "pn": "2718",
      "abstract": [
        "Traditional methods for building spoken language understanding systems require manual rules or annotated data, which are expensive. In this work, we present an unsupervised method for bootstrapping a relation classifier, which identifies the knowledge graph relations present in an input query. Unlike existing work, we utilize only one knowledge graph entity instead of two for mining relevant query patterns from query click logs. As a result, the mined patterns can be used to infer both explicit relations (where the objects of the relations are expressed in the queries) and implicit relations (where the objects of the relations are being asked about). Using only the mined queries, the final classifier achieves an F-measure of 55.5%, which is significantly higher than the previous unsupervised learning baselines.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-572",
      "author_area_id": "11",
      "author_area_label": "Spoken Language Processing \u2013 Dialogue, Summarization, Understanding"
    },
    "nguyen15_interspeech": {
      "authors": [
        [
          "The Tung",
          "Nguyen"
        ],
        [
          "Graham",
          "Neubig"
        ],
        [
          "Hiroyuki",
          "Shindo"
        ],
        [
          "Sakriani",
          "Sakti"
        ],
        [
          "Tomoki",
          "Toda"
        ],
        [
          "Satoshi",
          "Nakamura"
        ]
      ],
      "title": "A latent variable model for joint pause prediction and dependency parsing",
      "original": "i15_2719",
      "page_count": 5,
      "order": 621,
      "p1": "2719",
      "pn": "2723",
      "abstract": [
        "The prosody of speech is closely related to syntactic structure of the spoken sentence, and thus analysis models that jointly consider these two types of information are promising. However, manual annotation of syntactic information and prosodic information such as pauses is laborious, and thus it can be difficult to obtain sufficient data to train such joint models. In this paper, we tackle this problem by introducing a joint pause prediction and dependency parsing model that treats pauses between consecutive words as latent variables. Using this model, it is possible to learn from not only data labeled with both syntax and pause information, but also data labeled with only syntactic information, which can be obtained in larger quantities. Experiments find that a joint pause prediction and dependency parsing model obtains better pause prediction F-measure than a decision-tree-based baseline trained on the same data, and that the addition of more data using the proposed latent variable model leads for further gains of up to 11.6 points in F-measure.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-573",
      "author_area_id": "11",
      "author_area_label": "Spoken Language Processing \u2013 Dialogue, Summarization, Understanding"
    },
    "bokaei15_interspeech": {
      "authors": [
        [
          "Mohammad Hadi",
          "Bokaei"
        ],
        [
          "Hossein",
          "Sameti"
        ],
        [
          "Yang",
          "Liu"
        ]
      ],
      "title": "Extractive meeting summarization through speaker zone detection",
      "original": "i15_2724",
      "page_count": 5,
      "order": 622,
      "p1": "2724",
      "pn": "2728",
      "abstract": [
        "In this paper we investigate the role of discourse analysis in extractive meeting summarization task. Specifically our proposed method comprises of two distinct steps. First we use a meeting segmentation algorithm in order to detect various functional parts of the input meeting. Afterwards, a two level scoring mechanism in a graph-based framework is used to score each dialogue act in order to extract the most valuable ones and include them in the extracted summary. We evaluate our proposed method on AMI and ICSI corpora and compare it with other state-of-the-art graph based algorithms according to various evaluation metrics. The experimental results show that our algorithm outperforms the other state-of-the-art ones according to most of the metrics and on both datasets.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-574",
      "author_area_id": "11",
      "author_area_label": "Spoken Language Processing \u2013 Dialogue, Summarization, Understanding"
    },
    "liu15h_interspeech": {
      "authors": [
        [
          "Shih-Hung",
          "Liu"
        ],
        [
          "Kuan-Yu",
          "Chen"
        ],
        [
          "Berlin",
          "Chen"
        ],
        [
          "Hsin-Min",
          "Wang"
        ],
        [
          "Hsu-Chun",
          "Yen"
        ],
        [
          "Wen-Lian",
          "Hsu"
        ]
      ],
      "title": "Positional language modeling for extractive broadcast news speech summarization",
      "original": "i15_2729",
      "page_count": 5,
      "order": 623,
      "p1": "2729",
      "pn": "2733",
      "abstract": [
        "Extractive summarization, with the intention of automatically selecting a set of representative sentences from a text (or spoken) document so as to concisely express the most important theme of the document, has been an active area of experimentation and development. A recent trend of research is to employ the language modeling (LM) approach for important sentence selection, which has proven to be effective for performing extractive summarization in an unsupervised fashion. However, one of the major challenges facing the LM approach is how to formulate the sentence models and estimate their parameters more accurately for each text (or spoken) document to be summarized. This paper extends this line of research and its contributions are three-fold. First, we propose a positional language modeling framework using different granularities of position-specific information to better estimate the sentence models involved in summarization. Second, we also explore to integrate the positional cues into relevance modeling through a pseudo-relevance feedback procedure. Third, the utilities of the various methods originated from our proposed framework and several well-established unsupervised methods are analyzed and compared extensively. Empirical evaluations conducted on a broadcast news summarization task seem to demonstrate the performance merits of our summarization methods.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-575",
      "author_area_id": "11",
      "author_area_label": "Spoken Language Processing \u2013 Dialogue, Summarization, Understanding"
    },
    "mokaram15_interspeech": {
      "authors": [
        [
          "Saeid",
          "Mokaram"
        ],
        [
          "Roger K.",
          "Moore"
        ]
      ],
      "title": "Speech-based location estimation of first responders in a simulated search and rescue scenario",
      "original": "i15_2734",
      "page_count": 5,
      "order": 624,
      "p1": "2734",
      "pn": "2738",
      "abstract": [
        "In our research, we explore possible solutions for extracting valuable information about first responders' (FR) location from speech communication channels during crisis response. Fine-grained identification of fundamental units of meaning (e. g. sentences, named entities and dialogue acts) is sensitive to high error rate in automatic transcriptions of noisy speech. However, looking from a topic-based perspective and utilizing text vectorization techniques such as Latent Dirichlet Allocation (LDA) make this more robust to such errors. In this paper, the location estimation problem is framed as a topic segmentation task on FRs' spoken reports about their observations and actions. Identifying the changes in the content of a report over time is an indication that the speaker has moved from one particular location to another. This provides an estimation about the location of the speaker. A goal-oriented human/human conversational speech corpus was collected based on an abstract communication model between FR and task leader during a search process in a simulation environment. Results show the effectiveness of a topic-based approach and especially low sensitivity of the LDA-based method to the highly imperfect automatic transcriptions.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-576",
      "author_area_id": "11",
      "author_area_label": "Spoken Language Processing \u2013 Dialogue, Summarization, Understanding"
    },
    "sousa15_interspeech": {
      "authors": [
        [
          "Tahir",
          "Sousa"
        ],
        [
          "Lucie",
          "Flekova"
        ],
        [
          "Margot",
          "Mieskes"
        ],
        [
          "Iryna",
          "Gurevych"
        ]
      ],
      "title": "Constructive feedback, thinking process and cooperation: assessing the quality of classroom interaction",
      "original": "i15_2739",
      "page_count": 5,
      "order": 625,
      "p1": "2739",
      "pn": "2743",
      "abstract": [
        "Analyzing and assessing the quality of classroom lessons on a range of quality dimensions is a number one educational research topic, as this allows developing teacher trainings and interventions to improve lesson quality. We model this assessment as a text classification task, exploiting linguistic features to predict the scores in several lesson quality dimensions relevant for educational researchers. Our work relies on a variety of phenomena, amongst them paralinguistic features, such as laughter, from real classroom interactions. We used these features to train machine learning models to assess various quality dimensions of school lessons. Our results show, that especially features focusing on the discourse and semantics are beneficial for this classification task.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-577",
      "author_area_id": "11",
      "author_area_label": "Spoken Language Processing \u2013 Dialogue, Summarization, Understanding"
    },
    "huang15f_interspeech": {
      "authors": [
        [
          "Dong-Yan",
          "Huang"
        ],
        [
          "Minghui",
          "Dong"
        ],
        [
          "Haizhou",
          "Li"
        ]
      ],
      "title": "A real-time variable-q non-stationary Gabor transform for pitch shifting",
      "original": "i15_2744",
      "page_count": 5,
      "order": 626,
      "p1": "2744",
      "pn": "2748",
      "abstract": [
        "This paper proposes a real-time variable-Q non-stationary Gabor transform (VQ-NSGT) system for speech pitch shifting. The system allows for time-frequency representations of speech on variable-Q (VQ) with perfect reconstruction and computational efficiency. The proposed VQ-NSGT phase vocoder can be used for pitch shifting by simple frequency translation (transposing partials along the frequency axis) instead of spectral stretching in frequency domain by the Fourier transform. In order to retain natural sounding pitch shifted speech, a hybrid of smoothly varying Q scheme is used to retain the formant structure of the original signal at both low and high frequencies. Moreover, the preservation of transients of speech are improved due to the high time resolution of VQ-NSGT at high frequencies. A sliced VQ-NSGT is used to retain inter-partials phase coherence by synchronized overlap-add method. Therefore, the proposed system lends itself to real-time processing while retaining the formant structure of the original signal and inter-partial phase coherence. The simulation results showed that the proposed approach is suitable for pitch shifting of both speech and music signals.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-578",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "aihara15_interspeech": {
      "authors": [
        [
          "Ryo",
          "Aihara"
        ],
        [
          "Testuya",
          "Takiguchi"
        ],
        [
          "Yasuo",
          "Ariki"
        ]
      ],
      "title": "Many-to-many voice conversion based on multiple non-negative matrix factorization",
      "original": "i15_2749",
      "page_count": 5,
      "order": 627,
      "p1": "2749",
      "pn": "2753",
      "abstract": [
        "We present in this paper an exemplar-based Voice Conversion (VC) method using Non-negative Matrix Factorization (NMF), which is different from conventional statistical VC. NMF-based VC has advantages of noise robustness and naturalness of converted voice compared to Gaussian Mixture Model (GMM)-based VC. However, because NMF-based VC is based on parallel training data of source and target speakers, we cannot convert the voice of arbitrary speakers in this framework. In this paper, we propose a many-to-many VC method that makes use of Multiple Non-negative Matrix Factorization (Multi-NMF). By using Multi-NMF, an arbitrary speaker's voice is converted to another arbitrary speaker's voice without the need for any input or output speaker training data. We assume that this method is flexible because we can adopt it to voice quality control or noise robust VC.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-579",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "kobayashi15b_interspeech": {
      "authors": [
        [
          "Kazuhiro",
          "Kobayashi"
        ],
        [
          "Tomoki",
          "Toda"
        ],
        [
          "Graham",
          "Neubig"
        ],
        [
          "Sakriani",
          "Sakti"
        ],
        [
          "Satoshi",
          "Nakamura"
        ]
      ],
      "title": "Statistical singing voice conversion based on direct waveform modification with global variance",
      "original": "i15_2754",
      "page_count": 5,
      "order": 628,
      "p1": "2754",
      "pn": "2758",
      "abstract": [
        "This paper presents techniques to improve the quality of voices generated through statistical singing voice conversion with direct waveform modification based on spectrum differential (DIFFSVC). The DIFFSVC method makes it possible to convert singing voice characteristics of a source singer into those of a target singer without using vocoder-based waveform generation. However, quality of the converted singing voice still degrades compared to that of a natural singing voice due to various factors, such as the over-smoothing of the converted spectral parameter trajectory. To alleviate this over-smoothing, we propose a technique to restore the global variance of the converted spectral parameter trajectory within the framework of the DIFFSVC method. We also propose another technique to specifically avoid over-smoothing at unvoiced frames. Results of subjective and objective evaluations demonstrate that the proposed techniques significantly improve speech quality of the converted singing voice while preserving the conversion accuracy of singer identity compared to the conventional DIFFSVC.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-580",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "tian15b_interspeech": {
      "authors": [
        [
          "Xiaohai",
          "Tian"
        ],
        [
          "Zhizheng",
          "Wu"
        ],
        [
          "Siu Wa",
          "Lee"
        ],
        [
          "Quy Hy",
          "Nguyen"
        ],
        [
          "Minghui",
          "Dong"
        ],
        [
          "Eng Siong",
          "Chng"
        ]
      ],
      "title": "System fusion for high-performance voice conversion",
      "original": "i15_2759",
      "page_count": 5,
      "order": 629,
      "p1": "2759",
      "pn": "2763",
      "abstract": [
        "Recently, a number of voice conversion methods have been developed. These methods attempt to improve conversion performance by using diverse mapping techniques in various acoustic domains, e.g. high-resolution spectra and low-resolution Mel-cepstral coefficients. Each individual method has its own pros and cons. In this paper, we introduce a system fusion framework, which leverages and synergizes the merits of these state-of-the-art and even potential future conversion methods. For instance, methods delivering high speech quality are fused with methods capturing speaker characteristics, bringing another level of performance gain. To examine the feasibility of the proposed framework, we select two state-of-the-art methods, Gaussian mixture model and frequency warping based systems, as a case study. Experimental results reveal that the fusion system outperforms each individual method in both objective and subjective evaluation, and demonstrate the effectiveness of the proposed fusion framework.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-581",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "alonso15_interspeech": {
      "authors": [
        [
          "Agustin",
          "Alonso"
        ],
        [
          "D.",
          "Erro"
        ],
        [
          "Eva",
          "Navas"
        ],
        [
          "Inma",
          "Hernaez"
        ]
      ],
      "title": "Speaker adaptation using only vocalic segments via frequency warping",
      "original": "i15_2764",
      "page_count": 5,
      "order": 630,
      "p1": "2764",
      "pn": "2768",
      "abstract": [
        "Speaker adaptation techniques allow hidden Markov model (HMM) based speech synthesis systems to mimic a target voice of which a few samples are available. However, usual adaptation approaches are not applicable when the target voice is dysarthric, i.e. the target speaker has an impairment which prevents the correct pronunciation of some phonemes. As a first step towards giving personalized synthetic voices to these particular speakers, this paper explores the possibility of adapting the whole statistical voice model using frequency warping (FW) based transformations trained exclusively with vowels. Perceptual evaluations performed for healthy voices show that the proposed method achieves reasonable results even when the adaptation data exhibit medium/low recording quality.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-582",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "tajiri15_interspeech": {
      "authors": [
        [
          "Yusuke",
          "Tajiri"
        ],
        [
          "Kou",
          "Tanaka"
        ],
        [
          "Tomoki",
          "Toda"
        ],
        [
          "Graham",
          "Neubig"
        ],
        [
          "Sakriani",
          "Sakti"
        ],
        [
          "Satoshi",
          "Nakamura"
        ]
      ],
      "title": "Non-audible murmur enhancement based on statistical conversion using air- and body-conductive microphones in noisy environments",
      "original": "i15_2769",
      "page_count": 5,
      "order": 631,
      "p1": "2769",
      "pn": "2773",
      "abstract": [
        "Non-Audible Murmur (NAM) is an extremely soft whispered voice detected by a special body-conductive microphone called a NAM microphone. Although NAM is a promising medium for silent speech communication, its quality is significantly degraded by its faint volume and spectral changes caused by body-conductive recording. To improve the quality of NAM, several enhancement methods based on statistical voice conversion (VC) techniques have been proposed, and their effectiveness has been confirmed in quiet environments. However, it can be expected that NAM will be used not only in quiet, but also in noisy environments, and it is thus necessary to develop enhancement methods that will also work in these cases. In this paper, we propose a framework for NAM enhancement using not only the NAM microphone but also an air-conductive microphone. Air- and body-conducted NAM signals are used as the input of VC to estimate a more naturally sounding speech signal. To clarify adverse effects of external noises on the performance of the proposed framework and investigate a possibility to alleviate them by revising VC models, we also implement noise-dependent VC models within the proposed framework. Experimental results demonstrate that the proposed framework yields significant improvements in the spectral conversion accuracy and listenability of enhanced speech under both quiet and noisy environments.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-583",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "polzehl15_interspeech": {
      "authors": [
        [
          "Tim",
          "Polzehl"
        ],
        [
          "Gina-Anne",
          "Levow"
        ]
      ],
      "title": "Advanced crowdsourcing for speech and beyond: introduction by the organizers",
      "original": "i15_4110",
      "page_count": 0,
      "order": 632,
      "p1": "0",
      "pn": "",
      "abstract": [
        "In this introduction we motivate future goals and current work on Advanced Crowdsourcing for Speech and Beyond. We will give a brief overview of the research grants awarded for best paper proposals in preparation of this session. 5 selected papers will be presented orally in the session, all other contributions will be presented as Poster. We include an overview of all accepted papers and posters in this introduction.\n",
        ""
      ]
    },
    "jyothi15_interspeech": {
      "authors": [
        [
          "Preethi",
          "Jyothi"
        ],
        [
          "Mark",
          "Hasegawa-Johnson"
        ]
      ],
      "title": "Transcribing continuous speech using mismatched crowdsourcing",
      "original": "i15_2774",
      "page_count": 5,
      "order": 633,
      "p1": "2774",
      "pn": "2778",
      "abstract": [
        "Mismatched crowdsourcing derives speech transcriptions using crowd workers unfamiliar with the language being spoken. This approach has been demonstrated for isolated word transcription tasks, but never yet for continuous speech. In this work, we demonstrate mismatched crowdsourcing of continuous speech with a word error rate of under 45% in a large-vocabulary transcription task of short speech segments. In order to scale mismatched crowdsourcing to continuous speech, we propose a number of new WFST pruning techniques based on explicitly low-entropy models of the acoustic similarities among orthographic symbols as understood within a transcriber community. We also provide an information-theoretic analysis and estimate the amount of information lost in transcription by the mismatched crowd workers to be under 5 bits.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-584",
      "author_area_id": "12",
      "author_area_label": "Spoken Language Processing \u2013 Translation, Information Retrieval, and Resources"
    },
    "chowdhury15b_interspeech": {
      "authors": [
        [
          "Shammur Absar",
          "Chowdhury"
        ],
        [
          "Marcos",
          "Calvo"
        ],
        [
          "Arindam",
          "Ghosh"
        ],
        [
          "Evgeny A.",
          "Stepanov"
        ],
        [
          "Ali Orkan",
          "Bayer"
        ],
        [
          "Giuseppe",
          "Riccardi"
        ],
        [
          "Fernando",
          "Garc\u00eda"
        ],
        [
          "Emilio",
          "Sanchis"
        ]
      ],
      "title": "Selection and aggregation techniques for crowdsourced semantic annotation task",
      "original": "i15_2779",
      "page_count": 5,
      "order": 634,
      "p1": "2779",
      "pn": "2783",
      "abstract": [
        "Crowdsourcing is an accessible and cost-effective alternative to traditional methods of collecting and annotating data. The application of crowdsourcing to simple tasks has been well investigated. However, complex tasks like semantic annotation transfer require workers to take simultaneous decisions on chunk segmentation and labeling while acquiring on-the-go domain-specific knowledge. The increased task complexity may generate low judgment agreement and/or poor performance. The goal of this paper is to cope with these crowdsourcing requirements with semantic priming and unsupervised quality control mechanisms. We aim at an automatic quality control that takes into account different levels of workers' expertise and annotation task performance. We investigate the judgment selection and aggregation techniques on the task of cross-language semantic annotation transfer. We propose stochastic modeling techniques to estimate the task performance of a worker on a particular judgment with respect to the whole worker group. These estimates are used for the selection of the best judgments as well as weighted consensus-based annotation aggregation. We demonstrate that the technique is useful for increasing the quality of collected annotations.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-585",
      "author_area_id": "12",
      "author_area_label": "Spoken Language Processing \u2013 Translation, Information Retrieval, and Resources"
    },
    "rothwell15_interspeech": {
      "authors": [
        [
          "Spencer",
          "Rothwell"
        ],
        [
          "Ahmad",
          "Elshenawy"
        ],
        [
          "Steele",
          "Carter"
        ],
        [
          "Daniela",
          "Braga"
        ],
        [
          "Faraz",
          "Romani"
        ],
        [
          "Michael",
          "Kennewick"
        ],
        [
          "Bob",
          "Kennewick"
        ]
      ],
      "title": "Controlling quality and handling fraud in large scale crowdsourcing speech data collections",
      "original": "i15_2784",
      "page_count": 5,
      "order": 635,
      "p1": "2784",
      "pn": "2788",
      "abstract": [
        "This paper presents strategies for measuring and assuring high quality when performing large-scale crowdsourcing data collections for acoustic model training. We examine different types of spam encountered while collecting and validating speech audio from unmanaged crowds and describe how we were able to identify these sources of spam and prevent our data from being tainted. We built a custom Android mobile application which funnels workers from a crowdsourcing platform and allows us to gather recordings and control conditions of the audio collection. We use a 2-step validation process which ensures that workers are paid only when they have actually used our application to complete their tasks. The collected audio is run through a second crowdsourcing job designed to validate that the speech matches the text with which the speakers were prompted. For the validation task, gold-standard test questions are used in combination with expected answer distribution rules and monitoring of worker activity levels over time to detect and expel likely spammers. Inter-annotator agreement is used to ensure high confidence of validated judgments. This process yielded millions of recordings with matching transcriptions in American English. The resulting set is 96% accurate with only minor errors.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-586",
      "author_area_id": "12",
      "author_area_label": "Spoken Language Processing \u2013 Translation, Information Retrieval, and Resources"
    },
    "rothwell15b_interspeech": {
      "authors": [
        [
          "Spencer",
          "Rothwell"
        ],
        [
          "Steele",
          "Carter"
        ],
        [
          "Ahmad",
          "Elshenawy"
        ],
        [
          "Vladislavs",
          "Dovgalecs"
        ],
        [
          "Safiyyah",
          "Saleem"
        ],
        [
          "Daniela",
          "Braga"
        ],
        [
          "Bob",
          "Kennewick"
        ]
      ],
      "title": "Data collection and annotation for state-of-the-art NER using unmanaged crowds",
      "original": "i15_2789",
      "page_count": 5,
      "order": 636,
      "p1": "2789",
      "pn": "2793",
      "abstract": [
        "This paper presents strategies for generating entity level annotated text utterances using unmanaged crowds. These utterances are then used to build state-of-the-art Named Entity Recognition (NER) models, a required component to build dialogue systems. First, a wide variety of raw utterances are collected through a variant elicitation task. We ensure that these utterances are relevant by feeding them back to the crowd for a domain validation task. We also flag utterances with potential spelling errors and verify these errors with the crowd before discarding them. These strategies, combined with a periodic CAPTCHA to prevent automated responses, allow us to collect high quality text utterances despite the inability to use the traditional gold test question approach for spam filtering. These utterances are then tagged with appropriate NER labels using unmanaged crowds. The crowd annotation was 23% more accurate and 29% more consistent than in-house annotation.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-587",
      "author_area_id": "12",
      "author_area_label": "Spoken Language Processing \u2013 Translation, Information Retrieval, and Resources"
    },
    "polzehl15b_interspeech": {
      "authors": [
        [
          "Tim",
          "Polzehl"
        ],
        [
          "Babak",
          "Naderi"
        ],
        [
          "Friedemann",
          "K\u00f6ster"
        ],
        [
          "Sebastian",
          "M\u00f6ller"
        ]
      ],
      "title": "Robustness in speech quality assessment and temporal training expiry in mobile crowdsourcing environments",
      "original": "i15_2794",
      "page_count": 5,
      "order": 637,
      "p1": "2794",
      "pn": "2798",
      "abstract": [
        "Following up on prior work on assessment of quality of speech in laboratory environments, this paper introduces two recently released mobile crowdsourcing paradigms. In comparison to web-based crowdsourcing, mobile crowdsourcing is carried out on smartphones or tablets in the field. Firstly, because involved hardware such as headphones cannot be known in this paradigm, we focus on the effect of mobile crowdsourcing on the assessment of quality of speech using quality degradation types which are described for the model in ITU-T Rec. P.863. As a result, indicators for degradation types that can reliably be assessed in mobile crowdsourcing paradigms are presented for the first time. This reliability is interpreted as robustness towards crowdsourcing assessment environments. Secondly, because working times, pauses and work fragmentation cannot be controlled, we introduce and focus on the analysis of temporarily expiring training certificates as qualifications. Accordingly, we design our study to automatically issue re-training job instances by timeouts, aiming at re-conditioning distracted or oblivious crowd workers. Results indicate a clear improvement in terms of correlation to laboratory test results, when applying the proposed training expiry. Eventually, the indicators presented contribute to build up preliminary guidelines on practical execution of quality assessment using mobile crowdsourcing.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-588",
      "author_area_id": "12",
      "author_area_label": "Spoken Language Processing \u2013 Translation, Information Retrieval, and Resources"
    },
    "naderi15_interspeech": {
      "authors": [
        [
          "Babak",
          "Naderi"
        ],
        [
          "Tim",
          "Polzehl"
        ],
        [
          "Ina",
          "Wechsung"
        ],
        [
          "Friedemann",
          "K\u00f6ster"
        ],
        [
          "Sebastian",
          "M\u00f6ller"
        ]
      ],
      "title": "Effect of trapping questions on the reliability of speech quality judgments in a crowdsourcing paradigm",
      "original": "i15_2799",
      "page_count": 5,
      "order": 638,
      "p1": "2799",
      "pn": "2803",
      "abstract": [
        "This paper reports on a crowdsourcing study investigating the influence of trapping questions on the reliability of the collected data. The crowd workers were asked to provide quality ratings for speech samples from a standard database. In addition, they were presented with different types of trapping questions, which were designed based on previous research. The ratings obtained from the crowd workers were compared to ratings collected in a laboratory setting. Best results (i.e. highest correlation with and lowest root-mean-square deviation from the lab ratings) were observed for the type of trapping question, for which a recorded voice was presented in the middle of a random stimuli. The voice explained to the workers that high quality responses are important to us, and asked them to select a specific item to show their concentration. We hypothesize that this kind of trapping question communicates the importance and the value of their work to the crowd workers. Based on Herzberg two-factor theory of job satisfaction, the presence of factors, such as acknowledgment and the feeling of being valued, facilitates satisfaction and motivation, and eventually leads to better performance.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-589",
      "author_area_id": "12",
      "author_area_label": "Spoken Language Processing \u2013 Translation, Information Retrieval, and Resources"
    },
    "leemann15b_interspeech": {
      "authors": [
        [
          "Adrian",
          "Leemann"
        ],
        [
          "Marie-Jos\u00e9",
          "Kolly"
        ],
        [
          "Jean-Philippe",
          "Goldman"
        ],
        [
          "Volker",
          "Dellwo"
        ],
        [
          "Ingrid",
          "Hove"
        ],
        [
          "Ibrahim",
          "Almajai"
        ],
        [
          "Sarah",
          "Grimm"
        ],
        [
          "Sylvain",
          "Robert"
        ],
        [
          "Daniel",
          "Wanitsch"
        ]
      ],
      "title": "Voice \u00c4pp: a mobile app for crowdsourcing Swiss German dialect data",
      "original": "i15_2804",
      "page_count": 5,
      "order": 639,
      "p1": "2804",
      "pn": "2808",
      "abstract": [
        "Crowdsourcing speech data through mobile applications is relatively new. In the present contribution we add to the existing body of research an innovative Android and iOS application called `Voice \u00c4pp'. The free app is pioneering in the sense that it leverages its function as a medium for science communication \u2014 thus attracting an extensive user base \u2014 to crowdsource audio and dialect data. The app was launched in early 2015 and has already been downloaded 19k times. Nearly half a million audio tokens have been crowdsourced. In this system levels contribution we describe the basic functionalities of the app \u2014 voice and dialect analysis \u2014, we present the scientific potential of the corpus created, and discuss methodological issues related to crowdsourcing audio data through mobile applications.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-590"
    },
    "loukina15b_interspeech": {
      "authors": [
        [
          "Anastassia",
          "Loukina"
        ],
        [
          "Melissa",
          "Lopez"
        ],
        [
          "Keelan",
          "Evanini"
        ],
        [
          "David",
          "Suendermann-Oeft"
        ],
        [
          "Klaus",
          "Zechner"
        ]
      ],
      "title": "Expert and crowdsourced annotation of pronunciation errors for automatic scoring systems",
      "original": "i15_2809",
      "page_count": 5,
      "order": 640,
      "p1": "2809",
      "pn": "2813",
      "abstract": [
        "This paper evaluates and compares different approaches to collecting judgments about pronunciation accuracy of non-native speech. We compare the common approach, which requires expert linguists to provide a detailed phonetic transcription of non-native English speech, with word-level judgments collected from multiple na\u00efve listeners using a crowd-sourcing platform. In both cases we found low agreement between annotators on what words should be marked as errors. We compare the error detection task to a simple transcription task in which the annotators were asked to transcribe the same fragments using standard English spelling. We argue that the transcription task is a simpler and more practical way of collecting annotations which also leads to more valid data for training an automatic scoring system.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-591",
      "author_area_id": "12",
      "author_area_label": "Spoken Language Processing \u2013 Translation, Information Retrieval, and Resources"
    },
    "kacorri15_interspeech": {
      "authors": [
        [
          "Hernisa",
          "Kacorri"
        ],
        [
          "Kaoru",
          "Shinkawa"
        ],
        [
          "Shin",
          "Saito"
        ]
      ],
      "title": "Capcap: an output-agreement game for video captioning",
      "original": "i15_2814",
      "page_count": 5,
      "order": 641,
      "p1": "2814",
      "pn": "2818",
      "abstract": [
        "CapCap is an output-agreement game that challenges players' listening and speaking skills. Players submit their transcriptions for short video segments against a countdown timer, in one of three pre-specified modes, to score points and support their team. Adding entertainment value, the game channels input toward captioning videos without monetary rewards. It deploys a novel human computation algorithm, which collects input from a crowd of non-experts, sequentially and in parallel, until a completion criterion is met. Rather than monetary incentive, CapCap uses motivational mechanisms like indirect feedback, mix of player skills, and community identification. Preliminary results from a field trial with mostly non-native English speakers improved the WER of English captions over ASR output.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-592",
      "author_area_id": "12",
      "author_area_label": "Spoken Language Processing \u2013 Translation, Information Retrieval, and Resources"
    },
    "burgos15_interspeech": {
      "authors": [
        [
          "Pepi",
          "Burgos"
        ],
        [
          "Eric",
          "Sanders"
        ],
        [
          "Catia",
          "Cucchiarini"
        ],
        [
          "Roeland van",
          "Hout"
        ],
        [
          "Helmer",
          "Strik"
        ]
      ],
      "title": "Auris populi: crowdsourced native transcriptions of Dutch vowels spoken by adult Spanish learners",
      "original": "i15_2819",
      "page_count": 5,
      "order": 642,
      "p1": "2819",
      "pn": "2823",
      "abstract": [
        "In this paper we report on a study in which Dutch vowels produced by Spanish adult L2 learners were orthographically transcribed by Dutch lay listeners through crowdsourcing. The aim of the crowdsourcing experiment was to investigate how the auris populi, the crowd's ear, would deal with possibly deviant L2 vowel realizations. We present data on the transcriptions of the non-expert listeners for all fifteen Dutch vowels. The results of our study indicate that Dutch vowels pronounced by Spanish learners were transcribed differently from their canonical (target) forms by native listeners. The listeners' transcriptions confirm findings of previous research based on expert annotations of Spanish learners' vowel realizations conducted at our lab, namely, that the five Spanish vowels seem to function as \u201cattractors\u201d for the larger set of the Dutch vowels. In general, the results are also in line with the outcomes of acoustic measurements of the same speech material, but there are some interesting discrepancies. We discuss these results with regard to previous studies on the speech production of adult Spanish learners of Dutch and outline perspectives for future research. Finally, given our results, we formulate some evaluative remarks on the auris populi methodology for future L2 speech research.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-593",
      "author_area_id": "12",
      "author_area_label": "Spoken Language Processing \u2013 Translation, Information Retrieval, and Resources"
    },
    "wray15_interspeech": {
      "authors": [
        [
          "Samantha",
          "Wray"
        ],
        [
          "Ahmed",
          "Ali"
        ]
      ],
      "title": "Crowdsource a little to label a lot: labeling a speech corpus of dialectal Arabic",
      "original": "i15_2824",
      "page_count": 5,
      "order": 643,
      "p1": "2824",
      "pn": "2828",
      "abstract": [
        "Arabic is a language with great dialectal variety, with Modern Standard Arabic (MSA) being the only standardized dialect. Spoken Arabic is characterized by frequent code-switching between MSA and Dialectal Arabic (DA). DA varieties are typically differentiated by region, but despite their wide-spread usage, they are under-resourced and lack viable corpora and tools necessary for speech recognition and natural language processing. Existing DA speech corpora are limited in scope, consisting of mainly telephone conversations and scripted speech.   In this paper we describe our efforts for using crowdsourcing to create a labeled multi-dialectal speech corpus. We obtained utterance-level dialect labels for 57 hours of high-quality audio from Al Jazeera consisting of four major varieties of DA: Egyptian, Levantine, Gulf, and North African. Using speaker linking to identify utterances spoken by the same speaker, and measures of label accuracy likelihood based on annotator behavior, we automatically labeled an additional 94 hours. The complete corpus contains 850 hours with approximately 18% DA speech.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-594",
      "author_area_id": "12",
      "author_area_label": "Spoken Language Processing \u2013 Translation, Information Retrieval, and Resources"
    },
    "gaur15_interspeech": {
      "authors": [
        [
          "Yashesh",
          "Gaur"
        ],
        [
          "Florian",
          "Metze"
        ],
        [
          "Yajie",
          "Miao"
        ],
        [
          "Jeffrey P.",
          "Bigham"
        ]
      ],
      "title": "Using keyword spotting to help humans correct captioning faster",
      "original": "i15_2829",
      "page_count": 5,
      "order": 644,
      "p1": "2829",
      "pn": "2833",
      "abstract": [
        "Automatic real-time captioning provides immediate and on demand access to spoken content in lectures or talks, and is a crucial accommodation for deaf and hard of hearing (DHH) people. However, in the presence of specialized content, like in technical talks, automatic speech recognition (ASR) still makes mistakes which may render the output incomprehensible. In this paper, we introduce a new approach, which allows audience or crowd workers, to quickly correct errors that they spot in ASR output. Prior approaches required the crowd worker to manually \u201cedit\u201d the ASR hypothesis by selecting and replacing the text, which is not suitable for real-time scenarios. Our approach is faster and allows the worker to simply type corrections for misrecognized words as soon as he or she spots them. The system then finds the most likely position for the correction in the ASR output using keyword search (KWS) and stitches the word into the ASR output. Our work demonstrates the potential of computation to incorporate human input quickly enough to be usable in real-time scenarios, and may be a better method for providing this vital accommodation to DHH people.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-595",
      "author_area_id": "12",
      "author_area_label": "Spoken Language Processing \u2013 Translation, Information Retrieval, and Resources"
    },
    "byun15_interspeech": {
      "authors": [
        [
          "Tara McAllister",
          "Byun"
        ],
        [
          "Elaine",
          "Hitchcock"
        ],
        [
          "Daphna",
          "Harel"
        ]
      ],
      "title": "Validating and optimizing a crowdsourced method for gradient measures of child speech",
      "original": "i15_2834",
      "page_count": 5,
      "order": 645,
      "p1": "2834",
      "pn": "2838",
      "abstract": [
        "There is broad consensus that speech sound development is a gradual process, with acoustic measures frequently revealing covert contrast between sounds perceived as identical. Well-constructed perceptual tasks using Visual Analog Scaling (VAS) can draw out these gradient differences. However, this method has not seen widespread uptake in speech acquisition research, possibly due to the time-intensive character of VAS data collection.   This project tested the validity of streamlined VAS data collection via crowdsourcing. It also addressed a methodological question that would be challenging to answer through conventional data collection: when collecting ratings of speech samples elicited from multiple individuals, should those samples be presented in fully random order, or grouped by speaker?   100 na\u00efve listeners recruited through Amazon Mechanical Turk provided VAS ratings for 120 /r/ words produced by 4 children before, during, and after intervention. 50 listeners rated the stimuli in fully randomized order and 50 in grouped-by-speaker order. Mean click location was compared against an acoustic standard, and standard error of click location was used to index variability. In both conditions, mean click location was highly correlated with the acoustic measure, supporting the validity of speech ratings obtained via crowdsourcing. Lower variability was observed in the grouped presentation condition.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-596",
      "author_area_id": "12",
      "author_area_label": "Spoken Language Processing \u2013 Translation, Information Retrieval, and Resources"
    },
    "wang15j_interspeech": {
      "authors": [
        [
          "Zhong-Qiu",
          "Wang"
        ],
        [
          "DeLiang",
          "Wang"
        ]
      ],
      "title": "Joint training of speech separation, filterbank and acoustic model for robust automatic speech recognition",
      "original": "i15_2839",
      "page_count": 5,
      "order": 646,
      "p1": "2839",
      "pn": "2843",
      "abstract": [
        "Robustness is crucial for automatic speech recognition systems in real-world environments. Speech enhancement/separation algorithms are normally used to enhance noisy speech before recognition. However, such algorithms typically introduce distortions unseen by acoustic models. In this study, we propose a novel joint training approach to reduce this distortion problem. At the training stage, we first concatenate a speech separation DNN, a filterbank and an acoustic model DNN to form a deeper network, and then jointly train all of them. This way, the separation frontend and filterbank can provide enhanced speech desired by the acoustic model. In addition, the linguistic information contained in the acoustic model can have a positive effect on the frontend and filterbank. Besides the commonly used log mel-spectrogram feature, we also add more robust features for acoustic modeling. Our system obtains 14.1% average word error rate on the noisy and reverberant CHIME-2 corpus (track 2), which outperforms the previous best result by 8.4% relatively.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-597",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition \u2013 Signal Processing, Acoustic Modeling, Robustness, and Adaptation"
    },
    "rath15_interspeech": {
      "authors": [
        [
          "Shakti",
          "Rath"
        ],
        [
          "Sunil",
          "Sivadas"
        ],
        [
          "Bin",
          "Ma"
        ]
      ],
      "title": "Joint environment and speaker normalization using factored front-end CMLLR",
      "original": "i15_2844",
      "page_count": 5,
      "order": 647,
      "p1": "2844",
      "pn": "2848",
      "abstract": [
        "The problem of joint compensation of environment and speaker variabilities is addressed. A factored feature-space transform, named factored front-end CMLLR (F-FE-CMLLR), is investigated, which comprises of the cascade of two transforms \u2014 front-end CMLLR for environment normalization and CMLLR for speaker normalization. In this paper, we propose an iterative estimation algorithm for F-FE-CMLLR. We believe that the iterative estimation helps to decouple the effect of the two acoustic factors, allowing each transform to learn the effect of only factor, thereby yielding an improvement in speech recognition performance compared to sequential estimation. However, it is noted that the estimation of environment transform yields full co-variance Gaussians in the GMM-HMM, which makes direct estimation computationally expensive. An efficient training algorithm is presented that helps to reduce the computational cost considerably. Further, it is shown that a row-by-row optimization procedure can be employed, which makes the algorithm more efficient and attractive. On the multi-condition Aurora 4 task and discriminatively trained GMM-HMM, it is shown that F-FE-CMLLR yields 11.6% and 8.7% relative improvements on two evaluation sets over the baseline features that is processed only by CMLLR for speaker normalization.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-598",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition \u2013 Signal Processing, Acoustic Modeling, Robustness, and Adaptation"
    },
    "abe15_interspeech": {
      "authors": [
        [
          "Akihiro",
          "Abe"
        ],
        [
          "Kazumasa",
          "Yamamoto"
        ],
        [
          "Seiichi",
          "Nakagawa"
        ]
      ],
      "title": "Robust speech recognition using DNN-HMM acoustic model combining noise-aware training with spectral subtraction",
      "original": "i15_2849",
      "page_count": 5,
      "order": 648,
      "p1": "2849",
      "pn": "2853",
      "abstract": [
        "Recently, acoustic models based on deep neural networks (DNNs) have been introduced and showed dramatic improvements over acoustic models based on GMM in a variety of tasks. In this paper, we considered the improvement of noise robustness of DNN. Inspired by Missing Feature Theory and static noise aware training, we proposed an approach that uses a noise-suppressed acoustic feature and estimated noise information as input of DNN. We used simple Spectral Subtraction as noise-suppression. As noise estimation, we used estimation per utterance or frame. In noisy speech recognition experiments, we compared the proposed method with other methods and the proposed method showed the superior performance than the other approaches. For noise estimation per utterance with log Mel Filterbank, we obtained 28.6% word error rate reduction compared with multi condition training, 5.9% reduction compared with noise adaptive training.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-599",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition \u2013 Signal Processing, Acoustic Modeling, Robustness, and Adaptation"
    },
    "yu15_interspeech": {
      "authors": [
        [
          "Chengzhu",
          "Yu"
        ],
        [
          "Atsunori",
          "Ogawa"
        ],
        [
          "Marc",
          "Delcroix"
        ],
        [
          "Takuya",
          "Yoshioka"
        ],
        [
          "Tomohiro",
          "Nakatani"
        ],
        [
          "John H. L.",
          "Hansen"
        ]
      ],
      "title": "Robust i-vector extraction for neural network adaptation in noisy environment",
      "original": "i15_2854",
      "page_count": 4,
      "order": 649,
      "p1": "2854",
      "pn": "2857",
      "abstract": [
        "In this study, we explore an i-vector based adaptation of deep neural network (DNN) in noisy environment. We first demonstrate the importance of encapsulating environment and channel variability into i-vectors for DNN adaptation in noisy conditions. To be able to obtain robust i-vector without losing noise and channel variability information, we investigate the use of parallel feature based i-vector extraction for DNN adaptation. Specifically, different types of features are used separately during two different stages of i-vector extraction namely   universal background model (UBM) state alignment and i-vector computation. To capture noise and channel-specific feature variation, the conventional MFCC features are still used for i-vector computation. However, much more robust features such as Vector Taylor Series (VTS) enhanced as well as bottleneck features are exploited for UBM state alignment. Experimental results on Aurora-4 show that the parallel feature-based i-vectors yield performance gains of up to 9.2% relative compared to a baseline DNN-HMM system and 3.3% compared to a system using conventional MFCC-based i-vectors.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-600",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition \u2013 Signal Processing, Acoustic Modeling, Robustness, and Adaptation"
    },
    "borsky15_interspeech": {
      "authors": [
        [
          "Michal",
          "Borsky"
        ],
        [
          "Petr",
          "Mizera"
        ],
        [
          "Petr",
          "Pollak"
        ]
      ],
      "title": "Spectrally selective dithering for distorted speech recognition",
      "original": "i15_2858",
      "page_count": 4,
      "order": 650,
      "p1": "2858",
      "pn": "2861",
      "abstract": [
        "The performance of speech recognition systems can be significantly degraded if the speech spectrum is distorted. This includes situations such as the usage of an improper recording device, enhancement technique or speech coder. This paper presents a front-end compensation method called spectrally selective dithering aimed at reconstructing the spectral characteristics of nonlinearly distorted speech. The technique is designed to detect the suppressed frequency bands in the speech signal and add a weighted amount of additive noise. The detection algorithm is based on the smoothness of the excitation signal spectrum obtained through analyzing LPC filtration. The gain of the added noise is estimated from the unaffected frequency bands. The practical usability of the algorithm has been studied in the task of MP3 speech recognition for very low bit-rates. The obtained results have demonstrated the advantage of using the proposed technique. We achieved up to 1.85% absolute WER reduction using the standard HMM-GMM architecture in LVCSR task.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-601",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition \u2013 Signal Processing, Acoustic Modeling, Robustness, and Adaptation"
    },
    "lu15d_interspeech": {
      "authors": [
        [
          "Liang",
          "Lu"
        ],
        [
          "Steve",
          "Renals"
        ]
      ],
      "title": "Feature-space speaker adaptation for probabilistic linear discriminant analysis acoustic models",
      "original": "i15_2862",
      "page_count": 5,
      "order": 651,
      "p1": "2862",
      "pn": "2866",
      "abstract": [
        "Probabilistic linear discriminant analysis (PLDA) acoustic models extend Gaussian mixture models by factorizing the acoustic variability using state-dependent and observation-dependent variables. This enables the use of higher dimensional acoustic features, and the capture of intra-frame feature correlations. In this paper, we investigate the estimation of speaker adaptive feature-space (constrained) maximum likelihood linear regression transforms from PLDA-based acoustic models. This feature-space speaker transformation estimation approach is potentially very useful due to the ability of PLDA acoustic models to use different types of acoustic features, for example applying these transforms to deep neural network (DNN) acoustic models for cross adaptation. We evaluated the approach on the Switchboard corpus, and observe significant word error reduction by using both the mel-frequency cepstral coefficients and DNN bottleneck features.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-602",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition \u2013 Signal Processing, Acoustic Modeling, Robustness, and Adaptation"
    },
    "cardinal15_interspeech": {
      "authors": [
        [
          "Patrick",
          "Cardinal"
        ],
        [
          "Najim",
          "Dehak"
        ],
        [
          "Yu",
          "Zhang"
        ],
        [
          "James",
          "Glass"
        ]
      ],
      "title": "Speaker adaptation using the i-vector technique for bottleneck features",
      "original": "i15_2867",
      "page_count": 5,
      "order": 652,
      "p1": "2867",
      "pn": "2871",
      "abstract": [
        "Deep Neural Networks (DNN) have been largely used and successfully applied in the context of speaker independent Automatic Speech Recognition (ASR). However, these models are not easily adapted to model a specific speaker characteristic. Recently, one approach was proposed to address this issue, which consists of using the I-vector representation as input to the DNN. The I-vector is playing the role of providing information about the speaker as well as the environmental conditions for a given recording. This approach achieved a significant improvement in the context of a hybrid system of DNN combined with Hidden Markov Model (HMM). In this paper, we study the effect of speaker adaptation based on the I-vector framework in the context of stacked bottleneck features. These features, extracted from a second level of DNNs, are modelled by a classical Gaussian Mixture Model (GMM) ASR system. The proposed approach achieved an absolute WER improvement of 1.2% on an Arabic Broadcast news task.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-603",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition \u2013 Signal Processing, Acoustic Modeling, Robustness, and Adaptation"
    },
    "karanasou15_interspeech": {
      "authors": [
        [
          "Penny",
          "Karanasou"
        ],
        [
          "Mark J. F.",
          "Gales"
        ],
        [
          "Philip C.",
          "Woodland"
        ]
      ],
      "title": "I-vector estimation using informative priors for adaptation of deep neural networks",
      "original": "i15_2872",
      "page_count": 5,
      "order": 653,
      "p1": "2872",
      "pn": "2876",
      "abstract": [
        "I-vectors are a well-known low-dimensional representation of speaker space and are becoming increasingly popular in adaptation of state-of-the-art deep neural network (DNN) acoustic models. One advantage of i-vectors is that they can be used with very little data, for example a single utterance. However, to improve robustness of the i-vector estimates with limited data, a prior is often used. Traditionally, a standard normal prior is applied to i-vectors, which is nevertheless not well suited to the increased variability of short utterances. This paper proposes a more informative prior, derived from the training data. As well as aiming to reduce the non-Gaussian behaviour of the i-vector space, it allows prior information at different levels, for example gender, to be used. Experiments on a US English Broadcast News (BN) transcription task for speaker and utterance i-vector adaptation show that more informative priors reduce the sensitivity to the quantity of data used to estimate the i-vector. The best configuration for this task was utterance-level test i-vectors enhanced with informative priors which gave a 13% relative reduction in word error rate over the baseline (no i-vectors) and a 5% over utterance-level test i-vectors with standard prior.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-604",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition \u2013 Signal Processing, Acoustic Modeling, Robustness, and Adaptation"
    },
    "garimella15_interspeech": {
      "authors": [
        [
          "Sri",
          "Garimella"
        ],
        [
          "Arindam",
          "Mandal"
        ],
        [
          "Nikko",
          "Strom"
        ],
        [
          "Bjorn",
          "Hoffmeister"
        ],
        [
          "Spyros",
          "Matsoukas"
        ],
        [
          "Sree Hari Krishnan",
          "Parthasarathi"
        ]
      ],
      "title": "Robust i-vector based adaptation of DNN acoustic model for speech recognition",
      "original": "i15_2877",
      "page_count": 5,
      "order": 654,
      "p1": "2877",
      "pn": "2881",
      "abstract": [
        "In the past, conventional i-vectors based on a Universal Background Model (UBM) have been successfully used as input features to adapt a Deep Neural Network (DNN) Acoustic Model (AM) for Automatic Speech Recognition (ASR). In contrast, this paper introduces Hidden Markov Model (HMM) based i-vectors that use HMM state alignment information from an ASR system for estimating i-vectors. Further, we propose passing these HMM based i-vectors though an explicit non-linear hidden layer of a DNN before combining them with standard acoustic features, such as log filter bank energies (LFBEs). To improve robustness to mismatched adaptation data, we also propose estimating i-vectors in a causal fashion for training the DNN, restricting the connectivity among hidden nodes in the DNN and applying a max-pool non-linearity at selected hidden nodes. In our experiments, these techniques yield about 5-7% relative word error rate (WER) improvement over the baseline speaker independent system in matched condition, and a substantial WER reduction for mismatched adaptation data.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-605",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition \u2013 Signal Processing, Acoustic Modeling, Robustness, and Adaptation"
    },
    "tomashenko15_interspeech": {
      "authors": [
        [
          "Natalia",
          "Tomashenko"
        ],
        [
          "Yuri",
          "Khokhlov"
        ]
      ],
      "title": "GMM-derived features for effective unsupervised adaptation of deep neural network acoustic models",
      "original": "i15_2882",
      "page_count": 5,
      "order": 655,
      "p1": "2882",
      "pn": "2886",
      "abstract": [
        "In this paper we investigate GMM-derived features recently introduced for adaptation of context-dependent deep neural network HMM (CD-DNN-HMM) acoustic models. We improve the previously proposed adaptation algorithm by applying the concept of speaker adaptive training (SAT) to DNNs built on GMM-derived features and by using fMLLR-adapted features for training an auxiliary GMM model. Traditional adaptation algorithms, such as maximum a posteriori adaptation (MAP) and feature space maximum likelihood linear regression (fMLLR) are performed for the auxiliary GMM model used in a SAT procedure for a DNN. Experimental results on theWall Street Journal (WSJ0) corpus show that the proposed adaptation technique can provide, on average, a 17-28% relative word error rate (WER) reduction on different adaptation sets under an unsupervised adaptation setup, compared to speaker independent (SI) DNN-HMM systems built on conventional features. We found that fMLLR adaptation for the SAT DNN trained on GMM-derived features outperforms fMLLR adaptation for the SAT DNN trained on conventional features by up to 14% of relative WER reduction.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-606",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition \u2013 Signal Processing, Acoustic Modeling, Robustness, and Adaptation"
    },
    "hsiao15b_interspeech": {
      "authors": [
        [
          "Roger",
          "Hsiao"
        ],
        [
          "Tim",
          "Ng"
        ],
        [
          "Stavros",
          "Tsakalidis"
        ],
        [
          "Long",
          "Nguyen"
        ],
        [
          "Richard",
          "Schwartz"
        ]
      ],
      "title": "Unsupervised adaptation for deep neural network using linear least square method",
      "original": "i15_2887",
      "page_count": 5,
      "order": 656,
      "p1": "2887",
      "pn": "2891",
      "abstract": [
        "In this paper, we propose a novel model based adaptation for deep neural networks based on a linear least square method. Our proposed algorithm can perform unsupervised adaptation even if the auto transcripts may have 60-70% of word error rate. We evaluate our algorithm on low resource languages, from the IARPA BABEL program, such as Assamese, Bengali, Haitian Creole, Lao and Zulu. Our experiments focus on unsupervised speaker, dialect and environment adaptation and we show that it can improve both speech recognition and keyword search performance.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-607",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition \u2013 Signal Processing, Acoustic Modeling, Robustness, and Adaptation"
    },
    "li15e_interspeech": {
      "authors": [
        [
          "Sheng",
          "Li"
        ],
        [
          "Xugang",
          "Lu"
        ],
        [
          "Yuya",
          "Akita"
        ],
        [
          "Tatsuya",
          "Kawahara"
        ]
      ],
      "title": "Ensemble speaker modeling using speaker adaptive training deep neural network for speaker adaptation",
      "original": "i15_2892",
      "page_count": 5,
      "order": 657,
      "p1": "2892",
      "pn": "2896",
      "abstract": [
        "In this paper, we introduce an ensemble speaker modeling using a speaker adaptive training (SAT) deep neural network (SAT-DNN). We first train a speaker-independent DNN (SI-DNN) acoustic model as a universal speaker model (USM). Based on the USM, a SAT-DNN is used to obtain a set of speaker-dependent models by assuming that all other layers except one speaker-dependent (SD) layer are shared among speakers. The speaker ensemble matrix is created by concatenating all of the SD neural weight matrices. With matrix factorization technique, an ensemble speaker subspace is extracted. When testing, an initial model for each target speaker is selected in this ensemble speaker subspace. Then, adaptation is carried out to obtain the final acoustic model for testing. In order to reduce the number of adaptation parameters, low-rank speaker subspace is further explored. We test our algorithm on lecture transcription task. Experimental results showed that our proposed method is effective for unsupervised speaker adaptation.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-608",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition \u2013 Signal Processing, Acoustic Modeling, Robustness, and Adaptation"
    },
    "doulaty15_interspeech": {
      "authors": [
        [
          "Mortaza",
          "Doulaty"
        ],
        [
          "Oscar",
          "Saz"
        ],
        [
          "Thomas",
          "Hain"
        ]
      ],
      "title": "Data-selective transfer learning for multi-domain speech recognition",
      "original": "i15_2897",
      "page_count": 5,
      "order": 658,
      "p1": "2897",
      "pn": "2901",
      "abstract": [
        "Negative transfer in training of acoustic models for automatic speech recognition has been reported in several contexts such as domain change or speaker characteristics. This paper proposes a novel technique to overcome negative transfer by efficient selection of speech data for acoustic model training. Here data is chosen on relevance for a specific target. A submodular function based on likelihood ratios is used to determine how acoustically similar each training utterance is to a target test set. The approach is evaluated on a wide-domain data set, covering speech from radio and TV broadcasts, telephone conversations, meetings, lectures and read speech. Experiments demonstrate that the proposed technique both finds relevant data and limits negative transfer. Results on a 6-hour test set show a relative improvement of 4% with data selection over using all data in PLP based models, and 2% with DNN features.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-609",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition \u2013 Signal Processing, Acoustic Modeling, Robustness, and Adaptation"
    },
    "lustyk15_interspeech": {
      "authors": [
        [
          "Tomas",
          "Lustyk"
        ],
        [
          "Petr",
          "Bergl"
        ],
        [
          "Tino",
          "Haderlein"
        ],
        [
          "Elmar",
          "N\u00f6th"
        ],
        [
          "Roman",
          "Cmejla"
        ]
      ],
      "title": "Language-independent method for analysis of German stuttering recordings",
      "original": "i15_2947",
      "page_count": 5,
      "order": 659,
      "p1": "2947",
      "pn": "2951",
      "abstract": [
        "The paper describes experiments where automatic acoustic algorithms initially intended to be used on Czech stuttering speakers were applied on recordings of German stuttering speakers. Four algorithms based on voice activity and abrupt spectral changes detection are introduced. The database consists of 34 speakers. The measure, the number of abrupt spectral changes in speech segments, reached a correlation with fluency rating of 0.85. The other measures have also good agreement with subjective evaluation. Results indicate that it could be basically possible to do language-independent analysis of stuttering, here demonstrated on read recordings of German speakers.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-610",
      "author_area_id": "1",
      "author_area_label": "Speech Perception, Production, and Acquisition"
    },
    "alnasheri15_interspeech": {
      "authors": [
        [
          "Ahmed",
          "Al-nasheri"
        ],
        [
          "Zulfiqar",
          "Ali"
        ],
        [
          "Ghulam",
          "Muhammad"
        ],
        [
          "Mansour",
          "Alsulaiman"
        ]
      ],
      "title": "An investigation of MDVP parameters for voice pathology detection on three different databases",
      "original": "i15_2952",
      "page_count": 5,
      "order": 660,
      "p1": "2952",
      "pn": "2956",
      "abstract": [
        "In this paper, an investigation of Multi-Dimensional Voice Program (MDVP) parameters to automatically detect voice pathology in three different databases was conducted. MDVP parameters are very popular acoustic analysis among physician/clinician to detect voice pathology. The main objective of the paper is to find out the most prominent MDVP parameters irrespective to the databases used. In this study, three different databases from three distinct languages were used. The databases are Arabic voice pathology database (AVPD), Massachusetts Eye and Ear Infirmary (MEEI) (English database), and Saarbruecken Voice Database (SVD) (German database). Only the sustained vowel /a/ was used in the study. Fisher discrimination ratio (FDR) was applied to rank the parameters. Support vector machine (SVM) was used to perform the detection process. The experimental results demonstrated that there was clear difference of the performance of the MDVP parameters using these databases. The highly ranked parameters were also different from one database to another. The accuracies that achieved are varied from one database to another with the same number of MVPD parameters. The best accuracies obtained by using the three highest MDVP parameters arranged according to FDR were 99.68%, 88.21% and 72.53 for SVD, MEEI and AVPD, respectively.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-611",
      "author_area_id": "1",
      "author_area_label": "Speech Perception, Production, and Acquisition"
    },
    "wu15f_interspeech": {
      "authors": [
        [
          "Jiantao",
          "Wu"
        ],
        [
          "Ping",
          "Yu"
        ],
        [
          "Nan",
          "Yan"
        ],
        [
          "Lan",
          "Wang"
        ],
        [
          "Xiaohui",
          "Yang"
        ],
        [
          "Manwa L.",
          "Ng"
        ]
      ],
      "title": "Energy distribution analysis and nonlinear dynamical analysis of adductor spasmodic dysphonia",
      "original": "i15_2957",
      "page_count": 5,
      "order": 661,
      "p1": "2957",
      "pn": "2961",
      "abstract": [
        "The present study investigated the voice quality associated with adductor spasmodic dysphonia by means of various acoustical measures. Energy distribution analyses and nonlinear dynamical measurements were used to depict the differences among voices associated with adductor spasmodic dysphonia (ADSD), vocal nodules (VN) and normal voices. Possible acoustical differences between voices associated with unilateral vocal fold paralysis (UVFP) and ADSD, UVFP and normal voice were investigated. Noise-to-harmonic ratio (NHR), harmonic-to-noise ratio (HNR), glottal-to-noise excitation (GNE), empirical mode decomposition excitation ratio (EMD-ER), nonlinear recurrence period density entropy (RPDE), detrended fluctuation analysis (DFA), correlation dimension (D2), and permutation entropy (PE) values were obtained from the sustained vowel /a/ produced by the subjects. Results revealed high specificity of these acoustic measures in distinguishing the voice quality of ADSD, VN, and UVFP voices from normal voices. In addition, combining GNE and D2 measures appears to be effective in distinguishing ADSD from normal and VN voices.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-612",
      "author_area_id": "1",
      "author_area_label": "Speech Perception, Production, and Acquisition"
    },
    "kasisopa15_interspeech": {
      "authors": [
        [
          "Benjawan",
          "Kasisopa"
        ],
        [
          "Nittayapa",
          "Klangpornkun"
        ],
        [
          "Denis",
          "Burnham"
        ]
      ],
      "title": "Auditory-visual tone perception in hearing impaired Thai listeners",
      "original": "i15_2962",
      "page_count": 5,
      "order": 662,
      "p1": "2962",
      "pn": "2966",
      "abstract": [
        "This study investigated the effects of hearing impairment and auditory vs. auditory-visual perception of lexical tone by native Thai hearing impaired listeners: Hearing Impaired with Hearing Aids (HI+HA), Hearing Impaired without Hearing Aids (HI-HA), and Normal Hearing (NH). Adults' discrimination of the 5 Thai tones was investigated in auditory-visual (AV), auditory-only (AO), and visual-only (VO) conditions. Generally, NH participants performed better than the two HI groups with hearing aids facilitating tone perception (HI+HA > HI-HA). The Falling-Rising (FR) pair of tones was the easiest to discriminate for all three groups and there was a similar ranking of the relative discriminability of all 10 tone contrasts across groups. There was better tone discrimination in AV than in AO and both were much better than VO; and this was equally the case for all groups. The results show that Hearing Impaired individuals either with or without hearing aids can and do use visual speech information to augment auditory perception of tone, but do so in a similar, not a significantly more enhanced manner as the Normal Hearing individuals.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-613",
      "author_area_id": "1",
      "author_area_label": "Speech Perception, Production, and Acquisition"
    },
    "rong15_interspeech": {
      "authors": [
        [
          "Panying",
          "Rong"
        ],
        [
          "Yana",
          "Yunusova"
        ],
        [
          "Jordan R.",
          "Green"
        ]
      ],
      "title": "Speech intelligibility decline in individuals with fast and slow rates of ALS progression",
      "original": "i15_2967",
      "page_count": 5,
      "order": 663,
      "p1": "2967",
      "pn": "2971",
      "abstract": [
        "Based on the slope of speech intelligibility decline, 54 individuals with amyotrophic lateral sclerosis (ALS) were classified as fast or slow bulbar disease progressors. Following group assignment, the course of speech decline was modeled separately for these two subgroups. Fast progressors were characterized by a three-phase progression pattern with intelligibility declining at a progressively faster rate within each phase. Slow progressors were characterized by a consistent rate of intelligibility decline and thus, a single phase progression pattern. The approach used to identify fast and slow progressors may be useful in the future for stratifying patients enrolled in clinical trials. The subgroup-based models of intelligibility provide an empirical basis for making predictions about the timing of speech loss and the impending need for assistive communication.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-614",
      "author_area_id": "1",
      "author_area_label": "Speech Perception, Production, and Acquisition"
    },
    "a15_interspeech": {
      "authors": [
        [
          "Rong Na",
          "A"
        ],
        [
          "Koichi",
          "Mori"
        ],
        [
          "Naomi",
          "Sakai"
        ]
      ],
      "title": "Latency analysis of speech shadowing reveals processing differences in Japanese adults who do and do not stutter",
      "original": "i15_2972",
      "page_count": 5,
      "order": 664,
      "p1": "2972",
      "pn": "2976",
      "abstract": [
        "Speech shadowing is a dual-task paradigm that could reveal certain features of speech processing, where the shadowing latency between the onsets of heard and reproduced speech has often been a key investigation tool. The present study investigated the shadowing latencies in native Japanese adults who do (AWS) and do not stutter (AWNS), with relevant analysis of speech errors. Fifteen AWS and fourteen AWNS participated in the study. They were required to shadow two meaningful Japanese passages of approximately 1.4 min. Fifty phrase onsets were chosen for measuring shadowing latencies. The resultant latencies were longer than previously reported for English in both groups, which most likely reflects the larger mean syllable numbers per word of Japanese than of English. The AWS group had a significantly shorter latency than the AWNS. Besides, it was significantly more error-prone than the AWNS. The eleven AWS whose latency was more than 500 ms showed a significant negative correlation (trade off) between speech errors and latencies, which was not the case with the AWNS group. Those results imply that only the AWS may have hit the limit of their working memory capacity during shadowing. Thus the shadowing paradigm brought new insights into speech processing and stuttering.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-615",
      "author_area_id": "1",
      "author_area_label": "Speech Perception, Production, and Acquisition"
    },
    "bigi15_interspeech": {
      "authors": [
        [
          "Brigitte",
          "Bigi"
        ],
        [
          "Katarzyna",
          "Klessa"
        ],
        [
          "Laurianne",
          "Georgeton"
        ],
        [
          "Christine",
          "Meunier"
        ]
      ],
      "title": "A syllable-based analysis of speech temporal organization: a comparison between speaking styles in dysarthric and healthy populations",
      "original": "i15_2977",
      "page_count": 5,
      "order": 665,
      "p1": "2977",
      "pn": "2981",
      "abstract": [
        "A comparison of how healthy and dysarthric pathological speakers adapt their production is a way to better understand the processes and constraints that interact during speech production in general. The present study focuses on spontaneous speech obtained with varying recording scenarios from five different groups of speakers. Patients suffering from motor speech disorder (dysarthria) affecting speech production are compared to healthy speakers. Three types of dysarthria have been explored: Parkinson's Disease, Amyotrophic Lateral Sclerosis and Cerebellar ataxia. This paper first presents general figures based on syllable-level annotation mining, including detailed information about healthy/pathological speakers variability. Then, we report on the results of automatic timing parsing of interval sequences in speech syllable annotations performed using TGA (Time Group Analysis) methodology. We observed that mean syllable-based speaking rates in time groups for the healthy speakers were higher than those measured in the recordings of dysarthric speakers. The variability in timing patterns (duration regression slopes, intercepts, and nPVI) depended also on the speaking styles in particular populations.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-616",
      "author_area_id": "1",
      "author_area_label": "Speech Perception, Production, and Acquisition"
    },
    "meyer15_interspeech": {
      "authors": [
        [
          "Bernd T.",
          "Meyer"
        ],
        [
          "Birger",
          "Kollmeier"
        ],
        [
          "Jasper",
          "Ooster"
        ]
      ],
      "title": "Autonomous measurement of speech intelligibility utilizing automatic speech recognition",
      "original": "i15_2982",
      "page_count": 5,
      "order": 666,
      "p1": "2982",
      "pn": "2986",
      "abstract": [
        "Measures of speech intelligibility are an essential tool for diagnosing hearing impairment and for tuning hearing aid parameters. This study explores the potential of automatic speech recognition (ASR) for conducting autonomous listening tests. In these tests (e.g., in the Oldenburg sentence matrix test employed here) the responses of participants are usually logged by a (human) supervisor. The target value is the speech reception threshold (SRT), i.e., the signal-to-noise ratio at which 50% speech intelligibility is achieved. We explore what ASR error rates can be obtained for such responses, and how ASR errors affect the measured SRT value. To this end, a speech database was recorded that contains utterances from 20 speakers and covers different levels of language complexity, ranging from simple five-word sentences to utterances as produced in typical human-human interactions during testing. While for the most complex speech material, the achievable SRT accuracy was not satisfactory, the ASR performance for sentences without out-of-vocabulary words was below 1.3% and hence sufficient to obtain a test-retest reliability of only 0.5 dB, which is identical to the reliability in human-supervised tests.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-617",
      "author_area_id": "1",
      "author_area_label": "Speech Perception, Production, and Acquisition"
    },
    "knoll15_interspeech": {
      "authors": [
        [
          "Monja Angelika",
          "Knoll"
        ],
        [
          "Melissa",
          "Johnstone"
        ],
        [
          "Charlene",
          "Blakely"
        ]
      ],
      "title": "Can you hear me? acoustic modifications in speech directed to foreigners and hearing-impaired people",
      "original": "i15_2987",
      "page_count": 4,
      "order": 667,
      "p1": "2987",
      "pn": "2990",
      "abstract": [
        "People modify their voice to accommodate the communicative needs of their audience. Previously, speech recipient groups with communicative needs such as foreigners and infants have been investigated, however, there is a lack of research into speech directed to hearing-impaired people. The aim of our study was to investigate prosodic modifications in speech directed to hearing-impaired people and foreigners, and the benefit of such modifications to the listener. Ten speakers were recorded in natural interactions with a hearing-impaired person (HIDS), a foreigner (FDS) and a native speaker (ADS). The resulting speech was acoustically analysed for duration, vowel hyperarticulation, intensity, mean fundamental frequency (F0) and F0 range. There was no significant difference in duration, F0 range or intensity between the conditions. HIDS and FDS displayed significantly higher mean F0 and hyperarticulation than ADS. The speech was also rated by twenty speakers for positive vocal affect and intelligibility. HIDS was perceived to be more positive and intelligible than both FDS and ADS, with no difference between the latter two groups. These results show that people modify the acoustic properties of their voice in interactions with foreigners and hearing-impaired people, but that the benefits of these modifications are more noticeable in HIDS than FDS.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-618",
      "author_area_id": "1",
      "author_area_label": "Speech Perception, Production, and Acquisition"
    },
    "yeung15_interspeech": {
      "authors": [
        [
          "Yu Ting",
          "Yeung"
        ],
        [
          "Ka Ho",
          "Wong"
        ],
        [
          "Helen",
          "Meng"
        ]
      ],
      "title": "Improving automatic forced alignment for dysarthric speech transcription",
      "original": "i15_2991",
      "page_count": 5,
      "order": 668,
      "p1": "2991",
      "pn": "2995",
      "abstract": [
        "Dysarthria is a motor speech disorder due to neurologic deficits. The impaired movement of muscles for speech production leads to disordered speech where utterances have prolonged pause intervals, slow speaking rates, poor articulation of phonemes, syllable deletions, etc. These present challenges towards the use of speech technologies for automatic processing of dysarthric speech data. In order to address these challenges, this work begins by addressing the performance degradation faced in forced alignment. We perform initial alignments to locate long pauses in dysarthric speech and make use of the pause intervals as anchor points. We apply speech recognition for word lattice outputs for recovering the time-stamps of the words in disordered or incomplete pronunciations. By verifying the initial alignments with word lattices, we obtain the reliably aligned segments. These segments provide constraints for new alignment grammars, that can improve alignment and transcription quality. We have applied the proposed strategy to the TORGO corpus and obtained improved alignments for most dysarthric speech data, while maintaining good alignments for non-dysarthric speech data.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-619",
      "author_area_id": "1",
      "author_area_label": "Speech Perception, Production, and Acquisition"
    },
    "wodarczak15_interspeech": {
      "authors": [
        [
          "Marcin",
          "W\u0142odarczak"
        ],
        [
          "Mattias",
          "Heldner"
        ],
        [
          "Jens",
          "Edlund"
        ]
      ],
      "title": "Communicative needs and respiratory constraints",
      "original": "i15_3051",
      "page_count": 5,
      "order": 669,
      "p1": "3051",
      "pn": "3055",
      "abstract": [
        "This study investigates timing of communicative behaviour with respect to speaker's respiratory cycle. The data is drawn from a corpus of multiparty conversations in Swedish. We find that while longer utterances (> 1 s) are tied, predictably, primarily to exhalation onset, shorter vocalisations are spread more uniformly across the respiratory cycle. In addition, nods, which are free from any respiratory constraints, are most frequently found around exhalation offsets, where respiratory requirements for even a short utterance are not satisfied. We interpret the results to reflect the economy principle in speech production, whereby respiratory effort, associated primarily with starting a new respiratory cycle, is minimised within the scope of speaker's communicative goals.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-620",
      "author_area_id": "2",
      "author_area_label": "Phonetics, Phonology, and Prosody"
    },
    "reichel15b_interspeech": {
      "authors": [
        [
          "Uwe D.",
          "Reichel"
        ],
        [
          "Nina",
          "P\u00f6rner"
        ],
        [
          "Dianne",
          "Nowack"
        ],
        [
          "Jennifer",
          "Cole"
        ]
      ],
      "title": "Analysis and classification of cooperative and competitive dialogs",
      "original": "i15_3056",
      "page_count": 5,
      "order": 670,
      "p1": "3056",
      "pn": "3060",
      "abstract": [
        "Cooperative and competitive game dialogs are comparatively examined with respect to temporal, basic text-based, and dialog act characteristics. The condition-specific speaker strategies are amongst others well reflected in distinct dialog act probability distributions, which are discussed in the context of the Gricean Cooperative Principle and of Relevance Theory. Based on the extracted features, we trained Bayes classifiers and support vector machines to predict the dialog condition, that yielded accuracies from 90 to 100%. Taken together the simplicity of the condition classification task and its probabilistic expressiveness for dialog acts suggests a two-stage classification of condition and dialog acts.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-621",
      "author_area_id": "2",
      "author_area_label": "Phonetics, Phonology, and Prosody"
    },
    "cervone15_interspeech": {
      "authors": [
        [
          "Alessandra",
          "Cervone"
        ],
        [
          "Catherine",
          "Lai"
        ],
        [
          "Silvia",
          "Pareti"
        ],
        [
          "Peter",
          "Bell"
        ]
      ],
      "title": "Towards automatic detection of reported speech in dialogue using prosodic cues",
      "original": "i15_3061",
      "page_count": 5,
      "order": 671,
      "p1": "3061",
      "pn": "3065",
      "abstract": [
        "The phenomenon of reported speech \u2014 whereby we quote the words, thoughts and opinions of others, or recount past dialogue \u2014 is widespread in conversational speech. Detecting such quotations automatically has numerous applications: for example, in enhancing automatic transcription or spoken language understanding applications. However, the task is challenging, not least because lexical cues of quotations are frequently ambiguous or not present in spoken language. The aim of this paper is to identify potential prosodic cues of reported speech which could be used, along with the lexical ones, to automatically detect quotations and ascribe them to their rightful source, that is reconstructing their attribution relations. In order to do so we analyze SARC, a small corpus of telephone conversations that we have annotated with attribution relations. The results of the statistical analysis performed on the data show how variations in pitch, intensity, and timing features can be exploited as cues of quotations. Furthermore, we build a SVM classifier which integrates lexical and prosodic cues to automatically detect quotations in speech that performs significantly better than chance.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-622",
      "author_area_id": "2",
      "author_area_label": "Phonetics, Phonology, and Prosody"
    },
    "rosenberg15_interspeech": {
      "authors": [
        [
          "Andrew",
          "Rosenberg"
        ],
        [
          "Raul",
          "Fernandez"
        ],
        [
          "Bhuvana",
          "Ramabhadran"
        ]
      ],
      "title": "Modeling phrasing and prominence using deep recurrent learning",
      "original": "i15_3066",
      "page_count": 5,
      "order": 672,
      "p1": "3066",
      "pn": "3070",
      "abstract": [
        "Models for the prediction of prosodic events, such as pitch accents and phrasal boundaries, often rely on machine learning models that combine a set of input features aggregated over a finite, and usually short, number of observations to model context. Dynamic models go a step further by explicitly incorporating a model of state sequence, but even then, many practical implementations are limited to a low-order finite-state machine. This Markovian assumption, however, does not properly address the interaction between short- and long-term contextual factors that is known to affect the realization and placement of these prosodic events. Bidirectional Recurrent Neural Networks (BiRNNs) are a class of models that overcome this limitation by predicting the outputs as a function of a state variable that accumulates information over the entire input sequence, and by stacking several layers to form a deep architecture able to extract more structure from the input features. These models have already demonstrated state-of-the-art performance on some prosodic regression tasks. In this work we examine a new application of BiRNNs to the task of classifying categorical prosodic events, and demonstrate that they outperform baseline systems.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-623",
      "author_area_id": "2",
      "author_area_label": "Phonetics, Phonology, and Prosody"
    },
    "looze15b_interspeech": {
      "authors": [
        [
          "C\u00e9line De",
          "Looze"
        ],
        [
          "Irena",
          "Yanushevskaya"
        ],
        [
          "Andy",
          "Murphy"
        ],
        [
          "Eoghan",
          "O'Connor"
        ],
        [
          "Christer",
          "Gobl"
        ]
      ],
      "title": "Pitch declination and reset as a function of utterance duration in conversational speech data",
      "original": "i15_3071",
      "page_count": 5,
      "order": 673,
      "p1": "3071",
      "pn": "3075",
      "abstract": [
        "This paper describes the declination trends of f0 in conversational speech data. A 10-minute dialogue interaction from a corpus of spontaneous speech was annotated to identify inter-silence units (ISU) and turns. Detailed annotation of the ISUs was conducted in terms of communicative types and pitch patterns. f0 declination was measured by (1) fitting a regression line to f0 trajectories and (2) by fitting additional regression lines to the data points below and above the original (central) regression line. The slope of declination as well as the height of ISU/turn-initial f0 peak were examined as a function of the duration of the ISU or turn. The results suggest that declination is indeed present in conversational speech data, at the level of both the ISU and the turn (73% of the analysed ISUs exhibited negative f0 declination slope). There is a tendency for the steepness of the slope to decrease and the height of IS-turn-initial f0 peak to increase as the duration of the ISU or turn increases. The results are discussed in the context of Projection and Reaction theories and of Hard vs. Soft preplanning of speech production. The findings are of potential interest for the development of human-machine dialogue systems.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-624",
      "author_area_id": "2",
      "author_area_label": "Phonetics, Phonology, and Prosody"
    },
    "freeman15_interspeech": {
      "authors": [
        [
          "Valerie",
          "Freeman"
        ],
        [
          "Gina-Anne",
          "Levow"
        ],
        [
          "Richard",
          "Wright"
        ],
        [
          "Mari",
          "Ostendorf"
        ]
      ],
      "title": "Investigating the role of `yeah' in stance-dense conversation",
      "original": "i15_3076",
      "page_count": 5,
      "order": 674,
      "p1": "3076",
      "pn": "3080",
      "abstract": [
        "This study investigates characteristics of stance-related discourse function, stance strength, and polarity in uses of the word `yeah.' In an annotated corpus of 20 talker dyads engaged in collaborative tasks, over 2300 `yeahs' fall into six common stance-act categories. While agreement, usually with weak, positive stance, accounts for about three-quarters of the instances, opinion-offering, convincing, reluctance to accept an idea, backchannels, and no-stance represent other common stance-related uses. We assess combinations of acoustic-prosodic characteristics (duration, intensity, pitch) to identify those which differentiate these stance categories for `yeah' and to determine how they relate to levels of stance strength and polarity. Differences in vowel duration and intensity help to differentiate these fine-grained functions of `yeah.' Within the larger agreement category, we can further assess the effects of stance strength and polarity, finding that positive polarity is signaled by higher pitch, lower intensity, and longer vowel duration, while greater stance strength shows higher pitch and intensity. Finally, a small set of negative `yeahs' is examined for more specific stance functions which may be distinguishable by differing pitch and intensity contours.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-625",
      "author_area_id": "2",
      "author_area_label": "Phonetics, Phonology, and Prosody"
    },
    "choi15b_interspeech": {
      "authors": [
        [
          "Jiyoun",
          "Choi"
        ],
        [
          "Mirjam",
          "Broersma"
        ],
        [
          "Anne",
          "Cutler"
        ]
      ],
      "title": "Enhanced processing of a lost language: linguistic knowledge or linguistic skill?",
      "original": "i15_3110",
      "page_count": 5,
      "order": 675,
      "p1": "3110",
      "pn": "3114",
      "abstract": [
        "Same-different discrimination judgments for pairs of Korean stop consonants, or of Japanese syllables differing in phonetic segment length, were made by adult Korean adoptees in the Netherlands, by matched Dutch controls, and Korean controls. The adoptees did not outdo either control group on either task, although the same individuals had performed significantly better than matched controls on an identification learning task. This suggests that early exposure to multiple phonetic systems does not specifically improve acoustic-phonetic skills; rather, enhanced performance suggests retained language knowledge.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-626",
      "author_area_id": "1",
      "author_area_label": "Speech Perception, Production, and Acquisition"
    },
    "grohe15_interspeech": {
      "authors": [
        [
          "Ann-Kathrin",
          "Grohe"
        ],
        [
          "Gregory J.",
          "Poarch"
        ],
        [
          "Adriana",
          "Hanul\u00edkov\u00e1"
        ],
        [
          "Andrea",
          "Weber"
        ]
      ],
      "title": "Production inconsistencies delay adaptation to foreign accents",
      "original": "i15_3115",
      "page_count": 5,
      "order": 676,
      "p1": "3115",
      "pn": "3119",
      "abstract": [
        "The effects of production inconsistencies and speaker's accented production preferences on speech comprehension were investigated in an eyetracking experiment. Using the visual world paradigm, native speakers of German with L2 English listened to single English words produced by a German speaker that had their th either pronounced canonically or substituted with an /s/ or a /t/. Looks to the target word were most likely for the canonical pronunciation and did not differ between the substitutes. However, target looks increased for items with th substitutions in the course of the experiment, indicating slow adaptation to inconsistently foreign accented speech.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-627",
      "author_area_id": "1",
      "author_area_label": "Speech Perception, Production, and Acquisition"
    },
    "ordin15_interspeech": {
      "authors": [
        [
          "Mikhail",
          "Ordin"
        ],
        [
          "Leona",
          "Polyanskaya"
        ]
      ],
      "title": "Acquisition of English speech rhythm by monolingual children",
      "original": "i15_3120",
      "page_count": 5,
      "order": 677,
      "p1": "3120",
      "pn": "3124",
      "abstract": [
        "We investigated how English rhythmic patterns develop in the course of first language acquisition by children between four and twelve years. We have empirically confirmed that rhythm becomes increasingly more stress-timed as acquisition progresses, which is revealed by higher durational variability of syllables, vocalic sequences and consonantal clusters in speech delivered by older children compared to younger children. The development of speech rhythm was studied using the same speech material produced by children at various ages, while controlling for the differences in phonotactics and syllable structure. The tendency to deliver speech with higher durational variability at later stages of language acquisition emerges even when the phonological differences in speech material \u2014 inevitable in uncontrolled speech delivered by children of different ages \u2014 are controlled for. This finding indicates that the language-specific phonetic timing patterns are established as a function of age, probably as a result of the motor control development.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-628",
      "author_area_id": "1",
      "author_area_label": "Speech Perception, Production, and Acquisition"
    },
    "scharenborg15_interspeech": {
      "authors": [
        [
          "Odette",
          "Scharenborg"
        ]
      ],
      "title": "Durational information in word-initial lexical embeddings in spoken Dutch",
      "original": "i15_3125",
      "page_count": 5,
      "order": 678,
      "p1": "3125",
      "pn": "3129",
      "abstract": [
        "There is a growing body of research showing the importance of durational information for the disambiguation of temporarily ambiguous speech due to lexical embedding (e.g., rye in rises) in laboratory settings. The current research investigates whether durational differences are present in non-laboratory speech. We focus on two types of speech: read speech and speech taken from interviews. Durations of thousands of instances of monosyllabic words and the same phonemic string embedded as the first syllable of a polysyllabic word (so-called embedded words) were obtained from the Spoken Dutch Corpus. These durations were first adjusted to many known sources of durational differences. A subsequent statistical analysis on these adjusted durations showed a significant difference in durations between monosyllabic words and embedded words for both speaking styles, suggesting that the presence of durational differences between monosyllabic words and embedded words is a general characteristic of spoken Dutch. Although the differences are small, it is argued that these durational differences are perceptually relevant.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-629",
      "author_area_id": "1",
      "author_area_label": "Speech Perception, Production, and Acquisition"
    },
    "chen15m_interspeech": {
      "authors": [
        [
          "Fei",
          "Chen"
        ],
        [
          "Nan",
          "Yan"
        ],
        [
          "Lan",
          "Wang"
        ],
        [
          "Tao",
          "Yang"
        ],
        [
          "Jiantao",
          "Wu"
        ],
        [
          "Han",
          "Zhao"
        ],
        [
          "Gang",
          "Peng"
        ]
      ],
      "title": "The development of categorical perception of lexical tones in Mandarin-speaking preschoolers",
      "original": "i15_3130",
      "page_count": 5,
      "order": 679,
      "p1": "3130",
      "pn": "3134",
      "abstract": [
        "The present study investigates how categorical perception of Mandarin tones develops along age from four- to six-year-old preschoolers. The results showed that Mandarin tones could be perceived categorically by children as young as four years of age, who were able to process tones as linguistic categories. The positions of the identification boundaries did not differ significantly between children and adults, but the boundary widths did differ significantly, with much narrower boundary widths (i.e., sharper boundaries) for the six-year-old group and the adult group. Moreover, the overall discrimination accuracies for all the children groups were lower than that for the adult group. However, children as early as five years of age could discriminate between-category tone pairs almost equally well as the matching adults. These findings lead us to conclude that five to six years of age seems to be a critical period for the development and refinement of Mandarin tone perception.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-630",
      "author_area_id": "1",
      "author_area_label": "Speech Perception, Production, and Acquisition"
    },
    "ooigawa15_interspeech": {
      "authors": [
        [
          "Tomohiko",
          "Ooigawa"
        ]
      ],
      "title": "Perception of Italian liquids by Japanese listeners: comparisons to Spanish liquids",
      "original": "i15_3135",
      "page_count": 5,
      "order": 680,
      "p1": "3135",
      "pn": "3139",
      "abstract": [
        "The present research tests Japanese listeners' perception of Italian liquids under the framework of the Perceptual Assimilation Model (PAM), a model of non-native speech perception. The study examines the discrimination and the identification of Italian liquids (/l/, /r/, /ll/, /rr/ and /\u028e/), while comparing the results with those of replicated perception experiments of Spanish liquids (/l/, /\u027e/ and /r/). Japanese listeners showed poor discrimination performance on Italian /l/-/r/ and very good discrimination performance on the other Italian liquid contrasts (/l/-/ll/, /l/-/rr/, /ll/-/r/, /ll/-/rr/, /r/-/rr/, /l/-/\u028e/, /ll/-/\u028e/, /r/-/\u028e/ and /rr/-/\u028e/). The listeners perceptually assimilated Italian /l/ and /r/ to Japanese /r/, Italian /ll/ to Japanese /Qr/, Italian /rr/ to Japanese /rur/, and Italian /\u028e/ to Japanese /rj/. The findings indicate that PAM accounts for the perception of Italian liquids by Japanese listeners.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-631",
      "author_area_id": "2",
      "author_area_label": "Phonetics, Phonology, and Prosody"
    },
    "saon15_interspeech": {
      "authors": [
        [
          "George",
          "Saon"
        ],
        [
          "Hong-Kwang J.",
          "Kuo"
        ],
        [
          "Steven",
          "Rennie"
        ],
        [
          "Michael",
          "Picheny"
        ]
      ],
      "title": "The IBM 2015 English conversational telephone speech recognition system",
      "original": "i15_3140",
      "page_count": 5,
      "order": 681,
      "p1": "3140",
      "pn": "3144",
      "abstract": [
        "We describe the latest improvements to the IBM English conversational telephone speech recognition system. Some of the techniques that were found beneficial are: maxout networks with annealed dropout rates; networks with a very large number of outputs trained on 2000 hours of data; joint modeling of partially unfolded recurrent neural networks and convolutional nets by combining the bottleneck and output layers and retraining the resulting model; and lastly, sophisticated language model rescoring with exponential and neural network LMs. These techniques result in an 8.0% word error rate on the Switchboard part of the Hub5-2000 evaluation test set which is 23% relative better than our previous best published result.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-632",
      "author_area_id": "10",
      "author_area_label": "Speech Recognition \u2013 Technologies and Systems for New Applications"
    },
    "liu15i_interspeech": {
      "authors": [
        [
          "Xunying",
          "Liu"
        ],
        [
          "Federico",
          "Flego"
        ],
        [
          "Linlin",
          "Wang"
        ],
        [
          "C.",
          "Zhang"
        ],
        [
          "Mark J. F.",
          "Gales"
        ],
        [
          "Philip C.",
          "Woodland"
        ]
      ],
      "title": "The cambridge university 2014 BOLT conversational telephone Mandarin Chinese LVCSR system for speech translation",
      "original": "i15_3145",
      "page_count": 5,
      "order": 682,
      "p1": "3145",
      "pn": "3149",
      "abstract": [
        "This paper presents the development of the 2014 Cambridge University conversational telephone Mandarin Chinese LVCSR system for the DARPA BOLT speech translation evaluation. A range of advanced modelling techniques were employed to both improve the recognition performance and provide a suitable integration with the translation system. These include an improved system combination technique using frame level acoustic model combination via joint decoding. Sequence trained deep neural network (DNN) based hybrid and tandem systems were combined on-the-fly to produce a consistent decoding output during search. A multi-level paraphrastic recurrent neural network LM (RNNLM) modelling both alternative paraphrase expressions and character sequences while preserving a consistent character to word segmentation was also used. This system gave an overall character error rate (CER) of 29.1% on the BOLT dev14 development set.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-633",
      "author_area_id": "10",
      "author_area_label": "Speech Recognition \u2013 Technologies and Systems for New Applications"
    },
    "thomas15_interspeech": {
      "authors": [
        [
          "Samuel",
          "Thomas"
        ],
        [
          "George",
          "Saon"
        ],
        [
          "Hong-Kwang J.",
          "Kuo"
        ],
        [
          "Lidia",
          "Mangu"
        ]
      ],
      "title": "The IBM BOLT speech transcription system",
      "original": "i15_3150",
      "page_count": 4,
      "order": 683,
      "p1": "3150",
      "pn": "3153",
      "abstract": [
        "We describe the IBM automatic speech recognition (ASR) system for the DARPA Broad Operational Language Translation (BOLT) program. The system is used to transcribe conversational telephone speech (CTS) prior to machine translation for Phase 3 of the program's Activity A. The ASR system is a combination of novel sequence trained ensemble deep neural network acoustic models on speaker adapted features and convolutional neural network models on two kinds of spectro-temporal representations of speech, in conjunction with a variety of class, neural network and n-gram based language models. Acoustic and language models for the recognition system are built on transcribed audio released under the program and further optimized for the final machine translation task as well. The evaluation system has a word error rate of 32.7% on a 2 hour Egyptian Arabic development set for this task.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-634",
      "author_area_id": "10",
      "author_area_label": "Speech Recognition \u2013 Technologies and Systems for New Applications"
    },
    "shaik15_interspeech": {
      "authors": [
        [
          "M. Ali Basha",
          "Shaik"
        ],
        [
          "Zolt\u00e1n",
          "T\u00fcske"
        ],
        [
          "M. Ali",
          "Tahir"
        ],
        [
          "Markus",
          "Nu\u00dfbaum-Thom"
        ],
        [
          "Ralf",
          "Schl\u00fcter"
        ],
        [
          "Hermann",
          "Ney"
        ]
      ],
      "title": "Improvements in RWTH LVCSR evaluation systems for Polish, Portuguese, English, urdu, and Arabic",
      "original": "i15_3154",
      "page_count": 5,
      "order": 684,
      "p1": "3154",
      "pn": "3158",
      "abstract": [
        "In this work, Portuguese, Polish, English, Urdu, and Arabic automatic speech recognition evaluation systems developed by the RWTH Aachen University are presented. Our LVCSR systems focus on various domains like broadcast news, spontaneous speech, and podcasts. All these systems but Urdu are used for Euronews and Skynews evaluations as part of the EU-Bridge project. Our previously developed LVCSR systems were improved using different techniques for the aforementioned languages. Significant improvements are obtained using multilingual tandem and hybrid approaches, minimum phone error training, lexical adaptation, open vocabulary long short term memory language models, maximum entropy language models and confusion-network based system combination.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-635",
      "author_area_id": "10",
      "author_area_label": "Speech Recognition \u2013 Technologies and Systems for New Applications"
    },
    "fragasilva15_interspeech": {
      "authors": [
        [
          "Thiago",
          "Fraga-Silva"
        ],
        [
          "Jean-Luc",
          "Gauvain"
        ],
        [
          "Lori",
          "Lamel"
        ],
        [
          "Antoine",
          "Laurent"
        ],
        [
          "Viet-Bac",
          "Le"
        ],
        [
          "Abdel",
          "Messaoudi"
        ]
      ],
      "title": "Active learning based data selection for limited resource STT and KWS",
      "original": "i15_3159",
      "page_count": 5,
      "order": 685,
      "p1": "3159",
      "pn": "3163",
      "abstract": [
        "This paper presents first results in using active learning (AL) for training data selection in the context of the IARPA-Babel program. Given an initial training data set, we aim to automatically select additional data (from an untranscribed pool data set) for manual transcription. Initial and selected data are then used to build acoustic and language models for speech recognition. The goal of the AL task is to outperform a baseline system built using a pre-defined data selection with the same amount of data, the Very Limited Language Pack (VLLP) condition. AL methods based on different selection criteria have been explored. Compared to the VLLP baseline, improvements are obtained in terms of Word Error Rate and Actual Term Weighted Values for the Lithuanian language. A description of methods and an analysis of the results are given. The AL selection also outperforms the VLLP baseline for other IARPA-Babel languages, and will be further tested in the upcoming NIST OpenKWS 2015 evaluation.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-636",
      "author_area_id": "10",
      "author_area_label": "Speech Recognition \u2013 Technologies and Systems for New Applications"
    },
    "jyothi15b_interspeech": {
      "authors": [
        [
          "Preethi",
          "Jyothi"
        ],
        [
          "Mark",
          "Hasegawa-Johnson"
        ]
      ],
      "title": "Improved hindi broadcast ASR by adapting the language model and pronunciation model using a priori syntactic and morphophonemic knowledge",
      "original": "i15_3164",
      "page_count": 5,
      "order": 686,
      "p1": "3164",
      "pn": "3168",
      "abstract": [
        "In this work, we present a new large-vocabulary, broadcast news ASR system for Hindi. Since Hindi has a largely phonemic orthography, the pronunciation model was automatically generated from text. We experiment with several variants of this model and study the effect of incorporating word boundary information with these models. We also experiment with knowledge-based adaptations to the language model in Hindi, derived in an unsupervised manner, that lead to small improvements in word error rate (WER). Our experiments were conducted on a new corpus assembled from publicly-available Hindi news broadcasts. We evaluate our techniques on an open-vocabulary task and obtain competitive WERs on an unseen test set.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-637",
      "author_area_id": "10",
      "author_area_label": "Speech Recognition \u2013 Technologies and Systems for New Applications"
    },
    "versteegh15_interspeech": {
      "authors": [
        [
          "Maarten",
          "Versteegh"
        ],
        [
          "Roland",
          "Thiolli\u00e8re"
        ],
        [
          "Thomas",
          "Schatz"
        ],
        [
          "Xuan Nga",
          "Cao"
        ],
        [
          "Xavier",
          "Anguera"
        ],
        [
          "Aren",
          "Jansen"
        ],
        [
          "Emmanuel",
          "Dupoux"
        ]
      ],
      "title": "The zero resource speech challenge 2015",
      "original": "i15_3169",
      "page_count": 5,
      "order": 687,
      "p1": "3169",
      "pn": "3173",
      "abstract": [
        "The Interspeech 2015 Zero Resource Speech Challenge aims at discovering subword and word units from raw speech. The challenge provides the first unified and open source suite of evaluation metrics and data sets to compare and analyse the results of unsupervised linguistic unit discovery algorithms. It consists of two tracks. In the first, a psychophysically inspired evaluation task (minimal pair ABX discrimination) is used to assess how well speech feature representations discriminate between contrastive subword units. In the second, several metrics gauge the quality of discovered word-like patterns. Two data sets are provided, one for English, one for Xitsonga. Both data sets are provided without any annotation except for voice activity and talker identity. This paper introduces the evaluation metrics, presents the results of baseline systems and discusses some of the key issues in unsupervised unit discovery.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-638",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "badino15_interspeech": {
      "authors": [
        [
          "Leonardo",
          "Badino"
        ],
        [
          "Alessio",
          "Mereta"
        ],
        [
          "Lorenzo",
          "Rosasco"
        ]
      ],
      "title": "Discovering discrete subword units with binarized autoencoders and hidden-Markov-model encoders",
      "original": "i15_3174",
      "page_count": 5,
      "order": 688,
      "p1": "3174",
      "pn": "3178",
      "abstract": [
        "In this paper we address the problem of unsupervised learning of discrete subword units. Our approach is based on Deep Autoencoders (AEs), whose encoding node values are thresholded to subsequently generate a symbolic, i.e., 1-of-K (with K = No. of subwords), representation of each speech frame. We experiment with two variants of the standard AE which we have named Binarized Autoencoder and Hidden-Markov-Model Encoder. The first forces the binary encoding nodes to have a U-shaped distribution (with peaks at 0 and 1) while minimizing the reconstruction error. The latter jointly learns the symbolic encoding representation (i.e., subwords) and the prior and transition distribution probabilities of the learned subwords.   The ABX evaluation of the Zero Resource Challenge - Track 1 shows that a deep AE with only 6 encoding nodes, which assigns to each frame a 1-of-K binary vector with K = 2^6, can outperform real-valued MFCC representations in the across-speaker setting. Binarized AEs can outperform standard AEs when using a larger number of encoding nodes, while HMM Encoders may allow more compact subword transcriptions without worsening the ABX performance.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-639",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "thiolliere15_interspeech": {
      "authors": [
        [
          "Roland",
          "Thiolli\u00e8re"
        ],
        [
          "Ewan",
          "Dunbar"
        ],
        [
          "Gabriel",
          "Synnaeve"
        ],
        [
          "Maarten",
          "Versteegh"
        ],
        [
          "Emmanuel",
          "Dupoux"
        ]
      ],
      "title": "A hybrid dynamic time warping-deep neural network architecture for unsupervised acoustic modeling",
      "original": "i15_3179",
      "page_count": 5,
      "order": 689,
      "p1": "3179",
      "pn": "3183",
      "abstract": [
        "We report on an architecture for the unsupervised discovery of talker-invariant subword embeddings. It is made out of two components: a dynamic-time warping based spoken term discovery (STD) system and a Siamese deep neural network (DNN). The STD system clusters word-sized repeated fragments in the acoustic streams while the DNN is trained to minimize the distance between time aligned frames of tokens of the same cluster, and maximize the distance between tokens of different clusters. We use additional side information regarding the average duration of phonemic units, as well as talker identity tags. For evaluation we use the datasets and metrics of the Zero Resource Speech Challenge. The model shows improvement over the baseline in subword unit modeling.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-640",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "agenbag15_interspeech": {
      "authors": [
        [
          "Wiehan",
          "Agenbag"
        ],
        [
          "Thomas",
          "Niesler"
        ]
      ],
      "title": "Automatic segmentation and clustering of speech using sparse coding and metaheuristic search",
      "original": "i15_3184",
      "page_count": 5,
      "order": 690,
      "p1": "3184",
      "pn": "3188",
      "abstract": [
        "We propose a constrained shift and scale invariant sparse coding model for the purpose of unsupervised segmentation and clustering of speech into acoustically relevant sub-word units for automatic speech recognition. We introduce a novel local search algorithm that iteratively improves the acoustic relevance of the automatically-determined sub-word units from a random initialization by repeated alignment and subsequent re-estimation with the training material. We also contribute an associated population-based metaheuristic optimisation procedure related to genetic approaches to achieve a global search for the most acoustically relevant set of sub-word units. A first application of this metaheuristic search indicates that it yields an improvement over a corresponding local search. Using a subset of TIMIT for training, we also find that some of the automatically-determined sub-word units in our final dictionaries exhibit a strong correlation with the reference phonetic transcriptions. Furthermore, in some cases our sub-word transcriptions yield a compact set of often-used pronunciations. Informal listening tests indicate that the clustering is robust, and provides optimism that our approach will be suited to the task of generating pronunciation dictionaries that can be used for ASR.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-641",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "chen15n_interspeech": {
      "authors": [
        [
          "Hongjie",
          "Chen"
        ],
        [
          "Cheung-Chi",
          "Leung"
        ],
        [
          "Lei",
          "Xie"
        ],
        [
          "Bin",
          "Ma"
        ],
        [
          "Haizhou",
          "Li"
        ]
      ],
      "title": "Parallel inference of dirichlet process Gaussian mixture models for unsupervised acoustic modeling: a feasibility study",
      "original": "i15_3189",
      "page_count": 5,
      "order": 691,
      "p1": "3189",
      "pn": "3193",
      "abstract": [
        "We adopt a Dirichlet process Gaussian mixture model (DPGMM) for unsupervised acoustic modeling and represent speech frames with Gaussian posteriorgrams. The model performs unsupervised clustering on untranscribed data, and each Gaussian component can be considered as a cluster of sounds from various speakers. The model infers its model complexity (i.e. the number of Gaussian components) from the data. For computation efficiency, we use a parallel sampler for the model inference. Our experiments are conducted on the corpus provided by the zero resource speech challenge. Experimental results show that the unsupervised DPGMM posteriorgrams obviously outperform MFCC, and perform comparably to the posteriorgrams derived from language-mismatched phoneme recognizers in terms of the error rate of ABX discrimination test. The error rates can be further reduced by the fusion of these two kinds of posteriorgrams.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-642",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "baljekar15_interspeech": {
      "authors": [
        [
          "Pallavi",
          "Baljekar"
        ],
        [
          "Sunayana",
          "Sitaram"
        ],
        [
          "Prasanna Kumar",
          "Muthukumar"
        ],
        [
          "Alan W.",
          "Black"
        ]
      ],
      "title": "Using articulatory features and inferred phonological segments in zero resource speech processing",
      "original": "i15_3194",
      "page_count": 5,
      "order": 692,
      "p1": "3194",
      "pn": "3198",
      "abstract": [
        "Unsupervised discovery of subword units is an important problem in recognition and synthesis of zero-resource languages, in which phonesets may not be known and the only resource that may be available is speech. We use techniques that we have recently developed for building synthetic voices for very low resource languages without a written form to discover such units. We use Articulatory Features trained on labeled speech in a higher resource language to infer phonological segments of varying granularity. We use both the raw Articulatory Features and the Articulatory Features of the inferred units as frame-based representations of speech. We evaluate our techniques on minimal pair ABX discrimination within and across speakers. In addition, to exploit the duration information we get from the inferred phonological units, we also present evaluation results on Mel Cepstral Distortion, an objective metric of speech synthesis quality. We evaluate our techniques on multiple databases of English, and also on Tsonga and Indic languages, in which we apply the above methods cross-lingually.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-643",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "renshaw15_interspeech": {
      "authors": [
        [
          "Daniel",
          "Renshaw"
        ],
        [
          "Herman",
          "Kamper"
        ],
        [
          "Aren",
          "Jansen"
        ],
        [
          "Sharon",
          "Goldwater"
        ]
      ],
      "title": "A comparison of neural network methods for unsupervised representation learning on the zero resource speech challenge",
      "original": "i15_3199",
      "page_count": 5,
      "order": 693,
      "p1": "3199",
      "pn": "3203",
      "abstract": [
        "The success of supervised deep neural networks (DNNs) in speech recognition cannot be transferred to zero-resource languages where the requisite transcriptions are unavailable. We investigate unsupervised neural network based methods for learning frame-level representations. Good frame representations eliminate differences in accent, gender, channel characteristics, and other factors to model subword units for within- and across-speaker phonetic discrimination. We enhance the correspondence autoencoder (cAE) and show that it can transform Mel Frequency Cepstral Coefficients (MFCCs) into more effective frame representations given a set of matched word pairs from an unsupervised term discovery (UTD) system. The cAE combines the feature extraction power of autoencoders with the weak supervision signal from UTD pairs to better approximate the extrinsic task's objective during training. We use the Zero Resource Speech Challenge's minimal triphone pair ABX discrimination task to evaluate our methods. Optimizing a cAE architecture on English and applying it to a zero-resource language, Xitsonga, we obtain a relative error rate reduction of 35% compared to the original MFCCs. We also show that Xitsonga frame representations extracted from the bottleneck layer of a supervised DNN trained on English can be further enhanced by the cAE, yielding a relative error rate reduction of 39%.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-644",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "rasanen15_interspeech": {
      "authors": [
        [
          "Okko",
          "R\u00e4s\u00e4nen"
        ],
        [
          "Gabriel",
          "Doyle"
        ],
        [
          "Michael C.",
          "Frank"
        ]
      ],
      "title": "Unsupervised word discovery from speech using automatic segmentation into syllable-like units",
      "original": "i15_3204",
      "page_count": 5,
      "order": 694,
      "p1": "3204",
      "pn": "3208",
      "abstract": [
        "This paper presents a syllable-based approach to unsupervised pattern discovery from speech. By first segmenting speech into syllable-like units, the system is able to limit potential word onsets and offsets to a finite number of candidate locations. These syllable tokens are then described using a set of features and clustered into a finite number of syllable classes. Finally, recurring syllable sequences or individual classes are treated as word candidates. Feasibility of the approach is investigated on spontaneous American English and Tsonga language samples with promising results. We also present a new and simple, oscillator-based algorithm for efficient unsupervised syllabic segmentation.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-645",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "lyzinski15_interspeech": {
      "authors": [
        [
          "Vince",
          "Lyzinski"
        ],
        [
          "Gregory",
          "Sell"
        ],
        [
          "Aren",
          "Jansen"
        ]
      ],
      "title": "An evaluation of graph clustering methods for unsupervised term discovery",
      "original": "i15_3209",
      "page_count": 5,
      "order": 695,
      "p1": "3209",
      "pn": "3213",
      "abstract": [
        "Unsupervised term discovery (UTD) is the task of automatically identifying the repeated words and phrases in a collection of speech audio without relying on any language-specific resources. While the solution space for the task is far from fully explored, the dominant approach to date decomposes the discovery problem into two steps, where (i) segmental dynamic time warping is used to search the speech audio for repeated acoustic patterns, and (ii) these individual repetitions are partitioned into word/phrase categories using graph clustering. In this paper, we perform an unprecedented evaluation of a wide range of advanced graph clustering methods for the UTD task. We conduct our study in the evaluation framework of the Zero Resource Speech Challenge. We find that, for a range of features and languages, modularity-based clustering improves UTD performance most consistently, often by a wide margin. When paired with out-of-language deep neural net bottleneck features, we find performance near that of a high-resource UTD system.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-646",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "peddinti15b_interspeech": {
      "authors": [
        [
          "Vijayaditya",
          "Peddinti"
        ],
        [
          "Daniel",
          "Povey"
        ],
        [
          "Sanjeev",
          "Khudanpur"
        ]
      ],
      "title": "A time delay neural network architecture for efficient modeling of long temporal contexts",
      "original": "i15_3214",
      "page_count": 5,
      "order": 696,
      "p1": "3214",
      "pn": "3218",
      "abstract": [
        "Recurrent neural network architectures have been shown to efficiently model long term temporal dependencies between acoustic events. However the training time of recurrent networks is higher than feedforward networks due to the sequential nature of the learning algorithm. In this paper we propose a time delay neural network architecture which models long term temporal dependencies with training times comparable to standard feed-forward DNNs. The network uses sub-sampling to reduce computation during training. On the Switchboard task we show a relative improvement of 7.3% over the baseline DNN model. We present results on several LVCSR tasks with training data ranging from 3 to 1800 hours to show the effectiveness of the TDNN architecture in learning wider temporal dependencies in both small and large data scenarios, with an average relative improvement of 5.5%.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-647",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition \u2013 Signal Processing, Acoustic Modeling, Robustness, and Adaptation"
    },
    "li15f_interspeech": {
      "authors": [
        [
          "Xiangang",
          "Li"
        ],
        [
          "Xihong",
          "Wu"
        ]
      ],
      "title": "Long short-term memory based convolutional recurrent neural networks for large vocabulary speech recognition",
      "original": "i15_3219",
      "page_count": 5,
      "order": 697,
      "p1": "3219",
      "pn": "3223",
      "abstract": [
        "Long short-term memory (LSTM) recurrent neural networks (RNNs) have been shown to give state-of-the-art performance on many speech recognition tasks, as they are able to provide the learned dynamically changing contextual window of all sequence history. On the other hand, the convolutional neural networks (CNNs) have brought significant improvements to deep feed-forward neural networks (FFNNs), as they are able to better reduce spectral variation in the input signal. In this paper, a network architecture called as convolutional recurrent neural network (CRNN) is proposed by combining the CNN and LSTM RNN. In the proposed CRNNs, each speech frame, without adjacent context frames, is organized as a number of local feature patches along the frequency axis, and then a LSTM network is performed on each feature patch along the time axis. We train and compare FFNNs, LSTM RNNs and the proposed LSTM CRNNs at various number of configurations. Experimental results show that the LSTM CRNNs can exceed state-of-the-art speech recognition performance.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-648",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition \u2013 Signal Processing, Acoustic Modeling, Robustness, and Adaptation"
    },
    "zhang15f_interspeech": {
      "authors": [
        [
          "C.",
          "Zhang"
        ],
        [
          "Philip C.",
          "Woodland"
        ]
      ],
      "title": "Parameterised sigmoid and reLU hidden activation functions for DNN acoustic modelling",
      "original": "i15_3224",
      "page_count": 5,
      "order": 698,
      "p1": "3224",
      "pn": "3228",
      "abstract": [
        "The form of hidden activation functions has been always an important issue in deep neural network (DNN) design. The most common choices for acoustic modelling are the standard Sigmoid and rectified linear unit (ReLU), which are normally used with fixed function shapes and no adaptive parameters. Recently, there have been several papers that have studied the use of parameterised activation functions for both computer vision and speaker adaptation tasks. In this paper, we investigate generalised forms of both Sigmoid and ReLU with learnable parameters, as well as their integration with the standard DNN acoustic model training process. Experiments using conversational telephone speech (CTS) Mandarin data, result in an average of 3.4% and 2.0% relative word error rate (WER) reduction with Sigmoid and ReLU parameterisations.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-649",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition \u2013 Signal Processing, Acoustic Modeling, Robustness, and Adaptation"
    },
    "zhang15g_interspeech": {
      "authors": [
        [
          "Chiyuan",
          "Zhang"
        ],
        [
          "Stephen",
          "Voinea"
        ],
        [
          "Georgios",
          "Evangelopoulos"
        ],
        [
          "Lorenzo",
          "Rosasco"
        ],
        [
          "Tomaso",
          "Poggio"
        ]
      ],
      "title": "Discriminative template learning in group-convolutional networks for invariant speech representations",
      "original": "i15_3229",
      "page_count": 5,
      "order": 699,
      "p1": "3229",
      "pn": "3233",
      "abstract": [
        "In the framework of a theory for invariant sensory signal representations, a signature which is invariant and selective for speech sounds can be obtained through projections in template signals and pooling over their transformations under a group. For locally compact groups, e.g., translations, the theory explains the resilience of convolutional neural networks with filter weight sharing and max pooling across their local translations in frequency or time. In this paper we propose a discriminative approach for learning an optimum set of templates, under a family of transformations, namely frequency transpositions and perturbations of the vocal tract length, which are among the primary sources of speech variability. Implicitly, we generalize convolutional networks to transformations other than translations, and derive data-specific templates by training a deep network with convolution-pooling layers and densely connected layers. We demonstrate that such a representation, combining group-generalized convolutions, theoretical invariance guarantees and discriminative template selection, improves frame classification performance over standard translation-CNNs and DNNs on TIMIT and Wall Street Journal datasets.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-650",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition \u2013 Signal Processing, Acoustic Modeling, Robustness, and Adaptation"
    },
    "sivadas15_interspeech": {
      "authors": [
        [
          "Sunil",
          "Sivadas"
        ],
        [
          "Zhenzhou",
          "Wu"
        ],
        [
          "Ma",
          "Bin"
        ]
      ],
      "title": "Investigation of parametric rectified linear units for noise robust speech recognition",
      "original": "i15_3234",
      "page_count": 5,
      "order": 700,
      "p1": "3234",
      "pn": "3238",
      "abstract": [
        "Convolutional neural networks with rectified linear unit (ReLU) have been successful in speech recognition and computer vision tasks. ReLU was proposed as a better match to biological neural activation functions compared to sigmoidal non-linearity function. However, ReLU has a disadvantage that the gradient is zero whenever the unit is not active or saturated. To alleviate the potential problems due the zero gradient, Leaky ReLU (LReLU) was proposed. Recently, a parametrized form of ReLU (PReLU) was shown to give superior performance compared to ReLU on large scale computer vision tasks. PReLU is a generalized version of LReLU where the gradient is learned adaptively from the training data. In this paper we investigate PReLU based deep convolutional neural networks for noise robust speech recognition. We report experimental results on Aurora-4 multi-condition training task. We show that PReLU gives slightly better Word Error Rates (WERs) on noisy test sets compared to ReLU. In combination with dropout generalization method we report one of the best WERs in the literature for this noisy speech recognition task.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-651",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition \u2013 Signal Processing, Acoustic Modeling, Robustness, and Adaptation"
    },
    "su15c_interspeech": {
      "authors": [
        [
          "Hang",
          "Su"
        ],
        [
          "Haihua",
          "Xu"
        ]
      ],
      "title": "Multi-softmax deep neural network for semi-supervised training",
      "original": "i15_3239",
      "page_count": 5,
      "order": 701,
      "p1": "3239",
      "pn": "3243",
      "abstract": [
        "In this paper we propose a Shared Hidden Layer Multi-softmax Deep Neural Network (SHL-MDNN) approach for semi-supervised training (SST). This approach aims to boost low-resource speech recognition where limited training data is available. Supervised data and unsupervised data share the same hidden layers but are fed into different softmax layers so that erroneous automatic speech recognition (ASR) transcriptions of the unsupervised data have less effect on shared hidden layers. Experimental results on Babel data indicate that this approach always outperform naive SST on DNN, and it can yield 1.3% word error rate (WER) reduction compared with supervised DNN hybrid system. In addition, if softmax layer is retrained with supervised data, it can lead up to another 0.8% WER reduction. Confidence based data selection is also studied in this setup. Experiments show that this method is not sensitive to ASR transcription errors.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-652",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition \u2013 Signal Processing, Acoustic Modeling, Robustness, and Adaptation"
    },
    "cui15_interspeech": {
      "authors": [
        [
          "Jia",
          "Cui"
        ],
        [
          "George",
          "Saon"
        ],
        [
          "Bhuvana",
          "Ramabhadran"
        ],
        [
          "Brian",
          "Kingsbury"
        ]
      ],
      "title": "A multi-region deep neural network model in speech recognition",
      "original": "i15_3244",
      "page_count": 5,
      "order": 702,
      "p1": "3244",
      "pn": "3248",
      "abstract": [
        "This work proposes a new architecture for deep neural network training. Instead of having one cascade of fully connected hidden layers between the input features and the target output, the new architecture organizes hidden layers into several regions with each region having its own target. Regions communicate with each other during the training process by connections among intermediate hidden layers to share learned internal representations from their respective targets. They do not have to share the same input features. This paper presents the performance of acoustic models built using this architecture with speaker independent and dependent features. Experimental results are compared with not only the baseline DNN model, but also the ensemble DNN, unfolded RNN and stacked DNN. Experiments on the IARPA sponsored Babel tasks demonstrate improvements ranging from 0.8% to 2.7% absolute reduction in WER.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-653",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition \u2013 Signal Processing, Acoustic Modeling, Robustness, and Adaptation"
    },
    "lu15e_interspeech": {
      "authors": [
        [
          "Liang",
          "Lu"
        ],
        [
          "Xingxing",
          "Zhang"
        ],
        [
          "Kyunghyun",
          "Cho"
        ],
        [
          "Steve",
          "Renals"
        ]
      ],
      "title": "A study of the recurrent neural network encoder-decoder for large vocabulary speech recognition",
      "original": "i15_3249",
      "page_count": 5,
      "order": 703,
      "p1": "3249",
      "pn": "3253",
      "abstract": [
        "Deep neural networks have advanced the state-of-the-art in automatic speech recognition, when combined with hidden Markov models (HMMs). Recently there has been interest in using systems based on recurrent neural networks (RNNs) to perform sequence modelling directly, without the requirement of an HMM superstructure. In this paper, we study the RNN encoder-decoder approach for large vocabulary end-to-end speech recognition, whereby an encoder transforms a sequence of acoustic vectors into a sequence of feature representations, from which a decoder recovers a sequence of words. We investigated this approach on the Switchboard corpus using a training set of around 300 hours of transcribed audio data. Without the use of an explicit language model or pronunciation lexicon, we achieved promising recognition accuracy, demonstrating that this approach warrants further investigation.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-654",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition \u2013 Signal Processing, Acoustic Modeling, Robustness, and Adaptation"
    },
    "zhu15b_interspeech": {
      "authors": [
        [
          "Linchen",
          "Zhu"
        ],
        [
          "Kevin",
          "Kilgour"
        ],
        [
          "Sebastian",
          "St\u00fcker"
        ],
        [
          "Alex",
          "Waibel"
        ]
      ],
      "title": "Gaussian free cluster tree construction using deep neural network",
      "original": "i15_3254",
      "page_count": 5,
      "order": 704,
      "p1": "3254",
      "pn": "3258",
      "abstract": [
        "This paper presents a Gaussian free approach to constructing the cluster tree (CT) that context dependent acoustic models (CD-AM) depend on. Over the last few years deep neural networks (DNN) have supplanted Gaussian mixture models (GMM) as the default method for acoustic modeling (AM). DNN AMs have also been successfully used to flat start context independent (CI) AMs and generate alignments on which CTs can be trained. Those approaches however still required Gaussians to build their CTs. Our proposed Gaussian free CT algorithm eliminates this requirements and allows, for the first time, the flat start training of state of the art DNN AMs without the use of Gaussian. An evaluation on the IWSLT transcription task demonstrates the effectiveness of this approach.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-655",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition \u2013 Signal Processing, Acoustic Modeling, Robustness, and Adaptation"
    },
    "bi15_interspeech": {
      "authors": [
        [
          "Mengxiao",
          "Bi"
        ],
        [
          "Yanmin",
          "Qian"
        ],
        [
          "Kai",
          "Yu"
        ]
      ],
      "title": "Very deep convolutional neural networks for LVCSR",
      "original": "i15_3259",
      "page_count": 5,
      "order": 705,
      "p1": "3259",
      "pn": "3263",
      "abstract": [
        "Convolutional Neural Networks (CNNs) have achieved state-of-the-art performance when embedded in large vocabulary continuous speech recognition (LVCSR) systems due to its capability of modeling local correlations and reducing translational variations. In all previous related works for ASR, only up to two convolutional layers are employed. In light of the recent success of very deep CNNs in image classification, it is of interest to investigate the deep structure of CNNs for speech recognition in detail. In contrast to image classification, the dimensionality of the speech feature, the span size of input feature and the relationship between temporal and spectral domain are new factors to consider while designing very deep CNNs. In this work, very deep CNNs are introduced for LVCSR task, by extending depth of convolutional layers up to ten. The contribution of this work is two-fold: performance improvement of very deep CNNs is investigated under different configurations; further, a better way to perform convolution operations on temporal dimension is proposed. Experiments showed that very deep CNNs offer a 8-12% relative improvement over baseline DNN system, and a 4-7% relative improvement over baseline CNN system, evaluated on both a 15-hr Callhome and a 51-hr Switchboard LVCSR tasks.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-656",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition \u2013 Signal Processing, Acoustic Modeling, Robustness, and Adaptation"
    },
    "chan15_interspeech": {
      "authors": [
        [
          "William",
          "Chan"
        ],
        [
          "Nan Rosemary",
          "Ke"
        ],
        [
          "Ian",
          "Lane"
        ]
      ],
      "title": "Transferring knowledge from a RNN to a DNN",
      "original": "i15_3264",
      "page_count": 5,
      "order": 706,
      "p1": "3264",
      "pn": "3268",
      "abstract": [
        "Deep Neural Network (DNN) acoustic models have yielded many state-of-the-art results in Automatic Speech Recognition (ASR) tasks. More recently, Recurrent Neural Network (RNN) models have been shown to outperform DNNs counterparts. However, state-of-the-art DNN and RNN models tend to be impractical to deploy on embedded systems with limited computational capacity. Traditionally, the approach for embedded platforms is to either train a small DNN directly, or to train a small DNN that learns the output distribution of a large DNN. In this paper, we utilize a state-of-the-art RNN to transfer knowledge to small DNN. We use the RNN model to generate soft alignments and minimize the Kullback-Leibler divergence against the small DNN. The small DNN trained on the soft RNN alignments achieved a 3.9 WER on the Wall Street Journal (WSJ) eval92 task compared to a baseline 4.6 WER or more than 13% relative improvement.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-657",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition \u2013 Signal Processing, Acoustic Modeling, Robustness, and Adaptation"
    },
    "liu15j_interspeech": {
      "authors": [
        [
          "Changliang",
          "Liu"
        ],
        [
          "Jinyu",
          "Li"
        ],
        [
          "Yifan",
          "Gong"
        ]
      ],
      "title": "SVD-based universal DNN modeling for multiple scenarios",
      "original": "i15_3269",
      "page_count": 5,
      "order": 707,
      "p1": "3269",
      "pn": "3273",
      "abstract": [
        "Speech recognition scenarios (aka tasks) differ from each other in acoustic transducers, acoustic environments, and speaking style etc. Building one acoustic model per task is one common practice in industry. However, this limits training data sharing across scenarios thus may not give highest possible accuracy. Based on the deep neural network (DNN) technique, we propose to build a universal acoustic model for all scenarios by utilizing all the data together. Two advantages are obtained: 1) leveraging more data sources to improve the recognition accuracy, 2) reducing substantially service deployment and maintenance costs. We achieve this by extending the singular value decomposition (SVD) structure of DNNs. The data from all scenarios are used to first train a single SVD-DNN model. Then a series of scenario-dependent linear square matrices are added on top of each SVD layer and updated with only scenario-related data. At the recognition time, a flag indicates different scenarios and guides the recognizer to use the scenario-dependent matrices together with the scenario-independent matrices in the universal DNN for acoustic score evaluation. In our experiments on Microsoft Winphone/Skype/Xbox data sets, the universal DNN model is better than traditional trained isolated models, with up to 15.5% relative word error rate reduction.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-658",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition \u2013 Signal Processing, Acoustic Modeling, Robustness, and Adaptation"
    },
    "chen15o_interspeech": {
      "authors": [
        [
          "Zhuo",
          "Chen"
        ],
        [
          "Shinji",
          "Watanabe"
        ],
        [
          "Hakan",
          "Erdogan"
        ],
        [
          "John R.",
          "Hershey"
        ]
      ],
      "title": "Speech enhancement and recognition using multi-task learning of long short-term memory recurrent neural networks",
      "original": "i15_3274",
      "page_count": 5,
      "order": 708,
      "p1": "3274",
      "pn": "3278",
      "abstract": [
        "Long Short-Term Memory (LSTM) recurrent neural network has proven effective in modeling speech and has achieved outstanding performance in both speech enhancement (SE) and automatic speech recognition (ASR). To further improve the performance of noise-robust speech recognition, a combination of speech enhancement and recognition was shown to be promising in earlier work. This paper aims to explore options for consistent integration of SE and ASR using LSTM networks. Since SE and ASR have different objective criteria, it is not clear what kind of integration would finally lead to the best word error rate for noise-robust ASR tasks. In this work, several integration architectures are proposed and tested, including: (1) a pipeline architecture of LSTM-based SE and ASR with sequence training, (2) an alternating estimation architecture, and (3) a multi-task hybrid LSTM network architecture. The proposed models were evaluated on the 2nd CHiME speech separation and recognition challenge task, and show significant improvements relative to prior results.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-659",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition \u2013 Signal Processing, Acoustic Modeling, Robustness, and Adaptation"
    },
    "liu15k_interspeech": {
      "authors": [
        [
          "Yuzhou",
          "Liu"
        ],
        [
          "DeLiang",
          "Wang"
        ]
      ],
      "title": "Speaker-dependent multipitch tracking using deep neural networks",
      "original": "i15_3279",
      "page_count": 5,
      "order": 709,
      "p1": "3279",
      "pn": "3283",
      "abstract": [
        "Multipitch tracking is a challenging problem for speech and signal processing. In this paper, we use deep neural networks (DNNs) to model the probabilistic pitch states of two simultaneous speakers. To closely capture speaker-dependent information and improve the accuracy of speaker assignment, we train a DNN for each enrolled speaker (speaker-dependent DNN). We also explore the feasibility of training a DNN for each speaker pair in the system (speaker-pair-dependent DNN). A factorial hidden Markov model (FHMM) then integrates the pitch probabilities and generates most likely pitch contours with a junction tree algorithm. We evaluate our system on the GRID corpus. Experiments show that our approach substantially outperforms state-of-the-art multipitch trackers on both same-gender and different-gender two-talker mixtures.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-660",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "p15_interspeech": {
      "authors": [
        [
          "Sujith",
          "P."
        ],
        [
          "A. P.",
          "Prathosh"
        ],
        [
          "A. G.",
          "Ramakrishnan"
        ],
        [
          "Prasanta Kumar",
          "Ghosh"
        ]
      ],
      "title": "An error correction scheme for GCI detection algorithms using pitch smoothness criterion",
      "original": "i15_3284",
      "page_count": 5,
      "order": 710,
      "p1": "3284",
      "pn": "3288",
      "abstract": [
        "Detection of error-free glottal closure instants (GCI) is a critical requirement for many applications including text-to-speech synthesis, causal anti-causal decomposition and voice morphing. Many existing GCI detection algorithms commit errors under certain conditions. In this paper, we propose a post-processing scheme for correcting errors of any GCI detection algorithm. The proposed error correction scheme works on the principle that the fundamental frequency over a voiced segment is slowly varying. The error correction is thus formulated as an optimization problem such that the pitch contour from the corrected GCIs has the least high frequency components. The proposed error correction scheme is experimentally evaluated on speech corpus with simultaneous EGG recordings using three state-of-the-art GCI detection algorithms viz., Dynamic Plosion Index (DPI), Zero Frequency Resonator (ZFR), and Speech Event Detection using the Residual Excitation And a Mean-based Signal (SEDREAMS). It is found that the proposed error correction scheme improves the performance of the GCI detection in clean speech as well as noisy conditions at different SNRs.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-661",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "prasad15b_interspeech": {
      "authors": [
        [
          "RaviShankar",
          "Prasad"
        ],
        [
          "B.",
          "Yegnanarayana"
        ]
      ],
      "title": "Robust pitch estimation in noisy speech using ZTW and group delay function",
      "original": "i15_3289",
      "page_count": 4,
      "order": 711,
      "p1": "3289",
      "pn": "3292",
      "abstract": [
        "Identification of pitch for speech signals recorded in noisy environments is a fundamental and long persistent problem in speech research. Several time domain based techniques attempt to exploit the periodic nature of the waveform using autocorrelation function and its variants. Other set of techniques utilize the harmonic structure in the spectral domain to identify pitch values. Either of these techniques suffer significant degradation in their performance in cases of noisy speech signals with low SNRs. The paper presents a robust technique to identify pitch values for speech signals. The proposed algorithm utilizes a speech analysis method called zero-time windowing (ZTW) where the signal is processed using a heavily decaying window, and the spectral characteristics are highlighted using the numerator of the group delay function. The amplitude contour of dominant resonances in the spectra are extracted, and processed further using a Gaussian window. The resulting contour reflects the energy profile of the signal which is utilized for estimation of the pitch values. The proposed algorithm is robust to degradations, and has been tested on several utterances with added noises. The algorithm exhibits significant increment in performance when compared to existing techniques.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-662",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "huang15g_interspeech": {
      "authors": [
        [
          "Zhaoqiong",
          "Huang"
        ],
        [
          "Ge",
          "Zhan"
        ],
        [
          "Dongwen",
          "Ying"
        ],
        [
          "Yonghong",
          "Yan"
        ]
      ],
      "title": "Robust localization of single sound source based on phase difference regression",
      "original": "i15_3293",
      "page_count": 5,
      "order": 712,
      "p1": "3293",
      "pn": "3297",
      "abstract": [
        "Phase difference regression (PDR) was widely utilized to estimate Direction-of-Arrival (DOA) for linear arrays because of its high time resolution and high computational efficiency. However, conventional regression methods were seldom reported to estimate DOA using planar arrays. This paper proposes a regression method to derive DOA from all phase differences on all frequencies for a planar array. The DOA is represented as the function of the array topology and phase differences between all microphones. Moreover, the proposed method considers another two problems that were often ignored by most regression methods. One is the problem about the period of phase difference in the regression cost function. The other is the signal enhancement that can effectively suppress the acoustic interference. We conducted some experiments in simulated environment to evaluate the proposed method using a 9-element circular array. The experimental results confirmed its superiority in both computational efficiency and robustness.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-663",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "salvati15_interspeech": {
      "authors": [
        [
          "Daniele",
          "Salvati"
        ],
        [
          "Carlo",
          "Drioli"
        ],
        [
          "Gian Luca",
          "Foresti"
        ]
      ],
      "title": "Frequency map selection using a RBFN-based classifier in the MVDR beamformer for speaker localization in reverberant rooms",
      "original": "i15_3298",
      "page_count": 4,
      "order": 713,
      "p1": "3298",
      "pn": "3301",
      "abstract": [
        "We present the weighted minimum variance distortionless response (WMVDR), which is a steered response power (SRP) algorithm, for near-field speaker localization in a reverberant environment. The proposed WMVDR is based on a machine learning approach for computing the incoherent frequency fusion of narrowband power maps. We adopt a radial basis function network (RBFN) classifier for the estimation of the weighting coefficients, and a marginal distribution of narrowband power map as feature for the supervised training operation. Simulations demonstrate the effectiveness of the proposed approach in different conditions.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-664",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "ma15d_interspeech": {
      "authors": [
        [
          "Ning",
          "Ma"
        ],
        [
          "Guy J.",
          "Brown"
        ],
        [
          "Tobias",
          "May"
        ]
      ],
      "title": "Exploiting deep neural networks and head movements for binaural localisation of multiple speakers in reverberant conditions",
      "original": "i15_3302",
      "page_count": 5,
      "order": 714,
      "p1": "3302",
      "pn": "3306",
      "abstract": [
        "This paper presents a novel machine-hearing system that exploits deep neural networks (DNNs) and head movements for binaural localisation of multiple speakers in reverberant conditions. DNNs are used to map binaural features, consisting of the complete cross-correlation function (CCF) and interaural level differences (ILDs), to the source azimuth. Our approach was evaluated using a localisation task in which sources were located in a full 360-degree azimuth range. As a result, front-back confusions often occurred due to the similarity of binaural features in the front and rear hemifields. To address this, a head movement strategy was incorporated in the DNN-based model to help reduce the front-back errors. Our experiments show that, compared to a system based on a Gaussian mixture model (GMM) classifier, the proposed DNN system substantially reduces localisation errors under challenging acoustic scenarios in which multiple speakers and room reverberation are present.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-665",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "nie15b_interspeech": {
      "authors": [
        [
          "Shuai",
          "Nie"
        ],
        [
          "Wei",
          "Xue"
        ],
        [
          "Shan",
          "Liang"
        ],
        [
          "Xueliang",
          "Zhang"
        ],
        [
          "Wenju",
          "Liu"
        ],
        [
          "Liwei",
          "Qiao"
        ],
        [
          "Jianping",
          "Li"
        ]
      ],
      "title": "Joint optimization of recurrent networks exploiting source auto-regression for source separation",
      "original": "i15_3307",
      "page_count": 5,
      "order": 715,
      "p1": "3307",
      "pn": "3311",
      "abstract": [
        "In music interferences condition, source separation is very difficult. In this paper, we propose a novel recurrent network exploiting the auto-regressions of speech and music interference for source separation. An auto-regression can capture the short-term temporal dependencies in data to help the source separation. For the separation, we independently separate the magnitude spectra of speech and interference from the mixture spectra by including an extra masking layer in the recurrent network. Compared to directly evaluating the ideal mask, the extra masking layer relaxes the assumption of independence between speech and interference which is more suitable for the real-world environments. Using the separated spectra of speech and interference, we further explore a discriminative training objective and joint optimization framework for the proposed network, which incorporates the correlations and spectral dependencies of speech and interference into the separation. Systematic experiments show that the proposed model is competitive with the state-of-the-art method in singing-voice separations.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-666",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "gong15_interspeech": {
      "authors": [
        [
          "Rong",
          "Gong"
        ],
        [
          "Philippe",
          "Cuvillier"
        ],
        [
          "Nicolas",
          "Obin"
        ],
        [
          "Arshia",
          "Cont"
        ]
      ],
      "title": "Real-time audio-to-score alignment of singing voice based on melody and lyric information",
      "original": "i15_3312",
      "page_count": 5,
      "order": 716,
      "p1": "3312",
      "pn": "3316",
      "abstract": [
        "Singing voice is specific in music: a vocal performance conveys both music (melody/pitch) and lyrics (text/phoneme) content. This paper aims at exploiting the advantages of melody and lyric information for real-time audio-to-score alignment of singing voice. First, lyrics are added as a separate observation stream into a template-based hidden semi-Markov model (HSMM), whose observation model is based on the construction of vowel templates. Second, early and late fusion of melody and lyric information are processed during real-time audio-to-score alignment. An experiment conducted with two professional singers (male/female) shows that the performance of a lyrics-based system is comparable to that of melody-based score following systems. Furthermore, late fusion of melody and lyric information substantially improves the alignment performance. Finally, maximum a posteriori adaptation (MAP) of the vowel templates from one singer to the other suggests that lyric information can be efficiently used for any singer.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-667",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "lee15i_interspeech": {
      "authors": [
        [
          "Jun-Yong",
          "Lee"
        ],
        [
          "Hye-Seung",
          "Cho"
        ],
        [
          "Hyoung-Gook",
          "Kim"
        ]
      ],
      "title": "Vocal separation from monaural music using adaptive auditory filtering based on kernel back-fitting",
      "original": "i15_3317",
      "page_count": 4,
      "order": 717,
      "p1": "3317",
      "pn": "3320",
      "abstract": [
        "Recently, kernel additive modeling with generalized spatial Wiener filtering (GW) was presented for music/voice separation. In this paper, an adaptive auditory filtering, called generalized weighted \u03b2-order MMSE estimation (WbE), is applied to the basic iterative kernel back-fitting algorithm for improving the separation performance of monaural music signal into music/voice components. In the proposed method, the perceptually weighting factor \u03b1 and the singular value decomposition (SVD)-based factorized spectral amplitude exponent \u03b2 for each kernel component are adaptively calculated for effective WbE-based auditory filtering performance. Experimental results show that the proposed method achieves better separation performance than GW and the existing Bayesian estimators.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-668"
    },
    "yen15_interspeech": {
      "authors": [
        [
          "Frederick Z.",
          "Yen"
        ],
        [
          "Mao-Chang",
          "Huang"
        ],
        [
          "Tai-Shih",
          "Chi"
        ]
      ],
      "title": "A two-stage singing voice separation algorithm using spectro-temporal modulation features",
      "original": "i15_3321",
      "page_count": 4,
      "order": 718,
      "p1": "3321",
      "pn": "3324",
      "abstract": [
        "A two-stage singing voice separation algorithm using spectro-temporal modulation features is proposed in this paper. First, music clips are transformed into auditory spectrograms and the spectral-temporal modulation contents of all time-frequency (T-F) units of the auditory spectrograms are extracted using an auditory model. Then, T-F units are sequentially clustered using the expectation-maximization (EM) algorithm into percussive, harmonic and vocal units through the proposed two-stage algorithm. Lastly, the singing voice is synthesized from clustered vocal T-F units via time-frequency masking. The algorithm was evaluated using the MIR-1K dataset and demonstrated better separation results than our previously proposed one-stage algorithm.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-669",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "lim15_interspeech": {
      "authors": [
        [
          "Hyungjun",
          "Lim"
        ],
        [
          "Myung Jong",
          "Kim"
        ],
        [
          "Hoirin",
          "Kim"
        ]
      ],
      "title": "Robust sound event classification using LBP-HOG based bag-of-audio-words feature representation",
      "original": "i15_3325",
      "page_count": 5,
      "order": 719,
      "p1": "3325",
      "pn": "3329",
      "abstract": [
        "This paper addresses the problem of sound event classification, focusing on feature extraction methods which are robust in noisy environments. In real world, sound events can be easily exposed in a noisy situation causing corruption of distinctive temporal and spectral characteristics. Therefore, extracting robust features to represent these characteristics is important in achieving good classification performance. In this paper, we employ a combination of local binary pattern (LBP) and histogram of oriented gradient (HOG) which are motivated from image processing technique to capture local characteristics of a spectrogram image in the noisy sound events. Furthermore, a bag-of-audio-words (BoAW) method is also applied to the combination of LBP and HOG to capture global characteristics of the spectrogram image. The proposed method is evaluated on a database consisting hundreds of audio clips for two groups of sound events which are aimed at audio surveillance applications. Test sounds are classified at various noise conditions by using a support vector machine and the proposed method shows over 20% relative improvements in average compared to other conventional feature based BoAW methods.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-670",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "tiainen15_interspeech": {
      "authors": [
        [
          "Mikko",
          "Tiainen"
        ],
        [
          "Lari",
          "Vainio"
        ],
        [
          "Kaisa",
          "Tiippana"
        ],
        [
          "Naeem",
          "Komeilipoor"
        ],
        [
          "Martti",
          "Vainio"
        ]
      ],
      "title": "Action planning and congruency effect between articulation and grasping",
      "original": "i15_3390",
      "page_count": 4,
      "order": 720,
      "p1": "3390",
      "pn": "3393",
      "abstract": [
        "Some theories concerning speech mechanisms assume that overlapping representations are involved in programming certain articulatory gestures and hand actions. In previous studies we have shown a compatibility effect between pronouncing or hearing meaningless syllables like [k\u0251] and [ti] and simultaneously performing a power or a precision grip, respectively. The present study investigated whether action selection was necessary for the effect to manifest. The participants were visually presented with a cue for the upcoming manual response. After that, a written syllable \u201cka\u201d or \u201cti\u201d was presented at which point the cued grip was performed and the syllable pronounced. There was also a condition, where the grip was cued but only the vocal response was performed. Manual and vocal reaction times were relatively faster when the grip and syllable were compatible (e.g. power & [k\u0251]) rather than incompatible (e.g. precision & [k\u0251]). When no grip was performed (only cued), the effect was still apparent in vocal reaction times. These results suggest that preparation of a manual action is sufficient to influence vocalizations, and also that action selection is not, however, mandatory for this kind of syllable-grip correspondence.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-671",
      "author_area_id": "1",
      "author_area_label": "Speech Perception, Production, and Acquisition"
    },
    "hecht15_interspeech": {
      "authors": [
        [
          "Ron M.",
          "Hecht"
        ],
        [
          "Aharon",
          "Bar-Hillel"
        ],
        [
          "Stas",
          "Tiomkin"
        ],
        [
          "Hadar",
          "Levi"
        ],
        [
          "Omer",
          "Tsimhoni"
        ],
        [
          "Naftali",
          "Tishby"
        ]
      ],
      "title": "Cognitive workload and vocabulary sparseness: theory and practice",
      "original": "i15_3394",
      "page_count": 5,
      "order": 721,
      "p1": "3394",
      "pn": "3398",
      "abstract": [
        "In this work we present a Language Model (LM) that accounts for the effects of speaker workload by drawing on recent findings in cognitive psychology research. Under workload, speakers tend to shorten their utterances, but still aim to convey their message; hence they use more informative words. Inspired by the Perception and Action Cycle Method (PACM), the LM is used as a baseline dictionary that is constrained to have higher entropy. We show that the resulting LM has a power law relation to the baseline dictionary; i.e., there is a linear relation between the word log-probability under workload and its baseline log-probability. We then test for the existence of this relation in transcriptions of audio text messages (SMS) dictated while driving under different workload conditions. Significance tests were conducted using Monte Carlo simulations, with the data modeled by principal component analysis (PCA) and linear regression (LR). Based on this power law, we suggest a simple algorithm for LM adaptation under workload. Experiments show encouraging results in perplexity improvement of the LM under workload, thus providing empirical support for our model.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-672",
      "author_area_id": "1",
      "author_area_label": "Speech Perception, Production, and Acquisition"
    },
    "andrei15_interspeech": {
      "authors": [
        [
          "Valentin",
          "Andrei"
        ],
        [
          "Horia",
          "Cucu"
        ],
        [
          "Andi",
          "Buzo"
        ],
        [
          "Corneliu",
          "Burileanu"
        ]
      ],
      "title": "Counting competing speakers in a timeframe \u2014 human versus computer",
      "original": "i15_3399",
      "page_count": 5,
      "order": 722,
      "p1": "3399",
      "pn": "3403",
      "abstract": [
        "We propose an automated solution for computing the number of simultaneous active speakers within a timeframe. The method is studied in parallel with a perception experiment realized with the help of 28 volunteers that were asked to detect how many speakers talk simultaneously in several recordings with variable length. For this study we focus on how listening time and the usage of familiar voices in the recordings impact the correct detection ratio. Regarding the automated method we discuss the influence of noise and the evolution of detection error determined by the speech duration. We observe that when capturing clean speech sources, the method is 76% accurate even for 10 simultaneous speakers, considering speech lengths longer than 3.5 seconds. The volunteers did not systematically detect correctly more than 4 competing speakers even when listening up to 80 seconds.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-673",
      "author_area_id": "1",
      "author_area_label": "Speech Perception, Production, and Acquisition"
    },
    "chen15p_interspeech": {
      "authors": [
        [
          "Fei",
          "Chen"
        ],
        [
          "Alexander Siu Tai",
          "Kwok"
        ]
      ],
      "title": "Segmental contribution to the intelligibility of ideal binary-masked sentences",
      "original": "i15_3404",
      "page_count": 4,
      "order": 723,
      "p1": "3404",
      "pn": "3407",
      "abstract": [
        "Many studies have shown the advantage of using ideal binary mask (IdBM) to improve the intelligibility of speech masked by interfering voices. The present work further investigated the segmental contribution to the intelligibility of the IdBM-processed sentences. Three types (i.e., vowel-only, consonant-only, and vowel-consonant transition) of Mandarin IdBM-processed stimuli were generated by using a noise-replacement paradigm to preserve the selected segments and replace the rest with noise. Normal-hearing subjects participated in listening experiments to recognize the IdBM-processed sentences. Experiment results showed that the recognition score of the vowel-only IdBM-processed sentences was significantly higher than that of the consonant-only IdBM-processed sentences, suggesting a greater contribution of vowels than consonants to the intelligibility of the IdBM-processed sentences. Vowel centers contained rich intelligibility information. While consonants and vowel-onset carried little intelligibility information, adding a small proportion of vowel-consonant transition significantly improved the recognition score of the consonant-only and vowel-onset IdBM-processed sentences.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-674",
      "author_area_id": "1",
      "author_area_label": "Speech Perception, Production, and Acquisition"
    },
    "ishida15_interspeech": {
      "authors": [
        [
          "Mako",
          "Ishida"
        ],
        [
          "Takayuki",
          "Arai"
        ]
      ],
      "title": "Perception of an existing and non-existing L2 English phoneme behind noise by Japanese native speakers",
      "original": "i15_3408",
      "page_count": 4,
      "order": 724,
      "p1": "3408",
      "pn": "3411",
      "abstract": [
        "This study investigates how similarly a person hears an existing and non-existing speech sound behind noise in L2, as compared to L1 reported in Mattys, Barkan, and Samuel (2014). Participants were Japanese native speakers who spoke English as a second language. They listened to English words and non-words in which a phoneme was covered by noise (added) or replaced by noise (replaced). The target phoneme was either a liquid or a nasal. In experiment, participants listened to a pair of a word with noise (added or replaced) and a word without noise (normal) in a row, and evaluated the similarity of the two by using an 8-point scale. The results suggested that L2 listeners perceived the added and replaced sound significantly differently. L2 listeners found the added sound (a phoneme + noise) more similar to a normal sound than the replaced sound (noise only), as was also reported in L1 listeners. At the same time, they also perceived the illusory sound of a missing phoneme in the replaced condition. A missing nasal was significantly more restored than a missing liquid. There was no lexical effect in perceptual restoration of phonemes among L2 listeners, although it was reported among L1 listeners.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-675",
      "author_area_id": "1",
      "author_area_label": "Speech Perception, Production, and Acquisition"
    },
    "bhat15_interspeech": {
      "authors": [
        [
          "Chitralekha",
          "Bhat"
        ],
        [
          "Sunil",
          "Kopparapu"
        ]
      ],
      "title": "Viseme comparison based on phonetic cues for varying speech accents",
      "original": "i15_3412",
      "page_count": 5,
      "order": 725,
      "p1": "3412",
      "pn": "3416",
      "abstract": [
        "Human interaction through speech is a multisensory activity, wherein the spoken audio is perceived using both auditory and visual cues. However, in the absence of auditory stimulus, speech content can be perceived through lip reading, using the dynamics of the social context. In our earlier work [1], we had presented a tool enabling hearing impaired to understand spoken speech in videos, through lip reading. During evaluation it was found that a hearing impaired person, trained to lip read Indian English was unable to lip read speech in other accents of English. We hypothesize that this difficulty can be attributed to a difference in viseme formation arising from underlying phonetic characteristics. In this paper, we present a comparison between auditory and visual space for the same speech utterance in English, as spoken by an Indian and a Croatian national. Results show a clear correlation between distances in the visual and auditory domain at viseme level. We then evaluate the feasibility of building visual subtitles through viseme adaptation from unknown accent to known accent.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-676",
      "author_area_id": "1",
      "author_area_label": "Speech Perception, Production, and Acquisition"
    },
    "oreilly15b_interspeech": {
      "authors": [
        [
          "Colm",
          "O'Reilly"
        ],
        [
          "Nicola M.",
          "Marples"
        ],
        [
          "David J.",
          "Kelly"
        ],
        [
          "Naomi",
          "Harte"
        ]
      ],
      "title": "Quantifying difference in vocalizations of bird populations",
      "original": "i15_3417",
      "page_count": 5,
      "order": 726,
      "p1": "3417",
      "pn": "3421",
      "abstract": [
        "Populations of a bird species can evolve over time to become a new species. While plumage patterns and other morphological information can remain constant, the vocalizations of a given population may have diversified enough to warrant reclassification. Thus ornithologists are interested in how to measure call and song similarity in birds in a systematic, repeatable manner. Given the success of speech processing methods applied to bird species classification, this paper presents work on developing a measure of bird call similarity. The method is inspired by human speech dialect separation measurement using a representation of the pitch contour micro-structure. The measure is applied to bird populations with calls that are considered very similar, very different and between these two extremes. Initial results are very promising, with the behavior of the metric consistent with accepted levels of similarity for the populations tested to date.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-677",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "choi15c_interspeech": {
      "authors": [
        [
          "Jae",
          "Choi"
        ],
        [
          "Jeunghun",
          "Kim"
        ],
        [
          "Shin Jae",
          "Kang"
        ],
        [
          "Nam Soo",
          "Kim"
        ]
      ],
      "title": "Reverberation-robust acoustic indoor localization",
      "original": "i15_3422",
      "page_count": 4,
      "order": 727,
      "p1": "3422",
      "pn": "3425",
      "abstract": [
        "We propose an effective approach to acoustic indoor localization that works in reverberant environments. The proposed localization system has been implemented to operate in an inaudible frequency range for practical applications. In order to achieve high performance, we propose an effective acoustic source data structure with a synchronization algorithm. To evaluate the proposed system, series of experiments were conducted in simulated reverberant environments and have shown good performance.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-678",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "ming15_interspeech": {
      "authors": [
        [
          "Huaiping",
          "Ming"
        ],
        [
          "Dong-Yan",
          "Huang"
        ],
        [
          "Lei",
          "Xie"
        ],
        [
          "Haizhou",
          "Li"
        ],
        [
          "Minghui",
          "Dong"
        ]
      ],
      "title": "An alternating optimization approach for phase retrieval",
      "original": "i15_3426",
      "page_count": 5,
      "order": 728,
      "p1": "3426",
      "pn": "3430",
      "abstract": [
        "In this paper, we address the problem of phase retrieval to recover a signal from the magnitude of its Fourier transform. In many applications of phase retrieval, the signals encountered are naturally sparse. In this work, we consider the case where the signal is sparse under the assumption that few components are nonzero. We exploit further the sparse nature of the signals and propose a two stage sparse phase retrieval algorithm. A simple iterative minimization algorithm recovers a sparse signal from measurements of its Fourier transform (or other linear transform) magnitude based on the minimization of a block l1 norm. We show in the experiments that the proposed algorithm achieves a competitive performance. It is robust to noise and scalable in practical implementation. The proposed method converges to a more accurate and stable solution than other existing techniques for synthetic signals. For speech signals, experiments show that the voice quality of reconstructed speech signals is almost as good as the original signals.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-679",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "xiao15c_interspeech": {
      "authors": [
        [
          "Xiong",
          "Xiao"
        ],
        [
          "Shengkui",
          "Zhao"
        ],
        [
          "Xionghu",
          "Zhong"
        ],
        [
          "Douglas L.",
          "Jones"
        ],
        [
          "Eng Siong",
          "Chng"
        ],
        [
          "Haizhou",
          "Li"
        ]
      ],
      "title": "Learning to estimate reverberation time in noisy and reverberant rooms",
      "original": "i15_3431",
      "page_count": 5,
      "order": 729,
      "p1": "3431",
      "pn": "3435",
      "abstract": [
        "The reverberation time, T60, is an important indicator of the reverberation strength in a room and has many applications in speech processing, such as dereverberation. However, the T60 must be blindly estimated if only reverberant speech is available. In this paper, we provide a learning based approach for T60 estimation. We treat the T60 estimation as a classification problem by dividing the T60 range into countable bins (e.g. 19 bins covering 0.1s to 1s with a bin width of 0.05s) and the estimation becomes predicting which bin the true T60 falls into for a given speech. We use deep neural networks (DNN) to learn such a mapping from speech to the T60. The DNN is trained on a large amount of reverberant and noisy speech signals generated from various simulated rooms with known reverberations. After training, we observe that the DNN can learn highly sensible features for the T60 estimation task. Experimental results on the data from both simulated rooms and real rooms confirmed the effectiveness of the DNN learning based approach. In all the test cases, the DNN method significantly outperforms the state-of-the-art SDD T60 estimation method.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-680",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "pang15_interspeech": {
      "authors": [
        [
          "Cheng",
          "Pang"
        ],
        [
          "Jie",
          "Zhang"
        ],
        [
          "Hong",
          "Liu"
        ]
      ],
      "title": "Direction of arrival estimation based on reverberation weighting and noise error estimator",
      "original": "i15_3436",
      "page_count": 5,
      "order": 730,
      "p1": "3436",
      "pn": "3440",
      "abstract": [
        "Direction of Arrival (DOA) estimation is an important technique for speech perception and speaker localization, which has received much attention in recent years. However, most conventional methods concentrate on noisy environments, which leads to serious degradation of their performance in the presence of reverberation. To resolve this phenomenon, a novel approach based on reverberation weighting and noise error estimator by two microphones is proposed for robust DOA estimation in this paper. Firstly, the reverberations in received microphone signals are suppressed by late and early reverberation gains estimated by a spectral subtraction rule and the coherence of direct-arrival signals respectively. Then, the reverberation-suppressed signals are utilized to extract the time difference of arrival (TDOA) by the noise error estimator to reduce the affect of noise. At last, the final DOA is determined by the obtained TDOA combing with the geometry of the microphone array. The proposed method is evaluated in a simulated rectangular room with different levels of noise and reverberation, and experimental results validate that our method achieves favorable performance compared with traditional ones.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-681",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "phan15_interspeech": {
      "authors": [
        [
          "Huy",
          "Phan"
        ],
        [
          "Lars",
          "Hertel"
        ],
        [
          "Marco",
          "Maass"
        ],
        [
          "Radoslaw",
          "Mazur"
        ],
        [
          "Alfred",
          "Mertins"
        ]
      ],
      "title": "Representing nonspeech audio signals through speech classification models",
      "original": "i15_3441",
      "page_count": 5,
      "order": 731,
      "p1": "3441",
      "pn": "3445",
      "abstract": [
        "The human auditory system is very well matched to both human speech and environmental sounds. Therefore, the question arises whether human speech material may provide useful information for training systems for analyzing nonspeech audio signals, such as in a recognition task. To find out how similar nonspeech signals are to speech, we measure the closeness between target nonspeech signals and different basis speech categories via a speech classification model. The speech similarities are finally employed as a descriptor to represent the target signal. We further show that a better descriptor can be obtained by learning to organize the speech categories hierarchically with a tree structure. We conduct experiments for the audio event analysis application by using speech words from the TIMIT dataset to learn the descriptors for the audio events of the Freiburg-106 dataset. Our results on the event recognition task outperform those achieved by the best system even though a simple linear classifier is used. Furthermore, integrating the learned descriptors as an additional source leads to improved performance.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-682",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "ferrer15b_interspeech": {
      "authors": [
        [
          "Luciana",
          "Ferrer"
        ],
        [
          "Mitchell",
          "McLaren"
        ],
        [
          "Aaron",
          "Lawson"
        ],
        [
          "Martin",
          "Graciarena"
        ]
      ],
      "title": "Mitigating the effects of non-stationary unseen noises on language recognition performance",
      "original": "i15_3446",
      "page_count": 5,
      "order": 732,
      "p1": "3446",
      "pn": "3450",
      "abstract": [
        "We introduce a new dataset for the study of the effect of highly non-stationary noises on language recognition (LR) performance. The dataset is based on the data from the 2009 Language Recognition Evaluation organized by the National Institute of Standards and Technology (NIST). Randomly selected noises are added to these signals to achieve a chosen signal-to-noise ratio and percentage of corruption. We study the effect of these noises on LR performance as a function of these parameters and present some initial methods to mitigate the degradation, focusing on the speech activity detection (SAD) step. These methods include discarding the C0 coefficient from the features used for SAD, using a more stringent threshold on the SAD scores, thresholding the speech likelihoods returned by the model as an additional way of detecting noise, and a final model adaptation step. We show that a system optimized for clean speech is clearly suboptimal on this new dataset since the proposed methods lead to gains of up to 35% on the corrupted data, without knowledge of the test noises and with very little effect on clean data performance.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-683",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "ajili15_interspeech": {
      "authors": [
        [
          "Moez",
          "Ajili"
        ],
        [
          "Jean-Fran\u00e7ois",
          "Bonastre"
        ],
        [
          "Solange",
          "Rossato"
        ],
        [
          "Juliette",
          "Kahn"
        ],
        [
          "Itshak",
          "Lapidot"
        ]
      ],
      "title": "An information theory based data-homogeneity measure for voice comparison",
      "original": "i15_3451",
      "page_count": 5,
      "order": 733,
      "p1": "3451",
      "pn": "3455",
      "abstract": [
        "In forensic voice comparison, it is strongly recommended to follow the Bayesian paradigm to present a forensic evidence to the court. In this paradigm, the strength of the forensic evidence is summarized by a likelihood ratio (LR). Theoretically, a LR embeds intrinsically the reliability information. So the LR could belong to large values in good conditions, about 10^\u00b110, while in bad conditions, the LR should be very close to one. But, in the real world, forensic processes are only proposing an empirical estimation of the LRs, sometime far from the theoretical ones and unable to embed reliability information. It is particularly true for speaker recognition systems. They are outputting a score in all situations regardless of the case specific conditions and use some normalization steps in order to see this score as a LR. Consequently, the reliability have to be taken into account separately to the LR, in order to allow to the forensic expert to make an appropriate judgement. The reliability depends firstly on the two signals which compose a voice comparison trial. The presence of speaker specific information and the homogeneity of this information between the two signals of a given voice comparison trial should be evaluated. This paper is dedicated to the latter, the homogeneity. We propose an information theory (IT) based homogeneity measure which determines whether a voice comparison is feasible or not, regardless of the used system.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-684",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "dean15_interspeech": {
      "authors": [
        [
          "David",
          "Dean"
        ],
        [
          "Ahilan",
          "Kanagasundaram"
        ],
        [
          "Houman",
          "Ghaemmaghami"
        ],
        [
          "Md. Hafizur",
          "Rahman"
        ],
        [
          "Sridha",
          "Sridharan"
        ]
      ],
      "title": "The QUT-NOISE-SRE protocol for the evaluation of noisy speaker recognition",
      "original": "i15_3456",
      "page_count": 5,
      "order": 734,
      "p1": "3456",
      "pn": "3460",
      "abstract": [
        "The QUT-NOISE-SRE protocol is designed to mix the large QUT-NOISE database, consisting of over 10 hours of background noise, collected across 10 unique locations covering 5 common noise scenarios, with commonly used speaker recognition datasets such as Switchboard, Mixer and the speaker recognition evaluation (SRE) datasets provided by NIST. By allowing common, clean, speech corpora to be mixed with a wide variety of noise conditions, environmental reverberant responses, and signal-to-noise ratios, this protocol provides a solid basis for the development, evaluation and benchmarking of robust speaker recognition algorithms, and is freely available to download alongside the QUT-NOISE database. In this work, we use the QUT-NOISE-SRE protocol to evaluate a state-of-the-art PLDA i-vector speaker recognition system, demonstrating the importance of designing voice-activity-detection front-ends specifically for speaker recognition, rather than aiming for perfect coherence with the true speech/non-speech boundaries.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-685",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "aronowitz15b_interspeech": {
      "authors": [
        [
          "Hagai",
          "Aronowitz"
        ]
      ],
      "title": "Score stabilization for speaker recognition trained on a small development set",
      "original": "i15_3461",
      "page_count": 5,
      "order": 735,
      "p1": "3461",
      "pn": "3465",
      "abstract": [
        "Nowadays state-of-the-art speaker recognition systems obtain quite accurate results for both text-independent and text-dependent tasks as long as they are trained on a fair amount of development data from the target domain (assuming clean speech). In this work, we address the challenge of building a speaker recognition system with a small development dataset from the target domain without using out-of-domain data whatsoever. When development data is limited, the Nuisance Attribute Projector (NAP) algorithm is (in general) superior to the i-vector approach. We have investigated the relative degradation observed from the different components of the NAP system trained on a small dataset and conclude that score normalization is a major source of degradation. We introduce a novel method for stabilizing the normalized scores. We explicitly estimate a low dimensional subspace in supervector space which accounts for high variability in score normalization parameters. We then compensate the estimated subspace. We report experiments on both text-dependent and text-independent tasks which validate our method and show large error reductions.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-686",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "misra15_interspeech": {
      "authors": [
        [
          "Abhinav",
          "Misra"
        ],
        [
          "Shivesh",
          "Ranjan"
        ],
        [
          "Chunlei",
          "Zhang"
        ],
        [
          "John H. L.",
          "Hansen"
        ]
      ],
      "title": "Anti-spoofing system: an investigation of measures to detect synthetic and human speech",
      "original": "i15_3466",
      "page_count": 5,
      "order": 736,
      "p1": "3466",
      "pn": "3470",
      "abstract": [
        "Automatic Speaker Verification (ASV) systems are prone to spoofing attacks of various kinds. In this study, we explore the effects of different features and spoofing algorithms on a state-of-the-art i-vector speaker verification system. Our study is based on the standard dataset and evaluation protocols released as part of the ASVspoof 2015 challenge. We compare how different features perform while detecting both genuine and spoofed speech. We observe that features that contain phase information (Modified Group Delay based features) are better in detecting synthetic speech, and give comparable performance when compared to standard MFCCs. We report an anti-spoofing system that performs well both on known as well as unknown spoofing attacks.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-687",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "carne15_interspeech": {
      "authors": [
        [
          "Michael J.",
          "Carne"
        ]
      ],
      "title": "A likelihood ratio-based forensic voice comparison in microphone vs. mobile mismatched conditions using Japanese /ai/",
      "original": "i15_3471",
      "page_count": 5,
      "order": 737,
      "p1": "3471",
      "pn": "3475",
      "abstract": [
        "This paper describes a likelihood ratio-based forensic voice comparison experiment in microphone versus mobile channel mismatched conditions using parametric representations of formant trajectories. Cubic polynomial coefficients of /ai/ from non-contemporaneous recordings of 30 Japanese male speakers are used to derive multivariate likelihood ratios. The results are evaluated separately for a matched and mismatched group to determine the effect of the mismatch on system performance. A calibrated cross-validated log-likelihood ratio cost (Cllr) of 0.93 is achieved for the F-pattern of /ai/ representing an 18% reduction in system validity relative to the matched group. Separate testing involving only F1 and F2 features evinces a smaller (10%) reduction; suggesting F3 may be more impacted by channel differences. Spectral analysis of F3 indicates this stems from formant tracking errors due to weak signal energy in transmission. As such, F3 in /ai/ should be excluded from analysis where it is poorly preserved. Given the relatively small percentage reductions in validity, it is concluded that /ai/ may be reasonably robust to the mismatch. However, poor performance in optimal conditions (Cllr= 0.77) suggests it may not be a particularly useful parameter in the first place. Limitations to the current study are also discussed.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-688",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "wester15c_interspeech": {
      "authors": [
        [
          "Mirjam",
          "Wester"
        ],
        [
          "Cassia",
          "Valentini-Botinhao"
        ],
        [
          "Gustav Eje",
          "Henter"
        ]
      ],
      "title": "Are we using enough listeners? no! \u2014 an empirically-supported critique of interspeech 2014 TTS evaluations",
      "original": "i15_3476",
      "page_count": 5,
      "order": 738,
      "p1": "3476",
      "pn": "3480",
      "abstract": [
        "Tallying the numbers of listeners that took part in subjective evaluations of synthetic speech at Interspeech 2014 showed that in more than 60% of papers conclusions are based on listening tests with less than 20 listeners. Our analysis of Blizzard 2013 data shows that for a MOS test measuring naturalness a stable level of significance is only reached when more than 30 listeners are used. In this paper, we set out a list of guidelines, i.e., a checklist for carrying out meaningful subjective evaluations. We further illustrate the importance of sentence coverage and number of listeners by presenting changes to rank order and number of significant pairs by re-analysing data from the Blizzard Challenge 2013.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-689",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "chevelu15_interspeech": {
      "authors": [
        [
          "Jonathan",
          "Chevelu"
        ],
        [
          "Damien",
          "Lolive"
        ],
        [
          "S\u00e9bastien Le",
          "Maguer"
        ],
        [
          "David",
          "Guennec"
        ]
      ],
      "title": "How to compare TTS systems: a new subjective evaluation methodology focused on differences",
      "original": "i15_3481",
      "page_count": 5,
      "order": 739,
      "p1": "3481",
      "pn": "3485",
      "abstract": [
        "Subjective evaluation is a crucial problem in the speech processing community and especially for the speech synthesis field, no matter what system is used. Indeed, when trying to assess the effectiveness of a proposed method, researchers usually conduct subjective evaluations by randomly choosing a small set of samples, from the same domain, taken from a baseline system and the proposed one. When selecting them randomly, statistically, samples with almost no differences are evaluated and the global measure is smoothed which may lead to judge the improvement not significant.   To solve this methodological flaw, we propose to compare speech synthesis systems on thousands of generated samples from various domains and to focus subjective evaluations on the most relevant ones by computing a normalized alignment cost between sample pairs. This process has been successfully applied both in the HTS statistical framework and in the corpus-based approach. We have conducted two perceptive experiments by generating more than 27,000 samples for each system under comparison. A comparison between tests involving most different samples and randomly chosen samples shows clearly that the proposed approach reveals significant differences between the systems.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-690",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "latacz15_interspeech": {
      "authors": [
        [
          "Lukas",
          "Latacz"
        ],
        [
          "Werner",
          "Verhelst"
        ]
      ],
      "title": "Double-ended prediction of the naturalness ratings of the blizzard challenge 2008-2013",
      "original": "i15_3486",
      "page_count": 5,
      "order": 740,
      "p1": "3486",
      "pn": "3490",
      "abstract": [
        "In this paper we describe a double-ended (i.e. reference-based or intrusive) approach to objective quality estimation of synthetic speech that uses a linear regression model whose parameters can easily be interpreted. The model was trained and evaluated on English data from the 2008 to 2013 Blizzard Challenges (BC) [1], which is the largest publically available resource of listener-evaluated synthetic speech. To our knowledge, this is the first attempt to train and evaluate a speech quality predictor on the whole data set. Predicting the naturalness of the different participating systems in the BC is not an easy task because some of the systems are quite close in quality. Our best results correspond to a Pearson correlation coefficient of 0.60 and 0.84 for sentences and systems, respectively, using a leave-one-system-out evaluation, which by far outperformed the ITU-T standard PESQ [2] for double-ended speech quality evaluation on this data.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-691",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "nose15_interspeech": {
      "authors": [
        [
          "Takashi",
          "Nose"
        ],
        [
          "Yusuke",
          "Arao"
        ],
        [
          "Takao",
          "Kobayashi"
        ],
        [
          "Komei",
          "Sugiura"
        ],
        [
          "Yoshinori",
          "Shiga"
        ],
        [
          "Akinori",
          "Ito"
        ]
      ],
      "title": "Entropy-based sentence selection for speech synthesis using phonetic and prosodic contexts",
      "original": "i15_3491",
      "page_count": 5,
      "order": 741,
      "p1": "3491",
      "pn": "3495",
      "abstract": [
        "This paper proposes a sentence selection method using a maximum entropy criterion to construct recording scripts for speech synthesis. In the conventional corpus design of speech synthesis, a greedy algorithm that maximizes phonetic coverage is often used. However, for statistical parametric speech synthesis, phonetic and prosodic contextual balance is important as well as the coverage. To take account of both of the phonetic and prosodic contextual balance in the sentence selection, we introduce and maximize the entropy of the phonetic and prosodic contexts, such as biphone, triphone, accent, and sentence length. The objective experimental results show that the proposed method achieves better coverage and balance of contexts and reduces spectral and F0 distortions compared to the random and coverage-based sentence selection methods.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-692",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "koriyama15_interspeech": {
      "authors": [
        [
          "Tomoki",
          "Koriyama"
        ],
        [
          "Takao",
          "Kobayashi"
        ]
      ],
      "title": "A comparison of speech synthesis systems based on GPR, HMM, and DNN with a small amount of training data",
      "original": "i15_3496",
      "page_count": 5,
      "order": 742,
      "p1": "3496",
      "pn": "3500",
      "abstract": [
        "In this paper, we evaluate a framework of statistical parametric speech synthesis based on Gaussian process regression (GPR) and compare it with those based on hidden Markov model (HMM) and deep neural network (DNN). Recently, for the purpose of improving the performance of HMM-based speech synthesis, novel frameworks using deep architectures have been proposed and have shown their effectiveness. GPR-based speech synthesis is also an alternative framework to HMM-based one, in which the frame-level acoustic features are predicted from frame-level linguistic features, as in DNN-based one. First we examine the clustering level of speech segments such as state, phone, mora, and accent phrase, used for GPR-based synthesis. Then we compare the modeling architecture and performance of GPR with DNN and HMM for statistical parametric speech synthesis. Experimental results show that the GPR-based speech synthesis system gives higher performance than both HMM- and DNN-based ones under the condition using a relatively small size training data of around 40 minutes.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-693",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "ullmann15_interspeech": {
      "authors": [
        [
          "Raphael",
          "Ullmann"
        ],
        [
          "Ramya",
          "Rasipuram"
        ],
        [
          "Mathew",
          "Magimai-Doss"
        ],
        [
          "Herv\u00e9",
          "Bourlard"
        ]
      ],
      "title": "Objective intelligibility assessment of text-to-speech systems through utterance verification",
      "original": "i15_3501",
      "page_count": 5,
      "order": 743,
      "p1": "3501",
      "pn": "3505",
      "abstract": [
        "Objective assessment of synthetic speech intelligibility can be a useful tool for the development of text-to-speech (TTS) systems, as it provides a reproducible and inexpensive alternative to subjective listening tests. In a recent work, it was shown that the intelligibility of synthetic speech could be assessed objectively by comparing two sequences of phoneme class conditional probabilities, corresponding to instances of synthetic and human reference speech, respectively. In this paper, we build on those findings to propose a novel approach that formulates objective intelligibility assessment as an utterance verification problem using hidden Markov models, thereby alleviating the need for human reference speech. Specifically, given each text input to the TTS system, the proposed approach automatically verifies the words in the output synthetic speech signal and estimates an intelligibility score based on word recall statistics. We evaluate the proposed approach on the 2011 Blizzard Challenge data, and show that the estimated scores and the subjective intelligibility scores are highly correlated (Pearson's |R| = 0.94).\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-694",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "fohr15_interspeech": {
      "authors": [
        [
          "Dominique",
          "Fohr"
        ],
        [
          "Irina",
          "Illina"
        ]
      ],
      "title": "Continuous word representation using neural networks for proper name retrieval from diachronic documents",
      "original": "i15_3506",
      "page_count": 5,
      "order": 744,
      "p1": "3506",
      "pn": "3510",
      "abstract": [
        "Developing high-quality transcription systems for very large vocabulary corpora is a challenging task. Proper names are usually key to understanding the information contained in a document. One approach for increasing the vocabulary coverage of a speech transcription system is to automatically retrieve new proper names from contemporary diachronic text documents. In recent years, neural networks have been successfully applied to a variety of speech recognition tasks. In this paper, we investigate whether neural networks can enhance word representation in vector space for the vocabulary extension of a speech recognition system. This is achieved by using high-quality word vector representation of words from large amounts of unstructured text data proposed by Mikolov. This model allows to take into account lexical and semantic word relationships. Proposed methodology is evaluated in the context of broadcast news transcription. Obtained recall and ASR proper name error rate is compared to that obtained using cosine-based vector space methodology. Experimental results show a good ability of the proposed model to capture semantic and lexical information.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-695",
      "author_area_id": "9",
      "author_area_label": "Speech Recognition \u2013 Architecture, Search, and Linguistic Components"
    },
    "chen15q_interspeech": {
      "authors": [
        [
          "X.",
          "Chen"
        ],
        [
          "T.",
          "Tan"
        ],
        [
          "Xunying",
          "Liu"
        ],
        [
          "Pierre",
          "Lanchantin"
        ],
        [
          "M.",
          "Wan"
        ],
        [
          "Mark J. F.",
          "Gales"
        ],
        [
          "Philip C.",
          "Woodland"
        ]
      ],
      "title": "Recurrent neural network language model adaptation for multi-genre broadcast speech recognition",
      "original": "i15_3511",
      "page_count": 5,
      "order": 745,
      "p1": "3511",
      "pn": "3515",
      "abstract": [
        "Recurrent neural network language models (RNNLMs) have recently become increasingly popular for many applications including speech recognition. In previous research RNNLMs have normally been trained on well-matched in-domain data. The adaptation of RNNLMs remains an open research area to be explored. In this paper, genre and topic based RNNLM adaptation techniques are investigated for a multi-genre broadcast transcription task. A number of techniques including Probabilistic Latent Semantic Analysis, Latent Dirichlet Allocation and Hierarchical Dirichlet Processes are used to extract show level topic information. These were then used as additional input to the RNNLM during training, which can facilitate unsupervised test time adaptation. Experiments using a state-of-the-art LVCSR system trained on 1000 hours of speech and more than 1 billion words of text showed adaptation could yield perplexity reductions of 8% relatively over the baseline RNNLM and small but consistent word error rate reductions.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-696",
      "author_area_id": "9",
      "author_area_label": "Speech Recognition \u2013 Architecture, Search, and Linguistic Components"
    },
    "jin15_interspeech": {
      "authors": [
        [
          "Wengong",
          "Jin"
        ],
        [
          "Tianxing",
          "He"
        ],
        [
          "Yanmin",
          "Qian"
        ],
        [
          "Kai",
          "Yu"
        ]
      ],
      "title": "Paragraph vector based topic model for language model adaptation",
      "original": "i15_3516",
      "page_count": 5,
      "order": 746,
      "p1": "3516",
      "pn": "3520",
      "abstract": [
        "Topic model is an important approach for language model (LM) adaptation and has attracted research interest for a long time. Latent Dirichlet Allocation (LDA), which assumes generative Dirichlet distribution with bag-of-word features for hidden topics, has been widely used as the state-of-the-art topic model. Inspired by recent development of a new paradigm of distributed paragraph representation called paragraph vector, a new topic model based on paragraph vector is proposed in this work. During training, each paragraph is mapped to a unique vector in continuous space. Then unsupervised clustering is performed to construct topic clusters. Topic-specific LM is then built based on clustering results. During adaptation, topic posterior is first estimated using the paragraph vector based topic model and new adapted LMs are constructed by interpolating the existing topic-specific models using topic posteriors. The proposed topic model is applied for N-gram LM adaptation and evaluated on Amazon Product Review Corpus for perplexity and a Chinese LVCSR task for CER evaluation. Results show that the proposed approach yields 11.1% relative perplexity reduction and 1.4% relative CER reduction over N-gram baseline, outperforming LDA based method proposed by previous work.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-697",
      "author_area_id": "9",
      "author_area_label": "Speech Recognition \u2013 Architecture, Search, and Linguistic Components"
    },
    "yeh15_interspeech": {
      "authors": [
        [
          "Ching-Feng",
          "Yeh"
        ],
        [
          "Yuan-ming",
          "Liou"
        ],
        [
          "Hung-yi",
          "Lee"
        ],
        [
          "Lin-shan",
          "Lee"
        ]
      ],
      "title": "Personalized speech recognizer with keyword-based personalized lexicon and language model using word vector representations",
      "original": "i15_3521",
      "page_count": 5,
      "order": 747,
      "p1": "3521",
      "pn": "3525",
      "abstract": [
        "The popularity of mobile devices offers an ideal platform for personalized recognizers. With data collected from the user, the personalized recognizer with better matched acoustic and linguistic characteristics can offer not only better recognition accuracy but also less computational time. In this paper, we propose a scenario that a small data set (500 utterances with annotation) can be collected for each user and used to personalize the recognizer. Based on this scenario, we present an overall framework for accuracy improvement and computational time reduction. We train Gaussian Mixture Models (GMMs) based on the word vector representations [1][2] and develop word clusters and keyword extraction approaches for personalization of the lexicon and language model. Prototype recognition systems with CD-DNN-HMM [3][4][5] acoustic models adapted by fDLR [6][7][8][9] were implemented and tested for 10 target users. It was shown that the personalized lexicon may include much more user-specific words not obtained before, and significant performance improvement in terms of tradeoff relationships between recognition accuracy and real time factor was observed.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-698",
      "author_area_id": "9",
      "author_area_label": "Speech Recognition \u2013 Architecture, Search, and Linguistic Components"
    },
    "li15g_interspeech": {
      "authors": [
        [
          "Sheng",
          "Li"
        ],
        [
          "Yuya",
          "Akita"
        ],
        [
          "Tatsuya",
          "Kawahara"
        ]
      ],
      "title": "Discriminative data selection for lightly supervised training of acoustic model using closed caption texts",
      "original": "i15_3526",
      "page_count": 5,
      "order": 748,
      "p1": "3526",
      "pn": "3530",
      "abstract": [
        "We present a novel data selection method for lightly supervised training of acoustic model, which exploits a large amount of data with closed caption texts but not faithful transcripts. In the proposed scheme, a sequence of the closed caption text and that of the ASR hypothesis by the baseline system are aligned. Then, a set of dedicated classifiers is designed and trained to select the correct one among them or reject both. It is demonstrated that the classifiers can effectively filter the usable data for acoustic model training without tuning any threshold parameters. A significant improvement in the ASR accuracy is achieved from the baseline system and also in comparison with the conventional method of lightly supervised training based on simple matching and confidence measure scores.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-699",
      "author_area_id": "9",
      "author_area_label": "Speech Recognition \u2013 Architecture, Search, and Linguistic Components"
    },
    "das15_interspeech": {
      "authors": [
        [
          "Amit",
          "Das"
        ],
        [
          "Mark",
          "Hasegawa-Johnson"
        ]
      ],
      "title": "Cross-lingual transfer learning during supervised training in low resource scenarios",
      "original": "i15_3531",
      "page_count": 5,
      "order": 749,
      "p1": "3531",
      "pn": "3535",
      "abstract": [
        "In this study, transfer learning techniques are presented for cross-lingual speech recognition to mitigate the effects of limited availability of data in a target language using data from richly resourced source languages. A maximum likelihood (ML) based regularization criterion is used to learn context-dependent Gaussian mixture model (GMM) based hidden Markov model (HMM) parameters for phones in target language using data from both target and source languages. Recognition results indicate improved HMM state alignments. The hidden layers of a deep neural network (DNN) are then initialized using unsupervised pre-training of a multilingual deep belief network (DBN). First, the DNN is fine-tuned using a modified cross entropy criterion that jointly uses HMM state alignments from both target and source languages. Second, another DNN fine-tuning technique is explored where the training is performed in a sequential manner \u2014 source language followed by the target language. Experiments conducted using varying amounts of target data indicate improvements in performance can be obtained using joint and sequential training of the DNN compared to existing techniques. Turkish and English were chosen to be the target and source languages respectively.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-700",
      "author_area_id": "9",
      "author_area_label": "Speech Recognition \u2013 Architecture, Search, and Linguistic Components"
    },
    "astudillo15_interspeech": {
      "authors": [
        [
          "Ram\u00f3n F.",
          "Astudillo"
        ],
        [
          "Shinji",
          "Watanabe"
        ],
        [
          "Ahmed Hussen",
          "Abdelaziz"
        ],
        [
          "Dorothea",
          "Kolossa"
        ]
      ],
      "title": "Robust speech processing using observation uncertainty and uncertainty propagation: session and paper overview",
      "original": "i15_4111",
      "page_count": 0,
      "order": 750,
      "p1": "0",
      "pn": "",
      "abstract": [
        "In this talk we will briefly comment on the session objectives and the role of observation uncertainty and uncertainty propagation techniques in the context of robust speech processing. As preparation towards the poster session, a brief overview of the different contributions to the session will also be carried out.\n",
        ""
      ]
    },
    "ribas15b_interspeech": {
      "authors": [
        [
          "Dayana",
          "Ribas"
        ],
        [
          "Emmanuel",
          "Vincent"
        ],
        [
          "Jos\u00e9 Ram\u00f3n",
          "Calvo"
        ]
      ],
      "title": "Uncertainty propagation for noise robust speaker recognition: the case of NIST-SRE",
      "original": "i15_3536",
      "page_count": 5,
      "order": 751,
      "p1": "3536",
      "pn": "3540",
      "abstract": [
        "Uncertainty propagation is an established approach to handle noisy and reverberant conditions in automatic speech recognition (ASR), but it has little been studied for speaker recognition so far. Yu et al. recently proposed to propagate uncertainty to the Baum-Welch (BW) statistics without changing the posterior probability of each mixture component. They obtained good results on a small dataset (YOHO) but little improvement on the NIST-SRE dataset, despite the use of oracle uncertainty estimates. In this paper, we propose to modify the computation of the posterior probability of each mixture component in order to obtain unbiased BW statistics. We show that our approach improves the accuracy of BW statistics on the Wall Street Journal (WSJ) corpus, but yields little or no improvement on NIST-SRE again. We provide a theoretical explanation for this that opens the way for more efficient exploitation of uncertainty on NIST-SRE and other large datasets in the future.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-701",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition \u2013 Signal Processing, Acoustic Modeling, Robustness, and Adaptation"
    },
    "tachioka15_interspeech": {
      "authors": [
        [
          "Yuuki",
          "Tachioka"
        ],
        [
          "Shinji",
          "Watanabe"
        ]
      ],
      "title": "Uncertainty training and decoding methods of deep neural networks based on stochastic representation of enhanced features",
      "original": "i15_3541",
      "page_count": 5,
      "order": 752,
      "p1": "3541",
      "pn": "3545",
      "abstract": [
        "Speech enhancement is an important front-end technique to improve automatic speech recognition (ASR) in noisy environments. However, the wrong noise suppression of speech enhancement often causes additional distortions in speech signals, which degrades the ASR performance. To compensate the distortions, ASR needs to consider the uncertainty of enhanced features, which can be achieved by using the expectation of ASR decoding/training process with respect to the probabilistic representation of input features. However, unlike the Gaussian mixture model, it is difficult for Deep Neural Network (DNN) to deal with this expectation analytically due to the non-linear activations. This paper proposes efficient Monte-Carlo approximation methods for this expectation calculation to realize DNN based uncertainty decoding and training. It first models the uncertainty of input features with linear interpolation between original and enhanced feature vectors with a random interpolation coefficient. By sampling input features based on this stochastic process in training, DNN can learn to generalize the variations of enhanced features. Our method also samples input features in decoding, and integrates multiple recognition hypotheses obtained from the samples. Experiments on the reverberated noisy speech recognition tasks (the second CHiME and REVERB challenges) show the effectiveness of our techniques.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-702",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition \u2013 Signal Processing, Acoustic Modeling, Robustness, and Adaptation"
    },
    "saeidi15b_interspeech": {
      "authors": [
        [
          "Rahim",
          "Saeidi"
        ],
        [
          "Paavo",
          "Alku"
        ]
      ],
      "title": "Accounting for uncertainty of i-vectors in speaker recognition using uncertainty propagation and modified imputation",
      "original": "i15_3546",
      "page_count": 5,
      "order": 753,
      "p1": "3546",
      "pn": "3550",
      "abstract": [
        "One of the biggest challenges in speaker recognition is incomplete observations in test phase caused by availability of only short duration utterances. The problem with short utterances is that speaker recognition needs to be handled by having information from only limited amount of acoustic classes. By considering limited observations from a test speaker, the resulting i-vector as a representative of short utterance will be uncertain; the shorter the duration, the higher the uncertainty. In recent studies, an uncertainty decoding technique has been employed in probabilistic linear discriminant analysis (PLDA) modeling in order to account for uncertain i-vectors. In this paper, we propose to extend uncertainty handling using simplified PLDA scoring and modified imputation. We experiment with a state-of-the-art speaker recognition system focusing on uncertainty caused by controlled utterance duration. The uncertainties after i-vector extraction are being propagated through pre-processing steps and both uncertainty decoding and modified imputation are considered. Our experimental results indicate improved equal error rate and detection cost attained by using uncertainty-of-observation techniques in dealing with short duration utterances.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-703",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition \u2013 Signal Processing, Acoustic Modeling, Robustness, and Adaptation"
    },
    "mallidi15_interspeech": {
      "authors": [
        [
          "Sri Harish",
          "Mallidi"
        ],
        [
          "Tetsuji",
          "Ogawa"
        ],
        [
          "Karel",
          "Vesel\u00fd"
        ],
        [
          "Phani S.",
          "Nidadavolu"
        ],
        [
          "Hynek",
          "Hermansky"
        ]
      ],
      "title": "Autoencoder based multi-stream combination for noise robust speech recognition",
      "original": "i15_3551",
      "page_count": 5,
      "order": 754,
      "p1": "3551",
      "pn": "3555",
      "abstract": [
        "Performances of automatic speech recognition (ASR) systems degrade rapidly when there is a mismatch between train and test acoustic conditions. Performance can be improved using a multi-stream framework, which involves combining posterior probabilities from several classifiers (often deep neural networks (DNNs)) trained on different features/streams. Knowledge about the confidence of each of these classifiers on a noisy test utterance can help in devising better techniques for posterior combination than simple sum and product rules [1]. In this work, we propose to use autoencoders which are multi-layer feed forward neural networks, for estimating this confidence measure. During the training phase, for each stream, an autocoder is trained on TANDEM features extracted from the corresponding DNN. On employing the autoencoder during the testing phase, we show that the reconstruction error of the auto-encoder is correlated to the robustness of the corresponding stream. These error estimates are then used as confidence measures to combine the posterior probabilities generated from each of the streams. Experiments on Aurora4 and BABEL databases indicate significant improvements, especially in the scenario of mismatch between train and test acoustic conditions.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-704",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition \u2013 Signal Processing, Acoustic Modeling, Robustness, and Adaptation"
    },
    "huemmer15_interspeech": {
      "authors": [
        [
          "Christian",
          "Huemmer"
        ],
        [
          "Roland",
          "Maas"
        ],
        [
          "Andreas",
          "Schwarz"
        ],
        [
          "Ram\u00f3n F.",
          "Astudillo"
        ],
        [
          "Walter",
          "Kellermann"
        ]
      ],
      "title": "Uncertainty decoding for DNN-HMM hybrid systems based on numerical sampling",
      "original": "i15_3556",
      "page_count": 5,
      "order": 755,
      "p1": "3556",
      "pn": "3560",
      "abstract": [
        "In this article, we propose an uncertainty decoding scheme for DNN-HMM hybrid systems based on numerical sampling. A finite set of samples is drawn from the estimated probability distribution of the acoustic features and subsequently passed through feature transformations/extensions and the deep neural network (DNN). Then, the nonlinearly-transformed feature samples are averaged at the output of the DNN in order to approximate the posterior distribution of the context-dependent Hidden Markov Model (HMM) states. This concept is experimentally verified for the REVERB challenge task using a reverberation-robust DNN-HMM hybrid system: The numerical sampling is performed in the logmelspec domain, where we estimate the posterior distribution of the acoustic features by combining coherence-based Wiener filtering and uncertainty propagation. The experimental results highlight the good performance of the proposed uncertainty decoding scheme with significantly increased recognition accuracy even for a small number of feature samples.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-705",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition \u2013 Signal Processing, Acoustic Modeling, Robustness, and Adaptation"
    },
    "abdelaziz15_interspeech": {
      "authors": [
        [
          "Ahmed Hussen",
          "Abdelaziz"
        ],
        [
          "Shinji",
          "Watanabe"
        ],
        [
          "John R.",
          "Hershey"
        ],
        [
          "Emmanuel",
          "Vincent"
        ],
        [
          "Dorothea",
          "Kolossa"
        ]
      ],
      "title": "Uncertainty propagation through deep neural networks",
      "original": "i15_3561",
      "page_count": 5,
      "order": 756,
      "p1": "3561",
      "pn": "3565",
      "abstract": [
        "In order to improve the ASR performance in noisy environments, distorted speech is typically pre-processed by a speech enhancement algorithm, which usually results in a speech estimate containing residual noise and distortion. Wemay also have some measures of uncertainty or variance of the estimate. Uncertainty decoding is a framework that utilizes this knowledge of uncertainty in the input features during acoustic model scoring. Such frameworks have been well explored for traditional probabilistic models, but their optimal use for deep neural network (DNN)-based ASR systems is not yet clear. In this paper, we study the propagation of observation uncertainties through the layers of a DNN-based acoustic model. Since this is intractable due to the nonlinearities of the DNN, we employ approximate propagation methods, including Monte Carlo sampling, the unscented transform, and the piecewise exponential approximation of the activation function, to estimate the distribution of acoustic scores. Finally, the expected value of the acoustic score distribution is used for decoding, which is shown to further improve the ASR accuracy on the CHiME database, relative to a highly optimized DNN baseline.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-706",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition \u2013 Signal Processing, Acoustic Modeling, Robustness, and Adaptation"
    },
    "kuhne15_interspeech": {
      "authors": [
        [
          "Marco",
          "K\u00fchne"
        ]
      ],
      "title": "Handling derivative filterbank features in bounded-marginalization-based missing data automatic speech recognition",
      "original": "i15_3566",
      "page_count": 5,
      "order": 757,
      "p1": "3566",
      "pn": "3570",
      "abstract": [
        "This paper extends the familiar missing-data bounded-marginalization technique from static to dynamic filterbank features for noise robust automatic speech recognition. Based on a well-known theorem from Statistics it is shown how the reliability of derivative filterbank features can be expressed in form of a probability density function. As another contribution, the corresponding HMM state emission likelihood equation (bounded-marginalization rule) for dynamic features is derived in closed-form. On the CHiME corpus the new approach showed a superior accuracy compared to previously proposed heuristics for handling missing dynamic features. To the author's best knowledge, the achieved average accuracy of 92.58% is the best result so far reported for the 2011 CHiME Challenge task.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-707",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition \u2013 Signal Processing, Acoustic Modeling, Robustness, and Adaptation"
    },
    "narayanan15_interspeech": {
      "authors": [
        [
          "Arun",
          "Narayanan"
        ],
        [
          "Ananya",
          "Misra"
        ],
        [
          "Kean K.",
          "Chin"
        ]
      ],
      "title": "Large-scale, sequence-discriminative, joint adaptive training for masking-based robust ASR",
      "original": "i15_3571",
      "page_count": 5,
      "order": 758,
      "p1": "3571",
      "pn": "3575",
      "abstract": [
        "Recently, it was shown that the performance of supervised time-frequency masking based robust automatic speech recognition techniques can be improved by training them jointly with the acoustic model [1]. The system in [1], termed deep neural network based joint adaptive training, used fully-connected feed-forward deep neural networks for estimating time-frequency masks and for acoustic modeling; stacked log mel spectra was used as features and training minimized cross entropy loss. In this work, we extend such jointly trained systems in several ways. First, we use recurrent neural networks based on long short-term memory (LSTM) units \u2014 this allows the use of unstacked features, simplifying joint optimization. Next, we use a sequence discriminative training criterion for optimizing parameters. Finally, we conduct experiments on large scale data and show that joint adaptive training can provide gains over a strong baseline. Systematic evaluations on noisy voice-search data show relative improvements ranging from 2% at 15 dB to 5.4% at -5 dB over a sequence discriminative, multi-condition trained LSTM acoustic model.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-708",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition \u2013 Signal Processing, Acoustic Modeling, Robustness, and Adaptation"
    },
    "astudillo15b_interspeech": {
      "authors": [
        [
          "Ram\u00f3n F.",
          "Astudillo"
        ],
        [
          "Joana",
          "Correia"
        ],
        [
          "Isabel",
          "Trancoso"
        ]
      ],
      "title": "Integration of DNN based speech enhancement and ASR",
      "original": "i15_3576",
      "page_count": 5,
      "order": 759,
      "p1": "3576",
      "pn": "3580",
      "abstract": [
        "Speech enhancement employing Deep Neural Networks (DNNs) is gaining strength as a data-driven alternative to classical Minimum Mean Square Error (MMSE) enhancement approaches. In the past, Observation Uncertainty approaches to integrate MMSE speech enhancement with Automatic Speech Recognition (ASR) have yielded good results as a lightweight alternative for robust ASR. In this paper we thus explore the integration of DNN-based speech enhancement with ASR by employing Observation Uncertainty techniques. For this purpose, we explore various techniques and approximations that allow propagating the uncertainty of inference of the DNN into feature domain. This uncertainty can then be used to dynamically compensate the ASR model utilizing techniques like uncertainty decoding. We test the proposed techniques on the AURORA4 corpus and show that notable improvements can be attained over the already effective DNN enhancement.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-709",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition \u2013 Signal Processing, Acoustic Modeling, Robustness, and Adaptation"
    },
    "zhang15h_interspeech": {
      "authors": [
        [
          "C.",
          "Zhang"
        ],
        [
          "Philip C.",
          "Woodland"
        ]
      ],
      "title": "A general artificial neural network extension for HTK",
      "original": "i15_3581",
      "page_count": 5,
      "order": 760,
      "p1": "3581",
      "pn": "3585",
      "abstract": [
        "This paper describes the recently developed artificial neural network (ANN) modules in HTK hidden Markov model toolkit, which enables ANN models with very general feed-forward architectures to be used for either acoustic modelling or feature extraction. The HTK ANN extension includes many recent ANN-based speech processing techniques, such as sequence training, model stacking, speaker adaptation, and parameterised activation functions. The implementation allows efficient training by supporting GPUs and various types of data cache. The ANN modules are fully integrated into the rest of the HTK toolkit, which allows existing GMM-HMM methods to be easily used in the ANN-HMM framework. Speech recognition results on a 300 hours DARPA BOLT conversational Mandarin task show that HTK can produce tandem and hybrid systems with state-of-the-art performance on this very challenging task. Furthermore, the flexibility of the implementation is illustrated using demo systems for a Wall Street Journal (WSJ) task. The HTK ANN extension is planned for release in HTK version 3.5.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-710",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition \u2013 Signal Processing, Acoustic Modeling, Robustness, and Adaptation"
    },
    "ko15_interspeech": {
      "authors": [
        [
          "Tom",
          "Ko"
        ],
        [
          "Vijayaditya",
          "Peddinti"
        ],
        [
          "Daniel",
          "Povey"
        ],
        [
          "Sanjeev",
          "Khudanpur"
        ]
      ],
      "title": "Audio augmentation for speech recognition",
      "original": "i15_3586",
      "page_count": 4,
      "order": 761,
      "p1": "3586",
      "pn": "3589",
      "abstract": [
        "Data augmentation is a common strategy adopted to increase the quantity of training data, avoid overfitting and improve robustness of the models. In this paper, we investigate audio-level speech augmentation methods which directly process the raw signal. The method we particularly recommend is to change the speed of the audio signal, producing 3 versions of the original signal with speed factors of 0.9, 1.0 and 1.1. The proposed technique has a low implementation cost, making it easy to adopt. We present results on 4 different LVCSR tasks with training data ranging from 100 hours to 960 hours, to examine the effectiveness of audio augmentation in a variety of data scenarios. An average relative improvement of 4.3% was observed across the 4 tasks.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-711",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition \u2013 Signal Processing, Acoustic Modeling, Robustness, and Adaptation"
    },
    "zhang15i_interspeech": {
      "authors": [
        [
          "Xiaohui",
          "Zhang"
        ],
        [
          "Daniel",
          "Povey"
        ],
        [
          "Sanjeev",
          "Khudanpur"
        ]
      ],
      "title": "A diversity-penalizing ensemble training method for deep learning",
      "original": "i15_3590",
      "page_count": 5,
      "order": 762,
      "p1": "3590",
      "pn": "3594",
      "abstract": [
        "A common way to improve the performance of deep learning is to train an ensemble of neural networks and combine them during decoding. However, this is computationally expensive in test time. In this paper, we propose an diversity-penalizing ensemble training (DPET) procedure, which trains an ensemble of DNNs, whose parameters were differently initialized, and penalizes differences between each individual DNN's output and their average output. This way each model learns to emulate the average of the whole ensemble of models, and in test time we can use one arbitrarily chosen member of the ensemble. Experimental results on a variety of speech recognition tasks show that this technique is effective, and gives us most of the WER improvement of the ensemble method while being no more expensive in test time than using a single model.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-712",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition \u2013 Signal Processing, Acoustic Modeling, Robustness, and Adaptation"
    },
    "kurata15_interspeech": {
      "authors": [
        [
          "Gakuto",
          "Kurata"
        ],
        [
          "Daniel",
          "Willett"
        ]
      ],
      "title": "Deep neural network training emphasizing central frames",
      "original": "i15_3595",
      "page_count": 5,
      "order": 763,
      "p1": "3595",
      "pn": "3599",
      "abstract": [
        "It is common practice to concatenate several consecutive frames of acoustic features as input of a Deep Neural Network (DNN) for speech recognition. A DNN is trained to map the concatenated frames as a whole to the HMM state corresponding to the center frame while the side frames close to both ends of the concatenated frames and the remaining central frames are treated as equally important. Though the side frames are relevant to the HMM state of the center frame, this relationship may not be fully generalized to unseen data. Thus putting more emphasis on the central frames than on the side frames avoids over-fitting to the DNN training data. We propose a new DNN training method to emphasize the central frames. We first conduct pre-training and fine-tuning with only the central frames and then conduct fine-tuning with all of the concatenated frames. In large vocabulary continuous speech recognition experiments with more than 1,000 hours of data for DNN training, we obtained a relative error rate reduction of 1.68%, which was statistically significant.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-713",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition \u2013 Signal Processing, Acoustic Modeling, Robustness, and Adaptation"
    },
    "chen15r_interspeech": {
      "authors": [
        [
          "Kai",
          "Chen"
        ],
        [
          "Zhi-Jie",
          "Yan"
        ],
        [
          "Qiang",
          "Huo"
        ]
      ],
      "title": "Training deep bidirectional LSTM acoustic model for LVCSR by a context-sensitive-chunk BPTT approach",
      "original": "i15_3600",
      "page_count": 5,
      "order": 764,
      "p1": "3600",
      "pn": "3604",
      "abstract": [
        "This paper presents a study of using deep bidirectional long short-term memory (DBLSTM) as acoustic model for DBLSTM-HMM based large vocabulary continuous speech recognition (LVCSR), where a context-sensitive-chunk (CSC) backpropagation through time (BPTT) approach is used to train DBLSTM by splitting each training sequence into chunks with appended contextual observations, and a (possibly overlapped) CSCs based decoding method is used for recognition. Our approach makes mini-batch based training on GPU more efficient and reduces the latency of DBLSTM-based LVCSR from a whole utterance to a short chunk. Evaluations have been made on Switchboard-I benchmark task. In comparison with epochwise BPTT training, our method can achieve about three times speed-up on a single GPU card. In comparison with a highly optimized DNN-HMM system trained by a frame-level cross entropy (CE) criterion, our CE-trained DBLSTM-HMM system achieves relative word error rate reductions of 9% and 5% on Eval2000 and RT03S testing sets, respectively.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-714",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition \u2013 Signal Processing, Acoustic Modeling, Robustness, and Adaptation"
    },
    "swietojanski15_interspeech": {
      "authors": [
        [
          "Pawel",
          "Swietojanski"
        ],
        [
          "Peter",
          "Bell"
        ],
        [
          "Steve",
          "Renals"
        ]
      ],
      "title": "Structured output layer with auxiliary targets for context-dependent acoustic modelling",
      "original": "i15_3605",
      "page_count": 5,
      "order": 765,
      "p1": "3605",
      "pn": "3609",
      "abstract": [
        "In previous work we have introduced a multi-task training technique for neural network acoustic modelling, in which context-dependent and context-independent targets are jointly learned. In this paper, we extend the approach by structuring the output layer such that the context-dependent outputs are dependent on the context-independent outputs, thus using the context-independent predictions at run-time. We have also investigated the applicability of this idea to unsupervised speaker adaptation as an approach to overcome the data sparsity issues that comes to the fore when estimating systems with a large number of context-dependent states, when data is limited. We have experimented with various amounts of training material (from 10 to 300 hours) and find the proposed techniques are particularly well suited to data-constrained conditions allowing to better utilise large context-dependent state-clustered trees. Experimental results are reported for large vocabulary speech recognition using the Switchboard and TED corpora.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-715",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition \u2013 Signal Processing, Acoustic Modeling, Robustness, and Adaptation"
    },
    "bell15b_interspeech": {
      "authors": [
        [
          "Peter",
          "Bell"
        ],
        [
          "Steve",
          "Renals"
        ]
      ],
      "title": "Complementary tasks for context-dependent deep neural network acoustic models",
      "original": "i15_3610",
      "page_count": 5,
      "order": 766,
      "p1": "3610",
      "pn": "3614",
      "abstract": [
        "We have previously found that context-dependent DNN models for automatic speech recognition can be improved with the use of monophone targets as a secondary task for the network. This paper asks whether the improvements derive from the regularising effect of having a much small number of monophone outputs \u2014 compared to the typical number of tied states \u2014 or from the use of targets that are not tied to an arbitrary state-clustering. We investigate the use of factorised targets for left and right context, and targets motivated by articulatory properties of the phonemes. We present results on a large-vocabulary lecture recognition task. Although the regularising effect of monophones seems to be important, all schemes give substantial improvements over the baseline single task system, even though the cardinality of the outputs is relatively high.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-716",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition \u2013 Signal Processing, Acoustic Modeling, Robustness, and Adaptation"
    },
    "li15h_interspeech": {
      "authors": [
        [
          "Jie",
          "Li"
        ],
        [
          "Heng",
          "Zhang"
        ],
        [
          "Xinyuan",
          "Cai"
        ],
        [
          "Bo",
          "Xu"
        ]
      ],
      "title": "Towards end-to-end speech recognition for Chinese Mandarin using long short-term memory recurrent neural networks",
      "original": "i15_3615",
      "page_count": 5,
      "order": 767,
      "p1": "3615",
      "pn": "3619",
      "abstract": [
        "End-to-end speech recognition systems have been successfully designed for English. Taking into account the distinctive characteristics between Chinese Mandarin and English, it is worthy to do some additional work to transfer these approaches to Chinese. In this paper, we attempt to build a Chinese speech recognition system using end-to-end learning method. The system is based on a combination of deep Long Short-Term Memory Projected (LSTMP) network architecture and the Connectionist Temporal Classification objective function (CTC). The Chinese characters (the number is about 6,000) are used as the output labels directly. To integrate language model information during decoding, the CTC Beam Search method is adopted and optimized to make it more effective and more efficient. We present the first-pass decoding results which are obtained by decoding from scratch using CTC-trained network and language model. Although these results are not as good as the performance of DNN-HMMs hybrid system, they indicate that it is feasible to choose Chinese characters as the output alphabet in the end-to-end speech recognition system.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-717",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition \u2013 Signal Processing, Acoustic Modeling, Robustness, and Adaptation"
    },
    "chen15s_interspeech": {
      "authors": [
        [
          "Mingming",
          "Chen"
        ],
        [
          "Zhanlei",
          "Yang"
        ],
        [
          "Jizhong",
          "Liang"
        ],
        [
          "Yanpeng",
          "Li"
        ],
        [
          "Wenju",
          "Liu"
        ]
      ],
      "title": "Improving deep neural networks based multi-accent Mandarin speech recognition using i-vectors and accent-specific top layer",
      "original": "i15_3620",
      "page_count": 5,
      "order": 768,
      "p1": "3620",
      "pn": "3624",
      "abstract": [
        "In this paper, we propose a method that use i-vectors and model adaptation techniques to improve the performance of deep neural networks(DNNs) based multi-accent Mandarin speech recognition. I-vectors which are speaker-specific features have been proved to be effective when used in accent identification. They can be used in company with conventional spectral features as the input features of DNNs to improve the discrimination for different accents. Meanwhile, we adapt DNNs to different accents by using an accent-specific top layer and shared hidden layers. The accent-specific top layer is used to adapt to different accents while the share hidden layers which can be seen as feature extractors can extract discriminative high-level features between different accents. These two techniques are complementary and can be easily combined together. Our experiments on the 400-hours Intel Accented Mandarin Speech Recognition Corpus show that our proposed method can significantly improve the performance of DNNs-based accented Mandarin speech recognition.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-718",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition \u2013 Signal Processing, Acoustic Modeling, Robustness, and Adaptation"
    },
    "huang15h_interspeech": {
      "authors": [
        [
          "Zhen",
          "Huang"
        ],
        [
          "Jinyu",
          "Li"
        ],
        [
          "Sabato Marco",
          "Siniscalchi"
        ],
        [
          "I-Fan",
          "Chen"
        ],
        [
          "Ji",
          "Wu"
        ],
        [
          "Chin-Hui",
          "Lee"
        ]
      ],
      "title": "Rapid adaptation for deep neural networks through multi-task learning",
      "original": "i15_3625",
      "page_count": 5,
      "order": 769,
      "p1": "3625",
      "pn": "3629",
      "abstract": [
        "We propose a novel approach to addressing the adaptation effectiveness issue in parameter adaptation for deep neural network (DNN) based acoustic models for automatic speech recognition by adding one or more small auxiliary output layers modeling broad acoustic units, such as mono-phones or tied-state (often called senone) clusters. In scenarios with a limited amount of available adaptation data, most senones are usually rarely seen or not observed, and consequently the ability to model them in a new condition is often not fully exploited. With the original senone classification task as the primary task, and adding auxiliary mono-phone/senone-cluster classification as the secondary tasks, multi-task learning (MTL) is employed to adapt the DNN parameters. With the proposed MTL adaptation framework, we improve the learning ability of the original DNN structure, then enlarge the coverage of the acoustic space to deal with the unseen senone problem, and thus enhance the discrimination power of the adapted DNN models. Experimental results on the 20,000-word open vocabulary WSJ task demonstrate that the proposed framework consistently outperforms the conventional linear hidden layer adaptation schemes without MTL by providing 5.4% relative reduction in word error rate (WERR) with only 1 single adaptation utterance, and 10.7% WERR with 40 adaptation utterances against the un-adapted DNN models.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-719",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition \u2013 Signal Processing, Acoustic Modeling, Robustness, and Adaptation"
    },
    "parthasarathi15_interspeech": {
      "authors": [
        [
          "Sree Hari Krishnan",
          "Parthasarathi"
        ],
        [
          "Bjorn",
          "Hoffmeister"
        ],
        [
          "Spyros",
          "Matsoukas"
        ],
        [
          "Arindam",
          "Mandal"
        ],
        [
          "Nikko",
          "Strom"
        ],
        [
          "Sri",
          "Garimella"
        ]
      ],
      "title": "fMLLR based feature-space speaker adaptation of DNN acoustic models",
      "original": "i15_3630",
      "page_count": 5,
      "order": 770,
      "p1": "3630",
      "pn": "3634",
      "abstract": [
        "We investigate the problem of speaker adaptation of DNN acoustic models in two settings: the traditional unsupervised adaptation and a supervised adaptation (SuA) where a few minutes of transcribed speech is available. SuA presents additional difficulties when a test speaker's adaptation information does not match the registered speaker's information. Employing feature-space maximum likelihood linear regression (fMLLR) transformed features as side-information to the DNN, we reintroduce some classical ideas for combining adapted and unadapted features: early and late fusion methods, as well as the estimation of the fMLLR transforms using simple target models (STM). Results show that early fusion helps DNNs generalize better when features are combined after a non-linear bottleneck layer, while late fusion improves robustness, specifically in mismatched cases. STM give consistent improvements in both settings.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-720",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition \u2013 Signal Processing, Acoustic Modeling, Robustness, and Adaptation"
    },
    "li15i_interspeech": {
      "authors": [
        [
          "Xiangang",
          "Li"
        ],
        [
          "Xihong",
          "Wu"
        ]
      ],
      "title": "I-vector dependent feature space transformations for adaptive speech recognition",
      "original": "i15_3635",
      "page_count": 5,
      "order": 771,
      "p1": "3635",
      "pn": "3639",
      "abstract": [
        "In this paper, we propose a new feature normalization approach for deep neural networks (DNNs) based adaptive speech recognition. Each speaker is represented by an i-vector, and the i-vector dependent block-diagonal transformation matrix is obtained by a tensor and performed on the input features. The parameters of tensor are shared by all the frames in the input window, and factorized into three matrices. The proposed approach is more practical for real-application speech recognition tasks since it eliminates the time-consuming adaptive training process to estimate the transformation matrix in feature-space discriminative linear regression (fDLR). We empirically evaluated the proposed approach on a conversational telephone speech recognition task. Experimental results show that the proposed approach can yield 7% relative improvement for the long short-term memory network based speech recognition system.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-721",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition \u2013 Signal Processing, Acoustic Modeling, Robustness, and Adaptation"
    },
    "doulaty15b_interspeech": {
      "authors": [
        [
          "Mortaza",
          "Doulaty"
        ],
        [
          "Oscar",
          "Saz"
        ],
        [
          "Thomas",
          "Hain"
        ]
      ],
      "title": "Unsupervised domain discovery using latent dirichlet allocation for acoustic modelling in speech recognition",
      "original": "i15_3640",
      "page_count": 5,
      "order": 772,
      "p1": "3640",
      "pn": "3644",
      "abstract": [
        "Speech recognition systems are often highly domain dependent, a fact widely reported in the literature. However the concept of domain is complex and not bound to clear criteria. Hence it is often not evident if data should be considered to be out-of-domain. While both acoustic and language models can be domain specific, work in this paper concentrates on acoustic modelling. We present a novel method to perform unsupervised discovery of domains using Latent Dirichlet Allocation (LDA) modelling. Here a set of hidden domains is assumed to exist in the data, whereby each audio segment can be considered to be a weighted mixture of domain properties. The classification of audio segments into domains allows the creation of domain specific acoustic models for automatic speech recognition. Experiments are conducted on a dataset of diverse speech data covering speech from radio and TV broadcasts, telephone conversations, meetings, lectures and read speech, with a joint training set of 60 hours and a test set of 6 hours. Maximum A Posteriori (MAP) adaptation to LDA based domains was shown to yield relative Word Error Rate (WER) improvements of up to 16% relative, compared to pooled training, and up to 10%, compared with models adapted with human-labelled prior domain knowledge.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-722",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition \u2013 Signal Processing, Acoustic Modeling, Robustness, and Adaptation"
    },
    "asami15_interspeech": {
      "authors": [
        [
          "Taichi",
          "Asami"
        ],
        [
          "Ryo",
          "Masumura"
        ],
        [
          "Hirokazu",
          "Masataki"
        ],
        [
          "Manabu",
          "Okamoto"
        ],
        [
          "Sumitaka",
          "Sakauchi"
        ]
      ],
      "title": "Training data selection for acoustic modeling via submodular optimization of joint kullback-leibler divergence",
      "original": "i15_3645",
      "page_count": 5,
      "order": 773,
      "p1": "3645",
      "pn": "3649",
      "abstract": [
        "This paper provides a novel training data selection method to construct acoustic models for automatic speech recognition (ASR). Various training data sets have been developed for acoustic modeling. Each training set was created for a specific ASR application such that acoustic characteristics in the set, e.g. speakers, noise and recording devices, match those in the application. A mixture of such already-created training sets (an out-of-domain set) becomes a large utterance set containing various acoustic characteristics. The proposed method selects the most appropriate subset of the out-of-domain set and uses it for supervised training of an acoustic model for a new ASR application. The subset that has the most similar acoustic characteristics to the target-domain set (i.e. untranscribed utterances recorded by the target application) is selected based on the proposed joint Kullback-Leibler (KL) divergence of speech and non-speech characteristics. Furthermore, in order to select one of the many subsets in practical computation time, we also propose a selection algorithm based on submodular optimization that minimizes the joint KL divergence by greedy selection with guaranteed optimality. Experiments on real meeting utterances that use deep neural network acoustic models show that the proposed method yields better acoustic models than random or likelihood-based selection.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-723",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition \u2013 Signal Processing, Acoustic Modeling, Robustness, and Adaptation"
    },
    "cho15_interspeech": {
      "authors": [
        [
          "Eunah",
          "Cho"
        ],
        [
          "Kevin",
          "Kilgour"
        ],
        [
          "Jan",
          "Niehues"
        ],
        [
          "Alex",
          "Waibel"
        ]
      ],
      "title": "Combination of NN and CRF models for joint detection of punctuation and disfluencies",
      "original": "i15_3650",
      "page_count": 5,
      "order": 774,
      "p1": "3650",
      "pn": "3654",
      "abstract": [
        "Inserting proper punctuation marks and deleting speech disfluencies are two of the most essential tasks in spoken language processing. This challenging task has prompted extensive research using various techniques, such as conditional random fields. Neural networks, however, are relatively under-explored for this task.   Combining different modeling techniques with different advantages has the potential to lead to improvements. In this work, we first establish the performance of joint modeling of punctuation prediction and disfluency detection using neural networks. We then combine a conditional random fields based model and a neural networks based model log-linearly, and show that the combined approach outperforms both individual models, by 2.7% and 3.5% in F-score for speech disfluency and punctuation detection, respectively. When used as a preprocessing step to machine translation this also results in an improved translation quality of 2.5 BLEU points compared to the baseline and of 0.6 BLEU points compared to the non-combined model.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-724",
      "author_area_id": "12",
      "author_area_label": "Spoken Language Processing \u2013 Translation, Information Retrieval, and Resources"
    },
    "lau15_interspeech": {
      "authors": [
        [
          "Tze Siong",
          "Lau"
        ],
        [
          "I-Fan",
          "Chen"
        ],
        [
          "Chin-Hui",
          "Lee"
        ]
      ],
      "title": "Tunable keyword-aware language modeling and context dependent fillers for LVCSR-based spoken keyword search",
      "original": "i15_3655",
      "page_count": 5,
      "order": 775,
      "p1": "3655",
      "pn": "3659",
      "abstract": [
        "We explore the potential of using keyword-aware language modeling to extend the ability of trading higher false alarm rates in exchange for lower miss detection rates in LVCSR-based keyword search (KWS). A context-dependent keyword language modeling method is also proposed to further enhance the keyword-aware language modeling framework by reducing the number of false alarms often sacrificed in order to achieve the desirable low miss detection rates. We demonstrate that by using keyword-aware language modeling, a KWS system is able to achieve different operating points (misses vs. false alarms) by tuning a parameter in language modeling. We observe a relative gain of 20% in actual term weighted value (ATWV) performance with the keyword-aware KWS systems over the conventional LVCSR-based KWS systems when testing on the English Switchboard data. Moreover the proposed context-dependent keyword language modeling could further achieve a 9% relative ATWV improvement over the original keyword-aware KWS systems for single-word keywords which cause the most false alarms.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-725",
      "author_area_id": "12",
      "author_area_label": "Spoken Language Processing \u2013 Translation, Information Retrieval, and Resources"
    },
    "wang15k_interspeech": {
      "authors": [
        [
          "Haipeng",
          "Wang"
        ],
        [
          "Anton",
          "Ragni"
        ],
        [
          "Mark J. F.",
          "Gales"
        ],
        [
          "Kate M.",
          "Knill"
        ],
        [
          "Philip C.",
          "Woodland"
        ],
        [
          "C.",
          "Zhang"
        ]
      ],
      "title": "Joint decoding of tandem and hybrid systems for improved keyword spotting on low resource languages",
      "original": "i15_3660",
      "page_count": 5,
      "order": 776,
      "p1": "3660",
      "pn": "3664",
      "abstract": [
        "Keyword spotting (KWS) for low-resource languages has drawn increasing attention in recent years. The state-of-the-art KWS systems are based on lattices or Confusion Networks (CN) generated by Automatic Speech Recognition (ASR) systems. It has been shown that considerable KWS gains can be obtained by combining the keyword detection results from different forms of ASR systems, e.g., Tandem and Hybrid systems. This paper investigates an alternative combination scheme for KWS using joint decoding. This scheme treats a Tandem system and a Hybrid system as two separate streams, and makes a linear combination of individual acoustic model log-likelihoods. Joint decoding is more efficient as it requires just a single pass of decoding and a single pass of keyword search. Experiments on six Babel OP2 development languages show that joint decoding is capable of providing consistent gains over each individual system. Moreover, it is possible to efficiently rescore the joint decoding lattices with Tandem or Hybrid acoustic models, and further KWS gains can be obtained by merging the detection posting lists from the joint decoding lattices and rescored lattices.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-726",
      "author_area_id": "12",
      "author_area_label": "Spoken Language Processing \u2013 Translation, Information Retrieval, and Resources"
    },
    "do15_interspeech": {
      "authors": [
        [
          "Quoc Truong",
          "Do"
        ],
        [
          "Shinnosuke",
          "Takamichi"
        ],
        [
          "Sakriani",
          "Sakti"
        ],
        [
          "Graham",
          "Neubig"
        ],
        [
          "Tomoki",
          "Toda"
        ],
        [
          "Satoshi",
          "Nakamura"
        ]
      ],
      "title": "Preserving word-level emphasis in speech-to-speech translation using linear regression HSMMs",
      "original": "i15_3665",
      "page_count": 5,
      "order": 777,
      "p1": "3665",
      "pn": "3669",
      "abstract": [
        "In speech, emphasis is an important type of paralinguistic information that helps convey the focus of an utterance, new information, and emotion. If emphasis can be incorporated into a speech-to-speech (S2S) translation system, it will be possible to convey this information across the language barrier. However, previous related work focuses only on the translation of particular prosodic features, such as F0, or works with emphasis but focuses on extremely small vocabularies, such as the 10 digits. In this paper, we describe a new S2S method that is able to translate the emphasis across languages and consider multiple features of emphasis such as power, F0, and duration over larger vocabularies. We do so by introducing two new components: word-level emphasis estimation using linear regression hidden semi-Markov models, and emphasis translation that translates the word-level emphasis to the target language with conditional random fields. The text-to-speech synthesis system is also modified to be able to synthesize emphasized speech. The result shows that our system can translate the emphasis correctly with 91.6% F-measure for objective test, and 87.8% for subjective test.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-727",
      "author_area_id": "12",
      "author_area_label": "Spoken Language Processing \u2013 Translation, Information Retrieval, and Resources"
    },
    "ngo15_interspeech": {
      "authors": [
        [
          "Hoang Gia",
          "Ngo"
        ],
        [
          "Nancy F.",
          "Chen"
        ],
        [
          "Binh Minh",
          "Nguyen"
        ],
        [
          "Bin",
          "Ma"
        ],
        [
          "Haizhou",
          "Li"
        ]
      ],
      "title": "Phonology-augmented statistical transliteration for low-resource languages",
      "original": "i15_3670",
      "page_count": 5,
      "order": 778,
      "p1": "3670",
      "pn": "3674",
      "abstract": [
        "Transliteration converts words in a source language (e.g., English) into phonetically equivalent words in a target language (e.g., Vietnamese). This conversion needs to take into account phonology of the target language, which are rules determining how phonemes can be organized. For example, a transliterated word in Vietnamese that begins with a consonant cluster is phonologically invalid. While statistical transliteration approaches have been widely adopted, most do not explicitly model the target language's phonology, and thus produce invalid outputs. The problem is compounded for low-resource languages where training data is scarce. In this work, we present a phonology-augmented statistical framework suitable for languages with minimal linguistic resources. We propose the concept of pseudo-syllables as structures representing how segments of a foreign word are arranged according to the target language's phonology. We use Vietnamese, a tonal language with monosyllabic structure as an example. We show that the proposed system outperforms the statistical baseline by up to 70.3% relative, when there are limited training examples (94 word pairs). We also investigate the trade-off between training corpus size and transliteration performance of different methods on two distinct corpora.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-728",
      "author_area_id": "12",
      "author_area_label": "Spoken Language Processing \u2013 Translation, Information Retrieval, and Resources"
    },
    "oouchi15_interspeech": {
      "authors": [
        [
          "Kazuki",
          "Oouchi"
        ],
        [
          "Ryota",
          "Konno"
        ],
        [
          "Takahiro",
          "Akyu"
        ],
        [
          "Kazuma",
          "Konno"
        ],
        [
          "Kazunori",
          "Kojima"
        ],
        [
          "Kazuyo",
          "Tanaka"
        ],
        [
          "Shi-wook",
          "Lee"
        ],
        [
          "Yoshiaki",
          "Itoh"
        ]
      ],
      "title": "Evaluation of re-ranking by prioritizing highly ranked documents in spoken term detection",
      "original": "i15_3675",
      "page_count": 5,
      "order": 779,
      "p1": "3675",
      "pn": "3679",
      "abstract": [
        "In spoken term detection, the detection of out-of-vocabulary (OOV) query terms is very important because of the high probability of OOV query terms occurring. This paper proposes a re-ranking method for improving the detection accuracy for OOV query terms after extracting candidate sections by conventional method. The candidate sections are ranked by using dynamic time warping to match the query terms to all available spoken documents. Because highly ranked candidate sections are usually reliable and users are assumed to input query terms that are specific to and appear frequently in the target documents, we prioritize candidate sections contained in highly ranked documents by adjusting the matching score. Experiments were conducted to evaluate the performance of the proposed method, using open test collections for the SpokenDoc-2 task in the NTCIR-10 workshop. Results showed that the mean average precision (MAP) was improved more than 7.0 points by the proposed method for the two test sets. Also, the proposed method was applied to the results obtained by other participants in the workshop, in which the MAP was improved by more than 6 points in all cases. This demonstrated the effectiveness of the proposed method.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-729",
      "author_area_id": "12",
      "author_area_label": "Spoken Language Processing \u2013 Translation, Information Retrieval, and Resources"
    },
    "saxena15_interspeech": {
      "authors": [
        [
          "Abhijeet",
          "Saxena"
        ],
        [
          "B.",
          "Yegnanarayana"
        ]
      ],
      "title": "Distinctive feature based representation of speech for query-by-example spoken term detection",
      "original": "i15_3680",
      "page_count": 5,
      "order": 780,
      "p1": "3680",
      "pn": "3684",
      "abstract": [
        "In this paper, we address the problem of searching spoken queries within spoken databases, which is referred to as query-by-example Spoken Term Detection (QbE STD). A knowledge-based posteriorgram representation of speech is proposed. The knowledge of sound pattern of a language can be captured in terms of binary distinctive features (DFs). This idea is tailored for the needs of an STD system. The proposed representation can be used as a front-end of a template-based QbE STD system. Template-based spoken term detection experiments are conducted on TIMIT database. Segmental dynamic time warping (DTW) is used for template matching. The performance of STD system improves from a mean average precision (MAP) score of 68.38% when using multi-layer perceptron (MLP) posteriorgram, to an MAP score of 75.35% when using proposed DF representation.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-730",
      "author_area_id": "12",
      "author_area_label": "Spoken Language Processing \u2013 Translation, Information Retrieval, and Resources"
    },
    "lee15j_interspeech": {
      "authors": [
        [
          "Shi-wook",
          "Lee"
        ],
        [
          "Kazuyo",
          "Tanaka"
        ],
        [
          "Yoshiaki",
          "Itoh"
        ]
      ],
      "title": "Combination of diverse subword units in spoken term detection",
      "original": "i15_3685",
      "page_count": 5,
      "order": 781,
      "p1": "3685",
      "pn": "3689",
      "abstract": [
        "This paper focuses on the following two points: First, we try to clarify the effect of combination systems from two aspects, accuracy and heterogeneity. And then we evaluate our unique subword unit, called Sub-Phonetic Segment (SPS) to maximize performance improvement by combination. Combination systems usually yield higher performance than any individual system. When the systems being combined are individually accurate but also mutually heterogeneous, the improvement by combination can be maximized. From this consideration, we estimate heterogeneity by correlation of false alarm errors of combined systems and confirm that lower correlation of two systems yields the better performance improvement by combination. Comparative tests of several combination approaches are carried out on subword-based spoken term detection. Since subword-based systems use constrained linguistic knowledge, it is fairly straightforward to verify the heterogeneity of combined systems. Experimental results show that the most significant improvements can be achieved by combination of two different subword units, triphone and SPS, which are highly heterogeneous subword units with low correlation of false alarm detections.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-731",
      "author_area_id": "12",
      "author_area_label": "Spoken Language Processing \u2013 Translation, Information Retrieval, and Resources"
    },
    "ram15_interspeech": {
      "authors": [
        [
          "Dhananjay",
          "Ram"
        ],
        [
          "Afsaneh",
          "Asaei"
        ],
        [
          "Pranay",
          "Dighe"
        ],
        [
          "Herv\u00e9",
          "Bourlard"
        ]
      ],
      "title": "Sparse modeling of posterior exemplars for keyword detection",
      "original": "i15_3690",
      "page_count": 5,
      "order": 782,
      "p1": "3690",
      "pn": "3694",
      "abstract": [
        "Sparse representation has been shown to be a powerful modeling framework for classification and detection tasks. In this paper, we propose a new keyword detection algorithm based on sparse representation of the posterior exemplars. The posterior exemplars are phone conditional probabilities obtained from a deep neural network. This method relies on the concept that a keyword exemplar lies in a low-dimensional subspace which can be represented as a sparse linear combination of the training exemplars. The training exemplars are used to learn a dictionary for sparse representation of the keywords and background classes. Given this dictionary, the sparse representation of a test exemplar is used to detect the keywords. The experimental results demonstrate the potential of the proposed sparse modeling approach and it compares favorably with the state-of-the-art HMM-based framework on Numbers'95 database.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-732",
      "author_area_id": "12",
      "author_area_label": "Spoken Language Processing \u2013 Translation, Information Retrieval, and Resources"
    },
    "nwe15_interspeech": {
      "authors": [
        [
          "Tin Lay",
          "Nwe"
        ],
        [
          "Qianli",
          "Xu"
        ],
        [
          "Cuntai",
          "Guan"
        ],
        [
          "Bin",
          "Ma"
        ]
      ],
      "title": "Stress level detection using double-layer subband filter",
      "original": "i15_3695",
      "page_count": 5,
      "order": 783,
      "p1": "3695",
      "pn": "3699",
      "abstract": [
        "Stress level detection is important for human error prevention and health care services. Speech based stress level detection is the most effective as speech data can be obtained in nonintrusive and inexpensive ways. In this paper, we explore the features that use Double-Layered Subband (DLS) filter for detecting stress level from speech. Spectral Centroid Frequency (SCF) and Spectral Centroid Amplitude (SCA) are acoustic features that complement each other if these two are fused together using appropriate weighting coefficient. We extract SCA using DLS filters. And, we present how DLS filter integrates SCF information to SCA feature without actually computing SCF feature parameters. We investigate the effectiveness of proposed approach over combining SCA and SCF using weighting coefficient. We build user independent stress level detection system. Stress is detected using the scale of 0 to 1 (`0' being index for no stress and `1' being index the highest stress level). The experiments show that the proposed system is able to detect the level of stress from speech with reasonably high accuracy.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-733",
      "author_area_id": "3",
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "trouvain15_interspeech": {
      "authors": [
        [
          "J\u00fcrgen",
          "Trouvain"
        ],
        [
          "Khiet P.",
          "Truong"
        ]
      ],
      "title": "Prosodic characteristics of read speech before and after treadmill running",
      "original": "i15_3700",
      "page_count": 5,
      "order": 784,
      "p1": "3700",
      "pn": "3704",
      "abstract": [
        "Physical activity leads to a respiratory behaviour that is very different to a resting state and that influences speech production. How speech parameters are exactly affected by physical activity remains largely unknown. Hence, we investigated how several prosodic parameters change under influence of physical activity and focused on temporal and breathing characteristics which have not been addressed in detail before. Speech from subjects reading aloud a text before and after a treadmill running exercise was analysed for prosodic differences between before and after running. The most important findings include a higher articulation rate, longer averaged pause and breath durations, a higher in-breath intensity, a higher out-breath rate, and a higher mean F0 for speech recorded immediately after vigorous treadmill running. These findings provide fundamental insights into how speech characteristics are affected by physical effort, and may help advance automatic classification of physical stress in speech.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-734",
      "author_area_id": "3",
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "truong15_interspeech": {
      "authors": [
        [
          "Khiet P.",
          "Truong"
        ],
        [
          "Arne",
          "Nieuwenhuys"
        ],
        [
          "Peter",
          "Beek"
        ],
        [
          "Vanessa",
          "Evers"
        ]
      ],
      "title": "A database for analysis of speech under physical stress: detection of exercise intensity while running and talking",
      "original": "i15_3705",
      "page_count": 5,
      "order": 785,
      "p1": "3705",
      "pn": "3709",
      "abstract": [
        "One of the ways to gauge your own exercise intensity while running, is to assess your capability of talking while running: if you can still speak comfortably, you are running within the recommended intensity guidelines. This subjective way of estimating one's exercise intensity by talking (i.e. the Talk Test) motivated us to investigate how speech characteristics are affected during running and whether it is possible to develop a more objective way of estimating exercise intensity levels while running through voice analysis. To this end, we developed the Talk & Run Speech database that contains speech recorded from people before, during, and after running. We present our database and show that it is possible to detect exercise intensity below or above the anaerobic threshold in speech during running with a performance of 73.5% and 60.0% (unweighted average recall) for female and male speakers respectively.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-735",
      "author_area_id": "3",
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "paul15_interspeech": {
      "authors": [
        [
          "Will",
          "Paul"
        ],
        [
          "Cecilia Ovesdotter",
          "Alm"
        ],
        [
          "Reynold",
          "Bailey"
        ],
        [
          "Joe",
          "Geigel"
        ],
        [
          "Linwei",
          "Wang"
        ]
      ],
      "title": "Stressed out: what speech tells us about stress",
      "original": "i15_3710",
      "page_count": 5,
      "order": 786,
      "p1": "3710",
      "pn": "3714",
      "abstract": [
        "Stress can have a negative and costly impact on people's lives. Mitigating stress before it becomes a problem requires early, noninvasive identification and a deeper understanding of the signals of stress. To test automatic stress detection a new dataset was created with subjects completing the Stroop task under unstressed and stressed conditions. This paper examines to what degree speech features respond to stress and if so, what features are most informative. Features were extracted from recorded speech data and trained with several classification algorithms. We explored binary classification of stressed vs. unstressed across gender and per gender, with the best results on a held-out test set improving over the majority class baseline (MCB) by 16% across genders and with 20% and 21% for the female and male subsets respectively. Overall maximum intensity emerged as the most informative feature when comparing across classification conditions. In addition, we explored leave-one subject-out classification, resulting in a 15% improvement on average considering both genders when using random forests.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-736",
      "author_area_id": "3",
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "tsiartas15_interspeech": {
      "authors": [
        [
          "Andreas",
          "Tsiartas"
        ],
        [
          "Andreas",
          "Kathol"
        ],
        [
          "Elizabeth",
          "Shriberg"
        ],
        [
          "Massimiliano de",
          "Zambotti"
        ],
        [
          "Adrian",
          "Willoughby"
        ]
      ],
      "title": "Prediction of heart rate changes from speech features during interaction with a misbehaving dialog system",
      "original": "i15_3715",
      "page_count": 5,
      "order": 787,
      "p1": "3715",
      "pn": "3719",
      "abstract": [
        "Most research on detecting a speaker's cognitive state when interacting with a dialog system has been based on self-reports, or on hand-coded subjective judgments based on audio or audio-visual observations. This study examines two questions: (1) how do undesirable system responses affect people physiologically, and (2) to what extent can we predict physiological changes from the speech signal alone? To address these questions, we use a new corpus of simultaneous speech and high-quality physiological recordings in the product returns domain (the SRI BioFrustration Corpus). \u201cTriggers\u201d were used to frustrate users at specific times during the interaction to produce emotional responses at similar times during the experiment across participants. For each of eight return tasks per participant, we compared speaker-normalized pre-trigger (cooperative system behavior) regions to post-trigger (uncooperative system behavior) regions. Results using random forest classifiers show that changes in spectral and temporal features of speech can predict heart rate changes with an accuracy of ~70%. Implications for future research and applications are discussed.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-737"
    },
    "pietrowicz15_interspeech": {
      "authors": [
        [
          "Mary",
          "Pietrowicz"
        ],
        [
          "Mark",
          "Hasegawa-Johnson"
        ],
        [
          "Karrie",
          "Karahalios"
        ]
      ],
      "title": "Acoustic correlates for perceived effort levels in expressive speech",
      "original": "i15_3720",
      "page_count": 5,
      "order": 788,
      "p1": "3720",
      "pn": "3724",
      "abstract": [
        "Actors and other vocal performers vary their speech across the continuum of vocal effort to express ideas, emphasize thoughts, communicate emotions, and create drama. They are experts at vocal expression. To analyze this range of expression across effort levels, we curated a corpus of professional actors' Hamlet soliloquy performances and present an acoustic feature set and classification model suitable for tracking actors' expressive speech from extreme to extreme \u2014 from whispered, to breathy, through modal, to resonant speech.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-738",
      "author_area_id": "3",
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "daoudi15_interspeech": {
      "authors": [
        [
          "Khalid",
          "Daoudi"
        ],
        [
          "Ashwini Jaya",
          "Kumar"
        ]
      ],
      "title": "Pitch-based speech perturbation measures using a novel GCI detection algorithm: application to pathological voice classification",
      "original": "i15_3725",
      "page_count": 4,
      "order": 789,
      "p1": "3725",
      "pn": "3728",
      "abstract": [
        "Classical pitch-based perturbation measures, such as Jitter and Shimmer, are generally based on detection algorithms of pitch marks which assume the existence of a periodic pitch pattern and/or rely on the linear source-filter speech model. While these assumptions can hold for normal speech, they are generally not valid for pathological speech. The latter can indeed present strong aperiodicity, nonlinearity and turbulence/noise. Recently, we introduced on a novel nonlinear algorithm for Glottal Closure Instants (GCI) detection which has the strong advantage of not making such assumptions. In this paper, we use this new algorithm to compute standard pitch-based perturbation measures and compare its performances to the widely used tool PRAAT.We address the task of classification between normal and pathological speech, and carry out the experiments using the popular MEEI database. The results show that our algorithm leads to significantly higher classification accuracy than PRAAT. Moreover, some important statistical features become significantly discriminative, while they are meaningless when using PRAAT (in the sense that they have almost no discrimination power).\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-739",
      "author_area_id": "3",
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "vergyri15_interspeech": {
      "authors": [
        [
          "Dimitra",
          "Vergyri"
        ],
        [
          "Bruce",
          "Knoth"
        ],
        [
          "Elizabeth",
          "Shriberg"
        ],
        [
          "Vikramjit",
          "Mitra"
        ],
        [
          "Mitchell",
          "McLaren"
        ],
        [
          "Luciana",
          "Ferrer"
        ],
        [
          "Pablo",
          "Garcia"
        ],
        [
          "Charles",
          "Marmar"
        ]
      ],
      "title": "Speech-based assessment of PTSD in a military population using diverse feature classes",
      "original": "i15_3729",
      "page_count": 5,
      "order": 790,
      "p1": "3729",
      "pn": "3733",
      "abstract": [
        "There is a critical need for detection and monitoring of Post-Traumatic Stress Disorder (PTSD) in both military and civilian populations. Current diagnosis is based on clinical interviews, but clinicians cannot keep up with the growing need. We examined the feasibility of using speech for assessment in a military population. We analyzed recordings of the Clinician-Administered PTSD Scale (CAPS) interview from military personnel diagnosed as PTSD positive versus negative. Three feature types were explored: frame-level spectral features, longer-range prosodic features, and lexical features. Results using gaussian backend, decision tree and neural network classifiers (for spectral and prosodic features) and boosting (for lexical features) showed an accuracy of 77% correct in split-half cross validation experiments, a figure significantly above chance (which was 61.5% for our dataset). Spectral and prosodic features outperformed lexical features, and feature combination yielded further gains. An important finding was that sparser prosodic features offered more robustness than acoustic features to channel-based variation in the interview recordings. Implications and future work are discussed.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-740",
      "author_area_id": "3",
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "yu15b_interspeech": {
      "authors": [
        [
          "Bea",
          "Yu"
        ],
        [
          "Thomas F.",
          "Quatieri"
        ],
        [
          "James R.",
          "Williamson"
        ],
        [
          "James C.",
          "Mundt"
        ]
      ],
      "title": "Cognitive impairment prediction in the elderly based on vocal biomarkers",
      "original": "i15_3734",
      "page_count": 5,
      "order": 791,
      "p1": "3734",
      "pn": "3738",
      "abstract": [
        "Remote, automated cognitive impairment (CI) diagnosis has the potential to facilitate care for the elderly. Speech is easily collected over the phone and already some common cognitive tests are administered remotely, resulting in regular audio data collections. Speech-based CI diagnosis leveraging existing audio is therefore an attractive approach for remote elderly cognitive health monitoring. In this paper, we demonstrate the predictive power of several speech features derived from remotely collected audio used for common clinical cognitive testing. Specifically, using phoneme-based measures, pseudo-syllable rate, pitch variance, and articulatory coordination derived from formant cross-correlation measures, we investigate the capability of speech features, estimated from paragraph-recall and animal fluency test speech, to predict clinical CI assessment. Using a database consisting of audio from elderly subjects collected over a 4 year period, we develop support vector machine classification models of the CI clinical assessments. The best performing models result in an average equal error rate (EER) of 13.5%.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-741",
      "author_area_id": "3",
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    },
    "gomezgarcia15_interspeech": {
      "authors": [
        [
          "J. -A.",
          "G\u00f3mez-Garc\u00eda"
        ],
        [
          "L.",
          "Moro-Vel\u00e1zquez"
        ],
        [
          "Juan Ignacio",
          "Godino-Llorente"
        ],
        [
          "G.",
          "Castellanos-Dom\u00ednguez"
        ]
      ],
      "title": "Automatic age detection in normal and pathological voice",
      "original": "i15_3739",
      "page_count": 5,
      "order": 792,
      "p1": "3739",
      "pn": "3743",
      "abstract": [
        "Systems that automatically detect voice pathologies are usually trained with recordings belonging to population of all ages. However such an approach might be inadequate because of the acoustic variations in the voice caused by the natural aging process. In top of that, elder voices present some perturbations in quality similar to those related to voice disorders, which make the detection of pathologies more troublesome. With this in mind, the study of methodologies which automatically incorporate information about speakers' age, aiming at a simplification in the detection of voice disorders is of interest. In this respect, the present paper introduces an age detector trained with normal and pathological voice, constituting a first step towards the study of age-dependent pathology detectors. The proposed system employs sustained vowels of the Saarbr\u00fccken database from which two age groups are examined: adults and elders. Mel frequency cepstral coefficients for characterization, and Gaussian mixture models for classification are utilized. In addition, fusion of vowels at score level is considered to improve detection performance. Results suggest that age might be effectively recognized using normal and pathological voices when using sustained vowels as acoustical material, opening up possibilities for the design of automatic age-dependent voice pathology detection systems.\n",
        ""
      ],
      "doi": "10.21437/Interspeech.2015-742",
      "author_area_id": "3",
      "author_area_label": "Analysis of Paralinguistics in Speech and Language"
    }
  },
  "sessions": [
    {
      "title": "Keynotes",
      "papers": [
        "beckman15_interspeech",
        "sarikaya15_interspeech",
        "amunts15_interspeech",
        "scherer15_interspeech"
      ]
    },
    {
      "title": "Feature Extraction and Modeling with Neural Networks",
      "papers": [
        "sainath15_interspeech",
        "bhargava15_interspeech",
        "palaz15_interspeech",
        "ogawa15_interspeech",
        "lin15_interspeech",
        "golik15_interspeech"
      ]
    },
    {
      "title": "Prosody 1-3",
      "papers": [
        "glavitsch15_interspeech",
        "windmann15_interspeech",
        "eriksson15_interspeech",
        "zahner15_interspeech",
        "mixdorff15_interspeech",
        "simonetti15_interspeech",
        "michalsky15_interspeech",
        "reichel15_interspeech",
        "sarma15_interspeech",
        "repp15_interspeech",
        "jabeen15_interspeech",
        "mady15_interspeech",
        "benus15_interspeech",
        "gendrot15_interspeech",
        "schauffler15_interspeech",
        "andreeva15_interspeech",
        "qiu15_interspeech",
        "oreilly15_interspeech",
        "sarmah15_interspeech",
        "wochner15_interspeech"
      ]
    },
    {
      "title": "Speech Intelligibility Enhancement",
      "papers": [
        "zorila15_interspeech",
        "koutsogiannaki15_interspeech",
        "jemaa15_interspeech",
        "schepker15_interspeech",
        "rottschafer15_interspeech",
        "jokinen15_interspeech"
      ]
    },
    {
      "title": "Detecting and Predicting Mental and Social Disorders",
      "papers": [
        "kumar15_interspeech",
        "orozcoarroyave15_interspeech",
        "villacanas15_interspeech",
        "vasquezcorrea15_interspeech",
        "cummins15_interspeech",
        "marchi15_interspeech"
      ]
    },
    {
      "title": "Spoken Language Understanding 1-3",
      "papers": [
        "liu15_interspeech",
        "tam15_interspeech",
        "vukotic15_interspeech",
        "ravuri15_interspeech",
        "lu15_interspeech",
        "morchid15_interspeech",
        "sheikh15_interspeech",
        "boutin15_interspeech",
        "yang15_interspeech",
        "chiu15_interspeech",
        "shen15_interspeech",
        "charlet15_interspeech",
        "ramanarayanan15_interspeech",
        "racca15_interspeech",
        "chen15_interspeech",
        "renkens15_interspeech",
        "bastianelli15_interspeech",
        "luan15_interspeech",
        "ferreira15_interspeech",
        "tafforeau15_interspeech",
        "yang15b_interspeech",
        "lopes15_interspeech",
        "crook15_interspeech",
        "solanki15_interspeech",
        "laskowski15_interspeech",
        "llimona15_interspeech",
        "callejas15_interspeech",
        "nakamura15_interspeech",
        "gravano15_interspeech",
        "chowdhury15_interspeech",
        "christodoulides15_interspeech",
        "hakkanitur15_interspeech",
        "despotovic15_interspeech",
        "svec15_interspeech",
        "chen15b_interspeech"
      ]
    },
    {
      "title": "Active Perception in Human and Machine Speech Communication (Special Session)",
      "papers": [
        "szekely15_interspeech",
        "cowan15_interspeech",
        "ma15_interspeech",
        "schymura15_interspeech",
        "may15_interspeech",
        "kayser15_interspeech",
        "gomez15_interspeech"
      ]
    },
    {
      "title": "Speaker Recognition and Diarization 1-3",
      "papers": [
        "chen15c_interspeech",
        "stafylakis15_interspeech",
        "knyazeva15_interspeech",
        "cumani15_interspeech",
        "panda15_interspeech",
        "doroshin15_interspeech",
        "novoselov15_interspeech",
        "vaquero15_interspeech",
        "kelly15_interspeech",
        "chen15d_interspeech",
        "george15_interspeech",
        "shiota15_interspeech",
        "hurmalainen15_interspeech",
        "alam15_interspeech",
        "lee15_interspeech",
        "he15_interspeech",
        "nautsch15_interspeech",
        "fredes15_interspeech",
        "shokouhi15_interspeech",
        "wang15_interspeech",
        "yella15_interspeech",
        "shapiro15_interspeech",
        "fedorova15_interspeech",
        "banse15_interspeech",
        "martin15_interspeech",
        "desplanques15_interspeech",
        "inoue15_interspeech",
        "delgado15_interspeech",
        "sell15_interspeech",
        "woubie15_interspeech",
        "madikeri15_interspeech"
      ]
    },
    {
      "title": "Speech Synthesis 1-3",
      "papers": [
        "raitio15_interspeech",
        "sitaram15_interspeech",
        "inai15_interspeech",
        "lu15b_interspeech",
        "ohtani15_interspeech",
        "xie15_interspeech",
        "mohammadi15_interspeech",
        "huber15_interspeech",
        "huang15_interspeech",
        "oshima15_interspeech",
        "toman15_interspeech",
        "wu15_interspeech",
        "wang15b_interspeech",
        "ninh15_interspeech",
        "merritt15_interspeech",
        "chen15e_interspeech",
        "watts15_interspeech",
        "betz15_interspeech",
        "szaszak15_interspeech",
        "lanchantin15_interspeech",
        "thu15_interspeech",
        "takaki15_interspeech",
        "yao15_interspeech",
        "kay15_interspeech",
        "eger15_interspeech",
        "ding15_interspeech",
        "tobing15_interspeech",
        "cornu15_interspeech",
        "sitaram15b_interspeech",
        "wester15_interspeech",
        "evrard15_interspeech",
        "ardaillon15_interspeech",
        "jauk15_interspeech",
        "aryal15_interspeech"
      ]
    },
    {
      "title": "Mining and Annotation of Spoken and Multimodal Resources",
      "papers": [
        "matousek15_interspeech",
        "schweitzer15_interspeech",
        "chen15f_interspeech",
        "wong15_interspeech",
        "arsikere15_interspeech",
        "can15_interspeech",
        "vapnarsky15_interspeech",
        "hasan15_interspeech",
        "kuang15_interspeech"
      ]
    },
    {
      "title": "Speech Production Data and Models",
      "papers": [
        "bao15_interspeech",
        "barbier15_interspeech",
        "sivaraman15_interspeech",
        "barbier15b_interspeech",
        "kaburagi15_interspeech",
        "skordilis15_interspeech"
      ]
    },
    {
      "title": "Deep Neural Networks in Language and Accent Recognition",
      "papers": [
        "fer15_interspeech",
        "mccree15_interspeech",
        "song15_interspeech",
        "lozanodiez15_interspeech",
        "hautamaki15_interspeech",
        "geng15_interspeech"
      ]
    },
    {
      "title": "Speech Transmission",
      "papers": [
        "asaei15_interspeech",
        "lenarczyk15_interspeech",
        "issing15_interspeech",
        "issing15b_interspeech",
        "hines15_interspeech",
        "gallardo15_interspeech"
      ]
    },
    {
      "title": "Language Modeling for Conversational Speech",
      "papers": [
        "levit15_interspeech",
        "kobayashi15_interspeech",
        "ma15b_interspeech",
        "masumura15_interspeech",
        "aleksic15_interspeech",
        "vasserman15_interspeech"
      ]
    },
    {
      "title": "Interspeech 2015 Computational Paralinguistics ChallengE (ComParE): Degree of Nativeness, Parkinson's &amp; Eating Condition (Special Session)",
      "papers": [
        "schuller15_interspeech",
        "honig15_interspeech",
        "montacie15_interspeech",
        "ribeiro15_interspeech",
        "black15_interspeech",
        "orozcoarroyave15b_interspeech",
        "sztaho15_interspeech",
        "zlotnik15_interspeech",
        "an15_interspeech",
        "hahm15_interspeech",
        "williamson15_interspeech",
        "batliner15_interspeech",
        "prasad15_interspeech",
        "wagner15_interspeech",
        "pir15_interspeech",
        "pellegrini15_interspeech",
        "milde15_interspeech",
        "kaya15_interspeech",
        "kim15_interspeech",
        "grosz15_interspeech",
        "steidl15_interspeech",
        "batliner15b_interspeech"
      ]
    },
    {
      "title": "Pronunciation, Prosody and Audiovisual Features and Models",
      "papers": [
        "houghton15_interspeech",
        "zeng15_interspeech",
        "chen15g_interspeech",
        "davel15_interspeech",
        "zheng15_interspeech",
        "marcheret15_interspeech",
        "kalantari15_interspeech",
        "kalantari15b_interspeech",
        "ninomiya15_interspeech",
        "kakouros15_interspeech",
        "cernak15_interspeech",
        "ning15_interspeech"
      ]
    },
    {
      "title": "Speech Analysis and Representation 1-3",
      "papers": [
        "bai15_interspeech",
        "uchida15_interspeech",
        "pappagari15_interspeech",
        "loweimi15_interspeech",
        "maia15_interspeech",
        "liberatore15_interspeech",
        "csapo15_interspeech",
        "preu15_interspeech",
        "abari15_interspeech",
        "hsu15_interspeech",
        "laporte15_interspeech",
        "websdale15_interspeech",
        "mertens15_interspeech",
        "godoy15_interspeech",
        "houghton15b_interspeech",
        "meenakshi15_interspeech",
        "tsai15_interspeech",
        "zohrer15_interspeech",
        "gowda15_interspeech",
        "drugman15_interspeech",
        "lu15c_interspeech",
        "anushiyarachel15_interspeech",
        "montano15_interspeech",
        "hyafil15_interspeech"
      ]
    },
    {
      "title": "Speech Recognition &#8212; Technologies and Systems for New Applications",
      "papers": [
        "lee15b_interspeech",
        "rasipuram15_interspeech",
        "ma15c_interspeech",
        "lunsford15_interspeech",
        "fontan15_interspeech",
        "chakravarthula15_interspeech",
        "liu15b_interspeech",
        "kamper15_interspeech",
        "tilk15_interspeech",
        "ylmaz15_interspeech",
        "gao15_interspeech",
        "joshi15_interspeech",
        "kumar15b_interspeech",
        "liu15c_interspeech",
        "sharma15_interspeech"
      ]
    },
    {
      "title": "Show and Tell Session 1-4 (Special Session)",
      "papers": [
        "niculescu15_interspeech",
        "yeo15_interspeech",
        "dharo15_interspeech",
        "salimbajevs15_interspeech",
        "gaka15_interspeech",
        "abdelali15_interspeech",
        "stadtschnitzer15_interspeech",
        "bell15_interspeech",
        "znotins15_interspeech",
        "assayag15_interspeech",
        "zioko15_interspeech",
        "faria15_interspeech",
        "masudakatsuse15_interspeech",
        "kaufhold15_interspeech",
        "azarov15_interspeech",
        "duplessis15_interspeech",
        "lange15_interspeech",
        "berkling15_interspeech",
        "wong15b_interspeech",
        "rothlisberger15_interspeech",
        "pfab15_interspeech",
        "patil15_interspeech",
        "anguera15_interspeech",
        "besacier15_interspeech",
        "lee15c_interspeech",
        "arai15_interspeech",
        "budnik15_interspeech",
        "kisler15_interspeech",
        "winkelmann15_interspeech",
        "wankerl15_interspeech",
        "kumar15c_interspeech",
        "metze15_interspeech",
        "rennies15_interspeech",
        "moller15_interspeech",
        "sityaev15_interspeech"
      ]
    },
    {
      "title": "Distant and Reverberant Speech Recognition",
      "papers": [
        "itakura15_interspeech",
        "himawan15_interspeech",
        "dekkers15_interspeech",
        "kim15b_interspeech",
        "ravanelli15_interspeech",
        "miao15_interspeech"
      ]
    },
    {
      "title": "L2 Speech Perception and Production",
      "papers": [
        "levy15_interspeech",
        "tong15_interspeech",
        "jugler15_interspeech",
        "bao15b_interspeech",
        "saha15_interspeech",
        "naganomadsen15_interspeech"
      ]
    },
    {
      "title": "Information and Metadata Extraction from Speech",
      "papers": [
        "sar15_interspeech",
        "mendels15_interspeech",
        "domoto15_interspeech",
        "zhang15_interspeech",
        "bertero15_interspeech",
        "hough15_interspeech"
      ]
    },
    {
      "title": "Deep Neural Networks for Speech Synthesis",
      "papers": [
        "hu15_interspeech",
        "achanta15_interspeech",
        "fan15_interspeech",
        "valentinibotinhao15_interspeech",
        "song15b_interspeech",
        "wu15b_interspeech"
      ]
    },
    {
      "title": "Speaker and Language Recognition",
      "papers": [
        "wang15c_interspeech",
        "irtza15_interspeech",
        "cumani15b_interspeech",
        "lykartsis15_interspeech",
        "saeidi15_interspeech",
        "rahman15_interspeech",
        "xu15_interspeech",
        "cai15_interspeech",
        "aronowitz15_interspeech",
        "hong15_interspeech",
        "jelil15_interspeech",
        "gallardo15b_interspeech",
        "yamamoto15_interspeech",
        "ribas15_interspeech"
      ]
    },
    {
      "title": "Neural Networks and Speaker Adaptation",
      "papers": [
        "huang15b_interspeech",
        "huang15c_interspeech",
        "li15_interspeech",
        "kumar15d_interspeech",
        "karthickb15_interspeech",
        "miao15b_interspeech"
      ]
    },
    {
      "title": "Brain- and Other Biosignal-based Spoken Communication",
      "papers": [
        "parisotto15_interspeech",
        "werff15_interspeech",
        "francoisnienaber15_interspeech",
        "yang15c_interspeech",
        "brumberg15_interspeech",
        "heger15_interspeech"
      ]
    },
    {
      "title": "Deep Neural Networks in Speaker Recognition",
      "papers": [
        "chen15h_interspeech",
        "garciaromero15_interspeech",
        "richardson15_interspeech",
        "tian15_interspeech",
        "xing15_interspeech",
        "zheng15b_interspeech"
      ]
    },
    {
      "title": "Statistical Parametric Speech Synthesis",
      "papers": [
        "yoshimura15_interspeech",
        "pouget15_interspeech",
        "takamichi15_interspeech",
        "black15b_interspeech",
        "hong15b_interspeech",
        "tsiaras15_interspeech"
      ]
    },
    {
      "title": "Speech Science in End-user Applications (Special Session)",
      "papers": [
        "looze15_interspeech",
        "lee15d_interspeech",
        "zainko15_interspeech",
        "mandal15_interspeech",
        "feher15_interspeech",
        "erro15_interspeech"
      ]
    },
    {
      "title": "Speech Recognition: Evaluation and Low-resource Languages",
      "papers": [
        "sahraeian15_interspeech",
        "golik15b_interspeech",
        "wang15d_interspeech",
        "juan15_interspeech",
        "korenevsky15_interspeech",
        "beck15_interspeech",
        "itoh15_interspeech",
        "jannet15_interspeech"
      ]
    },
    {
      "title": "Emotion 1, 2",
      "papers": [
        "mixdorff15b_interspeech",
        "khaki15_interspeech",
        "lee15e_interspeech",
        "mori15_interspeech",
        "mehrabani15_interspeech",
        "albin15_interspeech",
        "kadiri15_interspeech",
        "huang15d_interspeech",
        "gu15_interspeech",
        "gosztolya15_interspeech",
        "chong15_interspeech",
        "palogiannidi15_interspeech",
        "xu15b_interspeech",
        "lee15f_interspeech",
        "kim15c_interspeech",
        "vlasenko15_interspeech"
      ]
    },
    {
      "title": "Language Modeling for Speech Recognition",
      "papers": [
        "arisoy15_interspeech",
        "hall15_interspeech",
        "marin15_interspeech",
        "shazeer15_interspeech",
        "pelemans15_interspeech",
        "chelba15_interspeech",
        "botros15_interspeech",
        "bayer15_interspeech",
        "sun15_interspeech",
        "chong15b_interspeech"
      ]
    },
    {
      "title": "Fast Efficient and Scalable Computing for Neural Nets",
      "papers": [
        "paulik15_interspeech",
        "sak15_interspeech",
        "nakkiran15_interspeech",
        "sainath15b_interspeech",
        "berg15_interspeech",
        "strom15_interspeech"
      ]
    },
    {
      "title": "Source Separation and Computational Auditory Scene Analysis",
      "papers": [
        "kalkur15_interspeech",
        "zhang15b_interspeech",
        "nie15_interspeech",
        "xu15c_interspeech",
        "kwon15_interspeech",
        "khan15_interspeech"
      ]
    },
    {
      "title": "Computational Models of Human Speech Perception",
      "papers": [
        "hoge15_interspeech",
        "yamamoto15b_interspeech",
        "rasilo15_interspeech",
        "lin15b_interspeech",
        "cao15_interspeech",
        "bosch15_interspeech"
      ]
    },
    {
      "title": "Prosody Modeling for Speech Synthesis",
      "papers": [
        "chen15i_interspeech",
        "ribeiro15b_interspeech",
        "moungsri15_interspeech",
        "langarani15_interspeech",
        "gerazov15_interspeech",
        "fernandez15_interspeech"
      ]
    },
    {
      "title": "Speech and Language Processing of Children's Speech (Special Session)",
      "papers": [
        "liao15_interspeech",
        "bone15_interspeech",
        "fringi15_interspeech",
        "pucher15_interspeech",
        "shahnawazuddin15_interspeech",
        "giuliani15_interspeech",
        "govender15_interspeech",
        "kim15d_interspeech",
        "sadeghian15_interspeech",
        "proenca15_interspeech",
        "sundar15_interspeech",
        "guo15_interspeech"
      ]
    },
    {
      "title": "Syllables and Segments 1, 2",
      "papers": [
        "leemann15_interspeech",
        "fuchs15_interspeech",
        "li15b_interspeech",
        "leykum15_interspeech",
        "gibson15_interspeech",
        "arai15b_interspeech",
        "dutta15_interspeech",
        "kraus15_interspeech",
        "kolly15_interspeech",
        "jani15_interspeech",
        "zimmerer15_interspeech",
        "bruni15_interspeech",
        "prathosh15_interspeech",
        "barlaz15_interspeech",
        "michon15_interspeech",
        "howson15_interspeech",
        "yuan15_interspeech",
        "pouplier15_interspeech"
      ]
    },
    {
      "title": "Speech Enhancement",
      "papers": [
        "wong15c_interspeech",
        "vaz15_interspeech",
        "elshamy15_interspeech",
        "zhang15c_interspeech",
        "nrholm15_interspeech",
        "nrholm15b_interspeech",
        "kinoshita15_interspeech",
        "masaya15_interspeech",
        "kang15_interspeech",
        "lee15g_interspeech",
        "he15b_interspeech",
        "chinaev15_interspeech",
        "hao15_interspeech",
        "mayer15_interspeech"
      ]
    },
    {
      "title": "Phonetic Recognition: Novel Approaches and Understanding",
      "papers": [
        "zhao15_interspeech",
        "weber15_interspeech",
        "ganapathy15_interspeech",
        "travadi15_interspeech",
        "pesan15_interspeech",
        "nagamine15_interspeech"
      ]
    },
    {
      "title": "Varieties of Speech",
      "papers": [
        "loukina15_interspeech",
        "zimmerer15b_interspeech",
        "bonneau15_interspeech",
        "best15_interspeech",
        "bundgaardnielsen15_interspeech",
        "hirst15_interspeech"
      ]
    },
    {
      "title": "Conversational Interaction",
      "papers": [
        "gibson15b_interspeech",
        "malandrakis15_interspeech",
        "xia15_interspeech",
        "gupta15_interspeech",
        "bonin15_interspeech",
        "schrank15_interspeech"
      ]
    },
    {
      "title": "Speech and Audio Segmentation and Classification; Voice Activity Detection 1-3",
      "papers": [
        "ringeval15_interspeech",
        "nandwana15_interspeech",
        "dumpala15_interspeech",
        "gergen15_interspeech",
        "dennis15_interspeech",
        "choi15_interspeech",
        "hwang15_interspeech",
        "wang15e_interspeech",
        "zhan15_interspeech",
        "ghaemmaghami15_interspeech",
        "sriskandaraja15_interspeech",
        "tao15_interspeech",
        "raboshchuk15_interspeech",
        "dai15_interspeech",
        "liu15d_interspeech",
        "liu15e_interspeech",
        "espi15_interspeech",
        "transfeld15_interspeech",
        "bouchekif15_interspeech",
        "kraljevski15_interspeech",
        "lehner15_interspeech"
      ]
    },
    {
      "title": "Spoken Dialogue Systems",
      "papers": [
        "su15_interspeech",
        "griol15_interspeech",
        "griol15b_interspeech",
        "khan15b_interspeech",
        "wu15c_interspeech",
        "zukerman15_interspeech"
      ]
    },
    {
      "title": "Automatic Speaker Verification Spoofing and Countermeasures (ASVspoof 2015) (Special Session)",
      "papers": [
        "wu15d_interspeech",
        "wu15e_interspeech",
        "sanchez15_interspeech",
        "wester15b_interspeech",
        "xiao15_interspeech",
        "hanilci15_interspeech",
        "patel15_interspeech",
        "villalba15_interspeech",
        "alam15b_interspeech",
        "janicki15_interspeech",
        "liu15f_interspeech",
        "sahidullah15_interspeech",
        "wang15f_interspeech",
        "chen15j_interspeech",
        "yamagishi15_interspeech"
      ]
    },
    {
      "title": "Acoustic Modeling and Decoding Methods for Speech Recognition",
      "papers": [
        "lee15h_interspeech",
        "oualil15_interspeech",
        "allauzen15_interspeech",
        "xu15d_interspeech",
        "lecouteux15_interspeech",
        "gysel15_interspeech",
        "xu15e_interspeech",
        "ratajczak15_interspeech",
        "jalalvand15_interspeech"
      ]
    },
    {
      "title": "Speech Production Measurements and Analyses",
      "papers": [
        "parida15_interspeech",
        "canevari15_interspeech",
        "csapo15b_interspeech",
        "bandini15_interspeech",
        "chi15_interspeech",
        "drioli15_interspeech",
        "kolb15_interspeech",
        "andrademiranda15_interspeech",
        "kochetov15_interspeech",
        "zhu15_interspeech"
      ]
    },
    {
      "title": "Spoken Translation &amp; Speech-to-speech",
      "papers": [
        "ruiz15_interspeech",
        "bechet15_interspeech",
        "ng15_interspeech",
        "pelemans15b_interspeech",
        "mieno15_interspeech",
        "junczysdowmunt15_interspeech"
      ]
    },
    {
      "title": "Advances in iVector-based Speaker Verification",
      "papers": [
        "kenny15_interspeech",
        "correia15_interspeech",
        "li15c_interspeech",
        "rahman15b_interspeech",
        "glembek15_interspeech",
        "kanagasundaram15_interspeech"
      ]
    },
    {
      "title": "Voice Quality",
      "papers": [
        "gobl15_interspeech",
        "airaksinen15_interspeech",
        "narendra15_interspeech",
        "bundgaardnielsen15b_interspeech",
        "kreiman15_interspeech",
        "jiao15_interspeech"
      ]
    },
    {
      "title": "Neural Networks for Language Modeling",
      "papers": [
        "morioka15_interspeech",
        "irie15_interspeech",
        "emami15_interspeech",
        "masumura15b_interspeech",
        "chunwijitra15_interspeech",
        "gangireddy15_interspeech"
      ]
    },
    {
      "title": "Biosignal-based Spoken Communication (Special Session)",
      "papers": [
        "janke15_interspeech",
        "anderson15_interspeech",
        "mcloughlin15_interspeech",
        "bocquelet15_interspeech",
        "fabre15_interspeech",
        "wang15g_interspeech",
        "diener15_interspeech",
        "mirbagheri15_interspeech",
        "janke15b_interspeech"
      ]
    },
    {
      "title": "Robust Speech Recognition: Features, Far-field and Reverberation",
      "papers": [
        "mirsamadi15_interspeech",
        "mimura15_interspeech",
        "peddinti15_interspeech",
        "kumar15e_interspeech",
        "mitra15_interspeech",
        "karafiat15_interspeech",
        "harvilla15_interspeech",
        "huang15e_interspeech",
        "wang15h_interspeech",
        "su15b_interspeech",
        "baby15_interspeech",
        "han15_interspeech"
      ]
    },
    {
      "title": "Social Signals, Assessment and Paralinguistics",
      "papers": [
        "xiao15b_interspeech",
        "ando15_interspeech",
        "nasir15_interspeech",
        "gosztolya15b_interspeech",
        "kaushik15_interspeech",
        "pappu15_interspeech",
        "azais15_interspeech",
        "sergienko15_interspeech",
        "hsiao15_interspeech",
        "tsai15b_interspeech",
        "akira15_interspeech"
      ]
    },
    {
      "title": "Bandwidth Extension, Quality and Intelligibility Measures",
      "papers": [
        "koster15_interspeech",
        "berger15_interspeech",
        "gaich15_interspeech",
        "marxer15_interspeech",
        "andersen15_interspeech",
        "tang15_interspeech",
        "chen15k_interspeech",
        "li15d_interspeech",
        "pulakka15_interspeech",
        "turan15_interspeech",
        "wang15i_interspeech",
        "liu15g_interspeech"
      ]
    },
    {
      "title": "Discriminative Acoustic Training Methods for ASR",
      "papers": [
        "dalen15_interspeech",
        "manohar15_interspeech",
        "zhang15d_interspeech",
        "he15c_interspeech",
        "chen15l_interspeech",
        "gelly15_interspeech"
      ]
    },
    {
      "title": "Topics in Paralinguistics",
      "papers": [
        "quatieri15_interspeech",
        "zhang15e_interspeech",
        "toth15_interspeech",
        "sidorov15_interspeech",
        "ferrer15_interspeech",
        "kaushik15b_interspeech"
      ]
    },
    {
      "title": "Spoken Language Processing",
      "papers": [
        "pasupat15_interspeech",
        "nguyen15_interspeech",
        "bokaei15_interspeech",
        "liu15h_interspeech",
        "mokaram15_interspeech",
        "sousa15_interspeech"
      ]
    },
    {
      "title": "Voice Conversion",
      "papers": [
        "huang15f_interspeech",
        "aihara15_interspeech",
        "kobayashi15b_interspeech",
        "tian15b_interspeech",
        "alonso15_interspeech",
        "tajiri15_interspeech"
      ]
    },
    {
      "title": "Advanced Crowdsourcing for Speech and Beyond (Special Session)",
      "papers": [
        "polzehl15_interspeech",
        "jyothi15_interspeech",
        "chowdhury15b_interspeech",
        "rothwell15_interspeech",
        "rothwell15b_interspeech",
        "polzehl15b_interspeech",
        "naderi15_interspeech",
        "leemann15b_interspeech",
        "loukina15b_interspeech",
        "kacorri15_interspeech",
        "burgos15_interspeech",
        "wray15_interspeech",
        "gaur15_interspeech",
        "byun15_interspeech"
      ]
    },
    {
      "title": "Robust Speech Recognition: Adaptation",
      "papers": [
        "wang15j_interspeech",
        "rath15_interspeech",
        "abe15_interspeech",
        "yu15_interspeech",
        "borsky15_interspeech",
        "lu15d_interspeech",
        "cardinal15_interspeech",
        "karanasou15_interspeech",
        "garimella15_interspeech",
        "tomashenko15_interspeech",
        "hsiao15b_interspeech",
        "li15e_interspeech",
        "doulaty15_interspeech"
      ]
    },
    {
      "title": "Speech and Hearing Disorders",
      "papers": [
        "lustyk15_interspeech",
        "alnasheri15_interspeech",
        "wu15f_interspeech",
        "kasisopa15_interspeech",
        "rong15_interspeech",
        "a15_interspeech",
        "bigi15_interspeech",
        "meyer15_interspeech",
        "knoll15_interspeech",
        "yeung15_interspeech"
      ]
    },
    {
      "title": "Dialogue and Discourse",
      "papers": [
        "wodarczak15_interspeech",
        "reichel15b_interspeech",
        "cervone15_interspeech",
        "rosenberg15_interspeech",
        "looze15b_interspeech",
        "freeman15_interspeech"
      ]
    },
    {
      "title": "L1/L2 Speech Perception and Acquisition",
      "papers": [
        "choi15b_interspeech",
        "grohe15_interspeech",
        "ordin15_interspeech",
        "scharenborg15_interspeech",
        "chen15m_interspeech",
        "ooigawa15_interspeech"
      ]
    },
    {
      "title": "LVCSR Systems and Applications",
      "papers": [
        "saon15_interspeech",
        "liu15i_interspeech",
        "thomas15_interspeech",
        "shaik15_interspeech",
        "fragasilva15_interspeech",
        "jyothi15b_interspeech"
      ]
    },
    {
      "title": "Zero Resource Speech Technologies: Unsupervised Discovery of Linguistic Units (Special Session)",
      "papers": [
        "versteegh15_interspeech",
        "badino15_interspeech",
        "thiolliere15_interspeech",
        "agenbag15_interspeech",
        "chen15n_interspeech",
        "baljekar15_interspeech",
        "renshaw15_interspeech",
        "rasanen15_interspeech",
        "lyzinski15_interspeech"
      ]
    },
    {
      "title": "Neural Networks: Novel Architectures for LVCSR",
      "papers": [
        "peddinti15b_interspeech",
        "li15f_interspeech",
        "zhang15f_interspeech",
        "zhang15g_interspeech",
        "sivadas15_interspeech",
        "su15c_interspeech",
        "cui15_interspeech",
        "lu15e_interspeech",
        "zhu15b_interspeech",
        "bi15_interspeech",
        "chan15_interspeech",
        "liu15j_interspeech",
        "chen15o_interspeech"
      ]
    },
    {
      "title": "Speech and Music Analysis",
      "papers": [
        "liu15k_interspeech",
        "p15_interspeech",
        "prasad15b_interspeech",
        "huang15g_interspeech",
        "salvati15_interspeech",
        "ma15d_interspeech",
        "nie15b_interspeech",
        "gong15_interspeech",
        "lee15i_interspeech",
        "yen15_interspeech",
        "lim15_interspeech"
      ]
    },
    {
      "title": "Speech and Cognition in Adverse Conditions",
      "papers": [
        "tiainen15_interspeech",
        "hecht15_interspeech",
        "andrei15_interspeech",
        "chen15p_interspeech",
        "ishida15_interspeech",
        "bhat15_interspeech"
      ]
    },
    {
      "title": "Audio Signal Analysis and Representation",
      "papers": [
        "oreilly15b_interspeech",
        "choi15c_interspeech",
        "ming15_interspeech",
        "xiao15c_interspeech",
        "pang15_interspeech",
        "phan15_interspeech"
      ]
    },
    {
      "title": "Robustness in Speaker Recognition",
      "papers": [
        "ferrer15b_interspeech",
        "ajili15_interspeech",
        "dean15_interspeech",
        "aronowitz15b_interspeech",
        "misra15_interspeech",
        "carne15_interspeech"
      ]
    },
    {
      "title": "Evaluation of Speech Synthesis",
      "papers": [
        "wester15c_interspeech",
        "chevelu15_interspeech",
        "latacz15_interspeech",
        "nose15_interspeech",
        "koriyama15_interspeech",
        "ullmann15_interspeech"
      ]
    },
    {
      "title": "Adaptive Methods for LVCSR",
      "papers": [
        "fohr15_interspeech",
        "chen15q_interspeech",
        "jin15_interspeech",
        "yeh15_interspeech",
        "li15g_interspeech",
        "das15_interspeech"
      ]
    },
    {
      "title": "Robust Speech Processing Using Observation Uncertainty and Uncertainty Propagation (Special Session)",
      "papers": [
        "astudillo15_interspeech",
        "ribas15b_interspeech",
        "tachioka15_interspeech",
        "saeidi15b_interspeech",
        "mallidi15_interspeech",
        "huemmer15_interspeech",
        "abdelaziz15_interspeech",
        "kuhne15_interspeech",
        "narayanan15_interspeech",
        "astudillo15b_interspeech"
      ]
    },
    {
      "title": "Acoustic Model Adaptation and Training",
      "papers": [
        "zhang15h_interspeech",
        "ko15_interspeech",
        "zhang15i_interspeech",
        "kurata15_interspeech",
        "chen15r_interspeech",
        "swietojanski15_interspeech",
        "bell15b_interspeech",
        "li15h_interspeech",
        "chen15s_interspeech",
        "huang15h_interspeech",
        "parthasarathi15_interspeech",
        "li15i_interspeech",
        "doulaty15b_interspeech",
        "asami15_interspeech"
      ]
    },
    {
      "title": "Spoken Term Detection, Spoken MT &amp; Transliteration",
      "papers": [
        "cho15_interspeech",
        "lau15_interspeech",
        "wang15k_interspeech",
        "do15_interspeech",
        "ngo15_interspeech",
        "oouchi15_interspeech",
        "saxena15_interspeech",
        "lee15j_interspeech",
        "ram15_interspeech"
      ]
    },
    {
      "title": "Stress, Load, and Pathologies",
      "papers": [
        "nwe15_interspeech",
        "trouvain15_interspeech",
        "truong15_interspeech",
        "paul15_interspeech",
        "tsiartas15_interspeech",
        "pietrowicz15_interspeech",
        "daoudi15_interspeech",
        "vergyri15_interspeech",
        "yu15b_interspeech",
        "gomezgarcia15_interspeech"
      ]
    }
  ],
  "doi": "10.21437/Interspeech.2015"
}