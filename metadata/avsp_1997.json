{
 "location": "Rhodes, Greece",
 "startDate": "26/9/1997",
 "endDate": "27/9/1997",
 "conf": "AVSP",
 "year": "1997",
 "name": "avsp_1997",
 "series": "AVSP",
 "SIG": "AVISA",
 "title": "Auditory-Visual Speech Processing",
 "title1": "Auditory-Visual Speech Processing",
 "date": "26-27 September 1997",
 "papers": {
  "campbell97_avsp": {
   "authors": [
    [
     "Ruth",
     "Campbell"
    ],
    [
     "P. J.",
     "Benson"
    ],
    [
     "S. B.",
     "Wallace"
    ]
   ],
   "title": "The perception of mouthshape: photographic images of natural speech sounds can be perceived categorically",
   "original": "av97_001",
   "page_count": 4,
   "order": 1,
   "p1": "1",
   "pn": "4",
   "abstract": [
    "How are images of mouth shapes perceptually organised? We manipulated a 'vowel triangle' of photographic still images of the point English vowels ('oo, cee' and 'ah') and a similar triangle of consonants ('th,ff,mm'). Using a face-feature landmark algorithm, computer warping techniques controlled the physical step-size between images. Traditional two-stage categorical perception (CP) experiments (ABX discrimination and identification) with normal hearing subjects suggested that while the vowel series showed CP, consonant images did not. These findings are discussed in relation to recent findings of categoricity in visual image perception and implications for visual language processing.\n",
    ""
   ]
  },
  "magnocaldognetto97_avsp": {
   "authors": [
    [
     "Emanuela",
     "Magno Caldognetto"
    ],
    [
     "C.",
     "Zmarich"
    ],
    [
     "Piero",
     "Cosi"
    ],
    [
     "Franco",
     "Ferrero"
    ]
   ],
   "title": "Italian consonantal visemes: relationships between spatial/ temporal articulatory characteristics and coproduced acoustic signal",
   "original": "av97_005",
   "page_count": 4,
   "order": 2,
   "p1": "5",
   "pn": "8",
   "abstract": [
    "In order to identify the Italian consonantal visemes, to verify the results of perceptive tests and elaborate rules for bimodal synthesis and recognition, the 3D (lip height, lip width, lower lip protrusion) lip target shapes for all the 21 Italian consonants were determined. Moreover, the spatio-temporal characteristics of the closure/opening movements for the realisation of these consonantal targets were studied relative to the lip height (LH) parameter together with the temporal relationships between the characteristics of this articulately movement and the co-produced acoustic signal.\n",
    ""
   ]
  },
  "hiki97_avsp": {
   "authors": [
    [
     "Shizuo",
     "Hiki"
    ],
    [
     "Yumiko",
     "Fukuda"
    ]
   ],
   "title": "Negative effect of homophones on speechreading in Japanese",
   "original": "av97_009",
   "page_count": 4,
   "order": 3,
   "p1": "9",
   "pn": "12",
   "abstract": [
    "The speech information conveyed by the shape of the mouth differs according to the characteristic phonetic system of each language. Besides the phonetic characteristic, a peculiar feature found in Japanese speech is that there are many homophenouswords (words having different sound but the same mouth shape), because the Japanese vocabulary comprises a large number of homophones (words having different sound but the same mouth shape), compared with other languages. This results in a decrease of information obtainable through speechreading. In order to ascertain this feature quantitatively, firstly, the mouth shapes of vowels and consonants of Japanese speech are described symbolically. Detailed symbols are assigned to as many kinds of mouth shapes as are readable in a clear utterance. Using these symbols, the changes in mouth shapes in Japanese speech are described based on coarticulation rules and a combination of preceding and following vowels. Then, the ratios of homophenous words and homophones found in a basic vocabulary are estimated by referring to statistical data of the syllabic structure of Japanese words. By assembling these ratios, the nature of the information conveyed through speechreading is analyzed, and the negative effect of homophones on the speechreading of Japanese is discussed.\n",
    ""
   ]
  },
  "leybaert97_avsp": {
   "authors": [
    [
     "Jacqueline",
     "Leybaert"
    ],
    [
     "Daniela",
     "Marchetti"
    ]
   ],
   "title": "Visual rhyming effects in deaf children",
   "original": "av97_013",
   "page_count": 4,
   "order": 4,
   "p1": "13",
   "pn": "16",
   "abstract": [
    "An experiment was conducted on deaf children's ordered immediate recall of words presented in Cued-Speech (CS, i.e. speechreading+manual cues). The children recalled significantly fewer rhyming words than non rhyming words. They also recalled fewer words similar in speechreading and fewer words similar in CS than control words. These results are consistent with the notion that rhyming is not dependent upon a phonological code derived from audition (Dodd, 1987). They also indicate that deaf children use a code based on visual speech for short-term retention of CS stimuli. The relative weight of speechread and CS information in this visual code seems to vary with degree of intensity of exposure to CS.\n",
    ""
   ]
  },
  "poggi97_avsp": {
   "authors": [
    [
     "Isabella",
     "Poggi"
    ],
    [
     "Catherine",
     "Pelachaud"
    ]
   ],
   "title": "Context sensitive faces",
   "original": "av97_017",
   "page_count": 4,
   "order": 5,
   "p1": "17",
   "pn": "20",
   "abstract": [
    "The aim of this work is to simulate the facial behaviors of a Speaker while communicating to different Listeners in various situations. Our system takes as input the Speaker's goal of performing a Communicative Act, that is of communicating something to a Hearer, and outputs the appropriate performative of the communicative act and corresponding facial expressions.\n",
    ""
   ]
  },
  "auerjr97_avsp": {
   "authors": [
    [
     "E. T.",
     "Auer Jr."
    ],
    [
     "L. E.",
     "Bernstein"
    ],
    [
     "R. S.",
     "Waldstein"
    ],
    [
     "P. E.",
     "Tucker"
    ]
   ],
   "title": "Effects of phonetic variation and the structure of the lexicon on the uniqueness of words",
   "original": "av97_021",
   "page_count": 4,
   "order": 6,
   "p1": "21",
   "pn": "24",
   "abstract": [
    "Relatively little is known about the optical phonetic speech characteristics to which speechreaders are attuned. However, it is known that phonetic context can affect visual confusability of phonemes. In Study 1, behavioral experiments were performed to examine in detail effects of context-sensitive phonetic variation on the visual confusability of consonants and vowels. In Study 2, computational experiments were performed to assess the importance of patterns of context-sensitive visual confusability on the uniqueness of words in the language. Results from Study 1 further support the conclusion that phonetic context influences phoneme confusability. The computational experiments in Study 2 provide evidence that the distribution of words in English substantially preserves lexical uniqueness even when phonetic variability is taken into account.\n",
    ""
   ]
  },
  "cerrato97_avsp": {
   "authors": [
    [
     "Loredana",
     "Cerrato"
    ],
    [
     "Federico",
     "Albano Leoni"
    ],
    [
     "Andrea",
     "Paoloni"
    ]
   ],
   "title": "A methodology to quantify the contribution of visual and prosodic information to the process of speech comprehension",
   "original": "av97_025",
   "page_count": 4,
   "order": 7,
   "p1": "25",
   "pn": "28",
   "abstract": [
    "We will report in this paper the results of a series of comprehension tests designed in order to investigate the contribution of visual and prosodic information to the process of comprehension of conversational speech. Particular attention will be paid to the methodology used. Our research follows the new interest in studying the integration of various sources of information in the process of speech perception and approaches the problem from the point of view of general comprehension.\n",
    ""
   ]
  },
  "gagne97_avsp": {
   "authors": [
    [
     "Jean-Pierre",
     "Gagné"
    ],
    [
     "Lina",
     "Boutin"
    ]
   ],
   "title": "The effects of speaking rate on visual speech intelligibility",
   "original": "av97_029",
   "page_count": 4,
   "order": 8,
   "p1": "29",
   "pn": "32",
   "abstract": [
    "The effect of speaking rate on visual-speech intelligibility of talkers was investigated. Seven talkers were videotaped while they spoke a list of 25 sentences twice using conversational and clear speech. For each talker, the average overall speaking rate of the conversational and clear sentences was measured. The recordings were digitized and dubbed under four conditions: (1) normal conversational, (2) conversational speech decelerated to a speed equivalent to the speaking rate of the talker's average utterances of clear speech, (3) natural clear speech, (4) clear speech accelerated to a rate equivalent to the talker's average conversational speech. The test items were completely randomized and shown to a group of subjects. The subjects responses were used to determine the speech intelligibility of each talker for the four experimental conditions. The results suggest that overall speaking rate alone does not account for differences in visual-speech intelligibility between conversational and clear speech. [Work supported by NSERC].\n",
    ""
   ]
  },
  "magnocaldognetto97b_avsp": {
   "authors": [
    [
     "Emanuela",
     "Magno Caldognetto"
    ],
    [
     "Isabella",
     "Poggi"
    ]
   ],
   "title": "Micro- and macro-bimodality",
   "original": "av97_033",
   "page_count": 4,
   "order": 9,
   "p1": "33",
   "pn": "36",
   "abstract": [
    "In audio-visual communication, acoustic and optical modalities are simultaneously at work and their respective signals are intertwined in complex ways. Two kinds of bimodality are distinguished: micro-bimodality and macro-bimodality, the relationship of these optical and acoustic signals with the optical signals performed by hand gestures, facial expression and body movements. Some differences and similarities are described between the two kinds of bimodality on the production and perception side, and a model of context is proposed to account for how acoustic and optical modalities are arranged in the planning and understanding of bimodal communication.\n",
    ""
   ]
  },
  "girin97_avsp": {
   "authors": [
    [
     "L.",
     "Girin"
    ],
    [
     "Jean-Luc",
     "Schwartz"
    ],
    [
     "G.",
     "Feng"
    ]
   ],
   "title": "Can the visual input make the audio signal \"pop out\" in noise ? a first study of the enhancement of noisy VCV acoustic sequences by audio-visual fusion",
   "original": "av97_037",
   "page_count": 4,
   "order": 10,
   "p1": "37",
   "pn": "40",
   "abstract": [
    "This paper deals with a noisy speech enhancement technique based on the fusion of auditory and visual information. We first relate this approach to experimental data suggesting the existence of an \"audiovisual scene analysis module\". Then we present the implementation in die context of vowel-consonant-vowel transitions corrupted with white noise (four vowels and six plosives). A first evaluation of the system in this context is presented, including informal listening tests, distance measures and gaussian classification scores. The results shows that a good enhancement of the vocalic parts of the signals is obtained while the consonantal parts are not yet improved by the procedure. We present a pist to deal with this problem.\n",
    ""
   ]
  },
  "yehia97_avsp": {
   "authors": [
    [
     "Hani",
     "Yehia"
    ],
    [
     "Philip",
     "Rubin"
    ],
    [
     "Eric",
     "Vatikiotis-Bateson"
    ]
   ],
   "title": "Quantitative association of orofacial and vocal-tract shapes",
   "original": "av97_041",
   "page_count": 4,
   "order": 11,
   "p1": "41",
   "pn": "44",
   "abstract": [
    "This paper examines the degrees of correlation among vocal tract and orofacial movement data and the speech acoustics. Multilinear techniques are applied to support the claims that orofacial motion during speech is largely a by-product of producing the speech acoustics and further that the spectral envelope of the speech acoustics is better estimated by the 3D motion of the face than the mid-sagittal motion of the anterior vocal tract (lips, tongue, and jaw).\n",
    ""
   ]
  },
  "lyxell97_avsp": {
   "authors": [
    [
     "Björn",
     "Lyxell"
    ],
    [
     "Ulf",
     "Andersson"
    ],
    [
     "Stig",
     "Arlinger"
    ],
    [
     "Henrik",
     "Harder"
    ],
    [
     "Jerker",
     "Rönnberg"
    ]
   ],
   "title": "Phonological representaion and speech understanding with cochlear implants in deafened adults",
   "original": "av97_045",
   "page_count": 4,
   "order": 12,
   "p1": "45",
   "pn": "48",
   "abstract": [
    "Cognitive performance in 15 deafened adult cochlear implant candidates were examined and related to level of speech understanding after 12 months experience with the implant. The implant group performed on par with normal hearing controls on most tasks with one exception: They performed significantly worse in cognitive tasks (i.e., rhyme-judgement and lexical decision-making tasks) where use of a phonological representation of sound is a key feature. Observations of the implanted individuals' level of speech understanding indicates a possibility to predict the level of speech understanding by means of a preoperative assessment of the quality of the phonological representation. The results are discussed with respect to (a) deterioration in phonological representation of sounds as a function absence of external auditory stimulation and (b) the possibility to pre-operatively predict post-operative level of speech reception performance.\n",
    ""
   ]
  },
  "andreobrecht97_avsp": {
   "authors": [
    [
     "Regine",
     "André-Obrecht"
    ],
    [
     "Bruno",
     "Jacob"
    ],
    [
     "Nathalie",
     "Parlangeau"
    ]
   ],
   "title": "Audio visual speech recognition and segmental master slave HMM",
   "original": "av97_049",
   "page_count": 4,
   "order": 13,
   "p1": "49",
   "pn": "52",
   "abstract": [
    "Our work deals with the classical problem of merging heterogenous and asynchronous parameters. It's well known that lips reading improves the speech recognition score, specially in noise condition ; so we study more precisely the modeling of acoustic and labial parameters to propose two Automatic Speech Recognition Systems: a Direct Identification is performed by using a classical HMM approach: no correlation between visual and acoustic parameters is assumed. two correlated models : a master HMM and a slave HMM, process respectively the labial observations and the acoustic To assess each approach, we use a segmental pre-processing and an acoustic robust elementary unit \"the pseudodiphone\". Our task is the recognition of spelled french letters, in clear and noisy ( cocktail party ) environments. Whatever the approach and condition, the introduction of labial features improves the performances, but the difference between the two models isn't enough sufficient to provide any priority.\n",
    ""
   ]
  },
  "cox97_avsp": {
   "authors": [
    [
     "Stephen",
     "Cox"
    ],
    [
     "Iain",
     "Matthews"
    ],
    [
     "Andrew",
     "Bangham"
    ]
   ],
   "title": "Combining noise compensation with visual information in speech recognition",
   "original": "av97_053",
   "page_count": 4,
   "order": 14,
   "p1": "53",
   "pn": "56",
   "abstract": [
    "The addition of visual information derived from the speaker's lip movements to a speech recogniser (speechreading) can significantly enhance the performance of the recogniser when it is operating under adverse signal-to-noise ratios. However, processing of video signals imposes a large computational demand on the system and there is little point in using speechreading techniques if similar performance gains can be obtained using techniques which operate on only the audio signal and which are less computationally expensive. In this paper, we show that combining visual information with an audio noise compensation technique (spectral subtraction) leads to a performance significantly higher than that obtained using speechreading only or noise compensation only. The optimum method for speech recognition in the presence of noise is to use speech models that are matched to the input speech, and we show that the addition of visual information also gives a performance gain when matched models are used. We also describe a method of \"late\" integration which uses a measure of confidence derived from information output by the audio recogniser to achieve a performance which is close to optimum.\n",
    ""
   ]
  },
  "krone97_avsp": {
   "authors": [
    [
     "G.",
     "Krone"
    ],
    [
     "B.",
     "Talk"
    ],
    [
     "A.",
     "Wichert"
    ],
    [
     "G.",
     "Palm"
    ]
   ],
   "title": "Neural architectures for sensor fusion in speech recognition",
   "original": "av97_057",
   "page_count": 4,
   "order": 15,
   "p1": "57",
   "pn": "60",
   "abstract": [
    "In this paper we report on a systematic comparison of different neural architectures for the fusion of acoustic and optic information in speechrecognition. Experiments were performed with MLPs for a noiseless and a noisy acoustic channel. Two different kinds of input representations are investigated, resulting from a low level preprocessing and a linear discriminant analysis. We have done crossvalidation experiments. Our results suggest that given the same complexity of the architecture early and late integration models perform equal at least for the noiseless case. Pronounced differences in performance arise if one compares the different input representations. The linear discriminant analysis leads to highly distinguishable features and therefore a better recognition performance. This is especially true in case of a joinet preprocessing of the acoustic and optic signal.\n",
    ""
   ]
  },
  "rogozan97_avsp": {
   "authors": [
    [
     "Alexandrina",
     "Rogozan"
    ],
    [
     "Paul",
     "Deléglise"
    ],
    [
     "Mamoun",
     "Alissali"
    ]
   ],
   "title": "Adaptive determination of audio and visual weights for automatic speech recognition",
   "original": "av97_061",
   "page_count": 4,
   "order": 16,
   "p1": "61",
   "pn": "64",
   "abstract": [
    "This paper deals with adaptive integration of visual information in an automatic speech recognition system. Our method consists of attaching a different weight to each modality involved in the recognition process. These acoustic and visual weights are adjusted dynamically, manly according to the SNR, which is provided to the system as a contextual input. This method is tested on three different audio-visual CHMMs-based systems. They implement respectively: the direct identification scheme (DI), the separate identification scheme (SI) and the hybrid (DI+SI) one. System performances are compared on the same task: speaker-dependent continuous spelling of French letters. Results obtained using audio and visual weights dynamically adapting to the circumstances are better than those obtained with equal weights, over different test condition (clean data and data with artificial noise).\n",
    ""
   ]
  },
  "potamianos97_avsp": {
   "authors": [
    [
     "Gerasimos",
     "Potamianos"
    ],
    [
     "Eric",
     "Cosatto"
    ],
    [
     "Hans Peter",
     "Graf"
    ],
    [
     "David B.",
     "Roe"
    ]
   ],
   "title": "Speaker independent audio-visual database for bimodal ASR",
   "original": "av97_065",
   "page_count": 4,
   "order": 17,
   "p1": "65",
   "pn": "68",
   "abstract": [
    "This paper describes the audio-visual database collected at AT&T Labs-Research for the study of bimodal speech recognition. To date, this database consists of two multiple speaker parts, namely isolated confusable words and connected letters, thus allowing the study of some popular and relatively simple speaker independent audio-visual recognition tasks. In addition, a single speaker connected digits database is collected to facilitate speedy development and testing of various algorithms. Intentionally, no lip markings are used on the subjects during data collection. Development of robust and speaker independent algorithms for mouth location and lip contour extraction is thus necessary in order to obtain informative features about visual speech (visual front end). We describe our approach to this problem, and we report our automatic speech-reading and audio-visual speech recognition results on the single speaker connected digits task.\n",
    ""
   ]
  },
  "jourlin97_avsp": {
   "authors": [
    [
     "Pierre",
     "Jourlin"
    ]
   ],
   "title": "Word-dependent acoustic-labial weights in HMM-based speech recognition",
   "original": "av97_069",
   "page_count": 4,
   "order": 18,
   "p1": "69",
   "pn": "72",
   "abstract": [
    "This paper describes a novel approach for weighting the contribution of the acoustic and visual sources of information in a bimodal connected speech recognition system. We consider that a different acousticlabial weight is attached to each recognition unit. The values of the weighting vector are optimised in order to minimise error rate on a learning set. Experiments are performed on a two-speakers audio-visual database, composed of connected letters, with two different acoustic-labial speech recognition systems. For both speakers and both systems, the weights optimisation allows us to increase the recognition rate of our bimodal system.\n",
    ""
   ]
  },
  "remez97_avsp": {
   "authors": [
    [
     "Robert E.",
     "Remez"
    ],
    [
     "Jennifer M.",
     "Fellowes"
    ],
    [
     "David B.",
     "Pisoni"
    ],
    [
     "Winston D.",
     "Goh"
    ],
    [
     "Philip E.",
     "Rubin"
    ]
   ],
   "title": "Audio-visual speech perception without traditional speech cues: a second report",
   "original": "av97_073",
   "page_count": 4,
   "order": 19,
   "p1": "73",
   "pn": "76",
   "abstract": [
    "Theoretical and practical motives alike have prompted investigations of multimodal speech perception. Theoretically, such studies lead the explanation of perceptual organization beyond the familiar modalitybound accounts deriving from Gestalt psychology. Practically, existing perceptual accounts fail to explain the proficiency of multimodal speech perception using an electrocochlear prosthesis for hearing. Accordingly, our research sought improved measures of audiovisual integration of videotaped faces and selected acoustic constituents of speech signals with an acoustic signal that departs from the natural spectral properties of speech. A single sinewave tone accompanied a video image of an articulating face; the frequency and amplitude of the phonatory cycle or of one of the lower three oral formants supplied the pattern for a sinewave signal. Our results showed a distinct advantage for the condition pairing the video with a sinewave replicating the second formant, despite its unnatural timbre and its presentation in acoustic isolation from the balance of the speech signal.\n",
    ""
   ]
  },
  "gelder97_avsp": {
   "authors": [
    [
     "Beatrice de",
     "Gelder"
    ],
    [
     "Nancy",
     "Etcoff"
    ],
    [
     "Jean",
     "Vroomen"
    ]
   ],
   "title": "Impairment of visual speech integration in prosopagnosia",
   "original": "av97_077",
   "page_count": 4,
   "order": 20,
   "p1": "77",
   "pn": "80",
   "abstract": [
    "Our study of a prosopagnosic patient LH shows a strong association between severe face processing deficits and loss of speechreading skills. With simple dynamic stimuli some speechreading ability seems preserved but it is insufficient to affect the processing of auditory input and to generate audiovisual blends or to provoke cross-modal bias.\n",
    ""
   ]
  },
  "schwippert97_avsp": {
   "authors": [
    [
     "C.",
     "Schwippert"
    ],
    [
     "Christian",
     "Benoît"
    ]
   ],
   "title": "Audiovisual intelligibility of an androgynous speaker",
   "original": "av97_081",
   "page_count": 4,
   "order": 21,
   "p1": "81",
   "pn": "84",
   "abstract": [
    "This article reports an experimental study which aimed at evaluating the extent to which discrepancies in gender between a seen face and a heard voice affect the intelligibility of acoustically degraded speech presented audio-visually to naive subjects. Our results compare identification scores across two groups of subjects, depending on whether they had been first familiarized with one of the speakers' face and voice.\n",
    "Our results do not support the hypothesis that familiarity with a speaker would increase the benefit of lipreading when speaker identity is provided by the two modalities. More surprising are our results from the group of subjects who did not have a chance to see/hear the female speaker before the test. A majority of them identified her face as that of a male when presented visually. A majority of them also identified her voice as male when presented auditorily under highly degraded conditions. However, a majority identified her as female through bimodal presentation! This result challenges the models of bimodal integration based on synergetic reinforcement.\n",
    ""
   ]
  },
  "campbell97b_avsp": {
   "authors": [
    [
     "Ruth",
     "Campbell"
    ],
    [
     "A.",
     "Whittingham"
    ],
    [
     "U.",
     "Frith"
    ],
    [
     "Dominic W.",
     "Massaro"
    ],
    [
     "M. M.",
     "Cohen"
    ]
   ],
   "title": "Audiovisual speech perception in dyslexics: impaired unimodal perception but no audiovisual integration deficit",
   "original": "av97_085",
   "page_count": 4,
   "order": 22,
   "p1": "85",
   "pn": "88",
   "abstract": [
    "Dyslexic and control school students (mean age 13.5 years) were tested using a series of unimodal and bimodal speechtokens; ba,va,tha,da and ga in every combination and unimodally. They made significantly more errors than controls to unimodal auditory and unimodal visual tokens. Their integration of audiovisual speech was not significantly impaired, but it did show a different function than that of controls, reflecting the anomalous unimodal processing functions. These data fit recent suggestions that magnocellular function is impaired in dyslexia. They fit less easily both with auditory temporal processing deficit theories or with theories of defective phonemic processing . A general 'integration-deficit' theory (Paulesu etal, 1996) is not supported by these data.\n",
    ""
   ]
  },
  "bernstein97_avsp": {
   "authors": [
    [
     "Z. E.",
     "Bernstein"
    ],
    [
     "P.",
     "Iverson"
    ],
    [
     "E. T.",
     "Auer Jr."
    ]
   ],
   "title": "Elucidating the complex relationships between phonetic perception and word recognition in audiovisual speech perception",
   "original": "av97_089",
   "page_count": 4,
   "order": 23,
   "p1": "89",
   "pn": "92",
   "abstract": [
    "This paper reports studies on the relationship between form-based (phonetic) word similarity, particularly at the level typically employed for deriving visemes, and word identification. Three experiments were conducted to (1) obtain phoneme identifications, and then (2) investigate word homopheny, and (3) open-set word identification. Computational methods were employed to predict word similarity based on phoneme identifications. The results showed that the viseme level is inadequate to predict word homopheny. Results also showed that within conditions, the relative accuracy of word identification is related to the number of potentially ambiguous words in the lexicon.\n",
    ""
   ]
  },
  "burnham97_avsp": {
   "authors": [
    [
     "Denis",
     "Burnham"
    ],
    [
     "Sheila",
     "Keane"
    ]
   ],
   "title": "The Japanese Mcgurk effect: the role of linguistic and cultural factors an auditory-visual speech perception",
   "original": "av97_093",
   "page_count": 4,
   "order": 24,
   "p1": "93",
   "pn": "96",
   "abstract": [
    "Humans perceive auditory [b] dubbed onto visual [g] as [d] or [D], as in 'them'. When this is presented with an [a] vowel, \"th\" responses tend to dominate, while in an [i] vowel context, \"d\" responses dominate. This \"McGurk effect\" was used here to investigate humans' integration of auditory and visual speech information. In Experiment 1, Australian English and Japanese subjects viewed McGurk stimuli presented by an English speaker. Despite the phonological irrelevance of [D] in Japanese, both English and Japanese subjects showed the [a]/[i] x \"d\" / \"th\" crossover effect, suggesting a strong language-general (phonetic) influence in auditory-visual integration. Experiment 2 used a Japanese speaker. Here the incidence of \"th\" responses for Japanese subjects was severely dampened, showing that expectancies based on native phonology may overlay basic phonetic auditory-visual integration.\n",
    ""
   ]
  },
  "bertelson97_avsp": {
   "authors": [
    [
     "Paul",
     "Bertelson"
    ],
    [
     "Jean",
     "Vroomenti"
    ],
    [
     "Beatrice de",
     "Gelderti"
    ]
   ],
   "title": "Auditory-visual interaction in voice localization and in bimodal speech recognition: the effects of desynchronization",
   "original": "av97_097",
   "page_count": 4,
   "order": 25,
   "p1": "97",
   "pn": "100",
   "abstract": [
    "The effects of AV asynchrony on respectively the visual bias of auditory input localization and on the McGurk phenomenon were examined within a single experimental situation. On each trial, the face of a talker, articulating one of the two trisyllables /ama/ or /ana/, or staying still, was shown on a screen and his voice saying one of the two tokens was delivered on a hidden loudspeaker to the left of the right of the screen. The subject pointed to the apparent origin of voice, and repeated the heard utterance. With synchronous presentations or short lags of the auditory input, identification responses were influenced by the nature of the visual input (McGurk effect), and pointing responses were attracted toward the talker's face when it moved, compared with trials on which it did not (visual localization bias). Both effects tended to disappear with larger positive auditory lags or with negative ones. But the relation to lag depended on peculiarities of the presented token for localization, and not for identification.\n",
    ""
   ]
  },
  "sams97_avsp": {
   "authors": [
    [
     "M.",
     "Sams"
    ],
    [
     "V.",
     "Surakka"
    ],
    [
     "P.",
     "Helin"
    ],
    [
     "R.",
     "Kättö"
    ]
   ],
   "title": "Audiovisual fusion in finnish syllables and words",
   "original": "av97_101",
   "page_count": 3,
   "order": 26,
   "p1": "101",
   "pn": "104",
   "abstract": [
    "We studied the audiovisual fusion (acoustical /p/, visual /k/) in native Finnish speakers for Finnish 1) syllables, 2) words, and 3) words in sentences. 90%-97% of the identifications were biased by visual stimulation. The McGurk effect was very similar for the various stimulus types. The nature of the fusion was not dependent on the meaningfulness of the stimuli. Due to the fusion, acoustically proper words were perceived as non-words. The results demonstrate a strong McGurk effect for native Finnish speakers and support the idea that the audiovisual integration occurs at a relative early but language-specific stage.\n",
    ""
   ]
  },
  "ichikawa97_avsp": {
   "authors": [
    [
     "A.",
     "Ichikawa"
    ],
    [
     "Y.",
     "Okada"
    ],
    [
     "A.",
     "Imiya"
    ],
    [
     "K.",
     "Horiuchi"
    ]
   ],
   "title": "Analytical method for linguistic information of facial gestures in natural dialogue languages",
   "original": "av97_105",
   "page_count": 4,
   "order": 27,
   "p1": "105",
   "pn": "108",
   "abstract": [
    "A new dynamic method for the analysis of facial gestures used in natural dialogue languages in order to obtain linguistic information is proposed. Sign language is a typical natural dialogue language. In sign language, it is well known that facial gestures communicate very important linguistic information (Kimura, H. 1996, etc). We try to develop a methodology for the dynamic analysis of facial gestures used during sign language. The proposed method is based on the dynamic programming pattern matching method without any distinguishing marks on the subject's face. The experimental results show the feasibility of this method for the analysis of facial gestures which contain linguistic information. The computational load of the proposed image processing method is very light.\n",
    ""
   ]
  },
  "raducanu97_avsp": {
   "authors": [
    [
     "B.",
     "Raducanu"
    ],
    [
     "M.",
     "Grana"
    ]
   ],
   "title": "An approach to face localization based on signature analysis",
   "original": "av97_109",
   "page_count": 4,
   "order": 28,
   "p1": "109",
   "pn": "112",
   "abstract": [
    "Our work addresses the localization of faces in the setting of a relatively smooth background and under uncontrolled illumination conditions, and in relatively large indoor scenes. Our approach is based in some simple heuristics applied to the vertical and horizontal projections of the binarization of the gradient magnitudes extracted by a Sobel operator from the smoothed image. Experimental results on a sequence of images demonstrate the robustness of our approach. Our approach provides a fast and robust preprocessing that can be used by many systems that process facial information for sophisticated human-computer interaction, including speechreading systems.\n",
    ""
   ]
  },
  "meier97_avsp": {
   "authors": [
    [
     "Uwe",
     "Meier"
    ],
    [
     "Rainer",
     "Stiefelhagen"
    ],
    [
     "Me",
     "Yang"
    ]
   ],
   "title": "Preprocessing of visual speech under real world conditions",
   "original": "av97_113",
   "page_count": 4,
   "order": 29,
   "p1": "113",
   "pn": "116",
   "abstract": [
    "In this paper we present recent work on integration of visual information (automatic lip-reading) with acoustic speech for better overall speech recognition. We have developed a modular system for flexible human-computer interaction via speech. In order to give the speaker reasonable freedom of movement within a room, the speaker's face is automatically acquired and followed by a face tracker subsystem, which delivers constant size, centered images of the face in real time. The image of the lips is automatically extracted from the camera image of the speaker's face by the lip tracker module, which can track the lips in real time. Furthermore, we show how the system deals with problems in real environments such as different illuminations and image sizes, and how the system adapts automatically to different noise conditions.\n",
    ""
   ]
  },
  "reveret97_avsp": {
   "authors": [
    [
     "L.",
     "Revéret"
    ],
    [
     "F.",
     "Garcia"
    ],
    [
     "Christian",
     "Benoît"
    ],
    [
     "Eric",
     "Vatikiotis-Bateson"
    ]
   ],
   "title": "An hybrid approach to orientation-free liptracking",
   "original": "av97_117",
   "page_count": 4,
   "order": 30,
   "p1": "117",
   "pn": "120",
   "abstract": [
    "This paper examines the influence of head orientation in liptracking. There are two main conclusions: First, lip gesture analysis and head movement correction should be processed independently. Second, the measurement of articulatory parameters may be corrupted by head movement if it is performed directly at die pixel level. We thus propose an innovative technique of liptracking which relies on a \"3D active contour\" model of the lips controlled by articulatory parameters. The 3D model is projected onto the image of a speaking face through a camera model, thus allowing spatial re-orientation of the head. Liptracking is then performed by automatic adjustment of the control parameters, independently of head orientation. The final objective of our study is to apply a pixel-based method to detect head orientation. Nevertheless, we consider that head motion and lip gestures are detected by different processes, whether cognitive (by humans) or computational (by machines). Due to this, we decided to first develop and evaluate orientation-free liptracking through a non video-based head motion detection technique which is here presented.\n",
    ""
   ]
  },
  "basu97_avsp": {
   "authors": [
    [
     "Sumit",
     "Basu"
    ],
    [
     "Alex",
     "Pentland"
    ]
   ],
   "title": "Recovering 3d lip structure from 2d observations using a model trained from video",
   "original": "av97_121",
   "page_count": 4,
   "order": 31,
   "p1": "121",
   "pn": "124",
   "abstract": [
    "We present a method for recovering 3D lip structure from 2D video observations. We develop a physically-based 3D model of human lips and a framework for training it from real data. The model starts off with unconstrained degrees of freedom and learns a small subspace of permissible motions that explains over 99% of the variance in the observations. This resulting subspace allows estimation of the 3D lip shape from sparse or coarse observations. Results demonstrating the model's ability to reconstruct lip shapes from 2D data (marked points and raw video) are shown. The resulting model can be used for both analysis and synthesis.\n",
    ""
   ]
  },
  "vogt97_avsp": {
   "authors": [
    [
     "Michael",
     "Vogt"
    ]
   ],
   "title": "Interpreted multi-state lip models for audio-visual speech recognition",
   "original": "av97_125",
   "page_count": 4,
   "order": 32,
   "p1": "125",
   "pn": "128",
   "abstract": [
    "This paper presents the application of a specialized language for the task of lip modeling. Models presented here are switching their state during evaluation. State changes are based on a highly accurate neural network decision. Different model states include optional parts for the inner mouth region. Using the contour description and a neural coded color profile, encouraging performance figures have been achieved.\n",
    ""
   ]
  },
  "anderson97_avsp": {
   "authors": [
    [
     "Anne H.",
     "Anderson"
    ],
    [
     "Art",
     "Blokland"
    ]
   ],
   "title": "Intelligibility of speech mediated by low frame-rate video",
   "original": "av97_129",
   "page_count": 3,
   "order": 33,
   "p1": "129",
   "pn": "132",
   "abstract": [
    "We show that when speakers can see each other on a low-frame-rate video screen, they articulate more clearly than the case where they cannot see each other and are communicating only over an audio link. This contrasts with the case when speakers can see each other face to face, when their speech is less clear.\n",
    ""
   ]
  },
  "mcallister97_avsp": {
   "authors": [
    [
     "David F.",
     "McAllister"
    ],
    [
     "Robert D.",
     "Rodman"
    ],
    [
     "Donald L.",
     "Bitzer"
    ],
    [
     "Andrew S.",
     "Freeman"
    ]
   ],
   "title": "Lip synchronization of speech",
   "original": "av97_133",
   "page_count": 4,
   "order": 34,
   "p1": "133",
   "pn": "136",
   "abstract": [
    "Lip synchronization is the determination of the motion of the mouth and tongue during speech. It can be deduced from the speech signal without phonemic analysis, and irrespective of the content of the speech. Our method is based on the observation that the position of the mouth over a short interval of time can be correlated with the basic shape of the spectrum of the speech over that same interval. The spectrum is obtained from a Fast Fourier Transform (FFT) and treated like a discrete probability density function. Statistical measures called moments are used to describe the shape.\n",
    "For several canonical utterances, video measurements of a speaker's mouth are combined with the corresponding moments to produce continuous predictor surfaces for each of three mouth parameters: jaw position, horizontal opening between the lips and vertical opening between the lips. The method involves smoothing so it is independent of the local behavior of the spectrum.\n",
    ""
   ]
  },
  "yamamoto97_avsp": {
   "authors": [
    [
     "Eli",
     "Yamamoto"
    ],
    [
     "Satoshi",
     "Nakamura"
    ],
    [
     "Kiyohiro",
     "Shikano"
    ]
   ],
   "title": "Speech to lip movement synthesis by HMM",
   "original": "av97_137",
   "page_count": 4,
   "order": 35,
   "p1": "137",
   "pn": "140",
   "abstract": [
    "Synthesized lip movement images can compensate lack of auditory information for hearing impaired people, and also contribute to realize a human-like face of computer agents. We propose a novel method to synthesize lip movement based on mapping from an input speech using HMM. This paper compares the HMM method and a conventional method using VQ or ANN to convert speech-to-lip movement images. In the experiment, error and time difference error between synthesized lip movement images and original ones are utilized for evaluation. The result shows that the error of the HMM method is 8.6% smaller than that of the VQ method. Moreover, the HMM method reduces time difference error by 34.8% than the VQ's. The result also shows that the errors are mostly caused by phoneme /h/ and /Q/. Since those phonemes are dependent on succeeding phoneme, the context-dependent synthesis on the HMM method is applied to reduce the error. The context-dependent HMM method realizes that the error(difference error) is reduced by 11.3%(8.9%) compared with the original HMM method.\n",
    ""
   ]
  },
  "ezzat97_avsp": {
   "authors": [
    [
     "Tony",
     "Ezzat"
    ],
    [
     "Tomaso",
     "Poggio"
    ]
   ],
   "title": "Videorealistic talking faces: a morphing approach",
   "original": "av97_141",
   "page_count": 4,
   "order": 36,
   "p1": "141",
   "pn": "144",
   "abstract": [
    "We present a method for the construction of a video-realistic text-to-audiovisual speech synthesizer. A visual corpus of a subject enunciating a set of key words is initally recorded. The key words are chosen so that they collectively contain most of the American English viseme images, which are subsequently identified and extracted from the data by hand. Next, using optical flow methods borrowed from the computer vision literature, we compute realistic transitions between every viseme to every other viseme. The images along these transition paths are generated using a morphing method. Finally, we exploit phoneme and timing information extracted from a text-to-speech synthesizer to determine which viseme transitions to use, and the rate at which the morphing process should occur. In this manner, we are able to synchronize the visual speech stream with the audio speech stream, and hence give the impression of a videorealistic talking face.\n",
    ""
   ]
  },
  "goff97_avsp": {
   "authors": [
    [
     "Bertrand Le",
     "Goff"
    ],
    [
     "Christian",
     "Benoît"
    ]
   ],
   "title": "A French-speaking synthetic head",
   "original": "av97_145",
   "page_count": 4,
   "order": 37,
   "p1": "145",
   "pn": "148",
   "abstract": [
    "We have developed a visual speech synthesizer from unlimited French text, and synchronized it to an audio text-to-speech synthesizer also developed at the ICP (Le Goff & Benoit, 1996). The front-end of our synthesizer is a 3-D model of the face whose speech gestures are controlled by eight parameters: Five for the lips, one for the chin, two for the tongue. In contrast to most of the existing systems which are based on a limited set of prestored facial images, we have adopted the parametric approach to coarticulation first proposed by Cohen and Massaro (1993). We have thus implemented a coarticulation model based on spline-like functions, defined by three coefficients, applied to each target in a library of 16 French visemes. However, unlike Cohen & Massaro (1993), we have adopted a data-driven automatic approach to identify the many coefficients necessary to model coarticulation. To do so, we systematically analyzed an ad-hoc corpus uttered by a French male speaker. An intelligibility test has been run to quantify the benefit of seeing the synthetic face in addition to hearing the synthetic voice under several conditions of background noise (Le Goff, 1997). We here extended this evaluation to audiovisual material where the same corpus was acoustically uttered by a male speaker and synchronized to the synthetic head.\n",
    ""
   ]
  },
  "beskow97_avsp": {
   "authors": [
    [
     "Jonas",
     "Beskow"
    ]
   ],
   "title": "Animation of talking agents",
   "original": "av97_149",
   "page_count": 4,
   "order": 38,
   "p1": "149",
   "pn": "152",
   "abstract": [
    "It is envisioned that autonomous software agents that can communicate using speech and gesture will soon be on everybody's computer screen. This paper describes an architecture that can be used to design and animate characters capable of lip-synchronised synthetic speech as well as body gestures, for use in for example spoken dialogue systems. A general scheme for computationally efficient parametric deformation of facial surfaces is presented, as well as techniques for generation of bimodal speech, facial expressions and body gestures in a spoken dialogue system. Results indicating that an animated cartoon-like character can be a significant contribution to speech intelligibility, are also reported.\n",
    ""
   ]
  },
  "bregler97_avsp": {
   "authors": [
    [
     "Christoph",
     "Bregler"
    ],
    [
     "Michele",
     "Covell"
    ],
    [
     "Malcolm",
     "Slaney"
    ]
   ],
   "title": "Video rewrite: visual speech synthesis from video",
   "original": "av97_153",
   "page_count": 4,
   "order": 39,
   "p1": "153",
   "pn": "156",
   "abstract": [
    "Video Rewrite uses existing footage to create automatically new video of a person mouthing words that she did not speak in the original footage. Video Rewrite uses computer-vision techniques to track points on the speaker's mouth in the training footage, and morphing techniques to combine these mouth gestures into the final video sequence. The new video combines the dynamics of the original actor's articulations with the mannerisms and setting dictated by the background footage. Video Rewrite is the first facial-animation system to automate all the labeling and assembly tasks required to resync existing footage to a new soundtrack.\n",
    ""
   ]
  }
 },
 "sessions": [
  {
   "title": "Papers",
   "papers": [
    "campbell97_avsp",
    "magnocaldognetto97_avsp",
    "hiki97_avsp",
    "leybaert97_avsp",
    "poggi97_avsp",
    "auerjr97_avsp",
    "cerrato97_avsp",
    "gagne97_avsp",
    "magnocaldognetto97b_avsp",
    "girin97_avsp",
    "yehia97_avsp",
    "lyxell97_avsp",
    "andreobrecht97_avsp",
    "cox97_avsp",
    "krone97_avsp",
    "rogozan97_avsp",
    "potamianos97_avsp",
    "jourlin97_avsp",
    "remez97_avsp",
    "gelder97_avsp",
    "schwippert97_avsp",
    "campbell97b_avsp",
    "bernstein97_avsp",
    "burnham97_avsp",
    "bertelson97_avsp",
    "sams97_avsp",
    "ichikawa97_avsp",
    "raducanu97_avsp",
    "meier97_avsp",
    "reveret97_avsp",
    "basu97_avsp",
    "vogt97_avsp",
    "anderson97_avsp",
    "mcallister97_avsp",
    "yamamoto97_avsp",
    "ezzat97_avsp",
    "goff97_avsp",
    "beskow97_avsp",
    "bregler97_avsp"
   ]
  }
 ]
}